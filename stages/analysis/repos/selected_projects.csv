_id,url,Select (Dmitry),Comments,Select (Vincenzo),Select (total),I1,I2,I3,E1,E2,E3,E4,E5,E6,Selected,description,readme.text,stars,topic,primaryLanguage.name,languagesN,languages[0].name,languages[1].name,languages[2].name,languages[3].name,languages[4].name,languages[5].name,languages[6].name,languages[7].name,languages[8].name,languages[9].name,languages[10].name,languages[11].name,languages[12].name,languages[13].name,languages[14].name,languages[15].name,languages[16].name,languages[17].name,languages[18].name,languages[19].name,languages[20].name,languages[21].name,languages[22].name,languages[23].name,languages[24].name,languages[25].name,languages[26].name,languages[27].name,PRsAllN,PRsClosedN,PRsMergedN,PRsOpenN,branchesN,contributorsN,deploymentsN,diskUsage,forkCount,issuesAllN,issuesClosedN,issuesOpenN,lastCommit.oid,lastCommit.messageHeadline,lastCommit.committedDate,lastCommit.author.name,lastCommit.author.email,lastCommit.author.user.login,latestRelease.name,latestRelease.description,latestRelease.tag.name,latestRelease.author.name,latestRelease.author.email,latestRelease.author.login,licenseInfo.name,name,owner,releasesN,repositoryTopics[0].topic.name,repositoryTopics[1].topic.name,repositoryTopics[2].topic.name,repositoryTopics[3].topic.name,repositoryTopics[4].topic.name,repositoryTopics[5].topic.name,repositoryTopics[6].topic.name,repositoryTopics[7].topic.name,repositoryTopics[8].topic.name,repositoryTopics[9].topic.name,repositoryTopics[10].topic.name,repositoryTopics[11].topic.name,repositoryTopics[12].topic.name,repositoryTopics[13].topic.name,repositoryTopics[14].topic.name,repositoryTopics[15].topic.name,repositoryTopics[16].topic.name,repositoryTopics[17].topic.name,repositoryTopics[18].topic.name,repositoryTopics[19].topic.name,resourcePath,tagsN,watchersN,isInOrganization,licenseInfo,lastCommit.author.user,primaryLanguage,latestRelease,latestRelease.author,Column1,Column2,Column3,Column4,Column5,Column6,Column7,Column8,Column9,Column10,Column11,Column12,Column13,Column14,Column15,Column16,Column17,Column18,Column19,Column20,Column21,Column22,Column23,Column24,Column25,Column26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/zenustech/zeno,https://github.com/zenustech/zeno,0,,,0,0,0,0,0,0,1,0,0,0,0,ZEn NOde system - a simulation & rendering engine in nodes,"# ZENO node system\n\n[![CMake](https://github.com/zenustech/zeno/actions/workflows/cmake.yml/badge.svg)](https://github.com/zenustech/zeno/actions/workflows/cmake.yml)\n[![License](https://img.shields.io/badge/license-MPLv2-blue)](LICENSE)\n[![Version](https://img.shields.io/github/v/release/zenustech/zeno)](https://github.com/zenustech/zeno/releases)\n\n![Lines of code](https://img.shields.io/tokei/lines/github/zenustech/zeno)\n![Code size](https://img.shields.io/github/languages/code-size/zenustech/zeno)\n![Repo size](https://img.shields.io/github/repo-size/zenustech/zeno)\n\n![Commit activity](https://img.shields.io/github/commit-activity/m/zenustech/zeno)\n![Commits since latest release](https://img.shields.io/github/commits-since/zenustech/zeno/latest)\n![GitHub contributors](https://img.shields.io/github/contributors/zenustech/zeno)\n\n![GitHub release downloads](https://img.shields.io/github/downloads/zenustech/zeno/total)\n![GitHub latest release downloads](https://img.shields.io/github/downloads/zenustech/zeno/latest/total)\n![Repo stars](https://img.shields.io/github/stars/zenustech/zeno?style=social)\n\n[Download](https://github.com/zenustech/zeno/releases) | [Repo](https://github.com/zenustech/zeno) | [About us](https://zenustech.com) | [Docs](https://doc.zenustech.com/) | [Videos](https://space.bilibili.com/263032155) | [Q&A Forum](https://github.com/zenustech/zeno/discussions) | [Build from source](https://github.com/zenustech/zeno/blob/master/BUILD.md) | [FAQs](https://github.com/zenustech/zeno/blob/master/docs/FAQ.md) | [Contributor Guidelines](https://github.com/zenustech/zeno/blob/master/docs/CONTRIBUTING.md) | [Bug report](https://github.com/zenustech/zeno/issues)\n\n[国内高速下载](https://zenustech.com/d/) | [Gitee 镜像仓库](https://gitee.com/zenustech/zeno) | [公司主页](https://zenustech.com) | [中文文档](https://doc.zenustech.com/) | [视频教程](https://space.bilibili.com/263032155) | [问答论坛](https://github.com/zenustech/zeno/discussions) | [从源码构建](https://github.com/zenustech/zeno/blob/master/BUILD.md) | [常见问题](https://github.com/zenustech/zeno/blob/master/docs/FAQ.md) | [贡献者指南](https://github.com/zenustech/zeno/blob/master/docs/CONTRIBUTING.md) | [BUG 反馈](https://github.com/zenustech/zeno/issues)\n\nOpen-source node system framework, to change your algorithmic code into useful tools to create much more complicated simulations!\n\n<img src=""https://zenustech.oss-cn-beijing.aliyuncs.com/Place-in-Github/202312/ZENO2_v2023.jpg"" width=""640"" position=""left"">\n\nZENO is an open-source, Node based 3D system able to produce cinematic physics effects at High Efficiency, it was designed for large scale simulations and has been tested on complex setups.\nAside of its simulation Tools, ZENO provides necessary visualization nodes for users to import and run simulations if you feel that the current software you are using is too slow.\n\n- [Contributor guidelines](docs/CONTRIBUTING.md)\n- [How to build from source](BUILD.md)\n- [FAQ & troubleshooting](docs/FAQ.md)\n- [Introduction on Zeno](docs/introduction.md)\n- [Video tutorial series](https://space.bilibili.com/263032155)\n\n## Features\n\nIntegrated Toolbox, from volumetric geometry process tools (OpenVDB), to state-of-art, commercially robust, highly optimized physics solvers and visualization nodes, and various VFX and simulation solutions based on our nodes (provided by .zsg file in `graphs/` folder).\n\n## New\n\nMulti Importance Sampling\n\n<img src=""https://zenustech.oss-cn-beijing.aliyuncs.com/Place-in-Github/202307/multi_importace_sampling.jpg"" width=""640"" position=""left"">\n\n## Gallery\n\nFig.1 - Cloth simulation\n\n<img src=""https://zenustech.oss-cn-beijing.aliyuncs.com/Place-in-Github/202304/cloth.gif"" width=""640"" position=""left"">\n\nFig.2 - Fluid simulation\n\n<img src=""https://zenustech.oss-cn-beijing.aliyuncs.com/Place-in-Github/202304/flip.png"" width=""640"" position=""left"">\n<img src=""https://zenustech.oss-cn-beijing.aliyuncs.com/Place-in-Github/202304/liulang.gif"" width=""640"" position=""left"">\n\nFig.3 - Rigid simulation\n\n<img src=""https://zenustech.oss-cn-beijing.aliyuncs.com/Place-in-Github/202208/Bullet_Simulation.gif"" width=""640"" position=""left"">\n\nFig.4 - Biological simulation\n\n<img src=""https://zenustech.oss-cn-beijing.aliyuncs.com/Place-in-Github/202208/Biological_Simulation.gif"" width=""640"" position=""left"">\n\nFig.5 - Procedural material\n\n<img src=""https://zenustech.oss-cn-beijing.aliyuncs.com/Place-in-Github/202208/Procedural_Material.gif"" width=""640"" position=""left"">\n\nFig.6 - Procedural modeling\n\n<img src=""https://zenustech.oss-cn-beijing.aliyuncs.com/Place-in-Github/202304/programmatic.gif"" width=""640"" position=""left"">\n\nFig.7 - Human rendering\n\n<img src=""https://zenustech.oss-cn-beijing.aliyuncs.com/Place-in-Github/202304/face.png"" width=""640"" position=""left"">\n\n\nhttps://user-images.githubusercontent.com/25457920/234779878-a2f43b2f-5b9b-463b-950b-8842dad0c651.MP4\n\n\n\n# End-user Installation\n\n## Download binary release\n\nGo to the [release page](https://github.com/zenustech/zeno/releases/), and click Assets -> download `zeno-windows-20xx.x.x.zip` (`zeno-linux-20xx.x.x.tar.gz` for Linux).\n\nThen, extract this archive, and simply run `000_start.bat` (`./000_start.sh` for Linux), then the node editor window will shows up if everything is working well.\n\nApart from the GitHub release page, we also offer binary download from our official site for convinence of Chinese users: https://zenustech.com/d/\n\n## How to play\n\nThere are some example graphs in the `misc/graphs/` folder, you may open them in the editor and have fun!\nHint: To run an animation for 100 frames, change the `1` on the bottom-right of the viewport to `100`, then click `Run`.\nAlso MMB to drag in the node editor, LMB click on sockets to create connections.\nMMB drag in the viewport to orbit camera, Shift+MMB to pan camera.\nMore details are available in [our official tutorial](https://doc.zenustech.com/) and [my video tutorials](https://space.bilibili.com/263032155).\n\n## Bug report\n\nIf you find the binary version didn't worked properly or some error message has been thrown on your machine, please let me know by opening an [issue](https://github.com/zenustech/zeno/issues) on GitHub, thanks for you support!\n\n\n# Developer Build\n\nTo build ZENO from source, you need:\n\n- GCC 9+ or MSVC 19.28+, and CMake 3.16+ to build ZENO.\n- Qt 5.14+ to build the ZENO Qt editor.\n- (Optional) TBB for parallel support.\n- (Optional) OpenVDB for volume nodes.\n- (Optional) Eigen3 for solver nodes.\n- (Optional) CGAL for geometry nodes.\n- (Optional) CUDA 11.6 for GPU nodes.\n\n> Hint: WSL is not recommended because of its limited GUI and OpenGL support.\n\n- [Click me for detailed build instructions](BUILD.md)\n\n\n# Miscellaneous\n\n## Contributors\n\nThank you to all the people who have already contributed to ZENO!\n\n[![Contributors](https://contrib.rocks/image?repo=zenustech/zeno)](https://github.com/zenustech/zeno/graphs/contributors)\n\n- [Contributor guidelines and helps](docs/CONTRIBUTING.md)\n\n## Write your own extension!\n\nSee [`projects/FBX`](https://github.com/zenustech/zeno/projects/FBX) for an example on how to write custom nodes in ZENO.\n\n## Legacy version of Zeno\n\nCurrently the [`master`](https://github.com/zenustech/tree/master) branch is for Zeno 2.0.\nYou may find Zeno 1.0 in the [`zeno_old_stable`](https://github.com/zenustech/tree/zeno_old_stable) branch.\n\n## License\n\nZENO is licensed under the Mozilla Public License Version 2.0, see [LICENSE](LICENSE) for more information.\n\nZENO have also used many third-party libraries, some of which has little modifications. Their licenses could be found at [docs/licenses](docs/licenses).\n\n## Contact us\n\nYou may contact us via WeChat:\n\n* @zhxx1987: shinshinzhang\n\n* @archibate: tanh233\n\n... or sending E-mail:\n\n* @archibate: pengyb@zenustech.com\n\nJobs offering: zenustech.com/jobs\n",1166,graphics,C++,14,C++,CMake,Makefile,C,Python,Shell,Cuda,Batchfile,Assembly,HTML,GLSL,Lua,PowerShell,Dockerfile,,,,,,,,,,,,,,,1891,169,1708,14,413,47,2,594036,138,53,35,18,6dbb6f52ae7c488c4be28effa3e206f892e75a72,upd zpc,2024-07-18T15:15:56Z,littlemine,wxlwxl1993@zju.edu.cn,littlemine,16.11.2023,仅windows可用（直接解压即可），其他平台自行编译使用。,16.11.2023,luzh,,legobadman,Mozilla Public License 2.0,zeno,zenustech,51,node-editor,cpp,vfx,graphics,simulation,dataflow-programming,visualization,3d,rendering,,,,,,,,,,,,/zenustech/zeno,56,39,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/yt-project/yt,https://github.com/yt-project/yt,0,,,0,1,1,1,0,0,1,1,0,0,0,Main yt repository,"# The yt Project\n\n[![PyPI](https://img.shields.io/pypi/v/yt)](https://pypi.org/project/yt)\n[![Supported Python Versions](https://img.shields.io/pypi/pyversions/yt)](https://pypi.org/project/yt/)\n[![Latest Documentation](https://img.shields.io/badge/docs-latest-brightgreen.svg)](http://yt-project.org/docs/dev/)\n[![Users' Mailing List](https://img.shields.io/badge/Users-List-lightgrey.svg)](https://mail.python.org/archives/list/yt-users@python.org//)\n[![Devel Mailing List](https://img.shields.io/badge/Devel-List-lightgrey.svg)](https://mail.python.org/archives/list/yt-dev@python.org//)\n[![Data Hub](https://img.shields.io/badge/data-hub-orange.svg)](https://hub.yt/)\n[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](http://numfocus.org)\n[![Sponsor our Project](https://img.shields.io/badge/donate-to%20yt-blueviolet)](https://numfocus.salsalabs.org/donate-to-yt/index.html)\n\n<!--- Tests and style --->\n[![Build and Test](https://github.com/yt-project/yt/actions/workflows/build-test.yaml/badge.svg)](https://github.com/yt-project/yt/actions/workflows/build-test.yaml)\n[![CI (bleeding edge)](https://github.com/yt-project/yt/actions/workflows/bleeding-edge.yaml/badge.svg)](https://github.com/yt-project/yt/actions/workflows/bleeding-edge.yaml)\n[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/yt-project/yt/main.svg)](https://results.pre-commit.ci/latest/github/yt-project/yt/main)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v2.json)](https://github.com/charliermarsh/ruff)\n\n<!--- [![codecov](https://codecov.io/gh/yt-project/yt/branch/main/graph/badge.svg)](https://codecov.io/gh/yt-project/yt) --->\n\n<a href=""http://yt-project.org""><img src=""https://raw.githubusercontent.com/yt-project/yt/main/doc/source/_static/yt_logo.png"" width=""300""></a>\n\nyt is an open-source, permissively-licensed Python library for analyzing and\nvisualizing volumetric data.\n\nyt supports structured, variable-resolution meshes, unstructured meshes, and\ndiscrete or sampled data such as particles. Focused on driving\nphysically-meaningful inquiry, yt has been applied in domains such as\nastrophysics, seismology, nuclear engineering, molecular dynamics, and\noceanography. Composed of a friendly community of users and developers, we want\nto make it easy to use and develop - we'd love it if you got involved!\n\nWe've written a [method\npaper](https://ui.adsabs.harvard.edu/abs/2011ApJS..192....9T) you may be interested\nin; if you use yt in the preparation of a publication, please consider citing\nit.\n\n## Code of Conduct\n\nyt abides by a code of conduct partially modified from the PSF code of conduct,\nand is found [in our contributing\nguide](http://yt-project.org/docs/dev/developing/developing.html#yt-community-code-of-conduct).\n\n## Installation\n\nYou can install the most recent stable version of yt either with conda from\n[conda-forge](https://conda-forge.org/):\n\n```shell\nconda install -c conda-forge yt\n```\n\nor with pip:\n\n```shell\npython -m pip install yt\n```\n\nMore information on the various ways to install yt, and in particular to install from source,\ncan be found on [the project's website](https://yt-project.org/docs/dev/installing.html).\n\n## Getting Started\n\nyt is designed to provide meaningful analysis of data.  We have some Quickstart\nexample notebooks in the repository:\n\n * [Introduction](https://github.com/yt-project/yt/tree/main/doc/source/quickstart/1\)_Introduction.ipynb)\n * [Data Inspection](https://github.com/yt-project/yt/tree/main/doc/source/quickstart/2\)_Data_Inspection.ipynb)\n * [Simple Visualization](https://github.com/yt-project/yt/tree/main/doc/source/quickstart/3\)_Simple_Visualization.ipynb)\n * [Data Objects and Time Series](https://github.com/yt-project/yt/tree/main/doc/source/quickstart/4\)_Data_Objects_and_Time_Series.ipynb)\n * [Derived Fields and Profiles](https://github.com/yt-project/yt/tree/main/doc/source/quickstart/5\)_Derived_Fields_and_Profiles.ipynb)\n * [Volume Rendering](https://github.com/yt-project/yt/tree/main/doc/source/quickstart/6\)_Volume_Rendering.ipynb)\n\nIf you'd like to try these online, you can visit our [yt Hub](https://hub.yt/)\nand run a notebook next to some of our example data.\n\n## Contributing\n\nWe love contributions!  yt is open source, built on open source, and we'd love\nto have you hang out in our community.\n\nWe have developed some [guidelines](CONTRIBUTING.rst) for contributing to yt.\n\n**Imposter syndrome disclaimer**: We want your help. No, really.\n\nThere may be a little voice inside your head that is telling you that you're not\nready to be an open source contributor; that your skills aren't nearly good\nenough to contribute. What could you possibly offer a project like this one?\n\nWe assure you - the little voice in your head is wrong. If you can write code at\nall, you can contribute code to open source. Contributing to open source\nprojects is a fantastic way to advance one's coding skills. Writing perfect code\nisn't the measure of a good developer (that would disqualify all of us!); it's\ntrying to create something, making mistakes, and learning from those\nmistakes. That's how we all improve, and we are happy to help others learn.\n\nBeing an open source contributor doesn't just mean writing code, either. You can\nhelp out by writing documentation, tests, or even giving feedback about the\nproject (and yes - that includes giving feedback about the contribution\nprocess). Some of these contributions may be the most valuable to the project as\na whole, because you're coming to the project with fresh eyes, so you can see\nthe errors and assumptions that seasoned contributors have glossed over.\n\n(This disclaimer was originally written by\n[Adrienne Lowe](https://github.com/adriennefriend) for a\n[PyCon talk](https://www.youtube.com/watch?v=6Uj746j9Heo), and was adapted by yt\nbased on its use in the README file for the\n[MetPy project](https://github.com/Unidata/MetPy))\n\n## Resources\n\nWe have some community and documentation resources available.\n\n * Our latest documentation is always at http://yt-project.org/docs/dev/ and it\n   includes recipes, tutorials, and API documentation\n * The [discussion mailing\n   list](https://mail.python.org/archives/list/yt-users@python.org//)\n   should be your first stop for general questions\n * The [development mailing\n   list](https://mail.python.org/archives/list/yt-dev@python.org//) is\n   better suited for more development issues\n * You can also join us on Slack at yt-project.slack.com ([request an\n   invite](https://yt-project.org/slack.html))\n\nIs your code compatible with yt ? Great ! Please consider giving us a shoutout as a shiny badge in your README\n\n- markdown\n```markdown\n[![yt-project](https://img.shields.io/static/v1?label=""works%20with""&message=""yt""&color=""blueviolet"")](https://yt-project.org)\n```\n- rst\n```reStructuredText\n|yt-project|\n\n.. |yt-project| image:: https://img.shields.io/static/v1?label=""works%20with""&message=""yt""&color=""blueviolet""\n   :target: https://yt-project.org\n```\n\n## Powered by NumFOCUS\n\nyt is a fiscally sponsored project of [NumFOCUS](https://numfocus.org/).\nIf you're interested in\nsupporting the active maintenance and development of this project, consider\n[donating to the project](https://numfocus.salsalabs.org/donate-to-yt/index.html).\n",456,astronomy,Python,9,Python,Shell,C,C++,HTML,Cuda,CSS,JavaScript,Cython,,,,,,,,,,,,,,,,,,,,2584,300,2232,52,10,161,0,410331,272,2357,2009,348,874cb3f8f0112aee1da7c7138ba18c365b710a40,Add support for Enzo-E simulations with 0 ghost zones (#4932),2024-07-16T14:55:50Z,Matthew Abruzzo,matthewabruzzo@gmail.com,mabruzzo,yt-4.3.1,"## Summary\r\nThis is the first bugfix release in the yt 4.3.x series. It contains fixes to bugs discovered since the 4.3.0 release.\r\n\r\nThere are no new features or deprecations in this version, all users are encouraged to upgrade.\r\n\r\nPython 3.9.2 to 3.12 is supported.\r\n\r\n\r\n### How to upgrade\r\n\r\nTo upgrade from PyPI, run\r\n```\r\npython -m pip install --upgrade yt\r\n```\r\nor, with conda\r\n```\r\nconda update --channel conda-forge yt\r\n```\r\n\r\n## 🌟 Highlights\r\nThis is our first release with compatibility for NumPy 2.0 and unyt 3.0\r\nNumPy 2 and unyt 3 both contain nice features as well as breaking changes.\r\nUpgrading them is now possible but remains optional for the time being: we still\r\nsupport NumPy 1.19 and unyt 2.9 respectively.\r\n\r\n## 🐛 Bug fixes\r\n#4703 Fix broken urllib imports, by @neutrinoceros (issue #4700 by @yut23)\r\n#4699 Make `annotate_sphere` and `annotate_arrow` safe when run after plot invalidation, by @chrishavlin (issue #4698 by @neutrinoceros)\r\n#4722+#4724 Fix an incompatibility with unyt 3.0 (`amu_cgs` doesn't exist as a physical constant anymore), by @neutrinoceros (issue #4162)\r\n#4725 Handle deprecation warnings from numpy 2.0.0dev0 (`np.row_stack` -> `np.vstack` and `np.in1d` -> `np.isin`), by @neutrinoceros\r\n#4745 Avoid a deprecation warning in yt.load_sample on Python 3.12, by @neutrinoceros (issue #4689)\r\n#4768 Switch to field tuples for default fields in particle_trajectories, by @mtryan83 (issue #4766)\r\n#4792 Enforce Figure dpi in _show_mpl, by @chrishavlin (issue #4785 by @neutrinoceros)\r\n#4802 Fix bug where race condition results in incorrect fields categorization when computing particle_trajectories, by @mtryan83\r\n#4825 Use the validated center for YTCuttingPlane set_field_parameter, by @chrishavlin (issue #4823 by @biboyd)\r\n#4829 Avoid usage of deprecated numpy.core namespace (numpy 2 compat), by @neutrinoceros\r\n#4839 Fix geographic coordinate conversions, by @chrishavlin \r\n\r\n### 🤖 Frontend-specific fixes\r\n#4686 Minor bugfix in cholla frontend, by @mabruzzo\r\n#4708 The f90nml package is *optional* to load RAMSES, by @cphyc\r\n#4746 Avoid a noisy self-triggered warning about libconf being a hard dependency to load any enzo data, by @neutrinoceros\r\n#4797 Correct domain_dimensions in non-fixed block size uniform grid FLASH output, by @acreyes\r\n#4758 Allow boxlib frontend to read 1D cylindrical datasets by @yut23\r\n#4805 Add support for Quokka datasets, by @yut23\r\n#4801 Fix order of Athena++ cylindrical coords, by @forrestglines\r\n#4815 Assign domain dims after updates in Athena, by @mattewturk\r\n#4828 Fix parameter parsing for Castro and MAESTRO, by @yut23\r\n\r\n## 📚Documentation\r\n#4719 Fix notebook links, by @Xarthisius\r\n#4743 Use axis_order kwarg in geo notebook, by @chrishavlin\r\n#4776 Fix a broken docs example, by @neutrinoceros (issue #4774 by @xshaokun)\r\n\r\n## 🧩 Build and testing\r\n#4759 Update xarray integration test to use non-deprecated API, by @neutrinoceros\r\n#4764 Enzo testing framework - ignore units for ShockTube tests, by @clairekope\r\n#4778 Explicitly set minimum Python to 3.9.2, by @chrishavlin\r\n#4784 Ignore deprecation warning from pandas (Pyarrow will become a required dependency), by @neutrinoceros\r\n#4830 Ditch `np.int_t`, removed in numpy 2's C API (use np.int64_t instead), by @neutrinoceros\r\n#4852 Migrate module-level setup/teardown functions to pytest, by @neutrinoceros (issue #4850)\r\n#4859 Add support for numpy 2, by @neutrinoceros \r\n\r\n## Other fixes\r\n#4753 Add info about supported simulation types in YTSimulationNotIdentified, by @neutrinoceros \r\n#4819 Catch Py312+ warning when using ratarmount, by @cphyc (issue #4799 by @neutrinoceros)\r\n#4806 Replace vendored version of numpy.trapz with supported API in numpy 2, by @neutrinoceros\r\n#4835 Remove untested and broken notebook command from CLI, by @neutrinoceros\r\n\r\n\r\n\r\n**Full Changelog**: https://github.com/yt-project/yt/compare/yt-4.3.0...yt-4.3.1",yt-4.3.1,Clément Robert,,neutrinoceros,Other,yt,yt-project,14,data,visualization,analysis,python,scientific-computing,scientific-visualization,astrophysics,astronomy,geophysics,nuclear-engineering,finite-element-analysis,data-visualization,,,,,,,,,/yt-project/yt,58,21,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/XCharts-Team/XCharts,https://github.com/XCharts-Team/XCharts,0,,,0,0,0,0,0,0,1,1,1,0,0,A charting and data visualization library for Unity.   Unity数据可视化图表插件。,"<p align=""center"">\n  <a href="""">\n    <img src="""" alt="""" width="""" height=""""></img>\n  </a>\n</p>\n<h2 align=""center"">XCharts</h2>\n<p align=""center"">\n  A powerful, easy-to-use, configurable charting and data visualization library for Unity.\n  <br/>\n  Unity数据可视化图表插件。\n  <br/>\n  <a href=""README-en.md"">English README</a>\n</p>\n<p align=""center"">\n  <a href=""https://github.com/XCharts-Team/XCharts/blob/master/LICENSE"">\n    <img src=""https://img.shields.io/github/license/XCharts-Team/XCharts""></img>\n  </a>\n  <a href=""https://github.com/XCharts-Team/XCharts/releases"">\n    <img src=""https://img.shields.io/github/v/release/XCharts-Team/XCharts?include_prereleases""></img>\n  </a>\n  <a href=""https://github.com/XCharts-Team/XCharts"">\n    <img src=""https://img.shields.io/github/repo-size/monitor1394/unity-ugui-xcharts""></img>\n  </a>\n  <a href=""https://github.com/XCharts-Team/XCharts"">\n    <img src=""https://img.shields.io/github/languages/code-size/monitor1394/unity-ugui-xcharts""></img>\n  </a>\n  <a href=""https://xcharts-team.github.io/docs/tutorial01"">\n    <img src=""https://img.shields.io/badge/Unity-5.6+-green""></img>\n  </a>\n  <a href=""https://xcharts-team.github.io/docs/tutorial01"">\n    <img src=""https://img.shields.io/badge/TextMeshPro-YES-green""></img>\n  </a>\n</p>\n<p align=""center"">\n  <a href=""https://github.com/XCharts-Team/XCharts/stargazers"">\n    <img src=""https://img.shields.io/github/stars/XCharts-Team/XCharts?style=social""></img>\n  </a>\n  <a href=""https://github.com/XCharts-Team/XCharts/forks"">\n    <img src=""https://img.shields.io/github/forks/XCharts-Team/XCharts?style=social""></img>\n  </a>\n  <a href=""https://github.com/XCharts-Team/XCharts/issues"">\n    <img src=""https://img.shields.io/github/issues-closed/XCharts-Team/XCharts?color=green&label=%20%20%20%20issues&logoColor=green&style=social""></img>\n  </a>\n</p>\n\n![XCharts](Documentation~/zh/img/xcharts.png)\n\n一款基于`UGUI`的功能强大的简单易用的`Unity`数据可视化图表插件。支持`折线图`、`柱状图`、`饼图`、`雷达图`、`散点图`、`热力图`、`环形图`、`K线图`、`极坐标`、`平行坐标`等十几种内置图表，以及`3D饼图`、`3D柱图`、`3D金字塔`、`漏斗图`、`仪表盘`、`水位图`、`象形柱图`、`甘特图`、`矩形树图`、`桑基图`、`3D折线图`、`关系图`等十几种扩展图表。\n\n[XCharts 官方主页](https://xcharts-team.github.io)  \n[XCharts 在线示例](https://xcharts-team.github.io/examples)  \n\n[XCharts 教程：5分钟上手 XCharts](Documentation~/zh/tutorial01.md)  \n[XCharts API文档](Documentation~/zh/api.md)  \n[XCharts 常见问题](Documentation~/zh/faq.md)  \n[XCharts 配置项手册](Documentation~/zh/configuration.md)  \n[XCharts 更新日志](Documentation~/zh/changelog.md)  \n[XCharts 订阅服务](Documentation~/zh/support.md)  \n\n## 特性\n\n- __纯代码绘制__：图表纯代码绘制，无需额外的贴图和Shader资源。\n- __可视化配置__：参数可视化配置，效果实时预览，支持运行时代码动态修改配置和数据。\n- __高自由定制__：支持从主题和配置参数上任意调整；支持代码自定义绘制，自定义回调以及自定义实现图表。\n- __多内置图表__：支持线图、柱状图、饼图、雷达图、散点图、热力图、环形图、K线图、极坐标、平行坐标等多种内置图表。\n- __多扩展图表__：支持3D柱图、3D饼图、漏斗图、金字塔、仪表盘、水位图、象形柱图、甘特图、矩形树图、桑基图、3D折线图、关系图等多种扩展图表。\n- __多扩展组件__：支持表格、统计数值等多种扩展UI组件。\n- __多图表组合__：支持内置图表的任意组合，同一图中可同时显示多个相同或不同类型的图表。\n- __多种坐标系__：支持直角坐标系、极坐标系、单轴等多种坐标系。\n- __丰富的组件__：支持标题、图例、提示框、标线、标域、数据区域缩放、视觉映射等常用组件。\n- __多样式线图__：支持直线图、曲线图、虚线图、面积图、阶梯线图等多种线图。\n- __多样式柱图__：支持并列柱图、堆叠柱图、堆积百分比柱图、斑马柱图、胶囊柱图等多种柱状图。\n- __多样式饼图__：支持环形图、玫瑰图、环形玫瑰图等多种饼图。\n- __丰富的线条__：支持实线、曲线、阶梯线、虚线、点线、点划线、双点划线等线条。\n- __自定义绘制__：支持自定义图表内容绘制，提供绘制点、线、面等其他图形的强大的绘图API。\n- __大数据绘制__：支持万级大数据量绘制；支持采样绘制；特殊的简化图表支持更优的性能。\n- __自定义主题__：支持主题定制、导入和导出，内置明暗两种默认主题。\n- __动画和交互__：支持渐入动画、渐出动画、变更动画、新增动画、交互动画等多种动画；支持多平台的数据筛选、视图缩放、细节展示等交互操作。\n- __第三方扩展__：支持无缝接入`TexMeshPro`和`New Input System`。\n- __版本和兼容__：支持所有`5.6`以上的`Unity`版本；支持全平台运行。\n\n## 截图\n\n![内置图表](Documentation~/zh/img/readme_buildinchart.png)\n\n![扩展图表](Documentation~/zh/img/readme_extendchart.png)\n\n## 仓库\n\n- __[XCharts](https://github.com/XCharts-Team/XCharts)__ XCharts核心功能，完全免费。\n- __[XCharts-Daemon](https://github.com/XCharts-Team/XCharts-Daemon)__ XCharts守护程序，用于确保XCharts更新时的编译正常。非必须，但建议使用。\n- __[XCharts-Demo](https://github.com/XCharts-Team/XCharts-Demo)__ XCharts官方示例，不包含扩展图表部分。订阅购买扩展图表后可导入示例。\n- __[XCharts-Pro](https://github.com/XCharts-Team/XCharts-Pro)__ XCharts专业版，包含所有扩展图表和扩展组件。订阅SVIP后可访问使用。\n- __[XCharts-Pro-Demo](https://github.com/XCharts-Team/XCharts-Pro-Demo)__ XCharts专业版官方示例，包含所有扩展图表和扩展组件的示例。订阅SVIP后可访问使用。\n- __[XCharts-UI](https://github.com/XCharts-Team/XCharts-UI)__ XCharts的扩展UI组件。订阅VIP后可访问使用。\n- __[XCharts-Bar3DChart](https://github.com/XCharts-Team/XCharts-Bar3DChart)__ XCharts扩展图表：3D柱图。订阅购买后可访问使用。\n- __[XCharts-FunnelChart](https://github.com/XCharts-Team/XCharts-FunnelChart)__ XCharts扩展图表：漏斗图。订阅购买后可访问使用。\n- __[XCharts-GanttChart](https://github.com/XCharts-Team/XCharts-GanttChart)__ XCharts扩展图表：甘特图。订阅购买后可访问使用。\n- __[XCharts-GaugeChart](https://github.com/XCharts-Team/XCharts-GaugeChart)__ XCharts扩展图表：仪表盘。订阅购买后可访问使用。\n- __[XCharts-LiquidChart](https://github.com/XCharts-Team/XCharts-LiquidChart)__ XCharts扩展图表：水位图。订阅购买后可访问使用。\n- __[XCharts-PictorialBarChart](https://github.com/XCharts-Team/XCharts-PictorialBarChart)__ XCharts扩展图表：象形住图。订阅购买后可访问使用。\n- __[XCharts-Pie3DChart](https://github.com/XCharts-Team/XCharts-Pie3DChart)__ XCharts扩展图表：3D饼图。订阅购买后可访问使用。\n- __[XCharts-PyramidChart](https://github.com/XCharts-Team/XCharts-PyramidChart)__ XCharts扩展图表：3D金字塔。订阅购买后可访问使用。\n- __[XCharts-TreemapChart](https://github.com/XCharts-Team/XCharts-TreemapChart)__ XCharts扩展图表：矩形树图。订阅购买后可访问使用。\n- __[XCharts-SankeyChart](https://github.com/XCharts-Team/XCharts-SankeyChart)__ XCharts扩展图表：桑基图。订阅购买后可访问使用。\n- __[XCharts-Line3DChart](https://github.com/XCharts-Team/XCharts-Line3DChart)__ XCharts扩展图表：3D折线图。订阅购买后可访问使用。\n- __[XCharts-GraphChart](https://github.com/XCharts-Team/XCharts-GraphChart)__ XCharts扩展图表：关系图。订阅购买后可访问使用。\n\n## 分支\n\n- __[master](https://github.com/XCharts-Team/XCharts/tree/master)__ XCharts3.0的开发分支。最新的修改和功能都先提交到`master`分支，稳定后再发布`release`版本。\n- __[3.0](https://github.com/XCharts-Team/XCharts/tree/3.0)__ XCharts3.0的稳定分支。一般一个月一发布，`master`分支稳定后，`merge`到`3.0`分支，并发布`release`版本。\n- __[2.0](https://github.com/XCharts-Team/XCharts/tree/2.0)__ XCharts2.0的稳定分支。带Demo，目前基本不再维护，仅修改严重bug。\n- __[2.0-upm](https://github.com/XCharts-Team/XCharts/tree/2.0-upm)__ XCharts2.0的稳定UMP分支。不带Demo，只包含Package部分，不再维护。\n- __[1.0](https://github.com/XCharts-Team/XCharts/tree/1.0)__ XCharts1.0的稳定分支。带Demo，不再维护。\n- __[1.0-upm](https://github.com/XCharts-Team/XCharts/tree/1.0-upm)__ XCharts1.0的稳定UMP分支。不带Demo，不再维护。\n\n## 使用\n\n- 导入`XCharts`的`unitypackage`或者源码到项目。建议也导入`XCharts`守护程序 [XCharts-Daemon](https://github.com/XCharts-Team/XCharts-Daemon)。\n- 在`Hierarchy`视图下右键选择`XCharts->LineChart`，即可创建一个默认的折线图。\n- 用`Inspector`视图下的`Add Serie`和`Add Main Component`按钮可以添加`Serie`和`组件`。\n- 在`Inspector`视图下可以调整各个组件的参数，`Game`视图可看到实时效果。\n- 更多细节，请看[【XCharts教程：5分钟上手教程】](Documentation~/zh/tutorial01.md)。\n- 首次使用，建议先认真看一遍教程。\n\n## 注意\n\n- __XCharts3.0不完全兼容XCharts2.0版本。__ 升级`3.0`时，部分代码和配置可能需要重新调整。建议旧项目可以继续使用`XCharts2.0`，新项目推荐使用`XCharts3.0`。\n- __XCharts2.0只维护不加新功能。__ `2.0`只修复严重`bug`，原则上不再加新功能。\n- __XCharts支持Unity 5.6及以上版本。__ 但由于版本测试有限难免疏漏，发现问题可提`Issue`。\n- __本仓库只包含XCharts源码，不包含Demo示例部分。__ 需要查看`Demo`示例源码请到[XCharts-Demo](https://github.com/XCharts-Team/XCharts-Demo)仓库。也可以在浏览器查看运行效果 [在线Demo](https://xcharts-team.github.io/examples/) 。\n\n## FAQ\n\n- __XCharts可以免费使用吗？__ \n`XCharts`使用`MIT`协议，可以免费使用。也可以订阅`VIP`享受更多增值服务。\n\n- __XCharts支持代码动态添加和修改数据吗？支持从`Excel`或数据库中获取数据吗？__ \n`XCharts`提供了各种数据操作的接口，支持代码动态修改配置，添加和修改数据，但数据来源需要自己解析和获取，再调用`XCharts`的接口添加到图表。\n\n- __XCharts除了用在Unity平台，还能用在Winform或WPF等平台吗？__  \n`XCharts`只支持在`Unity`平台使用。理论上任何支持`UGUI`的`Unity`版本都能运行`XCharts`。\n\n- __锯齿怎么解决？支持多大量级的数据？__  \n`XCharts`是基于`UGUI`实现的，所以`UGUI`中碰到的问题，在`XCharts`中也会存在。比如锯齿问题，比如`Mesh`顶点数超`65535`的问题。这两个问题的解决可参考[问答16](Documentation~/zh/faq.md)和[问答27](Documentation~/zh/faq.md)。  \n由于`Mesh`的`65535`顶点数的限制，目前`XCharts`的单条`Line`支持约`2万`的数据量，当然开启采样可以支持更多数据的绘制，但同时也会更消耗CPU。\n\n## 日志\n\n- 各版本的详细更新日志请查看 [更新日志](Documentation~/zh/changelog.md)  \n\n## Licenses\n\n- [MIT License](https://github.com/XCharts-Team/XCharts/blob/master/LICENSE.md)\n- 可免费商用，可二次开发。\n- 扩展图表和高级功能部分需购买使用授权。\n\n## 订阅\n\n- `XCharts`核心库是开源的，可免费使用的。在此基础上，我们也提供多种订阅服务以满足不同用户的需求，订阅详情[☞ 请看这里](Documentation~/zh/support.md)。\n- 订阅不是必须的，不影响`XCharts`的核心功能使用。\n- 订阅是按年付费，订阅到期后，不要求必须续订，但中断订阅期间无法享受技术更新和支持等服务。\n\n## 其他\n\n- 邮箱：`monitor1394@gmail.com`  \n- QQ群：XCharts交流群（`202030963`）  \n- VIP群：XCharts VIP群（`867291970`）  \n- 捐助、合作、订阅和技术支持：[☞ 请看这里](Documentation~/zh/support.md)\n",3154,graphics,C#,2,C#,JavaScript,,,,,,,,,,,,,,,,,,,,,,,,,,,20,2,18,0,6,7,3,48946,551,275,269,6,35e8f3e902e6796265344442bd79f33fca750d77,修复`MarkLine`的`Label`在初始化时可能会闪烁的问题,2024-07-17T15:16:22Z,monitor1394,monitor1394@gmail.com,monitor1394,,## v3.11.1(2024-7-1)\r\n\r\n* (2024.07.01) 发布`v3.11.1`版本\r\n* (2024.07.01) 修复`Serie`有多个时颜色异常的问题\r\n* (2024.06.23) 修复`Label`在初始化时会堆积的问题,v3.11.1,,,monitor1394,MIT License,XCharts,XCharts-Team,62,unity,ugui,chart,data-visualization,graphics,charting,,,,,,,,,,,,,,,/XCharts-Team/XCharts,66,42,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/wurmlab/sequenceserver,https://github.com/wurmlab/sequenceserver,0,,,0,0,0,0,0,0,1,1,0,0,0,Intuitive graphical web interface for running BLAST bioinformatics tool (i.e. have your own custom NCBI BLAST site!),"[![gem version](https://img.shields.io/badge/version-3.0-green.svg)](http://rubygems.org/gems/sequenceserver)\n<!--[![total downloads](http://ruby-gem-downloads-badge.herokuapp.com/sequenceserver?type=total&color=brightgreen)](http://rubygems.org/gems/sequenceserver) -->\n[![coverage](https://codeclimate.com/github/wurmlab/sequenceserver/badges/coverage.svg)](https://codeclimate.com/github/wurmlab/sequenceserver)\n[![gitter chat](https://badges.gitter.im/gitterHQ/gitter.png)](https://gitter.im/wurmlab/sequenceserver)\n\n\n<!--[![code climate](https://codeclimate.com/github/wurmlab/sequenceserver/badges/gpa.svg)](https://codeclimate.com/github/wurmlab/sequenceserver)-->\n<!--[![browser matrix](https://saucelabs.com/browser-matrix/yeban.svg)](https://saucelabs.com/u/yeban)-->\n\n# SequenceServer - BLAST searching made easy!\n\nSequenceServer lets you rapidly set up a BLAST+ server with an intuitive user interface for personal or group use.\n\nIf you use SequenceServer, please cite our paper: \n[Sequenceserver: A modern graphical user interface for custom BLAST databases. Molecular Biology and Evolution (2019).](https://doi.org/10.1093/molbev/msz185)\n\n\n## Installation\n\nFor installation instructions and how to use SequenceServer please see\nhttps://sequenceserver.com/ - instructions for independently installing open-source sequenceserver are linked from the footer. \n\nIf you want to run SequenceServer directly from source code, please see\n'Develop and contribute' section below.\n\nWe also offer a [hosted cloud blast service](https://sequenceserver.com) for those who prefer fully point-and-click installation and want to avoid the complexities and costs of running a server. Running the cloud service enables us to further support the development of SequenceServer. \n\n## Referral scheme\n\n[Refer friends or colleagues](https://sequenceserver.com/referral-program) to SequenceServer Cloud and earn up to $400 per person who signs up (as of 2023-10; exact terms may change).\n\n## Release notes\n\nNew releases are announced on [GitHub release page](https://github.com/wurmlab/sequenceserver/releases) and on our [Support Page](https://support.sequenceserver.com).\n\n## Reporting issues\n\nPlease report any issues here: https://github.com/wurmlab/sequenceserver/issues or on the [community support forum](https://support.sequenceserver.com)\n\n## Develop and contribute\n\nTo develop and contribute, you will need to run SequenceServer from source (see below).\n\n### Run SequenceServer from source code\n\nYou will need [Ruby](https://www.ruby-lang.org/en/) and [RubyGems](https://rubygems.org/):\n\n    # Install bundler gem to install Ruby dependencies\n    gem install bundler\n\n    # Move to where you downloaded or cloned seqserv\n    cd sequenceserver\n\n    # Use bundler to install Ruby dependencies\n    bundle install\n\n    # Use bundler to run SequenceServer\n    bundle exec bin/sequenceserver\n\nIf you do not plan to develop, you can skip installing development dependencies\nby running `bundle install --without=development`.\n\n### Run SequenceServer from Docker\n\nHaving [installed Docker](https://docs.docker.com/get-docker/), to run SequenceServer locally as a\nDocker container, using the example database from the\n[ncbi-blast+ debian package](https://packages.debian.org/sid/ncbi-blast+):\n\n* Change `from final` at the end of the `Dockerfile` to `from dev`.\n* Build the image with:\n```bash\ndocker build -t sequenceserver .\n```\n* Run a container with...\n```bash\ndocker run --rm -it -p 4567:4567 sequenceserver\n```\n* then select the defaults when prompted.\n\nOtherwise, a database will need to be copied to the `db` volume.\n\n### Making changes to the code\n\nDuring development, you should use `-D` option to run SequenceServer in development mode. In this mode, SequenceServer will log verbosely.\n\n    # Run SequenceServer in development mode\n    bundle exec bin/sequenceserver -D\n\nIf you want to modify and build frontend code, you will additionally need [Node and npm](https://nodejs.org/). You can then run a watch server that will automatically build any changes you make the frontend code:\n\n    # Install frontend dependencies\n    npm install\n\n    # Run watch server to automatically build changes to the frontend code\n    npm run-script watch\n\nAlternatively, you can manually build the frontend code after you have made your changes:\n\n    # Build minified JS and CSS bundles\n    npm run-script build\n\nIf you are using docker, you can build the frontend code and include it in the image by specifying '--target=minify' to the docker build command:\n\n    docker build . -t seqserv-with-customisations --target=minify\n\n## **Testing**\n\n### **Ruby**\n\nWe use RSpec and Capybara for testing. Our test suite covers 87% of the codebase. Tests are run automatically when you open a pull-request (see Getting code merged section below) but it may be desirable sometimes to run a single test, whole file, or all tests locally:\n\nTo run a single test (a.k.a, scenario):\n\n`bundle exec rspec spec/foo_spec.rb -e 'bar'`\n\nTo run all tests in a single file:\n\n`bundle exec rspec spec/foo_spec.rb`\n\nTo run all tests:\n\n`bundle exec rspec`\n\n### **Javascript**\n\nUnit tests for the React frontend are written using React Testing Library and jest. \n\nOne option for installing jest: `npm install --save-dev jest`\n\nTo run a single test :\n\n`npm run test -e ""test name""`\n\nTo run all tests in a single file:\n\n`npm run test file_name`\n\nTo run all tests:\n\n`npm run test`\n\n\n### Linting\n\nWe use CodeClimate for static code analysis. CodeClimate is run automatically when you open a pull-request (see Getting code merged section below) but it may be desirable sometimes to run it locally.\n\nFor this, first install CodeClimate following the instructions at https://github.com/codeclimate/codeclimate.\n\nOnce CodeClimate is installed, install the required codeclimate 'engines':\n\n    codeclimate engines:install\n\nTo run all the style checkers:\n\n    codeclimate analyze\n\nTo run eslint:\n\n    codeclimate analyze -e eslint\n\nTo run rubocop:\n\n    codeclimate analyze -e rubocop\n\nstylelint is used for CSS:\n\n    codeclimate analyze -e stylelint\n\nThe above commands respect the respective style checker's config files, e.g., .rubocopy.yml for Rubocop and so on.\n\n### GitHub Workflows\n\nTo run workflows locally, ensure [nektos/act](https://github.com/nektos/act) is installed\nas a [GitHub CLI extension](https://github.com/nektos/act#installation-as-github-cli-extension).\n\nThen, for instance, `.github/workflows/test.yml` would be run by:\n\n```\ngh act -j test\n```\n\n[action-validator](https://github.com/mpalmer/action-validator) is claimed as a yaml validator for GitHub workflows.\n\n### Getting code merged\n\nPlease open a pull-request on GitHub to get code merged. Our test suite and the CodeClimate static code analysis system will be automatically run on your pull-request. These should pass for your code to be merged. If you want to add a new feature to SequenceServer, please also add tests. In addition, code should be `rubocop` and `eslint` compliant, and hard-wrapped to 80 chars per line.\n\nIf you change frontend code (JavaScript and CSS), please build (i.e., minify and compress) and commit the resulting JS and CSS bundles before opening a pull-request. This is because SequenceServer is run in production mode by the test suite.\n\n## Contact\n\n* Yannick Wurm (PI) - [email](mailto:yannickwurm@gmail.com) | https://wurmlab.com & https://sequenceserver.com\n* [Mailing list / forum](https://support.sequenceserver.com)\n",268,bioinformatics,JavaScript,6,Ruby,JavaScript,CSS,HTML,Dockerfile,Shell,,,,,,,,,,,,,,,,,,,,,,,331,106,211,14,19,39,135,86171,112,438,359,79,4a9ffcc06b21d7f195224fa3ae0248c4db0e7006,Referral $100 -> $400,2024-07-16T13:37:03Z,Yannick Wurm,yannickwurm@gmail.com,yannickwurm,v3.0.1,"We're delighted to release SequenceServer 3!\r\n\r\n## New features!\r\n* Better reporting of BLAST database formatting errors. BLAST has become better at identifying and at reporting database errors early on... e.g., duplicate identifiers, illegal characters, or identifiers longer than 50 characters. These are now better reported by sequenceserver  -  https://github.com/wurmlab/sequenceserver/pull/692 & https://github.com/wurmlab/sequenceserver/pull/690\r\n* Automatically detect and convert FASTQ to FASTA query inputs  -  https://github.com/wurmlab/sequenceserver/pull/721 - You typically should [avoid running BLAST on FASTQ files](https://sequenceserver.com/blog/blast_illumina_reads_in_fastq_format/)... but sometimes it *is* the right thing to do.\r\n* Allow bigger sequence queries  -  https://github.com/wurmlab/sequenceserver/pull/711 - We previously limited input queries to 10Mb. That could be frustrating when trying to BLAST/annotate a transcriptome or predicted geneset. By default we now support queries up to 250Mb. This is enabled by a recent change (2.1 or 2.2) which allows users to download table/xml results for huge results rather than having to load the full html report. \r\n* SequenceServer Cloud users now get email notifications when long running jobs complete -  https://github.com/wurmlab/sequenceserver/pull/723\r\n\r\n## Maintenance, bugfixes and minor improvements\r\n* Bump Sinatra dependency to v4  -  https://github.com/wurmlab/sequenceserver/pull/726 - we were previously on v2. This also means that our rack dependency shifted from v2 to v3. **This is a major dependency change. Based on semantic versioning rules, these backwards-incompatible changes justify our bump from 2.20 to 3.0**\r\n* BLAST+ 2.15\r\n* Set encoding defaults to UTF-8 -  https://github.com/wurmlab/sequenceserver/pull/694\r\n* Results page tweaks  -  https://github.com/wurmlab/sequenceserver/pull/699\r\n* CSS paragraph clamping  -  https://github.com/wurmlab/sequenceserver/pull/700\r\n* Fix of #695 issue - https://github.com/wurmlab/sequenceserver/pull/702\r\n* Make response gzipping optional  -  https://github.com/wurmlab/sequenceserver/pull/704\r\n* Fix an issue when initializing SequenceServer with unformatted DBs  -  https://github.com/wurmlab/sequenceserver/pull/708\r\n* Fixed the job 404 exception error #666 - https://github.com/wurmlab/sequenceserver/pull/710\r\n* Drop webshim polyfill which was needed for old browsers  -  https://github.com/wurmlab/sequenceserver/pull/718\r\n* Add a dedicated 404 page  -  https://github.com/wurmlab/sequenceserver/pull/719\r\n* Re Sidebar Element Clickability Fix - https://github.com/wurmlab/sequenceserver/pull/724\r\n* And other minor changes... see the **Full Changelog**: https://github.com/wurmlab/sequenceserver/compare/v2.2.0...v3.0.1",v3.0.1,Yannick Wurm,,yannickwurm,GNU Affero General Public License v3.0,sequenceserver,wurmlab,39,blast,bioinformatics,sequencing,genomics-visualization,genomics,visualization,sequence-alignment,hacktoberfest,ruby,javascript,,,,,,,,,,,/wurmlab/sequenceserver,60,19,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/veldrid/veldrid,https://github.com/veldrid/veldrid,0,,,0,0,0,0,0,0,1,1,0,0,0,"A low-level, portable graphics library for .NET.","# Veldrid\n\nVeldrid is a cross-platform, graphics API-agnostic rendering and compute library for .NET. It provides a powerful, unified interface to a system's GPU and includes more advanced features than any other .NET library. Unlike other platform- or vendor-specific technologies, Veldrid can be used to create high-performance 3D applications that are truly portable.\n\n___As of February 2023, I'm no longer able to publicly share updates to Veldrid and related libraries. If you're an active user or have contributed improvements in the past, feel free to reach out or join the [Discord server](https://discord.gg/s5EvvWJ) for more information about the status of Veldrid.___\n\nSupported backends:\n\n* Direct3D 11\n* Vulkan\n* Metal\n* OpenGL 3\n* OpenGL ES 3\n\n[Veldrid documentation site](https://mellinoe.github.io/veldrid-docs/)\n\nJoin the Discord server:\n\n[![Join the Discord server](https://img.shields.io/discord/757148685321895936?label=Veldrid)](https://discord.gg/s5EvvWJ)\n\nVeldrid is available on NuGet:\n\n[![NuGet](https://img.shields.io/nuget/v/Veldrid.svg)](https://www.nuget.org/packages/Veldrid)\n\nPre-release versions of Veldrid are also available from MyGet: https://www.myget.org/feed/mellinoe/package/nuget/Veldrid\n\n![Sponza](https://i.imgur.com/p6juqm9.jpg)\n\n### Build instructions\n\nVeldrid  uses the standard .NET Core tooling. [Install the tools](https://www.microsoft.com/net/download/core) and build normally (`dotnet build`).\n\nRun the NeoDemo program to see a quick demonstration of the rendering capabilities of the library.\n",2443,graphics,C#,7,C#,GLSL,HLSL,Batchfile,Metal,Shell,PowerShell,,,,,,,,,,,,,,,,,,,,,,231,47,132,52,96,54,7,231499,266,307,201,106,37ab5eebdb25cb0db0dcea59caca560bea8eb8b9,Merge pull request #533 from opticfluorine/vk-barrier-validation,2024-03-30T05:25:04Z,Dan Balasescu,smoogipoo@smgi.me,smoogipoo,v4.9.0,,v4.9.0,,,github-actions[bot],MIT License,veldrid,veldrid,7,graphics,opengl,direct3d,vulkan,game-development,metal,,,,,,,,,,,,,,,/veldrid/veldrid,36,100,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,61,15,32,14,2,26,0,746,335,510,177,333,f386e700134dc0daa7a06b3d82e924f99cfc7d6d,fix a typo,2024-07-03T12:09:12Z,Shifu Chen,shifu@ShifudeMacBook-Pro.local,,Fix a regression bug of FASTQ reader,https://github.com/OpenGene/fastp/issues/491,v0.23.4,Shifu Chen,,sfchen,MIT License,fastp,OpenGene,41,fastq,qc,preprocessing,filtering,adapter,overlap,quality,trimming,splitting,quality-control,filter,ngs,bioinformatics,umi,sequencing,illumina,polyg,duplication,merging,,/OpenGene/fastp,41,50,TRUE
/veg/hyphy,https://github.com/veg/hyphy,1,,,1,1,1,1,0,0,0,0,0,0,1,HyPhy: Hypothesis testing using Phylogenies,"[![Build Status](https://travis-ci.org/veg/hyphy.svg?branch=master)](https://travis-ci.org/veg/hyphy)\n\n# HyPhy - Hypothesis testing using Phylogenies\n\nHyPhy is an open-source software package for the analysis of genetic sequences using techniques in phylogenetics, molecular evolution, and machine learning. It features a complete graphical user interface (GUI) and a rich scripting language for limitless customization of analyses. Additionally, HyPhy features support for parallel computing environments (via message passing interface (MPI)) and it can be compiled as a shared library and called from other programming environments such as Python and R. HyPhy is the computational backbone powering datamonkey.org. Additional information is available at hyphy.org.\n\n## Quick Start\n\n#### Install  \n`conda install hyphy`\n\n#### Run with Command Line Arguments\n`hyphy <method_name> --alignment <path_to_alignment_file> <additional_method_specific_arguments>`  \n+ _`<method_name>` is the name of the analysis you wish to run (can be: absrel, bgm, busted, fade, fel, fubar, gard, meme, relax or slac)_\n+ _`<path_to_alignment_file>` is the relative or absolute path to a fasta, nexus or phylib file containing an alignment and tree_\n+ _A list of the available `<additional_method_specific_arguments>` can be seen by running `hyphy <method_name> --help`_\n\nor  \n\n#### Run in Interactive Mode\n`hyphy -i`  \n\n## Building from Source\n\n#### Requirements\n* cmake >= 3.12\n* gcc >= 4.9\n* libcurl\n* libpthread\n* openmp (can be installed on mac via `brew install libomp`)\n\n#### Download\nYou can download a specific release [here](https://github.com/veg/hyphy/releases) or clone this repo with\n\n`git clone https://github.com/veg/hyphy.git`\n\nChange your directory to the downloaded/cloned directory\n\n`cd hyphy`\n\n#### Build\n`cmake .`\n\n`make install`\n\n## Additional Options for Building from Source\n\n#### Build Systems\nIf you prefer to use other build systems, such as Xcode, configure using the -G switch\n\n`cmake -G Xcode .`\n\nCMake supports a number of build system generators, feel free to peruse these and use them if you wish.\n\nIf you are on an OS X platform, you can specify which OS X SDK to use\n\n`cmake -DCMAKE_OSX_SYSROOT=/Developer/SDKs/MacOSX10.9.sdk/ .`\n\nIf building on a heterogeneous cluster with some nodes that do not support auto-vectorization  \n\n`cmake -DNOAVX=ON .`.\n\nIf you're on a UNIX-compatible system, and you're comfortable with GNU make, then run `make` with one of the following build targets:\n\n+   MP or hyphy - build a HyPhy executable (This used to be ""HYPHYMP"" but is now just ""hyphy"") using pthreads to do multiprocessing\n+   MPI - build a HyPhy executable (HYPHYMPI) using MPI to do multiprocessing\n+   HYPHYMPI - build a HyPhy executable (HYPHYMPI) using openMPI \n+   LIB - build a HyPhy library (libhyphy_mp) using pthreads to do multiprocessing\n-   GTEST - build HyPhy's gtest testing executable (HYPHYGTEST)\n\n#### Example (MPI build of hyphy using openMPI)\nEnsure that you have openmpi installed and available on your  path. You can check if this is the case after running `cmake .` you should see something similar to this in your output\n\n`-- Found MPI_C: /opt/scyld/openmpi/1.6.3/gnu/lib/libmpi.so;/usr/lib64/libibverbs.so;/usr/lib64/libdat.so;/usr/lib64/librt.so;/usr/lib64/libnsl.so;/usr/lib64/libutil.so;/usr/lib64/libm.so;/usr/lib64/libtorque.so;/usr/lib64/libm.so;/usr/lib64/libnuma.so;/usr/lib64/librt.so;/usr/lib64/libnsl.so;/usr/lib64/libutil.so;/usr/lib64/libm.so `\n\n`-- Found MPI_CXX: /opt/scyld/openmpi/1.6.3/gnu/lib/libmpi_cxx.so;/opt/scyld/openmpi/1.6.3/gnu/lib/libmpi.so;/usr/lib64/libibverbs.so;/usr/lib64/libdat.so;/usr/lib64/librt.so;/usr/lib64/libnsl.so;/usr/lib64/libutil.so;/usr/lib64/libm.so;/usr/lib64/libtorque.so;/usr/lib64/libm.so;/usr/lib64/libnuma.so;/usr/lib64/librt.so;/usr/lib64/libnsl.so;/usr/lib64/libutil.so;/usr/lib64/libm.so `\n\nThen run \n\n`make HYPHYMPI`\n\nAnd then run make install to install the software\n\n`make install`\n\n+   hyphy will be installed at  `/location/of/choice/bin`\n+   libhyphy_mp.(so/dylib/dll) will be installed at `/location/of/choice/lib`\n+   HyPhy's standard library of batchfiles will go into `/location/of/choice/lib/hyphy`\n\n\n#### Testing\n\nUse `make test` after running `cmake .`.\n\n#### Benchmarks for CMake Tests\n\nBenchmarks, using Github Actions, can be found at http://hyphy.org/bench\n\n#### Executable Location\n\nBy default, HyPhy installs into `/usr/local` but it can be installed on any location of your system by providing an installation prefix\n\n`cmake -DCMAKE_INSTALL_PREFIX:PATH=/location/of/choice`\n\nFor example, this configuration will install hyphy at /opt/hyphy\n\n`mkdir -p /opt/hyphy`\n\n`cmake -DCMAKE_INSTALL_PREFIX:PATH=/opt/hyphy .`\n\n#### Building Documentation\n\n```\nmake docs\ncd docs\npython3 -m http.server\n```\n\n## CLI notes\n\nAs noted in the documentation [here](http://www.hyphy.org/tutorials/CLI-tutorial/) \n`hyphy` can be run as a command line tool.\nIndeed for many analyses the `hyphy` CLI will return useful \nhelp messages, showing which parameter values can be set to \nspecify your analysis. For example, running `hyphy gard --help`\n\n```{bash}\nhyphy gard --help \n\nAvailable analysis command line options\n---------------------------------------\nUse --option VALUE syntax to invoke\nIf a [reqired] option is not provided on the command line, the analysis will prompt for its value\n[conditionally required] options may or not be required based on the values of other options\n\ntype\n	The type of data to perform screening on\n	default value: nucleotide\n\ncode\n	Genetic code to use (for codon alignments)\n	default value: Universal\n	applies to: Choose Genetic Code\n\nalignment [required]\n	Sequence alignment to screen for recombination\n\nmodel\n	The substitution model to use\n	default value: JTT\n\nrv\n	Site to site rate variation\n	default value: None\n\nmax-breakpoints\n	Maximum number of breakpoints to consider\n	default value: 10000\n\nrate-classes\n	How many site rate classes to use\n	default value: 4\n\noutput\n	Write the resulting JSON to this file (default is to save to the same path as the alignment file + 'GARD.json')\n	default value: gard.defaultJsonFilePath [computed at run time]\n\nmode\n	Run mode (Normal or Faster)\n	default value: Normal\n\noutput-lf\n	Write the best fitting HyPhy analysis snapshot to (default is to save to the same path as the alignment file + 'best-gard')\n	default value: gard.defaultFitFilePath [computed at run time]\n```\n\nwill show you the options that can be set for the `gard` analysis.\nSo for instance one could specify a `gard` run on the command line\nwith the following command\n\n```{bash}\nhyphy gard --alignment /path/to/file --rv GDD --mode Faster --rate-classes 3\n```\n\nWhile this is a useful feature, it is not always the case that \nolder analyses will have the same level of support for command line.\nFor instance, the `acd` analysis does not have CLI support and \nso if one runs the help command\n\n```{bash}\nhyphy acd --help \n\nAvailable analysis command line options\n---------------------------------------\nUse --option VALUE syntax to invoke\nIf a [reqired] option is not provided on the command line, the analysis will prompt for its value\n[conditionally required] options may or not be required based on the values of other options\n\nNo annotated keyword arguments are available for this analysis\n```\n\none will see that there are no options available. In this case, \nyou can use a different CLI specification. Indeed the CLI will accept\nall of the options that are asked for in an interactive session, as \npositional arguments. In this case I could run the `acd` analysis with\n\n```{bash}\nhyphy acd Universal <alignment.fa> MG94CUSTOMCF3X4 Global 012345 <treefile> Estimate\n\n```\n\nwhere the options are specified in the exact order that they are asked for in the interactive session. This will work for all `hyphy` analyses\nand provides a less readable but more flexible way to run `hyphy` analyses.\n",201,bioinformatics,HyPhy,8,CMake,C,C++,Shell,Python,HyPhy,JavaScript,SWIG,,,,,,,,,,,,,,,,,,,,,399,41,358,0,7,24,167,60222,68,1317,1308,9,1460f3adb0bb41a5b9027e248edfc6fb8e11dbdf,Merge pull request #1715 from veg/develop,2024-05-31T13:16:39Z,Sergei Pond,spond@temple.edu,spond,02.05.1962,Bug fixes and improvements.,02.05.1962,Sergei Pond,,spond,Other,hyphy,veg,83,science,phylogenetics,c-plus-plus,bioinformatics,comparative-genomics,evolution,statistical-methods,,,,,,,,,,,,,,/veg/hyphy,96,22,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/usgs/pestpp,https://github.com/usgs/pestpp,0,,,0,0,0,0,0,0,1,0,0,0,0,"tools for scalable and non-intrusive parameter estimation, uncertainty analysis and sensitivity analysis","<p align=""center"">\n  <img src=""documentation/pestpplogo.png"" alt=""pestpplogo image"">\n</p>\n\n\n\n# PEST++\n\n## a Software Suite for Parameter Estimation, Uncertainty Analysis, Management Optimization and Sensitivity Analysis\n\nPEST++ is a software suite aimed at supporting complex numerical models in the decision-support context.  Much focus has been devoted to supporting environmental models (groundwater, surface water, etc) but these tools are readily applicable to any computer model.\n\n<br>\n\nMaster branch:  [![master branch](https://github.com/usgs/pestpp/actions/workflows/ci.yml/badge.svg?branch=master)](https://github.com/usgs/pestpp/actions/workflows/ci.yml/badge.svg?branch=master)\n\nDevelop branch:  [![develop](https://github.com/usgs/pestpp/actions/workflows/ci.yml/badge.svg?branch=develop)](https://github.com/usgs/pestpp/actions/workflows/ci.yml/badge.svg?branch=develop)\n\n<br>\n\n\n## Documentation\n\nThe latest official report documenting PEST++ is available from the USGS:\n\n[https://pubs.er.usgs.gov/publication/tm7C26](https://pubs.er.usgs.gov/publication/tm7C26)\n\n##### Suggested Citation:\n\nWhite, J.T., Hunt, R.J., Fienen, M.N., and Doherty, J.E., 2020, Approaches to Highly Parameterized Inversion: PEST++ Version 5, a Software Suite for Parameter Estimation, Uncertainty Analysis, Management Optimization and Sensitivity Analysis: U.S. Geological Survey Techniques and Methods 7C26, 52 p., [https://doi.org/10.3133/tm7C26](https://doi.org/10.3133/tm7C26).\n\n<br>\n\n## User's Manual\n\nThe lastest PEST++ users manual is available [here](documentation/pestpp_users_manual.md) or as a [word document](https://github.com/usgs/pestpp/tree/develop/documentation).\n\n<br>\n\n## Links to latest binaries\n\nAs of version 4.3.11, PEST++ pre-compiled binaries for windows and linux are available as a github release:  \n\n[https://github.com/usgs/pestpp/releases](https://github.com/usgs/pestpp/releases)\n\n<br>\n\n## Compiling\nThe develop branch includes a Visual Studio solution, as well as CMake files for building on all operating systems using g++, MSVC, and/or intel C++.\n\nSee details [here](documentation/cmake.md) to compile using CMake.\n\n<br>\n\n## Overview\nThe PEST++ software suite includes several stand-alone tools for model-independent (non-intrusive) computer model parameter estimation and uncertainty analysis.  Codes include:\n\n* ``pestpp-glm``: deterministic GLM parameter estimation using ""on-the-fly"" subspace reparameterization, effectively reproducing the SVD-Assist methodology of PEST without any user intervention and FOSM-based parameter and (optional) forecast uncertainty estimation with support for generating Bayes-linear posterior parameter realizations.\n\n* ``pestpp-sen``: Global sensitivity analysis using either Morris or Sobol\n\n* ``pestpp-swp``: a generic parallel run utility driven by a CSV file of parameter values\n\n* ``pestpp-opt``: chance-constrained linear programming\n\n* ``pestpp-ies``: iterative ensemble smoother implementation of GLM (based on the work Chen and Oliver 2013) with support for generic localization (local analysis and/or covariance localization)\n\n* ``pestpp-mou``: multi-objective optimization under uncertainty using evolutionary algorithms (single objective also!)\n* ``pestpp-da``: model-independent ensemble-based sequential and batch iterative data assimilation with options to use standard Kalman update, multiple data assimilation (MDA), or the GLM algorithm of Chen and Oliver (2013).\n\nAll members of the software suite can be compiled for PC, MAC, or Linux and have several run managers to support parallelization.  Windows users with older OS versions should use the ``iwin`` binaries (starting ""i"", compiled with intel C++) to avoid the dreaded MSVC missing runtime DLL issue.\n\n<br>\n\n## Funding\n\nFunding for PEST++ has been provided by the U.S. Geologial Survey. The New Zealand Strategic Science Investment Fund as part of GNS Science’s (https://www.gns.cri.nz/) Groundwater Research Programme has also funded contributions 2018-present.  Intera, Inc. also provides ongoing support for PEST++.\n\n<br>\n\n## PEST++ References:\n\nWhite, J.T., Hunt, R.J., Fienen, M.N., and Doherty, J.E., 2020, Approaches to Highly Parameterized Inversion: PEST++ Version 5, a Software Suite for Parameter Estimation, Uncertainty Analysis, Management Optimization and Sensitivity Analysis: U.S. Geological Survey Techniques and Methods 7C26, 52 p., https://doi.org/10.3133/tm7C26.\n\nWhite, J. T., 2018, A model-independent iterative ensemble smoother for efficient history-matching and uncertainty quantification in very high dimensions. Environmental Modelling & Software. 109. 10.1016/j.envsoft.2018.06.009. <a ref=""http://dx.doi.org/10.1016/j.envsoft.2018.06.009"">http://dx.doi.org/10.1016/j.envsoft.2018.06.009</a>.\n\nWhite, J. T., Fienen, M. N., Barlow, P. M., and Welter, D.E., 2017, A tool for efficient, model-independent management optimization under uncertainty. Environmental Modeling and Software.  <a ref=""http://dx.doi.org/10.1016/j.envsoft.2017.11.019"">http://dx.doi.org/10.1016/j.envsoft.2017.11.019</a>.\n\nWelter, D.E., White, J.T., Hunt, R.J., and Doherty, J.E., 2015, Approaches in highly parameterized inversion— PEST++ Version 3, a Parameter ESTimation and uncertainty analysis software suite optimized for large environmental models: U.S. Geological Survey Techniques and Methods, book 7, chap. C12, 54 p., <a ref=""http://dx.doi.org/10.3133/tm7C12"">http://dx.doi.org/10.3133/tm7C12</a>.\n\nWelter, D.E., Doherty, J.E., Hunt, R.J., Muffels, C.T., Tonkin, M.J., and Schreüder, W.A., 2012, Approaches in highly parameterized inversion—PEST++, a Parameter ESTimation code optimized for large environmental models: U.S. Geological Survey Techniques and Methods, book 7, section C5, 47 p., available only at <a ref=""http://pubs.usgs.gov/tm/tm7c5"">http://pubs.usgs.gov/tm/tm7c5</a>.\n\n<br>\n\n### Related Links:\n\n* <a ref=""https://www.usgs.gov/software/pest-parameter-estimation-code-optimized-large-environmental-models"">https://www.usgs.gov/software/pest-parameter-estimation-code-optimized-large-environmental-models </a>\n* <a ref=""http://www.pesthomepage.org"">http://www.pesthomepage.org </a>\n* <a ref=""https://github.com/pypest/pyemu"">https://github.com/pypest/pyemu </a>\n\n<br>\n\n## Testing\n\nThe ``benchmarks`` folder contains a simple worked example and basic testing routines that are used for basic CI testing.  Many full-worked test problems of varying sizes are now located in separate repos:\n\n* [pestpp-glm benchmarks](https://github.com/usgs/pestpp-glm_benchmarks)\n* [pestpp-ies benchmarks](https://github.com/pestpp/pestpp-ies_benchmarks)\n* [pestpp-opt benchmarks](https://github.com/pestpp/pestpp-opt_benchmarks)\n* [pestpp-mou benchmarks](https://github.com/pestpp/pestpp-mou_benchmarks)\n* [pestpp-da benchmarks](https://github.com/pestpp/pestpp-da_benchmarks)\n* \n\n<br>\n\n## Dependencies\n\nMuch work has been done to avoid additional external dependencies in PEST++.  As currently designed, the project is fully self-contained.  \n\n<br>\n\n# optional ``++`` arguments\n\nplease see the PEST++ users manual in the ``documentation`` directory for a current and complete description of all ``++`` options\n\n<br>\n\n###### USGS disclaimer\n\nThis software has been approved for release by the U.S. Geological Survey (USGS). Although the software has been subjected to rigorous review, the USGS reserves the right to update the software as needed pursuant to further analysis and review. No warranty, expressed or implied, is made by the USGS or the U.S. Government as to the functionality of the software and related material nor shall the fact of release constitute any such warranty. Furthermore, the software is released on condition that neither the USGS nor the U.S. Government shall be held liable for any damages resulting from its authorized or unauthorized use\n",125,uncertainty-quantification,C++,17,Makefile,CMake,C++,C,Shell,Fortran,JavaScript,CSS,Python,TeX,Smarty,Batchfile,Jupyter Notebook,HTML,Cuda,Visual Basic 6.0,FreeBasic,,,,,,,,,,,,116,8,108,0,2,219,0,1162807,68,133,111,22,93723e5ed0e7697dcfb3ba7f682dd287d28e187a,Merge remote-tracking branch 'origin/develop',2024-06-09T17:17:40Z,jwhite,jtwhite1000@gmail.com,jtwhite79,05.02.2007,Bug fix if using freeze-on-fail across multiple calls to the run manager.  A few other smaller bug fixes,05.02.2007,J Dub,,jtwhite79,,pestpp,usgs,53,uncertainty-quantification,parameter-estimation,ensembles,optimization-tools,optimization,sensitivity-analysis,global-sensitivity-analysis,parallel-computing,non-intrusive,ensemble-methods,,,,,,,,,,,/usgs/pestpp,65,22,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/unitaryfund/qrack,https://github.com/unitaryfund/qrack,1,,,1,1,1,1,0,0,0,0,0,0,1,"Comprehensive, GPU accelerated framework for developing universal virtual quantum processors","# Qrack\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5812507.svg)](https://doi.org/10.5281/zenodo.5812507) [![Mentioned in Awesome awesome-quantum-computing](https://awesome.re/mentioned-badge.svg)](https://github.com/desireevl/awesome-quantum-computing) [![Unitary Fund](https://img.shields.io/badge/Supported%20By-UNITARY%20FUND-brightgreen.svg?style=for-the-badge)](http://unitary.fund)\n\n## About\n\nThe open source vm6502q/qrack library and its associated plugins and projects under the vm6502q organization header comprise a framework for full-stack quantum computing development, via high performance and fundamentally optimized simulation. The intent of ""Qrack"" is to provide maximum performance for the simulation of an ideal, virtually error-free quantum computer, across the broadest possible set of hardware and operating systems.\n\nUsing the C++11 standard, at base, Qrack has an external-dependency-free CPU simulator ""engine,"" as well as a GPU simulator engine that depends only on OpenCL. The `QUnit` layer provides novel, fundamental optimizations in the simulation algorithm, based on ""[Schmidt decomposition](https://arxiv.org/abs/1710.05867),"" transformation of basis, 2 qubit controlled gate buffer caching, the physical nonobservability of arbitrary global phase factors on a state vector, and many other ""synergistic"" and incidental points of optimization between these approaches and in addition to them. `QUnit` can be placed ""on top"" of either CPU, GPU, or hybrid engine types, and an additional `QPager` layer can sit between these, or in place of `QUnit`. Optimizations and hardware support are highly configurable, particularly at build time.\n\nA `QUnit` or `QEngine` can be thought of as like simply a one-dimensional array of qubits, within which any qubit has the capacity to directly and fully entangle with any and all others. Bits can be manipulated on by a single bit gate at a time, or gates and higher level quantum instructions can be acted over arbitrary contiguous sets of bits. A qubit start index and a length is specified for parallel operation of gates over bits or for higher level instructions, like arithmetic on abitrary width registers. Some methods are designed for (bitwise and register-like) interface between quantum and classical bits. See the Doxygen for the purpose of gate-like and register-like functions.\n\nQrack has already been integrated with a [MOS 6502 emulator](https://github.com/vm6502q/vm6502q), which demonstrates Qrack's original purpose, for use in developing chip-like quantum computer emulators. (The base 6502 emulator to which Qrack was added for that project is by Marek Karcz, many thanks to Marek! See https://github.com/makarcz/vm6502.)\n\nA number of useful ""pseudo-quantum"" operations, which could not be carried out by true hardware quantum computers easily or at all, are included in the API for purposes like debugging, but also for potential speed-up, by allowing the classical emulation to leverage nonphysical exceptions to quantum logic, like by cloning a quantum state. In practice, though, quantum circuit programs might not rely on this (nonphysical) functionality at all. (The MOS 6502 emulator referenced above does not. It happens, many of these methods will not be exotic at all, to those familiar with other major quantum computing libraries. For example, notably, simply returning a full vector of probability amplitudes actually qualifies as ""pseudo-quantum,"" in this sense.)\n\nQrack compiles like a library. To include in your project:\n\n1. In your source code:\n\n```cpp\n#include ""qrack/qfactory.hpp""\n```\n\n2. On the command line, in the project directory\n\n```sh\n$ mkdir _build && cd _build && cmake .. && make all install\n```\n\nInstantiate a `Qrack::QUnit`, specifying the desired number of qubits. (Optionally, also specify the initial bit permutation state in the constructor.) `QUnit`s can be (Schmidt) ""composed"" and ""decomposed"" with and from each other, to join and separate the representations of qubit ""registers"" that are not entangled at the point (de)composition. Both single quantum gate commands and register-like multi-bit commands are available.\n\nFor distributed simulation, the `Qrack::QPager` layer will segment a single register into a power-of-two count of equal length pages, running on an arbitrary number of OpenCL accelerators. The `QPager` layer also scales to arbitrarily small as well as large qubit counts, such that it can be appropriate for use on a single accelerator for small width simulations. The `QPager` layer is also compatible with Clifford set preamble circuits simulated with `QStabilizerHybrid`, as a layer over `QPager`, and `QHybrid` for CPU/GPU switching can be used as the ""engine"" layer under it. For Qrack in a cluster environment, we support the SnuCL and VirtualCL OpenCL virtualization layers, with OpenCL v1.1 compliant host code without required ""host pointers.""\n\nFor more information, compile the `doxygen.config` in the root folder, and then check the `doc` folder.\n\n## Documentation\n\nLive version of the documentation, including API reference, can be obtained at: https://qrack.readthedocs.io/en/latest/\n\n## Community\n\nQrack has a community home at the Advanced Computing Topics server on Discord, at: https://discordapp.com/invite/Gj3CHDy\n\nFor help getting started with contributing, see our [CONTRIBUTING.md](CONTRIBUTING.md).\n\n## Installing Qrack\n\nIf you're on Ubuntu 18.04, 20.04, or 22.04 LTS, you're in luck: Qrack manages a PPA that provides binary installers for _all_ available CPU architectures (except any that require administrative attention from Ubuntu or Canonical).\n\n```sh\n    $ sudo add-apt-repository ppa:wrathfulspatula/vm6502q\n    $ sudo apt update\n    $ sudo apt install libqrack-dev\n```\n\n(You might need to install the `add-apt-repository` tool first, through `apt` itself.)\n\nOtherwise, standardized builds are available on the [releases](https://github.com/unitaryfund/qrack/releases) page. (Operating system targets include Linux, Windows, and Mac, alongside WebAssmembly. Qrack source also builds for native Android and iOS.)\n\nIf you're looking for [PyQrack](https://github.com/unitaryfund/pyqrack), know that the PyPi package has a self-contained Qrack release. (On Ubuntu, the PyPi package can be used, but it is **strongly recommended** that you instead install the `libqrack` or `libqrack-dev` packages from the PPA, as above, then install `main` branch PyQrack from source, which will use the Ubuntu `apt` packages.)\n\n## PyQrack Source Build\n\nThe CMake settings for default build of [PyQrack](https://github.com/unitaryfund/pyqrack) are as follows, (assuming you are in a build directory created inside the top-level directory of the repo clone):\n\nx86-64 Linux (OpenCL):\n```\ncmake -DENABLE_RDRAND=OFF -DENABLE_DEVRAND=ON -DQBCAPPOW=12 -DCPP_STD=14 -DUINTPOW=5 ..\n```\n\nx86-64 Linux (CUDA):\n```\ncmake -DENABLE_RDRAND=OFF -DENABLE_DEVRAND=ON -DQBCAPPOW=12 -DCPP_STD=14 -DENABLE_OPENCL=OFF -DENABLE_CUDA=ON -DUINTPOW=5 ..\n```\n\nx86-64 Mac (might need `-Werror`, ""warning to error,"" disabled in CMake files):\n```\ncmake -DQBCAPPOW=12 -DUINTPOW=5 -DCPP_STD=14 ..\n```\n\nRISC (ARM) Linux:\n```\ncmake -DENABLE_RDRAND=OFF -DENABLE_DEVRAND=ON -DENABLE_COMPLEX_X2=OFF -DQBCAPPOW=12 -DUINTPOW=5 -DCPP_STD=14 ..\n```\n\n[Emscripten (WASM)](https://qrack.net/):\n```\nemcmake cmake -DENABLE_RDRAND=OFF -DUINTPOW=5 -DENABLE_PTHREAD=OFF -DSEED_DEVRAND=OFF -DQBCAPPOW=12 -DUINTPOW=5 -DCPP_STD=14 ..\n```\n\nWindows-based systems are more specific, but there is a bit more information about them further below.\n\n## test/tests.cpp\n\nThe included `test/tests.cpp` contains unit tests and usage examples. The unittests themselves can be executed:\n\n```sh\n    $ _build/unittest\n```\n\nSimilarly, benchmarks are in `test/benchmarks.cpp`:\n\n```sh\n    $ _build/benchmarks [--optimal] [--max-qubits=30] [test_qft_cosmology]\n```\n\n## OpenCL on systems prior to OpenCL v2.0\n\nParticularly on older hardware, it is possible that you do not have OpenCL v2.0 available. In theory, Qrack should work off-the-shelf anyway. However, if the OpenCL implementation isn't even aware of the existence of v2.0, use the following option to completely manually force all v2.0 functionality off and to set the target OpenCL API level expressly to target v1.2 and minimum level v1.1:\n\n```sh\n    $ cmake -DENALBE_OOO_OCL=OFF ..\n```\n\n## Installing OpenCL on VMWare\n\nMost platforms offer a standardized way of installing OpenCL. However, a method for VMWare benefits from documentation, here.\n\n1.  Download the [AMD APP SDK](https://developer.amd.com/amd-accelerated-parallel-processing-app-sdk/)\n1.  Install it.\n1.  Add symlinks for `/opt/AMDAPPSDK-3.0/lib/x86_64/sdk/libOpenCL.so.1` to `/usr/lib`\n1.  Add symlinks for `/opt/AMDAPPSDK-3.0/lib/x86_64/sdk/libamdocl64.so` to `/usr/lib`\n1.  Make sure `clinfo` reports back that there is a valid backend to use (anything other than an error should be fine).\n1.  Install OpenGL headers: `$ sudo apt install mesa-common-dev`\n1.  Adjust the `makefile` to have the appropriate search paths\n\n## Installing OpenCL on Mac\n\nWhile the OpenCL framework is available by default on most modern Macs, the C++ header `cl.hpp` is usually not. One option for building for OpenCL on Mac is to download this header file and include it in the Qrack project folder under include/OpenCL (as `cl.hpp`). The OpenCL C++ header can be found at the Khronos OpenCL registry:\n\nhttps://www.khronos.org/registry/OpenCL/\n\nOtherwise, Homebrew offers a package with the headers: [opencl-clhpp-headers](https://formulae.brew.sh/formula/opencl-clhpp-headers) is the preferred method of installing headers, if `brew` is available.\n\n## Building and Installing Qrack on Windows\n\nQrack supports building on Windows, but some special configuration is required. Windows 10 usually comes with default OpenCL libraries for Intel (or AMD) CPUs and their graphics coprocessors, but NVIDIA graphics card support might require the CUDA Toolkit. The CUDA Toolkit also provides an OpenCL development environment, which is generally necessary to build Qrack.\n\nQrack requires the `xxd` command to convert its OpenCL kernel code into hexadecimal format for building. `xxd` is not natively available on Windows systems, but Windows executables for it are provided by sources including the [Vim editor Windows port](https://www.vim.org/download.php).\n\nCMake on Windows will set up a 32-bit Visual Studio project by default, (if using Visual Studio,) whereas 64-bit will probably be typically desired. `-DFPPOW=6` is used to set the systemic floating point accuracy to `double`, which is typically necessary for Q# accuracy tolerances. Putting together all of the above considerations, after installing the CUDA Toolkit and Vim, a typical CMake command for Windows might look like this:\n\n```sh\n    $ mkdir _build\n    $ cd _build\n    $ cmake -DCMAKE_GENERATOR_PLATFORM=x64 -DXXD_BIN=""C:/Program Files (x86)/Vim/vim82/xxd.exe"" -DFPPOW=6 ..\n```\n\nAfter CMake, the project must be built in Visual Studio. Once installed, the `qrack_pinvoke` DLL is compatible with the Qrack Q# runtime fork, to provide `QrackSimulator`.\n\n## C++ language standard\n\nTo change the C++ language standard language with which Qrack is applied, use `-DCPP_STD=n`, where ""`n`"" is the two-digit standard year:\n\n```sh\ncmake -DCPP_SIM=14 ..\n```\n\nBy default, Qrack builds for C++11. For minimum support for all optional dependencies, C++14 or later might be required.\n\n## Optional CUDA instead of OpenCL\n\nTheoretically, building with CUDA for your native supported architectures is as simple as installing the CUDA toolkit and compiler and using this CMake command:\n\n```sh\ncmake -DENABLE_CUDA=ON [-DENABLE_OPENCL=OFF] [-DQRACK_CUDA_ARCHITECTURES=86] ..\n```\n\nwhere `-DENABLE_CUDA=ON` is required to enable CUDA, `-DENABLE_OPENCL=OFF` will cause CUDA to be used in the default optimal simulation layer stack instead of OpenCL, and `-DQRACK_CUDA_ARCHITECTURES` optionally specifies an explicit list of CUDA architectures for which to build. (If `-DQRACK_CUDA_ARCHITECTURES` is not set, Qrack will attempt to detect your native installed GPU architectures and build for exactly that set.)\n\n## WebAssembly (WASM) builds\n\nBy nature of its pure C++11 design, Qrack happens to offer excellent compatibility with Emscripten (""WebAssembly"") projects. See [the qrack.net repository](https://github.com/vm6502q/qrack.net) for an example and [qrack.net](https://qrack.net) for a live demo. OpenCL GPU operation is not yet available for WASM builds. While CPU multithreading might be possible in WASM, it is advisable that `pthread` usage and linking is disabled for most conventional Web applications, with `-DENABLE_PTHREAD=OFF` in CMake:\n\n```sh\nemcmake cmake -DENABLE_RDRAND=OFF -DENABLE_PTHREAD=OFF -DSEED_DEVRAND=OFF -DUINTPOW=5 ..\n```\n\n`-DUINTPOW=5` is optional, but WASM RAM limitations currently preclude >=32 qubits of potentially entangled state vector, so 64 bit ket addressing is not necessary. However, `-DQBCAPPOW=10` could be added to the above to support high-width stabilizer and Schmidt decomposition cases, with the appropriate build of the Boost headers for the toolchain.\n\n## Compiling to C from WASM\n\nArbitrary Qrack executables will not all necessarily link, at this time. However, `wasm2c` can generate C code from Qrack executables and modules, and *some* will successfully link.\n\nFor example, generate a `.c` module and a `.h` header from your build/examples directory:\n\n```sh\nwasm2c teleport.wasm -o teleport.c\n```\n\nCompile and link it from `wabt`, (though the command below assumes that `wabt` is already in your path).\n\n```sh\ncc -o teleport -Iwabt/wasm2c/ teleport.c wabt/wasm2c/wasm-rt-impl.c -lm\n```\n\nWe apologize for providing an example that will not quite work. However, with `emcc` or `cc`, this is the general idea. Once the generated `.c` and `.h` file are syntactically valid, via modification of your original Qrack C++ program, then static linkage must be specified with `-l`, assuming appropriate libraries for static linkage are actually available. (Emscripten is under active development, and we thank its maintainers.)\n\n## Performing code coverage\n\n```sh\n    $ cd _build\n    $ cmake -DENABLE_CODECOVERAGE=ON ..\n    $ make -j 8 unittest\n    $ ./unittest\n    $ make coverage\n    $ cd coverage_results\n    $ python -m http.server\n```\n\n## Changing default OpenCL device\nOpenCL device(s) can be specified by index in `Qrack::QInterface` subclass constructors. The global default device can also be overridden with the environment variable `QRACK_OCL_DEFAULT_DEVICE=n`, where `n` is the index of the OpenCL device you want to use, as reported by the OpenCL initialization header.\n\n## Enable OpenCL device redistribution\nSetting the environment variable `QRACK_ENABLE_QUNITMULTI_REDISTRIBUTE` to any value except a null string enables reactive load redistribution or balancing, in `QUnitMulti`. Otherwise, `QUnitMulti` only tries to balance load as opportunity arises when new separable `QEngineShard` instances are created.\n\n## Tune OpenCL preferred concurrency\nPreferred concurrency has a tunable offset with default value of `3`, with the environment variable setting `export QRACK_GPU_OFFSET_QB=[m]` for some (positive or negative) integer `m`. For each integer increment of `m`, the preferred concurrency is multiplied by 2. (Preferred concurrency is calculated as `pow2(ceil(log2(([GPU processing element count] * [preferred group size for the single qubit gate kernel, usually warp size])))) << QRACK_GPU_OFFSET_QB`.)\n\n## QPager distributed simulation options\n`QPager` attempts to smartly allocate low qubit widths for maximum performance. For heterogeneous GPU simulation, based on `clinfo`, you can set a ceiling on your maximum OpenCL accelerator state vector page allocation with the environment variable `QRACK_MAX_PAGE_QB=n`, where n is an integer >=0. The default `n` is max integer, meaning that maximum allocation segment of your GPU RAM is always a single hardware page.\n\nTo set a maximum on how many qubits can be allocated on a single `QPager` instance, use the environment variable `QRACK_MAX_PAGING_QB`, for example, `export QRACK_MAX_PAGING_QB=30` to cause `QPager` to throw an exception that can be caught if it is asked to allocate 31 or more qubits. \n\nTo set the `QPager` device ID list, use the `QRACK_QPAGER_DEVICES` environment variable. This variable should contain an ordered list of Qrack OpenCL device IDs that should be automatically used in all `QPager` instances. Note that device IDs may be included multiple times in this list in order to achieve a simple form of load balancing. For example, since NVIDIA GPUs typically have 4 maximum allocation segments, a device list like `1,1,1,1,0,0,0,0` will allocate the first 4 maximum allocation segments on device `1` first, such that device `1` will be (roughly) 100% utilized before including any segments on device `0`. This list can also be written `4.1,4.0`, which means that 4 segments of `1` should be repeated before 4 segments of `0` are repeated. To repeat a pattern of multiple IDs, follow the multiplier with multiple `.` characters separating every ID in the pattern, like `4.1.0,4.2` for 4 repetitions of `1,0` followed by 4 repetitions of `2`. If device IDs are exhausted in the device list, `QPager` will automatically cycle the list as many times as it needs, to attempt higher segment count allocation.\n\nThere are two special device IDs that can be specified in these lists: `-1` is global Qrack default device. `-2` indicates that `QInterface`-local constructor-specified device ID should be used. (For example, a device list argument of just `-2` will indicate that distribution choices should defer to those of `QUnitMulti`, if in use.)\n\n`QRACK_QPAGER_DEVICES_HOST_POINTER` corresponds to each device ID in `QRACK_QPAGER_DEVICES`, per sequential item in that other variable, (with the same syntax and list wrapping behavior). If the value of this is `0` for a page, that page attempts OpenCL _device_ RAM allocation; if the value is `1` for a page, that page attempts OpenCL _host_ RAM allocation. `0` value, device RAM, is suggested for GPUs; `1` value, host RAM, is suggested for CPUs and APUs (which use general host RAM, anyway). By default, all devices attempt on-device RAM allocation, if this environment variable is not specified.\n\n## QUnitMulti device list\nSpecify a device list for `QUnitMulti` the same way you would for `QPager`, with environment variable `QRACK_QUNITMULTI_DEVICES`. Corresponding to each entry in `QRACK_QUNITMULTI_DEVICES`, use `QRACK_QUNITMULTI_DEVICES_MAX_QB` to (optionally) specify a per-entry ceiling on device usage. For smaller-width devices like CPUs, it might make sense to set the qubit ceiling to about the CPU `PSTRIDEPOW` plus logarithm base 2 of your hyperthread count.\n\n## QTensorNetwork options\n`QTensorNetwork` has a threshold up to which it is able to reuse more work in measurement and probability calculations, `QRACK_QTENSORNETWORK_THRESHOLD_QB`. Its default value is 30 qubits. Above (and not including) this threshold, `QTensorNetwork` will use techniques like restricting to ""past light cones"" for measurement and probablity calculation, in an attempt to reduce overall memory footprint at the cost of additional execution time.\n\n## QBdt and QBdtHybrid options\n`QBdtHybrid` sets a threshold for ""hybridization"" between ""quantum binary decision diagrams"" (see Acknowledgements at bottom of document) and state vector simulation, based on how efficiently the ""diagram"" or ""tree"" can be ""compressed."" The environment variable `QRACK_QBDT_HYBRID_THRESHOLD` (typically taking values between 0 and 1) sets a multiplicative fraction for maximally-compressed size of the tree, as fraction of node count vs. equivalent state vector amplitude count, before switching over to state vector simulation. Note that maximum `QBdt` node count is _twice_ the count of amplitudes in the equivalent state vector simulation, so set the variable to 2 or higher to completely suppress switching and recover `QBdt`-only simulation in all cases.\n\n## Build and environment options for CPU engines\n`QEngineCPU` and `QHybrid` batch work items in groups of 2^`PSTRIDEPOW` before dispatching them to single CPU threads, potentially greatly reducing waiting on mutexes without signficantly hurting utilization and scheduling. The default for this option can be controlled at build time, by passing `-DPSTRIDEPOW=n` to CMake, with ""n"" being an integer greater than or equal to 0. This can be overridden at run time by the enviroment variable `QRACK_PSTRIDEPOW=n`. If an environment variable is not defined for this option, the default from CMake build will be used. (The default is meant to work well across different typical consumer systems, but it might benefit from system-tailored tuning via the environment variable.)\n\n`-DENABLE_QUNIT_CPU_PARALLEL=OFF` disables asynchronous dispatch of `QStabilizerHybrid` and low width `QEngineCPU`/`QHybrid` gates with `std::future`. This option is on by default. Typically, `QUnit` stays safely under maximum thread count limits, but situations arise where async CPU simulation causes `QUnit` to dispatch too many CPU threads for the operating system. This build option can also reduce overall thread usage when Qrack user code operates in a multi-threaded or multi-shell environment. (Linux thread count limits might be smaller than Windows.)\n\n## Maximum allocation guard\nSet the maximum allowed allocation (in MB) for the global OpenCL pool with `QRACK_MAX_ALLOC_MB`. Per OpenCL device, this sets each maximum allocation limit with the same syntax as `QRACK_QPAGER_DEVICES`. (Succesive entries in the list are MB limits numbered according to Qrack's device IDs print-out on launch.) By default, each device is capped at 3/4 of its available global memory, for stability in common use cases. This includes (VRAM) state vectors and auxiliary buffers larger than approximately `sizeof(bitCapIntOcl) * sizeof(bitCapIntOcl)`. This should also include out-of-place single duplication of any state vector. This does **not** include non-OpenCL general heap or stack allocation.\n\n`QRACK_MAX_PAGING_QB` and `QRACK_MAX_CPU_QB` environment variables set a maximum on how many qubits can be allocated on a single `QPager` or `QEngineCPU` instance, respectively. This qubit limit is for maximum single `QPager` or `QEngineCPU` allocation, whereas `QUnit` and `QUnitMulti` might allocate _more_ qubits than this as separable subsystems, requiring that no individual separable subsystem exceeds the qubit limit environment variables. (`QEngineOCL` limits are automatically maximal according to a query Qrack makes of maximum allocation segment on a given OpenCL device.)\n\nNote that this controls total direct Qrack OpenCL buffer allocation, not total Qrack library allocation. Total OpenCL buffer allocation is **not** fully indicative of total library allocation.\n\n## Approximation and noise options\n`QUnit` can optionally round qubit subsystems proactively or on-demand to the nearest single or double qubit eigenstate with the `QRACK_QUNIT_SEPARABILITY_THRESHOLD=[0.0 - 1.0]` environment variable, with a value between `0.0` and `1.0`. When trying to find separable subsystems, Qrack will start by making 3-axis (independent or conditional) probability measurements. Based on the probability measurements, under the assumption that the state _is_ separable, an inverse state preparation to |0> procedure is fixed. If inverse state preparation would bring any single qubit Bloch sphere projection within parameter range of the edge of the Bloch sphere, (with unit length, `1.0`,) then the subsystem will be rounded to that state, normalized, and then ""uncomputed"" with the corresponding (forward) state preparation, effectively ""hyperpolarizing"" one and two qubit separable substates by replacing entanglement with local qubit Bloch sphere extent. (If 3-axis probability is _not_ within rounding range, nothing is done directly to the substate.)\n\nSimilarly functionality to above is available for `QBdt` with `QRACK_QBDT_SEPARABILITY_THRESHOLD=[0.0 - 0.5]`. In the case of this parameter, any branch with less than the parameter value for probability is rounded to 0, and its partner branch is renormalized to unit length. This same value is also used for branch equality comparison.\n\nEnvironment variable `QRACK_NONCLIFFORD_ROUNDING_THRESHOLD` sets the non-Clifford phase gate magnitude, as a fraction of `T` gate phase angle, (from the closest Clifford state Bloch sphere orientation,) that will be rounded to 0 in terminal measurement and sampling operations. (For `0`/default value, all non-Clifford phase gates are exactly preserved.)\n\nFor single-qubit depolarizing noise channels, `QInterfaceNoisy` exposes environment variable `QRACK_GATE_DEPOLARIZATION`, which is a per-gate, single-qubit (applied to all qubits in gate) noise parameter for depolarizing noise, typically taking a floating-point value between `0` and `1`.\n\n## Vectorization optimization\n\n```sh\n$ cmake -DENABLE_COMPLEX_X2=ON ..\n```\nMultiply complex numbers two at a time instead of one at a time. Requires AVX for double and SSE 1.0 (with optional SSE 3.0) for float. On by default, but can be turned off for double accuracy without the AVX requirement, or to completely remove vectorization with single float accuracy.\n\nIf `-DENABLE_COMPLEX_X2=ON`, then SSE 3.0 is used by default. Turn off the following option to limit to SSE 1.0 level:\n\n```sh\n$ cmake -DENABLE_SSE3=OFF ..\n```\n## Random number generation options (on-chip by default)\n\n```sh\n$ cmake -DENABLE_RDRAND=OFF ..\n```\nTurn off the option to attempt using on-chip hardware random number generation, which is on by default. If the option is on, Qrack might still compile to attempt using hardware random number generation, but fall back to software generation if the RDRAND opcode is not actually available. Some systems' compilers, such as that of the Raspberry Pi 3, do not recognize the compilation flag for enabling RDRAND, in which case this option needs to be turned off.\n\n```sh\n$ cmake [-DENABLE_RDRAND=OFF] -DENABLE_DEVRAND=ON ..\n```\nInstead of RDRAND, use Linux `/dev/urandom/` as the Qrack random number source. (The necessary system call will only be available on Linux systems.)\n\n```sh\n$ cmake -DSEED_DEVRAND=OFF ..\n```\nIf pure software pseudo-random number generator is used, it will be seeded from `/dev/random` by default. `-DSEED_DEVRAND=OFF` will use the system clock for Mersenne twister seeding, instead of `/dev/random`.\n\n## Pure 32 bit OpenCL kernels (including OpenCL on Raspberry Pi 3)\n\n```sh\n$ cmake -DENABLE_PURE32=ON ..\n```\nThis option is needed for certain older or simpler hardware. This removes all use of 64 bit types from the OpenCL kernels, as well as completely removing the use of SIMD intrinsics. Note that this build option theoretically supports only up to 32 qubits, whereas `-DENABLE_PURE32=OFF` could support up to 64 qubits, (if the memory requirements were realistically attainable for either 32-bit or 64-bit hardware, or in limited cases available for `QUnit` Schmidt decomposition). `-DENABLE_PURE32=ON` is necessary to support the VC4CL OpenCL compiler for the VideoCore GPU of the Raspberry Pi 3. (Additionally, for that platform, the RDRAND instruction is not available, and you should `-DENABLE_RDRAND=OFF`. VC4CL for the VideoCore GPU is currently fully supported.)\n\n## Reduced or increased coherent qubit addressing\n\n```sh\n$ cmake [-DUINTPOW=n] [-DQBCAPPOW=n] ..\n```\nQrack uses an unsigned integer primitive for ubiquitous qubit masking operations, for ""local"" qubits (`QEngine`) and ""global"" qubits (`QUnit` and `QPager`). This limits the maximum qubit capacity of any coherent `QInterface` to the total number of bits in the global (or local) masking type. By default, a 64-bit unsigned integer is used, corresponding to a maximum of 64 qubits in any coherent `QInterface` (if attainable, such as in limited cases with `QUnit`). `-DUINTPOW=n` reduces the ""local"" masking type to 2^n bits, (ex.: for max OpenCL sub-unit or page qubit width,) which might also be important with accelerators that might not support 64-bit types. `-DQBCAPPOW=n` sets the maximum power of ""global"" qubits in ""paged"" or `QUnit` types as potentially larger than single ""pages"" or ""sub-units,"" for ""n"" >= 5, with n=5 being 2^5=32 qubits. Large ""n"" is possible with the Boost big integer header. (Setting ""n"" the same for both build options can avoid casting between ""subunit"" and ""global qubit"" masking types, if larger ""paging"" or `QUnit` widths than `QEngine` types are not needed.)\n\n## Variable floating point precision\n\n```sh\n$ cmake [-DFPPOW=n] ..\n```\nLike for unsigned integer masking types, this sets the floating point accuracy for state vectors to n^2. By default n=5, for 2^5=32 bit floating point precision. ""half,"" ""double,"" and ""quad,"" availability depend on the system, but n=6 for ""double"" is commonly supported on modern hardware. n=4 for half is supported by GCC on ARM, header-only on x86_64, and by device pragma if available for OpenCL kernels. ""quad"" is supported on CPU only, if available.\n\n## Precompiled OpenCL kernels\n\n```sh\n$ qrack_cl_compile [path]\n```\nPrecompile the OpenCL programs for all available devices, and save them to the optional ""path"" parameter location. By default, programs will be saved to a folder in the ""home"" directory, such as `~/.qrack/` on most Linux systems. (The default path can also be specified as an environment variable, `QRACK_OCL_PATH`.) Also by default, Qrack will attempt to load precompiled binaries from the same path, but the library will fall back to JIT compilation if program binaries are not available or are corrupt. To turn off default loading of binaries, one can simply delete the programs from this folder.\n\nThe option to load and save precompiled binaries, and where to load them from, can be controlled with the initializing method of `Qrack::OCLEngine`:\n\n```cpp\nQrack::OCLEngine::InitOCL(true, true, Qrack::OCLEngine::GetDefaultBinaryPath());\n```\nTo use this method directly, _it needs to be called before any OpenCL simulators are created in the program,_ as initialization happens automatically upon creating any OpenCL simulator instance. Calling the `OCLEngine::InitOCL()` method directly also ensures that the singleton instance has been created, with the results of the initialization call. The initialization method prototype is as follows:\n\n```cpp\n/// Initialize the OCL environment, with the option to save the generated binaries. Binaries will be saved/loaded from the folder path ""home"".\nstatic void InitOCL(bool buildFromSource = false, bool saveBinaries = false, std::string home = ""*"");\n```\nThe `home` argument default indicates that the default home directory path should be used.\n\n## VM6502Q\n\n```sh\n$ cmake -DENABLE_VM6502Q_DEBUG=ON ..\n```\nQrack was originally written so that the disassembler of VM6502Q should show the classical expecation value of registers, following Ehrenfest's theorem. However, this incurs significant additional overhead for `QInterface::IndexedLDA()`, `QInterface::IndexedADC()`, and `QInterface::IndexedSBC()`. As such, this behavior in the VM6502Q disassembler is only supported when this CMake flag is specifically enabled. (It is off by default.) These three methods will return 0, if the flag is disabled.\n\n## Turn on/off optional API components\n\n```sh\n$ cmake -DENABLE_BCD=OFF -DENABLE_REG_GATES=OFF -DENABLE_ROT_API=OFF -DENABLE_ALU=ON ..\n```\n\nPrior to the Qrack v7 API, a larger set of convenience methods were included in all builds, which increased the size of the library binary. By default, `ENABLE_REG_GATES`, `ENABLE_ROT_API` and  `ENABLE_BCD` all default to `OFF`, while `ENABLE_ALU` for arithmetic logic unit methods defaults to `ON`.\n\n`ENABLE_REG_GATES` adds various looped-over-width gates to the API, like the lengthwise `CNOT(control, target, length)` method. This method is a convenience wrapper on a loop of `CNOT` operations for `length`, starting from `control` and `target`, to `control + length - 1` and `target + length - 1`. These methods were seen as opportunities for optimization, at a much earlier point, but they have fallen out of internal use, and basically none of them are optimized as special cases, anymore. Disabling `ENABLE_REG_GATES` does **not** remove lengthwise `X(target, length)` and `H(target, length)` methods, as these specific conve",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/UniMath/UniMath,https://github.com/UniMath/UniMath,0.5,"Seems scientific enough, but does not require server cluster and parallelization",0,0,0,0,0,0,0,1,0,0,0,0,This coq library aims to formalize a substantial body of mathematics using the univalent point of view.,"[![DOI](https://zenodo.org/badge/17321421.svg)](https://zenodo.org/badge/latestdoi/17321421)\n\nUnivalent Mathematics\n=====================\n\nThis [Coq](https://coq.inria.fr/) library aims to formalize a substantial body of mathematics using the\n[univalent point of view](https://en.wikipedia.org/wiki/Univalent_foundations).\n\n\nTrying out UniMath\n------------------\n\nYou can try out UniMath in the browser by clicking [here](https://unimath.github.io/live/).\nFor instance, you can run the files from the [School on Univalent Mathematics](https://unimath.github.io/Schools/) in the browser.\n\n\nUsing UniMath on your computer\n------------------------------\n\nTo install UniMath on your computer, there are two options:\n\n- Install a released binary version of UniMath via the [Coq Platform](https://coq.inria.fr/download).\n- To develop, and contribute to, UniMath, you should compile the latest version of UniMath yourself - see [INSTALL.md](https://github.com/UniMath/UniMath/blob/master/INSTALL.md).\n\n\nUsage\n-----\n\nSee [USAGE.md](./USAGE.md)\n\nContents\n--------\n\nThe [UniMath subdirectory](UniMath/) contains various packages of formalized\nmathematics. For more information, see the [UniMath Table of Contents](UniMath/CONTENTS.md).\n\nSome scientific articles describing the contents of the UniMath library or work using it are listed in the \n[wiki](https://github.com/UniMath/UniMath/wiki/Articles-with-accompanying-formalization-in-UniMath).\n\nContributing to UniMath\n-----------------------\n\nTo contribute to UniMath, submit a pull request.  Your code will be subject to the \ncopyright and license agreement in [LICENSE.md](LICENSE.md).\n\nFor the style guide and other instructions, see [UniMath/README.md](UniMath/README.md).\n\nDiscussing UniMath & Getting Help\n---------------------------------\n\n- **Questions** about the UniMath library, compilation, and installation of UniMath, etc.,\ncan be asked in the [UniMath Zulip](https://unimath.zulipchat.com) (click [here](https://unimath.zulipchat.com/register/) to register)\nor on the [UniMath mailing list](mailto:univalent-mathematics@googlegroups.com) (archived in a [Google Group](https://groups.google.com/forum/#!forum/univalent-mathematics)).\n- **Bugs** should be reported in our [UniMath bug tracker on GitHub](https://github.com/UniMath/UniMath/issues).\n\n\nCiting UniMath\n--------------\n\nTo cite UniMath in your article, you can use the following bibtex item:\n```bibtex\n@Misc{UniMath,\n    author = {Voevodsky, Vladimir and Ahrens, Benedikt and Grayson, Daniel and others},\n    title = {UniMath --- a computer-checked library of univalent mathematics},\n    url = {https://github.com/UniMath/UniMath},\n    howpublished = {available at \url{http://unimath.org}},\n    doi          = {10.5281/zenodo.10849216},\n    url          = {https://doi.org/10.5281/zenodo.10849216}\n }\n```\nNote that this requires ```\usepackage{url}``` or ```\usepackage{hyperref}```.\n\n\nThe UniMath Coordinating Committee\n----------------------------\n\nThe UniMath project was started in 2014 by merging the repository\n[Foundations](https://github.com/UniMath/Foundations), by Vladimir Voevodsky\n(written in 2010), with two repositories based on it:\n[rezk_completion](https://github.com/benediktahrens/rezk_completion), by\nBenedikt Ahrens, and [Ktheory](https://github.com/DanGrayson/Ktheory), by\nDaniel Grayson.  Vladimir Voevodsky was a member of the team until his death in\nSeptember, 2017.\n\nThe current members of the UniMath Coordinating Committee are:\n\n- Benedikt Ahrens\n- Daniel Grayson\n- Michael Lindgren\n- Peter LeFanu Lumsdaine\n- Ralph Matthes\n- Niels van der Weide\n\nAcknowledgments\n---------------\n\nThe UniMath development team gratefully acknowledges the great work by\nthe Coq development team in providing the [Coq proof assistant](https://coq.inria.fr/), as well\nas their support in keeping UniMath compatible with Coq.\n",933,mathematics,Coq,9,Makefile,Emacs Lisp,Coq,Perl,HTML,JavaScript,TeX,Shell,sed,,,,,,,,,,,,,,,,,,,,1384,130,1236,18,3,75,393,29038,171,507,334,173,fbee84679be30bfdacb49f23b18ccea9f2766292,"8.20-alpha replaced by '8.20' as extra Coq version for CI, thanks to …",2024-07-19T13:57:12Z,Ralph Matthes,ralph.matthes@irit.fr,rmatthes,UniMath 2024-03-31,This is the UniMath library of univalent mathematics as of 2024-03-31.,v20240331,Ralph Matthes,,rmatthes,Other,UniMath,UniMath,9,coq,mathematics,unimath,coq-library,foundations,,,,,,,,,,,,,,,,/UniMath/UniMath,9,56,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/ugeneunipro/ugene,https://github.com/ugeneunipro/ugene,1,,,1,1,1,1,0,0,0,0,0,0,1,UGENE is free open-source cross-platform bioinformatics software,"# UGENE\nDownload UGENE: [https://ugeneunipro.github.io/ugene/](https://ugeneunipro.github.io/ugene/)\n\n## Building UGENE\n\n### Prerequisites\n\nQt (>= 5.12.0 and <= 5.15.x) with the following components installed with Qt installer:\n* Desktop\n* QtScript\n\n### For Windows users:\n\nTo build with devenv (Visual Studio):\n\n1. `qmake -r -tp vc ugene.pro`\n2. open ugene.sln from Visual Studio and build or run `devenv.exe ugene.sln /Build` from MSVC command line\n\nTo build with nmake:\n\n1. `qmake -r ugene.pro`\n2. run `nmake`, `nmake debug` or `nmake release` to build UGENE\n\n### For *nix users:\n\nTo build and install UGENE on *nix:\n\n1. `qmake -r PREFIX=/opt/ugene-${VERSION}`.\n2. `make -j 4`\n3. `sudo make install`\n4. `sudo ln -s /opt/ugene-${VERSION}/ugene /usr/bin`\n5. `ugene -ui`\n\n> Note: you do not need 'sudo' if you select your own folder for the installation.\n\n",203,bioinformatics,C++,12,CMake,Python,Shell,QMake,Batchfile,NSIS,C++,C,HTML,JavaScript,Roff,Objective-C++,,,,,,,,,,,,,,,,,1600,94,1501,5,102,29,1599,207637,60,55,53,2,e911f113829727d9c62b1e43f2feea6499ff344f,"UGENE-8079. [Crash, wd, debug] Double opening sample and setting brea…",2024-07-18T12:59:39Z,EvelinaBiserova,39720063+EvelinaBiserova@users.noreply.github.com,EvelinaBiserova,Release 50.0,"### Changes\r\n <li style=""margin-bottom: 10px;""> Primer3 can be run without a target sequence.</li>\r\n    <li style=""margin-bottom: 10px;""> Primer-BLAST has been implemented in UGENE.</li>\r\n    <li style=""margin-bottom: 10px;""> The hairpin visualization tool <i>mfold</i> has been integrated into UGENE.</li>\r\n    <li style=""margin-bottom: 10px;""> The PlasMapper plasmids annotation database has been updated to the latest version.</li>\r\n    <li style=""margin-bottom: 10px;""> Python, included in the UGENE package, has been upgraded to version 3.</li>\r\n    <li style=""margin-bottom: 10px;""> The list of presets for Primer3 has been expanded.</li>\r\n    <li style=""margin-bottom: 10px;""> Improved compatibility with theme settings in macOS.</li>\r\n    <li style=""margin-bottom: 10px;""> Fixed issues related to the stability and usability of UGENE.</li>\r\n\r\n### Demo\r\n  See also a <a href=""https://youtu.be/8luxfbpfjt4?si=6ll_R_e9DmSRwW7r"">new video</a> about the recent improvements in UGENE.",50,Yuliya Algaer,,yalgaer,GNU General Public License v2.0,ugene,ugeneunipro,36,ugene,bioinformatics,ngs,dna,workflow,pipeline,science,cpp,qt5,sequencing,cross-platform,msa,,,,,,,,,/ugeneunipro/ugene,53,21,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/tum-pbs/PhiFlow,https://github.com/tum-pbs/PhiFlow,1,Involves machine learning,,1,1,1,1,0,0,0,0,0,0,1,A differentiable PDE solving framework for machine learning,"# ![PhiFlow](docs/figures/Logo_DallE2_3_layout.png)\n\n![Build Status](https://github.com/tum-pbs/PhiFlow/actions/workflows/unit-tests.yml/badge.svg)\n[![PyPI pyversions](https://img.shields.io/pypi/pyversions/phiflow.svg)](https://pypi.org/project/phiflow/)\n[![PyPI license](https://img.shields.io/pypi/l/phiflow.svg)](https://pypi.org/project/phiflow/)\n[![Code Coverage](https://codecov.io/gh/tum-pbs/PhiFlow/branch/develop/graph/badge.svg)](https://codecov.io/gh/tum-pbs/PhiFlow/branch/develop/)\n[![Google Collab Book](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tum-pbs/PhiFlow/blob/develop/docs/Fluids_Tutorial.ipynb)\n\n\nΦ<sub>Flow</sub> is an open-source simulation toolkit built for optimization and machine learning applications.\nIt is written mostly in Python and can be used with\n[NumPy](https://numpy.org/),\n[PyTorch](https://pytorch.org/),\n[Jax](https://github.com/google/jax)\nor [TensorFlow](https://www.tensorflow.org/).\nThe close integration with these machine learning frameworks allows it to leverage their automatic differentiation functionality,\nmaking it easy to build end-to-end differentiable functions involving both learning models and physics simulations.\n\n\n## Examples\n\n### Grids\n\n<table>\n    <tbody>\n        <tr>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/grids/Fluid_Logo.html""><img src=""docs/figures/examples/grids/Fluid_Logo.gif""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/grids/Wake_Flow.html""><img src=""docs/figures/examples/grids/Wake_Flow.png""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/grids/Lid_Driven_Cavity.html""><img src=""docs/figures/examples/grids/Lid_Driven_Cavity.png""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/grids/Taylor_Green.html""><img src=""docs/figures/examples/grids/Taylor_Green.jpg""></a></td>\n        </tr>\n        <tr>\n            <td align=""center"">Fluid logo</td>\n            <td align=""center"">Wake flow</td>\n            <td align=""center"">Lid-driven cavity</td>\n            <td align=""center"">Taylor-Green</td>\n        </tr>\n        <tr>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/grids/Smoke_Plume.html""><img src=""docs/figures/examples/grids/Smoke_Plume.png""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/grids/Variable_Boundaries.html""><img src=""docs/figures/examples/grids/Variable_Boundaries.jpg""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/grids/Batched_Smoke.html""><img src=""docs/figures/examples/grids/Batched_Smoke.png""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/grids/Moving_Obstacles.html""><img src=""docs/figures/examples/grids/Moving_Obstacles.png""></a></td>\n        </tr>\n        <tr>\n            <td align=""center"">Smoke plume</td>\n            <td align=""center"">Variable boundaries</td>\n            <td align=""center"">Parallel simulations</td>\n            <td align=""center"">Moving obstacles</td>\n        </tr>\n        <tr>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/grids/Rotating_Bar.html""><img src=""docs/figures/examples/grids/Rotating_Bar.jpg""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/grids/Multi_Grid_Fluid.html""><img src=""docs/figures/examples/grids/Multi_Grid_Fluid.jpg""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/grids/Higher_order_Kolmogorov.html""><img src=""docs/figures/examples/grids/Higher_Order_Kolmogorov.jpg""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/grids/Heat_Flow.html""><img src=""docs/figures/examples/grids/Heat_Flow.png""></a></td>\n        </tr>\n        <tr>\n            <td align=""center"">Rotating bar</td>\n            <td align=""center"">Multi-grid fluid</td>\n            <td align=""center"">Higher-order Kolmogorov</td>\n            <td align=""center"">Heat flow</td>\n        </tr>\n        <tr>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/grids/Burgers.html""><img src=""docs/figures/examples/grids/Burgers.png""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/grids/Reaction_Diffusion.html""><img src=""docs/figures/examples/grids/Reaction_Diffusion.png""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/grids/Waves.html""><img src=""docs/figures/examples/grids/Waves.png""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/grids/Julia_Set.html""><img src=""docs/figures/examples/grids/Julia_Set.png""></a></td>\n        </tr>\n        <tr>\n            <td align=""center"">Burgers' equation</td>\n            <td align=""center"">Reaction-diffusion</td>\n            <td align=""center"">Waves</td>\n            <td align=""center"">Julia Set</td>\n        </tr>\n    </tbody>\n</table>\n\n### Mesh\n\n<table>\n    <tbody>\n        <tr>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/mesh/FVM_BackStep.html""><img src=""docs/figures/examples/mesh/FVM_BackStep.png""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/mesh/FVM_Heat.html""><img src=""docs/figures/examples/mesh/FVM_Heat.png""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/mesh/Build_Mesh.html""><img src=""docs/figures/examples/mesh/Build_Mesh.png""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/mesh/FVM_Cylinder_GMsh.html""><img src=""docs/figures/examples/mesh/FVM_Cylinder_GMsh.png""></a></td>\n        </tr>\n        <tr>\n            <td align=""center"">Backward facing step</td>\n            <td align=""center"">Heat flow</td>\n            <td align=""center"">Mesh construction</td>\n            <td align=""center"">Wake flow</td>\n        </tr>\n    </tbody>\n</table>\n\n\n\n### Particles\n\n<table>\n    <tbody>\n        <tr>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/particles/SPH.html""><img src=""docs/figures/examples/particles/SPH.jpg""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/particles/FLIP.html""><img src=""docs/figures/examples/particles/FLIP.png""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/particles/Streamlines.html""><img src=""docs/figures/examples/particles/Streamlines.jpg""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/particles/Terrain.html""><img src=""docs/figures/examples/particles/Terrain.jpg""></a></td>\n        </tr>\n        <tr>\n            <td align=""center"">SPH</td>\n            <td align=""center"">FLIP</td>\n            <td align=""center"">Streamlines</td>\n            <td align=""center"">Terrain</td>\n        </tr>\n        <tr>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/particles/Gravity.html""><img src=""docs/figures/examples/particles/Gravity.jpg""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/particles/Billiards.html""><img src=""docs/figures/examples/particles/Billiards.png""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/particles/Ropes.html""><img src=""docs/figures/examples/particles/Ropes.png""></a></td>\n        </tr>\n        <tr>\n            <td align=""center"">Gravity</td>\n            <td align=""center"">Billiards</td>\n            <td align=""center"">Ropes</td>\n        </tr>\n    </tbody>\n</table>\n\n### Optimization & Networks\n\n<table>\n    <tbody>\n        <tr>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/optim/Gradient_Descent.html""><img src=""docs/figures/examples/optim/Gradient_Descent.png""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/optim/Optimize_Throw.html""><img src=""docs/figures/examples/optim/Optimize_Throw.png""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/optim/Learn_Throw.html""><img src=""docs/figures/examples/optim/Learn_Throw.jpg""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/optim/PIV.html""><img src=""docs/figures/examples/optim/PIV.jpg""></a></td>\n        </tr>\n        <tr>\n            <td align=""center"">Gradient Descent</td>\n            <td align=""center"">Optimize throw</td>\n            <td align=""center"">Learning to throw</td>\n            <td align=""center"">PIV</td>\n        </tr>\n        <tr>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/optim/Close_Packing.html""><img src=""docs/figures/examples/optim/Close_Packing.png""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/optim/Learn_Potential.html""><img src=""docs/figures/examples/optim/Learn_Potential.png""></a></td>\n            <td style=""width: 25%;""><a href=""https://tum-pbs.github.io/PhiFlow/examples/optim/Differentiable_Pressure.html""><img src=""docs/figures/examples/optim/Differentiable_Pressure.jpg""></a></td>\n        </tr>\n        <tr>\n            <td align=""center"">Close packing</td>\n            <td align=""center"">Learning Φ(x,y)</td>\n            <td align=""center"">Differentiable pressure</td>\n        </tr>\n    </tbody>\n</table>\n\n\n## Installation\n\nInstallation with [pip](https://pypi.org/project/pip/) on [Python 3.6](https://www.python.org/downloads/) and above:\n``` bash\n$ pip install phiflow\n```\nInstall [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/install) or [Jax](https://github.com/google/jax#installation) in addition to Φ<sub>Flow</sub> to enable machine learning capabilities and GPU execution.\nTo enable the web UI, also install [Dash](https://pypi.org/project/dash/).\nFor optimal GPU performance, you may compile the custom CUDA operators, see the [detailed installation instructions](https://tum-pbs.github.io/PhiFlow/Installation_Instructions.html).\n\nYou can verify your installation by running\n```bash\n$ python3 -c ""import phi; phi.verify()""\n```\nThis will check for compatible PyTorch, Jax and TensorFlow installations as well.\n\n\n## Features\n\n* Tight integration with PyTorch, Jax and TensorFlow for straightforward neural network training with fully differentiable simulations that can [run on the GPU](https://tum-pbs.github.io/PhiFlow/GPU_Execution.html#enabling-gpu-execution).\n* Built-in PDE operations with focus on fluid phenomena, allowing for concise formulation of simulations.\n* Flexible, easy-to-use [web interface](https://tum-pbs.github.io/PhiFlow/Web_Interface.html) featuring live visualizations and interactive controls that can affect simulations or network training on the fly.\n* Object-oriented, vectorized design for expressive code, ease of use, flexibility and extensibility.\n* Reusable simulation code, independent of backend and dimensionality, i.e. the exact same code can run a 2D fluid sim using NumPy and a 3D fluid sim on the GPU using TensorFlow or PyTorch.\n* High-level linear equation solver with automated sparse matrix generation.\n\n\n## 📖 Documentation and Tutorials\n[**Documentation Overview**](https://tum-pbs.github.io/PhiFlow/)\n&nbsp; • &nbsp; [**▶ YouTube Tutorials**](https://www.youtube.com/playlist?list=PLYLhRkuWBmZ5R6hYzusA2JBIUPFEE755O)\n&nbsp; • &nbsp; [**API**](https://tum-pbs.github.io/PhiFlow/phi/)\n&nbsp; • &nbsp; [**Demos**](https://github.com/tum-pbs/PhiFlow/tree/master/demos)\n&nbsp; • &nbsp; [<img src=""https://www.tensorflow.org/images/colab_logo_32px.png"" height=16> **Playground**](https://colab.research.google.com/drive/1zBlQbmNguRt-Vt332YvdTqlV4DBcus2S#offline=true&sandboxMode=true)\n\nΦ-Flow builds on the tensor functionality from [Φ<sub>ML</sub>](https://github.com/tum-pbs/PhiML).\nTo understand how Φ<sub>Flow</sub> works, check [named and typed dimensions](https://tum-pbs.github.io/PhiML/Introduction.html) first.\n\n### Getting started\n\n* [Installation instructions](https://tum-pbs.github.io/PhiFlow/Installation_Instructions.html)\n* [<img src=""https://www.tensorflow.org/images/colab_logo_32px.png"" height=16>](https://colab.research.google.com/github/tum-pbs/PhiFlow/blob/develop/docs/Math_Introduction.ipynb) [Tensors](https://tum-pbs.github.io/PhiFlow/Math_Introduction.html)\n* [<img src=""https://www.tensorflow.org/images/colab_logo_32px.png"" height=16>](https://colab.research.google.com/github/tum-pbs/PhiFlow/blob/develop/docs/Fluids_Tutorial.ipynb) [Fluids](https://tum-pbs.github.io/PhiFlow/Fluids_Tutorial.html)\n* [<img src=""https://www.tensorflow.org/images/colab_logo_32px.png"" height=16>](https://colab.research.google.com/github/tum-pbs/PhiFlow/blob/develop/docs/Cookbook.ipynb) [Cookbook](https://tum-pbs.github.io/PhiFlow/Cookbook.html)\n\n### Physics\n\n* [Grid-based fluids](https://tum-pbs.github.io/PhiFlow/Fluid_Simulation.html)\n* [Higher-order schemes](https://tum-pbs.github.io/PhiFlow/Taylor_Green_Comparison.html)\n\n### Fields\n\n* [Overview](https://tum-pbs.github.io/PhiFlow/Fields.html)\n* [Staggered grids](https://tum-pbs.github.io/PhiFlow/Staggered_Grids.html)\n* [I/O](https://tum-pbs.github.io/PhiFlow/Reading_and_Writing_Data.html) & [scene format](https://tum-pbs.github.io/PhiFlow/Scene_Format_Specification.html)\n\n### Geometry\n\n* [Overview](https://tum-pbs.github.io/PhiFlow/Geometry.html)\n* [Signed distance fields](https://tum-pbs.github.io/PhiFlow/SDF.html)\n* [Heightmaps](https://tum-pbs.github.io/PhiFlow/Heightmaps.html)\n\n### Tensors\n\n* [▶️ Introduction Video](https://youtu.be/4nYwL8ZZDK8)\n* [Introduction Notebook](Math_Introduction.html)\n* [GPU execution](https://tum-pbs.github.io/PhiFlow/GPU_Execution.html#enabling-gpu-execution)\n\n### Other\n\n* [Φ<sub>Flow</sub> to Blender](https://github.com/intergalactic-mammoth/phiflow2blender) \n* [What to Avoid](https://tum-pbs.github.io/PhiFlow/Known_Issues.html): How to keep your code compatible with PyTorch, TensorFlow and Jax\n* [Legacy visualization](https://tum-pbs.github.io/PhiFlow/Visualization.html) & [Dash](https://tum-pbs.github.io/PhiFlow/Web_Interface.html) & [Console](https://tum-pbs.github.io/PhiFlow/ConsoleUI.html)\n* [Legacy physics overview](https://tum-pbs.github.io/PhiFlow/Physics.html)\n\n\n## 📄 Citation\n\nPlease use the following citation:\n\n```\n@inproceedings{holl2024phiflow,\n  title={${\Phi}_{\text{Flow}}$ ({PhiFlow}): Differentiable Simulations for PyTorch, TensorFlow and Jax},\n  author={Holl, Philipp and Thuerey, Nils},\n  booktitle={International Conference on Machine Learning},\n  year={2024},\n  organization={PMLR}\n}\n```\n\n## Publications\n\nWe will upload a whitepaper, soon.\nIn the meantime, please cite the ICLR 2020 paper.\n\n* [Learning to Control PDEs with Differentiable Physics](https://ge.in.tum.de/publications/2020-iclr-holl/), *Philipp Holl, Vladlen Koltun, Nils Thuerey*, ICLR 2020.\n* [Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative PDE-Solvers](https://arxiv.org/abs/2007.00016), *Kiwon Um, Raymond Fei, Philipp Holl, Robert Brand, Nils Thuerey*, NeurIPS 2020.\n* [Φ<sub>Flow</sub>: A Differentiable PDE Solving Framework for Deep Learning via Physical Simulations](https://montrealrobotics.ca/diffcvgp/), *Nils Thuerey, Kiwon Um, Philipp Holl*, DiffCVGP workshop at NeurIPS 2020.\n* [Physics-based Deep Learning](https://physicsbaseddeeplearning.org/intro.html) (book), *Nils Thuerey, Philipp Holl, Maximilian Mueller, Patrick Schnell, Felix Trost, Kiwon Um*, DiffCVGP workshop at NeurIPS 2020.\n* [Half-Inverse Gradients for Physical Deep Learning](https://arxiv.org/abs/2203.10131), *Patrick Schnell, Philipp Holl, Nils Thuerey*, ICLR 2022.\n* [Scale-invariant Learning by Physics Inversion](https://arxiv.org/abs/2109.15048), *Philipp Holl, Vladlen Koltun, Nils Thuerey*, NeurIPS 2022.\n\n\n## Benchmarks & Data Sets\n\nΦ<sub>Flow</sub> has been used in the creation of various public data sets, such as\n[PDEBench](https://github.com/pdebench/PDEBench) and [PDEarena](https://microsoft.github.io/pdearena/).\n\n[See more packages that use Φ<sub>Flow</sub>](https://github.com/tum-pbs/PhiFlow/network/dependents)\n\n## 🕒 Version History\n\nThe [Version history](https://github.com/tum-pbs/PhiFlow/releases) lists all major changes since release.\nThe releases are also listed on [PyPI](https://pypi.org/project/phiflow/).\n\n## 👥 Contributions\n\nContributions are welcome! Check out [this document](CONTRIBUTING.md) for guidelines.\n\n## Acknowledgements\n\nThis work is supported by the ERC Starting Grant realFlow (StG-2015-637014) and the Intel Intelligent Systems Lab.\n",1286,pde-solver,Python,1,Python,,,,,,,,,,,,,,,,,,,,,,,,,,,,89,8,81,0,8,12,589,879500,183,76,70,6,cd40456a6b555c60b3a88c282cf7dda4f3af4ad1,Merge pull request #165 from tum-pbs/develop,2024-07-15T13:41:59Z,Nils Thuerey,nils.thuerey@tum.de,thunil,3.0.0,"Version 3.0 marks a milestone release for Φ-Flow, introducing many new features and simplifying the API. #165 \r\n\r\n**Highlights**\r\n\r\n* Support for unstructured meshes. This includes many field and physics operations, allowing grid simulations to be ported to FVM with little effort. Meshes can be loaded from `.su2` and `.gmsh` files.\r\n* Major plotting improvements: new plot types, such as bar charts, histograms, streamlines, points clouds with cmap. More flexible arguments, e.g. error bars, color, alpha, same_scale.\r\n* Improved documentation: The GitHub page now lists a collection of examples in the form of Jupyter notebooks.\r\n* Sparse neighborhood search using GPU-enabled hash grids. This enables simulations with interacting particles, such as SPH.\r\n* All linear solves can now use the ILU preconditioner.\r\n* The `phi.math` package is now stand-along as the `phiml` library.\r\n* All types of fields have been merge into the `Field` class which makes a lot of functionality more easily accessible. The legacy constructors still work but now return `Field` instances.\r\n* Boundaries are now easier to define, e.g. `{'x-': 0, 'x+': 1}`\r\n* Support for geometries defined by SDF grids.",3.0.0,,,holl-,MIT License,PhiFlow,tum-pbs,32,differentiable-simulations,fluid-simulations,deep-learning,neural-networks,pde-solver,,,,,,,,,,,,,,,,/tum-pbs/PhiFlow,34,25,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/tskit-dev/tskit,https://github.com/tskit-dev/tskit,0.5,"tool, not a product",0,0,1,0,0,0,0,1,0,0,0,0,Population-scale genomics,"# tskit  <img align=""right"" width=""145"" height=""90"" src=""https://github.com/tskit-dev/administrative/blob/main/tskit_logo.svg"">\n\n[![License](https://img.shields.io/github/license/tskit-dev/tskit)](https://github.com/tskit-dev/tskit/blob/main/LICENSE)\n[![Contributors](https://img.shields.io/github/contributors/tskit-dev/tskit)](https://github.com/tskit-dev/tskit/graphs/contributors)\n[![Commit activity](https://img.shields.io/github/commit-activity/m/tskit-dev/tskit)](https://github.com/tskit-dev/tskit/commits/main)\n[![Coverage](https://codecov.io/gh/tskit-dev/tskit/branch/main/graph/badge.svg)](https://codecov.io/gh/tskit-dev/tskit)\n![OS](https://img.shields.io/badge/OS-linux%20%7C%20OSX%20%7C%20win--64-steelblue)\n\n\nSuccinct tree sequences are a highly efficient way of storing a set of related DNA\nsequences by encoding their ancestral history as a set of correlated trees along the\ngenome. The tree sequence format is output by a number of software libraries and programs\n(such as [msprime](https://github.com/tskit-dev/msprime),\n[SLiM](https://github.com/MesserLab/SLiM),\n[fwdpp](http://molpopgen.github.io/fwdpp/), and\n[tsinfer](https://tsinfer.readthedocs.io/en/latest/)) that either simulate or infer\nthe evolutionary history of genetic sequences. The evolutionary history of genetic\nsequences is often technically referred to as an Ancestral Recombination Graph (ARG);\nsuccinct tree sequences are fully compatible with this formulation, and tskit is a\ntherefore a powerful platform for processing ARGs.\n\nThe `tskit` library provides the underlying functionality used to load, examine, and\nmanipulate tree sequences, including efficient methods for calculating genetic\nstatistics. It often forms part of an installation of other software packages such as\nthose listed above. Please see the\n[documentation](https://tskit.dev/tskit/docs/latest/) for further details, which\nincludes\n[installation instructions](https://tskit.dev/tskit/docs/latest/installation.html).\nAlso see the [road map](https://github.com/tskit-dev/tskit/blob/main/ROADMAP.md) for\nplanned improvements and additions to the library.\n\nTo get started with tskit, tutorials and other content are at http://tskit.dev. For help\nand support from the community you can use\n[discussions](https://github.com/tskit-dev/tskit/discussions) here on github, or raise an\nissue for a specific bug or feature request.\n\nWe warmly welcome contributions from the community. Raise an issue if you have an\nidea you'd like to work on, or submit a PR for comments and help.\n\nThe base `tskit` library provides both a Python and C API. A Rust API is provided in the\n[tskit-rust](https://github.com/tskit-dev/tskit-rust) repository.\n\n\n#### Python API\n[![PyPI version](https://img.shields.io/pypi/v/tskit.svg)](https://pypi.org/project/tskit/)\n[![Supported Python Versions](https://img.shields.io/pypi/pyversions/tskit.svg)](https://pypi.org/project/tskit/)\n[![Wheel](https://img.shields.io/pypi/wheel/tskit)](https://pypi.org/project/tskit/)\n[![Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![Travis](https://img.shields.io/travis/tskit-dev/tskit)](https://travis-ci.org/github/tskit-dev/tskit)\n\nMost users of `tskit` will use the python API as it provides a convenient, high-level API\nto access, analyse and create tree sequences. Full documentation is\n[here](https://tskit.dev/tskit/docs/latest/python-api.html).   \n\n#### C API\n[![C99](https://img.shields.io/badge/Language-C99-steelblue.svg)](https://en.wikipedia.org/wiki/C99)\n[![CircleCI](https://circleci.com/gh/tskit-dev/tskit.svg?style=shield)](https://circleci.com/gh/tskit-dev/tskit)\n\nThe `tskit` C API provides comprehensive, low-level methods for manipulating and\nprocessing tree-sequences. Written to the C99 standard and fully thread-safe, it can be\nused with either C or C++. Full documentation is\n[here](https://tskit.dev/tskit/docs/latest/c-api.html).\n",147,bioinformatics,Python,6,Python,Makefile,C,Meson,C++,Dockerfile,,,,,,,,,,,,,,,,,,,,,,,1603,286,1277,40,6,59,41,18117,70,1178,893,285,beafeba3f576da4524e24b00aa184e608f6de4c4,Merge pull request #2964 from jeromekelleher/compile-numpy2,2024-06-27T12:43:34Z,Ben Jeffery,ben.jeffery@bdi.ox.ac.uk,benjeffery,Python 0.5.8,"- Add support for numpy 2 ([@jeromekelleher](https://github.com/jeromekelleher), [@benjeffery](https://github.com/benjeffery), [#2964](https://github.com/tskit-dev/tskit/issues/2964))\r\n\r\n\r\n\r\n",0.5.8,,,github-actions[bot],MIT License,tskit,tskit-dev,54,bioinformatics,genomics,phylogenetics,python,science,tree-sequences,,,,,,,,,,,,,,,/tskit-dev/tskit,84,13,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/thorvg/thorvg,https://github.com/thorvg/thorvg,0,,,0,0,0,0,0,0,1,1,0,0,0,Thor Vector Graphics is a lightweight portable library used for drawing vector-based scenes and animations including SVG and Lottie. It can be freely utilized across various software platforms and applications to visualize graphical contents.,"[![Discord](https://img.shields.io/badge/Community-5865f2?style=flat&logo=discord&logoColor=white)](https://discord.gg/n25xj6J6HM)\n[![ThorVGPT](https://img.shields.io/badge/ThorVGPT-76A99C?style=flat&logo=openai&logoColor=white)](https://chat.openai.com/g/g-Ht3dYIwLO-thorvgpt)\n[![OpenCollective](https://img.shields.io/badge/OpenCollective-84B5FC?style=flat&logo=opencollective&logoColor=white)](https://opencollective.com/thorvg)\n[![License](https://img.shields.io/badge/licence-MIT-green.svg?style=flat)](LICENSE)\n![BinarySize](https://img.shields.io/badge/Size->150kb-blue)\n[![CodeFactor](https://www.codefactor.io/repository/github/hermet/thorvg/badge)](https://www.codefactor.io/repository/github/hermet/thorvg)\n<br>\n[![Build Ubuntu](https://github.com/thorvg/thorvg/actions/workflows/build_ubuntu.yml/badge.svg?branch=main&event=push)](https://github.com/thorvg/thorvg/actions/workflows/build_ubuntu.yml)\n[![Build Windows](https://github.com/thorvg/thorvg/actions/workflows/build_windows.yml/badge.svg?branch=main&event=push)](https://github.com/thorvg/thorvg/actions/workflows/build_windows.yml)\n[![Build macOS](https://github.com/thorvg/thorvg/actions/workflows/build_macos.yml/badge.svg?branch=main&event=push)](https://github.com/thorvg/thorvg/actions/workflows/build_macos.yml)\n[![Build iOS](https://github.com/thorvg/thorvg/actions/workflows/build_ios.yml/badge.svg?branch=main&event=push)](https://github.com/thorvg/thorvg/actions/workflows/build_ios.yml)\n[![Build Android](https://github.com/thorvg/thorvg/actions/workflows/build_android.yml/badge.svg?branch=main&event=push)](https://github.com/thorvg/thorvg/actions/workflows/build_android.yml)\n\n\n# ThorVG\n<p align=""center"">\n  <img width=""800"" height=""auto"" src=""https://github.com/thorvg/thorvg/blob/main/res/logo/512/thorvg-banner.png"">\n</p>\nThorVG is an open-source graphics library designed for creating vector-based scenes and animations. Embracing the philosophy of ""Simpler is better,"" the ThorVG project offers intuitive and user-friendly interfaces, all the while maintaining a compact size and minimal software complexity. <br />\n<br />\nThe following list shows primitives that are supported by ThorVG: <br />\n<br />\n\n * Shapes: Line, Arc, Curve, Path, Polygon\n * Filling: Solid Color, Linear & Radial Gradients and Texture Mapping \n * Stroking: Width, Join, Cap, Dash Patterns\n * Scene Graph & Transformations\n * Composition: Blending, Masking, Path Clipping\n * Text: Unicode Characters and Horizontal Text Layout using the Scalable Fonts (TTF)\n * Images: TVG, SVG, JPG, PNG, WebP, Raw Bitmap\n * Animations: Lottie\n<p align=""center"">\n  <img width=""700"" height=""auto"" src=""https://github.com/thorvg/thorvg/blob/main/res/example_primitives.png"">\n</p>\n​ThorVG is designed for a wide range of programs, offering adaptability for integration and use in various applications and systems. It achieves this through a single binary with selectively buildable, modular components in a building block style. This ensures both optimal size and easy maintenance. <br />\n<br />\n<p align=""center"">\n  <img width=""700"" height=""auto"" src=""https://github.com/thorvg/thorvg/blob/main/res/example_structure.png"">\n</p>\nIf your program includes the main renderer, you can seamlessly utilize ThorVG APIs by transitioning drawing contexts between the main renderer and ThorVG. Throughout these API calls, ThorVG effectively serializes drawing commands among volatile paint nodes. Subsequently, it undertakes synchronous or asynchronous rendering via its backend raster engines.<br />\n<br />\nThorVG is adept at handling vector images, including formats like SVG, and it remains adaptable for accommodating additional popular formats as needed. In the rendering process, the library may generate intermediate frame buffers for scene compositing, though only when essential. The accompanying diagram provides a concise overview of how to effectively incorporate ThorVG within your system.<br />\n<br />\n<p align=""center"">\n  <img width=""900"" height=""auto"" src=""https://github.com/thorvg/thorvg/blob/main/res/example_flow.png"">\n</p>\nThorVG incorporates a threading mechanism that aims to seamlessly acquire subsequent scenes without unnecessary delays. It operates using a finely-tuned task scheduler based on thread pools, encompassing various tasks such as encoding, decoding, updating, and rendering. This design ensures that all tasks can effectively leverage multi-processing capabilities.<br />\n<br />\nThe task scheduler has been meticulously crafted to conceal complexity, streamline integration, and enhance user convenience. Therefore, the policy it employs is optional, allowing users to select it based on their specific requirements.<br />\n<br />\n<p align=""center"">\n  <img width=""900"" height=""auto"" src=""https://github.com/thorvg/thorvg/blob/main/res/example_thread.png"">\n</p>\n\n## Contents\n- [ThorVG](#thorvg)\n  - [Installation](#installation)\n    - [Build and Install](#build-and-install)\n    - [Build with Visual Studio](#build-with-visual-studio)\n    - [Install with vcpkg](#install-with-vcpkg)\n    - [Install with Conan](#install-with-conan)\n    - [Install with MSYS2](#install-with-msys2)\n  - [Quick Start](#quick-start)\n  - [SVG](#svg)\n  - [Lottie](#lottie)\n  - [TVG Picture](#tvg-picture)\n  - [In Practice](#in-practice)\n    - [dotLottie](#dotlottie)\n    - [Godot](#godot)\n    - [Tizen](#tizen)\n  - [Examples](#examples)\n  - [Documentation](#documentation)\n  - [Tools](#tools)\n    - [ThorVG Viewer](#thorvg-viewer)\n    - [Lottie to GIF](#lottie-to-gif)\n    - [SVG to PNG](#svg-to-png)\n    - [SVG to TVG](#svg-to-tvg)\n  - [API Bindings](#api-bindings)\n  - [Dependencies](#dependencies)\n  - [Contributors](#contributors)\n  - [Communication](#communication)\n\n[](#contents)\n<br />\n## Installation\nThis section details the steps required to configure the environment for installing ThorVG.<br />\n<br />\n### Build and Install\nThorVG supports [meson](https://mesonbuild.com/) build system. Install [meson](http://mesonbuild.com/Getting-meson.html) and [ninja](https://ninja-build.org/) if you don't have them already.\n\nRun meson to configure ThorVG in the thorvg root folder.\n```\nmeson setup builddir\n```\nRun ninja to build & install ThorVG:\n```\nninja -C builddir install\n```\n\nRegardless of the installation, all build results (symbols, executable) are generated in the builddir folder in thorvg. Some results such as examples won't be installed, you can check More examples section to see how to change it. <br/>\n​<br/>\nNote that some systems might include ThorVG package as a default component. In that case, you can skip this manual installation.</br>\n\n### Build with Visual Studio\nIf you want to create Visual Studio project files, use the command --backend=vs. The resulting solution file (thorvg.sln) will be located in the build folder.\n```\nmeson setup builddir --backend=vs\n```\n\n### Install with vcpkg\nYou can download and install pre-packaged ThorVG using the [vcpkg](https://vcpkg.io/en/index.html) package manager.\n\nClone the vcpkg repo. Make sure you are in the directory you want the tool installed to before doing this.\n```\ngit clone https://github.com/Microsoft/vcpkg.git\n```\nRun the bootstrap script to build the vcpkg.\n```\n./bootstrap-vcpkg.sh\n```\nInstall the ThorVG package.\n```\n./vcpkg install thorvg\n```\n\n### Install with Conan\nYou can download and install pre-packaged ThorVG using the [Conan](https://conan.io/) package manager.\n\nFollow the instructions on [this page on how to set up Conan](https://conan.io/downloads).\n\nInstall the ThorVG package:\n\n```\nconan install --requires=""thorvg/[*]"" --build=missing\n```\n\n### Install with MSYS2\nYou can download and install pre-packaged ThorVG using the [MSYS2](https://www.msys2.org/) package manager.\n\nDownload and execute the MSYS2 installer on the web page above and follow the steps. When done, just launch one of the terminals in the start menu, according to the architecture and compiler you want (either 32 or 64 bits, with MSVCRT or UCRT library). Then you can install the ThorVG package :\n\n```\npacman -S thorvg\n```\n\nTo update to a newer release (and update all the packages, which is preferable), run :\n\n```\npacman -Syu\n```\n\n[Back to contents](#contents)\n<br />\n<br />\n## Quick Start\nThorVG renders vector shapes to a given canvas buffer. The following is a quick start to show you how to use the essential APIs.\n\nFirst, you should initialize the ThorVG engine:\n\n```cpp\ntvg::Initializer::init(0);   //thread count\n```\n\nThen it would be best if you prepared an empty canvas for drawing on it:\n\n```cpp\nstatic uint32_t buffer[WIDTH * HEIGHT];                                 //canvas target buffer\n\nauto canvas = tvg::SwCanvas::gen();                                     //generate a canvas\ncanvas->target(buffer, WIDTH, WIDTH, HEIGHT, tvg::SwCanvas::ARGB8888);  //buffer, stride, w, h, Colorspace\n```\n\nNext you can draw multiple shapes on the canvas:\n\n```cpp\nauto rect = tvg::Shape::gen();               //generate a shape\nrect->appendRect(50, 50, 200, 200, 20, 20);  //define it as a rounded rectangle (x, y, w, h, rx, ry)\nrect->fill(100, 100, 100);                   //set its color (r, g, b)\ncanvas->push(move(rect));                    //push the rectangle into the canvas\n\nauto circle = tvg::Shape::gen();             //generate a shape\ncircle->appendCircle(400, 400, 100, 100);    //define it as a circle (cx, cy, rx, ry)\n\nauto fill = tvg::RadialGradient::gen();      //generate a radial gradient\nfill->radial(400, 400, 150);                 //set the radial gradient geometry info (cx, cy, radius)\n\ntvg::Fill::ColorStop colorStops[2];          //gradient colors\ncolorStops[0] = {0.0, 255, 255, 255, 255};   //1st color values (offset, r, g, b, a)\ncolorStops[1] = {1.0, 0, 0, 0, 255};         //2nd color values (offset, r, g, b, a)\nfill->colorStops(colorStops, 2);             //set the gradient colors info\n\ncircle->fill(move(fill));                    //set the circle fill\ncanvas->push(move(circle));                  //push the circle into the canvas\n\n```\n\nThis code generates the following result:\n\n<p align=""center"">\n  <img width=""416"" height=""auto"" src=""https://github.com/thorvg/thorvg/blob/main/res/example_shapes.png"">\n</p>\n\nYou can also draw you own shapes and use dashed stroking:\n\n```cpp\nauto path = tvg::Shape::gen();               //generate a path\npath->moveTo(199, 34);                       //set sequential path coordinates\npath->lineTo(253, 143);\npath->lineTo(374, 160);\npath->lineTo(287, 244);\npath->lineTo(307, 365);\npath->lineTo(199, 309);\npath->lineTo(97, 365);\npath->lineTo(112, 245);\npath->lineTo(26, 161);\npath->lineTo(146, 143);\npath->close();\n\npath->fill(150, 150, 255);                   //path color\n\npath->strokeWidth(3);                        //stroke width\npath->strokeFill(0, 0, 255);                 //stroke color\npath->strokeJoin(tvg::StrokeJoin::Round);    //stroke join style\npath->strokeCap(tvg::StrokeCap::Round);      //stroke cap style\n\nfloat pattern[2] = {10, 10};                 //stroke dash pattern (line, gap)\npath->strokeDash(pattern, 2);                //set the stroke pattern\n\ncanvas->push(move(path));                    //push the path into the canvas\n\n```\n\nThe code generates the following result:\n\n<p align=""center"">\n  <img width=""300"" height=""auto"" src=""https://github.com/thorvg/thorvg/blob/main/res/example_path.png"">\n</p>\n\nNow begin rendering & finish it at a particular time:\n\n```cpp\ncanvas->draw();\ncanvas->sync();\n```\n\nThen you can acquire the rendered image from the buffer memory.\n\nLastly, terminate the engine after its usage:\n\n```cpp\ntvg::Initializer::term();\n```\n[Back to contents](#contents)\n<br />\n<br />\n## SVG\n\nThorVG facilitates [SVG Tiny Specification](https://www.w3.org/TR/SVGTiny12/) rendering via its dedicated SVG interpreter. Adhering to the SVG Tiny Specification, the implementation maintains a lightweight profile, rendering it particularly advantageous for embedded systems. While ThorVG comprehensively adheres to most of the SVG Tiny specs, certain features remain unsupported within the current framework. These include:</br>\n\n - Animation\n - Fonts & Text\n - Interactivity\n - Multimedia\n\nThe following code snippet shows how to draw SVG image using ThorVG:\n\n```cpp\nauto picture = tvg::Picture::gen();         //generate a picture\npicture->load(""tiger.svg"");                 //load a SVG file\ncanvas->push(move(picture));                //push the picture into the canvas\n```\n\nThe result is:\n\n<p align=""center"">\n  <img width=""300"" height=""auto"" src=""https://github.com/thorvg/thorvg/blob/main/res/example_tiger.png"">\n</p>\n\n[Back to contents](#contents)\n<br />\n<br />\n## Lottie\n\nThorVG supports the most powerful Lottie Animation [features](https://lottiefiles.com/supported-features). Lottie is a JSON-based vector animation file format that enables seamless distribution of animations on any platform, akin to shipping static assets. These files are compact and compatible with various devices, scaling up or down without pixelation. With Lottie, you can easily create, edit, test, collaborate, and distribute animations in a user-friendly manner. For more information, please visit [LottieFiles](https://www.lottiefiles.com)' website. <br />\n\nThe following code snippet demonstrates how to use ThorVG to play a Lottie animation.\n```cpp\nauto animation = tvg::Animation::gen();     //generate an animation\nauto picture = animation->picture()         //acquire a picture which associated with the animation.\npicture->load(""lottie.json"");               //load a Lottie file\nauto duration = animation->duration();      //figure out the animation duration time in seconds.\ncanvas->push(tvg::cast(picture));           //push the picture into the canvas\n```\nFirst, an animation and a picture are generated. The Lottie file (lottie.json) is loaded into the picture, and then the picture is added to the canvas. The animation frames are controlled using the animation object to play the Lottie animation. Also you might want to know the animation duration time to run your animation loop.\n```cpp\nanimation->frame(animation->totalFrame() * progress);  //Set a current animation frame to display\ncanvas->update(animation->picture());                  //Update the picture to be redrawn.\n```\nLet's suppose the progress variable determines the position of the animation, ranging from 0 to 1 based on the total duration time of the animation. Adjusting the progress value allows you to control the animation at the desired position. Afterwards, the canvas is updated to redraw the picture with the updated animation frame.<br />\n<br />\n<p align=""center"">\n  <img width=""600"" height=""auto"" src=""https://github.com/thorvg/thorvg/blob/main/res/example_lottie.gif"">\n</p>\n\nPlease check out the [ThorVG Test App](https://thorvg-perf-test.vercel.app/) to see the performance of various Lottie animations powered by ThorVG.\n\n[Back to contents](#contents)\n<br />\n<br />\n## TVG Picture\n\nThorVG introduces the dedicated vector data format, known as TVG Picture, designed to efficiently store Paint node properties within a scene in binary form. This format is meticulously optimized in advance, ensuring compact file sizes and swift data loading processes. </br>\n</br>\nTo leverage the TVG Picture format, ThorVG employs a specialized module called TVG Saver. This module is responsible for optimizing the data associated with all scene-tree nodes and storing them in binary form. During the optimization phase, TVG Saver intelligently eliminates unused information, eliminates duplicated properties, consolidates overlapping shapes, and employs data compression where feasible. Remarkably, these optimizations maintain compatibility with future versions of ThorVG libraries, with data compression utilizing the [Lempel-Ziv-Welchi](https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Welch) algorithm when applicable.</br>\n</br>\nAs a result of these efforts, the final data size is notably smaller than other text-based vector data formats, such as SVG. This reduction in data size not only minimizes I/O operations but also mitigates memory bandwidth requirements during data loading. This aspect proves particularly beneficial for programs reliant on substantial vector resources. </br>\n</br>\nFurthermore, TVG Picture substantially streamlines resource loading tasks by circumventing the need for data interpretation, resulting in reduced runtime memory demands and rendering tasks that subsequently enhance performance. </br>\n</br>\nBy adopting TVG Picture, you can achieve an average reduction of over 30% in data size and loading times (for more details, refer to ""[See More](https://github.com/thorvg/thorvg/wiki/TVG-Picture-Binary-Size)""). Notably, the extent of performance improvement is contingent on resource size and complexity. </br>\n</br>\n<p align=""center"">\n  <img width=""909"" height=""auto"" src=""https://github.com/thorvg/thorvg/blob/main/res/example_tvgsize.png"">\n</p>\n\nAs TVG Saver facilitates the export of the scene-tree into TVG Picture data files (TVG), the subsequent task of importing and restoring this data to programmable instances is efficiently handled by the TVG Loader. For seamless conversion from SVG to TVG, the ThorVG Viewer provides a swift solution.\n\n<p align=""center"">\n  <img width=""710"" height=""auto"" src=""https://github.com/thorvg/thorvg/blob/main/res/example_tvgmodule.png"">\n</p>\n\n\n[Back to contents](#contents)\n<br />\n<br />\n## In Practice\n### dotLottie\n[dotLottie](https://dotlottie.io/) is an open-source file format that aggregates one or more Lottie files and their associated resources, such as images and fonts, into a single file. This enables an efficient and easy distribution of animations. dotLottie files are ZIP archives compressed with the Deflate compression method and carry the file extension of “.lottie”. Think of it as a superset of Lottie. [LottieFiles](https://lottiefiles.com/) aims to achieve just that. [dotLottie player](https://github.com/LottieFiles/dotlottie-rs) by LottieFiles is now powered by ThorVG.\n<p align=""center"">\n  <img width=""798"" height=""auto"" src=""https://github.com/thorvg/thorvg/blob/main/res/example_dotlottie.png"">\n</p>\n\n### Godot\nThorVG has been integrated into the [Godot](https://www.godotengine.org) project to enable the creation of sleek and visually appealing user interfaces (UIs) and vector resources in the Godot game engine. Godot is a modern game engine that is both free and open-source, offering a comprehensive range of tools. With Godot, you can concentrate on developing your game without the need to recreate existing functionalities.\n\n<p align=""center"">\n  <img width=""798"" height=""auto"" src=""https://github.com/thorvg/thorvg/blob/main/res/example_godot.png"">\n</p>\n\n### LVGL\n[LVGL](https://lvgl.io/) is the most popular free and open-source embedded graphics library to create beautiful UIs for any MCU, MPU and display type. The complete graphic framework includes a variety of widgets for you to use in the creation of your GUI, and supports more advanced functions such as animations and anti-aliasing. ThorVG serves as the vector drawing primitives library in the LVGL framework.\n\n<p align=""center"">\n  <img width=""700"" height=""auto"" src=""https://github.com/thorvg/thorvg/blob/main/res/example_lvgl.png"">\n</p>\n\n### Tizen\nThorVG has been integrated into the [Tizen](https://www.tizen.org) platform as the vector graphics engine. [NUI](https://docs.tizen.org/application/dotnet/guides/user-interface/nui/overview/) is the name of Tizen UI framework which is written in C#. ThorVG is the backend engine of the [NUI Vector Graphics](https://docs.tizen.org/application/dotnet/guides/user-interface/nui/vectorgraphics/Overview/) which is used for vector primitive drawings and scalable image contents such as SVG and Lottie Animation among the Tizen applications.\n\n<p align=""center"">\n  <img width=""798"" height=""auto"" src=""https://github.com/thorvg/thorvg/blob/main/res/example_tizen.png"">\n</p>\n\n[Back to contents](#contents)\n<br />\n<br />\n## Examples\nThere are plenty of sample code in `thorvg/examples` to help you in understanding the ThorVG APIs.\n\nTo execute these examples, you can build them with the following meson build option:\n```\nmeson setup builddir -Dexamples=true\n```\nNote that these examples require the SDL dev package for launching. If you're using Linux-based OS, you can easily install this package from your OS distribution server. For Ubuntu, you can install it with this command.\n```\napt-get install libsdl2-dev\n```\nAlternatively, you can read the official guidance [here](https://wiki.libsdl.org/SDL2/Installation) for other platforms. Fore more information, please visit the official [SDL](https://www.libsdl.org/) site.\n\n[Back to contents](#contents)\n<br />\n<br />\n\n## Documentation\nThe ThorVG API documentation can be accessed at [thorvg.org/apis](https://www.thorvg.org/apis), and is also available in the [C++ API](https://github.com/thorvg/thorvg/blob/main/inc/thorvg.h), [C API](https://github.com/thorvg/thorvg/blob/main/src/bindings/capi/thorvg_capi.h) within this repository.\n\n[Back to contents](#contents)\n<br />\n<br />\n## Tools\n### ThorVG Viewer\nThorVG provides the resource verification tool for the ThorVG Engine. [ThorVG viewer](https://thorvg.github.io/thorvg.viewer/) does immediate rendering via web browser running on the ThorVG web-assembly binary, allowing real-time editing of the vector elements on it. It doesn't upload your resources to any external server while allowing to export to supported formats such as TVG, so the designer resource copyright is protected.</br>\n</br>\n\n<p align=""center"">\n  <img width=""700"" height=""auto"" src=""https://github.com/thorvg/thorvg/assets/3711518/edadcc5e-3bbf-489d-a9a1-9570079c7d55""/>\n</p>\n\n### Lottie to GIF\nThorVG provides an executable `lottie2gif` converter that generates a GIF file from a Lottie file.\n\nTo use the `lottie2gif`, you must turn on this feature in the build option:\n```\nmeson setup builddir -Dtools=lottie2gif -Dsavers=gif\n```\nTo use the 'lottie2gif' converter, you need to provide the 'Lottie files' parameter. This parameter can be a file name with the '.json' extension or a directory name. It also accepts multiple files or directories separated by spaces. If a directory is specified, the converter will search for files with the '.json' extension within that directory and all its subdirectories.<br />\n<br />\nOptionally, you can specify the image resolution in the 'WxH' format, with two numbers separated by an 'x' sign, following the '-r' flag.<br />\n<br />\nBoth flags, if provided, are applied to all of the `.json` files.\n\nThe usage examples of the `lottie2gif`:\n```\nUsage:\n    lottie2gif [Lottie file] or [Lottie folder] [-r resolution] [-f fps] [-b background color]\n\nFlags:\n    -r set the output image resolution.\n    -f specifies the frames per second (fps) for the generated animation.\n    -b specifies the base background color (RGB in hex). If not specified, the background color will follow the original content.\n\nExamples:\n    $ lottie2gif input.json\n    $ lottie2gif input.json -f 30\n    $ lottie2gif input.json -r 600x600 -f 30\n    $ lottie2gif lottiefolder\n    $ lottie2gif lottiefolder -r 600x600\n    $ lottie2gif lottiefolder -r 600x600 -f 30 -b fa7410\n```\n\n### SVG to PNG\nThorVG provides an executable `svg2png` converter that generates a PNG file from an SVG file.\n\nTo use the `svg2png`, you must turn on this feature in the build option:\n```\nmeson setup builddir -Dtools=svg2png\n```\nTo use the 'svg2png' converter, you need to provide the 'SVG files' parameter. This parameter can be a file name with the '.svg' extension or a directory name. It also accepts multiple files or directories separated by spaces. If a directory is specified, the converter will search for files with the '.svg' extension within that directory and all its subdirectories.<br />\n<br />\nOptionally, you can specify the image resolution in the 'WxH' format, with two numbers separated by an 'x' sign, following the '-r' flag.<br />\n<br />\nThe background color can be set with the `-b` flag. The `bgColor` parameter should be passed as a three-bytes hexadecimal value in the `ffffff` format. The default background is transparent.<br />\n<br />\nBoth flags, if provided, are applied to all of the `.svg` files.\n\nThe usage examples of the `svg2png`:\n```\nUsage:\n    svg2png [SVG files] [-r resolution] [-b bgColor]\n\nFlags:\n    -r set the output image resolution.\n    -b set the output image background color.\n\nExamples:\n    $ svg2png input.svg\n    $ svg2png input.svg -r 200x200\n    $ svg2png input.svg -r 200x200 -b ff00ff\n    $ svg2png input1.svg input2.svg -r 200x200 -b ff00ff\n    $ svg2png . -r 200x200\n```\n\n### SVG to TVG\nThorVG provides an executable `svg2tvg` converter that generates a TVG file from an SVG file.\n\nTo use `svg2tvg`, you need to activate this feature in the build option:\n```\nmeson setup builddir -Dtools=svg2tvg -Dsavers=tvg\n```\n\nExamples of the usage of the `svg2tvg`:\n```\nUsage:\n   svg2tvg [SVG file] or [SVG folder]\n\nExamples:\n    $ svg2tvg input.svg\n    $ svg2tvg svgfolder\n```\n\n[Back to contents](#contents)\n<br />\n<br />\n## API Bindings\nOur main development APIs are written in C++, but ThorVG also provides API bindings for C.\n\nTo enable CAPI binding, you need to activate this feature in the build options:\n```\nmeson setup builddir -Dbindings=""capi""\n```\n[Back to contents](#contents)\n<br />\n<br />\n## Dependencies\nThorVG offers versatile support for image loading, accommodating both static and external loaders. This flexibility ensures that, even in environments without external libraries, users can still leverage static loaders as a reliable alternative. At its foundation, the ThorVG core library is engineered to function autonomously, free from external dependencies. However, it is important to note that ThorVG also encompasses a range of optional feature extensions, each with its specific set of dependencies. The dependencies associated with these selective features are outlined as follows:\n\n* GL engine: [OpenGL v3.3](https://www.khronos.org/opengl/) or [GLES v3.0](https://www.khronos.org/opengles/)\n* WG engine: [webgpu-native](https://github.com/gfx-rs/wgpu-native)\n* External PNG support: [libpng](https://github.com/glennrp/libpng)\n* External JPG support: [turbojpeg](https://github.com/libjpeg-turbo/libjpeg-turbo)\n* External WebP support: [libwebp](https://developers.google.com/speed/webp/download)\n* Examples: [SDL2](https://www.libsdl.org/)\n\n[Back to contents](#contents)\n<br />\n<br />\n## Contributors\nThorVG stands as a purely open-source initiatives. We are grateful to the individuals, organizations, and companies that have contributed to the development of the ThorVG project. The dedicated efforts of the individuals and entities listed below have enabled ThorVG to reach its current state.\n\n* [Individuals](https://github.com/thorvg/thorvg/blob/main/AUTHORS)\n* [LottieFiles](https://lottiefiles.com/) by Design Barn Inc.\n* Samsung Electronics Co., Ltd\n\n[Back to contents](#contents)\n<br />\n<br />\n## Communication\nFor real-time conversations and discussions, please join us on [Discord](https://discord.gg/n25xj6J6HM)\n\n[Back to contents](#contents)\n",613,graphics,C++,5,Meson,C++,C,Shell,Python,,,,,,,,,,,,,,,,,,,,,,,,1971,262,1696,13,8,51,1806,369120,95,589,498,91,a47839a40b994bb362640dc7529b4e9b560218b5,sw_engine: hotfix simd build breaks,2024-07-19T14:07:31Z,Hermet Park,hermet@lottiefiles.com,hermet,ThorVG 0.14.3,This release includes several improvements and minor bug fixes:\r\n\r\n[SwEngine] Added support for drawing images in 8-bit grayscale format.\r\n[SwEngine] Improved performance of alpha blending operations for faster rendering.\r\n[Renderer] Introduced APIs for querying the engine version. https://github.com/thorvg/thorvg/issues/2543\r\n[Lottie] Properly supported counterclockwise direction for ellipse drawing.\r\n[Lottie]  Achieved (~10%) performance improvement by introducing a designated shape render pool.\r\n[Lottie] Rectified the Expressions where PolyStar was not implemented.\r\n\r\n**Full Changelog**: https://github.com/thorvg/thorvg/compare/v0.14.2...v0.14.3,v0.14.3,Hermet Park,,hermet,MIT License,thorvg,thorvg,62,drawing-library,shapes,svg,path-drawing,image,animation,vector-graphics-engine,png,jpeg,tvg,graphics,motion-graphics,webp,lottie,rendering-engine,webgpu,opengl,font,text,ttf,/thorvg/thorvg,62,28,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/thephpleague/geotools,https://github.com/thephpleague/geotools,0,,,0,0,0,0,0,0,1,1,0,0,0,Geo-related tools PHP 7.3+ library built atop Geocoder and React libraries,"Geotools\n========\n\n**Geotools** is a PHP geo-related library, built atop [Geocoder](https://github.com/willdurand/Geocoder) and\n[React](https://github.com/reactphp/react) libraries.\n\n[![Latest Version](https://poser.pugx.org/league/geotools/v/stable)](https://github.com/thephpleague/geotools/releases)\n[![Total Downloads](https://poser.pugx.org/league/geotools/downloads)](https://packagist.org/packages/league/geotools)\n[![Quality Score](https://img.shields.io/scrutinizer/g/thephpleague/geotools.svg?style=flat-square)](https://scrutinizer-ci.com/g/thephpleague/geotools/?branch=master)\n\nFeatures\n--------\n\n* **Batch** geocode & reverse geocoding request(s) in **series** / in **parallel** against one or a\n**set of providers**. [»](#batch)\n* **Cache** geocode & reverse geocoding result(s) with **PSR-6** to improve performances. [»](#batch)\n* Compute geocode & reverse geocoding in the **command-line interface** (CLI) + dumpers and formatters. [»](#cli)\n* Accept **almost** all kind of WGS84\n[geographic coordinates](http://en.wikipedia.org/wiki/Geographic_coordinate_conversion) as coordinates.\n[»](#coordinate--ellipsoid)\n* Support **23 different ellipsoids** and it's easy to provide a new one if needed. [»](#coordinate--ellipsoid)\n* **Convert** and **format** decimal degrees coordinates to decimal minutes or degrees minutes seconds coordinates.\n[»](#convert)\n* **Convert** decimal degrees coordinates in the\n[Universal Transverse Mercator](http://en.wikipedia.org/wiki/Universal_Transverse_Mercator_coordinate_system)\n(UTM) projection. [»](#convert)\n* Compute the distance in **meter** (by default), **km**, **mi** or **ft** between two coordinates using **flat**,\n**great circle**, **haversine** or **vincenty** algorithms. [»](#distance)\n* Compute the initial and final **bearing** from the origin coordinate to the destination coordinate in degrees.\n[»](#point)\n* Compute the initial and final **cardinal point** (direction) from the origin coordinate to the destination\ncoordinate, read more in [wikipedia](http://en.wikipedia.org/wiki/Cardinal_direction). [»](#point)\n* Compute the **half-way point** (coordinate) between the origin and the destination coordinates. [»](#point)\n* Compute the **destination point** (coordinate) with given bearing in degrees and a distance in meters. [»](#point)\n* Encode a coordinate to a **geo hash** string and decode it to a coordinate, read more in\n[wikipedia](http://en.wikipedia.org/wiki/Geohash) and on [geohash.org](http://geohash.org/). [»](#geohash)\n* Encode a coordinate via the 10:10 algorithm. [»](#1010)\n* **Polygon** class provides methods to check either a poing (coordinate) is in, or on the polygon's boundaries.\n[»](#polygon)\n* A **command-line interface** (CLI) for **Distance**, **Point**, **Geohash** and **Convert** classes. [»](#cli)\n* Integration with Frameworks: **Laravel 4**, **Silex** ... [»](#integration-with-frameworks)\n* ... more to come ...\n\n\nInstallation\n------------\n\n**Geotools** can be found on [Packagist](https://packagist.org/packages/league/geotools).\nThe recommended way to install **Geotools** is through [composer](http://getcomposer.org).\n\nRun the following on the command line:\n\n```\ncomposer require league/geotools\n```\n\n**Important:** you should use the `0.4` version if you use Geocoder `2.x` or/and PHP `5.3`.\n\nAnd install dependencies:\n\n```\ncomposer install\n```\n\nNow you can add the autoloader, and you will have access to the library:\n\n```php\n<?php\n\nrequire 'vendor/autoload.php';\n```\n\n\nUsage & API\n-----------\n\n## Coordinate & Ellipsoid\n\nThe default geodetic datum is [WGS84](http://en.wikipedia.org/wiki/World_Geodetic_System) and coordinates are in\ndecimal degrees.\n\nHere are the available ellipsoids: `AIRY`, `AUSTRALIAN_NATIONAL`, `BESSEL_1841`, `BESSEL_1841_NAMBIA`,\n`CLARKE_1866`, `CLARKE_1880`, `EVEREST`, `FISCHER_1960_MERCURY`, `FISCHER_1968`, `GRS_1967`, `GRS_1980`,\n`HELMERT_1906`, `HOUGH`, `INTERNATIONAL`, `KRASSOVSKY`, `MODIFIED_AIRY`, `MODIFIED_EVEREST`,\n`MODIFIED_FISCHER_1960`, `SOUTH_AMERICAN_1969`, `WGS60`, `WGS66`, `WGS72`, and `WGS84`.\n\nIf you need to use an other ellipsoid, just create an array like this:\n``` php\n<?php\n\n$myEllipsoid = \League\Geotools\Coordinate\Ellipsoid::createFromArray([\n    'name' => 'My Ellipsoid', // The name of the Ellipsoid\n    'a'    => 123.0, // The semi-major axis (equatorial radius) in meters\n    'invF' => 456.0 // The inverse flattening\n]);\n```\n\n**Geotools** is built atop [Geocoder](https://github.com/willdurand/Geocoder). It means it's possible to use the\n`\Geocoder\Model\Address` directly but it's also possible to use a *string* or a simple *array* with its\nlatitude and longitude.\n\nIt supports [valid and acceptable geographic coordinates](http://en.wikipedia.org/wiki/Geographic_coordinate_conversion)\nlike:\n* 40:26:46N,079:56:55W\n* 40:26:46.302N 079:56:55.903W\n* 40°26′47″N 079°58′36″W\n* 40d 26′ 47″ N 079d 58′ 36″ W\n* 40.446195N 79.948862W\n* 40.446195, -79.948862\n* 40° 26.7717, -79° 56.93172\n\nLatitudes below -90.0 or above 90.0 degrees are *capped* through `\League\Geotools\Coordinate\Coordinate::normalizeLatitude()`.\nLongitudes below -180.0 or above 180.0 degrees are *wrapped* through `\League\Geotools\Coordinate\Coordinate::normalizeLongitude()`.\n\n```php\n<?php\n\nuse League\Geotools\Coordinate\Coordinate;\nuse League\Geotools\Coordinate\Ellipsoid;\n\n// from an \Geocoder\Model\Address instance within Airy ellipsoid\n$coordinate = new Coordinate($geocoderResult, Ellipsoid::createFromName(Ellipsoid::AIRY));\n// or in an array of latitude/longitude coordinate within GRS 1980 ellipsoid\n$coordinate = new Coordinate([48.8234055, 2.3072664], Ellipsoid::createFromName(Ellipsoid::GRS_1980));\n// or in latitude/longitude coordinate within WGS84 ellipsoid\n$coordinate = new Coordinate('48.8234055, 2.3072664');\n// or in degrees minutes seconds coordinate within WGS84 ellipsoid\n$coordinate = new Coordinate('48°49′24″N, 2°18′26″E');\n// or in decimal minutes coordinate within WGS84 ellipsoid\n$coordinate = new Coordinate('48 49.4N, 2 18.43333E');\n// the result will be:\nprintf(""Latitude: %F\n"", $coordinate->getLatitude()); // 48.8234055\nprintf(""Longitude: %F\n"", $coordinate->getLongitude()); // 2.3072664\nprintf(""Ellipsoid name: %s\n"", $coordinate->getEllipsoid()->getName()); // WGS 84\nprintf(""Equatorial radius: %F\n"", $coordinate->getEllipsoid()->getA()); // 6378136.0\nprintf(""Polar distance: %F\n"", $coordinate->getEllipsoid()->getB()); // 6356751.317598\nprintf(""Inverse flattening: %F\n"", $coordinate->getEllipsoid()->getInvF()); // 298.257224\nprintf(""Mean radius: %F\n"", $coordinate->getEllipsoid()->getArithmeticMeanRadius()); // 6371007.772533\n// it's also possible to modify the coordinate without creating an other coodinate\n$coordinate->setFromString('40°26′47″N 079°58′36″W');\nprintf(""Latitude: %F\n"", $coordinate->getLatitude()); // 40.446388888889\nprintf(""Longitude: %F\n"", $coordinate->getLongitude()); // -79.976666666667\n```\n\n## Convert\n\nIt provides methods (and aliases) to convert *decimal degrees* WGS84 coordinates to *degrees minutes seconds*\nor *decimal minutes* WGS84 coordinates. You can format the output string easily.\n\nYou can also convert them in the Universal Transverse Mercator (UTM) projection (Southwest coast of Norway and the\nregion of Svalbard are covered).\n\n```php\n<?php\n\n$geotools   = new \League\Geotools\Geotools();\n$coordinate = new \League\Geotools\Coordinate\Coordinate('40.446195, -79.948862');\n$converted  = $geotools->convert($coordinate);\n// convert to decimal degrees without and with format string\nprintf(""%s\n"", $converted->toDecimalMinutes()); // 40 26.7717N, -79 56.93172W\n// convert to degrees minutes seconds without and with format string\nprintf(""%s\n"", $converted->toDegreesMinutesSeconds('<p>%P%D:%M:%S, %p%d:%m:%s</p>')); // <p>40:26:46, -79:56:56</p>\n// convert in the UTM projection (standard format)\nprintf(""%s\n"", $converted->toUniversalTransverseMercator()); // 17T 589138 4477813\n```\n\nHere is the mapping:\n\n**Decimal minutes** | Latitude | Longitude\n--- | --- | ---\nPositive or negative sign | `%P` | `%p`\nDirection | `%L` | `%l`\nDegrees | `%D` | `%d`\nDecimal minutes | `%N` | `%n`\n\n**Degrees minutes seconds** | Latitude | Longitude\n--- | --- | ---\nPositive or negative sign | `%P` | `%p`\nDirection | `%L` | `%l`\nDegrees | `%D` | `%d`\nMinutes | `%M` | `%m`\nSeconds | `%S` | `%s`\n\n## Batch\n\nIt provides a very handy way to batch geocode and reverse geocoding requests in *serie* or in *parallel* against\na set of providers.\nThanks to [Geocoder](https://github.com/willdurand/Geocoder) and [React](https://github.com/reactphp/react) libraries.\n\nIt's possible to batch *one request* (a string) or a *set of request* (an array) against *one provider* or\n*set of providers*.\n\nYou can use a provided **cache engine** or use your own by setting a cache object which should implement\n`League\Geotools\Cache\CacheInterface` and extend `League\Geotools\Cache\AbstractCache` if needed.\n\nAt the moment Geotools supports any PSR-6 cache.\n\nNB: Before you implement caching in your app please be sure that doing so does not violate the Terms of Service\nfor your(s) geocoding provider(s).\n\n```php\n<?php\n\n$geocoder = new \Geocoder\ProviderAggregator(); // or \Geocoder\TimedGeocoder\n$httpClient  = HttpClientDiscovery::find();\n\n$geocoder->registerProviders([\n    new \Geocoder\Provider\GoogleMaps\GoogleMaps($httpClient),\n    new \Geocoder\Provider\OpenStreetMap\OpenStreetMap($httpClient),\n    new \Geocoder\Provider\BingMaps\BingMaps($httpClient, '<FAKE_API_KEY>'), // throws InvalidCredentialsException\n    new \Geocoder\Provider\Yandex\Yandex($httpClient),\n    new \Geocoder\Provider\FreeGeoIp\FreeGeoIp($httpClient),\n    new \Geocoder\Provider\Geoip\Geoip(),\n]);\n\ntry {\n    $geotools = new \League\Geotools\Geotools();\n    $cache    = new \Cache\Adapter\PHPArray\ArrayCachePool();\n\n    $results  = $geotools->batch($geocoder)->setCache($cache)->geocode([\n        'Paris, France',\n        'Copenhagen, Denmark',\n        '74.200.247.59',\n        '::ffff:66.147.244.214'\n    ])->parallel();\n} catch (\Exception $e) {\n    die($e->getMessage());\n}\n\n$dumper = new \Geocoder\Dumper\WktDumper();\nforeach ($results as $result) {\n    // if a provider throws an exception (UnsupportedException, InvalidCredentialsException ...)\n    // an custom /Geocoder/Result/Geocoded instance is returned which embedded the name of the provider,\n    // the query string and the exception string. It's possible to use dumpers\n    // and/or formatters from the Geocoder library.\n    printf(""%s|%s|%s\n"",\n        $result->getProviderName(),\n        $result->getQuery(),\n        '' == $result->getExceptionMessage() ? $dumper->dump($result) : $result->getExceptionMessage()\n    );\n}\n```\n\nYou should get 24 results (4 values to geocode against 6 providers) something like:\n\n```\ngoogle_maps|Paris, France|POINT(2.352222 48.856614)\ngoogle_maps|Copenhagen, Denmark|POINT(12.568337 55.676097)\ngoogle_maps|74.200.247.59|The GoogleMapsProvider does not support IP addresses.\ngoogle_maps|::ffff:66.147.244.214|The GoogleMapsProvider does not support IP addresses.\nopenstreetmap|Paris, France|POINT(2.352133 48.856506)\nopenstreetmap|Copenhagen, Denmark|POINT(12.570072 55.686724)\nopenstreetmap|74.200.247.59|Could not execute query http://nominatim.openstreetmap.org/search?q=74.200.247.59&format=xml&addressdetails=1&limit=1\nopenstreetmap|::ffff:66.147.244.214|The OpenStreetMapProvider does not support IPv6 addresses.\nbing_maps|Paris, France|Could not execute query http://dev.virtualearth.net/REST/v1/Locations/?q=Paris%2C+France&key=<FAKE_API_KEY>\nbing_maps|Copenhagen, Denmark|Could not execute query http://dev.virtualearth.net/REST/v1/Locations/?q=Copenhagen%2C+Denmark&key=<FAKE_API_KEY>\nbing_maps|74.200.247.59|The BingMapsProvider does not support IP addresses.\nbing_maps|::ffff:66.147.244.214|The BingMapsProvider does not support IP addresses.\nyandex|Paris, France|POINT(2.341198 48.856929)\nyandex|Copenhagen, Denmark|POINT(12.567602 55.675682)\nyandex|74.200.247.59|The YandexProvider does not support IP addresses.\nyandex|::ffff:66.147.244.214|The YandexProvider does not support IP addresses.\nfree_geo_ip|Paris, France|The FreeGeoIpProvider does not support Street addresses.\nfree_geo_ip|Copenhagen, Denmark|The FreeGeoIpProvider does not support Street addresses.\nfree_geo_ip|74.200.247.59|POINT(-122.415600 37.748400)\nfree_geo_ip|::ffff:66.147.244.214|POINT(-111.613300 40.218100)\ngeoip|Paris, France|The GeoipProvider does not support Street addresses.\ngeoip|Copenhagen, Denmark|The GeoipProvider does not support Street addresses.\ngeoip|74.200.247.59|POINT(-122.415604 37.748402)\ngeoip|::ffff:66.147.244.214|The GeoipProvider does not support IPv6 addresses.\n```\n\nBatch reverse geocoding is something like:\n\n```php\n<?php\n\n// ... $geocoder like the previous example ...\n// If you want to reverse one coordinate\ntry {\n    $results = $geotools->batch($geocoder)->reverse(\n        new \League\Geotools\Coordinate\Coordinate([2.307266, 48.823405])\n    )->parallel();\n} catch (\Exception $e) {\n    die($e->getMessage());\n}\n// Or if you want to reverse geocoding 3 coordinates\n$coordinates = [\n    new \League\Geotools\Coordinate\Coordinate([2.307266, 48.823405]),\n    new \League\Geotools\Coordinate\Coordinate([12.568337, 55.676097]),\n    new \League\Geotools\Coordinate\Coordinate('-74.005973 40.714353')),\n];\n$results = $geotools->batch($geocoder)->reverse($coordinates)->parallel();\n// ...\n```\n\nIf you want to batch it in serie, replace the method `parallel()` by `serie()`.\n\nTo optimize batch requests you need to register providers according to their **capabilities** and what you're\n**looking for** (geocode street addresses, geocode IPv4, geocode IPv6 or reverse geocoding),\nplease read more at the [Geocoder library doc](https://github.com/willdurand/Geocoder#freegeoipprovider).\n\n## Distance\n\nIt provides methods to compute the distance in *meter* (by default), *km*, *mi* or *ft* between two coordinates\nusing *flat* (most performant), *great circle*, *haversine* or *vincenty* (most accurate) algorithms.\n\nThose coordinates should be in the same ellipsoid.\n\n```php\n<?php\n\n$geotools = new \League\Geotools\Geotools();\n$coordA   = new \League\Geotools\Coordinate\Coordinate([48.8234055, 2.3072664]);\n$coordB   = new \League\Geotools\Coordinate\Coordinate([43.296482, 5.36978]);\n$distance = $geotools->distance()->setFrom($coordA)->setTo($coordB);\n\nprintf(""%s\n"",$distance->flat()); // 659166.50038742 (meters)\nprintf(""%s\n"",$distance->greatCircle()); // 659021.90812846\nprintf(""%s\n"",$distance->in('km')->haversine()); // 659.02190812846\nprintf(""%s\n"",$distance->in('mi')->vincenty()); // 409.05330679648\nprintf(""%s\n"",$distance->in('ft')->flat()); // 2162619.7519272\n```\n\n## Point\n\nIt provides methods to compute the initial and final *bearing* in degrees, the initial and final *cardinal direction*,\nthe *middle point* and the *destination point*. The middle and the destination points returns a\n`\League\Geotools\Coordinate\Coordinate` object with the same ellipsoid.\n\n```php\n<?php\n\n$geotools = new \League\Geotools\Geotools();\n$coordA   = new \League\Geotools\Coordinate\Coordinate([48.8234055, 2.3072664]);\n$coordB   = new \League\Geotools\Coordinate\Coordinate([43.296482, 5.36978]);\n$vertex    =  $geotools->vertex()->setFrom($coordA)->setTo($coordB);\n\nprintf(""%d\n"", $vertex->initialBearing()); // 157 (degrees)\nprintf(""%s\n"", $vertex->initialCardinal()); // SSE (SouthSouthEast)\nprintf(""%d\n"", $vertex->finalBearing()); // 160 (degrees)\nprintf(""%s\n"", $vertex->finalCardinal()); // SSE (SouthSouthEast)\n\n$middlePoint = $vertex->middle(); // \League\Geotools\Coordinate\Coordinate\nprintf(""%s\n"", $middlePoint->getLatitude()); // 46.070143125815\nprintf(""%s\n"", $middlePoint->getLongitude()); // 3.9152401085931\n\n$destinationPoint = $geotools->vertex()->setFrom($coordA)->destination(180, 200000); // \League\Geotools\Coordinate\Coordinate\nprintf(""%s\n"", $destinationPoint->getLatitude()); // 47.026774650075\nprintf(""%s\n"", $destinationPoint->getLongitude()); // 2.3072664\n```\n\n## Geohash\n\nIt provides methods to get the *geo hash* and its *bounding box's coordinates* (SouthWest & NorthEast)\nof a coordinate and the *coordinate* and its *bounding box's coordinates* (SouthWest & NorthEast) of a geo hash.\n\n```php\n<?php\n\n$geotools       = new \League\Geotools\Geotools();\n$coordToGeohash = new \League\Geotools\Coordinate\Coordinate('43.296482, 5.36978');\n\n// encoding\n$encoded = $geotools->geohash()->encode($coordToGeohash, 4); // 12 is the default length / precision\n// encoded\nprintf(""%s\n"", $encoded->getGeohash()); // spey\n// encoded bounding box\n$boundingBox = $encoded->getBoundingBox(); // array of \League\Geotools\Coordinate\CoordinateInterface\n$southWest   = $boundingBox[0];\n$northEast   = $boundingBox[1];\nprintf(""http://www.openstreetmap.org/?minlon=%s&minlat=%s&maxlon=%s&maxlat=%s&box=yes\n"",\n    $southWest->getLongitude(), $southWest->getLatitude(),\n    $northEast->getLongitude(), $northEast->getLatitude()\n); // http://www.openstreetmap.org/?minlon=5.2734375&minlat=43.2421875&maxlon=5.625&maxlat=43.41796875&box=yes\n\n// decoding\n$decoded = $geotools->geohash()->decode('spey61y');\n// decoded coordinate\nprintf(""%s\n"", $decoded->getCoordinate()->getLatitude()); // 43.296432495117\nprintf(""%s\n"", $decoded->getCoordinate()->getLongitude()); // 5.3702545166016\n// decoded bounding box\n$boundingBox = $decoded->getBoundingBox(); //array of \League\Geotools\Coordinate\CoordinateInterface\n$southWest   = $boundingBox[0];\n$northEast   = $boundingBox[1];\nprintf(""http://www.openstreetmap.org/?minlon=%s&minlat=%s&maxlon=%s&maxlat=%s&box=yes\n"",\n    $southWest->getLongitude(), $southWest->getLatitude(),\n    $northEast->getLongitude(), $northEast->getLatitude()\n); // http://www.openstreetmap.org/?minlon=5.3695678710938&minlat=43.295745849609&maxlon=5.3709411621094&maxlat=43.297119140625&box=yes\n```\n\nYou can also get information about neighbor points ([image](art/geohash_neighbor_points.png)).\n\n```php\n<?php\n\n$geotools = new \League\Geotools\Geotools();\n\n// decoding\n$decoded = $geotools->geohash()->decode('spey61y');\n// get neighbor geohash\nprintf(""%s\n"", $decoded->getNeighbor(\League\Geotools\Geohash\Geohash::DIRECTION_NORTH)); // spey64n\nprintf(""%s\n"", $decoded->getNeighbor(\League\Geotools\Geohash\Geohash::DIRECTION_SOUTH_EAST)); // spey61x\n// get all neighbor geohashes\nprint_r($decoded->getNeighbors(true));\n/**\n * Array\n * (\n *     [north] => spey64n\n *     [south] => spey61w\n *     [west] => spey61v\n *     [east] => spey61z\n *     [north_west] => spey64j\n *     [north_east] => spey64p\n *     [south_west] => spey61t\n *     [south_east] => spey61x\n * )\n */\n```\n\n## 10:10\n\nRepresent a location with 10m accuracy using a 10 character code that includes features to prevent errors in\nentering the code. Read more about the algorithm [here](http://blog.jgc.org/2006/07/simple-code-for-entering-latitude-and.html).\n\n```php\n<?php\n\n$tenten = new \League\Geotools\Tests\Geohash\TenTen;\n$tenten->encode(new Coordinate([51.09559, 1.12207])); // MEQ N6G 7NY5\n```\n\n## Vertex\n\nRepresents a segment with a direction.\nYou can find if two vertexes are on the same line.\n\n```php\n<?php\n	$vertexA->setFrom(48.8234055);\n	$vertexA->setTo(2.3072664);\n\n	$vertexB->setFrom(48.8234055);\n	$vertexB->setTo(2.3072664);\n	$vertexA->isOnSameLine($vertexB);\n```\n\n## Polygon\n\nIt helps you to know if a point (coordinate) is in a Polygon or on the Polygon's boundaries and if this in on\na Polygon's vertex.\n\nFirst you need to create the polygon, you can provide:\n- an array of arrays\n- an array of `Coordinate`\n- a `CoordinateCollection`\n\n```php\n<?php\n\n$polygon = new \League\Geotools\Polygon\Polygon([\n    [48.9675969, 1.7440796],\n    [48.4711003, 2.5268555],\n    [48.9279131, 3.1448364],\n    [49.3895245, 2.6119995],\n]);\n\n$polygon->setPrecision(5); // set the comparision precision\n$polygon->pointInPolygon(new \League\Geotools\Coordinate\Coordinate([49.1785607, 2.4444580])); // true\n$polygon->pointInPolygon(new \League\Geotools\Coordinate\Coordinate([49.1785607, 5])); // false\n$polygon->pointOnBoundary(new \League\Geotools\Coordinate\Coordinate([48.7193486, 2.13546755])); // true\n$polygon->pointOnBoundary(new \League\Geotools\Coordinate\Coordinate([47.1587188, 2.87841795])); // false\n$polygon->pointOnVertex(new \League\Geotools\Coordinate\Coordinate([48.4711003, 2.5268555])); // true\n$polygon->pointOnVertex(new \League\Geotools\Coordinate\Coordinate([49.1785607, 2.4444580])); // false\n$polygon->getBoundingBox(); // return the BoundingBox object\n```\n\n## CLI\n\nIt provides command lines to compute methods provided by **Distance**, **Point**, **Geohash** and **Convert** classes.\nThanks to the [Symfony Console Component](https://github.com/symfony/Console).\n\n```bash\n$ php geotools list // list of available commands\n$ php geotools help distance:flat // get the help\n$ php geotools distance:flat ""40° 26.7717, -79° 56.93172"" ""30°16′57″N 029°48′32″W"" // 4690203.1048522\n$ php geotools distance:haversine ""35,45"" ""45,35"" --ft  // 4593030.9787593\n$ php geotools distance:vincenty ""35,45"" ""45,35"" --km  // 1398.4080717661\n$ php geotools d:v ""35,45"" ""45,35"" --km --ellipsoid=WGS60 // 1398.4145201642\n$ php geotools point:initial-cardinal ""40:26:46.302N 079:56:55.903W"" ""43.296482, 5.36978"" // NE (NordEast)\n$ php geotools point:final-cardinal ""40:26:46.302N 079:56:55.903W"" ""43.296482, 5.36978"" // ESE (EastSouthEast)\n$ php geotools point:destination ""40° 26.7717, -79° 56.93172"" 25 10000 // 40.527599285543, -79.898914904538\n$ php geotools p:d ""40° 26.7717, -79° 56.93172"" 25 10000 --ellipsoid=GRS_1980 // 40.527599272782, -79.898914912379\n$ php geotools geohash:encode ""40° 26.7717, -79° 56.93172"" --length=3 // dpp\n$ php geotools convert:dm ""40.446195, -79.948862"" --format=""%P%D°%N %p%d°%n"" // 40°26.7717 -79°56.93172\n$ php geotools convert:dms ""40.446195, -79.948862"" --format=""%P%D:%M:%S, %p%d:%m:%s"" // 40:26:46, -79:56:56\n$ php geotools convert:utm ""60.3912628, 5.3220544"" // 32V 297351 6700644\n$ php geotools c:u ""60.3912628, 5.3220544"" --ellipsoid=AIRY // 32V 297371 6700131\n...\n```\n\nCompute street addresses, IPv4s or IPv6s geocoding and reverse geocoding right in your console.\n\nIt's possible to define and precise your request through these options:\n* `--provider`: `bing_maps`, `yahoo`, `maxmind`... `google_maps` is the default one. See the full list\n[here](https://github.com/willdurand/Geocoder#providers).\n* `--raw`: the result output in RAW format, shows Adapter, Provider and Arguments if any.\n* `--json`: the result output in JSON string format.\n* `--args`: this option accepts multiple values (e.g. --args=""API_KEY"" --args=""LOCALE"") if your provider needs or\ncan have arguments.\n* `--dumper`: this option is available for geocoding, `gpx`, `geojson`, `kml`, `wkb` and `wkt` by default.\nRead more [here](https://github.com/willdurand/Geocoder#dumpers).\n* `--format`: this option is available for reverse geocoding, see the mapping\n[here](https://github.com/willdurand/Geocoder#formatter).\n\n```bash\n$ php geotools help geocoder:geocode // get the help\n$ php geotools geocoder:geocode ""Copenhagen, Denmark"" // 55.6760968, 12.5683371\n$ php geotools geocoder:geocode ""74.200.247.59"" --provider=""free_geo_ip"" // 37.7484, -122.4156\n$ php geotools geocoder:geocode Paris --args=""fr_FR"" --args=""France"" --args=""true"" // 48.856614, 2.3522219\n$ php geotools geocoder:geocode Paris --dumper=wkt // POINT(2.352222 48.856614)\n...\n$ php geotools geocoder:reverse ""48.8631507, 2.388911"" // Avenue Gambetta 10, 75020 Paris\n$ php geotools geocoder:reverse ""48.8631507, 2.388911"" --format=""%L, %A1, %C"" // Paris, Île-De-France, France\n$ php geotools geocoder:reverse ""48.8631507, 2.388911"" --format=""%L, %A1, %C"" --provider=""openstreetmap""\n// Paris, Île-De-France, France Métropolitaine\n...\n$ php geotools geocoder:geocode ""Tagensvej 47, Copenhagen"" --raw --args=da_DK --args=Denmark\n```\n\nThe last command will show an output like this:\n\n```\nHttpClient:    \Http\Client\Curl\Client\nProvider:      \Geocoder\Provider\GoogleMaps\nCache:         \League\Geotools\Cache\Redis\nArguments:     da_DK,Denmark\n---\nLatitude:      55.699953\nLongitude:     12.552736\nBounds\n - South: 55.699953\n - West:  12.552736\n - North: 55.699953\n - East:  12.552736\nStreet Number: 47\nStreet Name:   Tagensvej\nZipcode:       2200\nCity:          Copenhagen\nCity District: København N\nCounty:        København\nCounty Code:   KØBENHAVN\nRegion:        Capital Region Of Denmark\nRegion Code:   CAPITAL REGION OF DENMARK\nCountry:       Denmark\nCountry Code:  DK\nTimezone:\n```\n\nIntegration with Frameworks\n---------------------------\n\n* [Laravel 4 & 5](https://github.com/toin0u/Geotools-laravel)\n* [Silex](https://github.com/toin0u/Geotools-silex)\n* ...\n\n\nUnit Tests\n----------\n\nTo run unit tests, you'll need the `cURL` extension and a set of dependencies, you can install them using Composer:\n\n```bash\n$ php composer.phar install --dev\n```\n\nOnce installed, just launch the following command:\n\n```bash\n$ phpunit --coverage-text\n```\n\n\nCredits\n-------\n\n* [Antoine Corcy](https://twitter.com/toin0u)\n* [Pascal Borreli](https://twitter.com/pborreli)\n* [Phil Sturgeon](https://twitter.com/philsturgeon)\n* [Gabriel Bull](mailto:me@gabrielbull.com)\n* [All contributors](https://github.com/toin0u/Geotools/contributors)\n\n\nAcknowledgments\n---------------\n* [Geocoder](https://github.com/willdurand/Geocoder) -\n[MIT](https://raw.github.com/willdurand/Geocoder/master/LICENSE)\n* [ReactPHP](https://github.com/reactphp/) -\n[MIT](https://raw.github.com/reactphp/react/master/LICENSE)\n* [Symfony Console Component](https://github.com/symfony/Console) -\n[MIT](https://raw.github.com/symfony/Console/master/LICENSE)\n* [Symfony Serializer Component](https://github.com/symfony/Serializer) -\n[MIT](https://raw.github.com/symfony/Serializer/master/LICENSE)\n* [PHP client library for Redis](https://github.com/nrk/predis) -\n[MIT](https://raw.github.com/nrk/predis/master/LICENSE)\n* [Geokit](https://github.com/jsor/Geokit),\n[Geotools-for-CodeIgniter](https://github.com/weejames/Geotools-for-CodeIgniter),\n[geotools-php](https://github.com/jillesvangurp/geotools-php) ...\n\n\nChangelog\n---------\n\n[See the changelog file](https://github.com/thephpleague/geotools/blob/master/CHANGELOG.md)\n\nContributing\n------------\n\nPlease see [CONTRIBUTING](https://github.com/thephpleague/geotools/blob/master/CONTRIBUTING.md) for details.\n\nSupport\n-------\n\nBugs and feature request are tracked on [GitHub](https://github.com/thephpleague/geotools/issues)\n\nContributor Code of Conduct\n---------------------------\n\nAs contributors and maintainers of this project, we pledge to respect all people\nwho contribute through reporting issues, posting feature requests, updating\ndocumentation, submitting pull requests or patches, and other activities.\n\nWe are committed to making participation in this project a harassment-free\nexperience for everyone, regardless of level of experience, gender, gender\nidentity and expression, sexual orientation, disability, personal appearance,\nbody size, race, age, or religion.\n\nExamples of unacceptable behavior by participants include the use of sexual\nlanguage or imagery, derogatory comments or personal attacks, trolling, public\nor private harassment, insults, or other unprofessional conduct.\n\nProject maintainers have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct. Project maintainers who do not follow the\nCode of Conduct may be removed from the project team.\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by opening an issue or contacting one or more of the project\nmaintainers.\n\nThis Code of Conduct is adapted from the [Contributor\nCovenant](https://contributor-covenant.org), version 1.0.0, available at\n[https://contributor-covenant.org/version/1/0/0/](https://contributor-covenant.org/version/1/0/0/)\n\nLicense\n-------\n\nGeotools is released under the MIT License. See the bundled\n[LICENSE](https://github.com/thephpleague/geotools/blob/master/LICENSE) file for details.\n\n[![Bitdeli Badge](https://d2weczhvl823v0.cloudfront.net/toin0u/Geotools/trend.png)](https://bitdeli.com/free ""Bitdeli Badge"")\n",1362,geometry,PHP,1,PHP,,,,,,,,,,,,,,,,,,,,,,,,,,,,96,19,76,1,2,45,0,1741,123,93,71,22,28fd647ab84d590d79b65036f8e8e7701357a866,Removed useless pictures,2024-03-14T22:29:51Z,Surfoo,surfooo@gmail.com,Surfoo,Release 1.2.0,## What's Changed\r\n* PHP 8.2 CI support by @phpfui in https://github.com/thephpleague/geotools/pull/184\r\n* Add support for Symfony 7/Laravel 11 by @dwightwatson in https://github.com/thephpleague/geotools/pull/188\r\n* OpenStreetMap typo fixed #187. by @Surfoo in https://github.com/thephpleague/geotools/pull/189\r\n* Ability to receive codes of neighboring points by @OleksandrWebLab in https://github.com/thephpleague/geotools/pull/185\r\n\r\n## New Contributors\r\n* @dwightwatson made their first contribution in https://github.com/thephpleague/geotools/pull/188\r\n* @OleksandrWebLab made their first contribution in https://github.com/thephpleague/geotools/pull/185\r\n\r\n**Full Changelog**: https://github.com/thephpleague/geotools/compare/1.1.0...1.2.0,01.02.2000,Surfoo,,Surfoo,MIT License,geotools,thephpleague,15,php,geolocation,geotools,geo,geometry,,,,,,,,,,,,,,,,/thephpleague/geotools,33,47,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/TheFuseLab/VL.Fuse,https://github.com/TheFuseLab/VL.Fuse,0,,,0,0,0,0,0,0,1,1,0,0,0,"A library for visually programming on the GPU, built to enable rapid workflows and modular approaches to accelerated graphics, logic and computation.","# VL.Fuse\n\n[![Nuget (with prereleases)](https://img.shields.io/nuget/vpre/VL.Fuse?logo=nuget&style=flat-square)](https://www.nuget.org/packages/VL.Fuse/) [![Matrix](https://img.shields.io/matrix/VL.Fuse:matrix.org?label=chat%20on%20element&logo=element&style=flat-square)](https://app.element.io/#/room/#VL.Fuse:matrix.org) [![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg?style=flat-square)](https://opensource.org/licenses/MIT)\n\n![](/documentation/fuse_banner.png)\n\nAn open source library for visually programming on the GPU, built to enable rapid workflows and modular approaches to accelerated graphics, logic and computation. \n\nIt is built for use in [vvvv gamma](https://visualprogramming.net/) and follows its ""always runtime"" model allowing for fast design and programming work with no build or compile process in between you and your results.\n\nRendering uses the [Stride 3D Engine](https://stride3d.net/) integration for vvvv, allowing for game engine style PBR materials, lighting & post effects all without having to write a single script.\n\nAs a community supported effort we make it to use in real world projects and aim to share the fruits of its research & development with our peers in the creative coding and generative design communities.\n\nHere is the preview release presentation: [Release Presentation: FUSE](https://youtu.be/4xDShgbKTsQ?t=920)\n\n## Installing\n\nNew to vvvv? We have a short video tutorial that will show you how to install vvvv and VL.Fuse : [How to install vvvv and VL.Fuse](https://youtu.be/25sk7_NaEgM). You can then decide to install the stable or the preview version of Fuse. For both, you'll have to [open vvvv's command line](https://thegraybook.vvvv.org/reference/hde/managing-nugets.html) and type one of the following install commands :\n\n### Current stable\n\nTo install the current stable version, type\n\n```\nnuget install VL.Fuse\n```\n\nThen press <kbd>Enter</kbd> and wait for the end of the installation process. \n\n### Current preview\n\nTo install the current preview version, type\n\n```\nnuget install VL.Fuse -pre\n```\n\nThen press <kbd>Enter</kbd> and wait for the end of the installation process. This will give you access to the latest features but bewere, there might be bugs!\n\nPlease note that this version is only compatible with vvvv 5.0 and won't work with earlier version.\n\n## Getting started\n\nWith vvvv gamma open, press <kbd>F1</kbd> and look for `Fuse` in the help browser. You'll see some explanations and howtos to get you started\n\n## Supporters\n### Development Partner\n* Refik Anadol Studio     https://refikanadolstudio.com/\n### Platinum Sponsor    \n* Marshmallow Laser Feast https://www.marshmallowlaserfeast.com/\n### Gold Sponsor\n* Takuma Nakata     	    https://www.takumanakata.com/\n### Sponsors\n* M-Box             	    https://www.m-box.de/\n* Quayola         		    https://quayola.com/\n* Schnellebuntebilder     http://schnellebuntebilder.de/\n* Wirmachenbunt     	    https://wirmachenbunt.de/\n* CloneProduction			    https://cloneproduction.net\n### Super Backers\n* MESO             		    https://meso.design/en\n* Analog Native           https://analognative.net/\n* Naut             		    https://naut.ch/\n* Noir                    https://www.instagram.com/random_noir/\n* Arístides García        https://www.aristidesgarcia.de/\n\n### Backers\nTally Yalesford, Aleksei Lizunov, Shaul Tzemach, Amairu, Studio de Maan, Studio Brüll, Sebescudie, Urbandrone, C Nisidis, David Bührer, Bryan Mischling, Lorenz Potthast, Minoru Ito, Toby Knyvett, Noir, M4d, Circuitb, Karafiat, Konstantin Semilakovs, Martin Bvoerhof, Didi Bruckmayr, Randall Vásquez, Christine Mayerhofer, Tin Tran, Chris Plant, Metarchetype, Motzi, Achim Stromberger, Robe Santoro, Andreu Lucio, chk, Amir Bastan, ExperiensS, Andres Alvarez\n",256,graphics,C#,1,C#,,,,,,,,,,,,,,,,,,,,,,,,,,,,26,2,23,1,20,16,0,262754,19,95,43,52,c5acc1ef8091c124c6a5720a50ab497dd8062e26,add fluid updates,2024-07-18T17:36:57Z,Christian Riekoff,christian@riekoff.com,texone,,,,,,,MIT License,VL.Fuse,TheFuseLab,8,shader,gpu,procedural,generative-art,vvvv,vl,stride,graphics,,,,,,,,,,,,,/TheFuseLab/VL.Fuse,11,26,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/texmacs/texmacs,https://github.com/texmacs/texmacs,0,,,0,0,0,0,0,0,1,0,0,0,0,"Source Code of GNU TeXmacs, Developers Guide ==>","# GNU TeXmacs\n[![Join the chat at https://gitter.im/texmacs/Lobby](https://badges.gitter.im/texmacs/Lobby.svg)](https://gitter.im/texmacs/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n[GNU TeXmacs](https://texmacs.org) is a free wysiwyw (what you see is what you want) editing platform with special features for scientists. The software aims to provide a unified and user friendly framework for editing structured documents with different types of content (text, graphics, mathematics, interactive content, etc.). The rendering engine uses high-quality typesetting algorithms so as to produce professionally looking documents, which can either be printed out or presented from a laptop.\n\nThe software includes a text editor with support for mathematical formulas, a small technical picture editor and a tool for making presentations from a laptop. Moreover, TeXmacs can be used as an interface for many external systems for computer algebra, numerical analysis, statistics, etc. New presentation styles can be written by the user and new features can be added to the editor using the Scheme extension language. A native spreadsheet and tools for collaborative authoring are planned for later.\n\nTeXmacs runs on all major Unix platforms and Windows. Documents can be saved in TeXmacs, Xml or Scheme format and printed as Postscript or Pdf files. Converters exist for TeX/LaTeX and Html/Mathml. \n\n## Documentation\nGNU TeXmacs is self-documented. You may browse the manual in the `Help` menu or browse the online [one](https://www.texmacs.org/tmweb/manual/web-manual.en.html).\n\nFor developer, see [this](./COMPILE) to compile the project.\n\n## Contributing\nPlease report any [new bugs](https://www.texmacs.org/tmweb/contact/bugs.en.html) and [suggestions](https://www.texmacs.org/tmweb/contact/wishes.en.html) to us. It is also possible to [subscribe](https://www.texmacs.org/tmweb/help/tmusers.en.html) to the <texmacs-users@texmacs.org> mailing list in order to get or give help from or to other TeXmacs users.\n\nYou may contribute patches for TeXmacs using the [patch manager](http://savannah.gnu.org/patch/?group=texmacs) on Savannah or using the [pull request](https://github.com/texmacs/texmacs/pulls) on Github. Since we are using SVN on Savannah, PRs won't be directly accepted on Github. We will `git apply` the patch into SVN repo if the PR is accepted. And we will close the PR and change the title to `[SVN] xxx` after applying the PR.\n",587,mathematics,Tcl,27,CMake,Makefile,Tcl,Scheme,C++,TypeScript,C,Awk,QMake,PostScript,Prolog,TeX,Python,Emacs Lisp,Shell,M4,Roff,Perl,Common Lisp,MATLAB,M,R,Scilab,Objective-C,Objective-C++,CSS,JetBrains MPS,,78,61,1,16,25,7,0,77694,70,0,0,0,a3917c79ee65e41b85a1ff702f0778111f048509,forward CPPFLAGS and LDFLAGS to embedded guile configure options,2024-07-12T17:47:59Z,Grégoire Lecerf,lecerf,,GNU TeXmacs v2.1.4,"+ Linux: http://texmacs.org/tmweb/download/linux.en.html\r\n+ macOS: http://texmacs.org/tmweb/download/macosx.en.html\r\n+ Windows: http://texmacs.org/tmweb/download/windows.en.html\r\n\r\nWARNING: The tagged git source code might not be consist with the actual source code. To download the official source code, see http://texmacs.org/tmweb/download/sources.en.html",v2.1.4,Darcy Shen,,da-liii,GNU General Public License v3.0,texmacs,texmacs,21,texmacs,gnu,cpp,scheme,mathematics,qt,,,,,,,,,,,,,,,/texmacs/texmacs,77,29,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/Tencent/tgfx,https://github.com/Tencent/tgfx,0,,,0,0,0,0,0,0,1,1,0,0,0,"A lightweight 2D graphics library for rendering texts, geometries, and images with high-performance APIs that work across various platforms.","<img src=""resources/readme/TGFX.jpg"" alt=""TGFX Logo"" width=""992""/>\n\n[![license](https://img.shields.io/badge/license-BSD--3--Clause-blue)](https://github.com/Tencent/tgfx/blob/master/LICENSE.txt)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/Tencent/tgfx/pulls)\n[![codecov](https://codecov.io/gh/Tencent/tgfx/branch/main/graph/badge.svg)](https://codecov.io/gh/Tencent/tgfx)\n[![autotest](https://github.com/Tencent/tgfx/actions/workflows/autotest.yml/badge.svg?branch=main)](https://github.com/Tencent/tgfx/actions/workflows/autotest.yml)\n[![build](https://github.com/Tencent/tgfx/actions/workflows/build.yml/badge.svg?branch=main)](https://github.com/Tencent/tgfx/actions/workflows/build.yml)\n[![GitHub release (latest SemVer)](https://img.shields.io/github/v/release/Tencent/tgfx)](https://github.com/Tencent/tgfx/releases)\n\n## Introduction\n\nTGFX (Tencent Graphics) is a lightweight 2D graphics library designed for rendering texts,\ngeometries, and images. It provides high-performance APIs that work across a variety of GPU hardware\nand software platforms, including iOS, Android, macOS, Windows, Linux, Web, and more. TGFX was\noriginally designed to serve as the default graphics engine for the [PAG](https://pag.art) project\nstarting from version 4.0. Its main objective is to offer a compelling alternative to the Skia graphics\nlibrary while maintaining a much smaller binary size. Over time, it has found its way into many other\nproducts, such as [Hippy](https://github.com/Tencent/Hippy), [Tencent Docs](https://docs.qq.com) and\nvarious video-editing apps.\n\n## Platform Support\n\n- iOS 9.0 or later\n- Android 4.4 or later\n- macOS 10.15 or later\n- Windows 7.0 or later\n- Chrome 69.0 or later (Web)\n- Safari 11.3 or later (Web)\n\n## Backing Renderers\n\n| Vector Backend |  GPU Backend   |      Target Platforms        |    Status     |\n|:--------------:|:--------------:|:----------------------------:|:-------------:|\n|    FreeType    |  OpenGL        |  All                         |   complete    |\n|  CoreGraphics  |  OpenGL        |  iOS, macOS                  |   complete    |\n|    Canvas2D    |  WebGL         |  Web                         |   complete    |\n|  CoreGraphics  |  Metal         |  iOS, macOS                  |  in progress  |\n|    FreeType    |  Vulkan        |  Android, Linux              |    planned    |\n\n\n## Branch Management\n\n- The `main` branch is our active developing branch which contains the latest features and bugfixes.\n- The branches under `release/` are our stable milestone branches which are fully tested. We will\n  periodically cut a `release/{version}` branch from the `main` branch. After one `release/{version}`\n  branch is cut, only high-priority fixes are checked into it.\n\n## Build Prerequisites\n\nTGFX utilizes the **C++17** features for development. Below are the minimum tools needed for building tgfx on different platforms:\n\n- Xcode 11.0+\n- GCC 9.0+\n- Visual Studio 2019+\n- NodeJS 14.14.0+\n- Ninja 1.9.0+\n- CMake 3.13.0+\n- QT 5.13.0+\n- NDK 19.2+ (**19.2.5345600 recommended**)\n- Emscripten 3.1.20+ (**3.1.20 recommended**)\n\n\nPlease pay attention to the following additional notices:\n\n- Make sure you have installed at least the **[Desktop development with C++]** and **[Universal Windows Platform development]** components for VS2019.\n- It is **highly recommended** to use the **latest version of CMake**, Numerous outdated versions of CMake may carry various bugs across different platforms.\n\n## Dependencies\n\nTGFX uses [**depsync**](https://github.com/domchen/depsync) tool to manage third-party dependencies.\n\n**For macOS platform：**\n\nRun the script in the root of the project:\n\n```\n./sync_deps.sh\n```\n\nThis script will automatically install the necessary tools and synchronize all third-party repositories.\n\n**For other platforms：**\n\nFirst, make sure you have installed the latest version of node.js (You may need to restart your\ncomputer after this step). And then run the following command to install depsync tool:\n\n```\nnpm install -g depsync\n```\n\nAnd then run `depsync` in the root directory of the project.\n\n```\ndepsync\n```\n\nGit account and password may be required during synchronizing. Please make sure you have enabled the\n`git-credential-store` so that `CMakeList.txt` can trigger synchronizing automatically next time.\n\n\n## Getting Started\n\nWe offer concise demos for different platforms, demonstrating how to integrate the tgfx library into\nyour project. Once you've built the project, you'll find a straightforward app rendering various test\ncases from the `drawers/` directory. These test cases include rendering shapes, images, and simple \ntexts. With a simple touch on the screen, you can switch between different test cases. If you are \nlooking for further guidance on API usage, consider exploring the test cases found in the `test/` \ndirectories. They may provide valuable insights and assistance.\n\nBefore you begin building the demo projects, please make sure to carefully follow the instructions\nprovided above in the [**Build Prerequisites**](#build-prerequisites) and \n[**Dependencies**](#dependencies) sections. They will guide you through the \nnecessary steps to configure your development environment.\n\n### Android\n\nThe android demo project requires the **Android NDK**. We recommend using the **19.2.5345600** \nversion, which has been fully tested with the tgfx library. If you open the project with Android\nStudio, it will automatically download the NDK during Gradle synchronization. Alternatively, you\ncan download it from the [NDK Downloads](https://developer.android.com/ndk/downloads) page.\n\nIf you choose to manually download the Android NDK, please extract it to the default location. \nOn macOS, this would be：\n\n```\n/Users/yourname/Library/Android/sdk/ndk/19.2.5345600\n```\n\nOn Windows, it would be：\n\n```\nC:\Users\yourname\AppData\Local\Android\Sdk\ndk\19.2.5345600\n```\n\nAlternatively, you can set one of the following environment variables for tgfx to locate the NDK: \n\n```\n[""ANDROID_NDK_HOME"", ""ANDROID_NDK_ROOT"", ""ANDROID_NDK"", ""NDK_HOME"", ""NDK_ROOT"", ""NDK_PATH""]\n```\n\nTo get started, open the `android/` directory in Android Studio, and you'll be all set! If you \nencounter any issues during Gradle synchronization, please ensure that you haven't accidentally \nclicked on the pop-up hints for Gradle version upgrades. If you have, undo the changes you made to \nthe project and attempt synchronization again. If the issue is related to your IDE configuration,\nplease search for a solution on Google. However, if you believe the problem is associated with the \nproject configuration, you can open an [Issue](https://github.com/Tencent/tgfx/issues/new/choose)\nto address it.\n\n### iOS\n\nRun the following command in the `ios/` directory or double-click on it:\n\n```\n./gen_ios\n```\n\nThis will generate an XCode project for iPhone devices. If you prefer to generate a project for \nthe simulators, use the following command instead:\n\n```\n./gen_simulator\n```\n\nThis will generate a simulator project for the native architecture, for example, `arm64` for\nApple Silicon Macs and `x64` for Intel Macs. If you want to generate a project for the specific\narchitecture, you can use the `-a` option:\n\n```\n./gen_simulator -a x64\n```\n\nAdditionally, you can pass cmake options using the `-D` option. For instance, if you want to \ngenerate a project with webp encoding support, please run the following command:\n\n```\n./gen_ios -DTGFX_USE_WEBP_ENCODE=ON\n```\n\nFinally, open XCode and launch the `ios/Hello2D.xcworkspace` to build and run the demo project.\n\n### macOS\n\n\nRun the following command in the `mac/` directory or double-click on it:\n\n```\n./gen_mac\n```\n\nThis will generate a project for the native architecture, for example, `arm64` for Apple Silicon\nMacs and `x64` for Intel Macs. If you want to generate a project for the specific architecture, you\ncan use the `-a` option, for example:\n\n```\n./gen_mac -a x64\n```\n\nAdditionally, you can pass cmake options using the `-D` option. For example, if you want to generate\na project with freetype support, please run the following command:\n\n```\n./gen_mac -DTGFX_USE_FREETYPE=ON\n```\n\nAt last, launch XCode and open the `mac/Hello2D.xcworkspace`. You'll be ready to go!\n\n### Web\n\nThe web demo project requires the **Emscripten SDK**. You can download and install\nit from the [official website](https://emscripten.org/). We recommend using the **3.1.20** version,\nwhich has been fully tested with the tgfx library. If you are on macOS, you can also install it\nusing the following script:\n\n```\nweb/script/install-emscripten.sh\n```\n\nTo begin, navigate to the `web/` directory and execute the following command to install the \nnecessary node modules:\n\n```\nnpm install\n```\n\nAnd then run the following command in the `web/` directory to build the demo project:\n\n```\nnpm run build\n```\n\nThis will generate `hello2d.js` and `hello2d.wasm` files into the `web/demo/wasm` directory. \nAfterward, you can start an HTTP server by running the following command:\n\n```\nnpm run server\n```\n\nThis will open [http://localhost:8081/web/demo/index.html](http://localhost:8081/web/demo/index.html)\nin your default browser. You can also open it manually to see the demo.\n\nTo debug the C++ code, ensure that you have installed the browser plugin:\n[**C/C++ DevTools Support (DWARF)**](https://chromewebstore.google.com/detail/cc++-devtools-support-dwa/pdcpmagijalfljmkmjngeonclgbbannb).\nNext, open Chrome DevTools and navigate to Settings > Experiments. Check the option\n**WebAssembly Debugging: Enable DWARF support** to enable SourceMap support.\n\nAnd then, replace the previous build command with the following:\n\n```\nnpm run build:debug\n```\n\nWith these steps completed, you will be able to debug C++ files directly within Chrome DevTools.\n\nTo build the demo project in CLion, please Open the `Settings` panel in CLion and go to \n`Build, Execution, Deployment` > `CMake`. Create a new build target. And then set the `CMake options`\nto the following value:\n\n```\nDCMAKE_TOOLCHAIN_FILE=""path/to/emscripten/emscripten/version/cmake/Modules/Platform/Emscripten.cmake""\n```\n\nOnce you have created the build target, make sure to adjust the `Configurations` accordingly to \nalign with the newly created build target. By doing so, you will gain the ability to build the tgfx \nlibrary in CLion.\n\nAdditionally, please note that when using `ESModule` for your project, it is necessary to manually \npack the generated `.wasm` file into the final web program. This is because common packing tools \nusually ignore the `.wasm` file. Moreover, remember to upload the `.wasm` file to a server, enabling \nusers to access it from the network.\n\n### Linux\n\nWhen running Linux, the system usually lacks GPU hardware support. Therefore, we utilize the\n[**SwiftShader**](https://github.com/google/swiftshader) library to emulate the GPU rendering \nenvironment. Since SwiftShader relies on certain X11 header files, it is necessary to install the \nfollowing packages before building the demo project:\n\n```\nyum install libX11-devel --nogpg\n```\n\nNext, execute the following commands in the linux/ directory:\n\n```\ncmake -B ./build -DCMAKE_BUILD_TYPE=Release\ncmake --build ./build -- -j 12\n```\n\nYou will get the demo executable file in the build directory. You also have the option of opening\nthe `linux/` directory in CLion and building the demo project directly in the IDE.\n\n### Windows\n\nTo start, open the `win/` directory in CLion.  Next, open the `File->Setting` panel and navigate to \n`Build, Execution, Deployment->ToolChains`. Set the toolchain of CLion to `Visual Studio` with either\n`amd64` (Recommended) or `x86` architecture. Once done, you'll be able to build and run the `Hello2D`\ntarget.\n\nIf you prefer to use the VS Studio IDE, you can open the `x64 Native Tools Command Prompt for VS 2019` \nand execute the following command in the `win/` directory:\n\n```\ncmake -G ""Visual Studio 16 2019"" -A x64 -B ./build-x64\n```\n\nThis will generate a project for the `x64` architecture. If you want to generate a project for the\n`x86` architecture, open the `x86 Native Tools Command Prompt for VS 2019` and run the following \ncommand instead:\n\n```\ncmake -G ""Visual Studio 16 2019"" -A Win32 -B ./build-x86\n```\n\nFinally, go to the `build-x64/` or `build-x86/` directory and open the `Hello2D.sln` file. You'll be\nready to go!\n\n### QT\n\nFor **macOS** users, just open the `qt/` directory in CLion. Then, navigate to the `qt/QTCMAKE.cfg` \nfile to modify the QT path with your local QT installation path. Once done, you can proceed to build \nand run the `Hello2D` target.\n\nFor **Windows** users, ensure that the ToolChain of CLion is set to `Visual Studio` with `amd64`\narchitecture. Then, navigate to the `qt/` folder in CLion and find the `qt/QTCMAKE.cfg` file.\nModify the QT path to match your local QT installation path. Afterward, access the configuration \npanel of the `Hello2D` target in CLion. Enter the local QT DLL library path in the \n`Environment Variables` row, e.g., `PATH=C:\Qt\6.6.1\msvc2019_64\bin`. Finally, you're ready to \nbuild and run the `Hello2D` target.\n\n## Build Library\n\nAside from directly integrating the source code of tgfx into your project, you also have the option\nof linking with the precompiled libraries. TGFX utilizes the [**vendor_tools**](https://github.com/libpag/vendor_tools)\nproject as its build system, enabling a unified approach to build the tgfx library across all platforms.\n\nTo quickly get started, execute the following command in the root directory:\n\n```\nnode build_tgfx\n```\n\nThis command will build the release version of the tgfx library for the native platform. After the\nexecution, you will find the compiled tgfx libraries in the `out/release` directory. If you wish to\ntarget a specific platform, please use the `-p [--platform]` option. The supported platform names\nare as follows: `win`, `mac`, `ios`, `linux`, `android`, `web`.\n\n```\nnode build_tgfx -p ios\n```\n\nWhen developing for apple platforms, you have the convenient `-x [--xcframework]` option available. \nThis option enables you to effortlessly create xcframeworks:\n\n```\nnode build_tgfx -p mac -x\n```\n\nAfter the execution, you will find the `tgfx.xcframework` in the `out/release/mac` directory.\n\nAdditionally, you can pass cmake options using the `-D` prefix. For example, if you want to build\ntgfx with the freetype option enabled, please run the following command:\n\n```\nnode build_tgfx -DTGFX_USE_FREETYPE=ON\n```\n\nTo access more details and options, execute the command along with the `-h [--help]` option:\n\n```\nnode build_tgfx -h\n```\n\n\n## Contribution\n\nIf you have any ideas or suggestions to improve tgfx, welcome to open\na [discussion](https://github.com/Tencent/tgfx/discussions/new/choose)\n/ [issue](https://github.com/Tencent/tgfx/issues/new/choose)\n/ [pull request](https://github.com/Tencent/tgfx/pulls). Before making a pull request or issue,\nplease make sure to read [Contributing Guide](./CONTRIBUTING.md).\n\n## Support Us\n\nIf you find tgfx is helpful, please give us a **Star**. We sincerely appreciate your support :)\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Tencent/tgfx&type=Date)](https://star-history.com/#Tencent/tgfx&Date)\n\n## License\n\nTGFX is licensed under the [BSD-3-Clause License](./LICENSE.txt)\n\n",1046,graphics,C++,12,CMake,Python,Shell,C++,Objective-C,Objective-C++,CSS,HTML,TypeScript,JavaScript,Kotlin,QML,,,,,,,,,,,,,,,,,186,8,178,0,3,12,0,2612,62,13,13,0,acc5271f9b4c21b0fb7d4b60929c9d78163e0363,Add support for printing logs on the OHOS platform. (#220),2024-07-19T06:57:00Z,Dom Chen,dom@idom.me,domchen,v1.1.2,## What's Changed\r\n\r\n* Fix the out-of-bounds memory access issue in the FTMask class.,v1.1.2,Dom Chen,,domchen,Other,tgfx,Tencent,5,2d,graphics,tgfx,rendering,gpu,,,,,,,,,,,,,,,,/Tencent/tgfx,5,17,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/su2code/SU2,https://github.com/su2code/SU2,1,,,1,1,1,1,0,0,0,0,0,0,1,SU2: An Open-Source Suite for Multiphysics Simulation and Design,"<p align=""center"">\n<img width=""250"" height=""154"" src=""Docs/logoSU2small.png"">\n</p>\n\n\n# SU2 (ver. 8.0.1 ""Harrier""): The Open-Source CFD Code\n\nComputational analysis tools have revolutionized the way we design engineering systems, but most established codes are proprietary, unavailable, or prohibitively expensive for many users. The SU2 team is changing this, making multiphysics analysis and design optimization freely available as open-source software and involving everyone in its creation and development.\n\nFor an overview of the technical details in SU2, please see the following AIAA Journal article:\n\n""SU2: An open-source suite for multiphysics simulation and design,"" AIAA Journal, 54(3):828-846, 2016. <http://arc.aiaa.org/doi/10.2514/1.J053813>\n\nPlease note that this project is released with a [Contributor Code of Conduct](CODE_OF_CONDUCT.md). By participating in this project you agree to abide by its terms.\n\nContinuous Integration:<br/>\n[![Regression Testing](https://github.com/su2code/SU2/workflows/Regression%20Testing/badge.svg?branch=develop)](https://github.com/su2code/SU2/actions)\n[![Release](https://github.com/su2code/SU2/workflows/Release%20Management/badge.svg?branch=develop)](https://github.com/su2code/SU2/actions)\n\nCode Quality:<br/>\n[![CodeFactor](https://www.codefactor.io/repository/github/su2code/su2/badge)](https://www.codefactor.io/repository/github/su2code/su2)\n\n# SU2 Introduction\n\nSU2 is a suite of open-source software tools written in C++ for the numerical solution of partial differential equations (PDE) and performing PDE constrained optimization.\n\nThe primary applications are computational fluid dynamics and aerodynamic shape optimization, but has been extended to treat more general equations such as electrodynamics and chemically reacting flows.\n\nYou will find more information and the latest news in:\n\n- SU2 Home Page: <https://su2code.github.io>\n- GitHub repository: <https://github.com/su2code>\n- CFD Online: <http://www.cfd-online.com/Forums/su2/>\n- Twitter: <https://twitter.com/su2code>\n- Facebook: <https://www.facebook.com/su2code>\n\n# SU2 Installation\n\n## Precompiled binaries for Linux, MacOS, Windows\n\nYou can find precompiled binaries of the latest version on our [download page](https://su2code.github.io/download.html) or under [releases](https://github.com/su2code/SU2/releases).\n\n## Build SU2\n\nThe build system of SU2 is based on a combination of [meson](http://mesonbuild.com/) (as the front-end) and [ninja](https://ninja-build.org/) (as the back-end). Meson is an open source build system meant to be both extremely fast, and, even more importantly, as user friendly as possible. Ninja is a small low-level build system with a focus on speed.\n\nShort summary of the minimal requirements:\n\n- C/C++ compiler\n- Python 3\n\n**Note:** all other necessary build tools and dependencies are shipped with the source code or are downloaded automatically.\n\nIf you have these tools installed, you can create a configuration using the `meson.py` found in the root source code folder:\n\n```\n./meson.py build\n```\n\nUse `ninja` to compile and install the code\n\n```\n./ninja -C build install\n```\n\nFor more information on how to install and build SU2 on Linux, MacOS or Windows, have a look at the [documentation](https://su2code.github.io/docs_v7/).\n\n## SU2 Path setup\n\nWhen installation is complete, please be sure to add the `$SU2_HOME` and `$SU2_RUN` environment variables, and update your `$PATH` with `$SU2_RUN`.\n\nFor example, add these lines to your `.bashrc` file:\n\n```\nexport SU2_RUN=""your_prefix/bin""\nexport SU2_HOME=""/path/to/SU2vX.X.X/""\nexport PATH=$PATH:$SU2_RUN\nexport PYTHONPATH=$SU2_RUN:$PYTHONPATH\n```\n\n`$SU2_RUN` should point to the folder where all binaries and python scripts were installed. This is the prefix you set with the --prefix option to meson. Note that the bin/ directory is automatically added to your prefix path.\n\n`$SU2_HOME` should point to the root directory of the source code distribution, i.e., `/path/to/SU2vX.X.X/`.\n\nThanks for building, and happy optimizing!\n\n- The SU2 Development Team\n\n# SU2 Developers\n\nWe follow the popular ""GitFlow"" branching model for scalable development. In the SU2 repository, the master branch represents the latest stable major or minor release (7.0, 6.2.0, etc.), it should only be modified during version releases. Work that is staged for release is put into the develop branch via Pull Requests on GitHub from various ""feature"" branches where folks do their day-to-day work on the code. At release time, the work that has been merged into the develop branch is pushed to the master branch and tagged as a release.\n\nSU2 is being developed by individuals and organized teams all around the world.\n\nA list of current contributors can be found in the AUTHORS.md file.\n\n## Documentation\n\nCode documentation can be generated by calling doxygen from the root of the project, then open Docs/html/index.html in a browser to consult the documentation.\n",1291,physics,C++,8,C++,Python,Shell,C,Meson,Dockerfile,SWIG,GLSL,,,,,,,,,,,,,,,,,,,,,1169,164,977,28,279,177,1,936295,829,807,760,47,ef8e96812894fb990af9445c7c80b8a3d2e343c9,Merge pull request #2231 from su2code/dependabot/github_actions/actio…,2024-03-04T17:40:51Z,Pedro Gomes,38071223+pcarruscag@users.noreply.github.com,pcarruscag,"SU2 version 8.0.1 ""Harrier""",## Changes\r\n### :rocket: Experimental Features\r\n* Feature actuatordisk bem by @josy-nal in https://github.com/su2code/SU2/pull/2142\r\n* Python interface for updating translation and rotation rates of markers by @HahsFilip in https://github.com/su2code/SU2/pull/2095\r\n* Adaptive Edge Color Group Size by @jblueh in https://github.com/su2code/SU2/pull/2167\r\n* New turbo ouputs by @joshkellyjak in https://github.com/su2code/SU2/pull/2011\r\n* Add further parallel regions by @jblueh in https://github.com/su2code/SU2/pull/2208\r\n### :pill: Bug Fixes\r\n* Fix CST parameterization method by @Zcaic in https://github.com/su2code/SU2/pull/2124\r\n* fix bug Marker_Inlet_Species by @Cristopher-Morales in https://github.com/su2code/SU2/pull/2139\r\n* CoDiPack Update and Explicit Adjoints Locking by @jblueh in https://github.com/su2code/SU2/pull/2146\r\n* CoDiPack Update by @jblueh in https://github.com/su2code/SU2/pull/2153\r\n* Fix cpu arch returned value in TestCase.py by @TripleRider in https://github.com/su2code/SU2/pull/2163\r\n* Fix error in CGNS writing  by @rois1995 in https://github.com/su2code/SU2/pull/2178\r\n* AD Tool Updates by @jblueh in https://github.com/su2code/SU2/pull/2206\r\n* Fix bugs detected by the address sanitizer by @maxaehle in https://github.com/su2code/SU2/pull/2212\r\n* Fix double deformation of points shared by multiple markers by @pcarruscag in https://github.com/su2code/SU2/pull/2216\r\n* Fix wall emissivity out of bounds by @pcarruscag in https://github.com/su2code/SU2/pull/2221\r\n### :wrench: Maintenance\r\n* Update CoolProp to v6.5.0 by @Irvise in https://github.com/su2code/SU2/pull/2136\r\n* add Cur_Time column to the default history field for transient analysis; by @getwelsim in https://github.com/su2code/SU2/pull/2141\r\n* Further Explicit Adjoints Locking and Lock-Free Adjoints Access by @jblueh in https://github.com/su2code/SU2/pull/2161\r\n* Update tests and fix windows compilation by @pcarruscag in https://github.com/su2code/SU2/pull/2165\r\n* Chore cleanup turbo functions by @EvertBunschoten in https://github.com/su2code/SU2/pull/2155\r\n* Update config_template.cfg by @bigfooted in https://github.com/su2code/SU2/pull/1934\r\n* Remove `'U'` mode from Python `open()` function by @TripleRider in https://github.com/su2code/SU2/pull/2171\r\n* Upgrade external CGNS library to 4.4 release. by @MicK7 in https://github.com/su2code/SU2/pull/2179\r\n* Chore cleanup turbo functions by @EvertBunschoten in https://github.com/su2code/SU2/pull/2176\r\n\r\n## New Contributors\r\n* @Zcaic made their first contribution in https://github.com/su2code/SU2/pull/2124\r\n* @Irvise made their first contribution in https://github.com/su2code/SU2/pull/2136\r\n* @getwelsim made their first contribution in https://github.com/su2code/SU2/pull/2141\r\n* @TripleRider made their first contribution in https://github.com/su2code/SU2/pull/2163\r\n* @HahsFilip made their first contribution in https://github.com/su2code/SU2/pull/2095\r\n* @joshkellyjak made their first contribution in https://github.com/su2code/SU2/pull/2011\r\n\r\n**Full Changelog**: https://github.com/su2code/SU2/compare/v8.0.0...v8.0.1,v8.0.1,,,github-actions[bot],Other,SU2,su2code,42,cfd,c-plus-plus,simulation,optimization,python,opensource,physics,flow,fluid,fluid-dynamics,hpc,,,,,,,,,,/su2code/SU2,57,162,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/Stellarium/stellarium,https://github.com/Stellarium/stellarium,0,,,0,0,0,0,0,0,1,1,0,0,0,"Stellarium is a free GPL software which renders realistic skies in real time with OpenGL. It is available for Linux/Unix, Windows and macOS. With Stellarium, you really see what you can see with your eyes, binoculars or a small telescope.","# Stellarium\n[![GitHub release](https://img.shields.io/github/release/Stellarium/stellarium.svg)](https://github.com/Stellarium/stellarium/releases/latest)\n[![GitHub Release Date](https://img.shields.io/github/release-date/Stellarium/stellarium.svg)](https://github.com/Stellarium/stellarium/releases/latest)\n[![Github All Releases](https://img.shields.io/github/downloads/Stellarium/stellarium/total.svg)](https://github.com/Stellarium/stellarium/releases)\n[![Backers and sponsors](https://img.shields.io/opencollective/all/stellarium.svg?style=flat)](https://opencollective.com/stellarium)\n[![CI](https://github.com/Stellarium/stellarium/actions/workflows/ci.yml/badge.svg)](https://github.com/Stellarium/stellarium/actions/workflows/ci.yml)\n[![Build status](https://ci.appveyor.com/api/projects/status/sw8j9l8q95ejkalo?svg=true)](https://ci.appveyor.com/project/alex-w/stellarium)\n[![Coverage Status](https://coveralls.io/repos/github/Stellarium/stellarium/badge.svg)](https://coveralls.io/github/Stellarium/stellarium)\n[![CodeFactor](https://www.codefactor.io/repository/github/stellarium/stellarium/badge)](https://www.codefactor.io/repository/github/stellarium/stellarium)\n[![DOI:10.1558/jsa.17822](http://img.shields.io/badge/DOI-10.1558/jsa.17822-blue.svg)](https://doi.org/10.1558/jsa.17822)\n[![DOI:10.5281/zenodo.8377210](http://img.shields.io/badge/DOI-10.5281/zenodo.8377210-blue.svg)](https://doi.org/10.5281/zenodo.8377210)\n\nStellarium is a free open source planetarium for your computer. It shows a realistic sky\nin 3D, just like what you see with the naked eye, binoculars or a telescope.\n\nIf you are new to Stellarium, go to [www.stellarium.org](https://www.stellarium.org) for loads of additional information.\n\n## Installation Instructions & Quick Start\n\nPlease refer to the [User Guide, Getting Started section](https://github.com/Stellarium/stellarium/releases/download/v24.2/stellarium_user_guide-24.2-1.pdf).\n\n## Get & build the code\n\nSee instructions to [building Stellarium from source code](BUILDING.md).\n\n## Full References and Credits\n\nSee the [full credit file](CREDITS.md).\n\n## Contributing to Stellarium\n\nSee the [contributing guideline](CONTRIBUTING.md).\n\n## Contributors\n\nThis project exists thanks to all the people who contribute! List of contributors [on Github](https://github.com/Stellarium/stellarium/graphs/contributors) (code contributors) and [on Open Collective page](https://opencollective.com/stellarium#contributors) (financial contributors).\n\n## Our backers & sponsors\n\nThank you to all [our backers and sponsors](BACKERS.md)!  Become a [backer](https://opencollective.com/stellarium#backer) or [sponsor](https://opencollective.com/stellarium#sponsor).\n\n## Code Signing\nWindows packages of this program uses free code signing provided by [SignPath.io](https://signpath.io?utm_source=foundation&utm_medium=github&utm_campaign=stellarium), and a free code signing certificate by the [SignPath Foundation](https://signpath.org?utm_source=foundation&utm_medium=github&utm_campaign=stellarium)\n",7365,astronomy,C++,19,CMake,CSS,GLSL,C++,Perl,HTML,C,Shell,Python,TeX,QMake,JavaScript,Makefile,PostScript,Dockerfile,NASL,Inno Setup,EJS,Batchfile,,,,,,,,,,766,120,625,21,45,164,1,4845541,804,2027,1671,356,bff1d60ffb1330dd106080cebca8503c99b5a7d7,Removed unused code,2024-07-16T07:40:13Z,Alexander V. Wolf,aw@altspu.ru,alex-w,v24.2,"**The installers are located in the ""Assets"" section at the end of these release notes!**\r\n\r\n[![Github Release](https://img.shields.io/github/downloads/Stellarium/stellarium/v24.2/total.svg)](https://github.com/Stellarium/stellarium/releases/v24.2)\r\n\r\n## Release notes\r\nThe Stellarium Team has released version 24.2.\r\n\r\nThe major changes of this version:\r\n- Improvements in plugins and AstroCalc tools\r\n- Updates in sky cultures\r\n- Updates in GUI\r\n\r\nBehind the scenes, many more minor issues were fixed.\r\n\r\nTELESCOPE USERS\r\n\r\nWe have identified some bugs in the TelescopeControl plugin on\r\nthe Qt6-based releases.\r\n\r\nTelescope users, please try whether this solves your problems.\r\nIf not, we are looking for those of you with programming skills to help\r\nus fixing remaining issues. Reward possible!\r\n\r\n## What's Changed\r\nFull list of changes:\r\n- Added ability to show an approximate limiting magnitude of devices in Oculars plug-in\r\n- Added support Twilight Factor and Relative Brightness for binoculars in Oculars plug-in\r\n- Added support Adler Index for binoculars in Oculars plug-in\r\n- Added support Bishop Index for binoculars in Oculars plug-in\r\n- Added more actions for Exoplanets plug-in\r\n- Added set of navigational stars from Nevil Maskelyne's ""The British Mariner's Guide"" published in 1764\r\n- Added discovery circumstances for newly discovered comets\r\n- Added partial solar eclipse opening angle\r\n- Added ability to follow CMAKE_INSTALL_MANDIR variable (GH: #3732)\r\n- Added ability to add/subtract great years via hotkeys\r\n- Added support for localization for solar eclipses maps\r\n- Added export of an equirectangular raster eclipse map\r\n- Added missing discovery circumstances for some comets\r\n- Added note for unmarked entries for Sources in Satellites plugin (GH: #3617)\r\n- Fixed eclipse artifacts (GH: #3729)\r\n- Fixed building Scenery3D plugin with Qt 6.7 (GH: #3763)\r\n- Fixed Russian translation of Belarusian SC description\r\n- Fixed image path in German translation of Greek (Farnese) SC\r\n- Fixed references in Romanian SC\r\n- Fixed Galician translation of Xhosa SC description\r\n- Fixed Italian translation of Anutan SC description\r\n- Fixed calculation of twilight factor (or dusk index) for binoculars in Oculars plug-in\r\n- Fixed hiding/restoring of star name display in Navigational Stars plugin (GH: #3728)\r\n- Fixed morphological description of DSO\r\n- Fixed output for CLI option --list-landscapes in Windows\r\n- Fixed solar radius\r\n- Fixed CLI output in Windows (GH: #3710)\r\n- Fixed generator of KML \r\n- Fixed properties for some periodic comets\r\n- Fixed remembering the sorting rules between updates in AstroCalc/Positions tool (GH: #3684)\r\n- Fixed failure to parse too small numbers with from_chars\r\n- Changed AstroCalc/Eclipses tool: check number of penumbra limits in terms of rise-set line\r\n- Changed AstroCalc/Eclipses tool: rewrite generation of shadow limits\r\n- Changed AstroCalc/Eclipses tool: rewrite AstroCalcDialog::getMaximumEclipseAtRiseSet\r\n- Changed AstroCalc/Eclipses tool: rewrite AstroCalcDialog::getRiseSetLineCoordinates\r\n- Changed core/AstroCalc: Move solar eclipse computations out from Planet.cpp\r\n- Changed core/AstroCalc: Move some solar eclipse computations from AstroCalcDialog\r\n- Changed core/AstroCalc: do not show earthshine during annular or high-percentage partial eclipses\r\n- Changed core/AstroCalc: refine max eclipse at rise/set curve\r\n- Changed core: force redraw of NomenclatureItems if needed (GH: #3706)\r\n- Changed core: rename sky culture change signal for better consistency\r\n- Changed core: switch off IAU moon numbers from screen labels\r\n- Changed Satellites plug-in: compactification of GUI\r\n- Changed Satellites plug-in: improve explanatory tooltip\r\n- Changed Remote Control plug-in: add and fix missing functions and features\r\n- Changed scaling limits, special for real planetariums\r\n- Changed GUI: minor clarification\r\n- Changed Satellites plugin: expanding photometry and RCS data for satellites \r\n- Changed Scenery3D plugin: rename variable for debugging of the shaders (GH: #3670)\r\n- Changed visual style: enable table borders by default in sky cultures\r\n- Changed visual style: use single lines between table cells in sky cultures\r\n- Updated Vanuatu (Netwar) sky culture: clarified the license with author\r\n- Updated Anutan sky culture: fix formatting\r\n- Updated Chinese sky culture: fix formatting\r\n- Updated Tibetian sky culture: rename an image to match the name in the links\r\n- Updated default catalog of exoplanets\r\n- Updated default list of locations\r\n- Updated default catalog of pulsars\r\n- Updated discovery circumstances data for minor planets\r\n- Updated discovery circumstances data for comets\r\n- Updated photometry of satellites\r\n- Updated planetary nomenclature\r\n- Updated group for RPM\r\n- Removed extraneous borders in tables in About dialogs of plugins (GH: #3742)\r\n- Removed config option for an AppImage (GH: #3726)\r\n\r\n## Packages notes\r\npackage | note\r\n------------|---------\r\nstellarium-24.2.tar.gz | Stellarium 24.2 for UNIX/Linux (source code). <br />__MD5:__ 27a7cd3ab9421cb73790db6022f7c5aa <br />__SHA256:__ e6d8ee0792b7f77486b700d4669d0dd0c349319f379758ad643d76165d1d56d2\r\nstellarium-24.2.tar.bz2 | Stellarium 24.2 for UNIX/Linux (source code). <br />__MD5:__ ca705a368a3598561eb1d25df70e7676 <br />__SHA256:__ 55d55d4983c7263a32c23a579389283171dd9bb42e783cb26882a9546e85a7c5\r\nstellarium-24.2.tar.xz | Stellarium 24.2 for UNIX/Linux (source code). <br />__MD5:__ 97ebab4102e455ab73672d9caca1fbbe <br />__SHA256:__ f9e8e114e34265194ffff0c2a2806a6fdc998449192749c4ef60b0a5302101af\r\nstellarium-24.2.zip | Stellarium 24.2 for UNIX/Linux (source code). <br />__MD5:__ b46261ae4b205f06246b59957a9bf572 <br />__SHA256:__ 47e7702d90b1da5db4062a3c967924aedd071e424c070cc7c3a32ed6ca927f03\r\nstellarium-24.2-qt5-win32.exe | Stellarium 24.2 for Windows (x86_32; Windows 7+) based on Qt 5.12.6 (Visual Studio 2017). <br /> __MD5__: d45dea6a3878cdb22676189b9bed8a11 <br />__SHA256__: 35ef9fe4c4217c5dd36d611b29382c5af61423edec9c7ef13c2b27ce6b80a5ad\r\nstellarium-24.2-qt5-win64.exe | Stellarium 24.2 for Windows (x86_64; Windows 7+) based on Qt 5.12.6 (Visual Studio 2017). <br /> __MD5__: 3be9f2296cc0165faef74253b64bbb0f <br />__SHA256__: 60faba4a4570a7591e333836cec247bb1992719a04cb41c902194ba6f27394cb\r\nstellarium-24.2-qt6-win64.exe | Stellarium 24.2 for Windows (x86_64; Windows 10+) based on Qt 6.5.3 (Visual Studio 2019). <br /> __MD5__: 28c4313f8a4b9bc69b4676b830bd22e8 <br />__SHA256__: 73b1e6e73a167260d5d95dd02556274cdb66f43ead2b00d02f35024adb436e32\r\nstellarium-24.2-qt6-arm64.exe | Stellarium 24.2 for Windows (ARM64; Windows 10+) based on Qt 6.5.3 (Visual Studio 2022). <br /> __MD5__: 39a8856bf539e1b6efb33b82b2a554f5 <br />__SHA256__: 3615dbfca0e962b4ecc66f0010a2173337d87074f5ad4c05ec24b94547069512\r\nStellarium-24.2-qt6-macOS.zip | Stellarium 24.2 for macOS (universal; macOS 11.0+) based on Qt 6.5.3 (AppleClang 15.0.0). <br /> __MD5__: 29571975d74c4c23c86bd7cfcbe3a6b7 <br />__SHA256__: 34bdec53ba5dfb2569a52545ab06306f12843ef483970d548c59cd49e77b5c99\r\nStellarium-24.2-qt5-x86_64.zip | Stellarium 24.2 for macOS (x86_64; macOS 10.14+) based on Qt 5.12.12 (AppleClang 13.0.0). <br /> __MD5__: 5808774430dbd3a5ab9e299510357ef4 <br />__SHA256__: cd8c12db299dfb20fcc80f608bf81f4c913535676cd37c062067bd5875952eef\r\nStellarium-24.2-qt5-x86_64.AppImage | Stellarium 24.2 for Linux (amd64; glibc 2.31) based on Qt 5.12.8 (GCC 9.4.0). <br /> __MD5__: a7a4c75ed5e5ab4a2df287ae76e23e56 <br />__SHA256__: a619c1e2f28920b9b77a0a19ee5baf17907934b4a45596d64ab16e43b3a1547d\r\nStellarium-24.2-qt6-x86_64.AppImage | Stellarium 24.2 for Linux (amd64; glibc 2.35) based on Qt 6.2.4 (GCC 11.4.0). <br /> __MD5__: 1a32f601687092c9d615df7c366384be <br />__SHA256__: b463602f09d48f02f9e2813d5e74faea690d83db63dae63db84e55755c3d26d3\r\n\r\n## Verify\r\n### GPG signature\r\n1. Import Stellarium's GPG key\r\n  `curl -O https://stellarium.org/files/stellarium.gpg`\r\n2. List Stellarium's GPG key\r\n  `gpg --with-fingerprint --show-keys --keyid-format long stellarium.gpg`\r\n  **id:** `rsa4096/BF38D4D02A328DFF 2012-04-07`\r\n  **fingerprint:** `7915 1C2E 6351 E727 8DA1  A730 BF38 D4D0 2A32 8DFF`\r\n3. Verify the binary file is valid (tarball for example)\r\n  `gpgv --keyring ./stellarium.gpg stellarium-24.2.tar.gz.asc stellarium-24.2.tar.gz`",v24.2,Alexander V. Wolf,,alex-w,GNU General Public License v2.0,stellarium,Stellarium,60,c,c-plus-plus,astronomy,science,stars,sky,universe,planetarium,stellarium,qt5,qt6,,,,,,,,,,/Stellarium/stellarium,60,151,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/steineggerlab/foldseek,https://github.com/steineggerlab/foldseek,1,,,1,1,1,0,0,0,0,0,0,0,0,Foldseek enables fast and sensitive comparisons of large structure sets.,"\n# Foldseek \nFoldseek enables fast and sensitive comparisons of large protein structure sets.\n\n<p align=""center""><img src=""https://github.com/steineggerlab/foldseek/blob/master/.github/foldseek.png"" height=""250""/></p>\n\n## Publications\n[van Kempen M, Kim S, Tumescheit C, Mirdita M, Lee J, Gilchrist CLM, Söding J, and Steinegger M. Fast and accurate protein structure search with Foldseek. Nature Biotechnology, doi:10.1038/s41587-023-01773-0 (2023)](https://www.nature.com/articles/s41587-023-01773-0)\n\n[Barrio-Hernandez I, Yeo J, Jänes J, Mirdita M, Gilchrist CLM, Wein T, Varadi M, Velankar S, Beltrao P and Steinegger M. Clustering predicted structures at the scale of the known protein universe. Nature, doi:10.1038/s41586-023-06510-w (2023)](https://www.nature.com/articles/s41586-023-06510-w)\n\n[Kim W, Mirdita M, Levy Karin E, Gilchrist CLM, Schweke H, Söding J, Levy E, and Steinegger M. Rapid and Sensitive Protein Complex Alignment with Foldseek-Multimer. bioRxiv, doi:10.1101/2024.04.14.589414 (2024)](https://www.biorxiv.org/content/10.1101/2024.04.14.589414v1)\n\n# Table of Contents\n\n- [Foldseek](#foldseek)\n- [Webserver](#webserver)\n- [Installation](#installation)\n- [Memory requirements](#memory-requirements)\n- [Tutorial Video](#tutorial-video)\n- [Documentation](#documentation)\n- [Quick Start](#quick-start)\n  - [Search](#search)\n    - [Output](#output-search)\n    - [Important Parameters](#important-search-parameters)\n    - [Alignment Mode](#alignment-mode)\n    - [Structure search from FASTA input](#structure-search-from-fasta-input)\n  - [Databases](#databases)\n    - [Create Custom Databases and Indexes](#create-custom-databases-and-indexes)\n  - [Cluster](#cluster)\n    - [Output](#output-cluster)\n    - [Important Parameters](#important-cluster-parameters)\n  - [Multimer](#multimersearch)\n    - [Output](#multimer-search-output)\n- [Main Modules](#main-modules)\n- [Examples](#examples)\n\n## Webserver \nSearch your protein structures against the [AlphaFoldDB](https://alphafold.ebi.ac.uk/) and [PDB](https://www.rcsb.org/) in seconds using the Foldseek webserver ([code](https://github.com/soedinglab/mmseqs2-app)): [search.foldseek.com](https://search.foldseek.com) 🚀\n\n## Installation\n```\n# Linux AVX2 build (check using: cat /proc/cpuinfo | grep avx2)\nwget https://mmseqs.com/foldseek/foldseek-linux-avx2.tar.gz; tar xvzf foldseek-linux-avx2.tar.gz; export PATH=$(pwd)/foldseek/bin/:$PATH\n\n# Linux SSE2 build (check using: cat /proc/cpuinfo | grep sse2)\nwget https://mmseqs.com/foldseek/foldseek-linux-sse2.tar.gz; tar xvzf foldseek-linux-sse2.tar.gz; export PATH=$(pwd)/foldseek/bin/:$PATH\n\n# Linux ARM64 build\nwget https://mmseqs.com/foldseek/foldseek-linux-arm64.tar.gz; tar xvzf foldseek-linux-arm64.tar.gz; export PATH=$(pwd)/foldseek/bin/:$PATH\n\n# MacOS\nwget https://mmseqs.com/foldseek/foldseek-osx-universal.tar.gz; tar xvzf foldseek-osx-universal.tar.gz; export PATH=$(pwd)/foldseek/bin/:$PATH\n\n# Conda installer (Linux and macOS)\nconda install -c conda-forge -c bioconda foldseek\n```\nOther precompiled binaries for ARM64 amd SSE2 are available at [https://mmseqs.com/foldseek](https://mmseqs.com/foldseek).\n\n## Memory requirements \nFor optimal software performance, consider three options based on your RAM and search requirements:\n\n1. **With Cα info (default).** \n   Use this formula to calculate RAM - `(6 bytes Cα + 1 3Di byte + 1 AA byte) * (database residues)`. The 54M AFDB50 entries require 151GB.\n\n2. **Without Cα info.** \n   By disabling `--sort-by-structure-bits 0`, RAM requirement reduces to 35GB. However, this alters hit rankings and final scores but not E-values. Structure bits are mostly relevant for hit ranking for E-value > 10^-1.\n\n3. **Single query searches.** \n   Use the `--prefilter-mode 1`, which isn't memory-limited and computes all ungapped alignments. This option optimally utilizes foldseek's multithreading capabilities for single queries.\n\n## Tutorial Video\nWe presented a Foldseek tutorial at the SBGrid where we demonstrated Foldseek's webserver and command line interface. \nCheck it out [here](https://www.youtube.com/watch?v=k5Rbi22TtOA).\n\n<a href=""https://www.youtube.com/watch?v=k5Rbi22TtOA""><img src=""https://img.shields.io/youtube/views/k5Rbi22TtOA?style=social""></a>.\n\n## Documentation\nMany of Foldseek's modules (subprograms) rely on MMseqs2. For more information about these modules, refer to the [MMseqs2 wiki](https://github.com/soedinglab/MMseqs2/wiki). For documentation specific to Foldseek, checkout the Foldseek wiki [here](https://github.com/steineggerlab/foldseek/wiki).\n\n## Quick start\n\n### Search\nThe `easy-search` module allows to query one or more single-chain protein structures, formatted in PDB/mmCIF format (flat or gzipped), against a target database, folder or individual single-chain protein structures (for multi-chain proteins see [complexsearch](#complexsearch)). The default alignment information output is a [tab-separated file](#tab-separated) but Foldseek also supports [Superposed Cα PDBs](#superpositioned-cα-only-pdb-files) and [HTML](#interactive-html).\n\n    foldseek easy-search example/d1asha_ example/ aln tmpFolder\n    \n#### Output Search\n##### Tab-separated\n  \nThe default output fields are: `query,target,fident,alnlen,mismatch,gapopen,qstart,qend,tstart,tend,evalue,bits` but they can be customized with the `--format-output` option e.g., `--format-output ""query,target,qaln,taln""` returns the query and target accessions and the pairwise alignments in tab-separated format. You can choose many different output columns.\n\n| Code | Description |\n| --- | --- |\n|query | Query sequence identifier |\n|target | Target sequence identifier |\n|qca        | Calpha coordinates of the query |\n|tca        | Calpha coordinates of the target |\n|alntmscore | TM-score of the alignment | \n|qtmscore   | TM-score normalized by the query length |\n|ttmscore   | TM-score normalized by the target length |\n|u          | Rotation matrix (computed to by TM-score) |\n|t          | Translation vector (computed to by TM-score) |\n|lddt       | Average LDDT of the alignment |\n|lddtfull   | LDDT per aligned position |\n|prob       | Estimated probability for query and target to be homologous (e.g. being within the same SCOPe superfamily) |\n\nCheck out the [MMseqs2 documentation for additional output format codes](https://github.com/soedinglab/MMseqs2/wiki#custom-alignment-format-with-convertalis).\n\n##### Superpositioned Cα only PDB files\nFoldseek's `--format-mode 5` generates PDB files with all target Cα atoms superimposed onto the query structure based on the aligned coordinates. \nFor each pairwise alignment it will write its own PDB file, so be careful when using this options for large searches. \n\n##### Interactive HTML\nLocally run Foldseek can generate an HTML search result, similar to the one produced by the [webserver](https://search.foldseek.com) by specifying `--format-mode 3`\n\n```\nfoldseek easy-search example/d1asha_ example/ result.html tmp --format-mode 3\n```\n\n<p align=""center""><img src=""./.github/results.png"" height=""400""/></p>\n\n#### Important search parameters\n\n| Option            | Category        | Description                                                                                               |\n|-------------------|-----------------|-----------------------------------------------------------------------------------------------------------|\n| -s              | Sensitivity     | Adjust sensitivity to speed trade-off; lower is faster, higher more sensitive (fast: 7.5, default: 9.5)   |\n| --exhaustive-search | Sensitivity | Skips prefilter and performs an all-vs-all alignment (more sensitive but much slower)                     |\n| --max-seqs      | Sensitivity     | Adjust the amount of prefilter handed to alignment; increasing it can lead to more hits (default: 1000)   |\n| -e              | Sensitivity     | List matches below this E-value (range 0.0-inf, default: 0.001); increasing it reports more distant structures |\n| --alignment-type| Alignment       | 0: 3Di Gotoh-Smith-Waterman (local, not recommended), 1: TMalign (global, slow), 2: 3Di+AA Gotoh-Smith-Waterman (local, default) |\n| -c              | Alignment  | List matches above this fraction of aligned (covered) residues (see --cov-mode) (default: 0.0); higher coverage = more global alignment |\n| --cov-mode      | Alignment  | 0: coverage of query and target, 1: coverage of target, 2: coverage of query                               |\n\n#### Alignment Mode\nBy default, Foldseek uses its local 3Di+AA structural alignment but it also supports realigning hits using the global TMalign as well as rescoring alignments using TMscore. \n\n    foldseek easy-search example/d1asha_ example/ aln tmp --alignment-type 1\n\nIf alignment type is set to tmalign (`--alignment-type 1`), the results will be sorted by the TMscore normalized by query length. The TMscore is used for reporting two fields: the e-value=(qTMscore+tTMscore)/2 and the score=(qTMscore*100). All output fields (e.g., pident, fident, and alnlen) are calculated based on the TMalign alignment.\n\n#### Structure search from FASTA input\nSearch by predicting 3Di directly from amino acid sequences without the need for existing protein structures. \nThis feature uses the [ProstT5](https://www.biorxiv.org/content/10.1101/2023.07.23.550085v2) protein language model and runs by default on CPU and is about 400-4000x compared to predicted structures by [ColabFold](https://github.com/sokrypton/ColabFold).\n\n```\nfoldseek databases ProstT5 weights tmp\nfoldseek databases PDB pdb tmp\nfoldseek easy-search QUERY.fasta pdb result.m8 tmp --prostt5-model weights\n```\n\nOr create your a structural database from a fasta files.\n\n```\nfoldseek createdb db.fasta db --prostt5-model weights\n```\n\nFaster inference using GPU/CUDA is also supported. Compile from source with `cmake -DCMAKE_BUILD_TYPE=Release  -DENABLE_CUDA=1 -DCUDAToolkit_ROOT=Path-To-Cuda-Toolkit` and call with `createdb/easy-search --prostt5-model weights --gpu 1`.\n\n### Databases \nThe `databases` command downloads pre-generated databases like PDB or AlphaFoldDB.\n    \n    # pdb  \n    foldseek databases PDB pdb tmp \n    # alphafold db\n    foldseek databases Alphafold/Proteome afdb tmp \n\nWe currently support the following databases: \n```\n  Name                   	Type     	Taxonomy	Url\n- Alphafold/UniProt   	Aminoacid	     yes	https://alphafold.ebi.ac.uk/\n- Alphafold/UniProt50 	Aminoacid	     yes	https://alphafold.ebi.ac.uk/\n- Alphafold/Proteome  	Aminoacid	     yes	https://alphafold.ebi.ac.uk/\n- Alphafold/Swiss-Prot	Aminoacid	     yes	https://alphafold.ebi.ac.uk/\n- ESMAtlas30          	Aminoacid	       -	https://esmatlas.com\n- PDB                 	Aminoacid	     yes	https://www.rcsb.org\n```\n\n#### Create custom databases and indexes\nThe target database can be pre-processed by `createdb`. This is useful when searching multiple times against the same set of target structures. \n \n    foldseek createdb example/ targetDB\n    foldseek createindex targetDB tmp  #OPTIONAL generates and stores the index on disk\n    foldseek easy-search example/d1asha_ targetDB aln.m8 tmpFolder\n\n### Cluster\nThe `easy-cluster` algorithm is designed for structural clustering by assigning structures to a representative protein structure using structural alignment. It accepts input in either PDB or mmCIF format, with support for both flat and gzipped files. By default, easy-cluster generates three output files with the following prefixes: (1) `_clu.tsv`, (2) `_repseq.fasta`, and (3) `_allseq.fasta`. The first file (1) is a [tab-separated](#tab-separated-cluster) file describing the mapping from representative to member, while the second file (2) contains only [representative sequences](#representative-fasta), and the third file (3) includes all [cluster member sequences](#all-member-fasta).\n\n    foldseek easy-cluster example/ res tmp -c 0.9 \n    \n#### Output Cluster\n##### Tab-separated cluster\nThe provided format represents protein structure clustering in a tab-separated, two-column layout (representative and member). Each line denotes a cluster-representative and cluster-member relationship, signifying that the member shares significant structural similarity with the representative, and thus belongs to the same cluster.\n```\nQ0KJ32	Q0KJ32\nQ0KJ32	C0W539\nQ0KJ32	D6KVP9\nE3HQM9	E3HQM9\nE3HQM9	F0YHT8\n```\n\n##### Representative fasta\nThe `_repseq.fasta` contains all representative protein sequences of the clustering.\n```\n>Q0KJ32\nMAGA....R\n>E3HQM9\nMCAT...Q\n```\n\n##### All member fasta\nIn the `_allseq.fasta` file all sequences of the cluster are present. A new cluster is marked by two identical name lines of the representative sequence, where the first line stands for the cluster and the second is the name line of the first cluster sequence. It is followed by the fasta formatted sequences of all its members.\n\n```\n>Q0KJ32	\n>Q0KJ32\nMAGA....R\n>C0W539\nMVGA....R\n>D6KVP9\nMVGA....R\n>D1Y890\nMVGV....R\n>E3HQM9	\n>E3HQM9\nMCAT...Q\n>Q223C0\nMCAR...Q\n```\n\n#### Important cluster parameters\n\n| Option            | Category        | Description                                                                                               |\n|-------------------|-----------------|-----------------------------------------------------------------------------------------------------------|\n| -e              | Sensitivity     | List matches below this E-value (range 0.0-inf, default: 0.001); increasing it reports more distant structures |\n| --alignment-type| Alignment       | 0: 3Di Gotoh-Smith-Waterman (local, not recommended), 1: TMalign (global, slow), 2: 3Di+AA Gotoh-Smith-Waterman (local, default) |\n| -c              | Alignment  | List matches above this fraction of aligned (covered) residues (see --cov-mode) (default: 0.0); higher coverage = more global alignment |\n| --cov-mode      | Alignment  | 0: coverage of query and target, 1: coverage of target, 2: coverage of query                               |\n| --min-seq-id      | Alignment  | the minimum sequence identity to be clustered                               |\n| --tmscore-threshold      | Alignment  | accept alignments with an alignment TMscore > thr                               |\n| --lddt-threshold      | Alignment  | accept alignments with an alignment LDDT score > thr                               |\n\n\n### Multimersearch\nThe `easy-multimersearch` module is designed for querying one or more protein complex (multi-chain) structures (supported input formats: PDB/mmCIF, flat or gzipped) against a target database of protein complex structures. It reports the similarity metrices between the complexes (e.g., the TMscore).\n\n#### Using Multimersearch\nThe examples below use files that can be found in the `example` directory, which is part of the Foldseek repo, if you clone it. \nIf you use the precompiled version of the software, you can download the files directly: [1tim.pdb.gz](https://github.com/steineggerlab/foldseek/raw/master/example/1tim.pdb.gz) and [8tim.pdb.gz](https://github.com/steineggerlab/foldseek/raw/master/example/8tim.pdb.gz).\n\nFor a pairwise alignment of complexes using `easy-multimersearch`, run the following command:\n```\nfoldseek easy-multimersearch example/1tim.pdb.gz example/8tim.pdb.gz result tmpFolder\n```\nFoldseek `easy-multimersearch` can also be used for searching one or more query complexes against a target database: \n```\nfoldseek databases PDB pdb tmp \nfoldseek easy-multimersearch example/1tim.pdb.gz pdb result tmpFolder\n```\n\n#### Multimer Search Output\n##### Tab-separated-complex\nBy default, `easy-multimersearch` reports the output alignment in a tab-separated file.\nThe default output fields are: `query,target,fident,alnlen,mismatch,gapopen,qstart,qend,tstart,tend,evalue,bits,complexassignid` but they can be customized with the `--format-output` option e.g., `--format-output ""query,target,complexqtmscore,complexttmscore,complexassignid""` alters the output to show specific scores and identifiers.\n\n| Code | Description |\n| --- | --- |\n| **Commons** |\n|query | Query sequence identifier |\n|target | Target sequence identifier |\n| **Only for scorecomplex** |\n|complexqtmscore| TM-score of Complex alignment normalized by the query length |\n|complexttmscore| TM-score of Complex alignment normalized by the target length |\n|complexu       | Rotation matrix of Complex alignment (computed to by TM-score) |\n|complext       | Translation vector of Complex alignment (computed to by TM-score) |\n|complexassignid| Index of Complex alignment |\n\n**Example Output:**\n```\n1tim.pdb.gz_A   8tim.pdb.gz_A   0.967   247 8   0   1   247 1   247 5.412E-43   1527    0\n1tim.pdb.gz_B   8tim.pdb.gz_B   0.967   247 8   0   1   247 1   247 1.050E-43   1551    0\n```\n\n##### Complex Report\n`easy-multimersearch` also generates a report (prefixed `_report`), which provides a summary of the inter-complex chain matching, including identifiers, chains, TMscores, rotation matrices, translation vectors, and assignment IDs. The report includes the following fields:\n| Column | Description |\n| --- | --- |\n| 1 | Identifier of the query complex |\n| 2 | Identifier of the target complex |\n| 3 | Comma separated matched chains in the query complex |\n| 4 | Comma separated matched chains in the target complex |\n| 5 | TM score normalized by query length [0-1] |\n| 6 | TM score normalized by target length [0-1] |\n| 7 | Comma separated nine rotation matrix (U) values |\n| 8 | Comma separated three translation vector (T) values |\n| 9 | Complex alignment ID |\n\n**Example Output:**\n```\n1tim.pdb.gz 8tim.pdb.gz A,B A,B 0.98941 0.98941 0.999983,0.000332,0.005813,-0.000373,0.999976,0.006884,-0.005811,-0.006886,0.999959 0.298992,0.060047,0.565875  0\n```\n\n## Main Modules\n- `easy-search`       fast protein structure search  \n- `easy-cluster`      fast protein structure clustering  \n- `createdb`          create a database from protein structures (PDB,mmCIF, mmJSON)\n- `databases`         download pre-assembled databases\n\n## Examples\n### Rescore aligments using TMscore\nThe easiest way to get the alignment TMscore normalized by min(alnLen,qLen,targetLen) as well as a rotation matrix is through the following command:\n```\nfoldseek easy-search example/ example/ aln tmp --format-output query,target,alntmscore,u,t\n```\n\nAlternatively, it is possible to compute TMscores for the kind of alignment output (e.g., 3Di+AA) using the following commands: \n```\nfoldseek createdb example/ targetDB\nfoldseek createdb example/ queryDB\nfoldseek search queryDB targetDB aln tmpFolder -a\nfoldseek aln2tmscore queryDB targetDB aln aln_tmscore\nfoldseek createtsv queryDB targetDB aln_tmscore aln_tmscore.tsv\n```\n\nOutput format `aln_tmscore.tsv`: query and target identifiers, TMscore, translation(3) and rotation vector=(3x3)\n\n### Cluster search results \nThe following command performs an all-against-all alignments of the input structures and retains only the alignments, which cover 80% of the sequence (-c 0.8) (read more about alignment coverage options [here](https://github.com/soedinglab/MMseqs2/wiki#how-to-set-the-right-alignment-coverage-to-cluster)). It then clusters the results using a greedy set cover algorithm. The clustering mode can be adjusted using --cluster-mode, read more [here](https://github.com/soedinglab/MMseqs2/wiki#clustering-modes). The clustering output format is described [here](https://github.com/soedinglab/MMseqs2/wiki#cluster-tsv-format).\n\n```\nfoldseek createdb example/ db\nfoldseek search db db aln tmpFolder -c 0.8 \nfoldseek clust db aln clu\nfoldseek createtsv db db clu clu.tsv\n```\n\n### Query centered multiple sequence alignment \nFoldseek can output multiple sequence alignments in a3m format using the following commands. \nTo convert a3m to FASTA format, the following script can be used [reformat.pl](https://raw.githubusercontent.com/soedinglab/hh-suite/master/scripts/reformat.pl) (`reformat.pl in.a3m out.fas`).\n\n```\nfoldseek createdb example/ targetDB\nfoldseek createdb example/ queryDB\nfoldseek search queryDB targetDB aln tmpFolder -a\nfoldseek result2msa queryDB targetDB aln msa --msa-format-mode 6\nfoldseek unpackdb msa msa_output --unpack-suffix a3m --unpack-name-mode 0\n```\n",707,bioinformatics,C,18,CMake,Shell,C++,C,Dockerfile,Perl,Makefile,Batchfile,Python,Meson,Lua,Starlark,HTML,Roff,R,Rust,Jupyter Notebook,JavaScript,,,,,,,,,,,32,16,16,0,12,20,0,25419,93,276,146,130,bc212bc8602ef426c7b58368c65dd744443f802c,FoldseekBase.cpp update (#306),2024-07-15T04:52:15Z,SooyoungCha,97579193+ChaSooyoung@users.noreply.github.com,sooyoung-cha,9-427df8a,"At a glance: Foldseek release 9 features the fully benchmarked [Foldseek-multimer](https://www.biorxiv.org/content/10.1101/2024.04.14.589414v1) search and structure-based sequence search using [ProstT5](https://www.biorxiv.org/content/10.1101/2023.07.23.550085v2). Both Foldseek-multimer and structure-based sequence search are also available in the [Foldseek webserver](https://search.foldseek.com).\r\n### Major Features\r\n- **Foldseek-multimer**: Fully benchmarked and integrated into this release with the `easy-multimersearch` and `multimer` workflows (Thanks @Woosub-Kim). Check out our [preprint](​​https://www.biorxiv.org/content/10.1101/2024.04.14.589414v1) explaining the algorithm.\r\nRead more on how to get started in our [README](https://github.com/steineggerlab/foldseek?tab=readme-ov-file#multimersearch). \r\n- **Search requires less memory**: We optimized the memory consumption of Foldseek. It requires significant less memory now (f629bbe108595ebf0886dd6974e9959a4d962af3)\r\n- **Structure-based sequence search**: Predict protein 3Di directly from amino acid sequences without the need for existing protein structures. This is roughly 400-4000x faster than predicting full protein structures with [ColabFold](https://github.com/sokrypton/ColabFold). This feature uses the [ProstT5](https://www.biorxiv.org/content/10.1101/2023.07.23.550085v2) protein language model and runs by default on CPU: \r\n```\r\nfoldseek databases ProstT5 weights tmp\r\nfoldseek databases PDB pdb tmp\r\nfoldseek easy-search QUERY.fasta pdb result.m8 tmp --prostt5-model weights\r\n```\r\n\r\nFast inference using GPU/CUDA is also supported. Compile from source with `cmake -DCMAKE_BUILD_TYPE=Release -DENABLE_CUDA=1 -DCUDAToolkit_ROOT=Path-To-Cuda-Toolkit` and call with `createdb/easy-search --prostt5-model weights --gpu 1`.\r\n (Thanks @Victor-Mihaila).\r\n\r\n###  Breaking changes\r\n- Remove `.cif`/`.pdb` from filenames and remove `_MODEL_` from identifiers in `.lookup` #261 (Thanks @ChaSooyoung)\r\n- Removed `--tar-include` and `--tar-exclude` from `createdb` as they were unused (15c0516)\r\n- Not-breaking: workflows using `easy-complexsearch` and `complexsearch` will continue to work. These are hidden modules mapping to `easy-multimersearch` and `multimersearch` internally. However, the internals have had major changes since the last release.\r\n\r\n###  Other features\r\n- `convert2pdb` can output separate PDB files (346c1ddaf4)\r\n- `createdb` learned to read a large number of input files from a `.tsv` file (e1394aa)\r\n- Force input format with `createdb --input-format` (852434a)\r\n- Compute exact TM-score with `--exact-tmscore` (493cefe)\r\n- Added CATH50 database (6893dcc)\r\n- Update HTML output (not fully supported for multimer yet; c7e4a37, 361c22a, 1bc8d2e; Thanks @gamcil)\r\n- `compressca` learned new input and output modes (8e68e86, 5d2724d, 284bc81)\r\n\r\n### Bug Fixes\r\n- Fix broken symlinks with `databases PDB` download (9ef6d18557, fa6c53089d).\r\n- Fix AFDB Proteome and SwissProt download check (fa6c530, Thanks @TigerWindWood)\r\n- Fix AF3 mmCIF files crashing `createdb`\r\n- Fix `convert2pdb` creating broken PDB files for large structures (b6dac8a541)\r\n- Remove ligand and alt res within chain #198 (Thanks @NatureGeorge)\r\n- Skip residues without C-alpha #214 (75a50f7c)\r\n- `structurerescorediagonal` did not properly respect `--tmscore-threshold` (#205; 886021d)\r\n- Fallback alignment to Smith-Waterman when block-aligner produces invalid alignments (54c271c)\r\n\r\n### Developers\r\n- Foldseek now includes the [Candle](https://github.com/huggingface/candle) ML framework and has a further expanded Rust codebase.\r\n- Foldseek can be inherited from to create subprojects (e00a3dc, 7c2c08e, 9a1a087, 00d2033)\r\n\r\n\r\n\r\n",9-427df8a,Martin Steinegger,,martin-steinegger,GNU General Public License v3.0,foldseek,steineggerlab,9,protein-structure,alignments,bioinformatics,clustering,,,,,,,,,,,,,,,,,/steineggerlab/foldseek,10,20,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/stdlib-js/stdlib,https://github.com/stdlib-js/stdlib,0,,0,0,0,0,0,0,0,1,1,0,0,0,✨ Standard library for JavaScript and Node.js. ✨,"<!--\n\n@license Apache-2.0\n\nCopyright (c) 2018 The Stdlib Authors.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n-->\n\n<!-- lint disable first-heading-level -->\n\n<!-- Section to include announcements. If section is included, add a horizontal rule *after* the section. Make sure to keep an empty line after the `section` element and another before the `/section` close. -->\n\n<section class=""announcement"">\n\n</section>\n\n<!-- /.announcement -->\n\n<!-- Section to include assets such as the project logo, etc. Make sure to keep an empty line after the `section` element and another before the `/section` close. -->\n\n<section class=""banner"">\n    <div class=""image"" align=""center"">\n        <br>\n        <br>\n        <a href=""https://stdlib.io/"" />\n            <img src=""https://cdn.jsdelivr.net/gh/stdlib-js/stdlib@9f7d30f089ecc458a8b836a75afab75caf5c0b36/docs/assets/logo_banner.svg"" alt=""stdlib logo"">\n        </a>\n        <br>\n        <br>\n        <br>\n        <br>\n    </div>\n</section>\n\n<!-- /.banner -->\n\n* * *\n\n<!-- Section to include introductory text. Make sure to keep an empty line after the intro `section` element and another before the `/section` close. -->\n\n<section class=""intro"">\n\nWe believe in a future in which the web is a preferred environment for numerical computation. To help realize this future, we've built stdlib.\n\nstdlib ([/ˈstændərd lɪb/][ipa-english] ""standard lib"") is a standard library with an emphasis on numerical and scientific computation, written in JavaScript (and C) for execution in browsers and in Node.js. The library provides a collection of robust, high performance libraries for mathematics, statistics, data processing, streams, and more and includes many of the utilities you would expect from a standard library.\n\nWhat sets stdlib apart is its fully decomposable architecture, which allows you to swap out and mix and match APIs and functionality to cater to your exact preferences and use cases.\n\nWhen you use stdlib, you can be confident that you are using the most thorough, rigorous, well-written, studied, documented, tested, measured, and high-quality code available.\n\nWant to join us in bringing numerical computing to the web? **Start by starring the project.** :star2:\n\nExplore this GitHub repository for stdlib's source code and documentation. For guidance on developing stdlib, refer to the [development guide][stdlib-development].\n\nThank you for being a part of our community! Your support is invaluable to us!\n\n## Resources\n\n-   [**Installation**](#installation)\n-   [**Homepage**][stdlib-homepage]\n-   [**Documentation**][stdlib-documentation]\n-   [**Source code**][stdlib-source]\n-   [**Code coverage**][stdlib-code-coverage]\n-   [**FAQ**][stdlib-faq]\n\n### External Resources\n\n-   [**Open Collective**][open-collective-stdlib]\n-   [**Twitter**][stdlib-twitter]\n-   [**Gitter**][stdlib-gitter]\n\n## Features\n\n-   150+ [special math functions][@stdlib/math/base/special].\n\n    <div class=""image"" align=""center"">\n        <img src=""https://cdn.jsdelivr.net/gh/stdlib-js/stdlib@203839353bc74297fe641207270f7917d2bda560/docs/assets/readme/base_special_math.png"" alt=""Demo showcasing special math functions"">\n    </div>\n\n-   35+ [probability distributions][@stdlib/stats/base/dists], with support for evaluating probability density functions (PDFs), cumulative distribution functions (CDFs), quantiles, moments, and more.\n\n    <div class=""image"" align=""center"">\n        <img src=""https://cdn.jsdelivr.net/gh/stdlib-js/stdlib@e13885087939c064c69aa43ee80ea52710de5591/docs/assets/readme/base_dists.png"" alt=""Demo showcasing probability distributions"">\n    </div>\n\n-   40+ [seedable pseudorandom number generators][@stdlib/random/base] (PRNGs).\n\n    <div class=""image"" align=""center"">\n        <img src=""https://cdn.jsdelivr.net/gh/stdlib-js/stdlib@83dcd0fad98883320a8b1efc801b2fc1ed2a003d/docs/assets/readme/base_prngs.png"" alt=""Demo showcasing PRNGs"">\n    </div>\n\n-   200+ general [utilities][@stdlib/utils] for data transformation, functional programming, and asynchronous control flow.\n\n    <div class=""image"" align=""center"">\n        <img src=""https://cdn.jsdelivr.net/gh/stdlib-js/stdlib@e6eeea31e49d6db1b6f57ae59d2988d4b427e285/docs/assets/readme/utils.png"" alt=""Demo showcasing general utilities"">\n    </div>\n\n-   200+ [assertion utilities][@stdlib/assert] for data validation and feature detection.\n\n    <div class=""image"" align=""center"">\n        <img src=""https://cdn.jsdelivr.net/gh/stdlib-js/stdlib@6970c8b4eb546a10712734d225c15863db9b2c92/docs/assets/readme/assert.png"" alt=""Demo showcasing assertion utilities"">\n    </div>\n\n-   50+ [sample datasets][@stdlib/datasets] for testing and development.\n\n    <div class=""image"" align=""center"">\n        <img src=""https://cdn.jsdelivr.net/gh/stdlib-js/stdlib@f71a38e62247e31dc47d248f6f1b3e434abeb971/docs/assets/readme/datasets.png"" alt=""Demo showcasing sample datasets"">\n    </div>\n\n-   A [plot API][@stdlib/plot/ctor] for data visualization and exploratory data analysis.\n\n    <div class=""image"" align=""center"">\n        <img src=""https://cdn.jsdelivr.net/gh/stdlib-js/stdlib@39d75174b24ea2a84828d9624643776a164478e4/docs/assets/readme/plot.png"" alt=""Demo showcasing plot API"">\n    </div>\n\n-   Native add-ons for interfacing with BLAS libraries, with pure JavaScript fallbacks.\n\n    <div class=""image"" align=""center"">\n        <img src=""https://cdn.jsdelivr.net/gh/stdlib-js/stdlib@efede6af3ef957da08838903b0558441263adf85/docs/assets/readme/base_blas.png"" alt=""Demo showcasing BLAS APIs"">\n    </div>\n\n-   A [benchmark framework][@stdlib/bench/harness] supporting TAP.\n\n    <div class=""image"" align=""center"">\n        <img src=""https://cdn.jsdelivr.net/gh/stdlib-js/stdlib@4833049f8d1895585bd51ec6fa97b8ca0d37c6fb/docs/assets/readme/benchmark.gif"" alt=""Demo showcasing benchmark framework"">\n    </div>\n\n-   REPL environment with integrated help and examples.\n\n    <div class=""image"" align=""center"">\n        <img src=""https://cdn.jsdelivr.net/gh/stdlib-js/stdlib@3864ae6f86bbc215956c0e667d82d49a6eaca780/docs/assets/readme/repl.gif"" alt=""Demo showcasing REPL environment"">\n    </div>\n\n-   Can be bundled using [Browserify][browserify], [Webpack][webpack], and other bundlers for use in web browsers.\n\n    <div class=""image"" align=""center"">\n        <img src=""https://cdn.jsdelivr.net/gh/stdlib-js/stdlib@e54894a93697653dda22d11cd0aec1ccb292b7b8/docs/assets/readme/bundled.png"" alt=""Demo showcasing browser support"">\n    </div>\n\n-   Every function is accompanied by [TypeScript][typescript] declaration files, ensuring type safety and facilitating intelligent code completion in IDEs.\n\n    <div class=""image"" align=""center"">\n        <img src=""https://cdn.jsdelivr.net/gh/stdlib-js/stdlib@f5f1f915a7178d9bc76a95d34afd799e6092ec3a/docs/assets/readme/typescript.png"" alt=""Demo showcasing TypeScript declaration files"" >\n    </div>\n\n* * *\n\n## Installation\n\nTo accommodate various use cases, stdlib can be used in multiple ways. The preferred method of use depends on your individual use case. We've provided some user stories to help you identify the best approach. 😃\n\nWhile this project's installation instructions defaults to using [npm][npm] for package management, installation via other package managers, such as [yarn][yarn], should be a matter of simply swapping out [npm][npm] commands with those of the relevant package manager.\n\n### User Stories\n\n-   I want to perform **data analysis** and **data science** tasks in JavaScript and Node.js, similar to how I might use Python, Julia, R, and MATLAB.\n\n    -   Install the entire project as a [command-line utility](#install_command_line_utility).\n\n-   I am building a **web application**.\n\n    -   I plan on using [Browserify][browserify], [Webpack][webpack], and other bundlers for use in web browsers.\n\n        -   Install [individual packages](#install_individual_packages). Installing the entire project is likely unnecessary and will lead to slower installation times.\n\n    -   I would like to **vendor** a custom bundle containing various stdlib functionality.\n\n        -   Follow the steps for creating [custom bundles](#install_custom_bundles).\n\n    -   I would like to include stdlib functionality by just using a `script` tag.\n\n        -   I would like to use ES Modules.\n        \n            -   Use an individual package's ES Module [build](#install_env_builds_esm).\n            \n        -   I would like to use a pre-built bundle (possibly via a CDN, such as [unpkg][unpkg] or [jsDelivr][jsdelivr]).\n        \n            -   Install (or consume via a CDN) an individual package's pre-built UMD [browser bundle](#install_env_builds_umd).\n\n    -   I am interested in using a substantial amount of functionality found in a top-level stdlib namespace and don't want to separately install hundreds of individual packages (e.g., if building an on-line calculator application and wanting all of stdlib's math functionality).\n\n        -   Install one or more top-level [namespaces](#install_namespaces). Installing the entire project is likely unnecessary and will lead to slower installation times. Installing a top-level namespace is likely to mean installing functionality which will never be used; however, installing a top-level namespace is likely to be easier and less time-consuming than installing many individual packages separately.\n\n            When bundling, installing a top-level namespace should not be a concern, as individual functionality can still be independently required/imported. Project installation times may, however, be somewhat slower.\n\n-   I am building a [Node.js][node-js] **server application**.\n\n    -   I am interested in using various functionality found in stdlib.\n\n        -   Install [individual packages](#install_individual_packages). Installing the entire project is likely unnecessary and will lead to slower installation times.\n    \n    -   I would like to **vendor** stdlib functionality and avoid dependency trees.\n        \n        -   Install individual package UMD [bundles](#install_env_builds_nodejs).\n\n    -   I am interested in using a _substantial_ amount of functionality found in a top-level stdlib namespace and don't want to separately install hundreds of individual packages.\n\n        -   Install one or more top-level [namespaces](#install_namespaces). Installing the entire project is likely unnecessary and will lead to slower installation times. Installing a top-level namespace is likely to mean installing functionality which will never be used; however, installing a top-level namespace is likely to be easier and less time-consuming than installing many individual packages separately.\n\n-   I am using **Deno**.\n\n    -   Import [individual packages](#install_env_builds_deno) using pre-built Deno builds.\n\n-   I would like to use stdlib functionality in an [Observable][observable] notebook.\n\n    -   Consume a pre-built [browser bundles](#install_env_builds_umd) via a CDN, such as [unpkg][unpkg] or [jsDelivr][jsdelivr].\n\n-   I want to hack at stdlib, possibly even creating **customized** builds to link to platform-specific native libraries (such as Intel's MKL or some other numerical library).\n\n    -   Install the project as a [system library](#install_system_library) by cloning this repository and following the [installation][stdlib-development] instructions as described in the [development guide][stdlib-development].\n\n<a name=""install_complete_library""></a>\n\n### Complete Library\n\nTo install the entire project as a library or application dependency,\n\n<!-- run-disable -->\n\n```bash\n$ npm install @stdlib/stdlib\n```\n\nOnce installed, stdlib packages can be individually required/imported to minimize load times and decrease bundle sizes. For example, to use `require`\n\n```javascript\nvar ndarray = require( '@stdlib/ndarray/array' );\n\nvar arr = ndarray( [ [ 1, 2 ], [ 3, 4 ] ] );\n// returns <ndarray>\n```\n\nand to use `import`\n\n<!-- run-disable -->\n\n```javascript\nimport ndarray from '@stdlib/ndarray/array';\n\nvar arr = ndarray( [ [ 1, 2 ], [ 3, 4 ] ] );\n// returns <ndarray>\n```\n\n<a name=""install_individual_packages""></a>\n\n### Individual Packages\n\nstdlib is designed to allow decomposition of the main project into individual packages which can be independently consumed. Accordingly, users of the project can avoid installing all project functionality and only install the exact functionality they need.\n\nTo install individual packages, replace forward slashes `/` after `@stdlib/` with hyphens `-`. For example,\n\n<!-- run-disable -->\n\n```bash\n$ npm install @stdlib/ndarray-array\n```\n\nOnce installed, individual packages can be required/imported. For example, to use `require`\n\n```javascript\nvar ndarray = require( '@stdlib/ndarray-array' );\n\nvar arr = ndarray( [ [ 1, 2 ], [ 3, 4 ] ] );\n// returns <ndarray>\n```\n\nand to use `import`\n\n<!-- run-disable -->\n\n```javascript\nimport ndarray from '@stdlib/ndarray-array';\n\nvar arr = ndarray( [ [ 1, 2 ], [ 3, 4 ] ] );\n// returns <ndarray>\n```\n\n<a name=""install_namespaces""></a>\n\n### Namespaces\n\nstdlib is comprised of various top-level namespaces (i.e., collections of related functionality united by common themes). For example, to install all math functionality found in the top-level `math` namespace,\n\n<!-- run-disable -->\n\n```bash\n$ npm install @stdlib/math\n```\n\nOnce installed, packages within a top-level namespace can be individually required/imported to minimize load times and decrease bundle sizes. For example, to use `require`\n\n```javascript\nvar sin = require( '@stdlib/math/base/special/sin' );\n\nvar v = sin( 3.14 );\n// returns <number>\n```\n\nand to use `import`\n\n<!-- run-disable -->\n\n```javascript\nimport sin from '@stdlib/math/base/special/sin';\n\nvar v = sin( 3.14 );\n// returns <number>\n```\n\n**Note**: installing nested namespaces found within top-level namespaces (e.g., `math/base`) is **not** supported. Consider installing individual packages or the relevant top-level namespace.\n\n<a name=""install_command_line_utility""></a>\n\n### Command-line Utility\n\nTo install globally for use as a command-line utility and/or use the [REPL][@stdlib/repl],\n\n<!-- run-disable -->\n\n```bash\n$ npm install -g @stdlib/stdlib\n```\n\nwhich will expose the `stdlib` command. For example, to see available sub-commands\n\n<!-- run-disable -->\n\n```bash\n$ stdlib help\n```\n\nand to run the [REPL][@stdlib/repl]\n\n<!-- run-disable -->\n\n```bash\n$ stdlib repl\n```\n\n<a name=""install_env_builds""></a>\n\n### Environment Builds\n\n<a name=""install_env_builds_esm""></a>\n\n#### ES Modules\n\nTo use ES Modules via a `<script>` tag, use **ES Module builds** available in each package's repository via a dedicated `esm` branch (e.g., see the [`esm`][@stdlib/math-base-special-erf-esm] branch for [`@stdlib/math-base-special-erf`][@stdlib/math-base-special-erf-esm]). For example,\n\n<!-- run-disable -->\n\n```html\n<script type=""module"">\nimport linspace from 'https://cdn.jsdelivr.net/gh/stdlib-js/array-base-linspace@esm/index.mjs';\nimport erf from 'https://cdn.jsdelivr.net/gh/stdlib-js/math-base-special-erf@esm/index.mjs';\n\nconst x = linspace( -10.0, 10.0, 100 );\n\nfor ( let i = 0; i < x.length; i++ ) {\n    console.log( 'x: %d, erf(x): %d', x[ i ], erf( x[ i ] ) );\n}\n</script>\n```\n\n<a name=""install_env_builds_deno""></a>\n\n#### Deno\n\nTo use individual packages in Deno, use **Deno builds** available in each package's repository via a dedicated `deno` branch (e.g., see the [`deno`][@stdlib/ndarray-array-deno] branch for [`@stdlib/ndarray-array`][@stdlib/ndarray-array-deno]). For example,\n\n<!-- run-disable -->\n\n```javascript\nimport ndarray from 'https://cdn.jsdelivr.net/gh/stdlib-js/ndarray-array@deno/mod.js';\n\nvar arr = ndarray( [ [ 1, 2 ], [ 3, 4 ] ] );\n// returns <ndarray>\n````\n\n\n<a name=""install_env_builds_jquery""></a>\n\n#### jQuery-like Bundle\n\nFor those wanting a jQuery-like bundle, one can use pre-built distributable UMD bundles for use in browser environments or as shared (""vendored"") libraries in server environments available in each package's repository via a dedicated `umd` branch. See sections [UMD](#install_env_builds_umd) and [Node.js](#install_env_builds_nodejs) for more details.\n\n<a name=""install_env_builds_umd""></a>\n\n#### UMD\n\nTo use UMD bundles either via a `<script>` tag or in [Observable][observable], use UMD **browser builds** available in each package's repository via a dedicated `umd` branch (e.g., see the [`umd`][@stdlib/math-base-special-erf-umd] branch for [`@stdlib/math-base-special-erf`][@stdlib/math-base-special-erf-umd]). For example,\n\n<!-- run-disable -->\n\n```html\n<script type=""text/javascript"" src=""https://cdn.jsdelivr.net/gh/stdlib-js/array-base-linspace@umd/browser.js""></script>\n<script type=""text/javascript"" src=""https://cdn.jsdelivr.net/gh/stdlib-js/math-base-special-erf@umd/browser.js""></script>\n<script type=""text/javascript"">\n(function () {\n\nvar x = linspace( -10.0, 10.0, 100 );\n\nfor ( var i = 0; i < x.length; i++ ) {\n    console.log( 'x: %d, erf(x): %d', x[ i ], erf( x[ i ] ) );\n}\n\n})();\n</script>\n```\n\n<a name=""install_env_builds_nodejs""></a>\n\n#### Node.js\n\nTo **vendor** stdlib functionality and avoid installing dependency trees, use UMD **server builds** available in each package's repository via a dedicated `umd` branch (e.g., see the [`umd`][@stdlib/math-base-special-erf-umd] branch for [`@stdlib/math-base-special-erf`][@stdlib/math-base-special-erf-umd]). For example,\n\n<!-- run-disable -->\n\n```javascript\nvar linspace = require( '/path/to/vendor/umd/@stdlib/array-base-linspace' );\nvar erf = require( '/path/to/vendor/umd/@stdlib/math-base-special-erf' );\n\nvar x = linspace( -10.0, 10.0, 100 );\n\nfor ( var i = 0; i < x.length; i++ ) {\n    console.log( 'x: %d, erf(x): %d', x[ i ], erf( x[ i ] ) );\n}\n```\n\n<a name=""install_custom_bundles""></a>\n\n### Custom Bundles\n\nTo create a custom bundle based on project needs,\n\n1.  follow the [download][stdlib-development], [configuration][stdlib-development], and [installation][stdlib-development] instructions as described in the [development guide][stdlib-development].\n\n2.  navigate to the local installation directory.\n\n3.  run the following command to print help documentation for providing a list of stdlib package names to bundle\n\n    <!-- run-disable -->\n\n    ```bash\n    $ NODE_PATH=./lib/node_modules node ./bin/cli bundle-pkg-list -- -h\n    ```\n\n4.  modify and run the above command with the list of packages to bundle\n\n    <!-- run-disable -->\n\n    ```bash\n    $ NODE_PATH=./lib/node_modules node ./bin/cli bundle-pkg-list -- <pkg> <pkg> <pkg> ...\n    ```\n\n<!-- FIXME: the following is not possible atm as we don't publish `@stdlib/_tools` which is needed in order for the command-line utility to work!\n\nAlternatively, install stdlib as a command-line utility (as described above) and run the following command\n-->\n\n<!-- run-disable -->\n\n<!--\n```bash\n$ stdlib bundle-pkg-list -- <pkg> <pkg> <pkg> ...\n```\n-->\n\nUpon generating a bundle, the bundle can be loaded via a `<script>` tag as described above for pre-built distributable UMD bundles.\n\n<a name=""install_system_library""></a>\n\n### System Library\n\nTo install as a system library (e.g., for the purposes of creating custom builds), follow the [download][stdlib-development], [configuration][stdlib-development], and [installation][stdlib-development] instructions as described in the [development guide][stdlib-development].\n\n* * *\n\n## Prerequisites\n\nInstalling and running stdlib for use in [Node.js][node-js] **requires** the following prerequisites:\n\n-   [Node.js][node-js]: JavaScript runtime (version `>= 0.10`)\n-   [npm][npm]: package manager (version `> 2.7.0`; if Node `< 1.0.0`, version `> 2.7.0` and `< 4.0.0`; if Node `<= 10.x.x`, version `> 2.7.0` and `< 6.0.0`)\n\nMost functionality in stdlib is implemented in JavaScript and no further prerequisites are required to use stdlib (i.e., you can safely avoid installing any additional prerequisites); however, some implementations try to capture performance benefits by using [native bindings][node-js-add-ons] and/or [WebAssembly][webassembly]. While **not** required to run stdlib, as **every** stdlib implementation has a JavaScript fallback, the following dependencies are **required** for building native add-ons, including linking to BLAS and LAPACK libraries:\n\n-   [GNU make][make]: development utility and task runner\n-   [GNU bash][bash]: an sh-compatible shell\n-   [gcc & g++][gcc] or [Clang][clang]: C/C++ compilation and linking (g++ version `>= 4.8`; clang version `>= 3.5`, Xcode version `>=8.3.1` on OS X)\n-   [gfortran][gfortran]: Fortran compilation and linking (version `>= 4.8`)\n\nWhile **not** required to run stdlib, the following dependencies are **required** for automatically downloading external libraries:\n\n-   [curl][curl], [wget][wget], or [fetch][fetch] (FreeBSD): utilities for downloading remote resources\n\nThe following external libraries can be automatically downloaded and compiled from source using `make`:\n\n-   [OpenBLAS][openblas]: optimized BLAS library\n-   [Electron][electron]: framework for cross-platform desktop applications\n\n* * *\n\n## Contributing\n\nFirst time contributor?\n\n-   See the [contributing guidelines][stdlib-contributing].\n\nAlready an expert?\n\n-   Fork the repository.\n\n-   Clone the forked repository\n\n    ```bash\n    $ git clone --depth=1 https://github.com/<username>/stdlib.git\n    ```\n\n    where `<username>` is your GitHub username.\n\n-   Navigate to the `stdlib` directory\n\n    ```bash\n    $ cd stdlib\n    ```\n\n-   Install dependencies\n\n    ```bash\n    $ make install-node-modules\n    ```\n\n-   Initialize your stdlib development environment\n\n    ```bash\n    $ make init\n    ```\n\n<!-- Project sponsors. If sponsors are included, add a horizontal rule *before* the section. Make sure to keep an empty line after the `section` element and another before the `/section` close. -->\n\n* * *\n\n<section class=""sponsors"">\n\n## Sponsors\n\nstdlib development is generously supported by the following sponsors:\n\n<div class=""image"" align=""center"">\n    <br>\n    <a href=""https://labs.quansight.org/"">\n        <img src=""https://cdn.jsdelivr.net/gh/stdlib-js/stdlib@2719e1d3ecab2cc29985bca35fd33594e65adb55/docs/assets/sponsors/quansight_labs_logo.png"" alt=""Quansight Labs"">\n    </a>\n    <br>\n    <br>\n</div>\n\nAre you interested in supporting stdlib? If so, join our [Open Collective][open-collective-stdlib]!\n\n</section>\n\n<!-- /.sponsors -->\n\n<!-- Project users. If users are included, add a horizontal rule *before* the section. Make sure to keep an empty line after the `section` element and another before the `/section` close. -->\n\n* * *\n\n<section class=""users"">\n\n## Users\n\nThe following organizations and key stakeholders trust and rely on stdlib:\n\n<div class=""image"" align=""center"">\n    <br>\n    <a href=""https://www.cmu.edu/"">\n        <img src=""https://cdn.jsdelivr.net/gh/stdlib-js/stdlib@1c8c7dbc9d081eeb13e16c62764f27a65c6553f8/docs/assets/misc/cmu_logo.png"" alt=""Carnegie Mellon University"">\n    </a>\n    <br>\n    <br>\n</div>\n\nDoes your organization use stdlib? If so, we'd love to hear from you!\n\n</section>\n\n<!-- /.users -->\n\n* * *\n\n## Governance\n\nFor information about the governance of the stdlib project, see [GOVERNANCE.md][stdlib-governance].\n\n## License\n\nSee [LICENSE][stdlib-license].\n\n## Copyright\n\nCopyright © 2016-2024. The Stdlib [Authors][stdlib-authors].\n\n</section>\n\n<!-- /.intro -->\n\n<!-- Project badges. If badges are included, add a horizontal rule *before* the section. Make sure to keep an empty line after the `section` element and another before the `/section` close. -->\n\n* * *\n\n<section class=""badges"">\n\n## Status\n\n#### Version\n\n<!-- lint disable no-paragraph-content-indent -->\n\n[![git tag][tag-image]][tag-url] [![NPM version][npm-image]][npm-url] [![Node.js version][node-image]][node-url]\n\n<!-- lint enable no-paragraph-content-indent -->\n\n<!-- #### Build -->\n\n<!-- TODO: distinguish between Linux and Windows code coverage -->\n\n<!-- lint disable table-pipe-alignment -->\n\n<!-- | OS         | Build (master)                                                                           | Coverage (master)                                                  | Build (develop)                                                                             | Coverage (develop)                                                    |\n| ---------- | ---------------------------------------------------------------------------------------- | ------------------------------------------------------------------ | ------------------------------------------------------------------------------------------- | --------------------------------------------------------------------- |\n| Linux/OS X | [![Linux/OS X build status (master)][build-image-master]][build-url-master]              | [![coverage (master)][coverage-image-master]][coverage-url-master] | [![Linux/OS X build status (develop)][build-image-develop]][build-url-develop]              | [![coverage (develop)][coverage-image-develop]][coverage-url-develop] |\n| Windows    | [![Windows build status (master)][windows-build-image-master]][windows-build-url-master] | [![coverage (master)][coverage-image-master]][coverage-url-master] | [![Windows build status (develop)][windows-build-image-develop]][windows-build-url-develop] | [![coverage (develop)][coverage-image-develop]][coverage-url-develop] | -->\n\n<!-- lint enable table-pipe-alignment -->\n\n<!-- #### Dependencies -->\n\n<!-- lint disable no-paragraph-content-indent -->\n\n<!-- [![Dependencies][dependencies-image]][dependencies-url] [![DevDependencies][dev-dependencies-image]][dev-dependencies-url] -->\n\n<!-- lint enable no-paragraph-content-indent -->\n\n#### Community\n\n[![Chat][chat-image]][chat-url]\n\n</section>\n\n<!-- /.badges> -->\n\n<!-- Project acknowledgments. If section is included, add a horizontal rule *before* the section. Make sure to keep an empty line after the `section` element and another before the `/section` close. -->\n\n<!-- * * *\n\n<section class=""acknowledgments"">\n\n## Acknowledgments\n\n### Build Infrastructure\n\nTest and build infrastructure is generously provided by the following services:\n\n<div class=""image"" align=""center"">\n    <img src=""https://cdn.jsdelivr.net/gh/stdlib-js/stdlib@3de52540666d1635df046d7e5dd07a1fc5b87d85/docs/assets/misc/ci_logo_banner.svg"" alt=""Continuous Integration Service Logos"">\n    <br>\n</div>\n\n</section> -->\n\n<!-- /.acknowledgments -->\n\n<!-- Section for all links. Make sure to keep an empty line after the `section` element and another before the `/section` close. -->\n\n<section class=""links"">\n\n[npm-image]: https://img.shields.io/npm/v/@stdlib/stdlib.svg\n\n[npm-url]: https://npmjs.com/package/@stdlib/stdlib\n\n[tag-image]: https://img.shields.io/github/v/tag/stdlib-js/stdlib.svg\n\n[tag-url]: https://github.com/stdlib-js/stdlib/tags\n\n[node-image]: https://img.shields.io/node/v/@stdlib/stdlib.svg\n\n[node-url]: https://github.com/stdlib-js/stdlib\n\n<!-- [build-image-develop]: https://img.shields.io/travis/stdlib-js/stdlib/develop.svg\n\n[build-url-develop]: https://travis-ci.org/stdlib-js/stdlib\n\n[coverage-image-develop]: https://img.shields.io/codecov/c/github/stdlib-js/stdlib/develop.svg\n\n[coverage-url-develop]: https://codecov.io/github/stdlib-js/stdlib/branch/develop\n\n[dependencies-image]: https://img.shields.io/david/stdlib-js/stdlib\n\n[dependencies-url]: https://socket.dev/npm/package/@stdlib/stdlib/dependencies\n\n[dev-dependencies-image]: https://img.shields.io/david/dev/stdlib-js/stdlib\n\n[dev-dependencies-url]: https://socket.dev/npm/package/@stdlib/stdlib/dependencies -->\n\n[chat-image]: https://img.shields.io/gitter/room/stdlib-js/stdlib.svg\n\n[chat-url]: https://app.gitter.im/#/room/#stdlib-js_stdlib:gitter.im\n\n[make]: https://www.gnu.org/software/make/\n\n[bash]: https://www.gnu.org/software/bash/\n\n[curl]: https://curl.se/\n\n[wget]: https://www.gnu.org/software/wget/\n\n[fetch]: https://www.freebsd.org/cgi/man.cgi?fetch%281%29\n\n[node-js]: https://nodejs.org/en/\n\n[npm]: https://www.npmjs.com/\n\n[yarn]: https://yarnpkg.com/\n\n[gcc]: http://gcc.gnu.org/\n\n[clang]: https://clang.llvm.org/\n\n[gfortran]: https://gcc.gnu.org/fortran/\n\n[openblas]: https://github.com/xianyi/OpenBLAS\n\n[electron]: https://www.electronjs.org/\n\n[webassembly]: https://webassembly.org/\n\n[node-js-add-ons]: https://nodejs.org/api/addons.html\n\n[browserify]: https://github.com/substack/node-browserify\n\n[webpack]: https://webpack.js.org/\n\n[typescript]: https://www.typescriptlang.org/\n\n[unpkg]: https://unpkg.com/#/\n\n[jsdelivr]: https://www.jsdelivr.com/\n\n[observable]: https://observablehq.com/\n\n[ipa-english]: https://en.wikipedia.org/wiki/Help:IPA/English\n\n[stdlib-contributing]: https://github.com/stdlib-js/stdlib/blob/develop/CONTRIBUTING.md\n\n[stdlib-development]: https://github.com/stdlib-js/stdlib/blob/develop/docs/development.md\n\n[stdlib-authors]: https://github.com/stdlib-js/stdlib/graphs/contributors\n\n[stdlib-license]: https://raw.githubusercontent.com/stdlib-js/stdlib/develop/LICENSE\n\n[stdlib-governance]: https://raw.githubusercontent.com/stdlib-js/stdlib/develop/GOVERNANCE.md\n\n[stdlib-homepage]: https://stdlib.io\n\n[stdlib-documentation]: https://stdlib.io/docs/api\n\n[stdlib-faq]: https://github.com/stdlib-js/stdlib/blob/develop/FAQ.md\n\n[stdlib-source]: https://github.com/stdlib-js/stdlib/tree/develop/lib/node_modules/%40stdlib\n\n[stdlib-code-coverage]: https://codecov.io/github/stdlib-js/stdlib/branch/develop\n\n[open-collective-stdlib]: https://opencollective.com/stdlib\n\n[stdlib-twitter]: https://twitter.com/stdlibjs\n\n[stdlib-gitter]: https://gitter.im/stdlib-js/stdlib\n\n[@stdlib/math/base/special]: https://github.com/stdlib-js/stdlib/tree/develop/lib/node_modules/%40stdlib/math/base/special\n\n[@stdlib/stats/base/dists]: https://github.com/stdlib-js/stdlib/tree/develop/lib/node_modules/%40stdlib/stats/base/dists\n\n[@stdlib/random/base]: https://github.com/stdlib-js/stdlib/tree/develop/lib/node_modules/%40stdlib/random/base\n\n[@stdlib/assert]: https://github.com/stdlib-js/stdlib/tree/develop/lib/node_modules/%40stdlib/assert\n\n[@stdlib/datasets]: https://github.com/stdlib-js/stdlib/tree/develop/lib/node_modules/%40stdlib/datasets\n\n[@stdlib/utils]: https://github.com/stdlib-js/stdlib/tree/develop/lib/node_modules/%40stdlib/utils\n\n[@stdlib/plot/ctor]: https://github.com/stdlib-js/stdlib/tree/develop/lib/node_modules/%40stdlib/plot/ctor\n\n[@stdlib/bench/harness]: https://github.com/stdlib-js/stdlib/tree/develop/lib/node_modules/%40stdlib/bench/harness\n\n[@stdlib/repl]: https://github.com/stdlib-js/stdlib/tree/develop/lib/node_modules/%40stdlib/repl\n\n[@stdlib/ndarray-array-deno]: https://github.com/stdlib-js/ndarray-array/tree/deno\n\n[@stdlib/math-base-special-erf-esm]: https://github.com/stdlib-js/math-base-special-erf/tree/esm\n\n[@stdlib/math-base-special-erf-umd]: https://github.com/stdlib-js/math-base-special-erf/tree/umd\n\n</section>\n\n<!-- /.links -->\n",4207,mathematics,JavaScript,13,JavaScript,Julia,C++,C,Awk,HTML,Shell,CSS,Python,R,Fortran,WebAssembly,TypeScript,,,,,,,,,,,,,,,,1652,180,1373,99,20,92,2,1962228,412,952,589,363,c89a171db9b4664582fab09dc95a5cc3b58f9f21,docs: update REPL namespace documentation,2024-07-19T09:46:27Z,stdlib-bot,82920195+stdlib-bot@users.noreply.github.com,stdlib-bot,v0.2.0,"## New features, refactorings, and bug fixes\r\n\r\nThis release has several breaking changes. For those migrating from v0.1.0 and earlier, you are advised to explicitly confirm whether your API usage is still supported. For most use cases, everything should work the same; however, for some APIs, we've reorganized implementations and refactored existing implementations to adopt new API signatures.",v0.2.0,Athan,,kgryte,Apache License 2.0,stdlib,stdlib-js,4,javascript,stdlib,nodejs,numeric,library,standard,math,js,mathematics,science,scientific,scientific-computing,lib,node-js,node,statistics,stats,utilities,utils,numerical-computing,/stdlib-js/stdlib,98,55,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/stardist/stardist,https://github.com/stardist/stardist,1,,,1,1,1,1,0,0,0,0,0,0,1,StarDist - Object Detection with Star-convex Shapes,"[![PyPI version](https://badge.fury.io/py/stardist.svg)](https://pypi.org/project/stardist)\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/stardist/badges/version.svg)](https://anaconda.org/conda-forge/stardist)\n[![Test](https://github.com/stardist/stardist/workflows/Test/badge.svg)](https://github.com/stardist/stardist/actions?query=workflow%3ATest)\n[![Test (PyPI)](https://github.com/stardist/stardist/workflows/Test%20(PyPI)/badge.svg)](https://github.com/stardist/stardist/actions?query=workflow%3A%22Test+%28PyPI%29%22)\n[![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&url=https%3A%2F%2Fforum.image.sc%2Ftags%2Fstardist.json&query=%24.topic_list.tags.0.topic_count&colorB=brightgreen&suffix=%20topics&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tags/stardist)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/stardist)](https://pypistats.org/packages/stardist)\n\n# *StarDist* - Object Detection with Star-convex Shapes\n\n![](https://github.com/stardist/stardist/raw/main/images/stardist_overview.png)\n\nThis repository contains the Python implementation of star-convex object detection for 2D and 3D images, as described in the papers:\n\n<img src=""https://github.com/stardist/stardist/raw/main/images/stardist_logo.jpg"" title=""siân is the king of the universe"" width=""25%"" align=""right"">\n\n- Uwe Schmidt, Martin Weigert, Coleman Broaddus, and Gene Myers.  \n[*Cell Detection with Star-convex Polygons*](https://arxiv.org/abs/1806.03535).  \nInternational Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), Granada, Spain, September 2018.\n\n- Martin Weigert, Uwe Schmidt, Robert Haase, Ko Sugawara, and Gene Myers.  \n[*Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy*](http://openaccess.thecvf.com/content_WACV_2020/papers/Weigert_Star-convex_Polyhedra_for_3D_Object_Detection_and_Segmentation_in_Microscopy_WACV_2020_paper.pdf).  \nThe IEEE Winter Conference on Applications of Computer Vision (WACV), Snowmass Village, Colorado, March 2020.\n\n- Martin Weigert and Uwe Schmidt.  \n[*Nuclei Instance Segmentation and Classification in Histopathology Images with Stardist*](https://arxiv.org/abs/2203.02284).  \nThe IEEE International Symposium on Biomedical Imaging Challenges (ISBIC), Kolkata, India, March 2022.\n\nPlease [cite the paper(s)](#how-to-cite) if you are using this code in your research.\n\n\n## Overview\n\nThe following figure illustrates the general approach for 2D images. The training data consists of corresponding pairs of input (i.e. raw) images and fully annotated label images (i.e. every pixel is labeled with a unique object id or 0 for background).\nA model is trained to densely predict the distances (r) to the object boundary along a fixed set of rays and object probabilities (d), which together produce an overcomplete set of candidate polygons for a given input image. The final result is obtained via non-maximum suppression (NMS) of these candidates.\n\n![](https://github.com/stardist/stardist/raw/main/images/overview_2d.png)\n\nThe approach for 3D volumes is similar to the one described for 2D, using pairs of input and fully annotated label volumes as training data.\n\n![](https://github.com/stardist/stardist/raw/main/images/overview_3d.png)\n\n## Webinar/Tutorial\n\nIf you want to know more about the concepts and practical applications of StarDist, please have a look at the following webinar that was given at NEUBIAS Academy @Home 2020:\n\n[![webinar video](http://img.youtube.com/vi/Amn_eHRGX5M/0.jpg)](http://www.youtube.com/watch?v=Amn_eHRGX5M ""Webinar"")\n\n\n## Installation\n\nThis package is compatible with Python 3.6 - 3.12.\n\nIf you only want to use a StarDist plugin for a GUI-based software, please read [this](#plugins-for-other-software).\n\n1. Please first [install TensorFlow](https://www.tensorflow.org/install)\n(either TensorFlow 1 or 2) by following the official instructions.\nFor [GPU support](https://www.tensorflow.org/install/gpu), it is very\nimportant to install the specific versions of CUDA and cuDNN that are\ncompatible with the respective version of TensorFlow. (If you need help and can use `conda`, take a look at [this](https://github.com/CSBDeep/CSBDeep/tree/main/extras#conda-environment).)\n\n2. *StarDist* can then be installed with `pip`:\n\n    - If you installed TensorFlow 2 (version *2.x.x*):\n\n          pip install stardist\n\n    - If you installed TensorFlow 1 (version *1.x.x*):\n\n          pip install ""stardist[tf1]""\n\n\n#### Notes\n\n- Depending on your Python installation, you may need to use `pip3` instead of `pip`.\n- You can find out which version of TensorFlow is installed via `pip show tensorflow`.\n- We provide pre-compiled binaries (""wheels"") that should work for most Linux, Windows, and macOS platforms. If you're having problems, please see the [troubleshooting](#installation-1) section below.\n- *(Optional)* You need to install [gputools](https://github.com/maweigert/gputools) if you want to use OpenCL-based computations on the GPU to speed up training.\n- *(Optional)* You might experience improved performance during training if you additionally install the [Multi-Label Anisotropic 3D Euclidean Distance Transform (MLAEDT-3D)](https://github.com/seung-lab/euclidean-distance-transform-3d).\n\n\n## Usage\n\nWe provide example workflows for 2D and 3D via Jupyter [notebooks](https://github.com/stardist/stardist/tree/main/examples) that illustrate how this package can be used.\n\n![](https://github.com/stardist/stardist/raw/main/images/example_steps.png)\n\n### Pretrained Models for 2D\n\nCurrently we provide some pretrained models in 2D that might already be suitable for your images:\n\n\n| key | Modality (Staining) | Image format | Example Image    | Description  |\n| :-- | :-: | :-:| :-:| :-- |\n| `2D_versatile_fluo` `2D_paper_dsb2018`| Fluorescence (nuclear marker) | 2D single channel| <img src=""https://github.com/stardist/stardist/raw/main/images/example_fluo.jpg"" title=""example image fluo"" width=""120px"" align=""center"">       | *Versatile (fluorescent nuclei)* and *DSB 2018 (from StarDist 2D paper)* that were both trained on a [subset of the DSB 2018 nuclei segmentation challenge dataset](https://github.com/stardist/stardist/releases/download/0.1.0/dsb2018.zip). |\n|`2D_versatile_he` | Brightfield (H&E) | 2D RGB  | <img src=""https://github.com/stardist/stardist/raw/main/images/example_histo.jpg"" title=""example image histo"" width=""120px"" align=""center"">       | *Versatile (H&E nuclei)* that was trained on images from the [MoNuSeg 2018 training data](https://monuseg.grand-challenge.org/Data/) and the [TNBC dataset from Naylor et al. (2018)](https://zenodo.org/record/1175282#.X6mwG9so-CN). |\n\n\nYou can access these pretrained models from `stardist.models.StarDist2D`\n\n```python\nfrom stardist.models import StarDist2D\n\n# prints a list of available models\nStarDist2D.from_pretrained()\n\n# creates a pretrained model\nmodel = StarDist2D.from_pretrained('2D_versatile_fluo')\n```\n\nAnd then try it out with a test image:\n\n```python\nfrom stardist.data import test_image_nuclei_2d\nfrom stardist.plot import render_label\nfrom csbdeep.utils import normalize\nimport matplotlib.pyplot as plt\n\nimg = test_image_nuclei_2d()\n\nlabels, _ = model.predict_instances(normalize(img))\n\nplt.subplot(1,2,1)\nplt.imshow(img, cmap=""gray"")\nplt.axis(""off"")\nplt.title(""input image"")\n\nplt.subplot(1,2,2)\nplt.imshow(render_label(labels, img=img))\nplt.axis(""off"")\nplt.title(""prediction + input overlay"")\n```\n\n![](images/pretrained_example.png)\n\n\n### Annotating Images\n\nTo train a *StarDist* model you will need some ground-truth annotations: for every raw training image there has to be a corresponding label image where all pixels of a cell region are labeled with a distinct integer (and background pixels are labeled with 0).\nTo create such annotations in 2D, there are several options, among them being [Fiji](http://fiji.sc/), [Labkit](https://imagej.net/Labkit), or [QuPath](https://qupath.github.io). In 3D, there are fewer options: [Labkit](https://github.com/maarzt/imglib2-labkit) and [Paintera](https://github.com/saalfeldlab/paintera) (the latter being very sophisticated but having a steeper learning curve).\n\nAlthough each of these provide decent annotation tools, we currently recommend using Labkit (for 2D or 3D images) or QuPath (for 2D):\n\n#### Annotating with LabKit (2D or 3D)\n\n1. Install [Fiji](https://fiji.sc) and the [Labkit](https://imagej.net/Labkit) plugin\n2. Open the (2D or 3D) image and start Labkit via `Plugins > Labkit > Open Current Image With Labkit`\n3. Successively add a new label and annotate a single cell instance with the brush tool until *all* cells are labeled.  \n   (Always disable `allow overlapping labels` or – in older versions of LabKit – enable the `override` option.) \n4. Export the label image via `Labeling > Save Labeling ...` with `Files of Type > TIF Image` making sure that the file name ends with `.tif` or `.tiff`.\n\n![](https://github.com/stardist/stardist/raw/main/images/labkit_2d_labkit.png)\n\n\nAdditional tips:\n\n* The Labkit viewer uses [BigDataViewer](https://imagej.net/BigDataViewer) and its keybindings (e.g. <kbd>s</kbd> for contrast options, <kbd>CTRL</kbd>+<kbd>Shift</kbd>+<kbd>mouse-wheel</kbd> for zoom-in/out etc.)\n* For 3D images (XYZ) it is best to first convert it to a (XYT) timeseries (via `Re-Order Hyperstack` and swapping `z` and `t`) and then use <kbd>[</kbd> and <kbd>]</kbd> in Labkit to walk through the slices.\n\n#### Annotating with QuPath (2D)\n\n1. Install [QuPath](https://qupath.github.io/)\n2. Create a new project (`File -> Project...-> Create project`) and add your raw images\n3. Annotate nuclei/objects\n4. Run [this script](https://raw.githubusercontent.com/stardist/stardist/main/extras/qupath_export_annotations.groovy) to export the annotations (save the script and drag it on QuPath. Then execute it with `Run for project`). The script will create a `ground_truth` folder within your QuPath project that includes both the `images` and `masks` subfolder that then can directly be used with *StarDist*.\n\nTo see how this could be done, have a look at the following [example QuPath project](https://raw.githubusercontent.com/stardist/stardist/main/extras/qupath_example_project.zip) (data courtesy of Romain Guiet, EPFL).\n\n![](https://github.com/stardist/stardist/raw/main/images/qupath.png)\n\n\n### Multi-class Prediction\n\nStarDist also supports multi-class prediction, i.e. each found object instance can additionally be classified into a fixed number of discrete object classes (e.g. cell types):\n\n![](https://github.com/stardist/stardist/raw/main/images/stardist_multiclass.png)\n\nPlease see the [multi-class example notebook](https://nbviewer.jupyter.org/github/stardist/stardist/blob/main/examples/other2D/multiclass.ipynb) if you're interested in this.\n\n## Instance segmentation metrics\n\nStarDist contains the `stardist.matching` submodule that provides functions to compute common instance segmentation metrics between ground-truth label masks and predictions (not necessarily from StarDist). Currently available metrics are\n\n* `tp`, `fp`, `fn`\n* `precision`, `recall`, `accuracy`, `f1`\n* `panoptic_quality`\n* `mean_true_score`, `mean_matched_score`\n\nwhich are computed by matching ground-truth/prediction objects if their IoU exceeds a threshold (by default 50%). See the documentation of `stardist.matching.matching` for a detailed explanation.\n\nHere is an example how to use it:\n\n```python\n\n# create some example ground-truth and dummy prediction data\nfrom stardist.data import test_image_nuclei_2d\nfrom scipy.ndimage import rotate\n_, y_true = test_image_nuclei_2d(return_mask=True)\ny_pred = rotate(y_true, 2, order=0, reshape=False)\n\n# compute metrics between ground-truth and prediction\nfrom stardist.matching import matching\n\nmetrics =  matching(y_true, y_pred)\n\nprint(metrics)\n```\n```\nMatching(criterion='iou', thresh=0.5, fp=88, tp=37, fn=88, precision=0.296, \n       recall=0.296, accuracy=0.1737, f1=0.296, n_true=125, n_pred=125, \n       mean_true_score=0.19490, mean_matched_score=0.65847, panoptic_quality=0.19490)\n```\n\nIf you want to compare a list of images you can use `stardist.matching.matching_dataset`:\n\n```python\n\nfrom stardist.matching import matching_dataset\n\nmetrics = matching_dataset([y_true, y_true], [y_pred, y_pred])\n\nprint(metrics)\n```\n```\nDatasetMatching(criterion='iou', thresh=0.5, fp=176, tp=74, fn=176, precision=0.296, \n    recall=0.296, accuracy=0.1737, f1=0.296, n_true=250, n_pred=250, \n    mean_true_score=0.19490, mean_matched_score=0.6584, panoptic_quality=0.1949, by_image=False)\n```\n\n\n\n## Troubleshooting & Support\n\n1. Please first take a look at the [frequently asked questions (FAQ)]( https://stardist.net/docs/faq.html).\n2. If you need further help, please go to the [image.sc forum](https://forum.image.sc) and try to find out if the issue you're having has already been discussed or solved by other people. If not, feel free to create a new topic there and make sure to use the tag `stardist` (we are monitoring all questions with this tag). When opening a new topic, please provide a clear and concise description to understand and ideally reproduce the issue you're having (e.g. including a code snippet, Python script, or Jupyter notebook).\n3. If you have a technical question related to the source code or believe to have found a bug, feel free to [open an issue](https://github.com/stardist/stardist/issues), but please check first if someone already created a similar issue.\n\n### Installation\n\nIf `pip install stardist` fails, it could be because there are no compatible wheels (`.whl`) for your platform ([see list](https://pypi.org/project/stardist/#files)). In this case, `pip` tries to compile a C++ extension that our Python package relies on (see below). While this often works on Linux out of the box, it will likely fail on Windows and macOS without installing a suitable compiler. (Note that you can enforce compilation by installing via `pip install stardist --no-binary :stardist:`.)\n\nInstallation without using wheels requires Python 3.6 (or newer) and a working C++ compiler. We have only tested [GCC](http://gcc.gnu.org) (macOS, Linux), [Clang](https://clang.llvm.org) (macOS), and [Visual Studio](https://visualstudio.microsoft.com) (Windows 10). Please [open an issue](https://github.com/stardist/stardist/issues) if you have problems that are not resolved by the information below.\n\nIf available, the C++ code will make use of [OpenMP](https://en.wikipedia.org/wiki/OpenMP) to exploit multiple CPU cores for substantially reduced runtime on modern CPUs. This can be important to prevent slow model training.\n\n#### macOS\nThe default C/C++ compiler Clang that comes with the macOS command line tools (installed via `xcode-select --install`) does not support OpenMP out of the box, but it can be added. Alternatively, a suitable compiler can be installed from [conda-forge](https://conda-forge.org). Please see this [detailed guide](https://scikit-learn.org/stable/developers/advanced_installation.html#macos)  for more information on both strategies (although written for [scikit-image](https://scikit-learn.org), it also applies here).\n\nA third alternative (and what we did until StarDist 0.8.1) is to install the OpenMP-enabled GCC compiler via [Homebrew](https://brew.sh) with `brew install gcc` (e.g. installing `gcc-12`/`g++-12` or newer). After that, you can build the package like this (adjust compiler names/paths as necessary):\n\n    CC=gcc-12 CXX=g++-12 pip install stardist\n\nIf you use `conda` on macOS and after `import stardist` see errors similar to `Symbol not found: _GOMP_loop_nonmonotonic_dynamic_next`, please see [this issue](https://github.com/stardist/stardist/issues/19#issuecomment-535610758) for a temporary workaround.\n\nIf you encounter an `ImportError: dlopen(...): symbol not found in flat namespace ...` error on `import stardist`, you may try to install it like so:\n\n```\nbrew install libomp\n\nexport HOMEBREW_PREFIX=/opt/homebrew #set to your homebrew prefix\nexport CPPFLAGS=""$CPPFLAGS -Xpreprocessor -fopenmp""\nexport CFLAGS=""$CFLAGS -I/usr/local/opt/libomp/include""\nexport CXXFLAGS=""$CXXFLAGS -I/usr/local/opt/libomp/include""\nexport LDFLAGS=""$LDFLAGS -Wl,-rpath,/usr/local/opt/libomp/lib -L/usr/local/opt/libomp/lib -lomp""\n\npip install stardist --no-binary :all:\n```\n\n##### Apple Silicon\n\nAs of StarDist 0.8.2, we provide `arm64` wheels that should work with [macOS on Apple Silicon](https://support.apple.com/en-us/HT211814) (M1 chip or newer). \nWe recommend setting up an `arm64` `conda` environment with GPU-accelerated TensorFlow following [Apple's instructions](https://developer.apple.com/metal/tensorflow-plugin/) (ensure you are using macOS 12 Monterey or newer) using [conda-forge miniforge3 or mambaforge](https://github.com/conda-forge/miniforge). Then install `stardist` using `pip`.\n```\nconda create -y -n stardist-env python=3.9   \nconda activate stardist-env\nconda install -c apple tensorflow-deps\npip install tensorflow-macos tensorflow-metal\npip install stardist\n```\n\n#### Windows\nPlease install the [Build Tools for Visual Studio 2019](https://www.visualstudio.com/downloads/#build-tools-for-visual-studio-2019) (or newer) from Microsoft to compile extensions for Python 3.6+ (see [this](https://wiki.python.org/moin/WindowsCompilers) for further information). During installation, make sure to select the *C++ build tools*. Note that the compiler comes with OpenMP support.\n\n## Plugins for other software\n\n### ImageJ/Fiji\n\nWe currently provide a ImageJ/Fiji plugin that can be used to run pretrained StarDist models on 2D or 2D+time images. Installation and usage instructions can be found at the [plugin page](https://imagej.net/StarDist).\n\n### Napari\n\nWe made a plugin for the Python-based multi-dimensional image viewer [napari](https://napari.org). It directly uses the StarDist Python package and works for 2D and 3D images. Please see the [code repository](https://github.com/stardist/stardist-napari) for further details.\n\n### QuPath\n\nInspired by the Fiji plugin, [Pete Bankhead](https://github.com/petebankhead) made a custom implementation of StarDist 2D for [QuPath](https://qupath.github.io) to use pretrained models. Please see [this page](https://qupath.readthedocs.io/en/latest/docs/advanced/stardist.html) for documentation and installation instructions.\n\n### Icy\n\nBased on the Fiji plugin, [Deborah Schmidt](https://github.com/frauzufall) made a StarDist 2D plugin for [Icy](https://github.com/stardist/stardist-icy) to use pretrained models. Please see the [code repository](https://github.com/stardist/stardist-icy) for further details.\n\n### KNIME\n\n[Stefan Helfrich](https://github.com/stelfrich) has modified the Fiji plugin to be compatible with [KNIME](https://www.knime.com). Please see [this page](https://hub.knime.com/stelfrich/spaces/Public/latest/StarDist/StarDist%202D) for further details.\n\n## How to cite\n```bibtex\n@inproceedings{schmidt2018,\n  author    = {Uwe Schmidt and Martin Weigert and Coleman Broaddus and Gene Myers},\n  title     = {Cell Detection with Star-Convex Polygons},\n  booktitle = {Medical Image Computing and Computer Assisted Intervention - {MICCAI} \n  2018 - 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part {II}},\n  pages     = {265--273},\n  year      = {2018},\n  doi       = {10.1007/978-3-030-00934-2_30}\n}\n\n@inproceedings{weigert2020,\n  author    = {Martin Weigert and Uwe Schmidt and Robert Haase and Ko Sugawara and Gene Myers},\n  title     = {Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy},\n  booktitle = {The IEEE Winter Conference on Applications of Computer Vision (WACV)},\n  month     = {March},\n  year      = {2020},\n  doi       = {10.1109/WACV45572.2020.9093435}\n}\n\n@inproceedings{weigert2022,\n  author    = {Martin Weigert and Uwe Schmidt},\n  title     = {Nuclei Instance Segmentation and Classification in Histopathology Images with Stardist},\n  booktitle = {The IEEE International Symposium on Biomedical Imaging Challenges (ISBIC)},\n  year      = {2022},\n  doi       = {10.1109/ISBIC56247.2022.9854534}\n}\n```\n",859,cell-segmentation,Python,7,Python,C++,C,Dockerfile,Makefile,Groovy,Jupyter Notebook,,,,,,,,,,,,,,,,,,,,,,55,6,42,7,27,14,0,119684,216,223,175,48,c6c261081c6f9717fa9f5c47720ad2d5a9153224,Update README.md,2024-04-27T15:49:46Z,Uwe Schmidt,uschmidt83@users.noreply.github.com,uschmidt83,StarDist 0.9.1,"Bugfix release to address issues with arm64 wheels for macOS.\r\n\r\n(If you've cloned the repository, also note that the `master` branch has been renamed to `main`.)\r\n\r\n**Full Changelog**: https://github.com/stardist/stardist/compare/0.9.0...0.9.1",0.9.1,Uwe Schmidt,,uschmidt83,"BSD 3-Clause ""New"" or ""Revised"" License",stardist,stardist,30,bioimage-analysis,cell-segmentation,nuclei-segmentation,object-detection,deep-learning,python,,,,,,,,,,,,,,,/stardist/stardist,30,26,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/spacetelescope/jwst,https://github.com/spacetelescope/jwst,1,,,1,1,1,1,0,0,0,0,0,0,1,Python library for science observations from the James Webb Space Telescope,"# JWST Calibration Pipeline\n\n[![Build Status](https://github.com/spacetelescope/jwst/workflows/CI/badge.svg?branch=master)](https://github.com/spacetelescope/jwst/actions)\n[![codecov](https://codecov.io/gh/spacetelescope/jwst/branch/master/graph/badge.svg?token=Utf5Zs9g7z)](https://codecov.io/gh/spacetelescope/jwst)\n[![Documentation Status](https://readthedocs.org/projects/jwst-pipeline/badge/?version=latest)](http://jwst-pipeline.readthedocs.io/en/latest/?badge=latest)\n[![Powered by STScI Badge](https://img.shields.io/badge/powered%20by-STScI-blue.svg?colorA=707170&colorB=3e8ddd&style=flat)](http://www.stsci.edu)\n[![Powered by Astropy Badge](http://img.shields.io/badge/powered%20by-AstroPy-orange.svg?style=flat)](http://www.astropy.org/)\n[![DOI](https://zenodo.org/badge/60551519.svg)](https://zenodo.org/badge/latestdoi/60551519)\n\n![STScI Logo](docs/_static/stsci_logo.png)\n\n> [!IMPORTANT]\n> JWST requires a C compiler for dependencies and is currently limited to Python 3.10, 3.11, or 3.12.\n\n> [!NOTE]\n> Linux and MacOS platforms are tested and supported.  Windows is not currently supported.\n\n> [!WARNING]\n> Installation on MacOS Mojave 10.14 will fail due to lack of a stable build for dependency ``opencv-python``.\n\n## Installation\n\nPlease contact the [JWST Help Desk](https://jwsthelp.stsci.edu) for installation issues.\n\nThe easiest way to install the latest `jwst` release into a fresh virtualenv or conda environment is\n\n    pip install jwst\n\n### Detailed Installation\n\nThe `jwst` package can be installed into a virtualenv or conda environment via `pip`.\nWe recommend that for each installation you start by creating a fresh\nenvironment that only has Python installed and then install the `jwst` package and\nits dependencies into that bare environment.\nIf using conda environments, first make sure you have a recent version of Anaconda\nor Miniconda installed.\nIf desired, you can create multiple environments to allow for switching between different\nversions of the `jwst` package (e.g. a released version versus the current development version).\n\nIn all cases, the installation is generally a 3-step process:\n* Create a conda environment\n* Activate that environment\n* Install the desired version of the `jwst` package into that environment\n\nDetails are given below on how to do this for different types of installations,\nincluding tagged releases, DMS builds used in operations, and development versions.\nRemember that all conda operations must be done from within a bash/zsh shell.\n\n\n### Installing latest releases\n\nYou can install the latest released version via `pip`.  From a bash/zsh shell:\n\n    conda create -n <env_name> python=3.11\n    conda activate <env_name>\n    pip install jwst\n\nYou can also install a specific version:\n\n    conda create -n <env_name> python=3.11\n    conda activate <env_name>\n    pip install jwst==1.9.4\n\n### Installing the development version from Github\n\nYou can install the latest development version (not as well tested) from the\nGithub master branch:\n\n    conda create -n <env_name> python=3.11\n    conda activate <env_name>\n    pip install git+https://github.com/spacetelescope/jwst\n\n\n### Installing a DMS Operational Build\n\nThere may be occasions where an exact copy of an operational DMS build is\ndesired (e.g. for validation testing or debugging operational issues).\nWe package releases for DMS builds via environment snapshots that specify the\nexact versions of all packages to be installed.\n\nTo install a particular DMS build, consult the\n[Software vs DMS build version map](https://github.com/spacetelescope/jwst#software-vs-dms-build-version-map)\ntable shown below to determine the correct jwst tag. For example, to install the\nversion of `jwst` used in DMS build 9.0, use jwst tag 1.8.2. The overall\nprocedure is similar to the 3-step process outlined in the previous section, but the\ndetails of each command vary, due to the use of environment snapshot files that specify\nall of the particular packages to install. Also note that different snapshot files are\nused for Linux and Mac OS systems.\n\nLinux:\n\n    conda create -n jwstdp-1.12.5 --file https://ssb.stsci.edu/releases/jwstdp/1.12.5/conda_python_stable-deps.txt\n    conda activate jwstdp-1.12.5\n    pip install -r https://ssb.stsci.edu/releases/jwstdp/1.12.5/reqs_stable-deps.txt\n\nMacOS:\n\n    conda create -n jwstdp-1.12.5 --file https://ssb.stsci.edu/releases/jwstdp/1.12.5/conda_python_macos-stable-deps.txt\n    conda activate jwstdp-1.12.5\n    pip install -r https://ssb.stsci.edu/releases/jwstdp/1.12.5/reqs_macos-stable-deps.txt\n\nEach DMS delivery has its own installation instructions, which may be found in\nthe corresponding release documentation linked from this page:\nhttps://github.com/astroconda/astroconda-releases/tree/master/jwstdp\nThe installation procedures may change from time to time, so consulting the\ndocumentation page for the specific version in question is the best way to get\nthat version installed.\n\n\n### Installing for Developers\n\nIf you want to be able to work on and test the source code with the `jwst` package,\nthe high-level procedure to do this is to first create a conda environment using\nthe same procedures outlined above, but then install your personal copy of the\ncode overtop of the original code in that environment. Again, this should be done\nin a separate conda environment from any existing environments that you may have\nalready installed with released versions of the `jwst` package.\n\nAs usual, the first two steps are to create and activate an environment:\n\n    conda create -n <env_name> python=3.11\n    conda activate <env_name>\n\nTo install your own copy of the code into that environment, you first need to\nfork and clone the `jwst` repo:\n\n    cd <where you want to put the repo>\n    git clone https://github.com/<your_github_username>/jwst.git\n    cd jwst\n\n*Note: `python setup.py install` and `python setup.py develop` commands do not work.*\n\nInstall from your local checked-out copy as an ""editable"" install:\n\n    pip install -e .\n\nIf you want to run the unit or regression tests and/or build the docs, you can make\nsure those dependencies are installed too:\n\n    pip install -e "".[test]""\n    pip install -e "".[docs]""\n    pip install -e "".[test,docs]""\n\nNeed other useful packages in your development environment?\n\n    pip install ipython jupyter matplotlib pylint\n\n\n## Calibration References Data System (CRDS) Setup\n\n**Note: As of November 10, 2022, the process of deprecating the CRDS PUB Server will start.\nFor details, refer to the [CRDS PUB Server Freeze\nand Deprecation page](https://jwst-pipeline.readthedocs.io/en/stable/jwst/user_documentation/pub_deprecation.html#crds-pub-server-freeze-and-deprecation)**\n\n\nCRDS is the system that manages the reference files needed to run the pipeline.\nFor details about CRDS, see the [User's\nGuide](https://jwst-crds.stsci.edu/static/users_guide/index.html)\n\nThe JWST CRDS server is available at  https://jwst-crds.stsci.edu\n\nIt supports the automatic processing pipeline at STScI.\nInside the STScI network, the same server is used by the pipeline by default with no modifications.\nTo run the pipeline outside the STScI network, CRDS must be configured by setting\ntwo environment variables:\n\n    export CRDS_PATH=<locally-accessable-path>/crds_cache/jwst_ops\n    export CRDS_SERVER_URL=https://jwst-crds.stsci.edu\n\n\n``<locally-accessable-path>`` can be any the user has permissions to use, such as `$HOME`.\nExpect to use upwards of 200GB of disk space to cache the latest couple of contexts.\n\nTo use a specific CRDS context, other than the current default, set the ``CRDS_CONTEXT``\nenvironment variable:\n\n    export CRDS_CONTEXT=jwst_1179.pmap\n\n## Documentation\n\nDocumentation (built daily from the Github `master` branch) is available at:\n\nhttps://jwst-pipeline.readthedocs.io/en/latest/\n\nTo build the docs yourself, clone this repository and build the documentation with:\n\n    pip install -e "".[docs]""\n    cd docs\n    make html\n    make latexpdf\n\n\n## Contributions and Feedback\n\nWe welcome contributions and feedback on the project. Please follow the\n[contributing guidelines](CONTRIBUTING.md) to submit an issue or a pull request.\n\nWe strive to provide a welcoming community to all of our users by abiding with\nthe [Code of Conduct](CODE_OF_CONDUCT.md).\n\nIf you have questions or concerns regarding the software, please open an issue\nat https://github.com/spacetelescope/jwst/issues or\ncontact the [JWST Help Desk](https://jwsthelp.stsci.edu).\n\n\n## Software vs DMS build version map\n\nThe table below provides information on each release of the `jwst` package\nand its relationship to software builds used in the STScI JWST DMS operations\nenvironment. The `Released` column gives the date on which the `jwst` tag\nwas released on PyPi and the `Ops Install` column gives the date on which\nthe build incorporating that release was installed in DMS operations.\nNote that the `CRDS_CONTEXT` listed is a minimum context that can be used with\nthat release. A release should work with any contexts between\nthe specified context and less than the context for the next release.\n\n| jwst tag            | DMS build | SDP_VER  | CRDS_CONTEXT | Released   | Ops Install | Notes                                         |\n|---------------------|-----------|----------|--------------|------------|-------------|-----------------------------------------------|\n| 1.15.1              | B11.0rc2  | TBD      | 1242         | 2024-07-08 | TBD         | Second release candidate for B11.0            |\n| 1.15.0              | B11.0rc1  | TBD      | 1241         | 2024-06-26 | TBD         | First release candidate for B11.0             |\n| 1.14.1              |           |          | 1238         | 2024-06-27 |             | PyPI-only release for external users          |\n| 1.14.0              | B10.2.1   | 2024.1.1 | 1238         | 2024-03-29 | 2024-06-12  | Final release candidate for B10.2.1           |\n| 1.13.4              |           |          | 1185         | 2024-01-25 |             | PyPI-only release for external users          |\n| 1.13.3              | B10.1     | 2023.4.0 | 1181         | 2024-01-05 |             | Final release candidate for B10.1             |\n| 1.13.2              | B10.1rc3  | 2023.4.0 | 1181         | 2023-12-21 |             | Third release candidate for B10.1             |\n| 1.13.1              | B10.1rc2  | 2023.4.0 | 1181         | 2023-12-19 |             | Second release candidate for B10.1            |\n| 1.13.0              | B10.1rc1  | 2023.4.0 | 1179         | 2023-12-15 |             | First release candidate for B10.1             |\n| 1.12.5              | B10.0.1   | 2023.3.1 | 1166         | 2023-10-19 | 2023-12-05  | Patch release B10.0.1                         |\n| 1.12.4              |           |          |              | 2023-10-12 |             | Pinning dependencies for external users       |\n| 1.12.3              | B10.0     | 2023.3.0 | 1135         | 2023-10-03 | 2023-12-05  | Final release candidate for B10.0             |\n| 1.12.2              | B10.0rc3  |          | 1135         | 2023-10-02 |             | Third release candidate for B10.0             |\n| 1.12.1              | B10.0rc2  |          | 1132         | 2023-09-26 |             | Second release candidate for B10.0            |\n| 1.12.0              | B10.0rc1  |          | 1130         | 2023-09-18 |             | First release candidate for B10.0             |\n| 1.11.4              | B9.3.1    | 2023.2.1 | 1107         | 2023-08-14 | 2023-08-24  | Final release for B9.3.1 patch                |\n| 1.11.3              | B9.3      | 2023.2.0 | 1097         | 2023-07-17 |             | Final release candidate for B9.3              |\n| 1.11.2              | B9.3rc3   |          | 1097         | 2023-07-12 |             | Third release candidate for B9.3              |\n| 1.11.1              | B9.3rc2   |          | 1094         | 2023-06-29 |             | Second release candidate for B9.3             |\n| 1.11.0              | B9.3rc1   |          | 1094         | 2023-06-21 |             | First release candidate for B9.3              |\n| 1.10.2              |           |          | 1077         | 2023-04-14 |             | Pinning dependencies for external users       |\n| 1.10.1              | B9.2.x    | 2023.1.1 | 1077         | 2023-04-13 | 2023-05-23  | Final release candidate for B9.2              |\n| 1.10.0              | B9.2rc1   |          | 1075         | 2023-03-31 |             | First release candidate for B9.2              |\n| 1.9.6               | B9.1.2    | 2022.5.2 | 1068         | 2023-03-09 | 2023-03-15  | Final release candidate for B9.1.2            |\n| 1.9.5               |           |          | 1061         | 2023-03-02 |             | First release candidate for B9.1.2            |\n| 1.9.4               | B9.1.1    | 2022.5.1 | 1041         | 2023-01-27 | 2023-02-28  | Final release candidate for B9.1.1            |\n| 1.9.3               | B9.1      | 2022.5.0 | 1030         | 2023-01-12 | 2023-02-28  | Final release candidate for B9.1              |\n| 1.9.2               | B9.1rc2   |          |              | 2023-01-04 |             | Second release candidate for B9.1 (hotfix)    |\n| 1.9.1               | B9.1rc2   |          |              | 2023-01-03 |             | Second release candidate for B9.1             |\n| 1.9.0               | B9.1rc1   |          |              | 2022-12-27 |             | First release candidate for B9.1              |\n| 1.8.5               | B9.0      |          | 1019         | 2022-12-12 |             | Documentation patch release for B9.0          |\n| 1.8.4               | B9.0      |          |              | 2022-11-16 |             | Documentation patch release for B9.0          |\n| 1.8.3               | B9.0      |          |              | 2022-11-11 |             | Documentation patch release for B9.0          |\n| 1.8.2               | B9.0      | 2022.4.0 | 1017         | 2022-10-19 | 2022-11-17  | Final release candidate for B9.0              |\n| 1.8.1               | B9.0rc2   |          |              | 2022-10-17 |             | Second release candidate for B9.0             |\n| 1.8.0               | B9.0rc1   |          |              | 2022-10-10 |             | First release candidate for B9.0              |\n| 1.7.2               | B8.1.2    | 2022.3.1 | 0984         | 2022-09-12 | 2022-09-21  | Final release candidate for B8.1.2            |\n| 1.7.1               | B8.1.2rc2 |          |              | 2022-09-07 |             | Second release candidate for B8.1.2           |\n| 1.7.0               | B8.1.2rc1 |          |              | 2022-09-01 |             | First release candidate for B8.1.2            |\n| 1.6.2               | B8.1      | 2022.3.0 | 0953         | 2022-07-19 | 2022-08-19  | Final release candidate for B8.1              |\n| 1.6.1               | B8.1rc2   |          |              | 2022-07-15 |             | Second release candidate for B8.1             |\n| 1.6.0               | B8.1rc1   |          |              | 2022-07-11 |             | First release candidate for B8.1              |\n| 1.5.3               | B8.0.1    | 2022.2.1 | 0913         | 2022-06-20 | 2022-06-30  | Patch release B8.0.1                          |\n| 1.5.2               | B8.0      | 2022.2.0 | 0874         | 2022-05-20 | 2022-06-16  | Final release candidate for B8.0              |\n| 1.5.1               | B8.0rc2   |          |              | 2022-05-17 |             | Second release candidate for B8.0             |\n| 1.5.0               | B8.0rc1   |          |              | 2022-05-05 |             | First release candidate for B8.0              |\n| 1.4.6               | B7.9.3    | 2022.1.2 | 0800         | 2022-03-25 |             | Final release candidate for B7.9.3            |\n| 1.4.5               | B7.9.3rc2 |          |              | 2022-03-23 |             | Second release candidate for B7.9.3           |\n| 1.4.4               | B7.9.3rc1 |          |              | 2022-03-16 |             | First release candidate for B7.9.3            |\n| 1.4.3               | B7.9.1    | 2022.1.1 | 0800         | 2022-02-03 |             | Final B7.9.1                                  |\n| 1.4.2               | B7.9      | 2022.1.0 | 0797         | 2022-01-20 |             | Final release candidate for B7.9              |\n| 1.4.1               | B7.9rc2   |          |              | 2022-01-15 |             | Second release candidate for B7.9             |\n| 1.4.0               | B7.9rc1   |          |              | 2022-01-10 |             | First release candidate for B7.9              |\n| Pre-launch releases |           |          |              |            |             |                                               |\n| 1.3.3               | B7.8.2    | 2021.4.0 | 0764         | 2021-10-05 |             | Same as 1.3.2, but with installation bug fix  |\n| 1.3.2               | B7.8.2    | 2021.4.0 | 0764         | 2021-09-03 |             | Final release candidate for B7.8.2            |\n| 1.3.1               | B7.8.1    | 2021.3.0 | 0742         | 2021-08-09 |             | Final release candidate for B7.8.1            |\n| 1.3.0               | B7.8.1rc1 |          | 0741         | 2021-08-02 |             | First release candidate for B7.8.1            |\n| 1.2.3               | B7.8      | 2021.2.0 | 0732         | 2021-06-08 |             | Final release candidate for B7.8              |\n| 1.2.2               | B7.8rc3   |          |              | 2021-06-08 |             | Third release candidate for B7.8              |\n| 1.2.1               | B7.8rc2   |          |              | 2021-06-07 |             | Second release candidate for B7.8             |\n| 1.2.0               | B7.8rc1   |          | 0723         | 2021-05-24 |             | First release candidate for B7.8              |\n| 1.1.0               | B7.7.1    | 2021.1.0 | 0682         | 2021-02-26 |             | Final release candidate for B7.7.1            |\n| 1.0.0               | B7.7.1rc1 |          | 0678         | 2021-02-22 |             | First release candidate for B7.7.1            |\n| 0.18.3              | B7.7      | 2020.4.0 | 0670         | 2021-01-25 |             | Final release candidate for B7.7              |\n| 0.18.2              | B7.7rc3   |          | 0668         | 2021-01-19 |             | Third release candidate for B7.7              |\n| 0.18.1              | B7.7rc2   |          | 0664         | 2021-01-08 |             | Second release candidate for B7.7             |\n| 0.18.0              | B7.7rc1   |          | 0645         | 2020-12-21 |             | First release candidate for B7.7              |\n| 0.17.1              | B7.6      | 2020.3.0 | 0641         | 2020-09-15 |             | Final release candidate for B7.6              |\n| 0.17.0              | B7.6rc1   |          | 0637         | 2020-08-28 |             | First release candidate for B7.6              |\n| 0.16.2              | B7.5      | 2020.2.0 | 0619         | 2020-06-10 |             | Same as 0.16.1, but with installation bug fix |\n| 0.16.1              | B7.5      | 2020.2.0 | 0619         | 2020-05-19 |             | Final release candidate for B7.5              |\n| 0.16.0              | B7.5rc1   |          | 0614         | 2020-05-04 |             | First release candidate for B7.5              |\n| 0.15.1              | B7.4.2    | 2020.1.0 | 0586         | 2020-03-10 |             | Final release candidate for B7.4.2            |\n| 0.15.0              | B7.4.2rc1 |          | 0585         | 2020-02-28 |             | First release candidate for B7.4.2            |\n| 0.14.2              | B7.4      | 2019.3.0 | 0570         | 2019-11-18 |             | Final release candidate for B7.4              |\n| 0.14.1              | B7.4rc2   |          | 0568         | 2019-11-11 |             | Second release candidate for B7.4             |\n| 0.14.0              | B7.4rc1   |          | 0563         | 2019-10-25 |             | First release candidate for B7.4              |\n| 0.13.8              | B7.3.1    | 2019.2.0 | 0541         | 2019-09-05 |             | Patch for Build 7.3 released as Build 7.3.1   |\n| 0.13.7              | B7.3      | 2019.1.0 | 0535         | 2019-06-21 |             | Final release candidate for Build 7.3         |\n| 0.13.6              | B7.3rc4   |          | 0534         | 2019-06-20 |             | Fourth release candidate for Build 7.3        |\n| 0.13.5              | B7.3rc3   |          | 0534         | 2019-06-19 |             | Third release candidate for Build 7.3         |\n| 0.13.4              | B7.3rc2   |          | 0534         | 2019-06-18 |             | Second release candidate for Build 7.3        |\n| 0.13.3              | B7.3rc1   |          | 0532         | 2019-06-04 |             | First release candidate for Build 7.3         |\n| 0.13.2              |           |          | 0500         | 2019-05-14 |             | DMS test, no delivery to I&T                  |\n| 0.13.1              |           |          | 0500         | 2019-03-08 |             | DMS test, no delivery to I&T                  |\n| 0.13.0              |           |          | 0500         | 2019-02-15 |             | DMS test, no delivery to I&T                  |\n| 0.12.3              | B7.2.1    |          | 0500         | 2019-01-15 |             | DMS Build 7.2.1 patch release                 |\n| 0.12.2              | B7.2      | 2018_2   | 0495         | 2018-11-07 |             | Final release candidate for Build 7.2         |\n| 0.12.1              | B7.2rc2   |          | 0495         | 2018-11-01 |             | Second release candidate for Build 7.2        |\n| 0.12.0              | B7.2rc1   |          | 0493         | 2018-10-09 |             | First release candidate for Build 7.2         |\n| 0.11.0              |           |          | 0482         | 2018-09-10 |             | DMS test, no delivery to I&T                  |\n| 0.10.0              |           |          | 0477         | 2018-07-31 |             | DMS test, no delivery to I&T                  |\n| 0.9.6               | B7.1.3    | 2018_1   | 0468         | 2018-06-08 |             | Final release candidate for Build 7.1.3       |\n| 0.9.5               | B7.1.3rc3 |          | 0468         | 2018-06-06 |             | Third release candidate for Build 7.1.3       |\n| 0.9.4               | B7.1.3rc2 |          | 0463         | 2018-05-29 |             | Second release candidate for Build 7.1.3      |\n| 0.9.3               | B7.1.3rc1 |          | 0457         | 2018-05-11 |             | First release candidate for Build 7.1.3       |\n| 0.9.2               |           |          | 0441         | 2018-03-28 |             | DMS test, no delivery to I&T                  |\n| 0.9.1               |           |          | 0432         | 2018-02-16 |             | DMS test, no delivery to I&T                  |\n| 0.9.0               | B7.1.2    |          | 0422         | 2017-12-22 |             | DMS patch release to I&T 2018-02-15           |\n| 0.8.0               | B7.1.1    |          | 0422         | 2017-11-06 |             | DMS patch release to I&T 2018-01-17           |\n| 0.8.0               | B7.1      | 2017_1   | 0422         | 2017-11-06 |             | Final release for Build 7.1                   |\n| 0.7.7               | B7.0      | 2016_2   | 0303         | 2016-12-13 |             | Final release for Build 7.0                   |\n\n\n## Unit Tests\n\nUnit tests can be run via `pytest`.  Within the top level of your local `jwst` repo checkout:\n\n    pip install -e "".[test]""\n    pytest\n\nNeed to parallelize your test runs over all available cores?\n\n    pip install pytest-xdist\n    pytest -n auto\n\n\n## Regression Tests\n\nLatest regression test results can be found here (STScI staff only):\n\nhttps://plwishmaster.stsci.edu:8081/job/RT/job/JWST/\n\nThe test builds start at 6pm local Baltimore time Monday through Saturday on `jwcalibdev`.\n\nTo run the regression tests on your local machine, get the test dependencies\nand set the environment variable TEST_BIGDATA to our Artifactory server\n(STSci staff members only):\n\n    pip install -e "".[test]""\n    export TEST_BIGDATA=https://bytesalad.stsci.edu/artifactory\n\nTo run all the regression tests (except the very slow ones):\n\n    pytest --bigdata jwst/regtest\n\nYou can control where the test results are written with the\n`--basetemp=<PATH>` arg to `pytest`.  _NOTE that `pytest` will wipe this directory clean\nfor each test session, so make sure it is a scratch area._\n\nIf you would like to run a specific test, find its name or ID and use the `-k` option:\n\n    pytest --bigdata jwst/regtest -k nirspec\n\nIf developers need to update the truth files in our nightly regression tests,\nthere are instructions in the repository wiki.\n\nhttps://github.com/spacetelescope/jwst/wiki/Maintaining-Regression-Tests\n",547,astronomy,Jupyter Notebook,10,Python,C++,SourcePawn,Pascal,Jupyter Notebook,Pawn,NASL,HTML,C,BitBake,,,,,,,,,,,,,,,,,,,4524,356,4131,37,46,419,0,100399,159,4125,3693,432,de39de07b6f0380860caa86e8485666b4cef35c2,JP-3636: Fix crash in tweakreg due to MalformedPolygonError (#8657),2024-07-18T17:47:46Z,Mihai Cara,mcara@users.noreply.github.com,mcara,JWST 1.15.1 (DMS build B11.0rc2),## What's Changed\r\n* fix ruff style check by @braingram in https://github.com/spacetelescope/jwst/pull/8615\r\n* add `1.14.1` release to README table by @zacharyburnett in https://github.com/spacetelescope/jwst/pull/8617\r\n* minor fixes to outlier detection tso unit test by @braingram in https://github.com/spacetelescope/jwst/pull/8623\r\n* metadata for release `1.15.1` (`B11.0rc2`) by @zacharyburnett in https://github.com/spacetelescope/jwst/pull/8629\r\n\r\n\r\n**Full Changelog**: https://github.com/spacetelescope/jwst/compare/1.15.0...1.15.1,1.15.1,Zach Burnett,,zacharyburnett,Other,jwst,spacetelescope,70,astronomy,jwst,python,,,,,,,,,,,,,,,,,,/spacetelescope/jwst,175,31,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/sourmash-bio/sourmash,https://github.com/sourmash-bio/sourmash,1,,1,1,1,1,1,0,0,0,0,0,0,1,"Quickly search, compare, and analyze genomic and metagenomic data sets.","# sourmash\n\nQuickly search, compare, and analyze genomic and metagenomic data sets.\n\n[![Project Status: Active – The project has reached a stable, usable state and is being actively developed.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n<a href=""https://github.com/sourmash-bio/sourmash/blob/latest/LICENSE""><img alt=""License: 3-Clause BSD"" src=""https://img.shields.io/badge/License-BSD%203--Clause-blue.svg""></a>\n[![Documentation](https://readthedocs.org/projects/sourmash/badge/?version=latest)](http://sourmash.readthedocs.io/en/latest/)\n[![Gitter](https://badges.gitter.im/sourmash-bio/community.svg)](https://gitter.im/sourmash-bio/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n\n[![DOI](https://joss.theoj.org/papers/10.21105/joss.06830/status.svg)](https://doi.org/10.21105/joss.06830)\n[![pyOpenSci](https://tinyurl.com/y22nb8up)](https://github.com/pyOpenSci/software-submission/issues/129)\n\n[![Bioconda install](https://img.shields.io/conda/dn/bioconda/sourmash.svg?style=flag&label=Bioconda)](https://anaconda.org/bioconda/sourmash)\n<a href=""https://pypi.org/project/sourmash/""><img alt=""PyPI"" src=""https://badge.fury.io/py/sourmash.svg""></a>\n[![Conda Platforms](https://img.shields.io/conda/pn/conda-forge/sourmash-minimal.svg)](https://anaconda.org/conda-forge/sourmash-minimal)\n\n![Python 3.10](https://img.shields.io/badge/python-3.10-blue.svg)\n![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)\n![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)\n[![Build Status](https://github.com/sourmash-bio/sourmash/workflows/Python%20tests/badge.svg)](https://github.com/sourmash-bio/sourmash/actions/)\n[![codecov](https://codecov.io/gh/sourmash-bio/sourmash/branch/latest/graph/badge.svg)](https://codecov.io/gh/sourmash-bio/sourmash)\n\n<p align=""center""><img src=""https://raw.githubusercontent.com/sourmash-bio/sourmash/latest/doc/_static/logo.png"" height=""256"" /></p>\n\nUsage:\n\n    sourmash sketch dna *.fq.gz\n    sourmash compare *.sig -o distances.cmp -k 31\n    sourmash plot distances.cmp\n\nsourmash 1.0 is [published on JOSS](https://doi.org/10.21105/joss.06830); please cite that paper if you use sourmash (`doi: 10.21105/joss.06830`):.\n\nThe latest major release is sourmash v4, which has several\ncommand-line and Python incompatibilities with previous\nversions. Please\n[visit our migration guide](https://sourmash.readthedocs.io/en/latest/support.html#migrating-from-sourmash-v3-x-to-sourmash-4-x)\nto upgrade!\n\n----\n\nsourmash is a k-mer analysis multitool, and we aim to provide stable, robust programmatic and command-line APIs for a variety of sequence comparisons. Some of our special sauce includes:\n- `FracMinHash` sketching, which enables accurate comparisons (including ANI) between data sets of different sizes\n- `sourmash gather`, a combinatorial k-mer approach for more accurate metagenomic profiling\n\nPlease see the [sourmash publications](https://sourmash.readthedocs.io/en/latest/publications.html#sourmash-fundamentals) for details.\n\nThe name is a riff off of [Mash](https://github.com/marbl/Mash),\ncombined with @ctb's love of whiskey.\n([Sour mash](https://en.wikipedia.org/wiki/Sour_mash) is used in\nmaking whiskey.)\n\nMaintainers: [C. Titus Brown](mailto:titus@idyll.org) ([@ctb](http://github.com/ctb)), [Luiz C. Irber, Jr](mailto:luiz@sourmash.bio) ([@luizirber](http://github.com/luizirber)), and [N. Tessa Pierce-Ward](mailto:tessa@sourmash.bio) ([@bluegenes](http://github.com/bluegenes)).\n\nsourmash was initially developed by the\n[Lab for Data-Intensive Biology](http://ivory.idyll.org/lab/) at the\n[UC Davis School of Veterinary Medicine](http://www.vetmed.ucdavis.edu),\nand now includes contributions from the global research and developer\ncommunity.\n\n## Installation\n\nWe recommend using conda-forge to install sourmash:\n\n```\nconda install -c conda-forge sourmash-minimal\n```\nThis will install the latest stable version of sourmash 4.\n\nYou can also use pip to install sourmash:\n\n```\npip install sourmash\n```\n\nA quickstart tutorial [is available](https://sourmash.readthedocs.io/en/latest/tutorials.html).\n\n### Requirements\n\nsourmash runs under Python 3.10 and later on Windows, Mac OS X, and\nLinux.  The base requirements are screed, cffi, numpy, matplotlib, and\nscipy.  Conda will install everything necessary, and is\nour recommended installation method (see below).\n\n### Installation with conda\n\nconda-forge is a community maintained channel for the\n[conda](http://conda.pydata.org/docs/intro.html) package manager.\n[installing conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/),\nyou can install sourmash by running:\n\n```bash\n$ conda create -n sourmash_env -c conda-forge sourmash-minimal\n$ conda activate sourmash_env\n$ sourmash --help\n```\n\nwhich will install\n[the latest released version](https://github.com/sourmash-bio/sourmash/releases).\n\n## Support\n\nFor questions, please open an issue [on Github](https://github.com/sourmash-bio/sourmash/issues), or ask in our [chat](https://gitter.im/sourmash-bio/community?utm_source=share-link&utm_medium=link&utm_campaign=share-link).\n\n## Development\n\nDevelopment happens on github at\n[sourmash-bio/sourmash](https://github.com/sourmash-bio/sourmash).\n\nsourmash is developed in Python and Rust, and you will need a Rust\nenvironment to build it; see [the developer notes](doc/developer.md)\nfor our suggested development setup.\n\nAfter installation, `sourmash` is the main command-line entry point;\nrun it with `python -m sourmash`, or do `pip install -e /path/to/repo` to\ndo a developer install in a virtual environment.\n\nThe `sourmash/` directory contains the Python library and command-line interface code.\n\nThe `src/core/` directory contains the Rust library implementing core\nfunctionality.\n\nTests require py.test and can be run with `make test`.\n\nPlease see [the developer notes](doc/developer.md) for more information\non getting set up with a development environment.\n\nCTB\nJan 2024\n",453,bioinformatics,Python,7,Python,Shell,Makefile,TeX,Rust,C,Nix,,,,,,,,,,,,,,,,,,,,,,1597,280,1260,57,73,39,0,36930,78,1658,1006,652,55f26bc179ce4faf5f2422daf2f4fa645ccb2394,[pre-commit.ci] pre-commit autoupdate (#3255),2024-07-15T22:17:43Z,pre-commit-ci[bot],66853113+pre-commit-ci[bot]@users.noreply.github.com,pre-commit-ci[bot],v4.8.10,"This release is accompanied by the publication of a JOSS paper: [ “sourmash v4: A multitool to quickly search, compare, and analyze genomic and metagenomic data sets”](https://joss.theoj.org/papers/10.21105/joss.06830), our new citation handle!\r\n\r\nMinor new features:\r\n\r\n* check `select` parameters; enforce types when building manifests (#3212)\r\n* patch-fix `sig extract` to no longer create empty zips (#3214)\r\n\r\nBug fixes:\r\n\r\n* adjust how ANI is calculated in the revindex code. (#3218)\r\n\r\nCleanup and documentation updates:\r\n\r\n* final updates for 2024 JOSS publication (#3225)\r\n* Improve JOSS paper affiliations (#3224)\r\n* fix DOI for Rahman Hera paper in JOSS pub. (#3221)\r\n* upd citations, minor text (#3220)\r\n\r\nDeveloper updates:\r\n\r\n* bump sourmash core version to 0.14.1 (#3219)\r\n* bump version to 4.8.10-dev (#3211)\r\n\r\nDependabot updates:\r\n\r\n- Bump proptest from 1.4.0 to 1.5.0 (#3222)\r\n- [pre-commit.ci] pre-commit autoupdate (#3223)\r\n- [pre-commit.ci] pre-commit autoupdate (#3003)\r\n- Bump histogram from 0.10.2 to 0.11.0 (#3216)\r\n- Bump pypa/cibuildwheel from 2.19.0 to 2.19.1 (#3217)\r\n- Bump histogram from 0.10.1 to 0.10.2 (#3207)\r\n- Bump statrs from 0.16.1 to 0.17.1 (#3205)\r\n- Bump roaring from 0.10.4 to 0.10.5 (#3206)\r\n- Bump primal-check from 0.3.3 to 0.3.4 (#3208)\r\n- Bump niffler from 2.5.0 to 2.6.0 (#3204)\r\n- Bump pypa/cibuildwheel from 2.18.1 to 2.19.0 (#3202)",v4.8.10,,,github-actions[bot],Other,sourmash,sourmash-bio,57,minhash,bioinformatics,rust,python,sourmash,fracminhash,scaled-minhash,kmer,sketching,taxonomic-classification,taxonomic-profiling,hacktoberfest,,,,,,,,,/sourmash-bio/sourmash,114,20,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/sortmerna/sortmerna,https://github.com/sortmerna/sortmerna,0.5,Too small of a project,1,1,1,1,1,0,0,0,0,0,0,1,SortMeRNA: next-generation sequence filtering and alignment tool,"# sortmerna\n\nSortMeRNA is a local sequence alignment tool for filtering, mapping and clustering.\n\nThe core algorithm is based on approximate seeds and allows for sensitive analysis of NGS reads.\nThe main application of SortMeRNA is filtering rRNA from metatranscriptomic data.\nSortMeRNA takes as input files of reads (fasta, fastq, fasta.gz, fastq.gz) and one or multiple\nrRNA database file(s), and sorts apart aligned and rejected reads into two files. SortMeRNA works\nwith Illumina, Ion Torrent and PacBio data, and can produce SAM and BLAST-like alignments.\n\nSortMeRNA is also available through [QIIME v1.9.1](http://qiime.org) and\nthe [nf-core RNA-Seq pipeline v.3.9](https://nf-co.re/rnaseq/3.9).\n\n## Table of Contents\n\n- [Getting Started](#getting-started)\n  - [Using Conda package](#using-conda-package)\n  - [Using GitHub release binaries on Linux](#using-github-release-binaries-on-linux)\n  - [Running](#running)\n    - [Execution trace](#execution-trace)\n- [Building from sources](#building-from-sources)\n- [User Manual](#user-manual)\n- [Taxonomies](#taxonomies)\n- [Citation](#citation)\n- [Contributors](#contributors)\n- [Support](#support)\n\n\n## Getting Started\n\nSortMeRNA 4 is C++17 compliant, and mostly uses standard libraries. It uses CMake as the build system, and can be run/built on all major OS including Linux, Windows, and Mac, on AMD64 and ARM64 processors.\n\n### Using Conda package\n\n[Install conda]:(https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html)\n```\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\n```\nThen, as per the [Bioconda guidelines](https://bioconda.github.io), add the following conda channels:\n```\nconda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\nconda config --set channel_priority strict\n\n\nconda search sortmerna\n  Loading channels: done\n  # Name                       Version           Build  Channel\n  sortmerna                        2.0               0  bioconda\n  ...\n  sortmerna                      4.3.4               0  bioconda\n  ...\n  sortmerna                      4.3.6               0  bioconda\n\n# create a new environment and install SortMeRNA in it\nconda create --name sortmerna_env\nconda activate sortmerna_env\nconda install sortmerna\nwhich sortmerna\n  /home/biocodz/miniconda3/envs/sortmerna_env/bin/sortmerna\n\n# test the installation\nsortmerna --version\n  SortMeRNA version 4.3.6\n  Build Date: Aug 16 2022\n  sortmerna_build_git_sha:@db8c1983765f61986b46ee686734749eda235dcc@\n  sortmerna_build_git_date:@2022/08/16 11:42:59@\n\n# view help\nsortmerna -h\n```\n\n### Using GitHub release binaries on Linux\n\nVisit [Sortmerna GitHub Releases](https://github.com/biocore/sortmerna/releases)\n\nLinux distribution is a Shell script with the embedded installation archive.\n\nIssue the following bash commands:\n\n```\npushd ~\n\n# get the distro\nwget https://github.com/biocore/sortmerna/releases/download/v4.3.6/sortmerna-4.3.6-Linux.sh\n\n# view the installer usage\nbash sortmerna-4.3.6-Linux.sh --help\n    Options: [defaults in brackets after descriptions]\n      --help            print this message\n      --version         print cmake installer version\n      --prefix=dir      directory in which to install\n      --include-subdir  include the sortmerna-4.3.6-Linux subdirectory\n      --exclude-subdir  exclude the sortmerna-4.3.6-Linux subdirectory\n      --skip-license    accept license\n\n# run the installer\nbash sortmerna-4.3.6-Linux.sh --skip-license\n  sortmerna Installer Version: 4.3.6, Copyright (c) Clarity Genomics\n  This is a self-extracting archive.\n  The archive will be extracted to: $HOME/sortmerna\n  \n  Using target directory: /home/biocodz/sortmerna\n  Extracting, please wait...\n  \n  Unpacking finished successfully\n\n# check the installed binaries\nls -lrt /home/biocodz/sortmerna/bin/\nsortmerna\n\n# set PATH\nexport PATH=$HOME/sortmerna/bin:$PATH\n\n# test the installation\nsortmerna --version\n  SortMeRNA version 4.3.6\n  Build Date: Jul 17 2021\n  sortmerna_build_git_sha:@921fa40256760ea2d44c49b21eb326afda748d5e@\n  sortmerna_build_git_date:@2022/08/16 10:59:31@\n\n# view help\nsortmerna -h\n```\n\n### Running\n\n* The only required options are `--ref` and `--reads`\n* Options (any) can be specified usig a single dash e.g. `-ref` and `-reads`\n* Both plain `fasta/fastq` and archived `fasta.gz/fastq.gz` files are accepted\n* file extensions `.fastq, .fastq.gz, .fq, .fq.gz, .fasta, ...` are optional. The format and compression are automatically recognized\n* Relative paths are accepted\n\nfor example\n\n```\n# single reference and single reads file\nsortmerna --ref REF_PATH --reads READS_PATH\n\n# for multiple references use multiple '--ref'\nsortmerna --ref REF_PATH_1 --ref REF_PATH_2 --ref REF_PATH_3 --reads READS_PATH\n\n# for paired reads use '--reads' twice\nsortmerna --ref REF_PATH_1 --ref REF_PATH_2 --ref REF_PATH_3 --reads READS_PATH_1 --reads READS_PATH_2\n\n```\n\nMore examples can be found in [test.jinja](https://github.com/biocore/sortmerna/blob/master/scripts/test.jinja) and [run.py](https://github.com/biocore/sortmerna/blob/master/scripts/run.py)\n\n#### Execution trace\n\nHere is a [sample execution trace](https://sortmerna.readthedocs.io/en/latest/trace4.3.2.html).  \n\n`IMPORTANT`\n- Progressing execution trace showing the number of reads processed so far indicates a normally running program. \n- Non-progressing trace means a problem. Please, kill the process (no waiting for two days), and file an issue [here](https://github.com/biocore/sortmerna/issues)  \n- please, provide the execution trace when filing issues.\n\n[Sample execution statistics](https://github.com/biocore/sortmerna/wiki/sample-execution-statistics) are provided to give an idea on what the execution time might be.\n\n## Building from sources\n\n[Build instructions](https://sortmerna.readthedocs.io/en/latest/building.html)\n\n## User Manual\n\nSee [Sortmerna Read The Docs project](https://sortmerna.readthedocs.io/en/latest/index.html).\n\nIn case you need PDF, any modern browser can print web pages to PDF.\n\n## Taxonomies\n\nThe folder `data/rRNA_databases/silva_ids_acc_tax.tar.gz` contains SILVA taxonomy strings (extracted from XML file generated by ARB)\nfor each of the reference sequences in the representative databases. The format of the files is three tab-separated columns,\nthe first being the reference sequence ID, the second being the accession number and the final column is the taxonomy.\n\n## Citation\n\nIf you use SortMeRNA, please cite:\nKopylova E., Noé L. and Touzet H., ""SortMeRNA: Fast and accurate filtering of ribosomal RNAs in metatranscriptomic data"", Bioinformatics (2012), doi: 10.1093/bioinformatics/bts611.\n\n## Contributors\n\nSee [AUTHORS](./AUTHORS) for a list of contributors to this project.\n\n## Support\n\nFor questions and comments, feel free to file an [issue](https://github.com/sortmerna/sortmerna/issues), or start a [discussion](https://github.com/sortmerna/sortmerna/discussions).\n	\n",234,bioinformatics,C++,7,Shell,C++,C,Python,CMake,Dockerfile,Jinja,,,,,,,,,,,,,,,,,,,,,,103,15,88,0,3,9,204,93227,69,308,265,43,6fc9cb6a8ea29485cfd0c09c46a0f8b89feb73d2,fixing stoi error - issue 379,2024-07-15T19:59:27Z,biocodz,biocodz@protonmail.com,biocodz,Release v4.3.7,"This release provides bug fixes. builds on MacOS, and ARM64.\r\nStill waiting for [Conda build PR](https://github.com/conda-forge/staged-recipes/pull/25438) merge to provide installation of the newer SortMeRNA version through Conda",v4.3.7,biocodz,,biocodz,GNU General Public License v3.0,sortmerna,sortmerna,23,cpp,python,alignment,bioinformatics,metatranscriptomics,ngs,sequencing,,,,,,,,,,,,,,/sortmerna/sortmerna,23,14,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/sorgerlab/indra,https://github.com/sorgerlab/indra,0.5,NLP tool,1,1,1,1,1,0,0,0,0,0,0,1,"INDRA (Integrated Network and Dynamical Reasoning Assembler) is an automated model assembly system interfacing with NLP systems and databases to collect knowledge, and through a process of assembly, produce causal graphs and dynamical models.","# INDRA\n\n[![License](https://img.shields.io/badge/License-BSD%202--Clause-orange.svg)](https://opensource.org/licenses/BSD-2-Clause)\n[![Build](https://github.com/sorgerlab/indra/workflows/Tests/badge.svg)](https://github.com/sorgerlab/indra/actions)\n[![Documentation](https://readthedocs.org/projects/indra/badge/?version=latest)](https://indra.readthedocs.io/en/latest/?badge=latest)\n[![PyPI version](https://badge.fury.io/py/indra.svg)](https://badge.fury.io/py/indra)\n[![Python 3](https://img.shields.io/pypi/pyversions/indra.svg)](https://www.python.org/downloads/release/python-357/)\n\n<img align=""left"" src=""https://raw.githubusercontent.com/sorgerlab/indra/master/doc/indra_logo.png"" width=""300"" height=""224"" />\n\nINDRA (Integrated Network and Dynamical Reasoning Assembler) is an automated\nmodel assembly system, originally developed for molecular systems biology and\nthen generalized to other domains (see [INDRA\nWorld](https://github.com/indralab/indra_world)). INDRA draws on natural\nlanguage processing systems and structured databases to collect mechanistic and\ncausal assertions, represents them in a standardized form (INDRA Statements),\nand assembles them into various modeling formalisms including causal graphs and\ndynamical models.\n\nAt the core of INDRA are its knowledge-level assembly procedures, allowing\nsources to be assembled into coherent models, a process that involves\ncorrecting systematic input errors, finding and resolving redundancies,\ninferring missing information, filtering to a relevant scope and assessing the\nreliability of causal information.\n\nThe detailed INDRA documentation is available at\n[http://indra.readthedocs.io](http://indra.readthedocs.io).\n\n## Contents\n\n- [INDRA Modules](#indra-modules)\n    - [Knowledge sources](#knowledge-sources)\n    - [Output model assemblers](#output-model-assemblers)\n    - [Internal knowledge assembly](#internal-knowledge-assembly)\n    - [Other modules](#other-modules)\n- [Citation](#citation)\n- [Installation](#installation)\n- [INDRA REST API](#indra-rest-api)\n- [INDRA Docker](#indra-docker)\n- [Using INDRA](#using-indra)\n- [Funding](#funding)\n\n## INDRA Modules\n\n### Knowledge sources\n\nINDRA is currently integrated with the following natural language processing\nsystems and structured databases. These input modules (available in\n`indra.sources`) all produce INDRA Statements.\n\nReading systems:\n\n| Reader     | Module                                                                                                 | Reference                                       |\n|------------|--------------------------------------------------------------------------------------------------------|-------------------------------------------------|\n| TRIPS/DRUM | [`indra.sources.trips`](https://indra.readthedocs.io/en/latest/modules/sources/trips/index.html)       | http://trips.ihmc.us/parser/cgi/drum            |\n| REACH      | [`indra.sources.reach`](https://indra.readthedocs.io/en/latest/modules/sources/reach/index.html)       | https://github.com/clulab/reach                 |\n| Sparser    | [`indra.sources.sparser`](https://indra.readthedocs.io/en/latest/modules/sources/sparser/index.html#)  | https://github.com/ddmcdonald/sparser           |\n| Eidos      | [`indra.sources.eidos`](https://indra.readthedocs.io/en/latest/modules/sources/eidos/index.html#)      | https://github.com/clulab/eidos                 |\n| TEES       | [`indra.sources.tees`](https://indra.readthedocs.io/en/latest/modules/sources/tees/index.html)         | https://github.com/jbjorne/TEES                 |\n| MedScan    | [`indra.sources.medscan`](https://indra.readthedocs.io/en/latest/modules/sources/medscan/index.html)   | https://doi.org/10.1093/bioinformatics/btg207   |\n| RLIMS-P    | [`indra.sources.rlimsp`](https://indra.readthedocs.io/en/latest/modules/sources/rlimsp/index.html)     | https://research.bioinformatics.udel.edu/rlimsp |\n| ISI/AMR    | [`indra.sources.isi`](https://indra.readthedocs.io/en/latest/modules/sources/isi/index.html)           | https://github.com/sgarg87/big_mech_isi_gg      |\n| Geneways   | [`indra.sources.geneways`](https://indra.readthedocs.io/en/latest/modules/sources/geneways/index.html) | https://www.ncbi.nlm.nih.gov/pubmed/15016385    |\n| GNBR       | [`indra.sources.gnbr`](https://indra.readthedocs.io/en/latest/modules/sources/gnbr/index.html)         | https://zenodo.org/record/3459420               |\n| SemRep     | [`indra.sources.semrep`](https://indra.readthedocs.io/en/latest/modules/sources/semrep.html)     | https://github.com/lhncbc/SemRep                |\n\n\nBiological pathway databases:\n\n| Database / Exchange format | Module                     | Reference                                                       |\n|----------------------------|----------------------------|-----------------------------------------------------------------|\n| PathwayCommons / BioPax    | [`indra.sources.biopax`](https://indra.readthedocs.io/en/latest/modules/sources/biopax/index.html)     | http://pathwaycommons.org/ <br/> http://www.biopax.org/         |\n| Large Corpus / BEL         | [`indra.sources.bel`](https://indra.readthedocs.io/en/latest/modules/sources/bel/index.html)        | https://github.com/pybel/pybel <br/> https://github.com/OpenBEL |\n| Signor                     | [`indra.sources.signor`](https://indra.readthedocs.io/en/latest/modules/sources/signor/index.html)     | https://signor.uniroma2.it/                                     |\n| BioGRID                    | [`indra.sources.biogrid`](https://indra.readthedocs.io/en/latest/modules/sources/biogrid/index.html)    | https://thebiogrid.org/                                         |\n| Target Affinity Spectrum   | [`indra.sources.tas`](https://indra.readthedocs.io/en/latest/modules/sources/tas/index.html#)        | https://doi.org/10.1101/358978                                  |\n| HPRD                       | [`indra.sources.hprd`](https://indra.readthedocs.io/en/latest/modules/sources/hprd/index.html) | http://www.hprd.org                                             |                   |\n| TRRUST                     | [`indra.sources.trrust`](https://indra.readthedocs.io/en/latest/modules/sources/trrust.html) | https://www.grnpedia.org/trrust/                                |                   |\n| Phospho.ELM                | [`indra.sources.phosphoelm`](https://indra.readthedocs.io/en/latest/modules/sources/phosphoelm/index.html) | http://phospho.elm.eu.org/                                      |\n| VirHostNet                | [`indra.sources.virhostnet`](https://indra.readthedocs.io/en/latest/modules/sources/virhostnet/index.html) | http://virhostnet.prabi.fr/                                     |\n| CTD                  | [`indra.sources.ctd`](https://indra.readthedocs.io/en/latest/modules/sources/ctd/index.html) | http://ctdbase.org                                              |\n| DrugBank                  | [`indra.sources.drugbank`](https://indra.readthedocs.io/en/latest/modules/sources/drugbank/index.html) | https://www.drugbank.ca/                                        |\n| OmniPath                  | [`indra.sources.omnipath`](https://indra.readthedocs.io/en/latest/modules/sources/omnipath/index.html) | https://omnipathdb.org/                                         |\n| DGI                  | [`indra.sources.dgi`](https://indra.readthedocs.io/en/latest/modules/sources/dgi/index.html) | https://www.dgidb.org/                                          |\n| CRoG                  | [`indra.sources.crog`](https://indra.readthedocs.io/en/latest/modules/sources/crog/index.html) | https://github.com/chemical-roles/chemical-roles                |\n| CREEDS                     | [`indra.sources.creeds`](https://indra.readthedocs.io/en/latest/modules/sources/creeds/index.html) | https://maayanlab.cloud/CREEDS/                                 |\n| UbiBrowser                 | [`indra.sources.ubibrowser`](https://indra.readthedocs.io/en/latest/modules/sources/ubibrowser.html) | http://ubibrowser.ncpsb.org.cn/                |\n| ACSN                       | [`indra.sources.acsn`](https://indra.readthedocs.io/en/latest/modules/sources/acsn.html) | https://acsn.curie.fr/ACSN2/ACSN2.html   |\n\nCustom knowledge bases:\n\n| Database / Exchange format | Module                        | Reference                            |\n|----------------------------|-------------------------------|--------------------------------------|\n| NDEx / CX                  | [`indra.sources.ndex_cx`](https://indra.readthedocs.io/en/latest/modules/sources/ndex_cx/index.html)       | http://ndexbio.org                   |\n| INDRA DB / INDRA Statements| [`indra.sources.indra_db_rest`](https://indra.readthedocs.io/en/latest/modules/sources/indra_db_rest/index.html) | https://github.com/indralab/indra_db |\n| Hypothes.is                  | [`indra.sources.hypothesis`](https://indra.readthedocs.io/en/latest/modules/sources/hypothesis/index.html)       | https://hypothes.is                   |\n| Biofactoid                  | [`indra.sources.biofactoid`](https://indra.readthedocs.io/en/latest/modules/sources/biofactoid/index.html)       | https://biofactoid.org/                   |\n| MINERVA                    | [`indra.sources.minerva`](https://indra.readthedocs.io/en/latest/modules/sources/minerva/index.html)         | https://covid19map.elixir-luxembourg.org/minerva/ |\n\n\n### Output model assemblers\n\nINDRA also provides several model output assemblers that take INDRA Statements\nas input. The most sophisticated model assembler is the PySB Assembler, which\nimplements a policy-guided automated assembly procedure of a rule-based\nexecutable model (that can then be further compiled into other formats such as\nSBML, Kappa, BNGL and SBGN to connect to a vast ecosystem of downstream tools).\nSeveral other model assembly modules target various network formats for\nvisualization, and graph/structural analysis (PyBEL, CyJS, Graphviz, SBGN,\nCX, SIF, etc.) and curation (HTML, TSV, IndexCards).\nFinally, the English Assembler produces English language descriptions of a set\nof INDRA Statements.\n\nINDRA also supports extension by outside model assembly tools which take\nINDRA Statements as input and produce models. One such example is Delphi\n(https://github.com/ml4ai/delphi), which is a Dynamic Bayesian Network\nmodel assembler. Similarly, outside tools that support INDRA Statements\ncan implement custom visualization methods, such as CauseMos, developed\nby Uncharted Software (https://uncharted.software/).\n\nAssemblers aimed at model-driven discovery and analysis:\n\n| Modeling formalism / Exchange format           | Purpose                                              | Module                  | Reference           |\n|------------------------------------------------|------------------------------------------------------|-------------------------|---------------------|\n| PySB (-> SBML, SBGN, BNGL, Kappa, etc.)        | Detailed, mechanistic modeling, simulation, analysis | [`indra.assemblers.pysb`](https://indra.readthedocs.io/en/latest/modules/assemblers/pysb_assembler.html#) | http://pysb.org     |\n| PyBEL                                          | Causal analysis, visualization                       | [`indra.assemblers.pybel`](https://indra.readthedocs.io/en/latest/modules/assemblers/pybel_assembler.html)| https://github.com/pybel/pybel <br/> https://bel-commons.scai.fraunhofer.de/ |\n| IndraNet                                       | Causal analysis, signed and unsigned                 | [`indra.assemblers.indranet`](https://indra.readthedocs.io/en/latest/modules/assemblers/indranet_assembler.html) |                  |\n| SIF                                            | Network analysis, logic modeling, visualization      | [`indra.assemblers.sif`](https://indra.readthedocs.io/en/latest/modules/assemblers/sif_assembler.html)  | [SIF format](http://manual.cytoscape.org/en/stable/Supported_Network_File_Formats.html#sif-format) |\n| KAMI                                           | Knowledge aggregation of protein sites/states and Kappa modeling | [`indra.assemblers.kami`](https://indra.readthedocs.io/en/latest/modules/assemblers/kami_assembler.html) | https://github.com/Kappa-Dev/KAMI |\n\n\nAssemblers primarily aimed at visualization:\n\n| Network / Exchange format                      | Purpose                                              | Module                  | Reference           |\n|------------------------------------------------|------------------------------------------------------|-------------------------|---------------------|\n| Causal Analysis Graph                          | General causal graph visualization                   | [`indra.assemblers.cag`](https://indra.readthedocs.io/en/latest/modules/assemblers/cag_assembler.html)  |                     |\n| CX                                             | Network browsing, versioning on NDEx                 | [`indra.assemblers.cx`](https://indra.readthedocs.io/en/latest/modules/assemblers/cx_assembler.html)   | http://ndexbio.org  |\n| Cytoscape JS                                   | Interactive Cytoscape JS network to embed in websites| [`indra.assemblers.cyjs`](https://indra.readthedocs.io/en/latest/modules/assemblers/cyjs_assembler.html) | http://js.cytoscape.org/ |\n| Graphviz                                       | Static PDF/PNG visualization with powerful automated layout using Graphviz | [`indra.assemblers.graph`](https://indra.readthedocs.io/en/latest/modules/assemblers/graph_assembler.html) | https://www.graphviz.org/ |\n| SBGN                                           | Visualization with Systems Biology Graphical Notation| [`indra.assemblers.sbgn`](https://indra.readthedocs.io/en/latest/modules/assemblers/sbgn_assembler.html) | http://sbgn.org     |\n\nAssemblers primarily aimed at expert curation and browsing:\n\n| Output format                                  | Purpose                                                | Module                  | Reference           |\n|------------------------------------------------|------------------------------------------------------  |-------------------------|---------------------|\n| English language                               | Human-readable descriptions, reports, dialogue         | [`indra.assemblers.english`](https://indra.readthedocs.io/en/latest/modules/assemblers/english_assembler.html) |                  |\n| HTML                                           | Web-based browsing, linking out to provenance, curation| [`indra.assemblers.html`](https://indra.readthedocs.io/en/latest/modules/assemblers/html_assembler.html) | [Curation tutorial](https://indra.readthedocs.io/en/latest/tutorials/html_curation.html) |\n| TSV (Tab/Comma Separated Values)               | Spreadsheet-based browsing and curation                | [`indra.assemblers.tsv`](https://indra.readthedocs.io/en/latest/modules/assemblers/tsv_assembler.html)  |                     |\n| Index Cards                                    | Custom JSON format for curating biological mechanisms  | [`indra.assemblers.index_card`](https://indra.readthedocs.io/en/latest/modules/assemblers/index_card_assembler.html) |               |\n\n### Internal knowledge assembly\n\nA key feature of INDRA is providing internal knowledge-assembly modules\nthat operate on INDRA Statements and perform the following tasks:\n- Redundancy/subsumption/generalization/contradiction finding and resolution\nwith respect to an ontology with the Preassembler\n([`indra.preassembler.Preassembler`](https://indra.readthedocs.io/en/latest/modules/preassembler/index.html#module-indra.preassembler))\n- Belief calculation based on evidence using the BeliefEngine\n([`indra.belief`](https://indra.readthedocs.io/en/latest/modules/belief/index.html))\n- Mapping grounding between multiple ontologies\n([`indra.preassembler.ont_mapper.OntMapper`](https://indra.readthedocs.io/en/latest/modules/preassembler/index.html#module-indra.preassembler.ontology_mapper))\n- Grounding override and disambiguation\n([`indra.preassembler.grounding_mapper.GroundingMapper`](https://indra.readthedocs.io/en/latest/modules/preassembler/index.html#module-indra.preassembler.grounding_mapper))\n- Protein sequence mapping ([`indra.preassembler.site_mapper.SiteMapper`](https://indra.readthedocs.io/en/latest/modules/preassembler/index.html#module-indra.preassembler.sitemapper))\n\nThe internal assembly steps of INDRA including the ones listed above, and also\na large collection of filters (filter by source, belief, the presence of\ngrounding information, semantic filters by entity role, etc.) are exposed\nin the\n[indra.tools.assemble_corpus](http://indra.readthedocs.io/en/latest/modules/tools/index.html#module-indra.tools.assemble_corpus) \nsubmodule. This submodule contains functions that\ntake Statements as input and produce processed Statements as output. They can\nbe composed to form an assembly pipeline connecting knowledge collected from\nsources with an output model.\n\nThis diagram illustrates the assembly pipeline process.\n\n![assembly](doc/images/assembly.png)\n\nThe choice of assembly functions can vary depending on the domain (i.e,\nbiology or world modeling), the modeling goal (i.e., the type of model that\nwill be assembled and how that model will be used), desired features, and\nconfidence (e.g., filter to human genes only or apply a belief cutoff),\nand any other user preferences.\n\nAn example of a typical assembly pipeline for biology statements is as follows.\nSome of the below steps can be removed, rearranged, and other steps added\nto change the assembly pipeline.\n\n[//]: # (If code is changed here, also update it in tests/test_docs_code.py)\n\n```python\nfrom indra.tools import assemble_corpus as ac\nstmts = <the collection of all raw statements to use>\nstmts = ac.filter_no_hypothesis(stmts)  # Filter out hypothetical statements\nstmts = ac.map_grounding(stmts)         # Map grounding\nstmts = ac.filter_grounded_only(stmts)  # Filter out ungrounded agents\nstmts = ac.filter_human_only(stmts)     # Filter out non-human genes\nstmts = ac.map_sequence(stmts)          # Map sequence\nstmts = ac.run_preassembly(stmts,       # Run preassembly\n                           return_toplevel=False)\nstmts = ac.filter_belief(stmts, 0.8)    # Apply belief cutoff of 0.8\n```\n\nAn example of an assembly pipeline for statements in the world modeling domain\nis as follows (note how biology-specific functions are not used, and a custom\nbelief_scorer and ontology is passed to `run_preassembly` here, while the\nbiology pipeline used default values). Note that this example requires\nthe `indra_world` package to be installed.\n\n[//]: # (If code is changed here, also update it in tests/test_docs_code.py)\n\n```python\nfrom indra.tools import assemble_corpus as ac\nfrom indra_world.belief.wm_scorer import get_eidos_scorer\nfrom indra_world.ontology.world import world_ontology\nstmts = <the collection of all raw statements to use>\nstmts = ac.filter_grounded_only(stmts)  # Filter out ungrounded agents\nbelief_scorer = get_eidos_scorer()\nstmts = ac.run_preassembly(stmts,       # Run preassembly\n                           return_toplevel=False,\n                           belief_scorer=belief_scorer,\n                           ontology=world_ontology,\n                           normalize_equivalences=True,     # Optional: rewrite equivalent groundings to one standard\n                           normalize_opposites=True,        # Optional: rewrite opposite groundings to one standard\n                           normalize_ns='WM')               # Use 'WM' namespace to normalize equivalences and opposites \nstmts = ac.filter_belief(stmts, 0.8)    # Apply belief cutoff of e.g., 0.8\n```\nAssembled statements returned after running the assembly pipeline can be\npassed into any of the output model assemblers.\n\n### Other modules\n\nINDRA also contains modules to access literature content (e.g., PubMed, Elsevier), available in [`indra.literature`](\nhttps://indra.readthedocs.io/en/latest/modules/literature/index.html), and \naccess ontological information and convert between identifiers (e.g., UniProt, \nHGNC), available in [`indra.databases`](\nhttps://indra.readthedocs.io/en/latest/modules/databases/index.html).\nA full list of further INDRA modules is available in the [`documentation`](\nhttps://indra.readthedocs.io/en/latest/modules/index.html).\n\n## Citation\n\nGyori B.M., Bachman J.A., Subramanian K., Muhlich J.L., Galescu L., Sorger P.K.\n[From word models to executable models of signaling networks using automated\nassembly](http://msb.embopress.org/content/13/11/954) (2017),\nMolecular Systems Biology, 13, 954.\n\nBachman J.A., Gyori B.M., Sorger P.K.\n[Automated assembly of molecular mechanisms at scale from text mining and\ncurated databases](https://doi.org/10.15252/msb.202211325) (2023),\nMolecular Systems Biology, e11325.\n\n## Installation\n\nFor detailed installation instructions,\n[see the documentation](http://indra.readthedocs.io/en/latest/installation.html).\n\nINDRA currently supports Python 3.8-3.10. The last release of INDRA compatible\nwith Python 2.7 is 1.10, the last release fully compatible with Python 3.5\nis 1.17. Most usages of INDRA will work with other Python versions, however,\nfull compatibility is currently only tested with 3.8-3.10.\n\nThe preferred way to install INDRA is by pointing pip to the source repository\nas\n\n    $ pip install git+https://github.com/sorgerlab/indra.git\n\nReleases of INDRA are also available on\n[PyPI](https://pip.pypa.io/en/latest/installing/), you can install the latest\nrelease as\n\n    $ pip install indra\n\nHowever, releases will usually be behind the latest code available in this\nrepository.\n\nINDRA depends on a few standard Python packages. These packages are installed\nby pip during setup.\nFor certain modules and use cases, other ""extra"" dependencies may be needed,\nwhich are described in detail in the\n[documentation](http://indra.readthedocs.io/en/latest/installation.html).\n\n\n## INDRA REST API\nA REST API for INDRA is available at http://api.indra.bio:8000.\nNote that the REST API is ideal for prototyping and for building light-weight\nweb apps, but should not be used for large reading and assembly workflows.\n\n\n## INDRA Docker\nINDRA is available as a Docker image on Dockerhub and can be pulled as\n\n```\ndocker pull labsyspharm/indra\n```\n\nYou can run the INDRA REST API using the container as\n```\ndocker run -id -p 8080:8080 --entrypoint python labsyspharm/indra /sw/indra/rest_api/api.py\n```\n\nThe Dockerfile to build the image locally is available in a separate repository\nat https://github.com/indralab/indra_docker.\n\n## Using INDRA\n\nIn this example INDRA assembles a PySB model from the natural language\ndescription of a mechanism via the [TRIPS reading web\nservice](http://trips.ihmc.us/parser/cgi/drum).\n\n[//]: # (If code is changed here, also update it in tests/test_docs_code.py)\n\n```python\nfrom indra.sources import trips\nfrom indra.assemblers.pysb import PysbAssembler\npa = PysbAssembler()\n# Process a natural language description of a mechanism\ntrips_processor = trips.process_text('MEK2 phosphorylates ERK1 at Thr-202 and Tyr-204')\n# Collect extracted mechanisms in PysbAssembler\npa.add_statements(trips_processor.statements)\n# Assemble the model\nmodel = pa.make_model(policies='two_step')\n```\n\nINDRA also provides an interface for the\n[REACH](http://github.com/clulab/reach) natural language\nprocessor. In this example, a full paper from [PubMed\nCentral](http://www.ncbi.nlm.nih.gov/pmc/) is processed. The paper's PMC ID is\n[PMC8511698](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8511698/). The example\nassumest that a REACH server is running locally (see documentation at\n[`indra.sources.reach`](https://indra.readthedocs.io/en/latest/modules/sources/reach/index.html)).\nNote that REACH takes about 8 minutes to read this full-text paper.\n\n\n[//]: # (If code is changed here, also update it in tests/test_docs_code.py)\n\n```python\nfrom indra.sources import reach\nreach_processor = reach.process_pmc('PMC8511698', url=reach.local_nxml_url)\n```\nAt this point, `reach_processor.statements` contains a list of INDRA statements\nextracted from the PMC paper.\n\nNext we look at an example of reading the 10 most recent PubMed abstracts on\nBRAF and collecting the results in INDRA statements.\n\n[//]: # (If code is changed here, also update it in tests/test_docs_code.py)\n\n```python\nfrom indra.sources import reach\nfrom indra.literature import pubmed_client\n# Search for 10 most recent abstracts in PubMed on 'BRAF'\npmids = pubmed_client.get_ids('BRAF', retmax=10)\nall_statements = []\nfor pmid in pmids:\n    abs = pubmed_client.get_abstract(pmid)\n    if abs is not None:\n        reach_processor = reach.process_text(abs, url=reach.local_text_url)\n        if reach_processor is not None:\n            all_statements += reach_processor.statements\n```\nAt this point, the `all_statements` list contains all the statements\nextracted from the 10 abstracts.\n\nThe next example shows querying the [BEL large\ncorpus](https://github.com/cthoyt/selventa-knowledge)\nnetwork for a neighborhood of a given list of proteins using their\nHGNC gene names. This example performs the query via PyBEL.\n\n[//]: # (If code is changed here, also update it in tests/test_docs_code.py)\n\n```python\nfrom indra.sources import bel\n# Process the neighborhood of BRAF and MAP2K1\nbel_processor = bel.process_pybel_neighborhood(['BRAF', 'MAP2K1'])\n```\nAt this point, `bel_processor.statements` contains a list of INDRA statements\nextracted from the neighborhood query.\n\nNext, we look at an example of querying the [Pathway Commons\ndatabase](http://pathwaycommons.org) for paths between two lists of proteins.\n\n[//]: # (If code is changed here, also update it in tests/test_docs_code.py)\n\n```python\nfrom indra.sources import biopax\n# Process the neighborhood of BRAF and MAP2K1\nbiopax_processor = biopax.process_pc_pathsfromto(['BRAF', 'RAF1'], ['MAP2K1', 'MAP2K2'])\n```\nAt this point, `biopax_processor.statements` contains a list of INDRA \nStatements extracted from the paths-from-to query.\n\n## Funding\n\nThe development of INDRA has been funded from the following sources:\n\n| Program                                          | Grant number         |\n|--------------------------------------------------|----------------------|\n| DARPA Big Mechanism                              | W911NF-14-1-0397     |\n| DARPA World Modelers                             | W911NF-18-1-0014     |\n| DARPA Communicating with Computers               | W911NF-15-1-0544     |\n| DARPA Automated Scientific Discovery Framework   | W911NF-18-1-0124     |\n| DARPA Automating Scientific Knowledge Extraction | HR00111990009        |\n| DARPA Panacea                                    | HR00111920022        |\n| DARPA Young Faculty Award                        | W911NF-20-1-0255     |\n",167,bioinformatics,Python,5,Python,JavaScript,HTML,CSS,Dockerfile,,,,,,,,,,,,,,,,,,,,,,,,993,49,936,8,13,24,1493,380902,64,460,424,36,e064450975352a4d261a58650aa023239b407d30,Merge pull request #1453 from bgyori/signor_fix,2024-07-10T16:00:04Z,Benjamin M. Gyori,ben.gyori@gmail.com,bgyori,INDRA v1.22.0,"Package structure and dependencies\r\n- Protmapper and Gilda version requirements increased, PySB dependency restriction relaxed\r\n\r\nCore assembly modules\r\n- Ontology graph represents obsolete ID replacements in [`indra.ontology`](https://indra.readthedocs.io/en/latest/modules/ontology/ontology.html)\r\n- Improved BioOntology graph which now includes MONDO, ECCODE, and improves OBO hierarchy import to avoid cycles [`indra.ontology.bio`](https://indra.readthedocs.io/en/latest/modules/ontology/bio_ontology.html)\r\n- Identifier standardization now applies replaced IDs in [`indra.ontology.standardize`](https://indra.readthedocs.io/en/latest/modules/ontology/standardize.html)\r\n- Refactor preassembler refinement finding to expose more endpoints in [`indra.preassembler.refinement`](https://indra.readthedocs.io/en/latest/modules/preassembler/preassembler.html#module-indra.preassembler.refinement)\r\n- Refinement finding optimized when split index isn used in [`indra.preassembler.refinement`](https://indra.readthedocs.io/en/latest/modules/preassembler/preassembler.html#module-indra.preassembler.refinement)\r\n- Grounding mapping disambiguation additional back-end options in [`indra.preassembler.grounding_mapper`](https://indra.readthedocs.io/en/latest/modules/preassembler/grounding_mapper.html)\r\n- Ontology analysis extends to finding cycles [`indra.tools.analyze_ontology`](https://indra.readthedocs.io/en/latest/modules/tools/index.html#module-indra.tools.analyze_ontology)\r\n\r\nStatement representation\r\n- Additional functions for JSON (de)serialization of single Statements in [`indra.statements`](https://indra.readthedocs.io/en/latest/modules/statements.html#module-indra.statements.io)\r\n- Validation: multiple validators supported, refactored to use classes [`indra.statements.validate`](https://indra.readthedocs.io/en/latest/modules/statements.html#module-indra.statements.validate)\r\n\r\nKnowledge sources\r\n- New API and processor for running the SemRep reading system and processing its outputs [`indra.sources.semrep`](https://indra.readthedocs.io/en/latest/modules/sources/semrep.html)\r\n- New API and processor for CREEDS data in [`indra.sources.creeds`](https://indra.readthedocs.io/en/latest/modules/sources/creeds/index.html)\r\n- New API and processor for UbiBrowser in [`indra.sources.ubibrowser`](https://indra.readthedocs.io/en/latest/modules/sources/ubibrowser.html)\r\n- New API and processor for the ACSN database in [`indra.sources.acsn`](https://indra.readthedocs.io/en/latest/modules/sources/acsn.html)\r\n- BEL processor now uses Selventa IDs instead of names in [`indra.sources.bel`](https://indra.readthedocs.io/en/latest/modules/sources/bel/index.html)\r\n- Improved identifier extraction from BioPAX in [`indra.sources.biopax`](https://indra.readthedocs.io/en/latest/modules/sources/biopax/index.html)\r\n- Eidos reader/CLI updated to work with the latest Eidos version in [`indra.sources.eidos`](https://indra.readthedocs.io/en/latest/modules/sources/eidos/index.html)\r\n- Improved HPRD API with option to process compressed archive directly in [`indra.sources.hprd`](https://indra.readthedocs.io/en/latest/modules/sources/hprd/index.html)\r\n- MINERVA client improved to process latest project automatically in [`indra.sources.minerva`](https://indra.readthedocs.io/en/latest/modules/sources/minerva/index.html)\r\n- Improved OmniPath processing to decouple evidences among Statements in [`indra.sources.omnipath`](https://indra.readthedocs.io/en/latest/modules/sources/omnipath/index.html)\r\n- Streamlined RLIMS-P API with improved identifier extraction in [`indra.sources.rlimsp`](https://indra.readthedocs.io/en/latest/modules/sources/rlimsp/index.html)\r\n- SIGNOR processor is now compatible with latest data format [`indra.sources.signor`](https://indra.readthedocs.io/en/latest/modules/sources/signor/index.html)\r\n\r\n\r\nModel assemblers\r\n- Refactored (more options, better consistency) styling for the HTML Assembler in [`indra.assemblers.html`](https://indra.readthedocs.io/en/latest/modules/assemblers/html_assembler.html)\r\n- New option in IndraNetAssembler to represent self loops [`indra.assemblers.indranet`](https://indra.readthedocs.io/en/latest/modules/assemblers/indranet_assembler.html)\r\n\r\nModel analysis\r\n- Additional options added to ModelChecker to control explanation paths [`indra.explanation.model_checker`](https://indra.readthedocs.io/en/latest/modules/explanation/model_checker.html)\r\n\r\nTools\r\n- Additional cases handled in validity fixing of identifiers [`indra.tools.fix_invalidities`](https://indra.readthedocs.io/en/latest/modules/tools/index.html#module-indra.tools.fix_invalidities)\r\n\r\nResources and database clients\r\n- New client to access Bioregistry content in [`indra.databases.bioregistry_client`](https://indra.readthedocs.io/en/latest/modules/databases/index.html#module-indra.databases.bioregistry_client)\r\n- New client to access the Expasy enzyme (EC code) database in [`indra.databases.ec_client`](https://indra.readthedocs.io/en/latest/modules/databases/index.html#module-indra.databases.ec_client)\r\n- New client to interact with the Biolookup web service in [`indra.databases.biolookup_client`](https://indra.readthedocs.io/en/latest/modules/databases/index.html#module-indra.databases.biolookup_client)\r\n- Extended HGNC client to provide enzyme code mappings for genes in [`indra.databases.hgnc_client`](https://indra.readthedocs.io/en/latest/modules/databases/index.html#module-indra.databases.hgnc_client)\r\n- Extended MeSH client to access CAS mappings for compounds [`indra.databases.mesh_client`](https://indra.readthedocs.io/en/latest/modules/databases/index.html#module-indra.databases.mesh_client)\r\n- New client to interact with MONDO diseases [`indra.databases.mondo_client`](https://indra.readthedocs.io/en/latest/modules/databases/index.html#module-indra.databases.mondo_client)\r\n- Extended OBO client allowing access to more ontologies [`indra.databases.obo_client`](https://indra.readthedocs.io/en/latest/modules/databases/index.html#module-indra.databases.obo_client)\r\n- Extended PubChem client with additional functionalities [`indra.databases.pubchem_client`](https://indra.readthedocs.io/en/latest/modules/databases/index.html#module-indra.databases.pubchem_client)\r\n- Multiple resource files updated including for Biomappings, ChEBI, FamPlex, HGNC,\r\n  identifiers.org, MeSH, and Selventa entries\r\n\r\nLiterature clients\r\n- PubMed client extended to provide substance-specific publications [`indra.literature.pubmed_client`](https://indra.readthedocs.io/en/latest/modules/literature/index.html#module-indra.literature.pubmed_client)\r\n- New client to access COCI data from OpenCitations in [`indra.literature.coci_client`](https://indra.readthedocs.io/en/latest/modules/literature/index.html#module-indra.literature.coci_client)\r\n",1.22.0,Benjamin M. Gyori,,bgyori,"BSD 2-Clause ""Simplified"" License",indra,sorgerlab,24,systems-biology,modeling,biology,computational-biology,bioinformatics,pysb,nlp,indra,sbml,,,,,,,,,,,,/sorgerlab/indra,30,23,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/sofa-framework/sofa,https://github.com/sofa-framework/sofa,1,,,1,1,1,1,0,0,0,0,0,0,1,Real-time multi-physics simulation with an emphasis on medical simulation.,"[![SOFA, Simulation Open-Framework Architecture](https://www.sofa-framework.org/wp-content/uploads/2013/01/SOFA_LOGO_ORANGE_2-normal.png)](https://www.sofa-framework.org/)\n\n<br/>\n\n[![Documentation](https://img.shields.io/badge/doc-on_website-brightgreen.svg)](https://www.sofa-framework.org/community/doc/)\n[![Support](https://img.shields.io/badge/support-on_GitHub_Discussions-orange.svg)](https://github.com/sofa-framework/sofa/discussions)\n[![Discord](https://img.shields.io/badge/chat-on_Discord-darkred.svg)](https://discord.gg/G63t3a8Ra6)\n\n<a href=""https://twitter.com/intent/follow?original_referer=https%3A%2F%2Fpublish.twitter.com%2F%3FbuttonType%3DFollowButton%26query%3Dhttps%253A%252F%252Ftwitter.com%252FSofaFramework%26widget%3DButton&ref_src=twsrc%5Etfw&region=follow_link&screen_name=SofaFramework&tw_p=followbutton""><img src=""https://img.shields.io/twitter/follow/SofaFramework?label=Follow%20%40SofaFramework&style=social""></a>\n[![Contact](https://img.shields.io/badge/contact-on_website-brightgreen.svg)](https://www.sofa-framework.org/consortium/contact/)\n[![we're hiring](https://img.shields.io/badge/we're%20hiring!-join%20us-orange)](https://www.sofa-framework.org/about/jobs/)\n\n## What is SOFA\n\nSOFA is an open source framework targeted at interactive physics simulation based on the Finite Element Method (FEM), with an emphasis on medical simulation and robotics.  \nIt is mainly intended for the research community to help foster newer algorithms, but can also be used as an efficient prototyping tool.  \nSOFA's advanced software architecture allows:  \n- the creation of complex and evolving simulations by combining new algorithms with existing algorithms \n- the modification of key parameters of the simulation such as deformable behavior, \n  surface representation, solvers, constraints, collision algorithm, etc.  \n- the synthesis of complex models from simpler ones using a graph description\n- the efficient simulation of the dynamics of interacting objects using abstract equation solvers\n- the comparison of various algorithms and mathematical models\n\nSOFA is often presented as a standalone software (runSofa) and a simulation tool, but the project is most importantly a bundle of libraries and thus can be used/integrated in any project.  \n\nSOFA provides a plugin system allowing the coupling of additional codes to add functionalities. A lot of plugins are already referenced in the [source code](https://github.com/sofa-framework/sofa/tree/master/applications/plugins), under [sofa-framework organisation](https://github.com/orgs/sofa-framework/repositories?q=plugin&sort=name), and on the [Marketplace](https://www.sofa-framework.org/applications/marketplace/). Feel free to contact us to get your own plugins referenced!\n\n\n## How to download and install\n\nPlease refer to SOFA download page: https://www.sofa-framework.org/download/\n\n\n## How to contribute\n\nThe SOFA community will be pleased to welcome you!  \nFind all the ways to contribute to the project: https://www.sofa-framework.org/community/get-involved/\n\nBefore creating any issue or pull request, please read carefully [our CONTRIBUTING rules](https://github.com/sofa-framework/sofa/blob/master/CONTRIBUTING.md).\n\n\n## Information\n\n### Authors\nSee [Authors.txt](https://github.com/sofa-framework/sofa/blob/master/Authors.txt)\n\n### Licenses\nSOFA is LGPL, except:\n- applications/projects (GPL)\n- applications/tutorials (GPL)\n- directories with a license file specifying a different license\n\nLGPL refers to the GNU Lesser General Public License as published by the Free Software\nFoundation; either version 2.1 of the License, or (at your option) any later \nversion.\n\nGPL refers to the GNU General Public License as published by the Free Software Foundation;\neither version 2 of the License, or (at your option) any later version.\n\n### Contact\ncontact@sofa-framework.org\n\n-----------------------------------------------------------------------------\n\nSOFA, Simulation Open-Framework Architecture  \n(c) 2006 INRIA, USTL, UJF, CNRS, MGH\n",896,physics,C++,17,CMake,C++,C,Python,HTML,Shell,TeX,Cuda,GLSL,CSS,Objective-C,Awk,PHP,sed,SWIG,Qt Script,QMake,,,,,,,,,,,,3253,339,2867,47,71,104,0,471678,305,814,512,302,6d71ea77888fb24f92d120de4f573db64934b3c6,[All] Reduce includes to helper/set.h (#4820),2024-07-17T19:24:24Z,Alex Bilger,alxbilger@users.noreply.github.com,alxbilger,v24.06.00,"## Changes\r\n\r\nChangelog: https://github.com/sofa-framework/sofa/blob/v24.06/CHANGELOG.md#v240600\r\nDiff with v23.12: https://github.com/sofa-framework/sofa/compare/v23.12...v24.06\r\n\r\n## Required dependencies\r\n\r\n### Windows\r\n\r\n1. Install **[Microsoft Visual C++ 2022 Redistributable](https://aka.ms/vs/17/release/vc_redist.x64.exe)**.\r\n2. Install **Python 3.10 + Numpy + Scipy + pybind11** if you want to use the SofaPython3 plugin or any of its dependers.\r\n        Download and install [**Python 3.10 (amd64)**](https://www.python.org/ftp/python/3.10.10/python-3.10.10-amd64.exe).  \r\n        Make sure to enable PIP installation and addition to PATH.  \r\n        Then, open a console and run `python -V && python -m pip install numpy scipy pybind11==2.9.1`\r\n\r\n### Linux\r\n\r\n1. Install **libopengl0** \r\n\r\n        sudo apt install libopengl0 \r\n\r\n3. Install **Python 3.10 + Numpy + Scipy + pybind11** if you want to use the SofaPython3 plugin or any of its dependers.\r\n\r\n        sudo apt install python3.10-dev python3.10-distutils pybind11-dev\r\n        curl -L https://bootstrap.pypa.io/pip/get-pip.py --output /tmp/get-pip3.py\r\n        python3.10 /tmp/get-pip3.py\r\n        python3.10 -m pip install --upgrade pip\r\n        python3.10 -m pip install numpy scipy pybind11==2.9.1\r\n\r\n### MacOS\r\n\r\n1. All core dependencies are included in the binaries.\r\n\r\n2. Install **Python 3.10 + Numpy + Scipy + pybind11** if you want to use the SofaPython3 plugin or any of its dependers.\r\n\r\n        brew install python@3.10\r\n        brew link --force python@3.10\r\n        python3 -m pip install --upgrade pip\r\n        python3 -m pip install numpy scipy\r\n        brew install pybind11\r\n\r\n## Check SHA256 checksums\r\n\r\nFeel free to check SHA256 checksums of our official binaries:\r\n- SOFA_v24.06.00_Linux.run → `c7b3a44498845b7558bd2acae664f4349582a159b820482827610679058e42cf`\r\n- SOFA_v24.06.00_Linux.zip → `9d515e2f25f657c744821be8a5361e22803c18947b33af7a0b357c259202236a`\r\n- SOFA_v24.06.00_MacOS.zip → `5be15622ff75fae962390f0d3837147fba73897cd205c7a69c41220102106e06`\r\n- SOFA_v24.06.00_Win64.exe → `28a3483726e53e6799219c840757f82b4ca89ed77ba649e781c42c01a8e2b3c7`\r\n- SOFA_v24.06.00_Win64.zip → `06c704106d68174ed290e9db218508847a8ba72295a9f5c4faabe83d2bd946b1`\r\n\r\nCompare these sha with yours:\r\n- Windows : on Powershell `Get-FileHash path/to/file`\r\n- Ubuntu : in a terminal `sha256sum path/to/file`\r\n- MacOS : in a terminal `shasum -a 256 /path/to/file`\r\n\r\n## Info for developers\r\n\r\nSOFA binaries were generated using the following libraries:\r\n|              | Qt     | Boost  | Eigen | Python | pybind11 | TinyXML2 | _Glew_  | _Zlib_   | _libPNG_ | _libJPEG_ | _libTIFF_ |\r\n|----------------|--------|--------|-------|--------|----------|----------|-------|--------|--------|---------|---------|\r\n| **Windows**    | 5.12.12 | 1.74.0 | 3.4.0 | 3.10.10  | 2.9.1   | 9.0.0   | _1.13_  | _1.2.8_  | _1.6.18_ | _6b_      | _4.0.9_   |\r\n| **Linux**      | 5.12.12 | 1.71.0 | 3.3.7 | 3.10.10 |  2.9.1    | 9.0.0   | _2.2.0_ | _1.2.11_ | _1.6.37_ | _8c_      | _4.3.0_   |\r\n| **MacOS**      | 5.12.8 | 1.76.0 | 3.4.0 | 3.10.10  | 2.6.2    | 10.0.0  | _2.2.0_ | _1.2.11_ | _1.6.37_ | _9e_      | _4.3.0_  |\r\n",v24.06.00,Paul Baksic,,bakpaul,GNU Lesser General Public License v2.1,sofa,sofa-framework,33,real-time,simulation,medical,physics,cpp,framework,engine,research,sofa-framework,,,,,,,,,,,,/sofa-framework/sofa,49,57,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/soedinglab/plass,https://github.com/soedinglab/plass,1,,,1,1,1,1,0,0,0,0,0,0,1,sensitive and precise assembly of short sequencing reads,"# PLASS and PenguiN assembler\n[![BioConda Install](https://img.shields.io/conda/dn/bioconda/plass.svg?style=flag&label=BioConda%20install)](https://anaconda.org/bioconda/plass)\n[![BioContainer Pulls](https://img.shields.io/endpoint?url=https%3A%2F%2Fmmseqs.com%2Fbiocontainer.php%3Fcontainer%3Dplass)](https://biocontainers.pro/#/tools/plass)\n[![DOI](https://zenodo.org/badge/118119513.svg)](https://zenodo.org/badge/latestdoi/118119513)\n\nPlass (Protein-Level ASSembler) and PenguiN (Protein guided nucleotide assembler) are software to assemble protein sequences or DNA/RNA contigs from short read sequencing data meant to work best for complex metagenomic or metatranscriptomic datasets. Plass and Penguin are GPL-licensed open source software implemented in C++ and available for Linux and macOS and are designed to run on multiple cores. \n\n[Plass:](https://github.com/soedinglab/plass/tree/master?tab=readme-ov-file#plass---protein-level-assembler) [Steinegger M, Mirdita M and Soeding J. Protein-level assembly increases protein sequence recovery from metagenomic samples manyfold. Nature Methods, doi: doi.org/10.1038/s41592-019-0437-4 (2019)](https://www.nature.com/articles/s41592-019-0437-4).\n\n[PenguiN:](https://github.com/soedinglab/plass/tree/master?tab=readme-ov-file#penguin---Protein-guided-Nucleotide-Assembler) [Jochheim A, Jochheim FA, Kolodyazhnaya A, Morice E, Steinegger M, Soeding J. Strain-resolved de-novo metagenomic assembly of viral genomes and microbial 16S rRNAs. bioRxiv (2024)](https://www.biorxiv.org/content/10.1101/2024.03.29.587318v1)\n\n<p align=""center""><img src=""https://raw.githubusercontent.com/soedinglab/plass/master/.github/plass.png"" height=""256"" /></p>\n\n### Soil Reference Catalog (SRC) and Marine Eukaryotic Reference Catalog (MERC)\nSRC was created by assembling 640 soil metagenome samples. MERC was assembled from the the metatranscriptomics datasets created by the TARA ocean expedition. Both catalogues were redundancy reduced to 90% sequence identity at 90% coverage.\nEach catalog is a single FASTA file containing the sequences, the header identifiers contain the Sequence Read Archive (SRA) identifiers.\nThe catalogues can be downloaded [here](http://wwwuser.gwdg.de/~compbiol/plass/current_release/).\nWe provide a [HH-suite3](https://github.com/soedinglab/hh-suite) database called ""BFD"" containing sequences from the Metaclust, SRC, MERC and Uniport at [here](https://bfd.mmseqs.com/).\n\n# PenguiN - Protein-guided Nucleotide assembler\nPenguiN a software to assemble short read sequencing data on a nucleotide level. In a first step it assembles coding sequences using the information from the translated protein sequences. In a second step it links them across non-coding regions. The main purpose of PenguiN is the assembly of complex metagenomic and metatranscriptomic datasets. It was especially tested for the assembly of viral genomes as well as 16S rRNA gene sequences. It assembles 3-40 times more complete viral genomes and six times as many 16S rRNA sequences than state of the art assemblers like Megahit and the SPAdes variants.\n\n### Install Plass and PenguiN\nOur software can be install via [conda](https://github.com/conda-forge/miniforge) or as statically compiled binaries. It requires a 64-bit Linux or macOS system.\n\n     # install from bioconda\n     conda install -c conda-forge -c bioconda plass \n     # install docker\n     docker pull ghcr.io/soedinglab/plass:latest\n     # static build with AVX2 (fastest)\n     wget https://mmseqs.com/plass/plass-linux-avx2.tar.gz; tar xvfz plass-linux-avx2.tar.gz; export PATH=$(pwd)/plass/bin/:$PATH\n     # static build with SSE4.1\n     wget https://mmseqs.com/plass/plass-linux-sse41.tar.gz; tar xvfz plass-linux-sse41.tar.gz; export PATH=$(pwd)/plass/bin/:$PATH\n     # universal build with macOS (Intel or Apple Silicon)\n     wget https://mmseqs.com/plass/plass-osx-universal.tar.gz; tar xvfz plass-osx-universal.tar.gz; export PATH=$(pwd)/plass/bin/:$PATH\n\nOther precompiled binaries for SSE2, ARM and PowerPC can be found at [mmseqs.com/plass](https://mmseqs.com/plass).\n\n## How to assemble\nPlass and PenguiN can assemble both paired-end reads (FASTQ) and single reads (FASTA or FASTQ):\n\n      # assemble paired-end reads \n      plass assemble examples/reads_1.fastq.gz examples/reads_2.fastq.gz assembly.fas tmp\n\n      # assemble single-end reads \n      plass assemble examples/reads_1.fastq.gz assembly.fas tmp\n\n      # assemble single-end reads using stdin\n      cat examples/reads_1.fastq.gz | plass assemble stdin assembly.fas tmp\n\n\nImportant parameters: \n\n     --min-seq-id         Adjusts the overlap sequence identity threshold\n     --min-length         minimum codon length for ORF prediction (default: 40)\n     -e                   E-value threshold for overlaps \n     --num-iterations     Number of iterations of assembly\n     --filter-proteins    Switches the neural network protein filter off/on\n\nPlass workflows: \n\n      plass assemble      Assembles proteins (i:Nucleotides -> o:Proteins)\n      \n      \nPenguiN workflows: \n\n      penguin guided_nuclassemble  Assembles nucleotides using protein and nucleotide information (i:Nucleotides -> o:Nucleotides)\n      penguin nuclassemble         Assembles nucleotides using only nucleotdie information (i:Nucleotides -> o:Nucleotides)\n\n### Assemble using MPI \nBoth tools can be distributed over several homogeneous computers. However the `tmp` folder has to be shared between all nodes (e.g. NFS). The following command assembles on several nodes:\n\n    RUNNER=""mpirun -np 42"" plass assemble examples/reads_1.fastq.gz examples/reads_2.fastq.gz assembly.fas tmp\n\n\n### Compile from source\nCompiling from source has the advantage that it will be optimized to the specific system, which should improve its performance. To compile `git`, `g++` (4.9 or higher) and `cmake` (3.0 or higher) are required. Afterwards, the PLASS and PenguiN binaries will be located in the `build/bin` directory.\n\n      git clone https://github.com/soedinglab/plass.git\n      cd plass\n      git submodule update --init\n      mkdir build && cd build\n      cmake -DCMAKE_BUILD_TYPE=RELEASE -DCMAKE_INSTALL_PREFIX=. ..\n      make -j 4 && make install\n      export PATH=""$(pwd)/bin/:$PATH""\n        \n:exclamation: If you want to compile PLASS or PenguiN on macOS, please install and use `gcc` from Homebrew. The default macOS `clang` compiler does not support OpenMP and PLASS will not be able to run multithreaded. Use the following cmake call:\n\n      CXX=""$(brew --prefix)/bin/g++-13"" cmake -DCMAKE_BUILD_TYPE=RELEASE -DCMAKE_INSTALL_PREFIX=. ..\n\n#### Dependencies\n\nWhen compiling from source, our sofwtare requires the `zlib` and `bzip` installed.\n\n### Use the docker image\nWe also provide a Docker image of Plass. You can mount the current directory containing the reads to be assembled and run plass with the following command:\n\n      docker run -ti --rm -v ""$(pwd):/app"" -w /app ghcr.io/soedinglab/plass:latest assemble reads_1.fastq reads_2.fastq assembly.fas tmp\n\n## Hardware requirements\nPlass needs roughly 1 byte of memory per residue to work efficiently. Plass will scale its memory consumption based on the available main memory of the machine. Plass needs a CPU with at least the SSE4.1 instruction set to run. \n\n## Known problems \n* The assembly of Plass includes all ORFs having a start and end codon that includes even very short ORFs < 60 amino acids. Many of these short ORFs are spurious since our neural network cannot distingue them well. We would recommend to use other method to verify the coding potential of these. Assemblies above 100 amino acids are mostly genuine protein sequences. \n* Plass in default searches for ORFs of 40 amino acids or longer. This limits the read length to > 120. To assemble this protein, you need to lower the `--min-length` threshold. Be aware using short reads (< 100 length) might result in lower sensitivity.\n",132,proteins,C,15,CMake,Shell,C++,C,Dockerfile,Perl,R,Batchfile,Makefile,Python,Meson,Lua,Starlark,HTML,Roff,,,,,,,,,,,,,,1,0,1,0,4,10,0,28369,14,43,17,26,5ac4c80a5f9bf2d991bea211243b2fcac07cfe30,added shellcheck disable line,2024-06-12T20:58:36Z,Annika Jochheim,annika.jochheim@mpinat.mpg.de,AnnSeidel,Release 5-cf8933,"# Plass & Penguin Release Notes\r\n\r\nFirst release of Penguin, a metagenomic assembler that assembles DNA/RNA through a novel greedy AA/DNA-hybrid bayesian overlap extension strategy.\r\n\r\n## New Features and Enhancements\r\n- **First release of Penguin**: We generate now two binaries, `plass` and `penguin`. Plass assembles protein sequences from DNA while Penguin assembles DNA contigs. Penguin comes in two variants `penguin guided_nuclassemble`, which first assembles using AA six-framed-translated overlaps and then further assemble the contigs using nucleotide information and a pure nucleotide assembler `penguin nuclassemble`.\r\n- **Compatibility and Portability**: Thanks to [simde](https://github.com/simd-everywhere/simde) Plass and Penguin now run on ARM (including Apple Silicon) and POWERPC.",5-cf8933,Martin Steinegger,,martin-steinegger,GNU General Public License v3.0,plass,soedinglab,6,bioinformatics,metagenomics,sequence-assembler,proteins,opensource,proteomics,metatranscriptomics,,,,,,,,,,,,,,/soedinglab/plass,6,10,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/soedinglab/MMseqs2,https://github.com/soedinglab/MMseqs2,1,,,1,1,1,1,0,0,0,0,0,0,1,MMseqs2: ultra fast and sensitive search and clustering suite,"# MMseqs2: ultra fast and sensitive sequence search and clustering suite\nMMseqs2 (Many-against-Many sequence searching) is a software suite to search and cluster huge protein and nucleotide sequence sets. MMseqs2 is open source GPL-licensed software implemented in C++ for Linux, MacOS, and (as beta version, via cygwin) Windows. The software is designed to run on multiple cores and servers and exhibits very good scalability. MMseqs2 can run 10000 times faster than BLAST. At 100 times its speed it achieves almost the same sensitivity. It can perform profile searches with the same sensitivity as PSI-BLAST at over 400 times its speed.\n\n##  Publications\n\n[Steinegger M and Soeding J. MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets. Nature Biotechnology, doi: 10.1038/nbt.3988 (2017)](https://www.nature.com/articles/nbt.3988).\n\n[Steinegger M and Soeding J. Clustering huge protein sequence sets in linear time. Nature Communications, doi: 10.1038/s41467-018-04964-5 (2018)](https://www.nature.com/articles/s41467-018-04964-5).\n\n[Mirdita M, Steinegger M and Soeding J. MMseqs2 desktop and local web server app for fast, interactive sequence searches. Bioinformatics, doi: 10.1093/bioinformatics/bty1057 (2019)](https://academic.oup.com/bioinformatics/article/35/16/2856/5280135).\n\n[Mirdita M, Steinegger M, Breitwieser F, Soding J, Levy Karin E: Fast and sensitive taxonomic assignment to metagenomic contigs. Bioinformatics, doi: 10.1093/bioinformatics/btab184 (2021)](https://doi.org/10.1093/bioinformatics/btab184).\n\n[![BioConda Install](https://img.shields.io/conda/dn/bioconda/mmseqs2.svg?style=flag&label=BioConda%20install)](https://anaconda.org/bioconda/mmseqs2) [![Github All Releases](https://img.shields.io/github/downloads/soedinglab/mmseqs2/total.svg)](https://github.com/soedinglab/mmseqs2/releases/latest) [![Biocontainer Pulls](https://img.shields.io/endpoint?url=https%3A%2F%2Fmmseqs.com%2Fbiocontainer.php%3Fcontainer%3Dmmseqs2)](https://biocontainers.pro/#/tools/mmseqs2) [![Build Status](https://dev.azure.com/themartinsteinegger/mmseqs2/_apis/build/status/soedinglab.MMseqs2?branchName=master)](https://dev.azure.com/themartinsteinegger/mmseqs2/_build/latest?definitionId=2&branchName=master) <a href=""https://chat.mmseqs.com/""><img src=""https://chat.mmseqs.com/api/v1/shield.svg?type=online&name=chat&icon=false"" /></a>\n\n<p align=""center""><img src=""https://raw.githubusercontent.com/soedinglab/mmseqs2/master/.github/mmseqs2_logo.png"" height=""256"" /></p>\n\n\n## Documentation\nThe MMseqs2 user guide is available in our [GitHub Wiki](https://github.com/soedinglab/mmseqs2/wiki) or as a [PDF file](https://mmseqs.com/latest/userguide.pdf) (Thanks to [pandoc](https://github.com/jgm/pandoc)!). The wiki also contains [tutorials](https://github.com/soedinglab/MMseqs2/wiki/Tutorials) to learn how to use MMseqs2 with real data. For questions please open an issue on [GitHub](https://github.com/soedinglab/MMseqs2/issues) or ask in our [chat](https://chat.mmseqs.com). \nKeep posted about MMseqs2/Linclust updates by following Martin on [Twitter](https://twitter.com/thesteinegger).\n\n## Installation\nMMseqs2 can be used by [compiling from source](https://github.com/soedinglab/MMseqs2/wiki#installation), downloading a statically compiled binary, using [Homebrew](https://github.com/Homebrew/brew), [conda](https://github.com/conda/conda) or [Docker](https://github.com/moby/moby).\n     \n    # install by brew\n    brew install mmseqs2\n    # install via conda\n    conda install -c conda-forge -c bioconda mmseqs2\n    # install docker\n    docker pull ghcr.io/soedinglab/mmseqs2\n    # static build with AVX2 (fastest)\n    wget https://mmseqs.com/latest/mmseqs-linux-avx2.tar.gz; tar xvfz mmseqs-linux-avx2.tar.gz; export PATH=$(pwd)/mmseqs/bin/:$PATH\n    # static build with SSE4.1\n    wget https://mmseqs.com/latest/mmseqs-linux-sse41.tar.gz; tar xvfz mmseqs-linux-sse41.tar.gz; export PATH=$(pwd)/mmseqs/bin/:$PATH\n    # static build with SSE2 (slowest, for very old systems)\n    wget https://mmseqs.com/latest/mmseqs-linux-sse2.tar.gz; tar xvfz mmseqs-linux-sse2.tar.gz; export PATH=$(pwd)/mmseqs/bin/:$PATH\n\nMMseqs2 requires an AMD or Intel 64-bit system (check with `uname -a | grep x86_64`). We recommend using a system with at least the SSE4.1 instruction set (check by executing `cat /proc/cpuinfo | grep sse4_1` on Linux or `sysctl -a | grep machdep.cpu.features | grep SSE4.1` on MacOS). The AVX2 version is faster than SSE4.1, check if AVX2 is supported by executing `cat /proc/cpuinfo | grep avx2` on Linux and `sysctl -a | grep machdep.cpu.leaf7_features | grep AVX2` on MacOS). A SSE2 version is also available for very old systems.\n\nMMseqs2 also works on ARM64 systems and on PPC64LE systems with POWER8 ISA or newer.\n\nWe provide static binaries for all supported platforms at [mmseqs.com/latest](https://mmseqs.com/latest).\n\nMMseqs2 comes with a bash command and parameter auto completion, which can be activated by adding the following lines to your $HOME/.bash_profile:\n\n<pre>\nif [ -f /<b>Path to MMseqs2</b>/util/bash-completion.sh ]; then\n    source /<b>Path to MMseqs2</b>/util/bash-completion.sh\nfi\n</pre>\n         \n## Getting started\nWe provide `easy` workflows to cluster, search and assign taxonomy. These `easy` workflows are a shorthand to deal directly with FASTA/FASTQ files as input and output. MMseqs2 provides many modules to transform, filter, execute external programs and search. However, these modules use the MMseqs2 database formats, instead of the FASTA/FASTQ format. For maximum flexibility, we recommend using MMseqs2 workflows and modules directly. Please read more about this in the [documentation](https://github.com/soedinglab/mmseqs2/wiki).\n\n### Cluster\n\nFor clustering, MMseqs2 `easy-cluster` and `easy-linclust` are available.\n\n`easy-cluster` by default clusters the entries of a FASTA/FASTQ file using a cascaded clustering algorithm.\n        \n    mmseqs easy-cluster examples/DB.fasta clusterRes tmp --min-seq-id 0.5 -c 0.8 --cov-mode 1\n        \n`easy-linclust` clusters the entries of a FASTA/FASTQ file. The runtime scales linearly with input size. This mode is recommended for huge datasets.\n                \n    mmseqs easy-linclust examples/DB.fasta clusterRes tmp\n                \nRead more about the [clustering format](https://github.com/soedinglab/mmseqs2/wiki#clustering-format) in our user guide.\n                \nPlease adjust the [clustering criteria](https://github.com/soedinglab/MMseqs2/wiki#clustering-criteria) and check if temporary directory provides enough free space. For disk space requirements, see the user guide.\n\n### Search\n         \nThe `easy-search` workflow searches directly with a FASTA/FASTQ files against either another FASTA/FASTQ file or an already existing MMseqs2 database.\n        \n    mmseqs easy-search examples/QUERY.fasta examples/DB.fasta alnRes.m8 tmp\n \nIt is also possible to pre-compute the index for the target database. This reduces overhead when searching repeatedly against the same database.\n\n    mmseqs createdb examples/DB.fasta targetDB\n    mmseqs createindex targetDB tmp\n    mmseqs easy-search examples/QUERY.fasta targetDB alnRes.m8 tmp\n        \nThe `databases` workflow provides download and setup procedures for many public reference databases, such as the Uniref, NR, NT, PFAM and many more (see [Downloading databases](https://github.com/soedinglab/mmseqs2/wiki#downloading-databases)). For example, to download and search against a database containing the Swiss-Prot reference proteins run: \n\n    mmseqs databases UniProtKB/Swiss-Prot swissprot tmp\n    mmseqs easy-search examples/QUERY.fasta swissprot alnRes.m8 tmp\n        \nThe speed and sensitivity of the `search` can be adjusted with `-s` parameter and should be adapted based on your use case (see [setting sensitivity -s parameter](https://github.com/soedinglab/mmseqs2/wiki#set-sensitivity--s-parameter)). A very fast search would use a sensitivity of `-s 1.0`, while a very sensitive search would use a sensitivity of up to `-s 7.0`. A detailed guide how to speed up searches is [here](https://github.com/soedinglab/MMseqs2/wiki#how-to-control-the-speed-of-the-search).\n\nThe output can be customized with the `--format-output` option e.g. `--format-output ""query,target,qaln,taln""` returns the query and target accession and the pairwise alignments in tab separated format. You can choose many different [output columns](https://github.com/soedinglab/mmseqs2/wiki#custom-alignment-format-with-convertalis).\n\n:exclamation: `easy-search` in default computes the sequence identity by dividing the number of identical residues by the alignment length (`numIdentical/alnLen`). However, `search` [estimates](https://github.com/soedinglab/MMseqs2/wiki#how-does-mmseqs2-compute-the-sequence-identity) the identity in default. To output real sequence identity use `--alignment-mode 3` or `-a`.\n\n### Taxonomy\nThe `easy-taxonomy` workflow can be used to assign sequences taxonomical labels. It performs a search against a sequence database with taxonomy information (seqTaxDb), chooses the most representative sets of aligned target sequences according to different strategies (according to `--lca-mode`) and computes the lowest common ancestor among those.\n\n    mmseqs createdb examples/DB.fasta targetDB\n    mmseqs createtaxdb targetDB tmp\n    mmseqs createindex targetDB tmp\n    mmseqs easy-taxonomy examples/QUERY.fasta targetDB alnRes tmp\n\nBy default, `createtaxdb` assigns a Uniprot accession to a taxonomical identifier to every sequence and downloads the NCBI taxonomy. We also support [BLAST](https://github.com/soedinglab/MMseqs2/wiki#create-a-sequence-database-with-taxonomic-information-from-an-existing-blast-database), [SILVA](https://github.com/soedinglab/MMseqs2/wiki#create-a-sequence-database-with-taxonomic-information-for-silva) or [custom taxonomical](https://github.com/soedinglab/MMseqs2/wiki#manually-annotate-a-sequence-database-with-taxonomic-information) databases. Many common taxonomic reference databases can be easily downloaded and set up by the [`databases` workflow](https://github.com/soedinglab/mmseqs2/wiki#downloading-databases).\n\nRead more about the [taxonomy format](https://github.com/soedinglab/MMseqs2/wiki#taxonomy-format) and the [classification](https://github.com/soedinglab/MMseqs2/wiki#taxonomy-assignment-using-mmseqs-taxonomy) in our user guide.\n\n### Supported search modes\n\nMMseqs2 provides many additional search modes:\n * Iterative sequences-profile searches (like PSI-BLAST) with the `--num-iterations` parameter\n * [Translated searches](https://github.com/soedinglab/MMseqs2/wiki#translated-sequence-searching) of nucleotides against proteins (blastx), proteins against nucleotides (tblastn) or nucleotide against nucleotide (tblastx)\n * [Iterative increasing sensitivity searches](https://github.com/soedinglab/MMseqs2/wiki#how-to-find-the-best-hit-the-fastest-way) to find only the best hits faster\n * [Taxonomic assignment](https://github.com/soedinglab/MMseqs2/wiki#taxonomy-assignment-using-mmseqs-taxonomy) using 2bLCA or LCA\n * Fast ungapped alignment searches to find [very similar sequence matches](https://github.com/soedinglab/MMseqs2/wiki#mapping-very-similar-sequences-using-mmseqs-map)\n * Very fast and sensitive searches against [profile databases such as the PFAM](https://github.com/soedinglab/MMseqs2/wiki#how-to-create-a-target-profile-database-from-pfam)\n * [Reciprocal best hits search](https://github.com/soedinglab/MMseqs2/wiki#reciprocal-best-hit-using-mmseqs-rbh)\n * [Web search API and user interface](https://github.com/soedinglab/MMseqs2-App)\n\nMany modes can also be combined. You can, for example, do a translated nucleotide against protein profile search.\n\n### Memory requirements\nMMseqs2 minimum memory requirements for `cluster` or `linclust` is 1 byte per sequence residue, `search` needs 1 byte per target residue. Sequence databases can be compressed using the `--compress` flag, DNA sequences can be reduced by a factor of `~3.5` and proteins by `~1.7`.\n   \nMMseqs2 checks the available system memory and automatically divides the target database in parts that fit into memory. Splitting the database will increase the runtime slightly. It is possible to control the memory usage using `--split-memory-limit`.\n\n### How to run MMseqs2 on multiple servers using MPI\nMMseqs2 can run on multiple cores and servers using OpenMP and Message Passing Interface (MPI).\nMPI assigns database splits to each compute node, which are then computed with multiple cores (OpenMP).\n\nMake sure that MMseqs2 was compiled with MPI by using the `-DHAVE_MPI=1` flag (`cmake -DHAVE_MPI=1 -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=. ..`). Our precompiled static version of MMseqs2 cannot use MPI. The version string of MMseqs2 will have a `-MPI` suffix, if it was built successfully with MPI support.\n\nTo search with multiple servers, call the `search` or `cluster` workflow with the MPI command exported in the RUNNER environment variable. The databases and temporary folder have to be shared between all nodes (e.g. through NFS):\n\n    RUNNER=""mpirun -pernode -np 42"" mmseqs search queryDB targetDB resultDB tmp\n\n## Contributors\n\nMMseqs2 exists thanks to all the people who contribute. \n<a href=""https://github.com/soedinglab/mmseqs2/graphs/contributors"">\n  <img src=""https://contributors-img.firebaseapp.com/image?repo=soedinglab/mmseqs2"" />\n</a>\n",1318,bioinformatics,C,15,CMake,Shell,C++,C,Perl,Batchfile,Dockerfile,R,Makefile,Python,Meson,Lua,Starlark,HTML,Roff,,,,,,,,,,,,,,61,11,43,7,2,39,0,31425,185,803,433,370,62975ca936b912083c2218e4e30ad962901cbb3b,Move azure to macos-12,2024-07-11T11:18:12Z,Milot Mirdita,milot@mirdita.de,milot-mirdita,MMseqs2 Release 15-6f452,"MMseqs2 Release 15 brings efficient single query searches with low memory overhead through the new ungapped-prefiltering mode (`--prefilter-mode 1`). We also improved our greedy clustering algorithm and added a large swath of smaller fixes and features. Thanks to all contributors for their vital contributions and fixes.\r\n\r\n## Breaking\r\n* Updated greedy cluster algorithm. The clustering picks better representatives to respect the sequence identity and coverage criteria. (25688290) Thanks @bbuchfink\r\n\r\n## New Features and Enhancements\r\n* Implement additional `prefilter` modes (standard double k-mer prefilter, ungapped prefilter, exhaustive searching) (5e119e9f)\r\n* Added `createclusearchdb` and `mkrepseqdb` modules to build cluster-search databases, this was implemented for Foldseek, cluster-search in MMseqs2 will be implemented at a later point (9ae4458a, 80f8b0be, 542f3621, ad6dfc66, 91f2a6ac, 8310cd6b, 00190267, 76b7df1e)\r\n* Implement target-side similar k-mer search mode for sequence-sequence prefiltering (71dd32ec)\r\n* Rework `ungappedprefilter` to improve performance and expose additional parameters such as taxon filtering and db-load-mode to `ungappedprefilter` (8a893050, 800eb094, eb01b5b7, 20d3afc7)\r\n* Added `gappedprefilter` module for Smith-Waterman prefiltering, similar to `ungappedprefilter`  (df77d9e6)\r\n* Reworked `pairaln` for the ColabFold greedy taxonomy pairing mode (15140153)\r\n* Implemented experimental module for A3M filtering (167bbd12, 499bb730)\r\n* Implemented weighted clustering (bd080e60, b36070af, fd1837b6) Thanks @AnnSeidel \r\n* Precomputed indices without k-mers can be created with `--index-subset` (314c1f0c, 8fe3bf9b)\r\n* Add `result2neff` module to extract Neff scores (4148e093) Thanks @neftlon \r\n* Add `ppos` format-output to `convertalis` for count of positive substitution scores (5edc79bc) Thanks @Dohyun-s\r\n* Speed-up FASTA parsing in `kseq.h` with memchr (98406dd7) Thanks @valentynbez @kloetzl\r\n\r\n## Bugfixes\r\n* Add min and max modes for `result2stats` (19dce033, 61e77340) Thanks @ClovisG \r\n* Fixed a segmentation fault in ca3m with the same database (f5f780ac) Thanks @ClovisG \r\n* Fix crash when some input file sizes are an exact multiple of 4096 in `convertalis` and `gff2db` (712f2887) Thanks @RuoshiZhang \r\n* Fixed issues for GTDB r214 database creation (4b522962) Thanks @apcamargo \r\n* Fix source number being limited to 16-bit (65k) (1d62fa0c)\r\n* `kseq` now correctly handles input sequences larger than 2^31 bytes (07ca4a7c)\r\n* Fixed `unpackdb` to work without a `.lookup` file and added support for writing compressed files (92d8cc37, 570e3eda)\r\n* `createindex --check-compatible` check the k-mer threshold correctly now (bb0a1b35)\r\n* Fixed `prefilter` exclusively long result lists reading to result truncation. This was primarily a Foldseek issue and shouldn't affect MMseqs2 (ed4c55fa)\r\n* Corrected handling of multiline checks in `createdb` (6b938846)\r\n* Fix crash by disabling wrapped scoring when the target sequence is shorter than the query (8459b6b3) Thanks @AnnSeidel \r\n* Fixed logic in reciprocal-best-hit by removing `resAB_sort` (3bcbdbab) Thanks @StephanieSKim \r\n* Corrected handling of differently ordered parts of sequence databases in `concatdbs` (ea17d30f)\r\n* Fix `--single-step-clustering` misspelled in cluster warning (fa6c0938) Thanks @valentynbez\r\n\r\n## Build and Compatibility Updates\r\n* Addressed build and compatibility issues, including updates for newer compilers and architectures (e.g., Mac ARM64) (e26b9ad6, 3e436173, b341b663, 932d32b1) Thanks @A-N-Other\r\n* Added Mac ARM64 support in GitHub actions and updated from Ubuntu 18.04 to a newer image (1fea43d0, 05132de1)\r\n* Updated regression testing to fix errors in MPI test (21137666)\r\n\r\n## Developer\r\n* Introduced `base:` prefix to enable inheriting subprojects to find shadowed modules (i.e. Foldseek shadows `createdb`, but can use `base:createdb` to use the MMseq2's one) (90aa9133)\r\n* Exported build architecture in CMake so subprojects can use it (fce06b11)\r\n",15-6f452,Milot Mirdita,,milot-mirdita,GNU General Public License v3.0,MMseqs2,soedinglab,16,bioinformatics,sequence-clustering,profile-search,sequence-search,linclust,mmseqs,metagenomics,alignment,blast,taxonomy,,,,,,,,,,,/soedinglab/MMseqs2,16,32,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/snakemake-workflows/rna-seq-star-deseq2,https://github.com/snakemake-workflows/rna-seq-star-deseq2,0,One of snakemake workflows,,0,1,0,1,0,0,0,0,0,0,0,RNA-seq workflow using STAR and DESeq2,"# Snakemake workflow: rna-seq-star-deseq2\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4737358.svg)](https://doi.org/10.5281/zenodo.4737358)\n[![Snakemake](https://img.shields.io/badge/snakemake-≥6.1.0-brightgreen.svg)](https://snakemake.github.io)\n[![GitHub actions status](https://github.com/snakemake-workflows/rna-seq-star-deseq2/workflows/Tests/badge.svg?branch=master)](https://github.com/snakemake-workflows/rna-seq-star-deseq2/actions?query=branch%3Amaster+workflow%3ATests)\n[![Conventional Commits](https://img.shields.io/badge/Conventional%20Commits-1.0.0-%23FE5196?logo=conventionalcommits&logoColor=white)](https://conventionalcommits.org)\n\nThis workflow performs a differential gene expression analysis with STAR and Deseq2.\n\n## Usage\n\nThe usage of this workflow is described in the [Snakemake Workflow Catalog](https://snakemake.github.io/snakemake-workflow-catalog/?usage=snakemake-workflows%2Frna-seq-star-deseq2).\n\nIf you use this workflow in a paper, don't forget to give credits to the authors by citing the URL of this (original) repository and its DOI (see above).\n",316,snakemake,Python,2,Python,R,,,,,,,,,,,,,,,,,,,,,,,,,,,45,16,29,0,3,14,0,17355,192,38,37,1,993dcfcf3c1210f75f6bfb0ef765a4ddb77cadf7,chore(master): release 2.1.2 (#82),2024-06-05T16:12:25Z,github-actions[bot],41898282+github-actions[bot]@users.noreply.github.com,github-actions[bot],v2.1.2,## [2.1.2](https://github.com/snakemake-workflows/rna-seq-star-deseq2/compare/v2.1.1...v2.1.2) (2024-06-05)\n\n\n### Bug Fixes\n\n* use derived input for star_index ([#81](https://github.com/snakemake-workflows/rna-seq-star-deseq2/issues/81)) ([87fffe6](https://github.com/snakemake-workflows/rna-seq-star-deseq2/commit/87fffe6a1beaa86e95c3564061d2720cc73308c7)),v2.1.2,,,github-actions[bot],MIT License,rna-seq-star-deseq2,snakemake-workflows,9,snakemake,sciworkflows,reproducibility,gene-expression-analysis,deseq2,,,,,,,,,,,,,,,,/snakemake-workflows/rna-seq-star-deseq2,9,11,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/snakemake/snakemake,https://github.com/snakemake/snakemake,0,,,0,1,1,0,0,0,0,0,0,0,0,"This is the development home of the workflow management system Snakemake. For general information, see","[![Gitpod Ready-to-Code](https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/snakemake/snakemake)\n[![GitHub Workflow Status (with event)](https://img.shields.io/github/actions/workflow/status/snakemake/snakemake/main.yml?label=tests)](https://github.com/snakemake/snakemake/actions?query=branch%3Amain++)\n[![Sonarcloud Status](https://sonarcloud.io/api/project_badges/measure?project=snakemake_snakemake&metric=alert_status)](https://sonarcloud.io/dashboard?id=snakemake_snakemake)\n[![Conda Version](https://img.shields.io/conda/vn/bioconda/snakemake)](https://anaconda.org/bioconda/snakemake)\n[![Bioconda](https://img.shields.io/conda/dn/bioconda/snakemake.svg?label=Bioconda)](https://bioconda.github.io/recipes/snakemake/README.html)\n[![Pypi](https://img.shields.io/pypi/pyversions/snakemake.svg)](https://pypi.org/project/snakemake)\n[![docker container status](https://img.shields.io/github/actions/workflow/status/snakemake/snakemake/docker-publish.yml?color=blue&label=docker%20container)](https://hub.docker.com/r/snakemake/snakemake)\n[![Stack Overflow](https://img.shields.io/badge/stack-overflow-orange.svg)](https://stackoverflow.com/questions/tagged/snakemake)\n[![Twitter](https://img.shields.io/twitter/follow/johanneskoester.svg?style=social&label=Follow)](https://twitter.com/search?l=&q=%23snakemake%20from%3Ajohanneskoester)\n![Mastodon Follow](https://img.shields.io/mastodon/follow/109308140471392959?domain=https%3A%2F%2Ffosstodon.org&label=Follow)\n[![Discord](https://img.shields.io/discord/753690260830945390?label=discord%20chat)](https://discord.gg/NUdMtmr)\n[![Github stars](https://img.shields.io/github/stars/snakemake/snakemake?style=social)](https://github.com/snakemake/snakemake/stargazers)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](CODE_OF_CONDUCT.md) \n\n# Snakemake\n\nThe Snakemake workflow management system is a tool to create **reproducible and scalable** data analyses.\nSnakemake is highly popular, with on average more than 7 new citations per week in 2021, and almost 400k downloads.\nWorkflows are described via a human readable, Python based language.\nThey can be seamlessly scaled to server, cluster, grid and cloud environments without the need to modify the workflow definition.\nFinally, Snakemake workflows can entail a description of required software, which will be automatically deployed to any execution environment.\n\n**Homepage: https://snakemake.github.io**\n\nCopyright (c) 2012-2022 Johannes Köster <johannes.koester@uni-due.com> (see LICENSE)\n",2190,snakemake,HTML,16,Shell,Dockerfile,Python,CSS,Roff,Rebol,R,Makefile,Julia,HTML,Jupyter Notebook,JavaScript,Jinja,Rust,Vim Script,C,,,,,,,,,,,,,1322,134,1100,88,126,305,0,92041,526,1653,660,993,e8735c1477a2a82110757ba86bbd1ccbcaf327ba,fmt,2024-07-13T06:50:30Z,Johannes Koester,johannes.koester@uni-due.de,johanneskoester,v8.16.0,## [8.16.0](https://github.com/snakemake/snakemake/compare/v8.15.2...v8.16.0) (2024-07-09)\n\n\n### Features\n\n* added snakemake.script.snakemake for type hinting ([#2917](https://github.com/snakemake/snakemake/issues/2917)) ([c85fb4b](https://github.com/snakemake/snakemake/commit/c85fb4b34ce983cb916bcce361d958d466ee1d07))\n\n\n### Documentation\n\n* use note directive to align with the capabilities of the used theme ([#2950](https://github.com/snakemake/snakemake/issues/2950)) ([fe27405](https://github.com/snakemake/snakemake/commit/fe274055dbd4663d7185114bc4f77ac862f8c7b3)),v8.16.0,,,github-actions[bot],MIT License,snakemake,snakemake,241,snakemake,reproducibility,workflow-management,,,,,,,,,,,,,,,,,,/snakemake/snakemake,319,19,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/skia4delphi/skia4delphi,https://github.com/skia4delphi/skia4delphi,0,,,0,0,0,0,0,0,1,1,0,0,0,"Skia4Delphi is a cross-platform 2D graphics API for Delphi platforms based on Google's Skia Graphics Library. It provides a comprehensive 2D API that can be used across mobile, server and desktop models to render images.","<p align=""center""><a href=""https://www.skia4delphi.org""><img src=""Assets/Artwork/logo-gradient.svg"" alt=""Logo"" height=""300"" width=""360"" /></a></p>\r\n\r\n<p>\r\n  <a href=""#compatibility""><img src=""https://img.shields.io/static/v1?label=rad%20studio&message=xe7%2B&color=silver&style=for-the-badge&logo=delphi&logoColor=white"" alt=""Delphi XE7+ support"" /></a>\r\n  <a href=""#compatibility""><img src=""https://img.shields.io/static/v1?label=platforms&message=cross-platform&color=silver&style=for-the-badge&logo=delphi&logoColor=white"" alt=""Cross-platform support"" /></a>\r\n  <a href=""#compatibility""><img src=""https://img.shields.io/static/v1?label=applications&message=console%2C%20fmx%2C%20vcl&color=silver&style=for-the-badge&logo=delphi&logoColor=white"" alt=""Console, FMX, VCL support"" /></a>\r\n  <a href=""https://t.me/skia4delphi""><img src=""https://img.shields.io/static/v1?label=telegram&message=skia4delphi&color=silver&style=for-the-badge&logo=telegram&logoColor=white"" alt=""Telegram group"" />\r\n  <a href=""https://www.youtube.com/@skia4delphi""><img src=""https://img.shields.io/static/v1?label=youtube&message=skia4delphi&color=silver&style=for-the-badge&logo=youtube&logoColor=white"" alt=""YouTube channel"" />\r\n</p>\r\n\r\n#\r\n\r\n**[Skia4Delphi](https://skia4delphi.org)** is an open-source, cross-platform 2D graphics library for Delphi, utilizing the esteemed [Google's Skia](https://skia.org/) library.\r\n\r\nhttps://user-images.githubusercontent.com/1863024/175955980-f6c57253-aaa3-4617-90dc-b0d9bf25e21b.mp4\r\n\r\n## About\r\n\r\nSkia is an exceptional open-source library dedicated to rendering 2D text, geometries and images, with a focus on precision, superior quality and high performance. It offers versatile APIs compatible with a wide range of hardware and software platforms.\r\n\r\nGoogle's Skia Graphics Library functions as the graphics engine for numerous products, including Google Chrome, Chrome OS, Android, Flutter, Xamarin, Mozilla Firefox, Firefox OS, and more.\r\n\r\n## Features\r\n\r\n- Canvas 2D and Text Layout;\r\n- CPU software rasterization;\r\n- GPU-accelerated rendering;\r\n- Right-to-Left rendering;\r\n- SVG rendering and creation;\r\n- PDF output;\r\n- Runtime effects;\r\n- Shading language;\r\n- Shaders, mask and color filters;\r\n- Image and path effects;\r\n- Animated image player; (Lottie, GIF, WebP)\r\n- Image codecs; (bmp, gif, ico, jpg, png, wbmp, webp and raw images)\r\n  **and much more...**\r\n\r\n## FMX render replacement\r\n\r\nUsing the **Skia4Delphi** library it is possible to override Firemonkey's graphic engine so that it can use Skia as its default Canvas. With that, your Firemonkey application will automatically:\r\n\r\n- Draw with antialiasing on any platform (the drawing quality is based on the Form.Quality property);\r\n- Increase the overall graphics performance of your application by up to 50% (even drawing with higher quality);\r\n- Resize images with better quality (also based on Form.Quality);\r\n- Support Right-To-Left text rendering;\r\n- Fix dozens of inconsistencies in drawings (especially in corners and strokes, such as dashes, and in texts with special emojis);\r\n- Increase the performance of the library in general ([controls](#controls-vclfmx), drawings, among others...).\r\n\r\n[Learn more...](#fmx-render)\r\n\r\n# Summary\r\n\r\n- [Using the library](#using-the-library)\r\n  - [Prerequisites](#prerequisites)\r\n    - [Install](#install)\r\n    - [Enable Skia](#enable-skia)\r\n  - [Examples](#examples)\r\n    - [Basic usage](#basic-usage)\r\n    - [PDF](#pdf)\r\n    - [Codecs](#codecs)\r\n- [Integration with Delphi](#integration-with-delphi)\r\n  - [Bitmap](#bitmap)\r\n  - [Image formats](#image-formats)\r\n  - **[FMX Render](#fmx-render)**\r\n    - [Enable Skia Render](#enable-skia-render)\r\n    - [Benchmark](#benchmark)\r\n    - [Skia canvas](#skia-canvas)\r\n    - [Right-to-Left](#right-to-left)\r\n    - [Custom fonts](#custom-fonts)\r\n- [Controls VCL/FMX](#controls-vclfmx)\r\n  - [TSkAnimatedImage](#tskanimatedimage)\r\n  - [TSkLabel](#tsklabel)\r\n  - [TSkPaintBox](#tskpaintbox)\r\n  - [TSkSvg](#tsksvg)\r\n    - [Limitations](Documents/SVG.md#limitations)\r\n- [Compatibility](#compatibility)\r\n- [Known issues](#known-issues)\r\n  - [Universal macOS Binary](#universal-macos-binary)\r\n  - [Submit your app to the Mac App Store](#submit-your-app-to-the-mac-app-store)\r\n- [Documentation](#documentation)\r\n- [Version](#version)\r\n\r\n# Using the library\r\n\r\n## Prerequisites\r\n\r\n### Install\r\n\r\nYou can install **Skia4Delphi** in 3 ways:\r\n\r\n- Setup (recommended)\r\n\r\n  Download the setup of [latest release](../../releases/latest) and install it.\r\n\r\n  <p><img src=""Assets/Documents/installation.png"" width=""511"" alt=""Skia4Delphi Installation"" /></p>\r\n\r\n- Embarcadero's GetIt _(RAD Studio > Tools > GetIt Package Manager...)_\r\n\r\n  <p><img src=""https://user-images.githubusercontent.com/11139086/214978288-11c87e9e-7a8b-4686-82c0-5922676d26df.png#gh-light-mode-only"" width=""511"" alt=""GetIt"" /></p>\r\n  <p><img src=""https://user-images.githubusercontent.com/11139086/214978346-c67bb0f6-ec96-4833-a1e4-7ee39d620e82.png#gh-dark-mode-only"" width=""511"" alt=""GetIt"" /></p>\r\n\r\n- Chocolatey package manager\r\n\r\n  ```batch\r\n  choco install skia4delphi\r\n  ```\r\n\r\n#### Remarks\r\n\r\n1. Manual installation is possible, although it is not recommended; [Learn more...](Documents/INSTALLATION.md)\r\n2. The pre-built Skia binaries were included in the source, but you can easily recompile them; [Learn more...](Documents/BUILD.md)\r\n\r\n### Enable Skia\r\n\r\nAfter install the **Skia4Delphi**, just right click in your application project and click **Enable Skia**.\r\n\r\n![Menu](https://user-images.githubusercontent.com/16469061/153612703-81a9d1f8-8ae4-4977-b58f-6520a8318756.png#gh-light-mode-only)\r\n![Menu](https://user-images.githubusercontent.com/16469061/153612789-38488c75-930a-48ac-8a6b-ea303f403e9e.png#gh-dark-mode-only)\r\n\r\n#### Tip\r\n\r\nTo improve the quality and performance of FMX drawings, the replacement of the the FMX graphics engine with the **Skia4Delphi** render is automatically enabled. [Learn more...](#fmx-render)\r\n\r\n## Examples\r\n\r\nIn this section you will find some examples of using **Skia4Delphi**, it works in **Console**, **FMX**, and **VCL** applications.\r\nThe code below is common code among all the examples in this section:\r\n\r\n```pascal\r\nuses\r\n  System.Skia;\r\n\r\ntype\r\n  TSkDrawExampleProc = reference to procedure(const ACanvas: ISkCanvas; const ADest: TRectF);\r\n\r\nprocedure DrawExample(const AWidth, AHeight: Integer; const ADrawProc: TSkDrawExampleProc);\r\nbegin\r\n  var LSurface := TSkSurface.MakeRaster(AWidth, AHeight);\r\n  LSurface.Canvas.Clear(TAlphaColors.Null);\r\n  ADrawProc(LSurface.Canvas, RectF(0, 0, AWidth, AHeight));\r\n  LSurface.MakeImageSnapshot.EncodeToFile('output.png');\r\nend;\r\n```\r\n\r\n### Basic usage\r\n\r\nThe code below demonstrate how to draw shapes:\r\n\r\n```pascal\r\nDrawExample(256, 256,\r\n  procedure (const ACanvas: ISkCanvas; const ADest: TRectF)\r\n  begin\r\n    var LPaint: ISkPaint := TSkPaint.Create;\r\n    LPaint.AntiAlias := True;\r\n\r\n    LPaint.Color := $FF4285F4;\r\n    var LRect := TRectF.Create(PointF(10, 10), 100, 160);\r\n    ACanvas.DrawRect(LRect, LPaint);\r\n\r\n    var LOval: ISkRoundRect := TSkRoundRect.Create;\r\n    LOval.SetOval(LRect);\r\n    LOval.Offset(40, 80);\r\n    LPaint.Color := $FFDB4437;\r\n    ACanvas.DrawRoundRect(LOval, LPaint);\r\n\r\n    LPaint.Color := $FF0F9D58;\r\n    ACanvas.DrawCircle(180, 50, 25, LPaint);\r\n\r\n    LRect.Offset(80, 50);\r\n    LPaint.Color := $FFF4B400;\r\n    LPaint.Style := TSkPaintStyle.Stroke;\r\n    LPaint.StrokeWidth := 4;\r\n    ACanvas.DrawRoundRect(LRect, 10, 10, LPaint);\r\n  end);\r\n```\r\n\r\nThis code results in the output below:\r\n\r\n<p><img src=""Assets/Documents/example1.svg"" width=""192"" height=""192"" alt=""Example 1"" /></p>\r\n\r\n[Learn more...](Documents/USAGE.md#basic-usage)\r\n\r\n### PDF\r\n\r\nWith **Skia4Delphi** it is possible to create PDF documents and draw anything on them, from text to images. The example below demonstrates how to create an PDF document and draw an SVG inside it:\r\n\r\n```pascal\r\n  var LSVGDOM := TSkSVGDOM.MakeFromFile('Samples\Demo\Assets\lion.svg');\r\n  var LSize := TSizeF.Create(600, 600);\r\n  LSVGDOM.SetContainerSize(LSize);\r\n\r\n  var LDocumentStream := TFileStream.Create('output.pdf', fmCreate);\r\n  try\r\n    var LDocument := TSkDocument.MakePDF(LDocumentStream);\r\n    try\r\n      var LCanvas := LDocument.BeginPage(LSize.Width, LSize.Height);\r\n      try\r\n        // Draw anything here with Skia canvas\r\n        LSVGDOM.Render(LCanvas);\r\n      finally\r\n        LDocument.EndPage;\r\n      end;\r\n    finally\r\n      LDocument.Close;\r\n    end;\r\n  finally\r\n    LDocumentStream.Free;\r\n  end;\r\n```\r\n\r\nThis code results in the output below:\r\n\r\n<p><img src=""Assets/Documents/lion.svg"" width=""200"" height=""200"" alt=""Lion"" /></p>\r\n\r\n### Codecs\r\n\r\nThe **Skia4Delphi** library supports many image formats. See below the list:\r\n\r\n- Supported formats for decoding\r\n\r\n  | Image Format                   | Extensions  |\r\n  | ------------------------------ | ----------- |\r\n  | Bitmap                         | .bmp        |\r\n  | GIF                            | .gif        |\r\n  | Icon                           | .ico        |\r\n  | JPEG                           | .jpg, .jpeg |\r\n  | PNG                            | .png        |\r\n  | Raw Adobe DNG Digital Negative | .dng        |\r\n  | Raw Canon                      | .cr2        |\r\n  | Raw Fujifilm RAF               | .raf        |\r\n  | Raw Nikon                      | .nef, .nrw  |\r\n  | Raw Olympus ORF                | .orf        |\r\n  | Raw Panasonic                  | .rw2        |\r\n  | Raw Pentax PEF                 | .pef        |\r\n  | Raw Samsung SRW                | .srw        |\r\n  | Raw Sony                       | .arw        |\r\n  | WBMP                           | .wbmp       |\r\n  | WebP                           | .webp       |\r\n\r\n  _Note: Raw images are limited to non-windows platforms_\r\n\r\n- Supported formats for encoding\r\n\r\n  | Image Format | Extensions  |\r\n  | ------------ | ----------- |\r\n  | JPEG         | .jpg, .jpeg |\r\n  | PNG          | .png        |\r\n  | WebP         | .webp       |\r\n\r\n#### About WebP\r\n\r\nWebP is a modern image format that provides superior lossless and lossy compression for images. WebP lossless images are 26% smaller in size compared to PNGs. WebP lossy images are 25-34% smaller than comparable JPEG images at equivalent quality.\r\n\r\nThe example below demonstrates how to encoder to WebP format:\r\n\r\n```pascal\r\n  var LImage := TSkImage.MakeFromEncodedFile('Samples\Demo\Assets\kung-fu-panda.png');\r\n  LImage.EncodeToFile('output.webp', TSkEncodedImageFormat.WEBP, 80);\r\n  LImage.EncodeToFile('output.jpg', TSkEncodedImageFormat.JPEG, 80);\r\n```\r\n\r\nThis code results in the output below:\r\n\r\n<p><img src=""Assets/Documents/kung-fu-panda.webp"" width=""400"" alt=""King Fu Panda"" /></p>\r\n\r\n| Format             | Size   |\r\n| ------------------ | ------ |\r\n| Png (100% quality) | 512 KB |\r\n| Jpeg (80% quality) | 65 KB  |\r\n| WebP (80% quality) | 51 KB  |\r\n\r\n# Integration with Delphi\r\n\r\n## Bitmap\r\n\r\nIt is possible to edit TBitmap (**VCL** or **FMX**) with Skia's canvas using the code below:\r\n\r\n```pascal\r\nuses\r\n  System.Skia, FMX.Skia {or Vcl.Skia};\r\n\r\n...\r\n\r\n  var LBitmap := TBitmap.Create(100, 100);\r\n  try\r\n    LBitmap.SkiaDraw(\r\n      procedure (const ACanvas: ISkCanvas)\r\n      begin\r\n        // Draw with Skia canvas...\r\n      end);\r\n```\r\n\r\n## Image formats\r\n\r\nThe library registers the following codecs:\r\n\r\n- **VCL**: .svg, .webp, .wbmp and raw images (.arw, .cr2, .dng, .nef, .nrw, .orf, .raf, .rw2, .pef and .srw).\r\n\r\n- **FMX**: .bmp, .gif, .ico, .webp, .wbmp and raw images (.arw, .cr2, .dng, .nef, .nrw, .orf, .raf, .rw2, .pef and .srw).\r\n\r\nAs a result, any Delphi control, such as a TImage, can normally load these new formats automatically.\r\n\r\n## **FMX Render**\r\n\r\nIt is possible to replace the default Canvas from FMX to Skia based Canvas. Once this feature is enabled, all FMX controls will be painted internally using **Skia4Delphi** automatically. With that it is possible to improve the quality and performance of the drawings throughout the FMX app, as well as generating better integration with other library features.\r\n\r\n### Enable Skia Render\r\n\r\nOpen the source of your Delphi Application Project _(.dpr)_, include the `FMX.Skia` unit right **after** the `FMX.Forms` unit, and set the `GlobalUseSkia` to **True**, as in the example below:\r\n\r\n```pascal\r\nuses\r\n  System.StartUpCopy,\r\n  FMX.Forms,\r\n  FMX.Skia,\r\n  Unit1 in 'Unit1.pas' {Form1};\r\n\r\n{$R *.res}\r\n\r\nbegin\r\n  GlobalUseSkia := True;\r\n  Application.Initialize;\r\n  ...\r\n```\r\n\r\n#### Remarks\r\n\r\n1. `FMX.Skia` unit must be included right after the `FMX.Forms`;\r\n2. The **Skia Metal** render can be used by including the `FMX.Types` unit right **after** the `FMX.Forms` unit, and setting `GlobalUseMetal` to **True** together with `GlobalUseSkia` to improve the speed in iOS and macOS;\r\n3. The **Skia Vulkan** render can be used on RAD Studio 12 Athens or newer by including the `FMX.Types` unit right **after** the `FMX.Forms` unit, and setting `GlobalUseVulkan` to **True** together with `GlobalUseSkia` to improve the speed on Android and Windows. On Windows, Vulkan will only be used if you also add `GlobalUseSkiaRasterWhenAvailable := False;`;\r\n4. This declaration of `GlobalUseSkia := True;`, as well as other variables of FMX itself, such as `GlobalUseMetal`, can also be made in the initialization of some unit instead of .dpr. Sometimes this is really necessary because if in the initialization or in the class constructor of some unit, bitmaps are used, the GlobalUseXXX declarations of the .dpr will have no effect. In this case, just create a unit in the project like ""Project.Startup.pas"", place the GlobalUseXXX declarations in the initialization of this new unit, and declare this new unit before any other unit of yours in the .dpr, that is, right after FMX.Forms.\r\n\r\n### Benchmark\r\n\r\nThe performance test is a simulation of a real application, with hundreds of controls, to measure the FPS rate when sliding a vertical scroll.\r\n\r\n| Device                                | Platform    |    FMX |      Skia |\r\n| ------------------------------------- | ----------- | -----: | --------: |\r\n| Motorola Moto 3rd Generation          | Android     | 25 fps |    38 fps |\r\n| LG K40s                               | Android     | 30 fps |    47 fps |\r\n| Samsung Galaxy A01 Core               | Android     | 20 fps |    26 fps |\r\n| Samsung Galaxy S7 Edge                | Android64   | 53 fps |    56 fps |\r\n| Samsung Galaxy S8 Plus                | Android64   | 50 fps |    55 fps |\r\n| Apple iPhone 11                       | iOSDevice64 | 59 fps |    60 fps |\r\n| Apple iPhone 12                       | iOSDevice64 | 59 fps |    59 fps |\r\n| Apple MacBook Air Model A2337         | OSXARM64    | 58 fps | 30 fps \* |\r\n| Intel Core i7-8565U / Radeon 520      | Win32       | 82 fps |    92 fps |\r\n| Intel Core i7-8565U / Radeon 520      | Win64       | 83 fps |    91 fps |\r\n| Intel Core i7-4500U / GeForce GT 720M | Win32       | 85 fps |    92 fps |\r\n| Intel Core i7-4500U / GeForce GT 720M | Win64       | 86 fps |    93 fps |\r\n\r\n#### Metal\r\n\r\n| Device                        | Platform    |    FMX |   Skia |\r\n| ----------------------------- | ----------- | -----: | -----: |\r\n| Apple iPhone 11               | iOSDevice64 | 59 fps | 60 fps |\r\n| Apple iPhone 12               | iOSDevice64 | 59 fps | 59 fps |\r\n| Apple MacBook Air Model A2337 | OSXARM64    | 60 fps | 60 fps |\r\n\r\n#### Remarks\r\n\r\n1. Default FMX renderer does not use anti-aliasing on some platforms (like on mobile) while Skia Render uses it. That is, Skia has better performance and quality in the drawings than default FMX Render.\r\n\r\n   | FMX default                                    | FMX with Skia render                             |\r\n   | :--------------------------------------------: | :----------------------------------------------: |\r\n   | ![FMX Circle](Assets/Documents/fmx-circle.png) | ![Skia Circle](Assets/Documents/skia-circle.png) |\r\n\r\n2. On macOS `Skia4Delphi`'s default renderer does not have GPU acceleration. Therefore, it is highly recommended to use **Skia Metal** (combining the activation of `GlobalUseSkia` and `GlobalUseMetal`), to get the full performance of the machine.\r\n\r\n3. Tests made from virtual machines are inconsistent with reality.\r\n\r\n### Skia canvas\r\n\r\nUsing Skia's Render, during the Scene of a Bitmap, Control or Form, it is possible to access the Skia canvas property as follows:\r\n\r\n#### In Bitmaps\r\n\r\n```pascal\r\nuses\r\n  System.Skia, FMX.Skia.Canvas;\r\n\r\nbegin\r\n  var LBitmap := TBitmap.Create(300, 300);\r\n  try\r\n    LBitmap.Canvas.BeginScene;\r\n    try\r\n      var LCanvas: ISkCanvas := TSkCanvasCustom(LBitmap.Canvas).Canvas;\r\n      // Draw using Skia canvas (LCanvas) directly to unlock new features...\r\n    finally\r\n      LBitmap.Canvas.EndScene;\r\n    end;\r\n  finally\r\n    LBitmap.Free;\r\n  end;\r\nend;\r\n```\r\n\r\n#### In Controls & Forms\r\n\r\n```pascal\r\ntype\r\n  TMyControl = class(TControl)\r\n  protected\r\n    procedure Paint; override;\r\n  end;\r\n\r\nimplementation\r\n\r\nuses\r\n  System.Skia, FMX.Skia.Canvas;\r\n\r\nprocedure TMyControl.Paint;\r\nbegin\r\n  var LCanvas: ISkCanvas := TSkCanvasCustom(Canvas).Canvas;\r\n  // Draw using Skia canvas (LCanvas) directly to unlock new features...\r\nend;\r\n```\r\n\r\n#### Remarks\r\n\r\n1. `Canvas` property will only be available during Scene, that is, between the `BeginScene` and `EndScene` of the Bitmaps, and during paint events/methods for Controls and Forms (such as OnPaint, OnPainting, PaintChildren, among others);\r\n2. Canvas for UI (created from a window _eg TRectangles, TCircles, objects inherited from TControl_) must draw exclusively from the **main thread**, while Canvas created from `TBitmap` are **thread safe**.\r\n\r\n### Right-to-Left\r\n\r\nUsing Skia's render, your application will now support Right-To-Left text rendering. But for that you will need to make 3 changes to your project:\r\n\r\n1. For RAD Studio prior to 11.3, open the source of your Delphi Application Project _(.dpr)_, include the line `Application.BiDiMode := TBiDiMode.bdRightToLeft;`, like below:\r\n\r\n```pascal\r\nprogram Project1;\r\n\r\nuses\r\n  System.StartUpCopy,\r\n  FMX.Forms,\r\n  System.Classes,\r\n  FMX.Skia,\r\n  Unit1 in 'Unit1.pas' {Form1};\r\n\r\n{$R *.res}\r\n\r\nbegin\r\n  Application.BiDiMode := TBiDiMode.bdRightToLeft;\r\n  GlobalUseSkia := True;\r\n  Application.Initialize;\r\n  Application.CreateForm(TForm1, Form1);\r\n  Application.Run;\r\nend.\r\n```\r\n\r\n2. Set the property `BiDiMode` of your forms to `bdRightToLeft`;\r\n3. Keyboard input controls like TEdit and TMemo, need to be fixed by Embarcadero, meanwhile, as a workaround, set the `ControlType` property of these controls to `Platform`.\r\n\r\n### Custom fonts\r\n\r\nUsing Skia's renderer, it is possible to use custom font in any FMX control, on any platform in a very simple way. Just register them in the app initialization:\r\n\r\n```pascal\r\nprogram Project1;\r\n\r\nuses\r\n  System.StartUpCopy,\r\n  FMX.Forms,\r\n  FMX.Skia,\r\n  Unit1 in 'Unit1.pas' {Form1};\r\n\r\n{$R *.res}\r\n\r\nbegin\r\n  GlobalUseSkia := True;\r\n  TSkDefaultProviders.RegisterTypeface('Poppins.ttf');\r\n  Application.Initialize;\r\n  Application.CreateForm(TForm1, Form1);\r\n  Application.Run;\r\nend.\r\n```\r\n\r\nOn RAD Studio 12 Athens or newer it is recommended to use `IFMXFontManagerService`:\r\n\r\n```pascal\r\nprogram Project1;\r\n\r\nuses\r\n  System.StartUpCopy,\r\n  FMX.Forms,\r\n  FMX.Platform,\r\n  FMX.FontManager,\r\n  FMX.Skia,\r\n  Unit1 in 'Unit1.pas' {Form1};\r\n\r\n{$R *.res}\r\n\r\nbegin\r\n  GlobalUseSkia := True;\r\n  var LFontManager: IFMXFontManagerService;\r\n  if TPlatformServices.Current.SupportsPlatformService(IFMXFontManagerService, LFontManager) then\r\n    LFontManager.AddCustomFontFromFile('Poppins.ttf');\r\n  Application.Initialize;\r\n  Application.CreateForm(TForm1, Form1);\r\n  Application.Run;\r\nend.\r\n```\r\n\r\n# Controls VCL/FMX\r\n\r\n## TSkAnimatedImage\r\n\r\n**TSkAnimatedImage** is the control that can load and render animated images, including vector animations, in a very simple way. The supported formats are:\r\n\r\n| Format           | Extensions     |\r\n| ---------------- | -------------- |\r\n| Lottie file      | .json, .lottie |\r\n| Telegram Sticker | .tgs           |\r\n| Animated GIF     | .gif           |\r\n| Animated WebP    | .webp          |\r\n\r\nThe example below demonstrates how to play lottie files using **TSkAnimatedImage**:\r\n\r\n```pascal\r\n  var LAnimatedimage := TSkAnimatedImage.Create(Self);\r\n  LAnimatedimage.LoadFromFile('Samples\Demo\Assets\rocket.json');\r\n  LAnimatedimage.Parent := Self;\r\n```\r\n\r\nThe example above results in the output below:\r\n\r\n![Rocket](Assets/Documents/rocket.webp)\r\n\r\n[Learn more...](Documents/ANIMATED-IMAGES.md#TSkAnimatedImage)\r\n\r\n## TSkLabel\r\n\r\n**TSkLabel** is the control that implements the SkParagraph internally, having several more features than the TLabel, such as:\r\n\r\n- Font families; (font fallback list like in css)\r\n- Font weight;\r\n- Font slant;\r\n- Support for multiple styles in text;\r\n- Support for BiDi; (Right-to-Left)\r\n- Support justify horizontal alignment;\r\n- Support custom font; (without install the font)\r\n- Supports background color on parts of the text;\r\n- Limit the maximum number of lines;\r\n- Auto size option; (width and height)\r\n- Advanced decorations; (like underline wavy, overline, dashed line, among others...)\r\n  **and much more...**\r\n\r\n![Label](https://user-images.githubusercontent.com/16469061/153615162-e2f51dd6-b22e-4f34-9493-244122faa5ae.png#gh-light-mode-only)\r\n![Label](https://user-images.githubusercontent.com/16469061/153615217-53e851a3-c20d-4cb9-92fb-b9b18319c342.png#gh-dark-mode-only)\r\n\r\n[Learn more...](Documents/LABEL.md#tsklabel)\r\n\r\n## TSkPaintBox\r\n\r\n**TSkPaintBox** is the ideal control for painting with Skia API directly on the canvas with the event `OnDraw`:\r\n\r\n```pascal\r\nprocedure TForm1.SkPaintBox1Draw(ASender: TObject; const ACanvas: ISkCanvas;\r\n  const ADest: TRectF; const AOpacity: Single);\r\nbegin\r\n  var LPaint: ISkPaint := TSkPaint.Create;\r\n  LPaint.Shader := TSkShader.MakeGradientSweep(ADest.CenterPoint,\r\n    [$FFFCE68D, $FFF7CAA5, $FF2EBBC1, $FFFCE68D]);\r\n  ACanvas.DrawPaint(LPaint);\r\nend;\r\n```\r\n\r\nThe example above results in the output below:\r\n\r\n![Paint Box](Assets/Documents/paintbox.png)\r\n\r\n_Note: The TSkPaintBox has a drawing caching system. To force a drawing refresh, call TSkPaintBox.Redraw. However, this cache system does not exist in FMX apps that have enabled [Skia4Delphi render](#fmx-render) for optimization reasons._\r\n\r\n## TSkSvg\r\n\r\n**TSkSvg** is the control to load and display SVG easily:\r\n\r\n```pascal\r\n  var LSvg := TSkSvg.Create(Self);\r\n  LSvg.Svg.Source := TFile.ReadAllText('Samples\Demo\Assets\panda.svg');\r\n  LSvg.Parent := Self;\r\n```\r\n\r\nThe example above results in the output below:\r\n\r\n<p><img src=""Samples/Demo/Assets/panda.svg"" width=""200"" height=""200"" alt=""Panda"" /></p>\r\n\r\n[Learn more...](Documents/SVG.md)\r\n\r\n# Compatibility\r\n\r\n| RAD Studio                        | Platforms        |\r\n| --------------------------------- | ---------------- |\r\n| RAD Studio 11 Alexandria or newer | All Platforms    |\r\n| RAD Studio 10.3 Rio or newer      | Windows, Android |\r\n| RAD Studio XE7 or newer           | Windows          |\r\n\r\nFor the platforms supported by **Skia4Delphi** (listed above), the OS versions supported by the library are the same [OS versions that RAD Studio supports.](https://docwiki.embarcadero.com/PlatformStatus/en/Main_Page)\r\n\r\n# Documentation\r\n\r\nThe APIs are very similar to Skia's, few methods and functions have been renamed for readability, so the [Skia documentation](https://skia.org/docs) can be used.\r\n\r\n# Version\r\n\r\n**[Skia4Delphi 6.2.0](/../../releases/latest)**\r\n\r\nSkia Version used: [chrome/m107](https://github.com/google/skia/tree/chrome/m107)\r\n\r\n# Sponsors & Partners\r\n\r\n<p>\r\n  <a href=""https://www.a-dato.com"">\r\n    <img src=""https://user-images.githubusercontent.com/11139086/186210969-0179cdbd-b65a-41cc-ad15-b7cc828a764f.png"" alt=""A-dato logo"" width=""200"" /></a>\r\n  <a href=""https://www.delphistyles.com"">\r\n    <img src=""https://user-images.githubusercontent.com/11139086/199366200-c5766e71-2684-4990-94bb-d44094fb90c4.png"" alt=""DelphiStyles logo"" width=""100"" margin-left=""100"" /></a>\r\n</p>\r\n\r\n# Contributors\r\n\r\n<a href=""https://github.com/skia4delphi/skia4delphi/graphs/contributors"">\r\n  <img src=""https://contrib.rocks/image?repo=skia4delphi/skia4delphi"" />\r\n</a>\r\n\r\n#\r\n\r\nHelp us responding a small questionnaire about our users in [this link](https://form.typeform.com/to/Qc6o3ELs)\r\n",672,graphics,Pascal,6,Pascal,Batchfile,Inno Setup,PowerShell,PHP,POV-Ray SDL,,,,,,,,,,,,,,,,,,,,,,,36,8,28,0,3,4,0,738234,134,255,224,31,11d6d65cc440b350abc92ba9e61253b8f7b422b1,[Controls] Fix oblique fonts on macOS and iOS,2024-07-10T15:15:54Z,Vinícius Felipe Botelho Barbosa,viniciusfbb2@gmail.com,viniciusfbb,06.02.2000,"<img src=""/Assets/Artwork/logo-gradient.svg"" width=360 height=300>\r\n\r\n# v6.2.0\r\n\r\n- Replaced Android and macOS shared libraries by static libraries; #299 `Library`\r\n- Improved overall performance of FMX render (reducing sample count to 1); `FMX Render`\r\n- Improved encoding file detection on SVG design-time dialog; `Controls`\r\n- Adjusted the animated codec for future releases; `Controls`\r\n- Added `Tests\Assets\Expected.zip` (on repository only) with images expected of all tests to help investigations of future regressions; `Tests`\r\n- Fixed Skia based decoders (like webp) loaded in designtime on Vcl; `Controls`\r\n- Fixed hpp emit; `C++`\r\n- Fixed small issue on uninstaller for new versions; `Setup`\r\n\r\n**Skia version**: [107.4.0](https://github.com/skia4delphi/skia/tree/v107.4.0)\r\n\r\n# \r\n\r\n### Deployment adjustments\r\nNow, it is not necessary to deploy the Skia library on Android and macOS, as it is now completely static. But there is no need to change anything manually when deploying your project, as during compilation the Skia4Delphi plugin already makes all the necessary adjustments to your project.\r\n\r\n### RAD Studio 12 Athens\r\nEmbarcadero has integrated Skia4Delphi into RAD Studio in its new release [RAD Studio 12 Athens](https://docwiki.embarcadero.com/RADStudio/Athens/en/Skia4Delphi) introducing the version of Skia4Delphi v6.0.0 on RAD Studio 12.0 and v6.1.0 on RAD Studio 12.1, and adding some extra units with [exclusive Embarcadero features](https://docwiki.embarcadero.com/RADStudio/Athens/en/Skia4Delphi_Exclusive_Features).\r\n\r\n# \r\n\r\n### Installation\r\n\r\nJust download and run the **Skia4Delphi_6.2.0_Setup.exe** attached.",v6.2.0,Vinícius Felipe Botelho Barbosa,,viniciusfbb,MIT License,skia4delphi,skia4delphi,29,android,windows,macos,ios,firemonkey,vcl,console,delphi,pascal,graphics,skia,fmx,,,,,,,,,/skia4delphi/skia4delphi,29,64,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/SixLabors/ImageSharp,https://github.com/SixLabors/ImageSharp,0,,,0,0,0,0,0,0,1,1,0,0,0,":camera: A modern, cross-platform, 2D Graphics library for .NET","<h1 align=""center"">\n\n<img src=""https://github.com/SixLabors/Branding/raw/main/icons/imagesharp/sixlabors.imagesharp.svg?sanitize=true"" alt=""SixLabors.ImageSharp"" width=""256""/>\n<br/>\nSixLabors.ImageSharp\n</h1>\n\n<div align=""center"">\n\n[![Build Status](https://img.shields.io/github/actions/workflow/status/SixLabors/ImageSharp/build-and-test.yml?branch=main)](https://github.com/SixLabors/ImageSharp/actions)\n[![Code coverage](https://codecov.io/gh/SixLabors/ImageSharp/branch/main/graph/badge.svg)](https://codecov.io/gh/SixLabors/ImageSharp)\n[![License: Six Labors Split](https://img.shields.io/badge/license-Six%20Labors%20Split-%23e30183)](https://github.com/SixLabors/ImageSharp/blob/main/LICENSE)\n[![Twitter](https://img.shields.io/twitter/url/http/shields.io.svg?style=flat&logo=twitter)](https://twitter.com/intent/tweet?hashtags=imagesharp,dotnet,oss&text=ImageSharp.+A+new+cross-platform+2D+graphics+API+in+C%23&url=https%3a%2f%2fgithub.com%2fSixLabors%2fImageSharp&via=sixlabors)\n\n</div>\n\n### **ImageSharp** is a new, fully featured, fully managed, cross-platform, 2D graphics API. \n\nImageSharp is a new, fully featured, fully managed, cross-platform, 2D graphics library. \nDesigned to simplify image processing, ImageSharp brings you an incredibly powerful yet beautifully simple API.\n\nImageSharp is designed from the ground up to be flexible and extensible. The library provides API endpoints for common image processing operations and the building blocks to allow for the development of additional operations.\n\nBuilt against [.NET 8](https://docs.microsoft.com/en-us/dotnet/standard/net-standard), ImageSharp can be used in device, cloud, and embedded/IoT scenarios.\n\n\n## License\n  \n- ImageSharp is licensed under the [Six Labors Split License, Version 1.0](https://github.com/SixLabors/ImageSharp/blob/main/LICENSE)  \n\n## Support Six Labors\n\nSupport the efforts of the development of the Six Labors projects. \n - [Purchase a Commercial License :heart:](https://sixlabors.com/pricing/)\n - [Become a sponsor via GitHub Sponsors :heart:]( https://github.com/sponsors/SixLabors)\n - [Become a sponsor via Open Collective :heart:](https://opencollective.com/sixlabors)\n\n## Documentation\n\n- [Detailed documentation](https://sixlabors.github.io/docs/) for the ImageSharp API is available. This includes additional conceptual documentation to help you get started.\n- Our [Samples Repository](https://github.com/SixLabors/Samples/tree/main/ImageSharp) is also available containing buildable code samples demonstrating common activities.\n\n## Questions\n\n- Do you have questions? Please [join our Discussions Forum](https://github.com/SixLabors/ImageSharp/discussions/categories/q-a). Do not open issues for questions.\n- For feature ideas please [join our Discussions Forum](https://github.com/SixLabors/ImageSharp/discussions/categories/ideas) and we'll be happy to discuss.  \n- Please read our [Contribution Guide](https://github.com/SixLabors/ImageSharp/blob/main/.github/CONTRIBUTING.md) before opening issues or pull requests!\n\n## Code of Conduct  \nThis project has adopted the code of conduct defined by the [Contributor Covenant](https://contributor-covenant.org/) to clarify expected behavior in our community.\nFor more information, see the [.NET Foundation Code of Conduct](https://dotnetfoundation.org/code-of-conduct).\n\n## Installation \n\nInstall stable releases via Nuget; development releases are available via MyGet.\n\n| Package Name                   | Release (NuGet) | Nightly (Feedz.io) |\n|--------------------------------|-----------------|-----------------|\n| `SixLabors.ImageSharp`         | [![NuGet](https://img.shields.io/nuget/v/SixLabors.ImageSharp.svg)](https://www.nuget.org/packages/SixLabors.ImageSharp/) | [![feedz.io](https://img.shields.io/badge/endpoint.svg?url=https%3A%2F%2Ff.feedz.io%2Fsixlabors%2Fsixlabors%2Fshield%2FSixLabors.ImageSharp%2Flatest)](https://f.feedz.io/sixlabors/sixlabors/nuget/index.json) |\n\n## Manual build\n\nIf you prefer, you can compile ImageSharp yourself (please do and help!)\n\n- Using [Visual Studio 2022](https://visualstudio.microsoft.com/vs/)\n  - Make sure you have the latest version installed\n  - Make sure you have [the .NET 8 SDK](https://www.microsoft.com/net/core#windows) installed\n\nAlternatively, you can work from command line and/or with a lightweight editor on **both Linux/Unix and Windows**:\n\n- [Visual Studio Code](https://code.visualstudio.com/) with [C# Extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode.csharp)\n- [.NET Core](https://www.microsoft.com/net/core#linuxubuntu)\n\nTo clone ImageSharp locally, click the ""Clone in [YOUR_OS]"" button above or run the following git commands:\n\n```bash\ngit clone https://github.com/SixLabors/ImageSharp\n```\n\nThen set the following config to ensure blame commands ignore mass reformatting commits.\n\n```bash\ngit config blame.ignoreRevsFile .git-blame-ignore-revs\n```\n\nIf working with Windows please ensure that you have enabled long file paths in git (run as Administrator).\n\n```bash\ngit config --system core.longpaths true\n```\n\nThis repository uses [Git Large File Storage](https://docs.github.com/en/github/managing-large-files/installing-git-large-file-storage). Please follow the linked instructions to ensure you have it set up in your environment.\n\nThis repository contains [Git Submodules](https://blog.github.com/2016-02-01-working-with-submodules/). To add the submodules to the project, navigate to the repository root and type:\n\n``` bash\ngit submodule update --init --recursive\n```\n\n## How can you help?\n\nPlease... Spread the word, contribute algorithms, submit performance improvements, unit tests, no input is too little. Make sure to read our [Contribution Guide](https://github.com/SixLabors/ImageSharp/blob/main/.github/CONTRIBUTING.md) before opening a PR.\n\nUseful tools for development and links to specifications can be found in our wikipage: [Useful-tools-and-links](https://github.com/SixLabors/ImageSharp/wiki/Useful-tools-and-links).\n\n## The ImageSharp Team\n\n- [James Jackson-South](https://github.com/jimbobsquarepants)\n- [Dirk Lemstra](https://github.com/dlemstra)\n- [Anton Firsov](https://github.com/antonfirsov)\n- [Scott Williams](https://github.com/tocsoft)\n- [Brian Popow](https://github.com/brianpopow)\n\n---\n\n<div>\n  <a href=""https://www.jetbrains.com/?from=ImageSharp"" align=""right""><img src=""https://resources.jetbrains.com/storage/products/company/brand/logos/jb_beam.svg"" alt=""JetBrains"" class=""logo-footer"" width=""72"" align=""left""></a>\n  <br/>\n\n  Special thanks to [JetBrains](https://www.jetbrains.com/?from=ImageSharp) for supporting us with open-source licenses for their IDEs.\n</div>\n",7265,graphics,C#,5,C#,HTML,JavaScript,PowerShell,Batchfile,,,,,,,,,,,,,,,,,,,,,,,,1264,114,1142,8,39,158,8,121966,844,912,871,41,31a4e1031dcca806e6a59fe8622d4e78ea61dd2d,Merge pull request #2773 from SixLabors/js/v4-2769,2024-07-16T01:26:41Z,James Jackson-South,james_south@hotmail.com,JimBobSquarePants,v3.1.4,## What's Changed\r\n* Backport - Only exit JPEG scan decoding after multiple EOF hits by @JimBobSquarePants in https://github.com/SixLabors/ImageSharp/pull/2702\r\n* Ensure VP8X alpha flag is updated correctly. by @JimBobSquarePants in https://github.com/SixLabors/ImageSharp/pull/2699\r\n* Backport APNG fix to release/3.1.x by @SpaceCheetah in https://github.com/SixLabors/ImageSharp/pull/2713\r\n* Limit Read Palette Indices by @JimBobSquarePants in https://github.com/SixLabors/ImageSharp/pull/2718\r\n* Limit all memory allocations in the MemoryAllocator layer by @antonfirsov in https://github.com/SixLabors/ImageSharp/pull/2706\r\n* Clear Pixel Buffers on Decode. by @JimBobSquarePants in https://github.com/SixLabors/ImageSharp/pull/2716\r\n\r\n\r\n**Full Changelog**: https://github.com/SixLabors/ImageSharp/compare/v3.1.3...v3.1.4,v3.1.4,James Jackson-South,,JimBobSquarePants,Other,ImageSharp,SixLabors,33,image-processing,drawing,c-sharp,jpeg,gif,bmp,png,netcore,exif,graphics,hacktoberfest,webp,tiff,,,,,,,,/SixLabors/ImageSharp,33,178,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/sherpa/sherpa,https://github.com/sherpa/sherpa,0,mostly for model fitting,,0,1,0,0,0,0,1,0,0,0,0,Fit models to your data in Python with Sherpa.,"![Build Status: Conda](https://github.com/sherpa/sherpa/workflows/Conda%20CI/badge.svg)\n![Build Status: Pip](https://github.com/sherpa/sherpa/workflows/Pip%20CI/badge.svg)\n[![Documentation Status](https://readthedocs.org/projects/sherpa/badge/)](https://sherpa.readthedocs.io/)\n[![DOI](https://zenodo.org/badge/683/sherpa/sherpa.svg)](https://zenodo.org/badge/latestdoi/683/sherpa/sherpa)\n[![GPLv3+ License](https://img.shields.io/badge/license-GPLv3+-blue.svg)](https://www.gnu.org/copyleft/gpl.html)\n![Python version](https://img.shields.io/badge/Python-3.9,3.10,3.11-green.svg?style=flat)\n\n<!-- TOC *generated with [DocToc](https://github.com/thlorenz/doctoc)* -->\n**Table of Contents**\n\n- [Sherpa](#sherpa)\n- [License](#license)\n- [How To Install Sherpa](#how-to-install-sherpa)\n  - [Using Conda](#using-conda)\n  - [Using pip](#using-pip)\n  - [Building from source](#building-from-source)\n- [History](#history)\n  - [Release History](#release-history)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n\nSherpa\n======\n\nSherpa is a modeling and fitting application for Python. It contains a\npowerful language for combining simple models into complex expressions\nthat can be fit to the data using a variety of statistics and\noptimization methods.  It is easily extensible to include user models,\nstatistics, and optimization methods.  It provides a high-level User\nInterface for interactive data-analysis work, such as within a\nJupyter notebook, and it can also be used as a library component,\nproviding fitting and modeling capabilities to an application.\n\nWhat can you do with Sherpa?\n\n- fit 1D (multiple) data including: spectra, surface brightness profiles, light curves, general ASCII arrays\n- fit 2D images/surfaces in Poisson/Gaussian regime\n- build complex model expressions\n- import and use your own models\n- use appropriate statistics for modeling Poisson or Gaussian data\n- import new statistics, with priors if required by analysis\n- visualize the parameter space with simulations or using 1D/2D cuts of the parameter space\n- calculate confidence levels on the best fit model parameters\n- choose a robust optimization method for the fit: Levenberg-Marquardt, Nelder-Mead Simplex or Monte Carlo/Differential Evolution.\n\nDocumentation for Sherpa is available at\n[Read The Docs](https://sherpa.readthedocs.io/)\nand also for [Sherpa in CIAO](http://cxc.harvard.edu/sherpa/).\n\nA [Quick Start Tutorial](http://nbviewer.ipython.org/github/sherpa/sherpa/tree/main/notebooks/SherpaQuickStart.ipynb)\nis included in the `notebooks` folder and can be opened with an `ipython notebook`.\n\nLicense\n=======\n\nThis program is free software: you can redistribute it and/or modify it under\nthe terms of the GNU General Public License as published by the Free Software\nFoundation, either version 3 of the License, or (at your option) any later\nversion. A copy of the GNU General Public License can be found in the\n`LICENSE` file provided with the source code, or from the\n[Free Software Foundation](http://www.gnu.org/licenses/).\n\nHow To Install Sherpa\n=====================\n\n[Full installation instructions](https://sherpa.readthedocs.io/en/latest/install.html)\nare part of the [Read The Docs](https://sherpa.readthedocs.io/)\ndocumentation, and should be read if the following is not sufficient.\n\nIt is strongly recommended that some form of *virtual environment* is\nused with Sherpa.\n\nSherpa is tested against Python versions 3.9, 3.10, and 3.11.\n\nThe last version of Sherpa which supported Python 2.7 is\n[Sherpa 4.11.1](https://doi.org/10.5281/zenodo.3358134).\n\nUsing Conda\n--------------\n\nSherpa is provided for both Linux and macOS operating systems running\nPython 3.9, 3.10, and 3.11. It can be installed with the `conda`\npackage manager by saying\n\n    $ conda install -c https://cxc.cfa.harvard.edu/conda/sherpa -c conda-forge sherpa\n\nUsing pip\n---------\n\nSherpa is also available\n[on PyPI](https://pypi.python.org/pypi/sherpa) and so can be installed\nwith the following command (which requires that the NumPy package is\nalready installed).\n\n    % pip install sherpa\n\nBuilding from source\n--------------------\n\nSource installation is available for platforms incompatible with the\nbinary builds, or for when the default build options are not sufficient\n(such as including support for the\n[`XSPEC` model library](https://heasarc.gsfc.nasa.gov/xanadu/xspec/)).\nThe steps are described in the\n[building from source](https://sherpa.readthedocs.io/en/latest/install.html#building-from-source)\ndocumentation.\n\nHistory\n=======\n\nSherpa is developed by the [Chandra X-ray\nObservatory](http://chandra.harvard.edu/) to provide fitting and modelling\ncapabilities to the [CIAO](http://cxc.harvard.edu/ciao/) analysis package. It\nhas been released onto [GitHub](https://github.com/sherpa/sherpa) for users to\nextend (whether to other areas of Astronomy or in other domains).\n\nRelease History\n---------------\n\n4.16.1: 21 May 2024 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.11236879.svg)](https://doi.org/10.5281/zenodo.11236879)\n\n4.16.0: 17 October 2023 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.825839.svg)](https://doi.org/10.5281/zenodo.825839)\n\n4.15.1: 18 May 2023 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7948720.svg)](https://doi.org/10.5281/zenodo.7948720)\n\n4.15.0: 11 October 2022 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7186379.svg)](https://doi.org/10.5281/zenodo.7186379)\n\n4.14.1: 20 May 2022 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6567264.svg)](https://doi.org/10.5281/zenodo.6567264)\n\n4.14.0: 07 October 2021 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5554957.svg)](https://doi.org/10.5281/zenodo.5554957)\n\n4.13.1: 18 May 2021 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4770623.svg)](https://doi.org/10.5281/zenodo.4770623)\n\n4.13.0: 08 January 2021 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4428938.svg)](https://doi.org/10.5281/zenodo.4428938)\n\n4.12.2: 27 October 2020 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4141888.svg)](https://doi.org/10.5281/zenodo.4141888)\n\n4.12.1: 14 July 2020 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3944985.svg)](https://doi.org/10.5281/zenodo.3944985)\n\n4.12.0: 30 January 2020 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3631574.svg)](https://doi.org/10.5281/zenodo.3631574)\n\n4.11.1: 1 August 2019 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3358134.svg)](https://doi.org/10.5281/zenodo.3358134)\n\n4.11.0: 20 February 2019 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.2573885.svg)](https://doi.org/10.5281/zenodo.2573885)\n\n4.10.2: 14 December 2018 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.2275738.svg)](https://doi.org/10.5281/zenodo.2275738)\n\n4.10.1: 16 October 2018 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1463962.svg)](https://doi.org/10.5281/zenodo.1463962)\n\n4.10.0: 11 May 2018 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1245678.svg)](https://doi.org/10.5281/zenodo.1245678)\n\n4.9.1: 01 August 2017 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.838686.svg)](https://doi.org/10.5281/zenodo.838686)\n\n4.9.0: 27 January 2017 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.260416.svg)](https://doi.org/10.5281/zenodo.260416)\n\n4.8.2: 23 September 2016 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.154744.svg)](https://doi.org/10.5281/zenodo.154744)\n\n4.8.1: 15 April 2016 [![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.49832.svg)](https://doi.org/10.5281/zenodo.49832)\n\n4.8.0: 27 January 2016 [![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.45243.svg)](https://doi.org/10.5281/zenodo.45243)\n",146,astronomy,Python,6,Python,C++,C,Shell,Jupyter Notebook,CSS,,,,,,,,,,,,,,,,,,,,,,,1091,147,900,44,45,21,0,57630,49,983,537,446,2f416f219bdca75fd9fe0a0a74ddca61206748e4,Merge #2079 (DougBurke) - minor cleanup of sherpa.optmethods,2024-07-16T13:48:00Z,wmclaugh,wmclaugh@cfa.harvard.edu,wmclaugh,Sherpa 4.16.1,"Sherpa 4.16.1\r\n=============\r\n\r\nThis release of Sherpa includes various enhancements, documentation updates, bug fixes, and infrastructure changes.\r\n\r\n* enhancements:\r\n    * minor plotting changes; add support for splitting model expression into\r\n      additive components and plot the results; support of log scale axes for\r\n      confidence plots; improved error messages for unavailable plot backends\r\n    * improved RMF plot display to allow choice of energy units\r\n* documentation changes:\r\n    * updates to fake_pha documentation\r\n    * updates to install.rst to fix incorrect links and outdated version references\r\n    * updated read the docs documentation to match current code\r\n* bug fixes:\r\n    * fixed multi-panel plot issue with Bokeh backend\r\n\r\nDetails\r\n-------\r\n\r\n#1608 - Remove the test setup.py option\r\n    Add the test configuration so that we can ensure the necessary testing\r\n    packages can be installed when the code is built. Support for running the\r\n    tests with 'python setup.py test' has been removed to match changes in the\r\n    Python packaging ecosystem.\r\n\r\n#1679 - Automatically split a model expression into additive components and plot the results\r\n    Add plot_source_components and plot_model_components calls that will split\r\n    up a model like gal * (pl + line) into two lines: one for gal * pl and one\r\n    for gal * line. There are also corresponding get_source_components_plot and\r\n    get_model_components_plot calls, and the ""source_components"" and\r\n    ""model_components"" arguments can be used with the plot call.\r\n\r\n#1684 -Fix fake_pha docs\r\n    Fix the documentation and some corner cases of sherpa.astro.fake.fake_pha.\r\n    There are several times when the simulated PHA output would not be correct\r\n    if the PHA contains at least one background component or a pileup model\r\n    was used.\r\n\r\n#1926 - Let users know they should use load_xstable_model not load_table_model\r\n    Support for XSPEC table models in load_table_model was deprecated in the\r\n    4.9 release. Ensure users know that they should be using load_xstable_model\r\n    instead by adding a warning message whenever it is used.\r\n\r\n#1934 - Zenodo broke the API we were using to request citation details.\r\n    Ensure that the sherpa.citation() command can query Zenodo for the release\r\n    information. Fix #1933.\r\n\r\n#1938 - CI: temporarily remove ds9 tests on macOS\r\n    Avoid the DS9 tests on the macOS build to avoid the failure case we\r\n    currently often, but not always, trigger.\r\n\r\n#1941 - Fix various typos\r\n    Updates from running codespell on source distribution\r\n\r\n#1945 - Better error checks for table loads\r\n    Insure that table models are sent numeric columns, to catch cases like #1943.\r\n\r\n#1946 - versioneer: update to version 0.29\r\n    Update the vendored copy of versioneer from 0.28 to 0.29.\r\n\r\n#1947 - Ensure tests can be run when the group library does not exist\r\n    Annotate several tests with the requires_group decorator.\r\n\r\n#1952 - Tests: better support of optional region/wcs dependencies\r\n    Ensure that the tests can pass if the region or WCS code is not available.\r\n\r\n#1954 - CI: avoid failures due to missing pyarrow installation\r\n    Allow the CI runs to pass with new changes to pandas warning messages\r\n    (completely unrelated to Sherpa but caused the tests to fail).\r\n\r\n#1957 - CI: support pytest 8.0.0\r\n    Allow the tests to pass when run with pytest 8.0.0.\r\n\r\n#1959 - Support logarithmic axes for 1D and 2D confidence plots\r\n    The projection and uncertainty plots, for both interval (1D) and region (2D),\r\n    now correctly create a logarithmic scale when requested. Fix #1561.\r\n\r\n#1962 - Pick up latest Python micro version in conda workflow\r\n    This resolves the missing crypt.h issue being seen in the conda test workflow\r\n    by picking up the latest micro version of the python packages for the conda\r\n    test build workflows.\r\n\r\n#1965 - Improve the error message when a plot backend is not available.\r\n    Improve the error message when a plot backend is not available. Fix #1964\r\n\r\n#1968 - Remove Conda build 3.25 pin from deployments\r\n    Removes the conda version specification (3.25) to avoid compatibility issues\r\n\r\n#1969 - Tests: group the DS9 tests so they can be run with one worker process\r\n    Ensure those tests that use the requires_ds9 decorator all have the same\r\n    group marker, so that they will be run within a single worker process when\r\n    using pytest-xdist to run tests in parallel. There is no change if\r\n    pytest-xdist is not installed.\r\n\r\n#1970 - Bump the minimum pytest version\r\n    Pytest occasionally changes behavior and it does not seem worth our time to\r\n    maintain backwards compatibility with old versions of pytest - see #1960\r\n\r\n#1972 - Remove excess brackets in model names\r\n    Remove excess brackets from model and parameter expressions. This is purely\r\n    a cosmetic change, but hopefully makes complicated model expressions easier\r\n    to read. Fix #780.\r\n\r\n#1973 - Fix the requires_pylab decorator\r\n    Change the testing code to better-handle those tests that want to check\r\n    plotting when using the pylab backend. Fix #1971.\r\n\r\n#1974 - Allow linked parameters to be fit without including them in the model expression\r\n    Treat linked parameters as part of the model expression (via the new lpars\r\n    attribute and get_thawed_pars routine) so that they can be included in a\r\n    fit without including the linked model as part of the model expression.\r\n    Fix #777\r\n\r\n#1976 - Fix bug in multi-panel plotting in bokeh\r\n    Fix bug in multi-panel plotting in bokeh\r\n\r\n#1977 - model: improve examples in docstrings\r\n    Allow model.py, parameter.py, and regrid.py to be included in the\r\n    docstring pytest run.\r\n\r\n#1984 - Rework x errorbar support to better-match community expectations\r\n    Improve the ""x errorbar"" handling for histogram-style data (including PHA),\r\n    including support for the wavelength setting (fix #1985), to better match\r\n    community expectations (e.g. #1817).\r\n\r\n#1994 - Tests: improve coverage of parameter module\r\n    Improve the test coverage of the parameter module.\r\n\r\n#1996 - docs: update examples to match current behavior\r\n    Update the ReadTheDocs documentation to match the current code.\r\n\r\n#1998 - Cleanup XSPEC interface\r\n    A small number of XSPEC model parameter values have been changed to match\r\n    the default frozen state of XSPEC version 12.13.1.\r\n\r\n#2000 - CM-481: update codecov action to v4, add upload token\r\n    Updates version of codecov being used\r\n\r\n#2003 - Minor plot work\r\n    Internal clean up of the plotting code in preparation for future changes.\r\n    The ARFPlot class will now generate an IOErr rather than PlotErr if sent\r\n    a non-PHA dataset (to better match other calls). The DataHistogramPlot\r\n    class now treats xerr as a property that can not be changed, and is fixed\r\n    to be the half-width of the X bins, although note that this field is not\r\n    really used and may be removed at some point in the future.\r\n\r\n#2004 - Minor improvements to the RMF plot capability\r\n    Improve the RMF plot display so that it recognizes the current units\r\n    setting and allows the choice of energies to be over-ridden by the user.\r\n\r\n#2006 - Use a separate context when handling multiprocessing calls\r\n    Ensure that Sherpa does not change the global multiprocessing state but\r\n    instead uses the (new) sherpa.utils.parallel.context attribute. Fix #1015.\r\n\r\n#2011 - Specialize the residual-style plots for histogram data\r\n    Improve the display of residual-style plots for integrated datasets.\r\n    Fix #1817.\r\n\r\n#2012 - [CI] Update ci-conda-workflow.yml to pin back to macos-12\r\n    Pin back to macos-12. This version is specific to macOS-intel. 13 doesn't\r\n    work as conda is removed from it.\r\n\r\n#2019 - update clone link and versions and DOI\r\n    This is a documentation update to fix the incorrect link in the source\r\n    install example. I also updated the version and DOI link in the description\r\n    of the source installation.\r\n\r\n#2020 - CI: bump the checkout actions versions\r\n    Internal change to how the GitHub actions are run for CI.\r\n\r\n#2021 - Update Conda Deployment Workflow Action Versions\r\n    Updates internal actions in the deployment workflow\r\n\r\n#2025 - Improve support for NumPy 2.0\r\n    Improvements for running Sherpa with NumPy 2.0.\r\n\r\n#2029 - Swap oldest-supported-numpy with numpy in requirements",4.16.1,,,Marie-Terrell,GNU General Public License v3.0,sherpa,sherpa,23,python,statistics,fitting,science,astronomy,,,,,,,,,,,,,,,,/sherpa/sherpa,37,15,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/SFML/SFML.Net,https://github.com/SFML/SFML.Net,0,,,0,0,0,0,0,0,1,0,0,0,0,Official binding of SFML for .Net languages,"# SFML.Net - Simple and Fast Multimedia Library for .Net\n\n[SFML](https://www.sfml-dev.org) is a simple, fast, cross-platform and object-oriented multimedia API. It provides access to windowing,\ngraphics, audio and network.\nIt is originally written in C++, and this project is its official binding for .Net languages (C#, VB, ...).\n\n## Authors\n\n* Laurent Gomila - main developer (laurent@sfml-dev.org)\n* Zachariah Brown - active maintainer (contact@zbrown.net)\n\n## Download\n\nYou can get the latest official release on [NuGet](https://www.nuget.org/packages/SFML.Net/) or on [the\nSFML website](https://www.sfml-dev.org/download/sfml.net).\nYou can also get the current development version from the [git repository](https://github.com/SFML/SFML.Net).\n\n## Learn\n\nThere is no tutorial for SFML.Net, but since it's a binding you can use the C++ resources:\n* [The official tutorials](https://www.sfml-dev.org/tutorials/)\n* [The online API documentation](https://www.sfml-dev.org/documentation/)\n* [The community wiki](https://github.com/SFML/SFML/wiki/)\n* [The community forum](https://en.sfml-dev.org/forums/) (or [for French speakers](https://fr.sfml-dev.org/forums/))\n\nOf course, you can also find the SFML.Net API documentation in the SDK.\n\n## Dependencies\n\nThe NuGet package of SFML.Net comes with all dependencies, including native CSFML\nand SFML libraries for most platforms.\n\nFor unsupported platforms or non-NuGet sources, you must have a copy of CSFML. CSFML can be compiled [from\nsource](https://github.com/SFML/CSFML/) or downloaded from [the official release\npage](https://www.sfml-dev.org/download/csfml/). Also note that since CSFML depends on\nthe main SFML project you also need all SFML runtime dependencies.\n\nAnother dependency is the OpenTK library. This is required by the examples to run correctly.\nIt is not required unless you plan on running the example programs that are included.\n\n## Contribute\n\nSFML and SFML.Net are open-source projects, and they need your help to go on growing and improving.\nDon't hesitate to post suggestions or bug reports on [the forum](https://en.sfml-dev.org/forums/)\nor post new bugs/features requests on the [issue tracker](https://github.com/SFML/SFML.Net/issues/).\nYou can even fork the project on GitHub, maintain your own version and send us pull requests periodically to merge your work.\n",509,graphics,C#,3,C#,PowerShell,Shell,,,,,,,,,,,,,,,,,,,,,,,,,,149,43,104,2,3,29,0,9588,87,116,107,9,bb4d22035b818cc69cb6d2354a44cc9e41270749,Bump package versions to 2.6.0,2024-06-09T21:46:34Z,Miron Alexandru,mironalex@hotmail.com,Marioalexsan,SFML.Net 2.6.0,"## What's Changed\r\n\r\n**Note:** If you have manually installed CSFML, you may first need to uninstall the CSFML NuGet package in order to update SFML.Net\r\n\r\n* Fix RenderTexture Display Method Return Incorrect Type #211 by @Rosst0pher in https://github.com/SFML/SFML.Net/pull/230\r\n* Handle null pointers in Cursor constructors by @Marioalexsan in https://github.com/SFML/SFML.Net/pull/233\r\n* Update CSFML to 2.6.0 by @eXpl0it3r in https://github.com/SFML/SFML.Net/pull/242\r\n* In-code Documentation Update by @DemoXinMC in https://github.com/SFML/SFML.Net/pull/239\r\n* Implement most CSFML 2.6.0 features by @Marioalexsan in https://github.com/SFML/SFML.Net/pull/243\r\n* Binding and API fixes by @Marioalexsan in https://github.com/SFML/SFML.Net/pull/244\r\n* Build Examples in CI by @eXpl0it3r in https://github.com/SFML/SFML.Net/pull/248\r\n* Use CSFML 2.6.1 and add missing WindowBase functions by @eXpl0it3r in https://github.com/SFML/SFML.Net/pull/249\r\n* Add missing Vulkan and Keyboard functions by @Marioalexsan in https://github.com/SFML/SFML.Net/pull/250\r\n* Rect Contains With Vector2 by @Rosst0pher in https://github.com/SFML/SFML.Net/pull/191\r\n* Fix AccessViolationException on KeyPress by @nulldg in https://github.com/SFML/SFML.Net/pull/254\r\n* Update sRGB API for Texture and RenderTarget by @Marioalexsan in https://github.com/SFML/SFML.Net/pull/252\r\n* Use UIntPtr for size_t arguments and return values from CSFML by @Marioalexsan in https://github.com/SFML/SFML.Net/pull/253\r\n* Remove unused DllImports by @Marioalexsan in https://github.com/SFML/SFML.Net/pull/257\r\n* Use fixed version for CSFML by @Marioalexsan in https://github.com/SFML/SFML.Net/pull/258\r\n* Fix documentation, up versions, change platform configuration for examples by @Marioalexsan in https://github.com/SFML/SFML.Net/pull/261\r\n\r\n## Download\r\n\r\nThe packages have been published to NuGet: https://www.nuget.org/packages/SFML.Net/2.6.0\r\n\r\n## Contributors\r\n\r\n* @Rosst0pher\r\n* @Marioalexsan\r\n* @eXpl0it3r\r\n* @DemoXinMC\r\n* @nulldg\r\n\r\nThanks to everyone who helped with this release! 🎉",02.06.2000,Lukas Dürrenberger,,eXpl0it3r,Other,SFML.Net,SFML,3,sfml,dotnet,multimedia,audio,graphics,opengl,crossplatform,hacktoberfest,,,,,,,,,,,,,/SFML/SFML.Net,9,33,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/seqan/seqan3,https://github.com/seqan/seqan3,0,"lib, not the standalone project",,0,1,0,0,0,0,1,0,0,0,0,The modern C++ library for sequence analysis. Contains version 3 of the library and API docs.,"# SeqAn3 -- the modern C++ library for sequence analysis\n\n<!--\n    SPDX-FileCopyrightText: 2006-2024 Knut Reinert & Freie Universität Berlin\n    SPDX-FileCopyrightText: 2016-2024 Knut Reinert & MPI für molekulare Genetik\n    SPDX-License-Identifier: CC-BY-4.0\n-->\n\n[![build status][1]][2]\n[![codecov][3]][4]\n[![license][5]][6]\n[![latest release][7]][8]\n[![platforms][9]][10]\n[![start][11]][12]\n[![twitter][13]][14]\n\n<!--\n    Above uses reference-style links with numbers.\n    See also https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#links.\n\n    For example, `[![build status][1]][2]` evaluates to the following:\n        `[link_text][2]`\n        `[2]` is a reference to a link, i.e. `[link_text](https://...)`\n\n        `[link_text]` = `[![build status][1]]`\n        `[1]` is once again a reference to a link - this time an image, i.e. `[![build status](https://...)]\n        `![build status]` is the text that should be displayed if the linked resource (`[1]`) is not available\n\n    `[![build status][1]][2]` hence means:\n    Show the picture linked under `[1]`. In case it cannot be displayed, show the text ""build status"" instead.\n    The picture, or alternative text, should link to `[2]`.\n-->\n\n[1]: https://img.shields.io/github/actions/workflow/status/seqan/seqan3/ci_linux.yml?branch=main&style=flat&logo=github&label=SeqAn3%20CI ""Open GitHub actions page""\n[2]: https://github.com/seqan/seqan3/actions?query=branch%3Amain\n[3]: https://codecov.io/gh/seqan/seqan3/branch/main/graph/badge.svg?token=BH1FQiBBle ""Open Codecov page""\n[4]: https://codecov.io/gh/seqan/seqan3\n[5]: https://img.shields.io/badge/license-BSD-green.svg ""Open Copyright page""\n[6]: https://docs.seqan.de/seqan3/main_user/about_copyright.html\n[7]: https://img.shields.io/github/release/seqan/seqan3.svg ""Get the latest release""\n[8]: https://github.com/seqan/seqan3/releases/latest\n[9]: https://img.shields.io/badge/platform-linux%20%7C%20bsd%20%7C%20osx-informational.svg ""Read more about our API""\n[10]: https://docs.seqan.de/seqan3/main_user/about_api.html\n[11]: https://img.shields.io/github/stars/seqan/seqan3.svg?style=social ""See who starred us""\n[12]: https://github.com/seqan/seqan3/stargazers\n[13]: https://img.shields.io/twitter/follow/SeqAnLib.svg?label=follow&style=social ""Follow us on Twitter""\n[14]: https://twitter.com/seqanlib\n\nSeqAn3 is the new version of the popular SeqAn template library for the analysis of biological sequences.\nIt enables the rapid development of high-performance solutions by providing generic algorithms and data structures\nfor:\n\n  * sequence representation and transformation\n  * full-text indexing and efficient search\n  * sequence alignment\n  * input/output of common file formats\n\nBy leveraging *Modern C++* it provides unprecedented ease-of-use without sacrificing performance.\n\nPlease see the [online documentation](https://docs.seqan.de/seqan3/main_user/) for more details.\n\n## Quick facts\n\n  * C++ header-only library: easy to integrate with your app & easy to distribute\n  * liberal open source license: allows integration with any app or library, requires only attribution\n  * very high code quality standards: >97% unit test coverage, performance regression tests, ...\n  * extensive API documentation & tutorials: more lines of documentation than lines of code\n  * aims to support any 64-bit architecture running Linux/POSIX; currently big-endian CPU architectures\n    like s390x are less supported\n\n## Dependencies\n\n|                   | requirement                                          | version  | comment                                     |\n|-------------------|------------------------------------------------------|----------|---------------------------------------------|\n|**compiler**       | [GCC](https://gcc.gnu.org)                           | ≥ 11     | no other compiler is currently supported!   |\n|**build system**   | [CMake](https://cmake.org)                           | ≥ 3.5    | optional, but recommended                   |\n|**required libs**  | [SDSL](https://github.com/xxsds/sdsl-lite)           | ≥ 3.0.3  |                                             |\n|**optional libs**  | [cereal](https://github.com/USCiLab/cereal)          | ≥ 1.3.1  | required for serialisation and CTD support  |\n|                   | [zlib](https://github.com/madler/zlib)               | ≥ 1.2    | required for `*.gz` and `.bam` file support |\n|                   | [bzip2](https://www.sourceware.org/bzip2)            | ≥ 1.0    | required for `*.bz2` file support           |\n\n## Usage\n\nWe recommend that you use CMake to build your project:\n\n  * [Setup-Tutorial](https://docs.seqan.de/seqan3/main_user/setup.html)\n  * Using CMake guarantees that all optional dependencies are automatically detected and activated.\n\nQuick-Setup without CMake:\n\n  * Clone the repository with submodules: `git clone --recurse-submodules https://github.com/seqan/seqan3.git`\n  * Add the following to your compiler invocation:\n    * the include directories of SeqAn and its dependencies\n    * C++20 mode\n    * Macros indicating the presence of zlib and bzip2 (set only if actually available in your paths!)\n  * The command could look like this:\n```sh\ng++-11 -O3 -DNDEBUG -Wall -Wextra                               \\n    -std=c++20                                                  \\n    -I       /path/to/seqan3/include                            \\n    -isystem /path/to/seqan3/submodules/sdsl-lite/include       \\n    -isystem /path/to/seqan3/submodules/cereal/include          \\n    -DSEQAN3_HAS_ZLIB=1 -DSEQAN3_HAS_BZIP2=1                    \\n    -lz -lbz2 -pthread                                          \\n  your_file.cpp\n```\n\n## Sponsorships\n\n[![Vercel](https://raw.githubusercontent.com/seqan/seqan3/main/test/documentation/.vercel/powered-by-vercel.svg)](https://vercel.com/?utm_source=seqan&utm_campaign=oss)\n\nVercel is kind enough to sponsor our documentation preview-builds within our pull requests. Check them out!\n",397,bioinformatics,C++,4,C++,CMake,Perl,Shell,,,,,,,,,,,,,,,,,,,,,,,,,2466,194,2267,5,2,48,3446,21794,81,766,739,27,c7ff420f8e9a1db00e9b6aa994bafaa4b3301f4a,Merge pull request #3274 from eseiler/fix/codecov,2024-07-18T19:49:04Z,Enrico Seiler,eseiler@users.noreply.github.com,eseiler,SeqAn 3.3.0,"[![GitHub commits since tagged version (branch)][version_diff_badge]][version_diff_commits]\r\n\r\n[version_diff_badge]: https://img.shields.io/github/commits-since/seqan/seqan3/3.2.0/3.3.0 ""Click to view commits""\r\n[version_diff_commits]: https://github.com/seqan/seqan3/compare/3.2.0...3.3.0\r\n\r\nWe are glad to announce the SeqAn 3.3.0 release that has a major compiler update:\r\n<p style=""text-align: center;"">We <b>dropped</b> GCC 10 and we <b>added GCC 13 support</b>.</p>\r\n\r\nWhile we will present essential changes of the 3.3.0 release in this message, you can also find a comprehensive list of the changes in our [changelog](https://docs.seqan.de/seqan/3.3.0/about_changelog.html).\r\n\r\n\r\n* Get to know SeqAn3 with our [tutorials](https://docs.seqan.de/seqan/3.3.0/usergroup1.html).\r\n* Visit our [API documentation](https://docs.seqan.de/seqan/3.3.0/index.html).\r\n* Check out our [SeqAn3 Cookbook](https://docs.seqan.de/seqan/3.3.0/cookbook.html) with all the copy & paste snippets you need.\r\n\r\n### :tada: New Features\r\n\r\n#### Alignment\r\n  * The function `seqan3::alignment_from_cigar` creates an alignment from a CIGAR vector ([\#3057](https://github.com/seqan/seqan3/pull/3057)) or a CIGAR string ([\#3077](https://github.com/seqan/seqan3/pull/3077)).\r\n  * The function `seqan3::cigar_from_alignment` creates a CIGAR vector from an alignment ([\#3057](https://github.com/seqan/seqan3/pull/3057)).\r\n\r\n#### Alphabet\r\n  * Improved performance of vector assignment for alphabets by a factor 2.5 ([\#3038](https://github.com/seqan/seqan3/pull/3038)).\r\n  * Improved performance of `seqan3::dna4::complement()` ([\#3026](https://github.com/seqan/seqan3/pull/3026)).\r\n  * Char literals returning `std::vector` are now `constexpr` if supported by the compiler ([\#3073](https://github.com/seqan/seqan3/pull/3073)).\r\n\r\n#### I/O\r\n  * Made `seqan3::sam_file_header::program_info_t` easier to copy ([\#3145](https://github.com/seqan/seqan3/pull/3145)).\r\n  * Reading SAM/BAM files is 2x faster than before ([\#3106](https://github.com/seqan/seqan3/pull/3106)).\r\n\r\n#### Search\r\n  * The uncompressed `seqan3::interleaved_bloom_filter` (IBF) can now be constructed from a compressed IBF ([\#3082](https://github.com/seqan/seqan3/pull/3082)).\r\n\r\n### :bug: Notable bug fixes\r\n\r\n#### Alignment\r\n  * Resolved an issue resulting in a wrong alignment score ([\#3098](https://github.com/seqan/seqan3/pull/3098)).\r\n\r\n#### I/O\r\n  * Fixed writing an empty SAM-BAM file resulting in an invalid file ([\#3081](https://github.com/seqan/seqan3/pull/3081)).\r\n  * `seqan3::sequence_file_input_traits` now allows `char` as a sequence alphabet ([\#3128](https://github.com/seqan/seqan3/pull/3128)).\r\n\r\n#### Utility\r\n  * Fixed spin delay having no effect on the PowerPC platform ([\#3129](https://github.com/seqan/seqan3/pull/3129)).\r\n\r\n### :hammer_and_wrench: Notable API changes\r\n\r\n#### Alignment\r\n  * The fields `seqan3::field::offset` and `seqan3::field::alignment` have been removed from `seqan3::sam_record` ([\#3058](https://github.com/seqan/seqan3/pull/3058), [\#3089](https://github.com/seqan/seqan3/pull/3089)).\r\n    For `seqan3::field::offset`, please check the soft clipping of the CIGAR string (`seqan3::sam_record::cigar()`).\r\n    For `seqan3::field::alignment`, please use `seqan3::alignment_from_cigar`.\r\n\r\n### :electric_plug: External dependencies\r\n  * Dropped support for GCC 10 ([\#3148](https://github.com/seqan/seqan3/pull/3148)).\r\n  * Added support for GCC 13 ([\#3148](https://github.com/seqan/seqan3/pull/3148)).\r\n  * We require at least CMake 3.16 for our test suite. Note that the minimum requirement for using SeqAn3 is unchanged ([\#3050](https://github.com/seqan/seqan3/pull/3050)).\r\n  * We now use Doxygen version 1.9.6 to build our documentation ([\#3116](https://github.com/seqan/seqan3/pull/3116)).\r\n  * Updated sdsl-lite to 3.0.3 ([\#3170](https://github.com/seqan/seqan3/pull/3170), [\#3174](https://github.com/seqan/seqan3/pull/3174)).\r\n  * Compatibility with SeqAn2: SeqAn2's namespace was changed from `seqan` to `seqan2`. For interoperability, an up-to-date checkout of [SeqAn2's main branch](https://github.com/seqan/seqan) is required ([\#3156](https://github.com/seqan/seqan3/pull/3156)).",03.03.2000,Enrico Seiler,,eseiler,Other,seqan3,seqan,13,sequence-analysis,seqan,cpp17,cpp20,cpp-concepts,bioinformatics,blast,sequence-alignment,fasta,fastq,samtools,fm-index,modern,,,,,,,,/seqan/seqan3,13,26,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/scverse/scanpy,https://github.com/scverse/scanpy,0.5,,1,1,1,1,1,0,0,0,0,0,0,1,Single-cell analysis in Python. Scales to >1M cells.,"[![Stars](https://img.shields.io/github/stars/scverse/scanpy?style=flat&logo=GitHub&color=yellow)](https://github.com/scverse/scanpy/stargazers)\n[![PyPI](https://img.shields.io/pypi/v/scanpy?logo=PyPI)](https://pypi.org/project/scanpy)\n[![Downloads](https://static.pepy.tech/badge/scanpy)](https://pepy.tech/project/scanpy)\n[![Conda](https://img.shields.io/conda/dn/conda-forge/scanpy?logo=Anaconda)](https://anaconda.org/conda-forge/scanpy)\n[![Docs](https://readthedocs.com/projects/icb-scanpy/badge/?version=latest)](https://scanpy.readthedocs.io)\n[![Build Status](https://dev.azure.com/scverse/scanpy/_apis/build/status/scverse.scanpy?branchName=main)](https://dev.azure.com/scverse/scanpy/_build)\n[![Discourse topics](https://img.shields.io/discourse/posts?color=yellow&logo=discourse&server=https%3A%2F%2Fdiscourse.scverse.org)](https://discourse.scverse.org/)\n[![Chat](https://img.shields.io/badge/zulip-join_chat-%2367b08f.svg)](https://scverse.zulipchat.com)\n[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org/)\n\n# Scanpy – Single-Cell Analysis in Python\n\nScanpy is a scalable toolkit for analyzing single-cell gene expression data\nbuilt jointly with [anndata][].  It includes\npreprocessing, visualization, clustering, trajectory inference and differential\nexpression testing.  The Python-based implementation efficiently deals with\ndatasets of more than one million cells.\n\nDiscuss usage on the scverse [Discourse][]. Read the [documentation][].\nIf you'd like to contribute by opening an issue or creating a pull request, please take a look at our [contribution guide][].\n\n[anndata]: https://anndata.readthedocs.io\n[discourse]: https://discourse.scverse.org/\n[documentation]: https://scanpy.readthedocs.io\n\n[//]: # (numfocus-fiscal-sponsor-attribution)\n\nscanpy is part of the scverse project ([website](https://scverse.org), [governance](https://scverse.org/about/roles)) and is fiscally sponsored by [NumFOCUS](https://numfocus.org/).\nIf you like scverse and want to support our mission, please consider making a [donation](https://numfocus.org/donate-to-scverse) to support our efforts.\n\n<div align=""center"">\n<a href=""https://numfocus.org/project/scverse"">\n  <img\n    src=""https://raw.githubusercontent.com/numfocus/templates/master/images/numfocus-logo.png""\n    width=""200""\n  >\n</a>\n</div>\n\n\n## Citation\n\nIf you use `scanpy` in your work, please cite the `scanpy` publication as follows:\n\n> **SCANPY: large-scale single-cell gene expression data analysis**\n>\n> F. Alexander Wolf, Philipp Angerer, Fabian J. Theis\n>\n> _Genome Biology_ 2018 Feb 06. doi: [10.1186/s13059-017-1382-0](https://doi.org/10.1186/s13059-017-1382-0).\n\nYou can cite the scverse publication as follows:\n\n> **The scverse project provides a computational ecosystem for single-cell omics data analysis**\n>\n> Isaac Virshup, Danila Bredikhin, Lukas Heumos, Giovanni Palla, Gregor Sturm, Adam Gayoso, Ilia Kats, Mikaela Koutrouli, Scverse Community, Bonnie Berger, Dana Pe’er, Aviv Regev, Sarah A. Teichmann, Francesca Finotello, F. Alexander Wolf, Nir Yosef, Oliver Stegle & Fabian J. Theis\n>\n> _Nat Biotechnol._ 2023 Apr 10. doi: [10.1038/s41587-023-01733-8](https://doi.org/10.1038/s41587-023-01733-8).\n\n\n[contribution guide]: CONTRIBUTING.md\n",1812,bioinformatics,Python,2,Python,R,,,,,,,,,,,,,,,,,,,,,,,,,,,1408,160,1185,63,48,141,14,40339,586,1715,1210,505,b918a23eb77462837df90d7b3a30a573989d4d48,"Revert ""fix layer use_raw (#3150)"" (#3154)",2024-07-12T16:08:30Z,Severin Dicks,37635888+Intron7@users.noreply.github.com,Intron7,01.10.2002,,01.10.2002,Ilan Gold,,ilan-gold,"BSD 3-Clause ""New"" or ""Revised"" License",scanpy,scverse,84,machine-learning,data-science,visualize-data,transcriptomics,bioinformatics,scanpy,anndata,python,scverse,,,,,,,,,,,,/scverse/scanpy,85,52,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/scverse/anndata,https://github.com/scverse/anndata,0,,0,0,0,0,0,0,0,1,0,0,0,0,Annotated data.,"[![Build Status](https://dev.azure.com/scverse/anndata/_apis/build/status/scverse.anndata?branchName=main)](https://dev.azure.com/scverse/anndata/_build)\n[![Conda](https://img.shields.io/conda/vn/conda-forge/anndata.svg)](https://anaconda.org/conda-forge/anndata)\n[![Coverage](https://codecov.io/gh/scverse/anndata/branch/main/graph/badge.svg?token=IN1mJN1Wi8)](https://codecov.io/gh/scverse/anndata)\n[![Docs](https://readthedocs.com/projects/icb-anndata/badge/?version=latest)](https://anndata.readthedocs.io)\n[![PyPI](https://img.shields.io/pypi/v/anndata.svg)](https://pypi.org/project/anndata)\n[![Downloads](https://static.pepy.tech/badge/anndata/month)](https://pepy.tech/project/anndata)\n[![Downloads](https://static.pepy.tech/badge/anndata)](https://pepy.tech/project/anndata)\n[![Stars](https://img.shields.io/github/stars/scverse/anndata?logo=GitHub&color=yellow)](https://github.com/scverse/anndata/stargazers)\n[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](http://numfocus.org)\n\n<img\n  src=""https://raw.githubusercontent.com/scverse/anndata/main/docs/_static/img/anndata_schema.svg""\n  class=""dark-light"" align=""right"" width=""350"" alt=""image""\n/>\n\n# anndata - Annotated data\n\nanndata is a Python package for handling annotated data matrices in memory and on disk, positioned between pandas and xarray. anndata offers a broad range of computationally efficient features including, among others, sparse data support, lazy operations, and a PyTorch interface.\n\n- Discuss development on [GitHub](https://github.com/scverse/anndata).\n- Read the [documentation](https://anndata.readthedocs.io).\n- Ask questions on the [scverse Discourse](https://discourse.scverse.org).\n- Install via `pip install anndata` or `conda install anndata -c conda-forge`.\n- See [Scanpy's documentation](https://scanpy.readthedocs.io/) for usage related to single cell data. anndata was initially built for Scanpy.\n\n[//]: # (numfocus-fiscal-sponsor-attribution)\n\nanndata is part of the scverse project ([website](https://scverse.org), [governance](https://scverse.org/about/roles)) and is fiscally sponsored by [NumFOCUS](https://numfocus.org/).\nPlease consider making a tax-deductible [donation](https://numfocus.org/donate-to-scverse) to help the project pay for developer time, professional services, travel, workshops, and a variety of other needs.\n\n\n<a href=""https://numfocus.org/project/scverse"">\n  <img\n    src=""https://raw.githubusercontent.com/numfocus/templates/master/images/numfocus-logo.png""\n    width=""200""\n  >\n</a>\n\n## Citation\n\nIf you use `anndata` in your work, please cite the `anndata` pre-print as follows:\n\n> **anndata: Annotated data**\n>\n> Isaac Virshup, Sergei Rybakov, Fabian J. Theis, Philipp Angerer, F. Alexander Wolf\n>\n> _bioRxiv_ 2021 Dec 19. doi: [10.1101/2021.12.16.473007](https://doi.org/10.1101/2021.12.16.473007).\n\nYou can cite the scverse publication as follows:\n\n> **The scverse project provides a computational ecosystem for single-cell omics data analysis**\n>\n> Isaac Virshup, Danila Bredikhin, Lukas Heumos, Giovanni Palla, Gregor Sturm, Adam Gayoso, Ilia Kats, Mikaela Koutrouli, Scverse Community, Bonnie Berger, Dana Pe’er, Aviv Regev, Sarah A. Teichmann, Francesca Finotello, F. Alexander Wolf, Nir Yosef, Oliver Stegle & Fabian J. Theis\n>\n> _Nat Biotechnol._ 2023 Apr 10. doi: [10.1038/s41587-023-01733-8](https://doi.org/10.1038/s41587-023-01733-8).\n",535,bioinformatics,Python,1,Python,,,,,,,,,,,,,,,,,,,,,,,,,,,,869,87,728,54,42,54,8,4846,150,692,504,188,991804420a7202512a12de96e8f166a828125016,(feat): allow writing from `cupy` inside `dask` arrays (#1550),2024-07-12T13:09:28Z,Ilan Gold,ilanbassgold@gmail.com,ilan-gold,0.10.8,,0.10.8,Philipp A.,,flying-sheep,"BSD 3-Clause ""New"" or ""Revised"" License",anndata,scverse,79,scanpy,data-science,transcriptomics,bioinformatics,machine-learning,scverse,anndata,,,,,,,,,,,,,,/scverse/anndata,80,14,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/ScottPlot/ScottPlot,https://github.com/ScottPlot/ScottPlot,0,,,0,0,0,0,0,0,1,1,0,0,0,Interactive plotting library for .NET,"# ScottPlot\n\n[![CI](https://github.com/ScottPlot/ScottPlot/actions/workflows/ci.yaml/badge.svg)](https://github.com/ScottPlot/ScottPlot/actions/workflows/ci.yaml)\n[![](https://img.shields.io/nuget/dt/scottplot?color=004880&label=Downloads&logo=NuGet)](https://www.nuget.org/packages/ScottPlot/)\n[![Discord](https://badgen.net/discord/members/Dru6fnY2UX?icon=discord&color=5562ea&label=Discord)](https://scottplot.net/discord/)\n\n**ScottPlot is a free and open-source plotting library for .NET** that makes it easy to interactively display large datasets. The [**ScottPlot Cookbook**](https://scottplot.net/cookbook/5.0/) demonstrates how to create line plots, bar charts, pie graphs, scatter plots, and more with just a few lines of code. The **[ScottPlot Demo](https://scottplot.net/demo/)** shows how to create plots in GUI environments with advanced interactive behaviors. ScottPlot supports \n    [Windows Forms](https://scottplot.net/quickstart/winforms/), \n    [WPF](https://scottplot.net/quickstart/wpf/),\n    [Console](https://scottplot.net/quickstart/console/),\n    [Blazor](https://scottplot.net/quickstart/blazor/),\n    [Avalonia](https://scottplot.net/quickstart/avalonia/),\n    [Eto](https://scottplot.net/quickstart/eto/),\n    [Notebooks](https://scottplot.net/quickstart/notebook/),\n    and [more](https://scottplot.net/quickstart/)!\n\n### Visit https://ScottPlot.NET for documentation and additional information\n\n<div align='center'>\n\n<a href='https://scottplot.net'><img src='dev/graphics/ScottPlot.gif'></a>\n\n<a href='https://scottplot.net/cookbook/5.0/'><img src='dev/graphics/cookbook.jpg'></a>\n\n</div>\n\n**Contributing:** We welcome contributions from the community! We invite contributions from anyone, including developers who may be new to contributing to open-source projects. Visit https://ScottPlot.net/contributing/ to get started!\n\n**License:** ScottPlot was created by [Scott W Harden](https://swharden.com/about/) and enhanced by [many contributions](https://scottplot.net/changelog/) from the [open-source community](https://scottplot.net/contributors/). It is provided under the permissive [MIT license](LICENSE) and is free to modify and use for any purpose.\n\nIf you enjoy ScottPlot ***give us a star!*** ⭐ ",4907,graphics,C#,9,C#,Batchfile,Python,Shell,Dockerfile,Jupyter Notebook,HTML,CSS,JavaScript,,,,,,,,,,,,,,,,,,,,1116,90,1018,8,2,144,196,140661,804,2276,2163,113,4078033d75672577455af1e8d8c742389124b82a,Merge pull request #4053 from ScottPlot/3186,2024-07-19T15:24:01Z,Scott W Harden,swharden@gmail.com,swharden,ScottPlot 5.0.36,"* Fonts: Made typeface caching thread-safe to improve support for multi-threaded environments (#3940) @Hawkwind250\r\n* Ticks: Added a custom `LabelFormatter` to DateTime axes which use fixed intervals (#3936) @Fruchtzwerg94\r\n* Fonts: Enabled sub-pixel text positioning for improved character placement (#3937) @bforlgreen\r\n* Axes: Improved automatic axis limit expansion for extremely large numbers (#3930) @CodeDevAM\r\n* Statistics: Added `ScottPlot.Statistics.Descriptive` methods `Median()` and `Percentile()`\r\n* Population: Added a new Population plot type for displaying collections of values (#3944, #3676)\r\n* IAxisLimitManager: Separated `GetAxisLimits()` into `GetRangeX()` and `GetRangeY()` for improved customization and performance (#3946) @drolevar\r\n* Experimental: Added `Plottables.Experimental.DataStreamer2` plot type for displaying streaming data in a circular buffer (#3946) @drolevar\r\n* Rendering: Automatically re-render if a render invokes an event that requests it (#3952) @BrianAtZetica\r\n* SVG: File encoding now supports text containing UTF8 characters (#3956, #3957) @aespitia\r\n* Documentation: Added a sandbox .NET API project and quickstart section to the website (#3959, #3824) @aespitia\r\n* Color: Added `ToColor()` and `FromColor()` to simplify conversion between `ScottPlot.Color` and `System.Drawing.Color` (#3964, ##3953) @aespitia\r\n* Console: Saved image path can be displayed by calling `myPlot.SavePng('demo.png', 600, 400).ConsoleWritePath()` (#3965, #3943) @aespitia\r\n* Rendering: Improved sharpness of axis frames, tick marks, and grid lines by disabling anti-aliasing by default and added `Plot.Axes.AntiAlias()` so users can customize this behavior (#3976) @bforlgreen\r\n* Signal: Added support for generic data sources in read-only lists (#3978, #3942) @sdpenner\r\n* LinearRegression: Added overload that accepts `IEnumerable<Coordinates>` (#3982, #3981) @ANGADJEET @CoderPM2011\r\n* Colormap: Added `GetColors()` for generating a given number of colors evenly spaced along a colormap (#3983, #3947) @CoderPM2011\r\n* CoordinateLine: Added additional constructors for creating lines given a point and slope (#3987, #3986) @aalgrou\r\n* DataLogger: Added `Clear()` and `ResetMinAndMaxValues()` to the data logger source class (#3993, #3969) @jpgarza93\r\n* Controls: Improved behavior of middle-click-drag zooming over axis panels for plots using DPI scaling (#3994) @bforlgreen\r\n* Style: Added `Plot.Axes.Hairline()` to enable axis frames, tick marks, and grid lines to render 1px wide regardless of scale factor (#3995) @bforlgreen\r\n* Axes: Display no ticks instead of throwing an exception if automatic DateTime ticks are used with invalid ranges (#4001) @githubkau\r\n* SignalXY: Improve support for data sources containing zero-length arrays (#4000) @githubkau\r\n* CoordinateRect: Added constructor that accepts `IAxes` (#4008, #3985) @CoderPM2011\r\n* CoordinateRect: Fixed an issue that caused `BottomRight` to return incorrect coordinates (#4009, #3996) @CoderPM2011\r\n* Cookbook: Added a demonstration of stacked filled line plots (#4010, #3967) @CoderPM2011 @MarkG008\r\n* Benchmark: Improved text default alignment of double-click benchmark (#4014) @banncan\r\n* SignalXY: Improved behavior of `MinRenderIndex` and `MaxRenderIndex` (#4011) @StendProg\r\n* Project: Cut dependency on `System.Runtime.InteropServices.RuntimeInformation` (#3911) @swaitvor @chhh\r\n* Legend: Added `IsVisible` property to `LegendItem` to customize visibility of items in manual legends (#3931) @cataclism\r\n* Signal: Exposed `Data` property setter so users can replace the `ISignalSource` without resetting the plottable (#3932) @danieljfarrell @bclehmann\r\n* Heatmap: Exposed `Intensities` setter to allow users to replace heatmap data with a 2D array of a different size (#3941) @sdpenner\r\n* Axes: Added `Plot.Axes.Link()` to simplify sharing axis limits between multiple plots or plot controls (#4003)\r\n* Blazor: Added automatic resizing options to the in-browser cookbook (#3710, #3664) @KroMignon\r\n* Axis Spans: Improved visibility of extremely narrow spans (#4017, #3968) @CoderPM2011\r\n* Generate: Added `RandomNormalNumber()` that returns a single value to compliment `RandomNormal()` which returns a collection (#4018, #3980) @CoderPM2011\r\n* Axis Lines: Offset label according to the panel offset to improve appearance on multi-axis plots (#1766) @fuxinsen @mengfanmin123",5.0.36,Scott W Harden,,swharden,MIT License,ScottPlot,ScottPlot,156,plot,plotting,visualization,data-visualization,charts,chart,charting,graphics,csharp,dotnet,,,,,,,,,,,/ScottPlot/ScottPlot,157,89,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/SciTools/cartopy,https://github.com/SciTools/cartopy,0.5,"Scientific tool, but not a project",0,0,0,0,0,0,0,1,1,0,0,0,Cartopy - a cartographic python library with matplotlib support,"<h1 align=""center"" style=""margin:1em;"">\n  <a href=""https://scitools.org.uk/cartopy/docs/latest/"">\n    <img src=""https://scitools.org.uk/cartopy/docs/latest/_static/cartopy.png""\n         alt=""Cartopy""></a>\n</h1>\n\n<h4 align=""center"">\n    Cartopy is a Python package designed to make drawing maps for\n    data analysis and visualisation easy.\n</h4>\n\n<p align=""center"">\n<!-- https://shields.io/ is a good source of these -->\n<a href=""https://anaconda.org/conda-forge/cartopy"">\n<img src=""https://img.shields.io/conda/dn/conda-forge/cartopy.svg""\n alt=""conda-forge downloads"" /></a>\n<a href=""https://github.com/SciTools/cartopy/releases"">\n<img src=""https://img.shields.io/github/tag/SciTools/cartopy.svg""\n alt=""Latest version"" /></a>\n<a href=""https://github.com/SciTools/cartopy/commits/main"">\n<img src=""https://img.shields.io/github/commits-since/SciTools/cartopy/latest.svg""\n alt=""Commits since last release"" /></a>\n<a href=""https://github.com/SciTools/cartopy/graphs/contributors"">\n<img src=""https://img.shields.io/github/contributors/SciTools/cartopy.svg""\n alt=""# contributors"" /></a>\n<a href=""https://zenodo.org/badge/latestdoi/5282596"">\n<img src=""https://zenodo.org/badge/5282596.svg""\n alt=""zenodo"" /></a>\n<a href=""https://gitter.im/SciTools/cartopy?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge"">\n<img src=""https://badges.gitter.im/SciTools/cartopy.svg"" alt=""Gitter Chat"" /></a>\n<a href=""https://results.pre-commit.ci/latest/github/SciTools/cartopy/main"">\n<img src=""https://results.pre-commit.ci/badge/github/SciTools/cartopy/main.svg""\n alt=""pre-commit.ci"" /></a>\n</p>\n<br>\n\n# Table of contents\n\n<!--\nNOTE: toc auto-generated with https://github.com/jonschlinkert/markdown-toc\n    $> markdown-toc -i --bullets='-' README.md\n\nNOTE: This entire README can be markdown linted with\n    https://github.com/igorshubovych/markdownlint-cli\n    $ echo '{""no-inline-html"": false}' > .markdownrc\n    $ markdownlint README.md\n-->\n\n<!-- toc -->\n\n- [Overview](#overview)\n- [Get in touch](#get-in-touch)\n- [License and copyright](#license-and-copyright)\n\n<!-- tocstop -->\n\n## Overview\n\nCartopy is a Python package designed to make drawing maps for data\nanalysis and visualisation easy.\n\nIt features:\n\n- object oriented projection definitions\n- point, line, polygon and image transformations between projections\n- integration to expose advanced mapping in Matplotlib with a simple and\n  intuitive interface\n- powerful vector data handling by integrating shapefile reading with Shapely\n  capabilities\n\nDocumentation can be found at <https://scitools.org.uk/cartopy/docs/latest/>.\n\n## Get in touch\n\n- Ask usage questions on\n  [StackOverflow](https://stackoverflow.com/questions/tagged/cartopy).\n- For less well defined questions, ideas, general discussion or announcements of\n  related projects use the\n  [Cartopy category on Matplotlib's Discourse](https://discourse.matplotlib.org/c/3rdparty/cartopy/19).\n- Report bugs, suggest features or view the source code on\n  [GitHub](https://github.com/SciTools/cartopy).\n- To chat with developers and other users you can use the\n  [Gitter Chat](https://gitter.im/SciTools/cartopy).\n\n\n## Credits, copyright and license\n\nCartopy is developed collaboratively under the SciTools umberella.\n\nA full list of codecontributors (""Cartopy contributors"") can be found at\nhttps://github.com/SciTools/cartopy/graphs/contributors.\n\nCode is just one of many ways of positively contributing to Cartopy, please see\nour [contributing guide](.github/CONTRIBUTING.md) for more details on how\nyou can get involved.\n\nCartopy is released under the 3-Clause BSD license with a shared copyright model.\nSee [LICENSE](LICENSE) for full terms.\n\nThe [Met Office](https://metoffice.gov.uk) has made a significant\ncontribution to the development, maintenance and support of this library.\nAll Met Office contributions are copyright on behalf of the British Crown.\n",1384,geometry,Python,2,Python,Cython,,,,,,,,,,,,,,,,,,,,,,,,,,,1178,190,954,34,7,119,2,84726,361,1239,964,275,b8618af24f02a312c06bc6fb4ba05e9a3c1d0dc6,Merge pull request #2416 from greglucas/fix-imshow-args,2024-07-12T00:41:56Z,Ryan May,rmay@ucar.edu,dopplershift,REL: v0.23,"This release is compatible with Python 3.12 and Numpy 2.\r\n\r\nSome of the major new features are:\r\n* Updates to the Gridliner to turn it into a Matplotlib Artist (which greatly enhances performance and interactivity)\r\n* Better support for Compound paths split across boundaries\r\n* Addition of GSHHS Antarctica features\r\n* Turning the Cartopy FeatureArtist into a Matplotlib Collection which enables arrays of values and styles to be used with the collection of geometries in the FeatureArtist (i.e. colormapping the features based on array values is supported)\r\n* Updates to the shapefile readers and improved documentation across the project.\r\n\r\nSee the below list for a detail of all PRs included in this release.\r\n\r\n## What's Changed\r\n* REL: Include sdist in PyPI release process by @greglucas in https://github.com/SciTools/cartopy/pull/2229\r\n* Bump pypa/gh-action-pypi-publish from 1.8.8 to 1.8.9 by @dependabot in https://github.com/SciTools/cartopy/pull/2234\r\n* Bump pypa/gh-action-pypi-publish from 1.8.9 to 1.8.10 by @dependabot in https://github.com/SciTools/cartopy/pull/2235\r\n* Bump pypa/cibuildwheel from 2.14.1 to 2.15.0 by @dependabot in https://github.com/SciTools/cartopy/pull/2232\r\n* Pass options of stock_img to imshow by @smartlixx in https://github.com/SciTools/cartopy/pull/2230\r\n* Consolidate mpl version checks by @rcomer in https://github.com/SciTools/cartopy/pull/2238\r\n* Add documentation, kwargs for Reader by @lgolston in https://github.com/SciTools/cartopy/pull/2236\r\n* Bump actions/checkout from 3 to 4 by @dependabot in https://github.com/SciTools/cartopy/pull/2241\r\n* Bump pypa/cibuildwheel from 2.15.0 to 2.16.0 by @dependabot in https://github.com/SciTools/cartopy/pull/2244\r\n* Make GridLiner into an Artist by @rcomer in https://github.com/SciTools/cartopy/pull/2249\r\n* Bump minimum Matplotlib version to 3.5 by @rcomer in https://github.com/SciTools/cartopy/pull/2258\r\n* Gridliner: don't destroy and recreate artists by @rcomer in https://github.com/SciTools/cartopy/pull/2252\r\n* Fix gridliner when datalim updated by constrained layout by @rcomer in https://github.com/SciTools/cartopy/pull/2254\r\n* STY: increase max line length to 88 by @rcomer in https://github.com/SciTools/cartopy/pull/2261\r\n* [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/SciTools/cartopy/pull/2255\r\n* MNT: Update gitignore items by @greglucas in https://github.com/SciTools/cartopy/pull/2267\r\n* Add xfails to current owslib failures by @greglucas in https://github.com/SciTools/cartopy/pull/2264\r\n* DOC: Change Stamen image tiles over to Google by @greglucas in https://github.com/SciTools/cartopy/pull/2265\r\n* Bump pypa/cibuildwheel from 2.16.0 to 2.16.2 by @dependabot in https://github.com/SciTools/cartopy/pull/2257\r\n* MNT: Add Python 3.12 to the tests and enable release testing by @greglucas in https://github.com/SciTools/cartopy/pull/2272\r\n* FIX: Handle invalid geometries for rectangular projections by @greglucas in https://github.com/SciTools/cartopy/pull/2262\r\n* Update Matplotlib min version in pyproject.toml by @rcomer in https://github.com/SciTools/cartopy/pull/2276\r\n* ENH: Add Stadia Maps image tile server class by @greglucas in https://github.com/SciTools/cartopy/pull/2269\r\n* MNT: Remove beautifulsoup4 from docs requirements by @greglucas in https://github.com/SciTools/cartopy/pull/2274\r\n* Matplotlib 3.8 test warning by @lgolston in https://github.com/SciTools/cartopy/pull/2277\r\n* MNT: remove deprecated Gridliner properties by @rcomer in https://github.com/SciTools/cartopy/pull/2280\r\n* Revert ""MNT: xfail owslib tests and documentation that use NASA"" by @greglucas in https://github.com/SciTools/cartopy/pull/2281\r\n* MNT: Use ruff for formatting checks by @greglucas in https://github.com/SciTools/cartopy/pull/2273\r\n* DOC: fix LAND zorder in feature example by @rcomer in https://github.com/SciTools/cartopy/pull/2287\r\n* Bump pyshp min version to v2.3 by @rcomer in https://github.com/SciTools/cartopy/pull/2290\r\n* Bump pypa/gh-action-pypi-publish from 1.8.10 to 1.8.11 by @dependabot in https://github.com/SciTools/cartopy/pull/2294\r\n* FIX: Handle non-earth bodies within Projections by @greglucas in https://github.com/SciTools/cartopy/pull/2283\r\n* Bump actions/setup-python from 4 to 5 by @dependabot in https://github.com/SciTools/cartopy/pull/2299\r\n* Bump actions/download-artifact from 3 to 4 by @dependabot in https://github.com/SciTools/cartopy/pull/2302\r\n* Bump actions/upload-artifact from 3 to 4 by @dependabot in https://github.com/SciTools/cartopy/pull/2303\r\n* FIX: Remove force_path_ccw of contours by @greglucas in https://github.com/SciTools/cartopy/pull/2298\r\n* [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/SciTools/cartopy/pull/2309\r\n* TST: Skip scipy tests if scipy isn't installed by @greglucas in https://github.com/SciTools/cartopy/pull/2282\r\n* MNT: Move cartopy feature download script into the package by @greglucas in https://github.com/SciTools/cartopy/pull/2263\r\n* Allow GSHHS levels 5 and 6 for Antarctica. by @kdungs in https://github.com/SciTools/cartopy/pull/2317\r\n* Relicense from LGPL-3 to BSD-3 by @lbdreyer in https://github.com/SciTools/cartopy/pull/2285\r\n* DOC: don't use deprecated collections attribute by @rcomer in https://github.com/SciTools/cartopy/pull/2324\r\n* MNT: xfail owslib tests and documentation that use NASA (again) by @rcomer in https://github.com/SciTools/cartopy/pull/2327\r\n* FIX: use compound paths to draw transformed geometries by @rcomer in https://github.com/SciTools/cartopy/pull/2325\r\n* Preparations for the new v5 SciTools CLA by @trexfeathers in https://github.com/SciTools/cartopy/pull/2306\r\n* MNT: add sg_execution_times.rst to .gitignore by @rcomer in https://github.com/SciTools/cartopy/pull/2331\r\n* Make `FeatureArtist` a `Collection` by @rcomer in https://github.com/SciTools/cartopy/pull/2323\r\n* MNT: Remove wmts-time from expected doc failures by @greglucas in https://github.com/SciTools/cartopy/pull/2332\r\n* FIX: gridliner auto-updates by default (and always in future) by @rcomer in https://github.com/SciTools/cartopy/pull/2330\r\n* Bump pypa/gh-action-pypi-publish from 1.8.11 to 1.8.12 by @dependabot in https://github.com/SciTools/cartopy/pull/2334\r\n* MNT: remove FeatureArtist.draw args and kwargs by @rcomer in https://github.com/SciTools/cartopy/pull/2333\r\n* DOC: add example showing FeatureArtist is a ScalarMappable by @rcomer in https://github.com/SciTools/cartopy/pull/2340\r\n* MNT: deprecate the style module by @rcomer in https://github.com/SciTools/cartopy/pull/2337\r\n* FIX: Add xfail for bad test url by @greglucas in https://github.com/SciTools/cartopy/pull/2344\r\n* Improve matrixing in GHA to work with SciTools/cartopy#2303 by @trexfeathers in https://github.com/SciTools/cartopy/pull/2313\r\n* CI: skip xfail output by @rcomer in https://github.com/SciTools/cartopy/pull/2345\r\n* Bump pypa/gh-action-pypi-publish from 1.8.12 to 1.8.14 by @dependabot in https://github.com/SciTools/cartopy/pull/2343\r\n* Prepare for Numpy 2.0 by @greglucas in https://github.com/SciTools/cartopy/pull/2338\r\n* CI: Only run coverage on a single runner by @greglucas in https://github.com/SciTools/cartopy/pull/2347\r\n* DOC: Address several typos by @lgolston in https://github.com/SciTools/cartopy/pull/2348\r\n* DOC: Skip failing WMTS time example again due to flaky servers by @greglucas in https://github.com/SciTools/cartopy/pull/2349\r\n* Bump pypa/cibuildwheel from 2.16.2 to 2.16.5 by @dependabot in https://github.com/SciTools/cartopy/pull/2322\r\n* Bump pypa/cibuildwheel from 2.16.5 to 2.17.0 by @dependabot in https://github.com/SciTools/cartopy/pull/2350\r\n* CI: Ruff linter settings warning by @lgolston in https://github.com/SciTools/cartopy/pull/2352\r\n* DOC: Update several links by @lgolston in https://github.com/SciTools/cartopy/pull/2353\r\n* MNT: xfail deprecated Google Image Tile API by @greglucas in https://github.com/SciTools/cartopy/pull/2365\r\n* DOC: Show module docstring by @lgolston in https://github.com/SciTools/cartopy/pull/2354\r\n* FIX: Deepcopy of CRS with str/epsg by @lgolston in https://github.com/SciTools/cartopy/pull/2356\r\n* [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/SciTools/cartopy/pull/2363\r\n* BLD: Use numpy2.0.0rc1 to build the package by @greglucas in https://github.com/SciTools/cartopy/pull/2366\r\n* DOC: Add whatsnew entry for v0.23 by @greglucas in https://github.com/SciTools/cartopy/pull/2346\r\n\r\n## New Contributors\r\n* @kdungs made their first contribution in https://github.com/SciTools/cartopy/pull/2317\r\n\r\n**Full Changelog**: https://github.com/SciTools/cartopy/compare/v0.22.0...v0.23.0",v0.23.0,Greg Lucas,,greglucas,"BSD 3-Clause ""New"" or ""Revised"" License",cartopy,SciTools,25,cartopy,matplotlib,python,geometry,maps,spatial,projections,,,,,,,,,,,,,,/SciTools/cartopy,41,54,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/scipipe/scipipe,https://github.com/scipipe/scipipe,0.5,Tool for scientific pipelines?,1,1,1,1,1,0,0,0,0,0,0,1,"Robust, flexible and resource-efficient pipelines using Go and the commandline","<h1 style=""margin-bottom: 0;""><img src=""docs/images/scipipe_logo_bluegrey_horiz_320px.png"" alt=""SciPipe""></h1>\n\n<big>Robust, flexible and resource-efficient pipelines using Go and the commandline</big>\n\n[![Build Status](https://img.shields.io/circleci/project/github/scipipe/scipipe.svg)](https://app.circleci.com/pipelines/github/scipipe)\n[![Test Coverage](https://img.shields.io/codecov/c/github/scipipe/scipipe.svg)](https://codecov.io/gh/scipipe/scipipe)\n[![Codebeat Grade](https://codebeat.co/badges/96e93624-2ac8-42c9-9e94-2d6e5325d8ff)](https://codebeat.co/projects/github-com-scipipe-scipipe-master)\n[![Go Report Card](https://goreportcard.com/badge/github.com/scipipe/scipipe)](https://goreportcard.com/report/github.com/scipipe/scipipe)\n[![GoDoc](https://godoc.org/github.com/scipipe/scipipe?status.svg)](https://godoc.org/github.com/scipipe/scipipe)\n[![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/scipipe/scipipe)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1157941.svg)](https://doi.org/10.5281/zenodo.1157941)\n\n<strong>Project links: [Documentation & Main Website](http://scipipe.org) | [Issue Tracker](https://github.com/scipipe/scipipe/issues) | [Chat](https://gitter.im/scipipe/scipipe)</strong>\n\n## Why SciPipe?\n\n- **Intuitive:** SciPipe works by flowing data through a network of channels\n  and processes\n- **Flexible:** Wrapped command-line programs can be combined with processes in\n  Go\n- **Convenient:** Full control over how your files are named\n- **Efficient:** Workflows are compiled to binary code that run fast\n- **Parallel:** Pipeline paralellism between processes as well as task\n  parallelism for multiple inputs, making efficient use of multiple CPU cores\n- **Supports streaming:** Stream data between programs to avoid wasting disk space\n- **Easy to debug:** Use available Go debugging tools or just `println()`\n- **Portable:** Distribute workflows as Go code or as self-contained executable\n  files\n\n\n## Project updates\n\n- <strong>Jan 2020: New screencast:</strong> <a href=""https://www.youtube.com/watch?v=hi0Uqwddrtg"" target=""_blank"">""Hello World"" scientific workflow in SciPipe</a>\n- <strong>May 2019: The SciPipe paper published open access in GigaScience:</strong> <a href=""https://doi.org/10.1093/gigascience/giz044"" target=""_blank"">SciPipe: A workflow library for agile development of complex and dynamic bioinformatics pipelines</a>\n- <strong>Nov 2018: Scientific study using SciPipe:</strong> <a href=""https://doi.org/10.3389/fphar.2018.01256"" target=""_blank"">Predicting off-target binding profiles with confidence using Conformal Prediction</a>\n- <strong>Slides:</strong> <a href=""https://pharmb.io/blog/saml-gostockholm2018/"">Presentation on SciPipe and more at Go Stockholm Conference</a>\n- <strong>Blog post:</strong> <a href=""http://bionics.it/posts/provenance-reports-in-scientific-workflows"">Provenance reports in Scientific Workflows</a> - going into details about how SciPipe is addressing provenance.\n- <strong>Blog post:</strong> <a href=""http://bionics.it/posts/first-production-workflow-run-with-scipipe"">First production workflow run with SciPipe</a\n\n## Introduction\n\n<img src=""docs/images/fbp_factory.png"" align=""right"">\n\nSciPipe is a library for writing [Scientific\nWorkflows](https://en.wikipedia.org/wiki/Scientific_workflow_system), sometimes\nalso called ""pipelines"", in the [Go programming language](http://golang.org).\n\nWhen you need to run many commandline programs that depend on each other in\ncomplex ways, SciPipe helps by making the process of running these programs\nflexible, robust and reproducible. SciPipe also lets you restart an interrupted\nrun without over-writing already produced output and produces an audit report\nof what was run, among many other things.\n\nSciPipe is built on the proven principles of [Flow-Based Programming](https://en.wikipedia.org/wiki/Flow-based_programming)\n(FBP) to achieve maximum flexibility, productivity and agility when designing\nworkflows.  Compared to plain dataflow, FBP provides the benefits that\nprocesses are fully self-contained, so that a library of re-usable components\ncan be created, and plugged into new workflows ad-hoc.\n\nSimilar to other FBP systems, SciPipe workflows can be likened to a network of\nassembly lines in a factory, where items (files) are flowing through a network\nof conveyor belts, stopping at different independently running stations\n(processes) for processing, as depicted in the picture above.\n\nSciPipe was initially created for problems in bioinformatics and\ncheminformatics, but works equally well for any problem involving pipelines of\ncommandline applications.\n\n**Project status:** SciPipe pretty stable now, and only very minor API changes\nmight still occur. We have successfully used SciPipe in a handful of both real\nand experimental projects, and it has had occasional use outside the research\ngroup as well.\n\n## Known limitations\n\n- There are still a number of missing good-to-have features for workflow\n  design. See the [issue tracker](https://github.com/scipipe/scipipe/issues)\n  for details.\n- There is not (yet) support for the [Common Workflow Language](http://common-workflow-language.github.io).\n\n## Installing\n\nFor full installation instructions, see the [intallation page](https://scipipe.org/install/).\nFor quick getting started steps, you can do:\n\n1. [Download](https://golang.org/dl/) and [install](https://golang.org/doc/install) Go\n2. Run the following command, to install the scipipe Go library (don't miss the\n   trailing dots!), and create a Go module for your script:\n\n```bash\ngo install github.com/scipipe/scipipe/...@latest\ngo mod init myfirstworkflow-module\n```\n\n## Hello World example\n\nLet's look at an example workflow to get a feel for what writing workflows in\nSciPipe looks like:\n\n```go\npackage main\n\nimport (\n    // Import SciPipe, aliased to sp\n    sp ""github.com/scipipe/scipipe""\n)\n\nfunc main() {\n    // Init workflow and max concurrent tasks\n    wf := sp.NewWorkflow(""hello_world"", 4)\n\n    // Initialize processes, and file extensions\n    hello := wf.NewProc(""hello"", ""echo 'Hello ' > {o:out|.txt}"")\n    world := wf.NewProc(""world"", ""echo $(cat {i:in}) World > {o:out|.txt}"")\n\n    // Define data flow\n    world.In(""in"").From(hello.Out(""out""))\n\n    // Run workflow\n    wf.Run()\n}\n```\n\nTo create a file with a similar simple example, you can run:\n\n```\nscipipe new hello_world.go\n```\n\n## Running the example\n\nLet's put the code in a file named `hello_world.go` and run it.\n\nFirst you need to make sure that the dependencies (SciPipe in this case) is\ninstalled in your local Go module. This you can do with:\n\n```bash\ngo mod tidy\n```\n\nThen you can go ahead and run the workflow:\n\n```bash\n$ go run hello_world.go\nAUDIT   2018/07/17 21:42:26 | workflow:hello_world             | Starting workflow (Writing log to log/scipipe-20180717-214226-hello_world.log)\nAUDIT   2018/07/17 21:42:26 | hello                            | Executing: echo 'Hello ' > hello.out.txt\nAUDIT   2018/07/17 21:42:26 | hello                            | Finished: echo 'Hello ' > hello.out.txt\nAUDIT   2018/07/17 21:42:26 | world                            | Executing: echo $(cat ../hello.out.txt) World > hello.out.txt.world.out.txt\nAUDIT   2018/07/17 21:42:26 | world                            | Finished: echo $(cat ../hello.out.txt) World > hello.out.txt.world.out.txt\nAUDIT   2018/07/17 21:42:26 | workflow:hello_world             | Finished workflow (Log written to log/scipipe-20180717-214226-hello_world.log)\n```\n\nLet's check what file SciPipe has generated:\n\n```\n$ ls -1 hello*\nhello.out.txt\nhello.out.txt.audit.json\nhello.out.txt.world.out.txt\nhello.out.txt.world.out.txt.audit.json\n```\n\nAs you can see, it has created a file `hello.out.txt`, and `hello.out.world.out.txt`, and\nan accompanying `.audit.json` for each of these files.\n\nNow, let's check the output of the final resulting file:\n\n```bash\n$ cat hello.out.txt.world.out.txt\nHello World\n```\n\nNow we can rejoice that it contains the text ""Hello World"", exactly as a proper\nHello World example should :)\n\nNow, these were a little long and cumbersome filenames, weren't they? SciPipe\ngives you very good control over how to name your files, if you don't want to\nrely on the automatic file naming. For example, we could set the first filename\nto a static one, and then use the first name as a basis for the file name for\nthe second process, like so:\n\n```go\npackage main\n\nimport (\n    // Import the SciPipe package, aliased to 'sp'\n    sp ""github.com/scipipe/scipipe""\n)\n\nfunc main() {\n    // Init workflow with a name, and max concurrent tasks\n    wf := sp.NewWorkflow(""hello_world"", 4)\n\n    // Initialize processes and set output file paths\n    hello := wf.NewProc(""hello"", ""echo 'Hello ' > {o:out}"")\n    hello.SetOut(""out"", ""hello.txt"")\n\n    world := wf.NewProc(""world"", ""echo $(cat {i:in}) World >> {o:out}"")\n    world.SetOut(""out"", ""{i:in|%.txt}_world.txt"")\n\n    // Connect network\n    world.In(""in"").From(hello.Out(""out""))\n\n    // Run workflow\n    wf.Run()\n}\n```\n\nIn the `{i:in...` part, we are re-using the file path from the file received on\nthe in-port named 'in', and then running a Bash-style trim-from-end command on\nit to remove the `.txt` extension.\n\nNow, if we run this, the file names get a little cleaner:\n\n```bash\n$ ls -1 hello*\nhello.txt\nhello.txt.audit.json\nhello_world.go\nhello_world.txt\nhello_world.txt.audit.json\n```\n\n## The audit logs\n\nFinally, we could have a look at one of those audit file created:\n\n```bash\n$ cat hello_world.txt.audit.json\n{\n    ""ID"": ""99i5vxhtd41pmaewc8pr"",\n    ""ProcessName"": ""world"",\n    ""Command"": ""echo $(cat hello.txt) World \u003e\u003e hello_world.txt.tmp/hello_world.txt"",\n    ""Params"": {},\n    ""Tags"": {},\n    ""StartTime"": ""2018-06-15T19:10:37.955602979+02:00"",\n    ""FinishTime"": ""2018-06-15T19:10:37.959410102+02:00"",\n    ""ExecTimeNS"": 3000000,\n    ""Upstream"": {\n        ""hello.txt"": {\n            ""ID"": ""w4oeiii9h5j7sckq7aqq"",\n            ""ProcessName"": ""hello"",\n            ""Command"": ""echo 'Hello ' \u003e hello.txt.tmp/hello.txt"",\n            ""Params"": {},\n            ""Tags"": {},\n            ""StartTime"": ""2018-06-15T19:10:37.950032676+02:00"",\n            ""FinishTime"": ""2018-06-15T19:10:37.95468214+02:00"",\n            ""ExecTimeNS"": 4000000,\n            ""Upstream"": {}\n        }\n    }\n```\n\nEach such audit-file contains a hierarchic JSON-representation of the full\nworkflow path that was executed in order to produce this file. On the first\nlevel is the command that directly produced the corresponding file, and then,\nindexed by their filenames, under ""Upstream"", there is a similar chunk\ndescribing how all of its input files were generated. This process will be\nrepeated in a recursive way for large workflows, so that, for each file\ngenerated by the workflow, there is always a full, hierarchic, history of all\nthe commands run - with their associated metadata - to produce that file.\n\nYou can find many more examples in the [examples folder](https://github.com/scipipe/scipipe/tree/master/examples) in the GitHub repo.\n\nFor more information about how to write workflows using SciPipe, and much more,\nsee [SciPipe website (scipipe.org)](http://scipipe.org)!\n\n## More material on SciPipe\n\n- See [a poster on SciPipe](http://dx.doi.org/10.13140/RG.2.2.34414.61760), presented at the [e-Science Academy in Lund, on Oct 12-13 2016](essenceofescience.se/event/swedish-e-science-academy-2016-2/).\n- See [slides from a recent presentation of SciPipe for use in a Bioinformatics setting](http://www.slideshare.net/SamuelLampa/scipipe-a-lightweight-workflow-library-inspired-by-flowbased-programming).\n- The architecture of SciPipe is based on an [flow-based programming](https://en.wikipedia.org/wiki/Flow-based_programming) like\n  pattern in pure Go presented in\n  [this](http://blog.gopheracademy.com/composable-pipelines-pattern) and\n  [this](https://blog.gopheracademy.com/advent-2015/composable-pipelines-improvements/)\n  blog posts on Gopher Academy.\n\n## Citing SciPipe\n\nIf you use SciPipe in academic or scholarly work, please cite the following paper as source:\n\nLampa S, Dahlö M, Alvarsson J, Spjuth O. SciPipe: A workflow library for agile development of complex and dynamic bioinformatics pipelines \n_Gigascience_. 8, 5 (2019). DOI: [10.1093/gigascience/giz044](https://dx.doi.org/10.1093/gigascience/giz044)\n\n## Acknowledgements\n\n- SciPipe is very heavily dependent on the proven principles form [Flow-Based\n  Programming (FBP)](http://www.jpaulmorrison.com/fbp), as invented by [John Paul Morrison](http://www.jpaulmorrison.com/fbp).\n  From Flow-based programming, SciPipe uses the ideas of separate network\n  (workflow dependency graph) definition, named in- and out-ports,\n  sub-networks/sub-workflows and bounded buffers (already available in Go's\n  channels) to make writing workflows as easy as possible.\n- This library is has been much influenced/inspired also by the\n  [GoFlow](https://github.com/trustmaster/goflow) library by [Vladimir Sibirov](https://github.com/trustmaster/goflow).\n- Thanks to [Egon Elbre](http://twitter.com/egonelbre) for helpful input on the\n  design of the internals of the pipeline, and processes, which greatly\n  simplified the implementation.\n- This work is financed by faculty grants and other financing for the [Pharmaceutical Bioinformatics group](http://pharmb.io) of [Dept. of\n  Pharmaceutical Biosciences](http://www.farmbio.uu.se) at [Uppsala University](http://www.uu.se), and by [Swedish Research Council](http://vr.se)\n  through the Swedish [National Bioinformatics Infrastructure Sweden](http://nbis.se).\n- Supervisor for the project is [Ola Spjuth](http://www.farmbio.uu.se/research/researchgroups/pb/olaspjuth).\n\n## Related tools\n\nFind below a few tools that are more or less similar to SciPipe that are worth worth checking out before\ndeciding on what tool fits you best (in approximate order of similarity to SciPipe):\n\n- [NextFlow](http://nextflow.io)\n- [Luigi](https://github.com/spotify/luigi)/[SciLuigi](https://github.com/samuell/sciluigi)\n- [BPipe](https://code.google.com/p/bpipe/)\n- [SnakeMake](https://bitbucket.org/johanneskoester/snakemake)\n- [Cuneiform](https://github.com/joergen7/cuneiform)\n",1060,scientific-workflows,Go,2,Go,Shell,,,,,,,,,,,,,,,,,,,,,,,,,,,25,2,21,2,13,10,0,1105,71,133,81,52,4d29035e8538d7d5362227eff037a1dd22d1e186,Bump Go version in CI to 1.18,2024-07-04T16:46:24Z,Samuel Lampa,samuel.lampa@scilifelab.se,samuell,v0.12.0: Important bugfix. Minor API change.,"This release contains an important bugfix, making sure that we properly handle all errors (#146).\r\n\r\nIt also contains a minor API change, that could affect custom components making use of the `AtomizeIPs()` method, which is now renamed to `FinalizePaths()`.",v0.12.0,Samuel Lampa,,samuell,MIT License,scipipe,scipipe,44,scientific-workflows,bioinformatics-pipeline,workflow,go,dataflow,pipeline,cheminformatics,bioinformatics,fbp,scipipe,workflow-engine,golang,,,,,,,,,/scipipe/scipipe,44,38,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/SciML/PolyChaos.jl,https://github.com/SciML/PolyChaos.jl,0,,,0,0,0,0,0,0,1,0,0,0,0,"A Julia package to construct orthogonal polynomials, their quadrature rules, and use it with polynomial chaos expansions.","# PolyChaos -- Orthogonal Polynomials, Quadrature, and Polynomial Chaos\n\n[![Join the chat at https://julialang.zulipchat.com #sciml-bridged](https://img.shields.io/static/v1?label=Zulip&message=chat&color=9558b2&labelColor=389826)](https://julialang.zulipchat.com/#narrow/stream/279055-sciml-bridged)\n[![Global Docs](https://img.shields.io/badge/docs-SciML-blue.svg)](https://docs.sciml.ai/PolyChaos/stable/)\n\n[![codecov](https://codecov.io/gh/SciML/PolyChaos.jl/branch/master/graph/badge.svg)](https://codecov.io/gh/SciML/PolyChaos.jl)\n[![Build Status](https://github.com/SciML/PolyChaos.jl/workflows/CI/badge.svg)](https://github.com/SciML/PolyChaos.jl/actions?query=workflow%3ACI)\n\n[![ColPrac: Contributor's Guide on Collaborative Practices for Community Packages](https://img.shields.io/badge/ColPrac-Contributor%27s%20Guide-blueviolet)](https://github.com/SciML/ColPrac)\n[![SciML Code Style](https://img.shields.io/static/v1?label=code%20style&message=SciML&color=9558b2&labelColor=389826)](https://github.com/SciML/SciMLStyle)\n\n[![Code DOI](https://zenodo.org/badge/165908711.svg)](https://zenodo.org/badge/latestdoi/165908711)\n[![Paper@arXiv](https://img.shields.io/badge/arXiv-2004.03970-green.svg)](https://arxiv.org/abs/2004.03970)\n\nA Julia package to construct orthogonal polynomials, their quadrature rules, and use it with polynomial chaos expansions.\n\n## Tutorials and Documentation\n\nFor information on using the package,\n[see the stable documentation](https://docs.sciml.ai/PolyChaos/stable/). Use the\n[in-development documentation](https://docs.sciml.ai/PolyChaos/dev/) for the version of\nthe documentation, which contains the unreleased features.\n\nThe package requires `Julia 1.3` or newer.\nIn `Julia` switch to the package manager\n\n```julia\nusing Pkg\nPkg.add(""PolyChaos"")\n```\n\nThis will install PolyChaos and its dependencies.\nOnce that is done, load the package:\n\n```julia\nusing PolyChaos\n```\n\nThat's it.\n\nLet's take a look at a simple example.\nWe would like to solve the integral\n\n![equation](https://latex.codecogs.com/gif.latex?%5Cint_0%5E1%206%20x%5E5%20%5Cmathrm%7Bd%7Dx.)\n\nExploiting the underlying uniform measure, the integration can be done exactly with a 3-point quadrature rule.\n\n```@example mysetup\nopq = Uniform01OrthoPoly(3)\nintegrate(x -> 6x^5, opq)\n```\n\nFor more information please visit the [documentation](https://docs.sciml.ai/PolyChaos/stable).\n\n## Citing\n\nIf you like `PolyChaos.jl`, consider citing our paper\n\n```\n@ARTICLE{2020arXiv200403970M,\n       author = {{M{\""u}hlpfordt}, Tillmann and {Zahn}, Frederik and {Hagenmeyer}, Veit and {Faulwasser}, Timm},\n        title = ""{PolyChaos.jl -- A Julia Package for Polynomial Chaos in Systems and Control}"",\n      journal = {arXiv e-prints},\n     keywords = {Electrical Engineering and Systems Science - Systems and Control, Mathematics - Numerical Analysis, Mathematics - Optimization and Control},\n         year = 2020,\n        month = apr,\n          eid = {arXiv:2004.03970},\n        pages = {arXiv:2004.03970},\narchivePrefix = {arXiv},\n       eprint = {2004.03970},\n primaryClass = {eess.SY},\n}\n```\n",115,uncertainty-quantification,Julia,2,Julia,MATLAB,,,,,,,,,,,,,,,,,,,,,,,,,,,98,17,76,5,15,49,125,4331,25,34,19,15,412319a4ce70239a979c5ee47b0e92b83205643e,Merge pull request #133 from SciML/at/use-reusable-workflows,2024-05-03T10:20:12Z,Anant Thazhemadam,47104651+thazhemadam@users.noreply.github.com,thazhemadam,v0.2.11,## PolyChaos v0.2.11\n\n[Diff since v0.2.10](https://github.com/SciML/PolyChaos.jl/compare/v0.2.10...v0.2.11)\n\n\n\n**Merged pull requests:**\n- [skip ci] add favicon (#102) (@ArnoStrouwen)\n- format markdown (#103) (@ArnoStrouwen)\n- various doc and style improvements (#104) (@ArnoStrouwen),v0.2.11,,,github-actions[bot],MIT License,PolyChaos.jl,SciML,16,polynomial-chaos-expansions,orthogonal-polynomials,quadrature-rules,quadrature,uncertainty-quantification,uncertainty-propagation,julia-language,quadrature-integration,polynomials,differential-equations,julia,scientific-machine-learning,,,,,,,,,/SciML/PolyChaos.jl,16,14,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/samtools/htslib,https://github.com/samtools/htslib,0,,,0,0,0,0,0,0,1,0,0,0,0,C library for high-throughput sequencing data formats,"[![Build Status](https://api.cirrus-ci.com/github/samtools/htslib.svg?branch=develop)](https://api.cirrus-ci.com/github/samtools/htslib)\n[![Build status](https://github.com/samtools/htslib/actions/workflows/windows-build.yml/badge.svg)](https://github.com/samtools/htslib/actions/workflows/windows-build.yml?query=branch%3Adevelop)\n[![Github All Releases](https://img.shields.io/github/downloads/samtools/htslib/total.svg)](https://github.com/samtools/htslib)\n\nHTSlib is an implementation of a unified C library for accessing common file\nformats, such as [SAM, CRAM and VCF][1], used for high-throughput sequencing\ndata, and is the core library used by [samtools][2] and [bcftools][3].\nHTSlib only depends on [zlib][4].\nIt is known to be compatible with gcc, g++ and clang.\n\nHTSlib implements a generalized BAM index, with file extension `.csi`\n(coordinate-sorted index). The HTSlib file reader first looks for the new index\nand then for the old if the new index is absent.\n\nThis project also includes the popular tabix indexer, which creates both `.tbi`\nand `.csi` formats, and the bgzip compression utility.\n\n[1]: http://samtools.github.io/hts-specs/\n[2]: http://github.com/samtools/samtools\n[3]: http://samtools.github.io/bcftools/\n[4]: http://zlib.net/\n\n### Building HTSlib\n\nSee [INSTALL](INSTALL) for complete details.\n[Release tarballs][download] contain generated files that have not been\ncommitted to this repository, so building the code from a Git repository\nrequires extra steps:\n\n```sh\nautoreconf -i  # Build the configure script and install files it uses\n./configure    # Optional but recommended, for choosing extra functionality\nmake\nmake install\n```\n\n[download]: http://www.htslib.org/download/\n\n### Citing\n\nPlease cite this paper when using HTSlib for your publications.\n\n> HTSlib: C library for reading/writing high-throughput sequencing data </br>\n> James K Bonfield, John Marshall, Petr Danecek, Heng Li, Valeriu Ohan, Andrew Whitwham, Thomas Keane, Robert M Davies </br>\n> _GigaScience_, Volume 10, Issue 2, February 2021, giab007, https://doi.org/10.1093/gigascience/giab007\n\n```\n@article{10.1093/gigascience/giab007,\n    author = {Bonfield, James K and Marshall, John and Danecek, Petr and Li, Heng and Ohan, Valeriu and Whitwham, Andrew and Keane, Thomas and Davies, Robert M},\n    title = ""{HTSlib: C library for reading/writing high-throughput sequencing data}"",\n    journal = {GigaScience},\n    volume = {10},\n    number = {2},\n    year = {2021},\n    month = {02},\n    abstract = ""{Since the original publication of the VCF and SAM formats, an explosion of software tools have been created to process these data files. To facilitate this a library was produced out of the original SAMtools implementation, with a focus on performance and robustness. The file formats themselves have become international standards under the jurisdiction of the Global Alliance for Genomics and Health.We present a software library for providing programmatic access to sequencing alignment and variant formats. It was born out of the widely used SAMtools and BCFtools applications. Considerable improvements have been made to the original code plus many new features including newer access protocols, the addition of the CRAM file format, better indexing and iterators, and better use of threading.Since the original Samtools release, performance has been considerably improved, with a BAM read-write loop running 5 times faster and BAM to SAM conversion 13 times faster (both using 16 threads, compared to Samtools 0.1.19). Widespread adoption has seen HTSlib downloaded \\&gt;1 million times from GitHub and conda. The C library has been used directly by an estimated 900 GitHub projects and has been incorporated into Perl, Python, Rust, and R, significantly expanding the number of uses via other languages. HTSlib is open source and is freely available from htslib.org under MIT/BSD license.}"",\n    issn = {2047-217X},\n    doi = {10.1093/gigascience/giab007},\n    url = {https://doi.org/10.1093/gigascience/giab007},\n    note = {giab007},\n    eprint = {https://academic.oup.com/gigascience/article-pdf/10/2/giab007/36332285/giab007.pdf},\n}\n```\n",787,bioinformatics,C,8,C,Makefile,Perl,Shell,M4,Roff,Scilab,C++,,,,,,,,,,,,,,,,,,,,,1101,284,794,23,15,87,0,12966,447,700,567,133,fbe5ff6c52544ee776a5d2ec0176c61ab103cce5,Fix an undefined addition to a NULL pointer in vcf_format.,2024-07-18T14:12:59Z,James Bonfield,jkb@sanger.ac.uk,jkbonfield,1.2,"_Download the source code here: [htslib-1.20.tar.bz2](https://github.com/samtools/htslib/releases/download/1.20/htslib-1.20.tar.bz2).(The ""Source code"" downloads are generated by GitHub and are incomplete as they are missing some generated files.)_\r\n---\r\n\r\nUpdates\r\n-------\r\n\r\n* When working on named files, bgzip now sets the modified and access times of the output files it makes to match those of the corresponding input. (PR #1727, feature request #1718.  Requested by Gert Hulselmans)\r\n\r\n* It's now possible to use a `-o` option to specify the output file name in bgzip. (PR #1747, feature request #1726.  Requested by Gert Hulselmans)\r\n\r\n* Improved error faidx error messages. (PR #1743, thanks to Nick Moore)\r\n\r\n* Faster reading of SAM array (type ""B"") tags.  These often turn up in ONT and PacBio data. (PR #1741)\r\n\r\n* Improved validity checking of base modification tags. (PR #1749)\r\n\r\n* `mpileup` overlap removal now works where one read has a deletion. (PR #1751, fixes samtools/samtools#1992.  Reported by Long Tian)\r\n\r\n* The S3 plugin can now find buckets via S3 access point aliases. (PR #1756, thanks to Matt Pawelczyk; fixes samtools/samtools#1984.  Reported by Albert Li)\r\n\r\n* Added a `--threads` option (and `-@` short option) to tabix. (PR #1755, feature request #1735.  Requested by Dan Bolser)\r\n\r\n* tabix can now index Graph Alignment Format (GAF) files. (See https://github.com/lh3/gfatools/blob/master/doc/rGFA.md) (PR #1763, thanks to Adam Novak)\r\n\r\nBug fixes\r\n---------\r\n\r\n* Security fix: Prevent possible heap overflow in `cram_encode_aux()` on bad `RG:Z` tags. (PR #1737)\r\n\r\n* Security fix: Prevent attempts to call a NULL pointer if certain URL schemes are used in CRAM `@SQ` `UR:` tags. (PR #1757)\r\n\r\n* Security fix: Fixed a bug where following certain AWS S3 redirects could downgrade the connection from TLS (i.e. `https://`) to unencrypted `http://`. This could happen when using path-based URLs and `AWS_DEFAULT_REGION` was set to a region other that the one where the data was stored. (PR #1762, fixes #1760. Reported by andaca)\r\n\r\n* Fixed arithmetic overflow when loading very long references for CRAM. (PR #1738, fixes #1738.  Reported by Shane McCarthy)\r\n\r\n* Fixed faidx and CRAM reference look-ups on compressed	fasta where the `.fai` index file was present, but the `.gzi` index of compressed offsets was not. (PR #1745, fixes #1744.  Reported by Theodore Li)\r\n\r\n* Fixed BCF indexing on-the-fly bug which produced invalid indexes when using multiple compression threads. (PR #1742, fixes #1740.  Reported by graphenn)\r\n\r\n* Ensure that pileup destructors are called by `bam_plp_destroy()`, to prevent memory leaks. (PR #1749, PR #1754)\r\n\r\n* Ensure on-the-fly index timestamps are always older than the data file. Previously the files could be closed out of order, leading to warnings being printed when using the index. (PR #1753, fixes #1732.  Reported by Gert Hulselmans)\r\n\r\n* To prevent data corruption when reading (strictly invalid) VCF files with duplicated `FORMAT` tags, all but the first copy of the data associated with the tag are now dropped with a warning. (PR #1752, PR #1761, fixes #1733.  Reported by anthakki)\r\n\r\n* Fixed a bug introduced in release 1.19 (PR #1689) which broke variant record data if it tried to remove an over-long tag. (PR #1752, PR #1761)\r\n\r\n* Changed error to warning when complaining about use of the `CG` tag in SAM or CRAM files. (PR #1758, fixes samtools/samtools#2002)\r\n",1.2,,,daviesrob,Other,htslib,samtools,26,htslib,bioinformatics,sam,bam,cram,vcf,bcf,ngs,,,,,,,,,,,,,/samtools/htslib,45,64,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/SakuraEngine/SakuraEngine,https://github.com/SakuraEngine/SakuraEngine,0,,,0,0,0,0,0,0,0,0,1,0,0,为高性能而生的游戏运行时与工具箱,"<h1 align=""center"">SakuraEngine</h1>\n\n<a href=""https://olivermak.es/"">\n  <img src=""https://media.githubusercontent.com/media/SakuraEngine/Sakura.Resources/main/logos/skr_icon.svg"" width=""100%"" height=""100%"">\n</a>\n\n<h2 align=""center""> 为下一代平台的功能性能需求而生 </h2>\n\n## 特性\n\n\n### 原生\n\n- 充分考虑易用性并针对硬件优化的实现；\n- 确保强缩放性且面向最先进平台功能特性的设计；\n- 集成大量原生开发需要的 SDK。\n\n### 直白\n\n- 面向过程的实现与设计；\n- **C API**\n\n### 现代\n\n- 基于 ECS 思想，特性丰富且高度正交的[数据驱动编程管线](https://github.com/SakuraEngine/Sakura.Runtime/tree/main/include/ecs)带来最大化的访存效率；\n- 混合 Fibers 和 Thread 的任务调度系统，配合 ECS 的依赖管线，赋予运行时前所未有的多线程任务吞吐量；\n- 完全面向现代 GPU 平台、几无性能开销的超薄跨平台 Graphics API；\n- 清晰的 Render Graph 前端让您可以在不接触同步原语和复杂描述符的情况下完成高度异步的现代 GPU 管线编程，并充分利用 Memory Aliasing 等高级特性；\n- 完全异步、针对 NVMe 驱动以及 GPU 异步拷贝引擎优化的 I/O 服务，轻松享受 Direct Storage 的极限吞吐，打破 SSD 性能桎梏。\n\n<div align=center>\n\nhttps://user-images.githubusercontent.com/39457738/192722537-6ab035a5-2789-43d0-b331-347e3669f3ae.mp4\n\n</div>\n\n## 模块列表\n\n| 模块 | 核心库 | 描述 |\n| --- | :---: | --- |\n| `runtime` | - | 核心模块 |\n| `runtime/cgpu` | - | 核心子模块，面向现代 GPU 平台的跨平台 Graphics API |\n| `runtime/sugoi` | - | 核心子模块，真正面向数据驱动编程的 ECS 对象模型 |\n| `runtime/math` | [RealtimeMath] | 核心子模块, 数学库 |\n| `runtime/io` | - | 核心子模块，同步/异步 I/O 服务 |\n| `runtime/task` | [FiberTaskingLib], [marl] | 核心子模块，基于协程的计算密集任务系统 |\n| `runtime/binary` | - | 核心子模块, 二进制序列化 |\n| `runtime/platform` | - | 核心子模块, 系统接口 |\n| `runtime/resource` | - | 核心子模块, 异步资源管理 |\n| `runtime/type` | - | 核心子模块, 提供 rtti 相关功能 |\n| `runtime/lua` | [lua], [luau] | 核心子模块，使用 lua 作为胶水语言 |\n| `SkrScene` | - | 场景模块，场景资源管理 |\n| `SkrRenderer` | - | 渲染器模块，渲染资源管理，驱动渲染管线 |\n| `SkrImgui` | [imgui] | 用于调试的运行时UI |\n| `SkrRenderGraph` | - | 基于 CGPU 的 Render Graph |\n| `SkrLive2d` | [live2d] | Live2d 的渲染后端 |\n| `SkrInputSystem` | [gaininput] | 输入的上层封装 |\n| `SkrAnim` | [ozz-animation] | 动画资源管理，动画计算的基础功能 |\n| `devtime/tweak` | - | 提供轻量的常量热更方案 |\n| `devtime/inspect` | - | 提供运行时的变量状态可视化 |\n| `SkrToolCore` | - | 管理并驱动资源烘焙流程 |\n| `SkrAnimTool` | [ozz-animation] | 导入并烘焙动画相关资源 |\n| `SkrGLTFTool` | [cgltf] | 导入gltf模型并烘焙 |\n| `SkrTextureCompiler` | [ISPCTextureCompressor] | 导入并烘焙贴图 |\n| `SkrShaderCompiler` | - | 导入并烘焙材质相关资源 |\n| `codegen` | [libtooling], [mako-template] | 提供代码生成 |\n\n[RealtimeMath]: https://github.com/nfrechette/rtm\n[imgui]: https://github.com/ocornut/imgui\n[live2d]: https://www.live2d.com/en/\n[gaininput]: https://github.com/jkuhlmann/gainput\n[ozz-animation]: https://github.com/guillaumeblanc/ozz-animation\n[cgltf]: https://github.com/jkuhlmann/cgltf\n[ISPCTextureCompressor]: https://github.com/GameTechDev/ISPCTextureCompressor\n[libtooling]: https://clang.llvm.org/docs/LibTooling.html\n[mako-template]: https://www.makotemplates.org/\n[FiberTaskingLib]: https://github.com/RichieSams/FiberTaskingLib\n[marl]: https://github.com/google/marl\n[lua]: https://github.com/lua/lua\n[luau]: https://luau-lang.org/\n\n## 组件支持矩阵\n\n### [构建](https://github.com/SakuraEngine/Sakura.Runtime/actions)\n\n| Platform                                                     | CI(Dev)                                                      | \n| ------------------------------------------------------------ | ------------------------------------------------------------ | \n| <img src=""https://media.githubusercontent.com/media/SakuraEngine/Sakura.Resources/main/platform-icons/Windows.png"" height=""20"" /> windows | [![windows-build](https://github.com/SakuraEngine/SakuraEngine/actions/workflows/ci-windows.yml/badge.svg)](https://github.com/SakuraEngine/SakuraEngine/actions/workflows/ci-windows.yml) |\n| <img src=""https://media.githubusercontent.com/media/SakuraEngine/Sakura.Resources/main/platform-icons/MacOS.png"" height=""20"" /> apple-clang | [![macos-build](https://github.com/SakuraEngine/SakuraEngine/actions/workflows/ci-macos.yml/badge.svg)](https://github.com/SakuraEngine/SakuraEngine/actions/workflows/ci-macos.yml) |\n\n### [CGPU](include/cgpu/README.md)\n\n| Platform | <img src=""https://media.githubusercontent.com/media/SakuraEngine/Sakura.Resources/main/platform-icons/DirectX12U.png"" height=""18"" /> D3D12 | <img src=""https://media.githubusercontent.com/media/SakuraEngine/Sakura.Resources/main/platform-icons/DirectX11.png"" height=""18"" />D3D11 | <img src=""https://media.githubusercontent.com/media/SakuraEngine/Sakura.Resources/main/platform-icons/Vulkan.png"" height=""18"" />Vulkan |<img src=""https://media.githubusercontent.com/media/SakuraEngine/Sakura.Resources/main/platform-icons/Metal.png"" height=""18"" />Metal |\n|----------|:-----:|:-----:|:------:|:-----:|\n| <img src=""https://media.githubusercontent.com/media/SakuraEngine/Sakura.Resources/main/platform-icons/Windows.png"" height=""20"" /> Windows |:heavy_check_mark: | :x: | :heavy_check_mark: | N/A |\n| <img src=""https://media.githubusercontent.com/media/SakuraEngine/Sakura.Resources/main/platform-icons/MacOS.png"" height=""20"" /> macOS | N/A | N/A | :heavy_check_mark: | :heavy_exclamation_mark: |\n\n### [ImageCoder](https://github.com/SakuraEngine/Sakura.Runtime/tree/main/modules/image_coder)\n\n| Platform                                                     | PNG             | JPEG                     | BMP                      | ICO                      | EXR                      | TGA                      |\n| ------------------------------------------------------------ | --------------- | ------------------------ | ------------------------ | ------------------------ | ------------------------ | ------------------------ |\n| <img src=""https://media.githubusercontent.com/media/SakuraEngine/Sakura.Resources/main/platform-icons/Windows.png"" height=""20"" /> Windows | libpng(v1.5.2)  | libjpeg-turbo | :heavy_exclamation_mark: | :heavy_exclamation_mark: | :heavy_exclamation_mark: | :heavy_exclamation_mark: |\n| <img src=""https://media.githubusercontent.com/media/SakuraEngine/Sakura.Resources/main/platform-icons/MacOS.png"" height=""20"" /> macOS | libpng(v1.5.27) | libjpeg-turbo | :heavy_exclamation_mark: | :heavy_exclamation_mark: | :heavy_exclamation_mark: | :heavy_exclamation_mark: |\n\n\n\n## 示例 (从上到下逐渐贴近底层)\n### [Multi-Player Server/Game](samples/application/multiplayer)\n\n基于 ECS 的多人游戏以及服务器。\n\n<div align=center>\n\nhttps://user-images.githubusercontent.com/39457738/232537408-1266011b-d722-4b0b-9bd3-5c3af0faf043.mp4\n\n</div>\n\n\n### [Next Generation Graphics](samples/cgpu/cgpu-statebuffer)\n使用 StateBuffer 的次世代 CGPU 图形接口。摒弃 PSO 的概念，使用 StateBuffer 作为图形管线的状态描述。传统的图形管线 API 往往使用 PSO，其中打包了所有的管线状态以及着色器 ISA，并整体上传到 GPU 上：\n\n<div align=center>\n\n![PSO](https://media.githubusercontent.com/media/SakuraEngine/Sakura.Resources/main/showcase/PSO.png)\n\n</div>\n\nStateBuffer 由一系列的 StateChunk 组成，每个 StateChunk 描述了一种图形管线状态，StateBuffer 通过 StateChunk 的组合来描述图形管线的完整状态。比起 PSO 的全量 Flush，StateBuffer 可以在绘制现场准备 StatePacket，在 DrawCall 产生时把状态切换推送到 GPU 的状态寄存器组中。\n\n<div align=center>\n\n![StateBuffer](https://media.githubusercontent.com/media/SakuraEngine/Sakura.Resources/main/showcase/StateStream.png)\n\n</div>\n\nStateBuffer 可以大幅缓解管线和着色器组合爆炸引起的内存膨胀问题，而 PSO 反而会加剧此问题。\n\n### [MVU GUI]()\n\nWIP...\n\n### [GUI Canvas](samples/application/ogui2/robjects)\n\nGUI 的渲染树 (RenderTree) 层，有排版和渲染 Render Object 的功能。支持基本图元、纹理、颜色刷和文本段落等。\n\n![RObjects](https://media.githubusercontent.com/media/SakuraEngine/Sakura.Resources/main/showcase/RObjects.gif)\n\n\n### [Live2D Viewer](samples/application/live2d-viewer)\n\n集成 Cubism Native SDK 且使用 Render Graph 进行 Live2D 模型高效绘制的程序示例。\n\n- Live2D 渲染器的实现摒弃了传统的变体流程，在 Live2D 模型绘制的过程中实现了 0 管线切换；\n- Live2D 渲染器的可动模型顶点信息会使用 CPU Visible VRAM，充分利用 PCIe 带宽进行最高效的顶点上传，并抹消 Copy Engine 在 GPU Timeline 上的时间消耗；\n- Live2D 的全部读取和贴图上传由 I/O 服务驱动，服务后台实现会使用最合适的平台 I/O API 最大化 NVMe 队列深度，提升实际带宽；\n- 在支持 Direct Storage 的 Windows 平台，还会充分利用自定义解压队列进行 png 的解码。\n\n<div align=center>\n\n![Live2DViewer](https://media.githubusercontent.com/media/SakuraEngine/Sakura.Resources/main/showcase/Live2DViewer.png)\n\n</div>\n\nLive2D 模型复合了多种源数据类型，所有数据类型异步地加载和解析。整个模型的加载过程复合了硬盘读取、内存流送到显存、文件解压流送到显存以及直接上传文件到显存。Demo 保证了所有类型的 I/O 操作保持带宽最高效，在此期间发起请求的主线程没有任何停顿与开销。未处理的 Live2D 模型包含了数十个小尺寸 JSON 文件、数个中尺寸模型顶点文件、2张需要解码的 4K PNG 贴图，构成了下图的 I/O 流水线 profile 图表。\n\n<div align=center>\n\n![Live2DViewerIO](https://media.githubusercontent.com/media/SakuraEngine/Sakura.Resources/main/showcase/Live2DAsyncIO.png)\n\n</div>\n\nShipping Build 的最终呈现帧数可以轻松地突破数千帧，这是 Cubism 官方示例基准的十数倍。\n\n### [RenderGraph Deferred](samples/render_graph/rg-deferred)\n这个 demo 展示了如何使用 RenderGraph 进行 Deferred 渲染，其中光照计算的部分有 ComputeShdaer 和 PixelShader 两种实现。实际的光照着色效果尚未在 demo 中完成，重点在于验证延迟流程的可行性。这个 demo 同样展示了如何使用自定义 Profiler 对 RenderGraph 的执行细节进行 Profile。\n\n<div align=center>\n\n![RenderGraphDeferred](https://media.githubusercontent.com/media/SakuraEngine/Sakura.Resources/main/showcase/rg-deferred.png)\n\n</div>\n\n\n### [RenderGraph Triangle](samples/render_graph/rg-triangle)\n这个 demo 展示了如何使用 RenderGraph 进行三角形渲染。\n\n\n### [纹理](samples/cgpu-texture)\n这个 demo 演示了如何在 CGPU 中使用纹理采样，demo 也演示了怎么在 CGPU 中启用 Static/Immutable Samplers。\n\n<div align=center>\n\n![cgpu-texture](https://media.githubusercontent.com/media/SakuraEngine/Sakura.Resources/main/showcase/cgpu-texture.png)\n\n</div>\n\n## 花絮\n\n我们最终抛弃但曾探索过的成果。抛弃不代表这些技术差或是不可用，而是我们综合考虑后选择性地放弃了它们。\n\n### [热更三角形](samples/hot-triangle)\n\n这是一个多后端的三角形绘制 demo。\n- 每个后端会拉起一个窗口, 并在一个独立的线程上绘制它；\n- drawcall 录制的逻辑可以运行在 host 程序或者 wasm 虚拟机后端中, host 程序和 wasm ‘脚本’共享[同一份C代码](samples/hot-triangle/triangle_module.wa.c)；\n- 实现了一个简单的 [filewatcher](samples/hot-triangle/hot_wasm.cpp)，自动对 drawcall 脚本进行变更检查，调用 SDK 编译 wasm，并基于产出物应用热修复。\n\n<div align=center>\n\n![hot-triangle](https://media.githubusercontent.com/media/SakuraEngine/Sakura.Resources/main/showcase/hot-triangle.gif)\n\n</div>\n\n## 核心组件\n- platform\n- math\n- cgpu: [[api]](include/cgpu/api.h) [[design]](include/cgpu/README.md)\n\n## 嵌入源码的开源库和版本\n- constexpr-xxh3 aebcee7 (BSD 2-Clause License)\n- [lru-cache](https://github.com/goldsborough/lru-cache) 13f30ad MIT\n- sole 1.0.1 (zlib License)\n- marl e007bd3 (Apache-2.0)\n- folly (Apache-2.0)\n- xxhash 0.8.1 (BSD)\n- fast_float v3.4.0\n- mimalloc v2.1.4 (MIT)\n- godot 5dccc940e7 (MIT)\n- OpenString 81926cc (MIT)\n- concurrentqueue d49fa2b (Simplified BSD)\n- VulkanMemoryAllocator 3.0.1, release\n- D3D12MemoryAllocator 2.0.1 release\n- SPIRV-Reflect b68b5a8 (Apache-2.0)\n- RealtimeMath 80d08a8 (MIT)\n- FiberTaskingLib 9d7b27d (Apache-2.0)\n- parallel-hashmap 1.3.11 (Apache-2.0)\n- [TSCNS](https://github.com/MengRao/tscns) v2.0 (MIT)\n\n## 在自定义包管理引入的开源库和版本\n- LEMON v1.3.1 (Boost Software License)\n- LMDB v0.9.29 (BSD)\n- zlib v1.2.8\n- cgltf v1.13 (MIT)\n- yyjson v0.9.0 (MIT)\n- cpu_features v0.9.0 (Apache-2.0)\n- freetype 2.13.0 (GNU)\n- icu 72.1 ([LICENSE](https://github.com/unicode-org/icu/blob/main/icu4c/LICENSE))\n- harfbuzz 7.1.0 ([LICENSE](https://github.com/harfbuzz/harfbuzz/blob/main/COPYING))\n- doctest v2.4.11\n\n## 参考和借鉴的开源库\n- [quill](https://github.com/odygrd/quill) v3.0.2 (MIT)\n\n## 接入的扩展 API 以及版本\n- [vulkan headers](https://github.com/KhronosGroup/Vulkan-Headers) & volk 1.3.250.0\n- nvapi R510\n- amd_ags 6.0.1\n\n## 内置 SDK /软件版本\n- ispc 1.18.0\n- python 3.10.8\n\n## 构建\n### 前置\n- xmake\n- 初始化 LFS\n\n### 编译\n使用以下命令编译\n\n```\n> xmake l setup.lua\n> xmake f -m debug -c\n> xmake \n```\n\nTips：\n- 默认构建只包含模块。要构建工具或例子，需要在 xmake f 时加上 --build_cgpu_samples=true 等参数 (详见 xmake/options.lua);\n- 目前版本构建中途失败可能产生 codegen 中断或是不全的问题，可以删除 `/build` 和 `.xmake` 文件夹后重试。如进一步出现问题，请务必上报 issues 😀\n- 上报 issue 时尽量提供 `xmake f -m debug -c -v` 在中断处的详细输出;\n- 当出现 xrepo 安装失败问题（例如 LFS 没有初始化造成错误的库文件安装）时，可用 `xrepo remove --all -y` 清理错误安装的仓库后再重新构建。\n\n## 编辑环境\n推荐使用 vscode + clangd 作为编辑环境，使用命令 `xmake project -k compile_commands` 来生成 clangd 需要的数据集\n",707,graphics,C++,23,C++,C,Objective-C,Objective-C++,HLSL,Lua,Mako,Python,Makefile,Perl,Shell,Hack,Ragel,Starlark,POV-Ray SDL,Roff,Assembly,HTML,CSS,CMake,BitBake,Pawn,NASL,,,,,,122,5,117,0,7,9,0,53208,59,95,70,25,cf3185c6a279899baf2f2a9aad58f748dee7e1c0,Merge pull request #222 from SakuraEngine/v8,2024-07-16T12:29:00Z,SaeruHikari,39457738+SaeruHikari@users.noreply.github.com,SaeruHikari,,,,,,,Other,SakuraEngine,SakuraEngine,2,gpu,cpp,graphics,game,game-engine,,,,,,,,,,,,,,,,/SakuraEngine/SakuraEngine,2,16,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/root-project/root,https://github.com/root-project/root,0.5,"package for the storage, processing, and analysis of scientific data. Package not good",1,1,1,1,1,0,0,0,0,0,0,1,"The official repository for ROOT: analyzing, storing and visualizing big data, scientifically","<img src=""https://root-forum.cern.ch/uploads/default/original/2X/3/3fb82b650635bc6d61461f3c47f41786afad4548.png"" align=""right""  height=""50""/>\n\n## About\n\nROOT is a unified software package for the storage, processing, and analysis of \nscientific data: from its acquisition to the final visualization in form of highly \ncustomizable, publication-ready plots. It is reliable, performant and well supported,\neasy to use and obtain, and strives to maximize the quantity and impact of scientific \nresults obtained per unit cost, both of human effort and computing resources.\n\nROOT provides a very efficient storage system for data models, \nthat demonstrated to scale at the Large Hadron Collider experiments: Exabytes \nof scientific data are written in columnar ROOT format.\nROOT comes with histogramming capabilities in an arbitrary number of \ndimensions, curve fitting, statistical modelling, minimization, to allow\nthe easy setup of a data analysis system that can query and process the data\ninteractively or in batch mode, as well as a general parallel processing\nframework, RDataFrame, that can considerably speed up an analysis, taking \nfull advantage of multi-core and distributed systems.\n\nROOT is performance critical software written in C++ and enables rapid prototyping \npowered by a unique C++ compliant interpreter called Cling. \nCling also enables performant C++ type introspection which is a building block of automatic \ninteroperability with Python. Thanks to PyROOT, leveraging the cppyy technology, \nROOT offers efficient, on-demand C++/Python interoperability in a uniform cross-language \nexecution environment.\n\nROOT fully embraces open-source, it's made with passion by its community,\nfor the benefit of its community.\n\n[![License: LGPL v2.1+](https://img.shields.io/badge/License-LGPL%20v2.1+-blue.svg)](https://www.gnu.org/licenses/lgpl.html)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/5060/badge)](https://bestpractices.coreinfrastructure.org/projects/5060)\n\n## Contribution Guidelines\n- [How to contribute](https://github.com/root-project/root/blob/master/CONTRIBUTING.md)\n- [Coding conventions](https://root.cern/coding-conventions)\n- [Meetings](https://root.cern/meetings)\n\n## Cite\nWhen citing ROOT, please use both the reference reported below and the DOI specific to your ROOT version available [on Zenodo](https://zenodo.org/badge/latestdoi/10994345) [![DOI](https://zenodo.org/badge/10994345.svg)](https://zenodo.org/badge/latestdoi/10994345). For example, you can copy-paste and fill in the following citation:\n\n    Rene Brun and Fons Rademakers, ROOT - An Object Oriented Data Analysis Framework,\n    Proceedings AIHENP'96 Workshop, Lausanne, Sep. 1996,\n    Nucl. Inst. & Meth. in Phys. Res. A 389 (1997) 81-86.\n    See also ""ROOT"" [software], Release vX.YY/ZZ, dd/mm/yyyy\n\n## Live Demo for CERN Users\n[![](https://img.shields.io/badge/Launch-SWAN-orange)](http://cern.ch/swanserver/cgi-bin/go?projurl=https://github.com/cernphsft/rootbinder.git)\n\nSee more screenshots on our [gallery](https://root.cern/gallery).\n\n## Installation and Getting Started\nSee https://root.cern/install for installation instructions.\nFor instructions on how to build ROOT from these source files, see https://root.cern/install/build_from_source.\n\nOur [""Getting started with ROOT""](https://root.cern/get_started) page is then the perfect place to get familiar with ROOT.\n\n## Help and Support\n- [Forum](https://root.cern/forum/)\n- [Issue tracker](https://github.com/root-project/root/issues)\n  * [Previous now read-only Jira issue tracker](https://sft.its.cern.ch/jira/projects/ROOT/issues/ROOT-5820?filter=allopenissues)\n- [Documentation](https://root.cern/guides/reference-guide)\n- [Tutorials](https://root.cern/doc/master/group__Tutorials.html)",2547,graphics,C++,27,HTML,Makefile,C++,C,CMake,Shell,Perl,Python,Emacs Lisp,Objective-C++,Objective-C,CSS,JavaScript,C#,AppleScript,Fortran,R,Batchfile,Smarty,Jupyter Notebook,Cuda,Roff,Dockerfile,SWIG,PowerShell,Vim Script,Assembly,,13849,1798,11774,277,17,409,0,1062603,1238,2213,1658,555,d4c8fa6bab03b4200e3abd8f1e568319bc8048f1,[gui] fix compile error in RWebWindowsManager.cxx,2024-07-19T15:39:16Z,silverweed,giacomo.parolini@cern.ch,silverweed,v6.32.02,"Patch release of v6.32 series.\r\n\r\n[:spiral_notepad: Release notes](https://root.cern/doc/v632/release-notes.html#release-6.32.02)\r\n[:floppy_disk: Install instructions](https://root.cern/install/)\r\n\r\nItems addressed in this release:\r\n* #7236 - Memory leak in TFile::WriteObjectAny, depending on data type\r\n* #10075 - Difficult to configure Xrootd to use non-builtin openssl\r\n* #14051 - [web graphics] Web graphics resizes canvas despite canvas size definition in the macro\r\n* #15321 - [MSVC] Root is failed with error G694476FC: static_assert failed ""Unexpected size""\r\n* #15405 - [RF] ExternalConstraints documentation incorrect for RooMCStudy\r\n* #15430 - Test failures with Python 3.13\r\n* #15473 - Segmentation fault when building with the mold linker\r\n* #15498 - gPad is not consistent in pyROOT with web canvas\r\n* #15511 - Possible memory corruption in cling\r\n* #15579 - Performance regression (slowdown) in ALICE event generation\r\n* #15686 - JITted code changes the execution order of computation graph nodes\r\n* #15688 - [PyROOT] TProfile2D::Fill ambiguities preventing use of some signatures in pyROOT\r\n* #15690 - [RF] SegFault in RooBernstein::fillBuffer\r\n* #15694 - [RF] New RooFit EvalBackend returning incorrect result for binned likelihoods\r\n* #15703 - Leaking memory though strings in PyROOT\r\n* #15727 - Windows CMake project cannot find_library() after integrating with ROOT.\r\n* #15751 - [RF] Using a conditional RooProdPdf in a multi-channel fit spawns too many integrals with new CPU evaluation backend\r\n* [[#15791](https://github.com/root-project/root/issues/15791)] - JS ROOT does not draw tprofile2d correctly with ""TEXT"" draw option\r\n* [[#15799](https://github.com/root-project/root/issues/15799)] - pyunittests-pyroot-pyz-ttree-setbranchaddress segfaults with Python 3.13\r\n* [[ROOT-7412](https://its.cern.ch/jira/browse/ROOT-7412)] - Strange results looking for nested types\r\n* [[ROOT-8439](https://its.cern.ch/jira/browse/ROOT-8439)] - PyROOT does not treat exceptions properly in overloaded methods\r\n* [[ROOT-9307](https://its.cern.ch/jira/browse/ROOT-9307)] - TPad::GetListOfPrimitives() double deletion error in TList::Clear()\r\n",v6-32-02,Danilo Piparo,,dpiparo,Other,root,root-project,21,data-analysis,root-cern,root,c-plus-plus,python,physics,statistics,mathematics,machine-learning,interpreter,cling,graphics,visualization,geometry,parallel,hacktoberfest,,,,,/root-project/root,431,125,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/ReactionMechanismGenerator/RMG-Py,https://github.com/ReactionMechanismGenerator/RMG-Py,1,,,1,1,1,1,0,0,0,0,0,0,1,Python version of the amazing Reaction Mechanism Generator (RMG).,"# <img align=""top"" src=""https://raw.githubusercontent.com/ReactionMechanismGenerator/RMG-Py/main/documentation/source/_static/rmg-logo-small.png""> Reaction Mechanism Generator (RMG)\n\n[![Codecov report](https://img.shields.io/codecov/c/github/ReactionMechanismGenerator/RMG-Py/main.svg)](https://codecov.io/gh/ReactionMechanismGenerator/RMG-Py)\n[![GitHub release](https://img.shields.io/github/release/ReactionMechanismGenerator/RMG-Py.svg)](https://github.com/ReactionMechanismGenerator/RMG-Py/releases)\n[![Anconda](https://img.shields.io/conda/v/rmg/rmg.svg)](https://anaconda.org/rmg/rmg)\n[![RMG Website](https://img.shields.io/website-up-down-green-red/http/rmg.mit.edu.svg?label=rmg%20website)](https://rmg.mit.edu/)\n\n## Description\nThis repository contains the Python version of **Reaction Mechanism Generator (RMG)**,\na tool for automatically generating chemical reaction\nmechanisms for modeling reaction systems including pyrolysis, combustion,\natmospheric science, and more.\n\nIt also includes **Arkane**, the package for calculating thermodynamics, high-pressure-limit\nrate coefficients, and pressure dependent rate coefficients from quantum chemical calculations.\nArkane is compatible with a variety of ab initio quantum chemistry software programs:\nGaussian, Q-Chem, Molpro, Orca, Psi4, and TeraChem.\n\n## Source Code Repository\n- [RMG Github Repository](https://github.com/ReactionMechanismGenerator/RMG-Py): contains the latest source code for RMG\n- [RMG-database Github Repository](https://github.com/ReactionMechanismGenerator/RMG-database): contains source code for the latest version of the database\n\n## How to Install\nYou can either download the source from GitHub and compile yourself, or download the binaries from Anaconda.\nPlease see the [Download and Install](http://reactionmechanismgenerator.github.io/RMG-Py/users/rmg/installation/index.html) page for detailed instructions.\n\n## Documentation\n- [RMG Documentation](http://ReactionMechanismGenerator.github.io/RMG-Py/users/rmg/index.html) ([PDF version](https://github.com/ReactionMechanismGenerator/RMG-Py/raw/main/documentation/RMG-Py_and_Arkane_Documentation.pdf))\n- [Arkane Documentation](http://ReactionMechanismGenerator.github.io/RMG-Py/users/arkane/index.html) ([PDF version](https://github.com/ReactionMechanismGenerator/RMG-Py/raw/main/documentation/RMG-Py_and_Arkane_Documentation.pdf))\n- [RMG API Reference](http://reactionmechanismgenerator.github.io/RMG-Py/reference/index.html) ([PDF version](https://github.com/ReactionMechanismGenerator/RMG-Py/raw/main/documentation/RMG-Py_API_Reference.pdf))\n\n## How to Contribute\nPlease see the [Contributor Guidelines](https://github.com/ReactionMechanismGenerator/RMG-Py/wiki/RMG-Contributor-Guidelines)\nfor details on how to contribute to RMG-Py, Arkane, or RMG-database.\n\n## How to Give Feedback\n\nPlease post any issues you may have to the [issues page](https://github.com/ReactionMechanismGenerator/RMG-Py/issues/)\nor drop in to the [chat room](https://gitter.im/ReactionMechanismGenerator/RMG-Py) or email [rmg_dev@mit.edu](mailto:rmg_dev@mit.edu) if you have questions.  \n\n## Useful Links\n- [Interactive Website](https://rmg.mit.edu): Visit this site to visualize RMG-generated models, view the databases, and \nperform thermodynamics and kinetics searches\n- [Wiki](https://github.com/ReactionMechanismGenerator/RMG-Py/wiki): a wiki for developer notes\n- [Issues Page](https://github.com/ReactionMechanismGenerator/RMG-Py/issues/): view current issues and feature requests\n\n## Credits\n- [Professor William H. Green's research group](http://cheme.scripts.mit.edu/green-group/) at the \n[Massachusetts Institute of Technology](http://web.mit.edu/) \n- [Professor Richard H. West's research group](http://www.northeastern.edu/comocheng/) at \n[Northeastern University](http://www.northeastern.edu/). \n\n## Resources and References\nThe resources and relevant publications are listed [here](https://rmg.mit.edu/resources) on the RMG-website. \nPlease at least cite our latest publication on Reaction Mechanism Generator v3.0 and other\nrelevant publications when publishing the results using our software.\n\n## How to cite\nPlease include the following citations if RMG, RMG-database, and/or Arkane were used for an academic study:\n- C.W. Gao, J.W. Allen, W.H. Green, R.H. West,\n  [Reaction Mechanism Generator: Automatic construction of chemical kinetic mechanisms](https://doi.org/10.1016/j.cpc.2016.02.013),\n  Computer Physics Communications 2016, 203, 212-225.\n- M. Liu, A. Grinberg Dana, M.S. Johnson, M.J. Goldman, A. Jocher, A.M. Payne, C.A. Grambow, K. Han, N.W. Yee,\n  E.J. Mazeau, K. Blondal, R.H. West, C.F. Goldsmith, W.H. Green,\n  [Reaction Mechanism Generator v3.0: Advances in Automatic Mechanism Generation](https://doi.org/10.1021/acs.jcim.0c01480),\n  Journal of Chemical Information and Modeling 2021, 61(6), 2686-2696.\n- M. S. Johnson, X. Dong, A. Grinberg Dana, Y. Chung, D. Farina, R. J. Gillis, M. Liu, N. W. Yee, K. Blondal, \n  E. Mazeau, C. A. Grambow, A. M. Payne, K. A. Spiekermann, H.-W. Pang, C. F. Goldsmith, R. H. West, W. H. Green,\n  [RMG Database for Chemical Property Prediction](https://pubs.acs.org/doi/10.1021/acs.jcim.2c00965),\n  Journal of Chemical Information and Modeling 2022, 62(20), 4906–4915.\n\n## License Information\nRMG is a free, open-source software package (distributed under the [MIT/X11 license](https://github.com/ReactionMechanismGenerator/RMG-Py/blob/main/LICENSE.txt)).\n",376,chemistry,Python,6,Makefile,Python,Shell,Jupyter Notebook,Cython,Dockerfile,,,,,,,,,,,,,,,,,,,,,,,1457,281,1148,28,312,102,244,177010,226,1201,1170,31,36bceb35bc6b0d58beb2f2fa53faa12cf35c7034,Merge pull request #2663 from ReactionMechanismGenerator/external_kin…,2024-06-18T13:10:40Z,Jackson Burns,33505528+JacksonBurns@users.noreply.github.com,JacksonBurns,RMG-Py v3.1.0,"RMG-Py Version 3.1.0\r\n====================\r\nDate: April 23, 2021\r\n\r\nWe recommend creating a new conda environment using the latest environment.yml\r\nas many dependencies have changed, and upgrading an existing environment is\r\nalways troublesome.\r\n\r\n- RMG-Py\r\n  - Added support for Bromine \r\n  - Added improved method to calculate temperature dependent solvation free energy\r\n  - Made Rank 1 accuracy correspond to 0.2 kcal/mol instead of 0 kcal/mol\r\n  - Improvements to Group Additivity comments, in particular adding missing group comments\r\n  - Added support for trimolecular units in ArrheniusBM fits\r\n  - Improvements to profiling\r\n  - Use kekulized structures for transport estimation\r\n  - Automatic tree generation script improvements\r\n  - Properly short circuit is_isomorphic when strict=False\r\n  - Added block for specifying species tuples to react when starting an RMG run\r\n  - Improve ArrheniusBM fitting to a single reaction\r\n  - Improvements in bidentate thermochemistry estimation\r\n  - Added new surface attributes for metals and facets\r\n  - Added support for Phosphorus\r\n  - Enable use LSRs to scale thermo from different metals and enable proper \r\n    use of training reactions from different metals\r\n  - Added maximumSurfaceSites constraint\r\n  \r\n- Arkane \r\n  - Added frequency scaling factors for apfd/deef2tzvp and wb97xd/def2svp\r\n  - Kinetics and pdep sensitivities additionally saved in YAML format\r\n  - Enable automatic isodesmic reaction generation\r\n  - AECs, BACs and frequency scaling factors moved from Arkane to RMG-database\r\n  - Added functionality for Petersson and Melius BAC fitting using Arkane and \r\n    the reference database\r\n  - Enabled two parameter Arrhenius fit option\r\n  - Added functionality for fitting AECs\r\n  - Added classes to standardize model chemistry definitions\r\n  - Use adjlists instead of smiles when saving\r\n  \r\n- Bugfixes\r\n  - QMTP updated to work with g16 executable\r\n  - Fixed various Sticking Coefficient bugs \r\n  - Fixed issues with Surface Arrhenius reactions written in the reverse being converted\r\n    to ArrheniusEP instead of SurfaceArrheniusBEP\r\n  - Fixed NaN handling in the explorer tool's steady state solve\r\n  - Fixed determine_qm_software for Orca \r\n  - Fixed bug where elementary_high_p library reactions with more than the maximum number of atoms for pdep \r\n    never entered the edge \r\n  - Fixed bug related to pdep networks having sources not contained in the core \r\n  - Fixed various profiling bugs\r\n  - Fixed issue with indexing when merging models\r\n  - Fixed bug with ranged liquid reactors\r\n  - Fixed bug with loading of autogenerated trees in Arkane\r\n  - Fixed bug related to collision limit violation checks in LiquidReactor\r\n  - Fixed bug related to Pmin and Pmax definition in SurfaceReactor\r\n  - Fixed bugs in global uncertainty analysis for LiquidReactor\r\n  - Fixed bug related to the units of reverse rate constants for reactions involving surface species\r\n  - Fixed bug in Molecule isomorphism where it would simply assume the given initial map was correct\r\n  - Remove deprecated matplotlib warn keyword \r\n  - Fixed bug related to reading Chebyshev forms in Chemkin files\r\n  - Fixed reference concentration for surface species when calculating Kc\r\n  - Fixed issue with the reaction generation using the reversee of Surface_ElleyRideal_Addition_MultipleBond\r\n  - Fixed bug with adjlist multiplicitly line being mistaken as the species name\r\n  - Fixed bug with the library to training notebook\r\n  - Remove temporary seed mechanisms if they exist from a previous run\r\n  \r\n- Miscellaneous\r\n  - Modified find_parameter_sources_and_assign_uncertainties to regenerate chem.inp as needed\r\n  - Added option to save atom order when labeling template reactions\r\n  - Added option to ignore atom type errors when creating molecule objects\r\n  - Enable use of critical_distance_factor in from_xyz\r\n  - Improved SIGINT handling when calling lpsolve\r\n  - Enable H-bond drawing\r\n  - Improvements to debug messages\r\n  - Updated dependencies cclib and OpenBabel\r\n\r\nNote that the upgrade to OpenBabel v3+ will change the interpretation\r\nof some ambiguous SMILES strings that use the lower-case aromatics notation.\r\nAlthough we think the new interpretation is not wrong, it might be different\r\nfrom previous versions, so take care.",03.01.2000,Matt Johnson,,mjohnson541,Other,RMG-Py,ReactionMechanismGenerator,22,reactions,python,thermodynamics,chemistry,chemistry-discovery,kinetics,,,,,,,,,,,,,,,/ReactionMechanismGenerator/RMG-Py,26,49,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/rapidsai/cucim,https://github.com/rapidsai/cucim,0,Image processing does not fit,,0,1,0,0,0,0,1,0,0,0,0,cuCIM - RAPIDS GPU-accelerated image processing library,"# <div align=""left""><img src=""https://rapids.ai/assets/images/rapids_logo.png"" width=""90px""/>&nbsp;cuCIM</div>\n\n[RAPIDS](https://rapids.ai) cuCIM is an open-source, accelerated computer vision and image processing software library for multidimensional images used in biomedical, geospatial, material and life science, and remote sensing use cases.\n\ncuCIM offers:\n\n- Enhanced Image Processing Capabilities for large and n-dimensional tag image file format (TIFF) files\n- Accelerated performance through Graphics Processing Unit (GPU)-based image processing and computer vision primitives\n- A Straightforward Pythonic Interface with Matching Application Programming Interface (API) for Openslide\n\ncuCIM supports the following formats:\n\n- Aperio ScanScope Virtual Slide (SVS)\n- Philips TIFF\n- Generic Tiled, Multi-resolution RGB TIFF files with the following compression schemes:\n  - No Compression\n  - JPEG\n  - JPEG2000\n  - Lempel-Ziv-Welch (LZW)\n  - Deflate\n\n**NOTE:** For the latest stable [README.md](https://github.com/rapidsai/cucim/blob/main/README.md) ensure you are on the `main` branch.\n\n- [GTC 2022 Accelerating Storage IO to GPUs with Magnum IO [S41347]](https://events.rainfocus.com/widget/nvidia/gtcspring2022/sessioncatalog/session/1634960000577001Etxp)\n  - cuCIM's GDS API examples: <https://github.com/NVIDIA/MagnumIO/tree/main/gds/readers/cucim-gds>\n- [SciPy 2021 cuCIM - A GPU image I/O and processing library](https://www.scipy2021.scipy.org/)\n  - [video](https://youtu.be/G46kOOM9xbQ)\n- [GTC 2021 cuCIM: A GPU Image I/O and Processing Toolkit [S32194]](https://www.nvidia.com/en-us/on-demand/search/?facet.mimetype[]=event%20session&layout=list&page=1&q=cucim&sort=date)\n  - [video](https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s32194/)\n\n**[Developer Page](https://developer.nvidia.com/multidimensional-image-processing)**\n\n**Blogs**\n- [Enhanced Image Analysis with Multidimensional Image Processing](https://developer.nvidia.com/blog/enhanced-image-analysis-with-multidimensional-image-processing/)\n- [Accelerating Scikit-Image API with cuCIM: n-Dimensional Image Processing and IO on GPUs](https://developer.nvidia.com/blog/cucim-rapid-n-dimensional-image-processing-and-i-o-on-gpus/)\n- [Accelerating Digital Pathology Pipelines with NVIDIA Clara™ Deploy](https://developer.nvidia.com/blog/accelerating-digital-pathology-pipelines-with-nvidia-clara-deploy-2/)\n\n**Webinars**\n\n- [cuCIM: a GPU Image IO and Processing Library](https://www.youtube.com/watch?v=G46kOOM9xbQ)\n\n**[Documentation](https://docs.rapids.ai/api/cucim/stable)**\n\n**Release notes** are available on our [wiki page](https://github.com/rapidsai/cucim/wiki/Release-Notes).\n\n## Install cuCIM\n\n### Conda\n\n#### [Conda (stable)](https://anaconda.org/rapidsai/cucim)\n\n```bash\nconda create -n cucim -c rapidsai -c conda-forge cucim cuda-version=`<CUDA version>`\n```\n\n`<CUDA version>` should be 11.2+ (e.g., `11.2`, `12.0`, etc.)\n\n#### [Conda (nightlies)](https://anaconda.org/rapidsai-nightly/cucim)\n\n```bash\nconda create -n cucim -c rapidsai-nightly -c conda-forge cucim cuda-version=`<CUDA version>`\n```\n\n`<CUDA version>` should be 11.2+ (e.g., `11.2`, `12.0`, etc.)\n\n### [PyPI](https://pypi.org/project/cucim/)\n\nInstall for CUDA 12:\n\n```bash\npip install cucim-cu12\n```\n\nAlternatively install for CUDA 11:\n\n```bash\npip install cucim-cu11\n```\n\n### Notebooks\n\nPlease check out our [Welcome](notebooks/Welcome.ipynb) notebook ([NBViewer](https://nbviewer.org/github/rapidsai/cucim/blob/main/notebooks/Welcome.ipynb))\n\n#### Downloading sample images\n\nTo download images used in the notebooks, please execute the following commands from the repository root folder to copy sample input images into `notebooks/input` folder:\n\n(You will need [Docker](https://www.docker.com/) installed in your system)\n\n```bash\n./run download_testdata\n```\nor\n\n```bash\nmkdir -p notebooks/input\ntmp_id=$(docker create gigony/svs-testdata:little-big)\ndocker cp $tmp_id:/input notebooks\ndocker rm -v ${tmp_id}\n```\n\n## Build/Install from Source\n\nSee build [instructions](CONTRIBUTING.md#setting-up-your-build-environment).\n\n## Contributing Guide\n\nContributions to cuCIM are more than welcome!\nPlease review the [CONTRIBUTING.md](https://github.com/rapidsai/cucim/blob/main/CONTRIBUTING.md) file for information on how to contribute code and issues to the project.\n\n## Acknowledgments\n\nWithout awesome third-party open source software, this project wouldn't exist.\n\nPlease find [LICENSE-3rdparty.md](LICENSE-3rdparty.md) to see which third-party open source software\nis used in this project.\n\n## License\n\nApache-2.0 License (see [LICENSE](LICENSE) file).\n\nCopyright (c) 2020-2022, NVIDIA CORPORATION.\n",324,digital-pathology,Jupyter Notebook,8,CMake,C,C++,Python,Shell,Jupyter Notebook,Dockerfile,Cuda,,,,,,,,,,,,,,,,,,,,,523,40,481,2,26,71,0,24906,55,225,123,102,7bbfe936daec341ae67fd798aba4629b9ba80688,Build and test with CUDA 12.5.1 (#747),2024-07-16T17:29:05Z,Kyle Edwards,kyedwards@nvidia.com,KyleFromNVIDIA,v24.06.00,"## 🚨 Breaking Changes\r\n\r\n- The `output` argument of `cucim.skimage.filters.gaussian` has been renamed to `out`. The old name is deprecated and will be removed in release 25.02 (#727)\r\n- Renamed `get_xyz_coords` function is now removed (use `skimage.color.xyz_tristimulus_values` instead) (#724)\r\n- Removed deprecated `return_error` kwarg from `phase_cross_correlation` (the error is now always returned) (#724)\r\n- Removed deprecated `random_state` kwarg from `medial_axis` (it was renamed to `rng` previously) (#724)\r\n\r\n## 🐛 Bug Fixes\r\n\r\n- Use SciPy&#39;s KDTree instead of deprecated cKDTree ([#733](https://github.com/rapidsai/cucim/pull/733)) [@grlee77](https://github.com/grlee77)\r\n- Binary and grayscale morphology functions have bug fixes in the case of even-sized/non-symmetric footprints (for details see upstream MR: https://github.com/scikit-image/scikit-image/pull/6695) (#728)\r\n\r\n## 🚀 New Features\r\n\r\n- `cucim.skimage.measure.regionprops` (and `regionprops_table`) support one new region property: `intensity_std` (#727)\r\n- `cucim.skimage.segmentation.expand_labels` now supports a `spacing` keyword argument to take a pixel's physical dimensions into account (#727)\r\n- binary morphology functions have a new `mode` argument that controls how values outside the image boundaries are interpreted (#728)\r\n- grayscale morphology functions have new `mode` and `cval` arguments that control how boundaries are extended (#728)\r\n\r\n## 🛠️ Improvements\r\n\r\n- Enable FutureWarnings/DeprecationWarnings as errors ([#734](https://github.com/rapidsai/cucim/pull/734)) [@mroeschke](https://github.com/mroeschke)\r\n- Migrate to `{{ stdlib(&quot;c&quot;) }}` ([#731](https://github.com/rapidsai/cucim/pull/731)) [@hcho3](https://github.com/hcho3)\r\n- Implement upstream changes from scikit-image 0.23 (part 2 of 2: morphology) ([#728](https://github.com/rapidsai/cucim/pull/728)) [@grlee77](https://github.com/grlee77)\r\n- Implement upstream changes from scikit-image 0.23 (part 1 of 2) ([#727](https://github.com/rapidsai/cucim/pull/727)) [@grlee77](https://github.com/grlee77)\r\n- Update the test criteria for test_read_random_region_cpu_memleak ([#726](https://github.com/rapidsai/cucim/pull/726)) [@gigony](https://github.com/gigony)\r\n- Remove code needed to support Python &lt; 3.9 and apply ruff&#39;s pyupgrade rules ([#725](https://github.com/rapidsai/cucim/pull/725)) [@grlee77](https://github.com/grlee77)\r\n- removal of deprecated functions/kwargs scheduled for release 24.06 ([#724](https://github.com/rapidsai/cucim/pull/724)) [@grlee77](https://github.com/grlee77)\r\n- Enable all tests for `arm` jobs ([#717](https://github.com/rapidsai/cucim/pull/717)) [@galipremsagar](https://github.com/galipremsagar)\r\n- prevent path conflict ([#713](https://github.com/rapidsai/cucim/pull/713)) [@AyodeAwe](https://github.com/AyodeAwe)\r\n- Updated cuCIM APIs for consistency with scikit-image 0.23.2 (#727 and #728)\r\n- Additional modules use `__init__.pyi` instead of just `__init__.py` (#727)\r\n- Some grayscale tests now compare directly to `skimage` CPU outputs instead fetching previously saved values (#728)\r\n- Refactored some test cases to better use `pytest.mark.parametrize` (#728)\r\n- Bumped version pinning for scikit-image to allow 0.23.x to be installed (#728)\r\n\r\n## 📖 Documentation\r\n- Various fixes to documentation strings (consistent shape notation, etc.) (#727)\r\n\r\n# cuCIM 24.04.00 (10 Apr 2024)\r\n\r\n## 🐛 Bug Fixes\r\n\r\n- Require `click` as a wheel dependency ([#719](https://github.com/rapidsai/cucim/pull/719)) [@jakirkham](https://github.com/jakirkham)\r\n- Fix docs upload directory ([#714](https://github.com/rapidsai/cucim/pull/714)) [@raydouglass](https://github.com/raydouglass)\r\n- Fix `popd` indent in `run` ([#693](https://github.com/rapidsai/cucim/pull/693)) [@jakirkham](https://github.com/jakirkham)\r\n- Re-run `ci/release/update-version.sh 24.04.00` ([#690](https://github.com/rapidsai/cucim/pull/690)) [@jakirkham](https://github.com/jakirkham)\r\n\r\n## 🚀 New Features\r\n\r\n- Support CUDA 12.2 ([#672](https://github.com/rapidsai/cucim/pull/672)) [@jameslamb](https://github.com/jameslamb)\r\n",v24.06.00,Ray Douglass,,raydouglass,Apache License 2.0,cucim,rapidsai,25,image-processing,computer-vision,medical-imaging,microscopy,digital-pathology,cuda,gpu,nvidia,image-analysis,image-data,segmentation,multidimensional-image-processing,,,,,,,,,/rapidsai/cucim,45,13,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/qutip/qutip,https://github.com/qutip/qutip,1,,,1,1,1,1,0,0,0,0,0,0,1,QuTiP: Quantum Toolbox in Python,"QuTiP: Quantum Toolbox in Python\n================================\n\n[A. Pitchford](https://github.com/ajgpitch),\n[C. Granade](https://github.com/cgranade),\n[A. Grimsmo](https://github.com/arnelg),\n[N. Shammah](https://github.com/nathanshammah),\n[S. Ahmed](https://github.com/quantshah),\n[N. Lambert](https://github.com/nwlambert),\n[E. Giguère](https://github.com/ericgig),\n[B. Li](https://github.com/boxili),\n[J. Lishman](https://github.com/jakelishman),\n[S. Cross](https://github.com/hodgestar),\n[A. Galicia](https://github.com/AGaliciaMartinez),\n[P. Menczel](https://github.com/pmenczel),\n[P. Hopf](https://github.com/flowerthrower/),\n[P. D. Nation](https://github.com/nonhermitian),\nand [J. R. Johansson](https://github.com/jrjohansson)\n\n[![Build Status](https://github.com/qutip/qutip/actions/workflows/tests.yml/badge.svg?branch=master)](https://github.com/qutip/qutip/actions/workflows/tests.yml)\n[![Coverage Status](https://img.shields.io/coveralls/qutip/qutip.svg?logo=Coveralls)](https://coveralls.io/r/qutip/qutip)\n[![Maintainability](https://api.codeclimate.com/v1/badges/df502674f1dfa1f1b67a/maintainability)](https://codeclimate.com/github/qutip/qutip/maintainability)\n[![license](https://img.shields.io/badge/license-New%20BSD-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)\n[![PyPi Downloads](https://img.shields.io/pypi/dm/qutip?label=downloads%20%7C%20pip&logo=PyPI)](https://pypi.org/project/qutip)\n[![Conda-Forge Downloads](https://img.shields.io/conda/dn/conda-forge/qutip?label=downloads%20%7C%20conda&logo=Conda-Forge)](https://anaconda.org/conda-forge/qutip)\n\nQuTiP is open-source software for simulating the dynamics of closed and open quantum systems.\nIt uses the excellent Numpy, Scipy, and Cython packages as numerical backends, and graphical output is provided by Matplotlib.\nQuTiP aims to provide user-friendly and efficient numerical simulations of a wide variety of quantum mechanical problems, including those with Hamiltonians and/or collapse operators with arbitrary time-dependence, commonly found in a wide range of physics applications.\nQuTiP is freely available for use and/or modification, and it can be used on all Unix-based platforms and on Windows.\nBeing free of any licensing fees, QuTiP is ideal for exploring quantum mechanics in research as well as in the classroom.\n\nSupport\n-------\n\n[![Unitary Fund](https://img.shields.io/badge/Supported%20By-UNITARY%20FUND-brightgreen.svg?style=flat)](https://unitary.fund)\n[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org)\n\nWe are proud to be affiliated with [Unitary Fund](https://unitary.fund) and [numFOCUS](https://numfocus.org).\n\nWe are grateful for [Nori's lab](https://dml.riken.jp/) at RIKEN and [Blais' lab](https://www.physique.usherbrooke.ca/blais/) at the Institut Quantique\nfor providing developer positions to work on QuTiP.\n\nWe also thank Google for supporting us by financing GSoC students to work on the QuTiP as well as [other supporting organizations](https://qutip.org/#supporting-organizations) that have been supporting QuTiP over the years.\n\n\nInstallation\n------------\n\n[![Pip Package](https://img.shields.io/pypi/v/qutip?logo=PyPI)](https://pypi.org/project/qutip)\n[![Conda-Forge Package](https://img.shields.io/conda/vn/conda-forge/qutip?logo=Conda-Forge)](https://anaconda.org/conda-forge/qutip)\n\nQuTiP is available on both `pip` and `conda` (the latter in the `conda-forge` channel).\nYou can install QuTiP from `pip` by doing\n\n```bash\npip install qutip\n```\n\nto get the minimal installation.\nYou can instead use the target `qutip[full]` to install QuTiP with all its optional dependencies.\nFor more details, including instructions on how to build from source, see [the detailed installation guide in the documentation](https://qutip.readthedocs.io/en/stable/installation.html).\n\nAll back releases are also available for download in the [releases section of this repository](https://github.com/qutip/qutip/releases), where you can also find per-version changelogs.\nFor the most complete set of release notes and changelogs for historic versions, see the [changelog](https://qutip.readthedocs.io/en/stable/changelog.html) section in the documentation.\n\n\nThe pre-release of QuTiP 5.0 is available on PyPI and can be installed using pip:\n\n```bash\npip install --pre qutip\n```\n\nThis version breaks compatibility with QuTiP 4.7 in many small ways.\nPlease see the [changelog](https://github.com/qutip/qutip/blob/master/doc/changelog.rst) for a list of changes, new features and deprecations.\nThis version should be fully working. If you find any bugs, confusing documentation or missing features, please create a GitHub issue.\n\n\nDocumentation\n-------------\n\n[![Documentation Status - Latest](https://readthedocs.org/projects/qutip/badge/?version=latest)](https://qutip.readthedocs.io/en/latest/?badge=latest)\n\nThe documentation for the latest [stable release](https://qutip.readthedocs.io/en/latest/) and the [master](https://qutip.readthedocs.io/en/master/) branch is available for reading on Read The Docs.\n\nThe documentation for official releases, in HTML and PDF formats, can be found in the [documentation section of the QuTiP website](https://qutip.org/documentation.html).\n\nThe latest development documentation is available in this repository in the `doc` folder.\n\nA [selection of demonstration notebooks is available](https://qutip.org/tutorials.html), which demonstrate some of the many features of QuTiP.\nThese are stored in the [qutip/qutip-tutorials repository](https://github.com/qutip/qutip-tutorials) here on GitHub.\n\n\nContribute\n----------\n\nYou are most welcome to contribute to QuTiP development by forking this repository and sending pull requests, or filing bug reports at the [issues page](https://github.com/qutip/qutip/issues).\nYou can also help out with users' questions, or discuss proposed changes in the [QuTiP discussion group](https://groups.google.com/g/qutip).\nAll code contributions are acknowledged in the [contributors](https://qutip.readthedocs.io/en/stable/contributors.html) section in the documentation.\n\nFor more information, including technical advice, please see the [""contributing to QuTiP development"" section of the documentation](https://qutip.readthedocs.io/en/stable/development/contributing.html).\n\n\nCiting QuTiP\n------------\n\nIf you use QuTiP in your research, please cite the original QuTiP papers that are available [here](https://dml.riken.jp/?s=QuTiP).\n",1637,quantum-mechanics,Python,4,Python,C++,Cython,C,,,,,,,,,,,,,,,,,,,,,,,,,1533,177,1337,19,19,166,0,60897,628,935,817,118,25b825399efb832686eda41fea04500ac577fc1f,Merge pull request #2485 from Ericgig/types_super,2024-07-17T20:29:07Z,Eric Giguère,eric.giguere@calculquebec.ca,Ericgig,QuTiP 5.0.3,"Micro release to add support for numpy 2.\r\n\r\nBug Fixes\r\n---------\r\n\r\n- Bug Fix in Process Matrix Rendering. (#2400, by Anush Venkatakrishnan)\r\n- Fix steadystate permutation being reversed. (#2443)\r\n- Add parallelizing support for `vernN` methods with `mcsolve`. (#2454 by Utkarsh)\r\n\r\n\r\nDocumentation\r\n-------------\r\n\r\n- Added `qutip.core.gates` to apidoc/functions.rst and a Gates section to guide-states.rst. (#2441, by alan-nala)\r\n\r\n\r\nMiscellaneous\r\n-------------\r\n\r\n- Add support for numpy 2 (#2421, #2457)\r\n- Add support for scipy 1.14 (#2469)\r\n\r\n",v5.0.3,Asier Galicia,,AGaliciaMartinez,"BSD 3-Clause ""New"" or ""Revised"" License",qutip,qutip,28,qutip,python,quantum-toolbox,quantum,quantum-computing,quantum-mechanics,quantum-information,quantum-optics,unitaryhack,,,,,,,,,,,,/qutip/qutip,35,77,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/qupath/qupath,https://github.com/qupath/qupath,1,,,1,1,1,1,0,0,0,0,0,0,1,QuPath - Bioimage analysis & digital pathology,"[![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&url=https%3A%2F%2Fforum.image.sc%2Ftags%2Fqupath.json&query=%24.topic_list.tags.0.topic_count&colorB=brightgreen&suffix=%20topics&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tag/qupath)\n[![Total downloads](https://img.shields.io/github/downloads/qupath/qupath/total?style=flat)](https://github.com/qupath/qupath/releases)\n[![Latest release downloads](https://img.shields.io/github/downloads/qupath/qupath/latest/total?style=flat)](https://github.com/qupath/qupath/releases/latest)\n[![Paper](https://zenodo.org/badge/DOI/10.1038/s41598-017-17204-5.svg)](https://doi.org/10.1038/s41598-017-17204-5)\n[![Twitter](https://img.shields.io/twitter/follow/qupath?style=flat)](http://twitter.com/qupath)\n\n# QuPath\n\n**QuPath is open source software for bioimage analysis**.\n\nFeatures include:\n\n* Lots of tools to annotate and view images, including whole slide & microscopy images\n* Workflows for brightfield & fluorescence image analysis\n* New algorithms for common tasks, including cell segmentation, tissue microarray dearraying\n* Interactive machine learning for object & pixel classification\n* Customization, batch-processing & data interrogation by scripting\n* Easy integration with other tools, including ImageJ\n\nTo **download QuPath**, go to the [Latest Releases](https://github.com/qupath/qupath/releases/latest) page.\n\nFor **documentation**, see [https://qupath.readthedocs.io](https://qupath.readthedocs.io)\n\nFor **help & support**, try [image.sc](https://forum.image.sc/tag/qupath) or the [links here](https://qupath.readthedocs.io/en/latest/docs/starting/help.html)\n\nTo **build QuPath from source** see [here](https://qupath.readthedocs.io/en/latest/docs/reference/building.html).\n\n**If you find QuPath useful in work that you publish, please [_cite the publication_](https://qupath.readthedocs.io/en/latest/docs/intro/citing.html)!**\n\n*QuPath is an academic project intended for research use only.*\n*The software has been made freely available under the terms of the [GPLv3](https://github.com/qupath/qupath/blob/main/LICENSE) in the hope it is useful for this purpose, and to make analysis methods open and transparent.*\n\n\n## Development & support\n\nQuPath is being actively developed at the University of Edinburgh by:\n\n* [Pete Bankhead](https://github.com/petebankhead) (creator)\n* [Fiona Inglis](https://github.com/finglis)\n* [Alan O'Callaghan](https://github.com/alanocallaghan)\n* [Léo Leplat](https://github.com/Rylern)\n* [Laura Nicolás-Sáenz](https://github.com/lauranicolass)\n\nPast QuPath dev team members:\n\n* Melvin Gelbard\n* Mahdi Lamb\n\nFor all contributors, see [here](https://github.com/qupath/qupath/graphs/contributors).\n\nThis work is made possible in part thanks to funding from:\n\n* [Wellcome Trust Technology Development Grant](https://wellcome.org/grant-funding/people-and-projects/grants-awarded/qupath-advanced-open-platform-next-generation) (2022-Present)\n* [CZI Essential Open Source Software for Science, Cycle 4](https://chanzuckerberg.com/eoss/proposals/qupath-boosting-bioimage-analysis-for-users-developers/) (2022-Present)\n* [CZI Essential Open Source Software for Science, Cycle 1](https://chanzuckerberg.com/eoss/proposals/qupath-open-source-bioimage-analysis-and-quantitative-pathology/) (2020-2022)\n* Wellcome Trust / University of Edinburgh Institutional Strategic Support Fund (ISSF3) (2019-2020)\n\n\n----\n\n## Background\n\nQuPath was first designed, implemented and documented by Pete Bankhead while at Queen's University Belfast, with additional code and testing by Jose Fernandez.\n\nVersions up to v0.1.2 are copyright 2014-2016 The Queen's University of Belfast, Northern Ireland.\nThese were written as part of projects that received funding from:\n\n* Invest Northern Ireland (RDO0712612)\n* Cancer Research UK Accelerator (C11512/A20256)\n\n\n![Image](https://raw.githubusercontent.com/wiki/qupath/qupath/images/qupath_demo.jpg)\n",1007,whole-slide-imaging,Java,5,Java,Assembly,CSS,Shell,Roff,,,,,,,,,,,,,,,,,,,,,,,,772,57,705,10,3,17,3,98864,271,783,744,39,96c452d1fe643d09bd1127fd3559219d81d6e7f7,Increment openslide version (#1557),2024-07-05T17:24:33Z,Alan O'Callaghan,alan.ocallaghan@outlook.com,alanocallaghan,v0.5.1,"## QuPath v0.5.1 is now available!\r\n\r\nThis is a **minor update** that is intended to be fully compatible with [v0.5.0](https://github.com/qupath/qupath/releases/tag/v0.5.0) while fixing bugs.\r\n\r\nTo see what it includes, check out the **[changelog here](https://github.com/qupath/qupath/blob/main/CHANGELOG.md)**.\r\n\r\n## How to help\r\n\r\n**Please remember to [cite the QuPath paper in any publications that use the software](https://qupath.readthedocs.io/en/stable/docs/intro/citing.html)!**\r\n\r\n**Please use the [Scientific Community Image Forum](http://forum.image.sc/tag/qupath) for QuPath questions & discussions!**\r\n\r\n## What to download\r\n* For **Windows** (two options, functionally the same)\r\n  * [`QuPath-v0.5.1-Windows.msi`](https://github.com/qupath/qupath/releases/download/v0.5.1/QuPath-v0.5.1-Windows.msi) - if you want a standard Windows (local) installer\r\n  * [`QuPath-v0.5.1-Windows.zip`](https://github.com/qupath/qupath/releases/download/v0.5.1/QuPath-v0.5.1-Windows.zip) - unzip it and double-click QuPath-v0.5.1.exe (no further installation needed)\r\n* For **Mac** (two options, depending upon processor)\r\n  * [`QuPath-v0.5.1-Mac-x64.pkg`](https://github.com/qupath/qupath/releases/download/v0.5.1/QuPath-v0.5.1-Mac-x64.pkg) - for Macs with Intel Processors *or* Apple Silicon (M1/M2)\r\n  *  [`QuPath-v0.5.1-Mac-arm64.pkg`](https://github.com/qupath/qupath/releases/download/v0.5.1/QuPath-v0.5.1-Mac-arm64.pkg) - for Macs using Apple Silicon. This runs faster, but lacks support for a small number of file formats through Bio-Formats (particularly .czi with jpeg-xr compression).\r\n     * To install: right-click and choose *Open* to install.\r\n* For **Linux**\r\n  * [`QuPath-v0.5.1-Linux.tar.xz`](https://github.com/qupath/qupath/releases/download/v0.5.1/QuPath-v0.5.1-Linux.tar.xz) - use `chmod u+x /path/to/QuPath/bin/QuPath` to make the launcher executable.",v0.5.1,,,github-actions[bot],GNU General Public License v3.0,qupath,qupath,40,bioimage-informatics,image-processing,digital-pathology,pathology,machine-learning,cell-segmentation,whole-slide-imaging,bioimage-analysis,cell-analysis,imagej,opencv,groovy,histology,javafx,tissue-microarray-analysis,computational-pathology,,,,,/qupath/qupath,41,57,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/quinoacomputing/quinoa,https://github.com/quinoacomputing/quinoa,1,,,1,1,1,1,0,0,0,0,0,0,1,Adaptive computational fluid dynamics,"## Quinoa\n\n<img src=""https://quinoacomputing.github.io/quinoa.svg"" align=""right"" width=""25%"" background=transparent>\n\n_Adaptive computational fluid dynamics_ - https://quinoacomputing.github.io\n\nQuinoa is a set of computational tools that enables research and numerical\nanalysis in fluid dynamics. Using the [Charm++](http://charmplusplus.org)\nruntime system, we employ _asynchronous_ (or non-blocking) parallel programming\nand decompose computational problems into a large number of work units (that may\nbe more than the available number of processors) enabling _arbitrary\noverlap_ of parallel computation, communication, input, and output. Then the\nruntime system _dynamically_ and _automatically_ homogenizes computational load\nacross the simulation distributed across many computers.\n\nOur ultimate goal is to simulate large and complex engineering multiphysics\nproblems with a production-quality code that is extensible and maintainable,\nusing hardware resources efficiently, even for problems with _a priori_\nunknown, heterogeneous, and dynamic load distribution.\n\nThis software has been acknowledged by the U.S. Department of Energy / National\nNuclear Security Administration for open source release, O4694.\n\nFor more details on philosophy, documentation, software design, journal papers,\nlicense, and contributing see the [documentation](https://quinoacomputing.github.io).\n",100,fluid-dynamics,C++,6,CMake,Shell,C++,C,Dockerfile,Lua,,,,,,,,,,,,,,,,,,,,,,,445,36,408,1,21,17,66,233321,21,177,170,7,c08ef2adaecf7fdf972403184491b040fe01b3a8,reset state for unbounded volfrac material,2024-07-18T03:07:36Z,adityakpandare,apandare@lanl.gov,adityakpandare,,,,,,,Other,quinoa,quinoacomputing,7,fluid-dynamics,charmplusplus,cpp,parallel-computing,finite-element-methods,asynchronous-tasks,hydrodynamics,quinoa,continuous-galerkin,discontinuous-galerkin,flux-corrected-transport,load-balancing,adaptive-refinement,simulation,,,,,,,/quinoacomputing/quinoa,7,11,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/quatrope/astroalign,https://github.com/quatrope/astroalign,0,,0,0,1,0,0,0,0,1,0,0,0,0,A tool to align astronomical images based on asterism matching,"![logo](docs/images/logo-inline-dark-light.png)\n\n***\n\n[![QuatroPe](https://img.shields.io/badge/QuatroPe-Applications-1c5896)](https://quatrope.github.io/)\n[![unit tests](https://github.com/quatrope/astroalign/actions/workflows/aa-ci.yml/badge.svg?branch=master)](https://github.com/quatrope/astroalign/actions/workflows/aa-ci.yml)\n[![Coverage](https://codecov.io/github/quatrope/astroalign/coverage.svg?branch=master)](https://codecov.io/github/quatrope/astroalign)\n[![Documentation Status](https://readthedocs.org/projects/astroalign/badge/?version=latest)](http://astroalign.readthedocs.org/en/latest/?badge=latest)\n[![PyPI](https://img.shields.io/pypi/v/astroalign)](https://pypi.org/project/astroalign/)\n![PyPI - Downloads](https://img.shields.io/pypi/dm/astroalign)\n[![ascl:1906.001](https://img.shields.io/badge/ascl-1906.001-blue.svg?colorB=262255)](http://ascl.net/1906.001)\n\n\n**ASTROALIGN** is a python module that will try to align two stellar astronomical images, especially when there is no WCS information available.\n\nIt does so by finding similar 3-point asterisms (triangles) in both images and deducing the affine transformation between them.\n\nGeneric registration routines try to match feature points, using corner\ndetection routines to make the point correspondence.\nThese generally fail for stellar astronomical images, since stars have very\nlittle stable structure and so, in general, indistinguishable from each other.\nAsterism matching is more robust, and closer to the human way of matching stellar images.\n\nAstroalign can match images of very different field of view, point-spread function, seeing and atmospheric conditions.\n\nIt may not work, or work with special care, on images of extended objects with few point-like sources or in very crowded fields.\n\nYou can find a Jupyter notebook example with the main features at [http://quatrope.github.io/astroalign/](http://quatrope.github.io/astroalign/).\n\n**Full documentation:** https://astroalign.readthedocs.io/\n\n# Installation\n\nUsing setuptools:\n\n```bash\n$ pip install astroalign\n```\n\nor from this distribution with\n\n```bash\n$ python setup.py install\n```\n\n## Performance: Optional\n\nThis library is optionally compatible with [bottleneck](https://github.com/pydata/bottleneck) and may offer performance improvements in some cases.\nInstall bottleneck in your project as a peer to astroalign using:\n\n```bash\npip install bottleneck\n```\n\n`Astroalign` will pick this optional dependency up and use it's performance improved functions for computing transforms.\n\n## Running Tests\n\n```bash\npython tests/test_align.py\n```\n\n# Usage example\n\n```\n>>> import astroalign as aa\n>>> aligned_image, footprint = aa.register(source_image, target_image)\n```\n\nIn this example `source_image` will be interpolated by a transformation to coincide pixel to pixel with `target_image` and stored in `aligned_image`.\n\nIf we are only interested in knowing the transformation and the correspondence of control points in both images, use `find_transform` will return the transformation in a [Scikit-Image](https://scikit-image.org/) `SimilarityTransform` object and a list of stars in source with the corresponding stars in target.\n\n```\n>>> transf, (s_list, t_list) = aa.find_transform(source, target)\n```\n\n`source` and `target` can each either be the numpy array of the image (grayscale or color),\nor an iterable of (x, y) pairs of star positions on the image.\n\nThe returned `transf` object is a scikit-image [`SimilarityTranform`](http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.SimilarityTransform) object that contains the transformation matrix along with the scale, rotation and translation parameters.\n\n`s_list` and `t_list` are numpy arrays of (x, y) point correspondence between `source` and `target`. `transf` applied to `s_list` will approximately render `t_list`.\n\n# Related Software\n\nThere are other related software that may offer similar functionality as astroalign.\nThis list is not exhaustive and may be others.\n\n* [astrometry.net](https://github.com/dstndstn/astrometry.net)\n* [reproject](https://github.com/astropy/reproject)\n* [Watney Astrometry Engine](https://github.com/Jusas/WatneyAstrometry)\n* [Stellar Solver](https://github.com/rlancaste/stellarsolver)\n* [THRASTRO](https://github.com/THRASTRO/astrometrylib)\n* [Montage](https://github.com/Caltech-IPAC/Montage)\n* [Aafitrans](https://github.com/prajwel/aafitrans)\n* [astrometry](https://github.com/neuromorphicsystems/astrometry)\n\n# Citation\n\nIf you use astroalign in a scientific publication, we would appreciate citations to the following [paper](https://www.sciencedirect.com/science/article/pii/S221313372030038X):\n\n    Astroalign: A Python module for astronomical image registration.\n    Beroiz, M., Cabral, J. B., & Sanchez, B.\n    Astronomy & Computing, Volume 32, July 2020, 100384.\n\n***\n\nTOROS Dev Team\n\n<martinberoiz@gmail.com>\n",136,astronomy,Python,1,Python,,,,,,,,,,,,,,,,,,,,,,,,,,,,12,5,6,1,10,6,3,38341,42,79,66,13,6aba41be74b5f8aebf905f2c6d28636f1cdd996f,Update tutorial.rst,2024-02-28T02:29:10Z,Mandeep Gill,me@mandeep.org,mssgill,Version 2.5.0,In this release:\r\n\r\n- Mask in source or target image passes over the mask to source detection. Now is possible to exclude regions outside the region of interest ROI to find the match between images.\r\n- Use of numpy's new random number generator API. This requires numpy version to be at least 1.17.\r\n- Updates to README.md.\r\n,v2.5.0,Martin Beroiz,,martinberoiz,MIT License,astroalign,quatrope,19,astronomy,alignment,registration,asterism-matching,stellar-astronomical-images,transformations,,,,,,,,,,,,,,,/quatrope/astroalign,26,15,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/qojulia/QuantumOptics.jl,https://github.com/qojulia/QuantumOptics.jl,0,,0,0,1,1,1,0,0,1,0,0,0,0,Library for the numerical simulation of closed as well as open quantum systems.,"<img src=""https://github.com/qojulia/QuantumOptics.jl-website/blob/master/src/images/logo.png"" alt=""QuantumOptics.jl"" width=""400"">\n\n#\n\n[![Chat on Gitter][gitter-img]][gitter-url] ![Build_state](https://github.com/qojulia/QuantumOptics.jl/workflows/CI/badge.svg) [![Stable documentation][docs-img]][docs-url]\n\n**QuantumOptics.jl** is a numerical framework written in [Julia] that makes it easy to simulate various kinds of quantum systems. It is inspired by the [Quantum Optics Toolbox] for MATLAB and the Python framework [QuTiP].\n\nMore information, documentation and examples can be found on our website http://qojulia.org.\n\n\n**Latest release:**\n  * Version: [![Latest version tag][version-img]][version-url]\n  * Test coverage:\n        [![Test coverage status on codecov][codecov-img]][codecov-url]\n\n\n### Project structure\n\nThe source content associated with **QuantumOptics.jl** is distributed over several repositories under the [qojulia] organization on github:\n\n* The main code: https://github.com/qojulia/QuantumOptics.jl\n* Documentation: https://github.com/qojulia/QuantumOptics.jl-documentation\n* Examples: https://github.com/qojulia/QuantumOptics.jl-examples\n* Benchmarks: https://github.com/qojulia/QuantumOptics.jl-benchmarks\n* Website: https://github.com/qojulia/QuantumOptics.jl-website\n\n\n### Questions & Contributions\n\nIf you have any questions or need help, hop on our [gitter channel](https://gitter.im/QuantumOptics-jl/Lobby?source=orgpage) and ask away. Also, contributions of any kind are always welcome! Be it as ideas for new features, bug reports or, our favorite case, sending pull requests.\n\n### Citing\n\nIf you like **QuantumOptics.jl**, we would appreciate it if you starred the repository in order to help us increase its visibility. Furthermore, if you find the framework useful in your research, we would be grateful if you could cite our [publication](https://www.sciencedirect.com/science/article/pii/S0010465518300328) using the following bibtex entry:\n\n```bib\n@article{kramer2018quantumoptics,\n  title={QuantumOptics. jl: A Julia framework for simulating open quantum systems},\n  author={Kr{\""a}mer, Sebastian and Plankensteiner, David and Ostermann, Laurin and Ritsch, Helmut},\n  journal={Computer Physics Communications},\n  volume={227},\n  pages={109--116},\n  year={2018},\n  publisher={Elsevier}\n}\n```\n\n[Julia]: http://julialang.org\n[qojulia]: https://github.com/qojulia\n[Quantum Optics Toolbox]: http://qo.phy.auckland.ac.nz/toolbox\n[QuTiP]: http://qutip.org\n\n[codecov-url]: https://codecov.io/gh/qojulia/QuantumOptics.jl\n[codecov-img]: https://codecov.io/gh/qojulia/QuantumOptics.jl/branch/master/graph/badge.svg\n\n[gitter-url]: https://gitter.im/QuantumOptics-jl/Lobby\n[gitter-img]: https://img.shields.io/gitter/room/nwjs/nw.js.svg\n\n[docs-url]: https://docs.qojulia.org/\n[docs-img]: https://img.shields.io/badge/docs-stable-blue.svg\n\n[version-url]: https://github.com/qojulia/QuantumOptics.jl/releases\n[version-img]: https://img.shields.io/github/release/qojulia/QuantumOptics.jl.svg\n",519,quantum-mechanics,Julia,1,Julia,,,,,,,,,,,,,,,,,,,,,,,,,,,,275,23,248,4,15,31,6,5157,101,126,98,28,7e722261d8acd329e2b3aa82bc6262fbe38aba86,Update Project.toml,2024-07-10T17:39:33Z,Ashley Milsted,ashmilsted@gmail.com,amilsted,v1.1.1,## QuantumOptics v1.1.1\n\n[Diff since v1.1.0](https://github.com/qojulia/QuantumOptics.jl/compare/v1.1.0...v1.1.1)\n\n\n**Merged pull requests:**\n- Don't tuplify time-dependent ops if types are homogenous (#401) (@amilsted),v1.1.1,,,github-actions[bot],Other,QuantumOptics.jl,qojulia,49,julia,quantum,quantum-mechanics,quantum-optics,quantum-toolbox,,,,,,,,,,,,,,,,/qojulia/QuantumOptics.jl,56,26,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/PyPSA/pypsa-eur,https://github.com/PyPSA/pypsa-eur,0.5,One of snakemake workflows,0,0,1,0,1,0,0,0,0,0,0,0,PyPSA-Eur: A Sector-Coupled Open Optimisation Model of the European Energy System,"<!--\nSPDX-FileCopyrightText: 2017-2024 The PyPSA-Eur Authors\nSPDX-License-Identifier: CC-BY-4.0\n-->\n\n![GitHub release (latest by date including pre-releases)](https://img.shields.io/github/v/release/pypsa/pypsa-eur?include_prereleases)\n[![Build Status](https://github.com/pypsa/pypsa-eur/actions/workflows/ci.yaml/badge.svg)](https://github.com/PyPSA/pypsa-eur/actions)\n[![Documentation](https://readthedocs.org/projects/pypsa-eur/badge/?version=latest)](https://pypsa-eur.readthedocs.io/en/latest/?badge=latest)\n![Size](https://img.shields.io/github/repo-size/pypsa/pypsa-eur)\n[![Zenodo PyPSA-Eur](https://zenodo.org/badge/DOI/10.5281/zenodo.3520874.svg)](https://doi.org/10.5281/zenodo.3520874)\n[![Zenodo PyPSA-Eur-Sec](https://zenodo.org/badge/DOI/10.5281/zenodo.3938042.svg)](https://doi.org/10.5281/zenodo.3938042)\n[![Snakemake](https://img.shields.io/badge/snakemake-≥7.7.0-brightgreen.svg?style=flat)](https://snakemake.readthedocs.io)\n[![REUSE status](https://api.reuse.software/badge/github.com/pypsa/pypsa-eur)](https://api.reuse.software/info/github.com/pypsa/pypsa-eur)\n[![Stack Exchange questions](https://img.shields.io/stackexchange/stackoverflow/t/pypsa)](https://stackoverflow.com/questions/tagged/pypsa)\n\n# PyPSA-Eur: A Sector-Coupled Open Optimisation Model of the European Energy System\n\nPyPSA-Eur is an open model dataset of the European energy system at the\ntransmission network level that covers the full ENTSO-E area. The model is suitable both for operational studies and generation and transmission expansion planning studies.\nThe continental scope and highly resolved spatial scale enables a proper description of the long-range\nsmoothing effects for renewable power generation and their varying resource availability.\n\n\n\n\nThe model is described in the [documentation](https://pypsa-eur.readthedocs.io)\nand in the paper\n[PyPSA-Eur: An Open Optimisation Model of the European Transmission\nSystem](https://arxiv.org/abs/1806.01613), 2018,\n[arXiv:1806.01613](https://arxiv.org/abs/1806.01613).\nThe model building routines are defined through a snakemake workflow.\nPlease see the [documentation](https://pypsa-eur.readthedocs.io/)\nfor installation instructions and other useful information about the snakemake workflow.\nThe model is designed to be imported into the open toolbox\n[PyPSA](https://github.com/PyPSA/PyPSA).\n\n**WARNING**: PyPSA-Eur is under active development and has several\n[limitations](https://pypsa-eur.readthedocs.io/en/latest/limitations.html) which\nyou should understand before using the model. The github repository\n[issues](https://github.com/PyPSA/pypsa-eur/issues) collect known topics we are\nworking on (please feel free to help or make suggestions). The\n[documentation](https://pypsa-eur.readthedocs.io/) remains somewhat patchy. You\ncan find showcases of the model's capabilities in the Joule paper [The potential\nrole of a hydrogen network in\nEurope](https://doi.org/10.1016/j.joule.2023.06.016), another [paper in Joule\nwith a description of the industry\nsector](https://doi.org/10.1016/j.joule.2022.04.016), or in [a 2021 presentation\nat EMP-E](https://nworbmot.org/energy/brown-empe.pdf). We do not recommend to\nuse the full resolution network model for simulations. At high granularity the\nassignment of loads and generators to the nearest network node may not be a\ncorrect assumption, depending on the topology of the underlying distribution\ngrid, and local grid bottlenecks may cause unrealistic load-shedding or\ngenerator curtailment. We recommend to cluster the network to a couple of\nhundred nodes to remove these local inconsistencies. See the discussion in\nSection 3.4 ""Model validation"" of the paper.\n\n\n![PyPSA-Eur Grid Model](doc/img/elec.png)\n\nThe dataset consists of:\n\n- A grid model based on a modified [GridKit](https://github.com/bdw/GridKit)\n  extraction of the [ENTSO-E Transmission System\n  Map](https://www.entsoe.eu/data/map/). The grid model contains 7072 lines\n  (alternating current lines at and above 220kV voltage level and all high\n  voltage direct current lines) and 3803 substations.\n- The open power plant database\n  [powerplantmatching](https://github.com/FRESNA/powerplantmatching).\n- Electrical demand time series from the\n  [OPSD project](https://open-power-system-data.org/).\n- Renewable time series based on ERA5 and SARAH, assembled using the [atlite tool](https://github.com/FRESNA/atlite).\n- Geographical potentials for wind and solar generators based on land use (CORINE) and excluding nature reserves (Natura2000) are computed with the [atlite library](https://github.com/PyPSA/atlite).\n\nA sector-coupled extension adds demand\nand supply for the following sectors: transport, space and water\nheating, biomass, industry and industrial feedstocks, agriculture,\nforestry and fishing. This completes the energy system and includes\nall greenhouse gas emitters except waste management and land use.\n\nThis diagram gives an overview of the sectors and the links between\nthem:\n\n![sector diagram](doc/img/multisector_figure.png)\n\nEach of these sectors is built up on the transmission network nodes\nfrom [PyPSA-Eur](https://github.com/PyPSA/pypsa-eur):\n\n![network diagram](https://github.com/PyPSA/pypsa-eur/blob/master/doc/img/base.png?raw=true)\n\nFor computational reasons the model is usually clustered down\nto 50-200 nodes.\n\nAlready-built versions of the model can be found in the accompanying [Zenodo\nrepository](https://doi.org/10.5281/zenodo.3601881).\n\n# Contributing and Support\nWe strongly welcome anyone interested in contributing to this project. If you have any ideas, suggestions or encounter problems, feel invited to file issues or make pull requests on GitHub.\n-   In case of code-related **questions**, please post on [stack overflow](https://stackoverflow.com/questions/tagged/pypsa).\n-   For non-programming related and more general questions please refer to the [mailing list](https://groups.google.com/group/pypsa).\n-   To **discuss** with other PyPSA users, organise projects, share news, and get in touch with the community you can use the [discord server](https://discord.com/invite/AnuJBk23FU).\n-   For **bugs and feature requests**, please use the [PyPSA-Eur Github Issues page](https://github.com/PyPSA/pypsa-eur/issues).\n\n# Licence\n\nThe code in PyPSA-Eur is released as free software under the\n[MIT License](https://opensource.org/licenses/MIT), see [`doc/licenses.rst`](doc/licenses.rst).\nHowever, different licenses and terms of use may apply to the various\ninput data.\n",310,snakemake,Python,2,Python,Shell,,,,,,,,,,,,,,,,,,,,,,,,,,,739,98,605,36,182,59,0,107286,213,427,296,131,3fba8dae3ee5aa98b3eaad038acc3e1445620749,[pre-commit.ci] pre-commit autoupdate (#1157),2024-07-16T06:08:46Z,pre-commit-ci[bot],66853113+pre-commit-ci[bot]@users.noreply.github.com,pre-commit-ci[bot],v0.11.0,"## What's Changed\r\n* Fix broken link to mamba installation guide in docs by @lumbric in https://github.com/PyPSA/pypsa-eur/pull/941\r\n* [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/PyPSA/pypsa-eur/pull/942\r\n* prepare_sector: automatically interpolate in config get() function by @fneum in https://github.com/PyPSA/pypsa-eur/pull/943\r\n* Allow absence of offshore wind from sector-coupled networks by @koen-vg in https://github.com/PyPSA/pypsa-eur/pull/944\r\n* adjust AC bus to low voltage by @lisazeyen in https://github.com/PyPSA/pypsa-eur/pull/948\r\n* Allow CPLEX for MIQP in cluster_network by @aodenweller in https://github.com/PyPSA/pypsa-eur/pull/949\r\n* change technology data version to 0.8.1 by @lisazeyen in https://github.com/PyPSA/pypsa-eur/pull/954\r\n* Eurostat 2023 data for energy totals by @toniseibold in https://github.com/PyPSA/pypsa-eur/pull/947\r\n* compute lifetime after grouping DateIn by @lindnemi in https://github.com/PyPSA/pypsa-eur/pull/958\r\n* Compatibility with `snakemake>=8` by @fneum in https://github.com/PyPSA/pypsa-eur/pull/825\r\n* Add check for turning off transmission expnasion if limit reached by @koen-vg in https://github.com/PyPSA/pypsa-eur/pull/952\r\n* Fix duplicated years in add_land_use_constraint_m by @tgi-climact in https://github.com/PyPSA/pypsa-eur/pull/968\r\n* remove copy_config rule and write config for each solved network by @fneum in https://github.com/PyPSA/pypsa-eur/pull/965\r\n* Sweep across multiple weather years by @fneum in https://github.com/PyPSA/pypsa-eur/pull/204\r\n* [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/PyPSA/pypsa-eur/pull/977\r\n* test.sh: set continuous chain of && commands and print out commands by @FabianHofmann in https://github.com/PyPSA/pypsa-eur/pull/976\r\n* prepare_perfect: ensure network.meta assignment by @FabianHofmann in https://github.com/PyPSA/pypsa-eur/pull/974\r\n* Only sanitize locations when there are buses with a location by @koen-vg in https://github.com/PyPSA/pypsa-eur/pull/971\r\n* Allow the selection of custom fork of technology-data by @koen-vg in https://github.com/PyPSA/pypsa-eur/pull/970\r\n* Don't use log path function in IRENA retrieval rule by @koen-vg in https://github.com/PyPSA/pypsa-eur/pull/969\r\n* Miscellaneous bugfixes by @koen-vg in https://github.com/PyPSA/pypsa-eur/pull/980\r\n* Use raw strings to avoid illegal backslash warnings python 12 by @koen-vg in https://github.com/PyPSA/pypsa-eur/pull/981\r\n* replace value of shadow in snakemake rules by @p-glaum in https://github.com/PyPSA/pypsa-eur/pull/979\r\n* snakefile: move copy_default_files and process_run_config to helpers by @FabianHofmann in https://github.com/PyPSA/pypsa-eur/pull/978\r\n* scenario management: reenable shared resources in one folder by @FabianHofmann in https://github.com/PyPSA/pypsa-eur/pull/975\r\n* Fix typo in reading input to build_sequestration_potentials rule by @koen-vg in https://github.com/PyPSA/pypsa-eur/pull/983\r\n* disable windows machines in CI by @FabianHofmann in https://github.com/PyPSA/pypsa-eur/pull/984\r\n* add draft HVDC projects from TYNDP 2024 by @fneum in https://github.com/PyPSA/pypsa-eur/pull/982\r\n* Fill in missing eurostat data on domestic aviation energy demand by @koen-vg in https://github.com/PyPSA/pypsa-eur/pull/973\r\n* Fix type error in cluster_network with ""m"" configuration by @tgi-climact in https://github.com/PyPSA/pypsa-eur/pull/986\r\n* cluster_network: ensure correct indexing of weights by @FabianHofmann in https://github.com/PyPSA/pypsa-eur/pull/988\r\n* scenario management: fix shared resources by @FabianHofmann in https://github.com/PyPSA/pypsa-eur/pull/989\r\n* Check if scenario file exists before attempting to load by @koen-vg in https://github.com/PyPSA/pypsa-eur/pull/993\r\n* postprocess: fix typo in benchmark folder by @FabianHofmann in https://github.com/PyPSA/pypsa-eur/pull/994\r\n* provide Path object as input to ConfigSettings by @lindnemi in https://github.com/PyPSA/pypsa-eur/pull/995\r\n* adjust resources perfect foresight and solver settings by @lisazeyen in https://github.com/PyPSA/pypsa-eur/pull/1000\r\n* [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/PyPSA/pypsa-eur/pull/999\r\n* Fix error with symbol of buses in simplify_network by @tgi-climact in https://github.com/PyPSA/pypsa-eur/pull/987\r\n* Fix grouping year reference in add_land_use_constraint_m by @tgi-climact in https://github.com/PyPSA/pypsa-eur/pull/991\r\n* Drop renewables by @lisazeyen in https://github.com/PyPSA/pypsa-eur/pull/1001\r\n* Rename existing capacities by @lisazeyen in https://github.com/PyPSA/pypsa-eur/pull/1002\r\n* Fix typo by @tgi-climact in https://github.com/PyPSA/pypsa-eur/pull/1005\r\n* Fix custom busmap read in cluster network by @tgi-climact in https://github.com/PyPSA/pypsa-eur/pull/1008\r\n* [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/PyPSA/pypsa-eur/pull/1007\r\n* Add prefix scenario management by @lindnemi in https://github.com/PyPSA/pypsa-eur/pull/1011\r\n* Correct co2 potentials by @lisazeyen in https://github.com/PyPSA/pypsa-eur/pull/1010\r\n* Reform grouping year logic by @lisazeyen in https://github.com/PyPSA/pypsa-eur/pull/1019\r\n* skip heat bus for CHPs in places where no central heating by @fneum in https://github.com/PyPSA/pypsa-eur/pull/1021\r\n* Update to BAU constraint formulation in function add_BAU_constraints by @SermishaNarayana in https://github.com/PyPSA/pypsa-eur/pull/1024\r\n* Add to documentation (Hackathon 4/10+11) by @chrstphtrs in https://github.com/PyPSA/pypsa-eur/pull/1014\r\n* Cleaning up data and resources by @toniseibold in https://github.com/PyPSA/pypsa-eur/pull/1020\r\n* [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/PyPSA/pypsa-eur/pull/1028\r\n* Climact feature/fix add land use constraint m by @lisazeyen in https://github.com/PyPSA/pypsa-eur/pull/1029\r\n* Store network shapes by @FabianHofmann in https://github.com/PyPSA/pypsa-eur/pull/1013\r\n* Remove connection costs output by @martacki in https://github.com/PyPSA/pypsa-eur/pull/1031\r\n* Fix double space in existing capacities by @koen-vg in https://github.com/PyPSA/pypsa-eur/pull/1039\r\n* reenable windows ci with snakemake 8.11 by @fneum in https://github.com/PyPSA/pypsa-eur/pull/1040\r\n* [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/PyPSA/pypsa-eur/pull/1044\r\n* Use powerplantmatching IRENASTAT for renewable capacities in `add_existing_baseyear` by @koen-vg in https://github.com/PyPSA/pypsa-eur/pull/1018\r\n* update env fixed by @FabianHofmann in https://github.com/PyPSA/pypsa-eur/pull/1048\r\n* Bugfix: integrate `build_bus_regions` into `base_network` by @koen-vg in https://github.com/PyPSA/pypsa-eur/pull/1051\r\n* Fix typo by @tgi-climact in https://github.com/PyPSA/pypsa-eur/pull/1045\r\n* Fix hydropower and load bugs by @joph in https://github.com/PyPSA/pypsa-eur/pull/1054\r\n* move all graphics to doc/img by @fneum in https://github.com/PyPSA/pypsa-eur/pull/1052\r\n* update and reduce databundle size by @fneum in https://github.com/PyPSA/pypsa-eur/pull/1027\r\n* Add floating wind technology by @p-glaum in https://github.com/PyPSA/pypsa-eur/pull/773\r\n* Clarify suffix usage in add existing baseyear by @tgi-climact in https://github.com/PyPSA/pypsa-eur/pull/1017\r\n* stop using `{sector_opts}`  wildcard by default by @fneum in https://github.com/PyPSA/pypsa-eur/pull/1058\r\n* bump powerplantmatching to 0.5.15 by @fneum in https://github.com/PyPSA/pypsa-eur/pull/1057\r\n* Add calculate_nodal_supply_energy in make summary by @tgi-climact in https://github.com/PyPSA/pypsa-eur/pull/1046\r\n* rename Greece iso-code by @lisazeyen in https://github.com/PyPSA/pypsa-eur/pull/1061\r\n* Exclude shared resources master by @fneum in https://github.com/PyPSA/pypsa-eur/pull/1059\r\n* Improve handling of plastics (for `master`) by @fneum in https://github.com/PyPSA/pypsa-eur/pull/1060\r\n* Fix gas network retrofit in brownfield by @tgi-climact in https://github.com/PyPSA/pypsa-eur/pull/1036\r\n* [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/PyPSA/pypsa-eur/pull/1063\r\n* post-discretization of lines and links by @fneum in https://github.com/PyPSA/pypsa-eur/pull/1064\r\n* Adding solar tracking single axis by @lisazeyen in https://github.com/PyPSA/pypsa-eur/pull/1066\r\n* Land transport fix by @lisazeyen in https://github.com/PyPSA/pypsa-eur/pull/957\r\n* Enhance Carbon budget distribution plot by @Parisra in https://github.com/PyPSA/pypsa-eur/pull/1070\r\n* fix_DC_cluster_issue by @p-glaum in https://github.com/PyPSA/pypsa-eur/pull/1067\r\n* Split out time aggregation to its own rule by @koen-vg in https://github.com/PyPSA/pypsa-eur/pull/1065\r\n* Modification to function ""add_operational_reserve_margin"" in solve_ne… by @SermishaNarayana in https://github.com/PyPSA/pypsa-eur/pull/1071\r\n* Fixing energy totals rescale function by @toniseibold in https://github.com/PyPSA/pypsa-eur/pull/990\r\n* Improve agg_p_nom_limits configuration by @tgi-climact in https://github.com/PyPSA/pypsa-eur/pull/1023\r\n* Fix non steel related coal demand during transition (using sector_ratios_fraction_future) by @tgi-climact in https://github.com/PyPSA/pypsa-eur/pull/1047\r\n* Fix fill missing in industry sector ratios intermediate by @tgi-climact in https://github.com/PyPSA/pypsa-eur/pull/1004\r\n* Define methanol energy demand for industry by @tgi-climact in https://github.com/PyPSA/pypsa-eur/pull/1068\r\n* Fixing biomass transport cost by @yerbol-akhmetov in https://github.com/PyPSA/pypsa-eur/pull/769\r\n* Fix disabling transmission limit for volume limit type by @koen-vg in https://github.com/PyPSA/pypsa-eur/pull/1076\r\n* Minor bugfixes for new time aggregation implementation by @koen-vg in https://github.com/PyPSA/pypsa-eur/pull/1075\r\n* update energy balances April 2023 link by @fneum in https://github.com/PyPSA/pypsa-eur/pull/1074\r\n* Update energy balance for residential based on new Eurostat data by @yerbol-akhmetov in https://github.com/PyPSA/pypsa-eur/pull/1025\r\n* avoid duplicate existing RES capacities (closes #1016) by @fneum in https://github.com/PyPSA/pypsa-eur/pull/1080\r\n* prepare release v0.11.0 by @fneum in https://github.com/PyPSA/pypsa-eur/pull/1081\r\n\r\n## New Contributors\r\n* @SermishaNarayana made their first contribution in https://github.com/PyPSA/pypsa-eur/pull/1024\r\n* @joph made their first contribution in https://github.com/PyPSA/pypsa-eur/pull/1054\r\n* @Parisra made their first contribution in https://github.com/PyPSA/pypsa-eur/pull/1070\r\n\r\n**Full Changelog**: https://github.com/PyPSA/pypsa-eur/compare/v0.10.0...v0.11.0",v0.11.0,Fabian Neumann,,fneum,,pypsa-eur,PyPSA,15,snakemake,energy,energy-system,power-systems,energy-model,pypsa,energy-system-model,energy-systems,,,,,,,,,,,,,/PyPSA/pypsa-eur,15,16,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/pypose/pypose,https://github.com/pypose/pypose,1,,,1,0,1,0,0,0,1,0,0,0,0,A library for differentiable robotics.,"## PyPose: A Library for Robot Learning with Physics-based Optimization\n\n![robot](https://user-images.githubusercontent.com/8695500/193484553-2da66824-4461-4aca-ad8c-b17c05bef067.png)\n\n-----\n\nDeep learning has had remarkable success in robotic perception, but its data-centric nature suffers when it comes to generalizing to ever-changing environments. By contrast, physics-based optimization generalizes better, but it does not perform as well in complicated tasks due to the lack of high-level semantic information and the reliance on manual parametric tuning. To take advantage of these two complementary worlds, we present PyPose: a **robotics-oriented**, **PyTorch-based** library that combines **deep perceptual models** with **physics-based optimization techniques**. Our design goal for PyPose is to make it **user-friendly**, **efficient**, and **interpretable** with a tidy and well-organized architecture. Using an **imperative style interface**, it can be easily integrated into **real-world robotic applications**. \n\n\n-----\n\n### Current Features\n\n##### [LieTensor](https://pypose.org/docs/main/modules/)\n\n- Lie group: [`SO3`](https://pypose.org/docs/main/generated/pypose.SO3/), [`SE3`](https://pypose.org/docs/main/generated/pypose.SE3/), [`Sim3`](https://pypose.org/docs/main/generated/pypose.Sim3/), [`RxSO3`](https://pypose.org/docs/main/generated/pypose.RxSO3/)\n- Lie algebra: [`so3`](https://pypose.org/docs/main/generated/pypose.so3/), [`se3`](https://pypose.org/docs/main/generated/pypose.se3/), [`sim3`](https://pypose.org/docs/main/generated/pypose.sim3/), [`rxso3`](https://pypose.org/docs/main/generated/pypose.rxso3/)\n\n##### [Modules](https://pypose.org/docs/main/modules/)\n\n- System: [`LTI`](https://pypose.org/docs/main/generated/pypose.module.LTI), [`LTV`](https://pypose.org/docs/main/generated/pypose.module.LTV), [`NLS`](https://pypose.org/docs/main/generated/pypose.module.NLS)\n- Filter: [`EKF`](https://pypose.org/docs/main/generated/pypose.module.EKF/), [`UKF`](https://pypose.org/docs/main/generated/pypose.module.UKF/), [`PF`](https://pypose.org/docs/main/generated/pypose.module.PF/)\n- PnP Solver: [`EPnP`](https://pypose.org/docs/main/generated/pypose.module.EPnP/)\n- Linear Quadratic Regulator: [`LQR`](https://pypose.org/docs/main/generated/pypose.module.LQR/)\n- IMU Preintegration: [`IMUPreintegrator`](https://pypose.org/docs/main/generated/pypose.module.IMUPreintegrator/)\n- ......\n\n##### [Second-order Optimizers](https://pypose.org/docs/main/optim/)\n\n- [`GaussNewton`](https://pypose.org/docs/main/generated/pypose.optim.GaussNewton)\n- [`LevenbergMarquardt`](https://pypose.org/docs/main/generated/pypose.optim.LevenbergMarquardt/)\n- ......\n\nWant more features? [Create an issue here](https://github.com/pypose/pypose/issues) to request new features.\n\n##### PyPose is highly efficient and supports parallel computing for Jacobian of Lie group and Lie algebra. See following comparison.\n\n<img width=""1167"" alt=""image"" src=""https://user-images.githubusercontent.com/8695500/203210668-1a90224a-ae08-4d31-b9d1-e293be75ef3e.png"">\n\nEfficiency and memory comparison of batched Lie group operations (we take Theseus performance as 1×).\n\nMore information about efficiency comparison goes to [our paper for PyPose](https://arxiv.org/abs/2209.15428).\n\n## Getting Started\n    \n### Installation\n\n#### Install from **pypi**\n```bash\npip install pypose\n```\n\n#### Install from source\n\n1. Requirement:\n\nOn Ubuntu, macOS, or Windows, install [PyTorch](https://pytorch.org/), then run:\n\n```bash\npip install -r requirements/runtime.txt\n```\n\n2. Install locally:\n\n```bash\ngit clone  https://github.com/pypose/pypose.git\ncd pypose && python setup.py develop\n```\n\n3. Run tests\n\n```bash\npytest\n```\n\n####  For contributors\n\n1. Make sure the above installation is correct. \n\n2. Go to [CONTRIBUTING.md](CONTRIBUTING.md)\n\n\n#### Examples\n\n1. The following code sample shows how to rotate random points and compute the gradient of batched rotation.\n\n```python\n>>> import torch, pypose as pp\n\n>>> # A random so(3) LieTensor\n>>> r = pp.randn_so3(2, requires_grad=True)\n    so3Type LieTensor:\n    tensor([[ 0.1606,  0.0232, -1.5516],\n            [-0.0807, -0.7184, -0.1102]], requires_grad=True)\n\n>>> R = r.Exp() # Equivalent to: R = pp.Exp(r)\n    SO3Type LieTensor:\n    tensor([[ 0.0724,  0.0104, -0.6995,  0.7109],\n            [-0.0395, -0.3513, -0.0539,  0.9339]], grad_fn=<AliasBackward0>)\n\n>>> p = R @ torch.randn(3) # Rotate random point\n    tensor([[ 0.8045, -0.8555,  0.5260],\n            [ 0.3502,  0.8337,  0.9154]], grad_fn=<ViewBackward0>)\n\n>>> p.sum().backward()     # Compute gradient\n>>> r.grad                 # Print gradient\n    tensor([[-0.7920, -0.9510,  1.7110],\n            [-0.2659,  0.5709, -0.3855]])\n```\n\n2. This example shows how to estimate batched inverse of transform by a second-order optimizer. Two usage options for a `scheduler` are provided, each of which can work independently.\n\n```python\n>>> from torch import nn\n>>> import torch, pypose as pp\n>>> from pypose.optim import LM\n>>> from pypose.optim.strategy import Constant\n>>> from pypose.optim.scheduler import StopOnPlateau\n\n>>> class InvNet(nn.Module):\n\n        def __init__(self, *dim):\n            super().__init__()\n            init = pp.randn_SE3(*dim)\n            self.pose = pp.Parameter(init)\n\n        def forward(self, input):\n            error = (self.pose @ input).Log()\n            return error.tensor()\n\n>>> device = torch.device(""cuda"")\n>>> input = pp.randn_SE3(2, 2, device=device)\n>>> invnet = InvNet(2, 2).to(device)\n>>> strategy = Constant(damping=1e-4)\n>>> optimizer = LM(invnet, strategy=strategy)\n>>> scheduler = StopOnPlateau(optimizer, steps=10, patience=3, decreasing=1e-3, verbose=True)\n\n>>> # 1st option, full optimization\n>>> scheduler.optimize(input=input)\n\n>>> # 2nd option, step optimization\n>>> while scheduler.continual():\n        loss = optimizer.step(input)\n        scheduler.step(loss)\n\n>>> # Note: remove one of the above options for usage!\n```\n\nFor more usage, see [Documentation](https://pypose.org/docs). For more applications, see [Examples](https://github.com/pypose/pypose/tree/main/examples).\n\n## Citing PyPose\n\nIf you use PyPose, please cite the paper below. You may also [download it here](https://arxiv.org/abs/2209.15428).\n\n```bibtex\n@inproceedings{wang2023pypose,\n  title = {{PyPose}: A Library for Robot Learning with Physics-based Optimization},\n  author = {Wang, Chen and Gao, Dasong and Xu, Kuan and Geng, Junyi and Hu, Yaoyu and Qiu, Yuheng and Li, Bowen and Yang, Fan and Moon, Brady and Pandey, Abhinav and Aryan and Xu, Jiahe and Wu, Tianhao and He, Haonan and Huang, Daning and Ren, Zhongqiang and Zhao, Shibo and Fu, Taimeng and Reddy, Pranay and Lin, Xiao and Wang, Wenshan and Shi, Jingnan and Talak, Rajat and Cao, Kun and Du, Yi and Wang, Han and Yu, Huai and Wang, Shanzhao and Chen, Siyu and Kashyap, Ananth  and Bandaru, Rohan and Dantu, Karthik and Wu, Jiajun and Xie, Lihua and Carlone, Luca and Hutter, Marco and Scherer, Sebastian},\n  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year = {2023}\n}\n```\n\nMore papers describing PyPose:\n\n```bibtex\n@inproceedings{zhan2023pypose,\n  title = {{PyPose} v0.6: The Imperative Programming Interface for Robotics},\n  author = {Zitong Zhan and Xiangfu Li and Qihang Li and Haonan He and Abhinav Pandey and Haitao Xiao and Yangmengfei Xu and Xiangyu Chen and Kuan Xu and Kun Cao and Zhipeng Zhao and Zihan Wang and Huan Xu and Zihang Fang and Yutian Chen and Wentao Wang and Xu Fang and Yi Du and Tianhao Wu and Xiao Lin and Yuheng Qiu and Fan Yang and Jingnan Shi and Shaoshu Su and Yiren Lu and Taimeng Fu and Karthik Dantu and Jiajun Wu and Lihua Xie and Marco Hutter and Luca Carlone and Sebastian Scherer and Daning Huang and Yaoyu Hu and Junyi Geng and Chen Wang},\n  year = {2023},\n  booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) Workshop},\n}\n```\n",1220,physics,Python,1,Python,,,,,,,,,,,,,,,,,,,,,,,,,,,,227,37,169,21,32,37,0,12667,102,119,90,29,1d2ba781c48706ba0526cce5254af270f1fd7548,fix cuda device failure (#350),2024-07-10T13:12:40Z,Zitong Zhan,ztzhan1108@gmail.com,zitongzhan,v0.6.8,## What's Changed\r\n* fix `cumops` where left arg is right. by @wang-chen in https://github.com/pypose/pypose/pull/339\r\n* relax pytest >=7.* by @wang-chen and @Xanthorapedia in https://github.com/pypose/pypose/pull/343\r\n\r\n\r\n**Full Changelog**: https://github.com/pypose/pypose/compare/v0.6.7...v0.6.8,v0.6.8,Chen Wang,,wang-chen,Apache License 2.0,pypose,pypose,21,robotics,pytorch,python,learning,lie-group,optimization,deep-learning,computer-graphics,geometric-deep-learning,physics,slam,control,planning,autonomous-robots,kalman-filter,pose-estimation,pose-graph-optimization,,,,/pypose/pypose,21,14,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/pygfx/pygfx,https://github.com/pygfx/pygfx,0,,,0,0,0,0,0,0,1,1,0,0,0,Powerful and versatile visualization for Python.,"<h1 align=""center""><img src=""docs/_static/pygfx.svg"" height=""80""><br>pygfx</h1>\n\n[![CI ](https://github.com/pygfx/pygfx/workflows/CI/badge.svg)\n](https://github.com/pygfx/pygfx/actions)\n[![Documentation Status\n](https://readthedocs.org/projects/pygfx/badge/?version=stable)\n](https://pygfx.readthedocs.io)\n[![PyPI version ](https://badge.fury.io/py/pygfx.svg)\n](https://badge.fury.io/py/pygfx)\n\nPygfx' purpose is to bring powerful and reliable visualization to the Python world. 🚀\n\nPygfx (pronounced ""py-graphics"") is built on [wgpu](https://github.com/pygfx/wgpu-py/), enabling superior performance and reliability compared to OpenGL-based solutions. It is designed for simplicity and versatility: with its modular architecture, you can effortlessly assemble graphical scenes for diverse applications, from scientific visualization to video game rendering.\n\n<p align=""center"">\n<img src=""./docs/_static/readme_sponza.png"" alt=""drawing"" width=""200""/>\n<img src=""./docs/_static/readme_pbr_example.webp"" alt=""drawing"" width=""200""/>\n<img src=""./docs/_static/readme_torus_knot_wire.png"" alt=""drawing"" width=""200""/>\n</p>\n<p align=""center"">\n[<a href=""https://pygfx.com/stable/guide.html"">User Guide</a>]\n[<a href=""https://pygfx.com/stable/_gallery/index.html"">Example Gallery</a>]\n[<a href=""https://pygfx.com/stable/reference.html"">API Reference</a>]\n</p>\n\n## Professional support\n\nNeed help? We offer the following professional services:\n\n* **Priority Support:** Rest assured with our dedicated support, prioritizing your needs for quick issue resolution and feature implementation.\n* **Integration Support:** Get assistance with integrating pygfx into your application, ensuring a smooth transition and optimal performance.\n* **Customized Solutions:** Whether it's crafting specific visualizations or developing shaders, we work closely with you to create tailored solutions that address your unique requirements.\n* **Training and Workshops:** We provide informative training sessions and workshops aimed at boosting skills and knowledge.\n\nAlso checkout the [sponsors page](https://github.com/sponsors/pygfx).\nFor further inquiries reach out to us at [support@pygfx.com](mailto:support@pygfx.com).\n\n## Installation\n\n```bash\npip install -U pygfx glfw\n```\n\nTo work correctly, pygfx needs _some_ window to render to. Glfw is one\nlightweight option, but there are others, too. If you use a different\nwgpu-compatible window manager or only render offscreen you may choose to omit\nglfw. Examples of alternatives include: `jupyter_rfb` (rendering in Jupyter),\n`PyQt`, `PySide`, or `wx`.\n\nIn addition there are some platform\nrequirements, see the [wgpu docs](https://wgpu-py.readthedocs.io/en/stable/start.html). In\nessence, you need modern (enough) graphics drivers, and `pip>=20.3`.\n\n## Status\n\nWe're currently working towards version `1.0`, which means that the API\ncan change with each version. We expect to reach `1.0` near the end of\n2024, at which point we start caring about backwards compatibility.\n\nThis means that until then, you should probably pin the pygfx version\nthat you're using, and check the [release notes](https://github.com/pygfx/pygfx/releases)\nwhen you update.\n\n## Usage Example\n\n> **Note**\n> The example below is designed against the `main` branch,\n> and may not work on the latest release from pypi, while we're in beta.\n\n> **Note**\n> A walkthrough of this example can be found in [the\n> guide](https://pygfx.com/stable/guide.html#how-to-use-pygfx).\n\n```python\nimport pygfx as gfx\nimport pylinalg as la\n\ncube = gfx.Mesh(\n    gfx.box_geometry(200, 200, 200),\n    gfx.MeshPhongMaterial(color=""#336699""),\n)\n\nrot = la.quat_from_euler((0, 0.01), order=""XY"")\n\ndef animate():\n    cube.local.rotation = la.quat_mul(rot, cube.local.rotation)\n\nif __name__ == ""__main__"":\n    gfx.show(cube, before_render=animate)\n\n```\n<img src=""./docs/_static/guide_rotating_cube.gif"" alt=""drawing"" width=""400""/>\n\n\n## Feature Highlights\nSome of pygfx's key features are:\n\n- SDF based text rendering ([example](\n  https://pygfx.com/stable/_gallery/feature_demo/text_contrast.html))\n- order-independent transparency (OIT) ([example](\n  https://pygfx.com/stable/_gallery/feature_demo/transparency2.html))\n- lights, shadows, and physically based rendering (PBR) ([example](\n  https://pygfx.com/stable/_gallery/feature_demo/pbr.html))\n- event system with built-in picking ([example](\n  https://pygfx.com/stable/_gallery/feature_demo/picking_points.html))\n- texture and color mapping supporting 1D, 2D and 3D data ([example](\n  https://pygfx.com/stable/_gallery/feature_demo/colormap_channels.html))\n\n\nAnd many more! Check out our [feature demos](\nhttps://pygfx.com/stable/_gallery/index.html) in the docs.\n\n## License\n\nPygfx is licensed under the [BSD 2-Clause ""Simplified"" License](LICENSE). This means:\n\n- :white_check_mark: It is free (and open source) forever. :cupid:\n- :white_check_mark: You _can_ use it commercially.\n- :white_check_mark: You _can_ distribute it and freely make changes.\n- :x: You _can not_ hold us accountable for the results of using pygfx.\n\n## Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md).\n\n### Development Install\nTo get a working dev install of pygfx you can use the following steps:\n\n```bash\n# Click the Fork button on GitHub and navigate to your fork\ngit clone <address_of_your_fork>\ncd pygfx\n# if you use a venv, create and activate it\npip install -e "".[dev,docs,examples]""\npytest tests\n```\n\n### Testing\n\nThe test suite is divided into two parts; unit tests for the core, and unit\ntests for the examples.\n\n* `pytest -v tests` runs the core unit tests.\n* `pytest -v examples` tests the examples.\n\n\n### Code of Conduct\n\nOur code of conduct can be found here: [Code of Conduct](./CODE_OF_CONDUCT.md)\n",390,graphics,Python,3,Python,HTML,WGSL,,,,,,,,,,,,,,,,,,,,,,,,,,454,36,407,11,6,21,0,28607,36,345,247,98,eaf8acd4e4188b8f32302413984829b86497bf1f,unproject to get range of screen space to set rulers (#814),2024-07-19T19:36:13Z,Kushal Kolar,kushalkolar@gmail.com,kushalkolar,v0.3.0,"\r\n\r\n## Changes\r\n* Change event time_stamp unit to seconds by @panxinmiao in https://github.com/pygfx/pygfx/pull/742\r\n  * Check your code for usage of `event[""time_stamp""]`. \r\n* Move text slightly in front of the outline. by @hmaarrfk in https://github.com/pygfx/pygfx/pull/776\r\n  * Text can now be placed *on* a surface.\r\n  * The text outline no longer bleeds over the face of neighboring characters. \r\n* More flexible linking controllers to cameras by @almarklein in https://github.com/pygfx/pygfx/pull/778\r\n  * `PerspectiveCamera.set_state()` allows setting `x`, `y`, and `z`.\r\n  * `Controller.add_camera()`  has new args ` include_state` and `exclude_state`.\r\n\r\n## Fixes\r\n* Fix volume example picking by @hmaarrfk in https://github.com/pygfx/pygfx/pull/788\r\n* Fix `Controller.cameras` property, add a very basic test by @kushalkolar in https://github.com/pygfx/pygfx/pull/791\r\n* Fix legacy linalg by @panxinmiao in https://github.com/pygfx/pygfx/pull/785\r\n\r\n## Added\r\n* Add Background.from_color by @almarklein in https://github.com/pygfx/pygfx/pull/733\r\n* GLTF Loader by @panxinmiao in https://github.com/pygfx/pygfx/pull/732\r\n* Additive blending mode by @tlambert03 in https://github.com/pygfx/pygfx/pull/752\r\n* Grid object by @almarklein in https://github.com/pygfx/pygfx/pull/743\r\n* Add SkinnedMesh by @panxinmiao in https://github.com/pygfx/pygfx/pull/715\r\n* Add isosurface volume material by @kevinyamauchi in https://github.com/pygfx/pygfx/pull/773\r\n* Add minip volume rendering by @kevinyamauchi in https://github.com/pygfx/pygfx/pull/784\r\n\r\n## Added examples\r\n* Add validation examples for blending by @almarklein in https://github.com/pygfx/pygfx/pull/734\r\n* Integration example of pygfx with pytorch lightning and pytorch geometric by @royvelich in https://github.com/pygfx/pygfx/pull/588\r\n* Color Conversion in PBR Example by @panxinmiao in https://github.com/pygfx/pygfx/pull/738\r\n* Add example to show cpu usage in title by @almarklein in https://github.com/pygfx/pygfx/pull/767\r\n* Audio visualization by @panxinmiao in https://github.com/pygfx/pygfx/pull/765\r\n* Add text overlays to the transparency examples by @hmaarrfk in https://github.com/pygfx/pygfx/pull/782\r\n* Tweak hello_text.py example by @almarklein in https://github.com/pygfx/pygfx/pull/779\r\n\r\n## Internal stuff\r\n* Refactor baseshader by @almarklein in https://github.com/pygfx/pygfx/pull/736\r\n* Tweak wgpu abstractions by @almarklein in https://github.com/pygfx/pygfx/pull/762\r\n* Use np.linalg.pinv instead of .inv by @almarklein in https://github.com/pygfx/pygfx/pull/766\r\n* Fix method prefix to be consistent with other cases by @almarklein in https://github.com/pygfx/pygfx/pull/768\r\n* Adjust to latest wgpu by @almarklein in https://github.com/pygfx/pygfx/pull/786\r\n\r\n## Docs, tests, CI, etc.\r\n* Suppress sphinx warning about caching gallery_conf by @almarklein in https://github.com/pygfx/pygfx/pull/735\r\n* Update documentation to reflect that modifiers and buttons are tuples not lists by @hmaarrfk in https://github.com/pygfx/pygfx/pull/731\r\n* Update writable flag documentation in the guide by @hmaarrfk in https://github.com/pygfx/pygfx/pull/747\r\n* Update dev install instructions by @kevinyamauchi in https://github.com/pygfx/pygfx/pull/755\r\n* Fix that `pytest examples` does not always work by @almarklein in https://github.com/pygfx/pygfx/pull/770\r\n* Bump github actions to address deprecation warnings by @hmaarrfk in https://github.com/pygfx/pygfx/pull/777\r\n* Move most builds from 3.9 to 3.12 by @almarklein in https://github.com/pygfx/pygfx/pull/793\r\n* Rebrand by @Korijn in https://github.com/pygfx/pygfx/pull/757\r\n* logo visibility in docs by @Korijn in https://github.com/pygfx/pygfx/pull/758\r\n* override text color for version text in docs by @Korijn in https://github.com/pygfx/pygfx/pull/759\r\n* Fix links by @Korijn in https://github.com/pygfx/pygfx/pull/760\r\n* Create FUNDING.yml by @almarklein in https://github.com/pygfx/pygfx/pull/796\r\n* Simplify texts on support a bit by @almarklein in https://github.com/pygfx/pygfx/pull/797\r\n\r\n## New Contributors\r\n* @royvelich made their first contribution in https://github.com/pygfx/pygfx/pull/588\r\n* @tlambert03 made their first contribution in https://github.com/pygfx/pygfx/pull/752\r\n* @kevinyamauchi made their first contribution in https://github.com/pygfx/pygfx/pull/755\r\n\r\n**Full Changelog**: https://github.com/pygfx/pygfx/compare/v0.2.0...v0.3.0",v0.3.0,,,github-actions[bot],"BSD 2-Clause ""Simplified"" License",pygfx,pygfx,17,python,vulkan,metal,dx12,wgpu,vizualisation,3d-graphics,3d-engine,3d,graphics,science,,,,,,,,,,/pygfx/pygfx,21,16,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/pygae/galgebra,https://github.com/pygae/galgebra,0,,,0,1,0,0,0,0,1,0,0,0,0,Symbolic Geometric Algebra/Calculus package for SymPy :crystal_ball:,"GAlgebra\n=========================================\n\nSymbolic Geometric Algebra/Calculus package for SymPy.\n\n[![PyPI](https://img.shields.io/pypi/v/galgebra.svg)](https://pypi.org/project/galgebra/)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/galgebra.svg)](https://pypi.org/project/galgebra/)\n[![Python CI](https://github.com/pygae/galgebra/actions/workflows/ci.yml/badge.svg)](https://github.com/pygae/galgebra/actions/workflows/ci.yml)\n[![Documentation Status](https://readthedocs.org/projects/galgebra/badge/?version=latest)](https://galgebra.readthedocs.io/en/latest/?badge=latest)\n[![DOI](https://zenodo.org/badge/113447311.svg)](https://zenodo.org/badge/latestdoi/113447311)\n\n![](https://raw.githubusercontent.com/pygae/galgebra/master/doc/images/n_vector_positive_spherical.svg?sanitize=true)\n\nDevelopment Status\n--------------------\n\n![PyPI - Status](https://img.shields.io/pypi/status/galgebra.svg)\n![GitHub contributors](https://img.shields.io/github/contributors/pygae/galgebra.svg)\n[![Codecov](https://img.shields.io/codecov/c/github/pygae/galgebra.svg)](https://codecov.io/gh/pygae/galgebra)\n[![Maintainability](https://api.codeclimate.com/v1/badges/26d1c1b351d32d2b1097/maintainability)](https://codeclimate.com/github/pygae/galgebra/maintainability)\n\n[brombo/galgebra](https://github.com/brombo/galgebra) was originally written by Alan Bromborsky, but was no longer actively maintained, and as of 2019-11-25 no longer exists.\n\n[pygae/galgebra](https://github.com/pygae/galgebra) is a community fork, maintained by [Pythonic Geometric Algebra Enthusiasts](https://github.com/pygae).\n\nThe fork supports Python 3, increases test coverage, sets up CI and linters, maintains releases to [PyPI](https://pypi.org/project/galgebra/#history), improves [docs](http://galgebra.readthedocs.io) and has many bug fixes, see [Changelog](https://galgebra.readthedocs.io/en/latest/changelog.html).\n\n> [!NOTE]\n> Readers of Prof. Alan Macdonald's [Linear and Geometric Algebra](http://www.faculty.luther.edu/~macdonal/laga/index.html) and [Vector and Geometric Calculus](http://www.faculty.luther.edu/~macdonal/vagc/index.html), please check out [**Migrating guide for readers of LAGA&VAGC**](#migrating-guide-for-readers-of-lagavagc) below.\n> \n> If you are coming from [sympy.galgebra](https://docs.sympy.org/0.7.6.1/modules/galgebra/) or [brombo/galgebra](https://github.com/brombo/galgebra), please check out section [Migration Guide](#migration-guide) below.\n\nFeatures\n--------------------\n\n### Geometric Algebra\n\n- Arbitrary Vector Basis and Metric\n- Scalar, Vector, Bivector, Multivector, Pseudoscalar, Spinor, Blade\n- Basic Geometic Algebra Operations\n  - Sum Difference\n  - Geometric Product\n  - Outer and Inner Products\n  - Left and Right Contractions\n  - Reverse, Dual, Exponential\n  - Commutator\n  - Projection, Reflection, Rotation\n  - Reciprocal Frames\n- Inspecting Base/Blade Representation\n- Symbolic Manipulations\n  - `expand`, `factor`, `simplify`, `subs`, `trigsimp` etc.\n\nOverloaded Python operators for basic GA operations:\n\n```math\n\begin{split}\begin{aligned}\n  A+B &=  \texttt{A+B} \\\n  A-B &=  \texttt{A-B} \\\n  AB &=  \texttt{A*B} \\\n  A \wedge B &=  \mathtt{A \verb!^! B} \\\n  A \cdot B &=  \texttt{A|B} \\\n  A \rfloor B &=  \mathtt{A \lt B} \\\n  A \lfloor B &=  \mathtt{A \gt B} \\\n  A/B &=  \texttt{A/B} \\\n\end{aligned}\end{split}\n```\n\n### Geometric Calculus\n\n- Geometric Derivative\n- Submanifolds\n- Linear Transformations\n- Differential Operators\n\nThe various derivatives of a multivector function is accomplished by multiplying the gradient operator vector with the function:\n\n```math\n\begin{aligned}\n  \nabla F &=  \texttt{grad*F} \\\n  F \bar{\nabla} &=  \texttt{F*rgrad} \\\n  \nabla {\wedge}F &=  \mathtt{grad \verb!^! F} \\\n  F {\wedge}\bar{\nabla} &=  \mathtt{F \verb!^! rgrad} \\\n  \nabla \cdot F &=  \texttt{grad|F} \\\n  F \cdot \bar{\nabla} &=  \texttt{F|rgrad} \\\n  \nabla \rfloor F &=  \mathtt{grad \lt F} \\\n  F \rfloor \bar{\nabla} &=  \mathtt{F \lt rgrad} \\\n  \nabla \lfloor F &=  \mathtt{grad \gt F} \\\n  F \lfloor \bar{\nabla} &= \mathtt{F \gt rgrad}\n\end{aligned}\n```\n\n```math\n\begin{aligned}\n  F \nabla &=  \texttt{F*grad} \\\n  \bar{\nabla} F &=  \texttt{rgrad*F} \\\n  F {\wedge}\nabla &=  \mathtt{F \verb!^! grad} \\\n  \bar{\nabla} {\wedge}F &=  \mathtt{rgrad \verb!^! F} \\\n  F \cdot \nabla &=  \texttt{F|grad} \\\n  \bar{\nabla}\cdot F &=  \texttt{rgrad|F} \\\n  F \rfloor \nabla &=  \mathtt{F \lt grad} \\\n  \bar{\nabla} \rfloor F &=  \mathtt{rgrad \lt F} \\\n  F \lfloor \nabla &=  \mathtt{F \gt grad} \\\n  \bar{\nabla} \lfloor F &= \mathtt{rgrad \gt F}\n\end{aligned}\n```\n\nTip: an example for getting `grad` and `rgrad` of a 3-d Euclidean geometric algebra in rectangular coordinates:\n\n```python\nfrom sympy import symbols\nfrom galgebra.ga import Ga\n\no3d = Ga('e', g=[1,1,1], coords=symbols('x,y,z',real=True))\n(grad,rgrad) = o3d.grads()\n```\n\n### Printing\n\n- Enhanced Console Printing\n- Latex Printing\n  - out-of-the-box support for Jupyter Notebook\n  - PDF generation and croping support if you have `pdflatex`/`pdfcrop` installed\n\n<!-- Note: These comments are parsed by our sphinx documentation -->\n\n<!-- begin: getting-started -->\n\nGetting Started\n---------------------\n\nAfter installing GAlgebra (see section [Installing GAlgebra](#installing-galgebra) below), in a Jupyter Notebook:\n\n```python\nfrom sympy import symbols\nfrom galgebra.ga import Ga\n\nfrom galgebra.printer import Format\nFormat(Fmode = False, Dmode = True)\n\nst4coords = (t,x,y,z) = symbols('t x y z', real=True)\nst4 = Ga('e',\n         g=[1,-1,-1,-1],\n         coords=st4coords)\n\nM = st4.mv('M','mv',f = True)\n\nM.grade(3).Fmt(3,r'\langle \mathbf{M} \rangle _3')\n```\n\nYou will see:\n\n```math\n\begin{aligned}   \langle \mathbf{M} \rangle _3 =& M^{txy}  \boldsymbol{e}_{t}\wedge \boldsymbol{e}_{x}\wedge \boldsymbol{e}_{y} \\  &  + M^{txz}  \boldsymbol{e}_{t}\wedge \boldsymbol{e}_{x}\wedge \boldsymbol{e}_{z} \\  &  + M^{tyz}  \boldsymbol{e}_{t}\wedge \boldsymbol{e}_{y}\wedge \boldsymbol{e}_{z} \\  &  + M^{xyz}  \boldsymbol{e}_{x}\wedge \boldsymbol{e}_{y}\wedge \boldsymbol{e}_{z}  \end{aligned}\n```\n\nYou may also check out more examples [here](https://github.com/pygae/galgebra/blob/master/examples/).\n\nFor detailed documentation, please visit https://galgebra.readthedocs.io/ .\n\n<!-- end: getting-started -->\n<!-- begin: installation -->\n\nInstalling GAlgebra\n---------------------\n\n### Prerequisites\n\n- Works on Linux, Windows, Mac OSX\n- [Python](https://www.python.org/) >= 3.8\n  - 0.5.0 was the last supported release for Python 3.5-3.7\n  - 0.4.x was the last supported release series for Python 2.7\n- [SymPy](https://www.sympy.org) >= 1.3 \n  - Only SymPy 1.12 is tested via CI, see `.github/workflows/ci.yml` for more details\n  - 0.5.0 was the last supported release for SymPy 1.7\n\n### Installing GAlgebra From PyPI (Recommended for users)\n\n```bash\npip install galgebra\n```\n\nThen you are all set!\n\n### Installing GAlgebra From Source (Recommended for developers)\n\nTo install from the latest source code of GAlgebra:\n\n```bash\ngit clone https://github.com/pygae/galgebra.git\ncd galgebra\npip install -e .\n```\n\nNote that the optional `-e` argument is used here for a developer install so modifying the source will take effect immediately without the need of reinstallation.\n\nNow you may run tests to verify the installation, run from the root of the repository:\n\n```bash\npip install pytest\npytest test\n```\n\nFurther, to run the complete test suite including the ones using [nbval](https://github.com/computationalmodelling/nbval), just run:\n\n```bash\npip install nbval\npytest --nbval examples/ipython/ --nbval examples/primer/ test --nbval-current-env --nbval-sanitize-with test/.nbval_sanitize.cfg\n```\n\nThis could take more than 10 minutes, please be patient.\n\n<!-- end: installation -->\n<!-- begin: migration -->\n\nMigration Guide\n----------------\n\n> Note: The APIs have changed since the era of `sympy.galgebra` and `brombo/galgebra`, some properties and methods are deprecated, the supported versions of Python and SymPy have also changed, please check [Changelog](https://galgebra.readthedocs.io/en/latest/changelog.html) and further update your scripts accordingly besides the following. If you encounter any problems, feel free to [open an issue](https://github.com/pygae/galgebra/issues/new)!\n\n### Migrating guide for readers of LAGA&VAGC\n\nReaders of [Linear and Geometric Algebra](http://www.faculty.luther.edu/~macdonal/laga/index.html) and [Vector and Geometric Calculus](http://www.faculty.luther.edu/~macdonal/vagc/index.html) might be guided by [GAlgebra Primer](http://www.faculty.luther.edu/~macdonal/GAlgebraPrimer.pdf) (version November 29, 2022, accessed May, 2024) to download [GAfiles.zip](http://www.faculty.luther.edu/~macdonal/GAfiles.zip) and copy `gprinter.py`, `lt.py`, `mv.py`, and `GAlgebraInit.py`¸ into where GAlgebra is installed. These steps are NO LONGER NEEDED since GAlgebra 0.6.0 as they are merge into GAlgebra with tests, copying these files will cause conflicts and regressions of fixed bugs.\n\nFor minor differences to those files, please check out [the change log for GAlgebra 0.6.0](https://galgebra.readthedocs.io/en/latest/changelog.html#0.6.0). Also please note that:\n\n- `GAlgebraInit.py` is renamed to `primer.py` and can be imported like `from galgebra.primer import *` but it's usage is discouraged, although it saves some boilerplate code, this is not part of GAlgebra's maintained API, GAlgebra might remove it in future.\n- Some notebooks from the zip is included in GAlgebra in `examples/primer`.\n\n### Migrating from [sympy.galgebra](https://docs.sympy.org/0.7.6.1/modules/galgebra/)\n\nGAlgebra is no longer part of SymPy since 1.0.0, if you have an import like this in your source:\n\n```python\nfrom sympy.galgebra.ga import *\n```\n\nSimply remove the `sympy.` prefix before `galgebra` then you are good to go:\n\n```python\nfrom galgebra.ga import *\n```\n\n### Migrating from [brombo/galgebra](https://github.com/brombo/galgebra)\n\nThe `setgapth.py` way to install is now deprecated by `pip install galgebra` and all modules in GAlgebra should be imported from `galgebra`, for example:\n\n```python\nfrom galgebra.printer import Format, Eprint, latex, GaPrinter\nfrom galgebra.ga import Ga\nfrom galgebra.mv import Mv, Nga\n```\n\n<!-- end: migration -->\n<!-- begin: bundled-resources -->\n\nBundled Resources\n------------------\n\nNote that in the [doc/books](https://github.com/pygae/galgebra/blob/master/doc/books/) directory there are:\n\n- `BookGA.pdf` which is a collection of notes on Geometric Algebra and Calculus based of ""Geometric Algebra for Physicists"" by Doran and Lasenby and on some papers by Lasenby and Hestenes.\n- `galgebra.pdf` which is the original main doc of GAlgebra in PDF format, while the math part is still valid, the part describing the installation and usage of GAlgebra is outdated, please read with caution or visit https://galgebra.readthedocs.io/ instead.\n- `Macdonald` which contains bundled supplementary materials for [Linear and Geometric Algebra](http://www.faculty.luther.edu/~macdonal/laga/index.html) and [Vector and Geometric Calculus](http://www.faculty.luther.edu/~macdonal/vagc/index.html) by Alan Macdonald, see [here](https://github.com/pygae/galgebra/blob/master/doc/books/Macdonald/) and [here](https://github.com/pygae/galgebra/blob/master/examples/Macdonald/) for more information.\n  - Particularly, `GAlgebraPrimer.pdf` is an archived version of [GAlgebra Primer](http://www.faculty.luther.edu/~macdonal/GAlgebraPrimer.pdf) by Alan Macdonald, last updated on November 29, 2022.\n\n<!-- end: bundled-resources -->\n\nStar History\n-------------------\n\n<a href=""https://star-history.com/#pygae/galgebra&Date"">\n <picture>\n   <source media=""(prefers-color-scheme: dark)"" srcset=""https://api.star-history.com/svg?repos=pygae/galgebra&type=Date&theme=dark"" />\n   <source media=""(prefers-color-scheme: light)"" srcset=""https://api.star-history.com/svg?repos=pygae/galgebra&type=Date"" />\n   <img alt=""Star History Chart"" src=""https://api.star-history.com/svg?repos=pygae/galgebra&type=Date"" />\n </picture>\n</a>\n\nContributors\n-------------------\n\n<a href=""https://github.com/pygae/galgebra/graphs/contributors"">\n  <img src=""https://contrib.rocks/image?repo=pygae/galgebra"" />\n</a>\n\nMade with [contrib.rocks](https://contrib.rocks).\n\nCiting This Library\n-------------------\n\nFor citation information, see [our `CITATION.md` file](https://github.com/pygae/galgebra/blob/master/CITATION.md).\n",225,physics,Python,1,Python,,,,,,,,,,,,,,,,,,,,,,,,,,,,351,25,322,4,30,10,3,55666,62,168,102,66,644a32a70928346e36ce5d17bd18afe75232c971,"Add module `gprinter`, methods `undual`, `g_invol`, `ccon`, `sp`, `no…",2024-05-15T01:38:26Z,Utensil,utensilcandel@gmail.com,utensil,v0.5.2,See [Changelog](https://galgebra.readthedocs.io/en/latest/changelog.html).\r\n\r\nhttps://pypi.org/project/galgebra/0.5.2/,v0.5.2,Utensil,,utensil,"BSD 3-Clause ""New"" or ""Revised"" License",galgebra,pygae,8,geometric-algebra,clifford-algebras,quaternions,physics,python,,,,,,,,,,,,,,,,/pygae/galgebra,12,17,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/psi4/psi4,https://github.com/psi4/psi4,1,,,1,1,1,1,0,0,0,0,0,0,1,Open-Source Quantum Chemistry – an electronic structure package in C++ driven by Python,"# <img src=""https://github.com/psi4/psi4media/blob/master/logos-psi4/psi4square.png"" height=150>\n\n| **Status** | [![Azure DevOps builds](https://img.shields.io/azure-devops/build/psi4/e80489d7-9619-4512-8e7b-255e355b3ab8/1?logo=azure%20devops)](https://dev.azure.com/psi4/psi4/_build?definitionId=1) [![Codecov coverage](https://img.shields.io/codecov/c/github/psi4/psi4.svg?logo=Codecov&logoColor=white)](https://codecov.io/gh/psi4/psi4) |\n| :------ | :------- |\n| **Latest Release** | [![Last release tag](https://img.shields.io/github/release/psi4/psi4.svg)](https://github.com/psi4/psi4/releases)  [![Commits since release](https://img.shields.io/github/commits-since/psi4/psi4/v1.8.svg)](https://github.com/psi4/psi4/releases/tag/v1.8) [![python](https://img.shields.io/badge/python-3.8%2C%203.9%2C%203.10%2C%203.11%2C%203.12-blue.svg)](https://psicode.org/psi4manual/master/introduction.html#supported-systems) |\n| **Communication** | [![User site](https://img.shields.io/badge/home-Psi4-5077AB.svg)](https://psicode.org/) [![docs latest](https://img.shields.io/badge/docs-latest-5077AB.svg?logo=read%20the%20docs)](https://psicode.org/psi4manual/master/index.html) [![chat on forum](https://img.shields.io/badge/chat-on_forum-808493.svg?logo=Discourse&logoColor=white)](http://forum.psicode.org/) [![dev chat on slack](https://img.shields.io/badge/dev_chat-on_slack-808493.svg?logo=slack)](https://join.slack.com/t/psi4/shared_invite/zt-5s36s4rb-SQH6_AWyfWOqlKYN3cFs4Q) |\n| **Foundation** | [![license](https://img.shields.io/github/license/psi4/psi4.svg)](https://opensource.org/licenses/LGPL-3.0) [![platforms](https://img.shields.io/badge/Platforms-Linux%2C%20MacOS%2C%20MacOS%20Silicon%2C%20Windows%2C%20Windows%20WSL-orange.svg)](https://psicode.org/psi4manual/master/introduction.html#supported-systems) [![python](https://img.shields.io/badge/python-3.8%2C%203.9%2C%203.10%2C%203.11%2C%203.12-blue.svg)](https://psicode.org/psi4manual/master/introduction.html#supported-systems) |\n| **Installation** | [![obtain latest](https://img.shields.io/badge/obtain-latest-green.svg)](https://psicode.netlify.com/installs/latest) [![Conda](https://img.shields.io/conda/v/conda-forge/psi4.svg)](https://anaconda.org/conda-forge/psi4) [![Anaconda-Server Badge](https://anaconda.org/conda-forge/psi4/badges/latest_release_relative_date.svg)](https://anaconda.org/conda-forge/psi4) |\n| **Demo** | [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/psi4/psi4/56fbc7787af67dabdf1897d0dfe4263d8d97e241?urlpath=lab%2Ftree%2Fdoc%2Fsphinxman%2Fsource%2Fpsiapi.ipynb) |\n\n<!--  -->\n<!-- [![Last release date](https://img.shields.io/github/release-date/psi4/psi4.svg)](https://github.com/psi4/psi4/releases) -->\n<!-- [![Anaconda-Server Badge](https://anaconda.org/psi4/psi4/badges/version.svg)](https://anaconda.org/psi4/psi4) -->\n\n<!--<a href=""https://psi4.slack.com/messages""> <img src=""https://img.shields.io/badge/dev_chat-on_slack-808493.svg"" /></a>\n<a href=""mailto:psi4aiqc+slackinvite@gmail.com?subject=request slack invite (incl. who, where, email)""> <img src=""https://img.shields.io/badge/dev_chat-invite-808493.svg"" /></a> -->\n\n<!--[![Anaconda-Server Badge](https://anaconda.org/psi4/psi4/badges/installer/conda.svg)](https://anaconda.org/psi4/psi4) \n[![Anaconda-Server Badge](https://anaconda.org/psi4/psi4/badges/platforms.svg)](https://anaconda.org/psi4/psi4) -->\n\n<!--\n| **PR Activity** | \n[![commit activity](https://img.shields.io/github/commit-activity/y/psi4/psi4.svg)](https://github.com/psi4/psi4/graphs/contributors) \n[![issues-pr-closed](https://img.shields.io/github/issues-pr-closed-raw/psi4/psi4.svg)](https://github.com/psi4/psi4/pulls)\n-->\n\nPsi4 is an open-source suite of *ab initio* quantum chemistry programs\ndesigned for efficient, high-accuracy simulations of\nmolecular properties. We routinely perform computations with >2500 basis functions on multi-core machines.\n\nWith computationally demanding portions written in C++, exports\nof many C++ classes into Python via Pybind11, and a flexible Python driver, Psi4\nstrives to be friendly to both users and developers.\n\n* **Users' Website**  www.psicode.org\n\n* **Downloading and Installing Psi4** https://psicode.org/psi4manual/master/build_faq.html (for the CMake adept, see [CMakeLists.txt](CMakeLists.txt#L27)\n\n* **Manual**  [http://bit.ly/psi4manual](https://psicode.org/psi4manual/master/index.html) (built nightly from master branch) or https://psicode.org/psi4manual/1.4.0/index.html (last release)\n\n* **Tutorial** https://psicode.org/psi4manual/master/tutorial.html for Psithon (``psi4 job.in``), https://psicode.org/psi4manual/master/psiapi.html for PsiAPI (``python job.py``)\n\n* **Forum** http://forum.psicode.org\n\n* **Communication & Support** https://psicode.org/psi4manual/master/introduction.html#technical-support\n\n* **GitHub**  https://github.com/psi4/psi4 (authoritative repository)\n\n* **Continuous Integration Status** [![Azure DevOps builds](https://img.shields.io/azure-devops/build/psi4/e80489d7-9619-4512-8e7b-255e355b3ab8/1/master.svg?logo=azure%20devops)](https://dev.azure.com/psi4/psi4/_build?definitionId=1) on Linux and Windows\n\n* **Anaconda**  https://anaconda.org/psi4 (binary available for Linux, Mac, Mac Silicon, Windows, and WSL Windows [![Binstar Badge](https://anaconda.org/psi4/psi4/badges/downloads.svg)](https://anaconda.org/psi4/psi4) ) [![Binstar Badge](https://anaconda.org/conda-forge/psi4/badges/downloads.svg)](https://anaconda.org/conda-forge/psi4) ) [instructions](https://psicode.org/psi4manual/master/conda.html#how-to-install-a-psi4-binary-with-the-psi4conda-installer-download-site)\n\n* **Coverage** Python and C++ source code lines hit by running most of the test suite. [![codecov](https://img.shields.io/codecov/c/github/psi4/psi4.svg?logo=Codecov&logoColor=white)](https://codecov.io/gh/psi4/psi4)\n\n* **Interested Developers**  https://psicode.org/developers.php (replacement page needed) (welcome to fork psi4/psi4 and follow [GitHub contribution procedure](https://psicode.org/psi4manual/master/build_obtaining.html#faq-githubworkflow)) [![PRs welcome](https://img.shields.io/badge/PRs-welcome-yellow.svg)](http://makeapullrequest.com)\n\n* **Sample Inputs**  http://www.psicode.org/psi4manual/master/testsuite.html (also in [`samples/`](samples))\n\n* **Download Tarball** https://github.com/psi4/psi4/releases \n\n<!--* **Build Dashboard** https://testboard.org/cdash/index.php?project=Psi\n\n* **YouTube Channel** https://www.youtube.com/psitutorials-->\n\n\nLicense [![license](https://img.shields.io/github/license/psi4/psi4.svg)](https://opensource.org/licenses/LGPL-3.0)\n=======\n\nPsi4: an open-source quantum chemistry software package\n\nCopyright (c) 2007-2024 The Psi4 Developers.\n\nThe copyrights for code used from other parties are included in\nthe corresponding files.\n\nPsi4 is free software; you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, version 3.\n\nPsi4 is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Lesser General Public License for more details.\n\nYou should have received a copy of the GNU Lesser General Public License along\nwith Psi4; if not, write to the Free Software Foundation, Inc.,\n51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n\nThe full text of the GNU Lesser General Public License (version 3) is included in the\nCOPYING.LESSER file of this repository, and can also be found\n[here](https://www.gnu.org/licenses/lgpl.txt).\n\n\nCitation [![doi](https://img.shields.io/badge/doi-10.1063/5.0006002-5077AB.svg)](https://doi.org/10.1063/5.0006002)\n========\n\nThe journal article reference describing Psi4 is:\n\nD. G. A. Smith, L. A. Burns, A. C. Simmonett, R. M. Parrish,\nM. C. Schieber, R. Galvelis, P. Kraus, H. Kruse, R. Di Remigio,\nA. Alenaizan, A. M. James, S. Lehtola, J. P. Misiewicz, M. Scheurer,\nR. A. Shaw, J. B. Schriber, Y. Xie, Z. L. Glick, D. A. Sirianni,\nJ. S. O'Brien, J. M. Waldrop, A. Kumar, E. G. Hohenstein,\nB. P. Pritchard, B. R. Brooks, H. F. Schaefer III, A. Yu. Sokolov,\nK. Patkowski, A. E. DePrince III, U. Bozkaya, R. A. King,\nF. A. Evangelista, J. M. Turney, T. D. Crawford, C. D. Sherrill,\n""Psi4 1.4: Open-Source Software for High-Throughput Quantum Chemistry"",\nJ. Chem. Phys. 152(18) 184108 (2020).\n\n* [![doi](https://img.shields.io/badge/doi-10.1021/acs.jctc.7b00174-5077AB.svg)](https://doi.org/10.1021/acs.jctc.7b00174) for Psi4 v1.1\n* [![doi](https://img.shields.io/badge/doi-10.1021/acs.jctc.8b00286-5077AB.svg)](https://doi.org/10.1021/acs.jctc.8b00286) for Psi4NumPy\n* [![doi](https://img.shields.io/badge/doi-10.1002/wcms.93-5077AB.svg)](https://doi.org/10.1002/wcms.93) for Psi4 alpha releases\n* [![doi](https://img.shields.io/badge/doi-10.1002/jcc.20573-5077AB.svg)](https://doi.org/10.1002/jcc.20573) for Psi3\n",946,physics,C++,9,CMake,Shell,Python,C++,C,Perl,Roff,Batchfile,Assembly,,,,,,,,,,,,,,,,,,,,1940,160,1721,59,20,127,0,136210,438,1256,916,340,7245e70546a7b37689df297c336a5d73201986dc,Update export_oeprop.cc (#3195),2024-07-16T10:20:17Z,Jonathon Misiewicz,jpmisiewicz@vt.edu,JonathonMisiewicz,"v1.9.1, 2024-02-08","Advertised Version: 1.9.1\r\nContinuous Version: 1.9.1\r\nRelease Date: 8 February 2024\r\nDocumentation: https://psicode.org/psi4manual/1.9.x\r\nAvailability: Public, GitHub source, CMake build, [Conda binary installers](https://psicode.netlify.com/installs/v191/), [Docker](https://hub.docker.com/r/psi4/psi4/tags?page=1&name=1.9.1)\r\nSpan: 5 PRs\r\n\r\n## Conda Package Updates\r\n* use pytest v7, as v8 is broken for psi4\r\n* #3107 prefer libint v2.8 (`conda install libint -c conda-forge`) over development (`-c conda-forge/label/libint_dev`) and psi4-stored builds\r\n* #3103 refine libxc minimum to v6.1.0\r\n\r\n## Bug Fixes\r\n* #3130 fixes symbols list for AM>=12\r\n* #3127 fixes import when $HOME directory is symlinked\r\n* #3097 Fix Python3 compatibility in vmd_cube.py\r\n",v1.9.1,Lori A. Burns,,loriab,GNU Lesser General Public License v3.0,psi4,psi4,17,quantum-chemistry,c-plus-plus,python,computational-chemistry,chemistry,physics,,,,,,,,,,,,,,,/psi4/psi4,32,64,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/project-gemmi/gemmi,https://github.com/project-gemmi/gemmi,0.5,lib,0,0,1,0,0,0,0,1,0,0,0,0,macromolecular crystallography library and utilities,"[![CI](https://github.com/project-gemmi/gemmi/workflows/CI/badge.svg)](https://github.com/project-gemmi/gemmi/actions)\n[![Build status](https://ci.appveyor.com/api/projects/status/cv5hu6h6hmxd9k5a?svg=true)](https://ci.appveyor.com/project/wojdyr/gemmi)\n\nGEMMI can help if you work with:\n\n* macromolecular models (from mmCIF, PDB and mmJSON files),\n* refinement restraints (CIF files),\n* crystallographic reflections (from MTZ and SF-mmCIF files),\n* electron density maps (MRC/CCP4 files),\n* crystallographic symmetries,\n* or if you just read and write CIF/STAR files (where C=Crystallographic).\n\nGEMMI is a C++11 library accompanied by:\n\n* command-line [tools](https://gemmi.readthedocs.io/en/latest/utils.html),\n* Python bindings (supporting CPython and PyPy),\n* Fortran 2003+ interface (in progress),\n* WebAssembly ports (see [here](https://project-gemmi.github.io/wasm/) and\n  [here](https://www.npmjs.com/package/mtz)),\n* and little data viz [projects](https://project-gemmi.github.io/pdb-stats/).\n\nDocumentation: http://gemmi.readthedocs.io/en/latest/\n\nGEMMI is an open-source project of [CCP4](https://www.ccp4.ac.uk/)\nand [Global Phasing Ltd](https://www.globalphasing.com/),\ntwo major providers of software for macromolecular crystallography.\n\nCiting: [JOSS paper](https://doi.org/10.21105/joss.04200).\n\nLicense: MPLv2, or (at your option) LGPLv3.\n© 2017-2023 Global Phasing Ltd.\n",206,structural-biology,C++,7,C++,Python,Shell,CMake,Fortran,JavaScript,Makefile,,,,,,,,,,,,,,,,,,,,,,120,18,101,1,4,15,0,9354,42,177,162,15,b180fff9a72bac7a3280f23f370a89d14e136c61,fix a broken test,2024-07-17T15:03:49Z,Keitaro Yamashita,keitaroyam@users.noreply.github.com,keitaroyam,0.6.6,"Library:\r\n* SmallStructure: changed how the space group is [read and accessed](https://gemmi.readthedocs.io/en/latest/mol.html#smallstructure-spacegroup).\r\n  Relying on H-M space group names alone was not always sufficient. The new mechanism uses the list of operations and Hall symbol in preference to the H-M symbol – the order is configurable.\r\n* symmetry triplets: parse decimal fractions (small molecule files may use notation such as x+0.25 instead of x+1/4)\r\n* tabulated space groups: a few more settings: B 1 2 1, B 1 21 1, F 1 m 1, F 1 d 1, F 1 2 1\r\n* X-ray scattering coefficients: changed the default value of `IT92::ignore_charge` to true (i.e. charges are now ignored by default; before version 0.6.3 they were always ignored)\r\n* cif::Table: added method `ensure_loop()` that converts tag-value pairs into a loop; might be needed before calling `append_row()`\r\n* place_hydrogens(): fix for NH3-like configurations\r\n* improved gemmi->mmdb conversion\r\n* Grid: tweaked good_grid_size() to ensure that when creating a grid up to a certain d_min, all reflections up to d_min are in the grid (it matters when no oversampling is applied)\r\n* DensityCalculator: deprecated function `set_grid_cell_and_spacegroup()`, use `grid.setup_from()`\r\n* fixed TNT-compatible reciprocal space ASU calculation for non-standard settings\r\n* infer_polymer_end(): complicate the heuristic even more, to detect files that have HETATM incorrectly used for standard residues in a polymer (such files were reported, they are either a result of mutating from non-standard residues, or a buggy program)\r\n* added function assign_het_flags() to re-set ATOM/HETATM flags\r\n* Model: added funtions `calculate_b_iso_range()` and `calculate_b_aniso_range()`; the first one can be used to detect if pLDDT is in the range 0-100 (like from AlphaFold) or 0-1 (like from ESMFold)\r\n* writing mmCIF: write _entity_poly_seq.hetero\r\n* added flag `Entity::reflects_microhetero` that shows if sequences were read from SEQRES (and don't account for point mutations) or from _entity_poly_seq; new function `add_microhetero_to_sequences()` changes the former to the latter \r\n\r\nProgram:\r\n* gemmi sfcalc: added a few more options\r\n* gemmi convert: added options `--assign-records[=A|H]`, improved `--sifts-num`, adding microheterogeneities to _entity_poly_seq when converting from PDB\r\n* gemmi cifdiff: added option `-t` for basic comparison of values for a single tag \r\n\r\nOther:\r\n* minimal WebAssembly port (C++ code compiled with emscripten) of Structure,\r\n  as a proof-of-concept and for reading mmCIF files in UglyMol\r\n* examples/to_rdkit.py: example of conversion of gemmi ChemComp to RDKit Mol\r\n\r\nand a number of less important changes",v0.6.6,Marcin Wojdyr,,wojdyr,Mozilla Public License 2.0,gemmi,project-gemmi,19,cif,mmcif,crystallography,protein-structure,molecular-structures,ccp4,pdb-files,structural-biology,mtz,,,,,,,,,,,,/project-gemmi/gemmi,49,16,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/processing/p5.js,https://github.com/processing/p5.js,0,,,0,0,0,0,0,0,1,1,0,0,0,"p5.js is a client-side JS platform that empowers artists, designers, students, and anyone to learn to code and express themselves creatively on the web. It is based on the core principles of Processing. http://twitter.com/p5xjs —",,21169,graphics,JavaScript,5,JavaScript,HTML,CSS,GLSL,Markdown,,,,,,,,,,,,,,,,,,,,,,,,3182,454,2685,43,7,677,0,100432,3231,3827,3562,265,db01247bd224b71fbf253a546f6a35e65b988267,Merge pull request #7138 from processing/all-contributors/add-Evorage0,2024-07-19T03:19:48Z,Dave Pagurek,davepagurek@gmail.com,davepagurek,v1.9.4,<!-- Release notes generated using configuration in .github/release.yml at v1.9.4 -->\r\n\r\n## What's Changed 🎊\r\n### Code \r\n* Account for pixel density when masking images by @Papershine in https://github.com/processing/p5.js/pull/6788\r\n* fix clearStorage function to match expected behaviour by @seralichtenhahn in https://github.com/processing/p5.js/pull/7006\r\n* Fix broken test by @davepagurek in https://github.com/processing/p5.js/pull/7010\r\n* textWrap() parameter validation uses FES instead of throwing error by @limzykenneth in https://github.com/processing/p5.js/pull/7009\r\n* fix The constructor of p5.Table by @asukaminato0721 in https://github.com/processing/p5.js/pull/7011\r\n* Fix autoSized for framebuffers by @davepagurek in https://github.com/processing/p5.js/pull/7012\r\n* fix: add the missing index of `attributes` by @IronBlood in https://github.com/processing/p5.js/pull/7027\r\n\r\n### Documentation\r\n* Add Japanese Version Contributor Docs by @RuimingShen in https://github.com/processing/p5.js/pull/6979\r\n* Add missing backtick by @aGuyWhoMadeGames in https://github.com/processing/p5.js/pull/6996\r\n* Fix default z value in p5.Camera::camera() docs by @davepagurek in https://github.com/processing/p5.js/pull/7001\r\n* Fix 'p5.Vector.sub()' documentation mistakenly referencing 'add'ing by @bobbykaz in https://github.com/processing/p5.js/pull/7024\r\n* Update structure references by @nickmcintyre in https://github.com/processing/p5.js/pull/7020\r\n* Update more structure references by @nickmcintyre in https://github.com/processing/p5.js/pull/7021\r\n* Fix foundation references by @nickmcintyre in https://github.com/processing/p5.js/pull/7022\r\n* Update array references by @nickmcintyre in https://github.com/processing/p5.js/pull/7032\r\n* Update conversion references by @nickmcintyre in https://github.com/processing/p5.js/pull/7033\r\n* Update storage references by @nickmcintyre in https://github.com/processing/p5.js/pull/7034\r\n* Update string references by @nickmcintyre in https://github.com/processing/p5.js/pull/7037\r\n* Update IO references by @nickmcintyre in https://github.com/processing/p5.js/pull/7038\r\n* Update time & date references by @nickmcintyre in https://github.com/processing/p5.js/pull/7039\r\n* Update p5.XML references by @nickmcintyre in https://github.com/processing/p5.js/pull/7040\r\n* Update rendering references by @nickmcintyre in https://github.com/processing/p5.js/pull/7041\r\n* Fix map example by @PalumboN in https://github.com/processing/p5.js/pull/7036\r\n* Update p5.Graphics references by @nickmcintyre in https://github.com/processing/p5.js/pull/7042\r\n* Update p5.Geometry references by @nickmcintyre in https://github.com/processing/p5.js/pull/7045\r\n* Update documentation_style_guide.md by @lottihill in https://github.com/processing/p5.js/pull/7053\r\n* Patch-CORNER-descr-cx by @JulioGitLab in https://github.com/processing/p5.js/pull/7062\r\n* Fix p5.Graphics.remove() reference by @nickmcintyre in https://github.com/processing/p5.js/pull/7057\r\n* Fix links and typos by @nickmcintyre in https://github.com/processing/p5.js/pull/7058\r\n* Update p5.Framebuffer references by @nickmcintyre in https://github.com/processing/p5.js/pull/7044\r\n\r\n## New Contributors 💗\r\n* @RuimingShen made their first contribution in https://github.com/processing/p5.js/pull/6979\r\n* @aGuyWhoMadeGames made their first contribution in https://github.com/processing/p5.js/pull/6996\r\n* @Papershine made their first contribution in https://github.com/processing/p5.js/pull/6788\r\n* @seralichtenhahn made their first contribution in https://github.com/processing/p5.js/pull/7006\r\n* @bobbykaz made their first contribution in https://github.com/processing/p5.js/pull/7024\r\n* @PalumboN made their first contribution in https://github.com/processing/p5.js/pull/7036\r\n* @IronBlood made their first contribution in https://github.com/processing/p5.js/pull/7027\r\n* @lottihill made their first contribution in https://github.com/processing/p5.js/pull/7053\r\n* @JulioGitLab made their first contribution in https://github.com/processing/p5.js/pull/7062\r\n\r\n**Full Changelog**: https://github.com/processing/p5.js/compare/v1.9.3...v1.9.4,v1.9.4,Qianqian Ye,,Qianqianye,GNU Lesser General Public License v2.1,p5.js,processing,89,javascript,graphics,education,learning,art,design,sound,html,p5js,creative-coding,processing,,,,,,,,,,/processing/p5.js,96,499,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/PolyMathOrg/PolyMath,https://github.com/PolyMathOrg/PolyMath,0,,0,0,1,0,0,0,0,1,0,0,0,0,Scientific Computing with Pharo,"<p align=""center""><img alt=""PolyMath"" src=""assets/logos/logo.png"" style=""width: 25%; height: 25%"">\n<h1 align=""center"">[PolyMath]</h1>\n  <p align=""center"">\n    Scientific Computing with Pharo\n    <br>\n    <a href=""https://github.com/PolyMathOrg/PolyMath/wiki""><strong>Explore the docs »</strong></a>\n    <br>\n    <br>\n    <a href=""https://github.com/PolyMathOrg/PolyMath/issues/new?labels=Type%3A+Defect"">Report a defect</a>\n    |\n    <a href=""https://github.com/PolyMathOrg/PolyMath/issues/new?labels=Type%3A+Feature"">Request feature</a>\n  </p>\n</p>\n\n[![Pharo version](https://img.shields.io/badge/Pharo-9.0-%23aac9ff.svg)](https://pharo.org/download)\n[![Pharo version](https://img.shields.io/badge/Pharo-10-%23aac9ff.svg)](https://pharo.org/download)\n[![CI matrix](https://github.com/PolyMathOrg/PolyMath/actions/workflows/smalltalk-ci.yml/badge.svg)](https://github.com/PolyMathOrg/PolyMath/actions/workflows/smalltalk-ci.yml)\n[![Coverage Status](https://coveralls.io/repos/github/PolyMathOrg/PolyMath/badge.svg?branch=master)](https://coveralls.io/github/PolyMathOrg/PolyMath?branch=master)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://raw.githubusercontent.com/PolyMathOrg/PolyMath/master/LICENSE)\n\n<img width=""1675"" alt=""Screenshot 2019-04-24 at 11 12 57"" src=""https://user-images.githubusercontent.com/327334/56652094-66eb7780-6682-11e9-9753-101be18df67c.png"">\n\n\nYou can load PolyMath 1.0.5 into a fresh Pharo 9.0 or 10 image with:\n\n```Smalltalk\nMetacello new\n        repository: 'github://PolyMathOrg/PolyMath:v1.0.5';\n        baseline: 'PolyMath';\n        load\n```\n\nand the latest development version of PolyMath:\n\n```Smalltalk\nMetacello new\n        repository: 'github://PolyMathOrg/PolyMath';\n        baseline: 'PolyMath';\n        load\n```\n\nWe have **900** green tests ! At the moment, all the development happens in the master branch (we are using [trunk-based development](https://trunkbaseddevelopment.com/)).\n\nPolyMath is a Pharo project, similar to existing scientific libraries like NumPy, SciPy for Python or SciRuby for Ruby. PolyMath already provides the following basic functionalities:\n- complex and quaternions extensions,\n- random number generators,\n- fuzzy algorithms,\n- automatic differentiation,\n- KDE-trees,\n- Numerical methods,\n- Ordinary Differential Equation (ODE) solvers.\n\nThe authoritative book on PolyMath is available online: https://github.com/SquareBracketAssociates/PolyMath-book\n\nSome documentation (work in progress) is available on the Wiki:\nhttps://github.com/PolyMathOrg/PolyMath/wiki\n\nNatalia wrote some explanation about benchmarking PolyMath in the Pharo For Enterprise Book: https://github.com/SquareBracketAssociates/PharoForTheEnterprise-english/blob/ae40e7ab6f7651f6e7c271869eb1efc4e531e774/ComparingSolutions/ComparingSolutions.pier\n\nTo add PolyMath to your baseline just add this:\n\n```Smalltalk\n    spec\n    	baseline: 'PolyMath'\n    	with: [ spec repository: 'github://PolyMathOrg/PolyMath:master/src' ]\n```\n\n## How to contribute to PolyMath\n\nWe welcome submissions! A google group exists for this project at http://groups.google.com/group/polymath-project\n",168,mathematics,Smalltalk,2,Smalltalk,StringTemplate,,,,,,,,,,,,,,,,,,,,,,,,,,,173,18,155,0,9,20,0,5135,41,152,96,56,d8d6fc0737322ff3b635410526ae360f175bb215,Merge pull request #326 from jordanmontt/patch-1,2024-05-02T10:02:49Z,Oleksandr Zaitsev,olk.zaytsev@gmail.com,olekscode,PolyMath v1.0.5,See changelog here: https://github.com/PolyMathOrg/PolyMath/milestone/6\r\nand all commits since v1.0.4: https://github.com/PolyMathOrg/PolyMath/compare/v1.0.4...v1.0.5\r\nThis version can be loaded with:\r\n\r\n```Smalltalk\r\nMetacello new\r\n        repository: 'github://PolyMathOrg/PolyMath:v1.0.5';\r\n        baseline: 'PolyMath';\r\n        load```,v1.0.5,Serge Stinckwich,,SergeStinckwich,MIT License,PolyMath,PolyMathOrg,7,pharo,mathematics,smalltalk,numerical-methods,pharo-smalltalk,,,,,,,,,,,,,,,,/PolyMathOrg/PolyMath,7,21,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/pmndrs/use-cannon,https://github.com/pmndrs/use-cannon,0,,,0,0,0,0,0,0,1,0,0,0,0,👋💣 physics based hooks for @react-three/fiber,[![Build Status](https://img.shields.io/github/actions/workflow/status/pmndrs/use-cannon/nodejs.yml?branch=master&style=flat&colorA=000000&logo=github)](https://github.com/pmndrs/use-cannon/actions/workflows/nodejs.yml)\n[![Discord Shield](https://img.shields.io/discord/740090768164651008?style=flat&colorA=000000&colorB=000000&label=discord&logo=discord&logoColor=ffffff)](https://discord.gg/poimandres)\n\n![Imgur](https://imgur.com/FpBsJPL.jpg)\n\nMonorepo for [cannon-es](https://github.com/pmndrs/cannon-es) web worker packages.\n\n| Package                                                                  | Description                                                                                                                                                      |\n| ------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`@react-three/cannon`](./packages/react-three-cannon)                   | React hooks for [cannon-es](https://github.com/pmndrs/cannon-es). Use this in combination with [react-three-fiber](https://github.com/pmndrs/react-three-fiber). |\n| [`@pmndrs/cannon-worker-api`](./packages/cannon-worker-api)              | Web worker api for [cannon-es](https://github.com/pmndrs/cannon-es). Used by `@react-three/cannon`.                                                              |\n| [`@react-three/cannon-examples`](./packages/react-three-cannon-examples) | Examples for `@react-three/cannon`                                                                                                                               |\n\n## `use-cannon` Documentation\n\nPlease see the [`@react-three/cannon` README](./packages/react-three-cannon/README.md) documentation and getting started guide for using the react hooks and jsx interface.\n\n## Demos\n\nCheck out all of our `@react-three/cannon` examples at https://cannon.pmnd.rs\n\nThe code for the examples lives in [./packages/react-three-cannon-examples](./packages/react-three-cannon-examples)\n,2738,physics,TypeScript,4,JavaScript,TypeScript,Shell,HTML,,,,,,,,,,,,,,,,,,,,,,,,,189,22,165,2,3,65,278,4360,153,148,118,30,601a417ad42d939467e1d73e99e73f859d083c65,chore: forgot to add yarn.lock,2024-02-25T02:30:14Z,Bjorn Stromberg,bjorn@bjornstar.com,bjornstar,@react-three/cannon@6.6.0,### Minor Changes\n\n-   22d49ef: chore: update @types/three dev dependency\n\n### Patch Changes\n\n-   Updated dependencies [800a687]\n-   Updated dependencies [22d49ef]\n    -   @pmndrs/cannon-worker-api@2.4.0\n,@react-three/cannon@6.6.0,,,github-actions[bot],,use-cannon,pmndrs,78,cannon-js,reactjs,physics,react,,,,,,,,,,,,,,,,,/pmndrs/use-cannon,131,31,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/plotly/react-plotly.js,https://github.com/plotly/react-plotly.js,0,,0,0,0,0,0,0,0,1,1,0,0,0,A plotly.js React component from Plotly 📈,"# react-plotly.js\n\n![plotly-react-logo](https://images.plot.ly/plotly-documentation/thumbnail/react.png)\n\n> A [plotly.js](https://github.com/plotly/plotly.js) React component from\n> [Plotly](https://plot.ly/). The basis of Plotly's\n> [React component suite](https://plot.ly/products/react/).\n\n👉 [DEMO](http://react-plotly.js-demo.getforge.io/)\n\n👉 [Demo source code](https://github.com/plotly/react-plotly.js-demo-app)\n\n<div align=""center"">\n  <a href=""https://dash.plotly.com/project-maintenance"">\n    <img src=""https://dash.plotly.com/assets/images/maintained-by-plotly.png"" width=""400px"" alt=""Maintained by Plotly"">\n  </a>\n</div>\n\n---\n\n## Contents\n\n- [Installation](#installation)\n- [Quick start](#quick-start)\n- [State management](#state-management)\n- [Refreshing the Plot](#refreshing-the-plot)\n- [API](#api)\n  - [Basic props](#basic-props)\n  - [Event handler props](#event-handler-props)\n- [Customizing the `plotly.js` bundle](#customizing-the-plotlyjs-bundle)\n- [Loading from a `<script>` tag](#loading-from-a-script-tag)\n- [Development](#development)\n\n## Installation\n\n```bash\n$ npm install react-plotly.js plotly.js\n```\n\n## Quick start\n\nThe easiest way to use this component is to import and pass data to a plot component:\n\n```javascript\nimport React from 'react';\nimport Plot from 'react-plotly.js';\n\nclass App extends React.Component {\n  render() {\n    return (\n      <Plot\n        data={[\n          {\n            x: [1, 2, 3],\n            y: [2, 6, 3],\n            type: 'scatter',\n            mode: 'lines+markers',\n            marker: {color: 'red'},\n          },\n          {type: 'bar', x: [1, 2, 3], y: [2, 5, 3]},\n        ]}\n        layout={{width: 320, height: 240, title: 'A Fancy Plot'}}\n      />\n    );\n  }\n}\n```\n\nYou should see a plot like this:\n\n<p align=""center"">\n <img src=""example.png"" alt=""Example plot"" width=""320"" height=""240"">\n</p>\n\nFor a full description of Plotly chart types and attributes see the following resources:\n\n- [Plotly JavaScript API documentation](https://plot.ly/javascript/)\n- [Full plotly.js attribute listing](https://plot.ly/javascript/reference/)\n\n## State management\n\nThis is a ""dumb"" component that doesn't merge its internal state with any updates. This means that if a user interacts with the plot, by zooming or panning for example, any subsequent re-renders will lose this information unless it is captured and upstreamed via the `onUpdate` callback prop.\n\nHere is a simple example of how to capture and store state in a parent object:\n\n```javascript\nclass App extends React.Component {\n  constructor(props) {\n    super(props);\n    this.state = {data: [], layout: {}, frames: [], config: {}};\n  }\n\n  render() {\n    return (\n      <Plot\n        data={this.state.data}\n        layout={this.state.layout}\n        frames={this.state.frames}\n        config={this.state.config}\n        onInitialized={(figure) => this.setState(figure)}\n        onUpdate={(figure) => this.setState(figure)}\n      />\n    );\n  }\n}\n```\n\n## Refreshing the Plot\n\nThis component will refresh the plot via [`Plotly.react`](https://plot.ly/javascript/plotlyjs-function-reference/#plotlyreact) if any of the following are true:\n\n- The `revision` prop is defined and has changed, OR;\n- One of `data`, `layout` or `config` has changed identity as checked via a shallow `===`, OR;\n- The number of elements in `frames` has changed\n\nFurthermore, when called, [`Plotly.react`](https://plot.ly/javascript/plotlyjs-function-reference/#plotlyreact) will only refresh the data being plotted if the _identity_ of the data arrays (e.g. `x`, `y`, `marker.color` etc) has changed, or if `layout.datarevision` has changed.\n\nIn short, this means that simply adding data points to a trace in `data` or changing a value in `layout` will not cause a plot to update unless this is done immutably via something like [immutability-helper](https://github.com/kolodny/immutability-helper) if performance considerations permit it, or unless `revision` and/or [`layout.datarevision`](https://plot.ly/javascript/reference/#layout-datarevision) are used to force a rerender.\n\n## API Reference\n\n### Basic Props\n\n**Warning**: for the time being, this component may _mutate_ its `layout` and `data` props in response to user input, going against React rules. This behaviour will change in the near future once https://github.com/plotly/plotly.js/issues/2389 is completed.\n\n| Prop               | Type                         | Default                                           | Description                                                                                                                                            |\n| ------------------ | ---------------------------- | ------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| `data`             | `Array`                      | `[]`                                              | list of trace objects (see https://plot.ly/javascript/reference/)                                                                                      |\n| `layout`           | `Object`                     | `undefined`                                       | layout object (see https://plot.ly/javascript/reference/#layout)                                                                                       |\n| `frames`           | `Array`                      | `undefined`                                       | list of frame objects (see https://plot.ly/javascript/reference/)                                                                                      |\n| `config`           | `Object`                     | `undefined`                                       | config object (see https://plot.ly/javascript/configuration-options/)                                                                                  |\n| `revision`         | `Number`                     | `undefined`                                       | When provided, causes the plot to update when the revision is incremented.                                                                             |\n| `onInitialized`    | `Function(figure, graphDiv)` | `undefined`                                       | Callback executed after plot is initialized. See below for parameter information.                                                                      |\n| `onUpdate`         | `Function(figure, graphDiv)` | `undefined`                                       | Callback executed when a plot is updated due to new data or layout, or when user interacts with a plot. See below for parameter information.           |\n| `onPurge`          | `Function(figure, graphDiv)` | `undefined`                                       | Callback executed when component unmounts, before `Plotly.purge` strips the `graphDiv` of all private attributes. See below for parameter information. |\n| `onError`          | `Function(err)`              | `undefined`                                       | Callback executed when a plotly.js API method rejects                                                                                                  |\n| `divId`            | `string`                     | `undefined`                                       | id assigned to the `<div>` into which the plot is rendered.                                                                                            |\n| `className`        | `string`                     | `undefined`                                       | applied to the `<div>` into which the plot is rendered                                                                                                 |\n| `style`            | `Object`                     | `{position: 'relative', display: 'inline-block'}` | used to style the `<div>` into which the plot is rendered                                                                                              |\n| `debug`            | `Boolean`                    | `false`                                           | Assign the graph div to `window.gd` for debugging                                                                                                      |\n| `useResizeHandler` | `Boolean`                    | `false`                                           | When true, adds a call to `Plotly.Plot.resize()` as a `window.resize` event handler                                                                    |\n\n**Note**: To make a plot responsive, i.e. to fill its containing element and resize when the window is resized, use `style` or `className` to set the dimensions of the element (i.e. using `width: 100%; height: 100%` or some similar values) and set `useResizeHandler` to `true` while setting `layout.autosize` to `true` and leaving `layout.height` and `layout.width` undefined. This can be seen in action in [this CodePen](https://codepen.io/nicolaskruchten/pen/ERgBZX) and will implement the behaviour documented here: https://plot.ly/javascript/responsive-fluid-layout/\n\n#### Callback signature: `Function(figure, graphDiv)`\n\nThe `onInitialized`, `onUpdate` and `onPurge` props are all functions which will be called with two arguments: `figure` and `graphDiv`.\n\n- `figure` is a serializable object with three keys corresponding to input props: `data`, `layout` and `frames`.\n  - As mentioned above, for the time being, this component may _mutate_ its `layout` and `data` props in response to user input, going against React rules. This behaviour will change in the near future once https://github.com/plotly/plotly.js/issues/2389 is completed.\n- `graphDiv` is a reference to the (unserializable) DOM node into which the figure was rendered.\n\n### Event handler props\n\nEvent handlers for specific [`plotly.js` events](https://plot.ly/javascript/plotlyjs-events/) may be attached through the following props:\n\n| Prop                      | Type       | Plotly Event                   |\n| ------------------------- | ---------- | ------------------------------ |\n| `onAfterExport`           | `Function` | `plotly_afterexport`           |\n| `onAfterPlot`             | `Function` | `plotly_afterplot`             |\n| `onAnimated`              | `Function` | `plotly_animated`              |\n| `onAnimatingFrame`        | `Function` | `plotly_animatingframe`        |\n| `onAnimationInterrupted`  | `Function` | `plotly_animationinterrupted`  |\n| `onAutoSize`              | `Function` | `plotly_autosize`              |\n| `onBeforeExport`          | `Function` | `plotly_beforeexport`          |\n| `onBeforeHover`           | `Function` | `plotly_beforehover`           |\n| `onButtonClicked`         | `Function` | `plotly_buttonclicked`         |\n| `onClick`                 | `Function` | `plotly_click`                 |\n| `onClickAnnotation`       | `Function` | `plotly_clickannotation`       |\n| `onDeselect`              | `Function` | `plotly_deselect`              |\n| `onDoubleClick`           | `Function` | `plotly_doubleclick`           |\n| `onFramework`             | `Function` | `plotly_framework`             |\n| `onHover`                 | `Function` | `plotly_hover`                 |\n| `onLegendClick`           | `Function` | `plotly_legendclick`           |\n| `onLegendDoubleClick`     | `Function` | `plotly_legenddoubleclick`     |\n| `onRelayout`              | `Function` | `plotly_relayout`              |\n| `onRelayouting`           | `Function` | `plotly_relayouting`           |\n| `onRestyle`               | `Function` | `plotly_restyle`               |\n| `onRedraw`                | `Function` | `plotly_redraw`                |\n| `onSelected`              | `Function` | `plotly_selected`              |\n| `onSelecting`             | `Function` | `plotly_selecting`             |\n| `onSliderChange`          | `Function` | `plotly_sliderchange`          |\n| `onSliderEnd`             | `Function` | `plotly_sliderend`             |\n| `onSliderStart`           | `Function` | `plotly_sliderstart`           |\n| `onSunburstClick`         | `Function` | `plotly_sunburstclick`         |\n| `onTransitioning`         | `Function` | `plotly_transitioning`         |\n| `onTransitionInterrupted` | `Function` | `plotly_transitioninterrupted` |\n| `onUnhover`               | `Function` | `plotly_unhover`               |\n| `onWebGlContextLost`      | `Function` | `plotly_webglcontextlost`      |\n\n## Customizing the `plotly.js` bundle\n\nBy default, the `Plot` component exported by this library loads a precompiled version of all of `plotly.js`, so `plotly.js` must be installed as a peer dependency. This bundle is around 6Mb unminified, and minifies to just over 2Mb.\n\nIf you do not wish to use this version of `plotly.js`, e.g. if you want to use a [different precompiled bundle](https://github.com/plotly/plotly.js/blob/master/dist/README.md#partial-bundles) or if your wish to [assemble you own customized bundle](https://github.com/plotly/plotly.js/blob/master/CUSTOM_BUNDLE.md), or if you wish to load `plotly.js` [from a CDN](https://github.com/plotly/plotly.js#use-the-plotlyjs-cdn-hosted-by-fastly), you can skip the installation of as a peer dependency (and ignore the resulting warning) and use the `createPlotComponent` method to get a `Plot` component, instead of importing it:\n\n```javascript\n// simplest method: uses precompiled complete bundle from `plotly.js`\nimport Plot from 'react-plotly.js';\n\n// customizable method: use your own `Plotly` object\nimport createPlotlyComponent from 'react-plotly.js/factory';\nconst Plot = createPlotlyComponent(Plotly);\n```\n\n## Loading from a `<script>` tag\n\nFor quick one-off demos on [CodePen](https://codepen.io/) or [JSFiddle](https://jsfiddle.net/), you may wish to just load the component directly as a script tag. We don't host the bundle directly, so you should never rely on this to work forever or in production, but you can use a third-party service to load the factory version of the component from, for example, [https://unpkg.com/react-plotly.js@latest/dist/create-plotly-component.js](https://unpkg.com/react-plotly.js@latest/dist/create-plotly-component.js).\n\nYou can load plotly.js and the component factory with:\n\n```html\n<script src=""https://cdn.plot.ly/plotly-latest.min.js""></script>\n<script src=""https://unpkg.com/react-plotly.js@latest/dist/create-plotly-component.js""></script>\n```\n\nAnd instantiate the component with\n\n```javascript\nconst Plot = createPlotlyComponent(Plotly);\n\nReactDOM.render(\n  React.createElement(Plot, {\n    data: [{x: [1, 2, 3], y: [2, 1, 3]}],\n  }),\n  document.getElementById('root')\n);\n```\n\nYou can see an example of this method in action\n[here](https://codepen.io/rsreusser/pen/qPgwwJ?editors=1010).\n\n## Development\n\nTo get started:\n\n```bash\n$ npm install\n```\n\nTo transpile from ES2015 + JSX into the ES5 npm-distributed version:\n\n```bash\n$ npm run prepublishOnly\n```\n\nTo run the tests:\n\n```bash\n$ npm run test\n```\n\n## License\n\n&copy; 2017-2020 Plotly, Inc. MIT License.\n",1004,bioinformatics,JavaScript,1,JavaScript,,,,,,,,,,,,,,,,,,,,,,,,,,,,71,19,46,6,6,96,38,5666,133,268,130,138,5cc0aa7dc755789a7d567a8e7ec149718146bea0,"Update README.md with ""maintained by Plotly"" badge",2024-06-07T17:44:17Z,Greg Wilson,gvwilson@third-bit.com,gvwilson,v2.6.0,#NAME?,v2.6.0,Alex Johnson,,alexcjohnson,MIT License,react-plotly.js,plotly,5,react,plotly,data-visualization,d3,charting-library,bioinformatics,fintech,,,,,,,,,,,,,,/plotly/react-plotly.js,19,45,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/plotly/dash-cytoscape,https://github.com/plotly/dash-cytoscape,0,,,0,0,0,0,0,0,1,1,0,0,0,"Interactive network visualization in Python and Dash, powered by Cytoscape.js","# Dash Cytoscape [![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/plotly/dash-cytoscape/blob/master/LICENSE) [![PyPi Version](https://img.shields.io/pypi/v/dash-cytoscape.svg)](https://pypi.org/project/dash-cytoscape/)\n\n<div align=""center"">\n  <a href=""https://dash.plotly.com/project-maintenance"">\n    <img src=""https://dash.plotly.com/assets/images/maintained-by-plotly.png"" width=""400px"" alt=""Maintained by Plotly"">\n  </a>\n</div>\n\n\n[![CircleCI](https://circleci.com/gh/plotly/dash-cytoscape.svg?style=svg)](https://circleci.com/gh/plotly/dash-cytoscape)\n\nA Dash component library for creating interactive and customizable networks in Python, wrapped around [Cytoscape.js](http://js.cytoscape.org/).\n\n![usage-stylesheet-demo](https://raw.githubusercontent.com/plotly/dash-cytoscape/master/demos/images/usage-stylesheet-demo.gif)\n\n-   🌟 [Medium Article](https://medium.com/@plotlygraphs/introducing-dash-cytoscape-ce96cac824e4)\n-   📣 [Community Announcement](https://community.plotly.com/t/announcing-dash-cytoscape/19095)\n-   💻 [Github Repository](https://github.com/plotly/dash-cytoscape)\n-   📚 [User Guide](https://dash.plotly.com/cytoscape)\n-   🗺 [Component Reference](https://dash.plotly.com/cytoscape/reference)\n-   📺 [Webinar Recording](https://www.youtube.com/watch?v=snXcIsCMQgk)\n\n## Getting Started in Python\n\n### Prerequisites\n\nMake sure that dash and its dependent libraries are correctly installed:\n\n```commandline\npip install dash\n```\n\nIf you want to install the latest versions, check out the [Dash docs on installation](https://dash.plotly.com/installation).\n\n### Usage\n\nInstall the library using `pip`:\n\n```\npip install dash-cytoscape\n```\n\nIf you wish to use the CyLeaflet mapping extension, you must install the optional `leaflet` dependencies:\n\n```\npip install dash-cytoscape[leaflet]\n```\n\nCreate the following example inside an `app.py` file:\n\n```python\nimport dash\nimport dash_cytoscape as cyto\nfrom dash import html\n\napp = dash.Dash(__name__)\napp.layout = html.Div([\n    cyto.Cytoscape(\n        id='cytoscape',\n        elements=[\n            {'data': {'id': 'one', 'label': 'Node 1'}, 'position': {'x': 50, 'y': 50}},\n            {'data': {'id': 'two', 'label': 'Node 2'}, 'position': {'x': 200, 'y': 200}},\n            {'data': {'source': 'one', 'target': 'two','label': 'Node 1 to 2'}}\n        ],\n        layout={'name': 'preset'}\n    )\n])\n\nif __name__ == '__main__':\n    app.run_server(debug=True)\n```\n\n![basic-usage](https://raw.githubusercontent.com/plotly/dash-cytoscape/master/demos/images/basic-usage.gif)\n\n### External layouts\n\nYou can also add external layouts. Use the `cyto.load_extra_layouts()` function to get started:\n\n```python\nimport dash\nimport dash_cytoscape as cyto\nfrom dash import html\n\ncyto.load_extra_layouts()\n\napp = dash.Dash(__name__)\napp.layout = html.Div([\n    cyto.Cytoscape(...)\n])\n```\n\nCalling `cyto.load_extra_layouts()` also enables generating SVG images.\n\n## Getting Started in R\n\n### Prerequisites\n\n```R\ninstall.packages(c(""devtools"", ""dash""))\n```\n\n### Usage\n\nInstall the library using devtools:\n\n```\ndevtools::install_github(""plotly/dash-cytoscape"")\n```\n\nCreate the following example inside an `app.R` file:\n\n```R\nlibrary(dash)\nlibrary(dashHtmlComponents)\nlibrary(dashCytoscape)\n\napp <- Dash$new()\n\napp$layout(\n  htmlDiv(\n    list(\n      cytoCytoscape(\n        id = 'cytoscape-two-nodes',\n        layout = list('name' = 'preset'),\n        style = list('width' = '100%', 'height' = '400px'),\n        elements = list(\n          list('data' = list('id' = 'one', 'label' = 'Node 1'), 'position' = list('x' = 75, 'y' = 75)),\n          list('data' = list('id' = 'two', 'label' = 'Node 2'), 'position' = list('x' = 200, 'y' = 200)),\n          list('data' = list('source' = 'one', 'target' = 'two'))\n        )\n      )\n    )\n  )\n)\n\napp$run_server()\n```\n\n## Documentation\n\nThe [Dash Cytoscape User Guide](https://dash.plotly.com/cytoscape/) contains everything you need to know about the library. It contains useful examples, functioning code, and is fully interactive. You can also use the [component reference](https://dash.plotly.com/cytoscape/reference/) for a complete and concise specification of the API.\n\nTo learn more about the core Dash components and how to use callbacks, view the [Dash documentation](https://dash.plotly.com/).\n\nFor supplementary information about the underlying Javascript API, view the [Cytoscape.js documentation](http://js.cytoscape.org/).\n\n## Contributing\n\nMake sure that you have read and understood our [code of conduct](CODE_OF_CONDUCT.md), then head over to [CONTRIBUTING](CONTRIBUTING.md) to get started.\n\n### Testing\n\nInstructions on how to run [tests](CONTRIBUTING.md#tests) are given in [CONTRIBUTING.md](CONTRIBUTING.md).\n\n## License\n\nDash, Cytoscape.js and Dash Cytoscape are licensed under MIT. Please view [LICENSE](LICENSE) for more details.\n\n## Contact and Support\n\nSee https://plotly.com/dash/support for ways to get in touch.\n\n## Acknowledgments\n\nHuge thanks to the Cytoscape Consortium and the Cytoscape.js team for their contribution in making such a complete API for creating interactive networks. This library would not have been possible without their massive work!\n\nThe Pull Request and Issue Templates were inspired from the\n[scikit-learn project](https://github.com/scikit-learn/scikit-learn).\n\n## Gallery\n\n### Dynamically expand elements\n\n[Code](usage-elements.py) | [Demo](https://dash-gallery.plotly.host/cytoscape-elements)\n![View usage-elements on Github](demos/images/usage-elements-demo.gif)\n\n### Interactively update stylesheet\n\n[Code](usage-stylesheet.py) | [Demo](https://dash-gallery.plotly.host/cytoscape-stylesheet)\n![View usage-stylesheet on Github](demos/images/usage-stylesheet.gif)\n\n### Automatically generate interactive phylogeny trees\n\n[Code](demos/usage-phylogeny.py) | [Demo](https://dash-gallery.plotly.host/cytoscape-phylogeny/)\n![View usage-phylogeny on Github](demos/images/usage-phylogeny.gif)\n\n### Create your own stylesheet\n\n[Code](usage-advanced.py) | [Demo](https://dash-gallery.plotly.host/cytoscape-advanced)\n![View usage-advanced on Github](demos/images/usage-advanced.gif)\n\n### Use event callbacks\n\n[Code](usage-events.py) | [Demo](https://dash-gallery.plotly.host/cytoscape-events)\n![View usage-events on Github](demos/images/usage-events.gif)\n\n### Use external layouts\n\n[Code](demos/usage-elements-extra.py)\n![View usage-elements-extra on Github](demos/images/usage-elements-extra.gif)\n\n### Use export graph as image\n\n[Code](demos/usage-image-export.py)\n![View usage-image-export on Github](demos/images/usage-image-export.gif)\n\n### Make graph responsive\n\n[Code](demos/usage-responsive-graph.py)\n![View usage-responsive-graph on Github](demos/images/usage-responsive-graph.gif)\n\nFor an extended gallery, visit the [demos' readme](demos/README.md).\n",585,bioinformatics,Python,5,CSS,Python,JavaScript,R,Julia,,,,,,,,,,,,,,,,,,,,,,,,108,28,74,6,27,33,49,80747,120,113,53,60,fa2800af9232e156e90be6bce2513b6fe8760f46,Merge pull request #219 from plotly/bugfix/cyleaflet-tiles-not-found,2024-07-11T09:10:38Z,Mario Rodriguez Ibañez,35932204+Farkites@users.noreply.github.com,Farkites,v1.0.1,## Added\r\n* Expose CyLeaflet subcomponent IDs + Add additional documentation of CyLeaflet design by @emilykl in https://github.com/plotly/dash-cytoscape/pull/211\r\n\r\n## Fixed\r\n* [#205](https://github.com/plotly/dash-cytoscape/pull/205) Fixed updating maxZoom via callback in CyLeaflet AIO component by @Farkites.\r\n* [#207](https://github.com/plotly/dash-cytoscape/pull/207) Allow access to updated lat/lon when Cytoscape nodes in CyLeaflet AIO component are modified via UI by @Farkites.\r\n* [#208](https://github.com/plotly/dash-cytoscape/pull/208) Allow updating Cytoscape elements in CyLeaflet AIO component via callback by @Farkites.\r\n* [#210](https://github.com/plotly/dash-cytoscape/pull/210) Performace improvement: added debounce to update elements event by @Farkites.\r\n* [#212](https://github.com/plotly/dash-cytoscape/pull/212) Added call to `cy.resize` to fix mouse position mismatch on `tapstart` event by @Farkites.,v1.0.1,Mario Rodriguez Ibañez,,Farkites,MIT License,dash-cytoscape,plotly,11,plotly-dash,plotly,dash,cytoscape,cytoscapejs,network-visualization,network-graph,graph-theory,bioinformatics,computational-biology,data-science,biopython,,,,,,,,,/plotly/dash-cytoscape,12,27,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/plotly/dash-bio,https://github.com/plotly/dash-bio,0,,,0,0,0,0,0,0,1,1,0,0,0,Open-source bioinformatics components for Dash,"# Dash Bio\n[![CircleCI](https://circleci.com/gh/plotly/dash-bio/tree/master.svg?style=svg)](https://circleci.com/gh/plotly/dash-bio)\n[![PyPI version](https://badge.fury.io/py/dash-bio.svg)](https://badge.fury.io/py/dash-bio)\n\n<div align=""center"">\n  <a href=""https://dash.plotly.com/project-maintenance"">\n    <img src=""https://dash.plotly.com/assets/images/maintained-by-plotly.png"" width=""400px"" alt=""Maintained by Plotly"">\n  </a>\n</div>\n\n\nDash Bio is a suite of bioinformatics components built to work with\n[Dash](https://github.com/plotly/dash/).\n\nAnnouncement: https://medium.com/@plotlygraphs/announcing-dash-bio-ed8835d5da0c\n\nDemo:\n[https://dash-gallery.plotly.host/Portal/?search=Bioinformatics](https://dash-gallery.plotly.host/Portal/?search=Bioinformatics)\n\nDocumentation:\n[https://dash.plotly.com/dash-bio](https://dash.plotly.com/dash-bio)\n\n## Components\n\nThe Dash Bio components each fall into one of three categories:\n\n- Custom chart types\n- Sequence analysis tools\n- 3D rendering tools\n\n\n### Custom chart types\n\n- Dash Circos\n- Dash Clustergram\n- Dash Manhattan Plot\n- Dash Needle Plot\n- Dash Volcano Plot\n\n### Sequence analysis tools\n\n- Dash Alignment Chart\n- Dash Onco Print\n- Dash Forna Container\n- Dash Sequence Viewer\n\n### Visualization tools\n\n- Dash Mol2D\n- Dash Mol3D\n- Dash Speck\n- Dash Ngl\n\n\n## Using Dash Bio\n\nIt's easy to add a fully interactive chromosomal, molecular or genomic visualization to your Dash app by simply\nincluding the Dash Bio component into your app layout as follows:\n\n```python\nimport urllib.request as urlreq\nfrom dash import Dash, html\nimport dash_bio as dashbio\n\napp = Dash(__name__)\n\ndata = urlreq.urlopen(\n    'https://raw.githubusercontent.com/plotly/dash-bio-docs-files/master/alignment_viewer_p53.fasta'\n).read().decode('utf-8')\n\napp.layout = html.Div([\n    dashbio.AlignmentChart(\n        id='my-default-alignment-viewer',\n        data=data\n    )\n])\n\nif __name__ == '__main__':\n    app.run_server(debug=True)\n```\n\nSee the [Dash Bio documentation](https://dash.plotly.com/dash-bio) for more components and examples.\n\n\n## Run Dash Bio in a JupyterLab environment\n\n1. Create a virtual environment:\n\n    The following steps require a virtual environment tool to be installed on your computer: `pip install virtualenv`\n\n    a. On macOS and Linux: `python3 -m venv env`\n\n    b. On Windows, enter: `py -m venv env`\n\n2. Activate your new environment:\n\n    a. On macOS and Linux, enter: `source env/bin/activate`\n\n    b. On Windows, enter: `.\env\Scripts\activate`\n\n3. Install required libraries (make sure you have pip installed with `pip help`):\n```\npip install dash dash-bio pandas numpy Jupyterlab\n```\n\n4. To run Dash inside Jupyter lab:\n\n    a. Install jupyter-dash:  `pip install jupyter-dash`\n\n    b. Enter `jupyter lab build`\n\n    (Note: This step requires Node.js and NPM installed on yourcomputer. To check if Node and NPM are installed, enter `node -v` and `npm -v` in your terminal. For install instructions see [nodejs.org](https://nodejs.org/en/).\n\n5. To display Plotly figures in JupyterLab:\n```\npip install jupyterlab ""ipywidgets>=7.5”\njupyter labextension install jupyterlab-plotly@4.14.3\n```\n\n6. Start JupyterLab by typing: `jupyter lab`\n\n    Important: JupyterLab must be run within the virtual environment that was previously activated.\n\n\nFor more on running a Dash app in Jupyter Lab visit [Getting Started with Jupyter Dash](https://github.com/plotly/jupyter-dash/blob/master/notebooks/getting_started.ipynb).\n\n## Dash\n\nLearn more about Dash at\n[https://plotly.com/products/dash/](https://plotly.com/products/dash/).\n\n## Consulting and OEM\n\nFor inquiries about Dash app development, advanced OEM integration,\nand more, please [reach\nout](https://plotly.typeform.com/to/mH1Cpb).\n\n## Contributing and Local Development\n\nIf you would like to contribute to this repository, or run demo apps and tests, please refer to\nthe [contributing\nguidelines](https://github.com/plotly/dash-bio/blob/master/CONTRIBUTING.md).\n",525,bioinformatics,Python,6,Python,JavaScript,CSS,R,Roff,Procfile,,,,,,,,,,,,,,,,,,,,,,,515,116,369,30,66,114,1717,298011,195,255,217,38,f6bd02de725a86779f6f0be260e730e1970cea5f,"Update README.md with ""maintained by Plotly"" badge",2024-06-07T17:18:47Z,Greg Wilson,gvwilson@third-bit.com,gvwilson,1.0.2 release,### Fixed\r\n* [#675](https://github.com/plotly/dash-bio/pull/675) Fixed an issue with the JS resource files in the npm release of dash-bio. As pointed out in [this community post](https://community.plotly.com/t/unable-to-use-dash-bio-for-plotting-ideograms-due-to-incorrect-javascript-dependency/61939/5) some of the dependencies had not been resolved at their external `unpkg` URL.\r\n* [#671](https://github.com/plotly/dash-bio/pull/671) Fixed Onco-Print range property description.\r\n,v1.0.2,,,HammadTheOne,MIT License,dash-bio,plotly,22,bioinformatics,biojs,dash,,,,,,,,,,,,,,,,,,/plotly/dash-bio,25,44,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/plotly/Dash.jl,https://github.com/plotly/Dash.jl,0,,,0,0,0,0,0,0,1,1,0,0,0,Dash for Julia - A Julia interface to the Dash ecosystem for creating analytic web applications in Julia. No JavaScript required.,"# Dash for Julia\n\n[![Juila tests](https://github.com/plotly/Dash.jl/actions/workflows/jl_test.yml/badge.svg?query=branch%3Adev)](https://github.com/plotly/Dash.jl/actions/workflows/jl_test.yml?query=branch%3Adev)\n[![CircleCI](https://img.shields.io/circleci/build/github/plotly/Dash.jl/dev.svg)](https://circleci.com/gh/plotly/Dash.jl/tree/dev)\n[![GitHub](https://img.shields.io/github/license/plotly/Dash.jl.svg?color=dark-green)](https://github.com/plotly/Dash.jl/blob/master/LICENSE)\n[![GitHub commit activity](https://img.shields.io/github/commit-activity/y/plotly/Dash.jl.svg?color=dark-green)](https://github.com/plotly/Dash.jl/graphs/contributors)\n\n<div align=""center"">\n  <a href=""https://dash.plotly.com/project-maintenance"">\n    <img src=""https://dash.plotly.com/assets/images/maintained-by-community.png"" width=""400px"" alt=""Maintained by the Plotly Community"">\n  </a>\n</div>\n\n#### Create beautiful, analytic applications in Julia\n\nBuilt on top of Plotly.js, React and HTTP.jl, [Dash](https://plotly.com/dash/) ties modern UI elements like dropdowns, sliders, and graphs directly to your analytical Julia code.\n\nJust getting started? Check out the [Dash for Julia User Guide](https://dash.plotly.com/julia)! If you can't find documentation there, then check out the unofficial [contributed examples](https://github.com/plotly/Dash.jl/issues/50) or check out source code from [demo applications](https://dash.gallery) in Python and then reference the Julia syntax style.\n\n## Other resources\n\n* <https://community.plotly.com/c/plotly-r-matlab-julia-net/julia/23>\n* <https://discourse.julialang.org/tag/dash>\n\n## Project Status\n\nJulia components can be generated in tandem with Python and R components. Interested in getting involved with the project? Sponsorship is a great way to accelerate the progress of open source projects like this one; please feel free to [reach out to us](https://plotly.com/consulting-and-oem/)!\n\n## Installation\n\nTo install the most recently released version:\n\n```julia\npkg> add Dash\n```\n\nTo install the latest (stable) development version instead:\n\n```julia\npkg> add Dash#dev\n```\n\n## Usage\n\n### Basic application\n\n```julia\nusing Dash\n\napp = dash(external_stylesheets = [""https://codepen.io/chriddyp/pen/bWLwgP.css""])\n\napp.layout = html_div() do\n    html_h1(""Hello Dash""),\n    html_div(""Dash.jl: Julia interface for Dash""),\n    dcc_graph(id = ""example-graph"",\n              figure = (\n                  data = [\n                      (x = [1, 2, 3], y = [4, 1, 2], type = ""bar"", name = ""SF""),\n                      (x = [1, 2, 3], y = [2, 4, 5], type = ""bar"", name = ""Montréal""),\n                  ],\n                  layout = (title = ""Dash Data Visualization"",)\n              ))\nend\n\nrun_server(app)\n```\n\n__then go to `http://127.0.0.1:8050` in your browser to view the Dash app!__\n\n### Basic Callback\n\n```julia\nusing Dash\n\napp = dash(external_stylesheets = [""https://codepen.io/chriddyp/pen/bWLwgP.css""])\n\napp.layout = html_div() do\n    dcc_input(id = ""my-id"", value = ""initial value"", type = ""text""),\n    html_div(id = ""my-div"")\nend\n\ncallback!(app, Output(""my-div"", ""children""), Input(""my-id"", ""value"")) do input_value\n    ""You've entered $(input_value)""\nend\n\nrun_server(app)\n```\n\n* You can make your Dash app interactive by registering callbacks with the `callback!` function.\n* Outputs and inputs (and states, see below) of callback can be `Output`, `Input`, `State` objects or splats / vectors of this objects.\n* Callback functions must have the signature ``(inputs..., states...)``, and provide a return value with the same number elements as the number of `Output`s to update.\n\n### States and Multiple Outputs\n\n```julia\nusing Dash\n\napp = dash(external_stylesheets = [""https://codepen.io/chriddyp/pen/bWLwgP.css""])\n\napp.layout = html_div() do\n    dcc_input(id = ""my-id"", value = ""initial value"", type = ""text""),\n    html_div(id = ""my-div""),\n    html_div(id = ""my-div2"")\nend\n\ncallback!(app,\n          Output(""my-div"",""children""),\n          Output(""my-div2"",""children""),\n          Input(""my-id"", ""value""),\n          State(""my-id"", ""type"")) do input_value, state_value\n    return (""You've entered $(input_value) in input with type $(state_value)"",\n            ""You've entered $(input_value)"")\nend\n\nrun_server(app)\n```\n\n## Comparison with original Python syntax\n\n### Component naming\n\n* Python:\n\n```python\nimport dash\n\ndash.html.Div\ndash.dcc.Graph\ndash.dash_table.DataTable\n```\n\n* Dash.jl:\n\n```julia\nusing Dash\n\nhtml_div\ndcc_graph\ndash_datatable\n```\n\n### Component creation\n\nJust as in Python, functions for declaring components have keyword arguments, which are the same as in Python. ``html_div(id = ""my-id"", children = ""Simple text"")``.\n\nFor components which declare `children`, two additional signatures are available:\n\n* ``(children; kwargs..)`` and\n* ``(children_maker::Function; kwargs...)``\n\nSo one can write ``html_div(""Simple text"", id = ""my-id"")`` for simple elements, or choose an abbreviated syntax with `do` syntax for complex elements:\n\n```julia\nhtml_div(id = ""outer-div"") do\n    html_h1(""Welcome""),\n    html_div(id = ""inner-div"") do\n        #= inner content =#\n    end\nend\n```\n\n### Application and layout\n\n* Python:\n\n```python\napp = dash.Dash(external_stylesheets=[""https://codepen.io/chriddyp/pen/bWLwgP.css""])\n\napp.layout = html.Div(children=[....])\n```\n\n* Dash.jl:\n\n```julia\napp = dash(external_stylesheets = [""https://codepen.io/chriddyp/pen/bWLwgP.css""])\n\napp.layout = html_div() do\n    #= inner content =#\nend\n```\n\n### Callbacks\n\n* Python:\n\n```python\n@app.callback(Output('output', 'children'),\n              Input('submit-button', 'n_clicks')],\n              State('state-1', 'value'),\n              State('state-2', 'value'))\ndef update_output(n_clicks, state1, state2):\n    # logic\n```\n\n* Dash.jl:\n\n```julia\ncallback!(app,\n          Output(""output"", ""children""),\n          Input(""submit-button"", ""n_clicks"")],\n          State(""state-1"", ""value""),\n          State(""state-2"", ""value"")) do n_clicks, state1, state2\n    # logic\nend\n```\n\n### JSON\n\nDash apps transfer data between the browser (aka the frontend) and the Julia process running the app (aka the backend) in JSON.\nDash.jl uses [JSON3.jl](https://github.com/quinnj/JSON3.jl) for JSON serialization/deserialization.\n\nNote that JSON3.jl converts\n\n* `Vector`s and `Tuple`s to JSON arrays\n* `Dict`s and `NamedTuple`s to JSON objects\n",483,bioinformatics,Julia,5,Julia,JavaScript,Python,CSS,Dockerfile,,,,,,,,,,,,,,,,,,,,,,,,113,22,89,2,5,28,0,80464,40,124,85,39,4d93ee8654a965eb188a7e90fad2f5f4c261c751,"Update README.md with ""maintained by community"" badge",2024-06-07T20:06:13Z,Greg Wilson,gvwilson@third-bit.com,gvwilson,Version 1.5.0,## Changed\r\n\r\n- Update callback `Dependency`  constructor dispatches for Julia 1.10. It seems that Julia 1.10 is more strict about\r\nelement type during conversion to `<:Vector` https://github.com/plotly/Dash.jl/pull/231\r\n\r\n## Internal\r\n\r\n### Tests\r\n\r\n- Update CircleCI browser tools to 1.4.6 https://github.com/plotly/Dash.jl/pull/234\r\n\r\n---\r\n\r\nFull diff: https://github.com/plotly/Dash.jl/compare/v1.4.0...v1.5.0,v1.5.0,etpinard,,etpinard,MIT License,Dash.jl,plotly,12,dash,plotly,julia,dashboard,data-visualization,data-science,gui-framework,react,finance,bioinformatics,technical-computing,charting,plotly-dash,web-app,productivity,no-javascript,modeling,no-vba,,,/plotly/Dash.jl,17,17,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/plotly/dash,https://github.com/plotly/dash,0,,,0,0,0,0,0,0,1,1,0,0,0,Data Apps & Dashboards for Python. No JavaScript Required.,"# Dash\n\n[![CircleCI](https://img.shields.io/circleci/project/github/plotly/dash/master.svg)](https://circleci.com/gh/plotly/dash)\n[![GitHub](https://img.shields.io/github/license/plotly/dash.svg?color=dark-green)](https://github.com/plotly/dash/blob/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/dash.svg?color=dark-green)](https://pypi.org/project/dash/)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/dash.svg?color=dark-green)](https://pypi.org/project/dash/)\n[![GitHub commit activity](https://img.shields.io/github/commit-activity/y/plotly/dash.svg?color=dark-green)](https://github.com/plotly/dash/graphs/contributors)\n\n#### *Dash is the most downloaded, trusted Python framework for building ML & data science web apps*.\n\nBuilt on top of [Plotly.js](https://github.com/plotly/plotly.js), [React](https://reactjs.org/) and [Flask](https://palletsprojects.com/p/flask/), Dash ties modern UI elements like dropdowns, sliders, and graphs directly to your analytical Python code. Read [our tutorial](https://dash.plotly.com/getting-started) (proudly crafted ❤️ with Dash itself).\n\n- [Docs](https://dash.plotly.com/getting-started): Create your first Dash app in under 5 minutes\n\n- [dash.gallery](https://dash.gallery): Dash app gallery with Python & R code\n\n<div align=""center"">\n  <a href=""https://dash.plotly.com/project-maintenance"">\n    <img src=""https://dash.plotly.com/assets/images/maintained-by-plotly.png"" width=""400px"" alt=""Maintained by Plotly"">\n  </a>\n</div>\n\n\n### Dash App Examples\n\n| Dash App | Description |\n|--- | :---: |\n|![Sample Dash App](https://user-images.githubusercontent.com/1280389/30086128-9bb4a28e-9267-11e7-8fe4-bbac7d53f2b0.gif) | Here’s a simple example of a Dash App that ties a Dropdown to a Plotly Graph. As the user selects a value in the Dropdown, the application code dynamically exports data from Google Finance into a Pandas DataFrame. This app was written in just **43** lines of code ([view the source](https://gist.github.com/chriddyp/3d2454905d8f01886d651f207e2419f0)). |\n|![Crossfiltering Dash App](https://user-images.githubusercontent.com/1280389/30086123-97c58bde-9267-11e7-98a0-7f626de5199a.gif)|Dash app code is declarative and reactive, which makes it easy to build complex apps that contain many interactive elements. Here’s an example with 5 inputs, 3 outputs, and cross filtering. This app was composed in just 160 lines of code, all of which were Python.|\n|![Dash App with Mapbox map showing walmart store openings](https://user-images.githubusercontent.com/1280389/30086299-768509d0-9268-11e7-8e6b-626ac9ca512c.gif)| Dash uses [Plotly.js](https://github.com/plotly/plotly.js) for charting. About 50 chart types are supported, including maps. |\n|![Financial report](https://user-images.githubusercontent.com/2678795/161153710-57952401-6e07-42d5-ba3e-bab6419998c7.gif)| Dash isn't just for dashboards. You have full control over the look and feel of your applications. Here's a Dash App that's styled to look like a PDF report. |\n\nTo learn more about Dash, read the [extensive announcement letter](https://medium.com/@plotlygraphs/introducing-dash-5ecf7191b503) or [jump in with the user guide](https://plotly.com/dash).\n\n### Dash OSS & Dash Enterprise\n\nWith Dash Open Source, Dash apps run on your local laptop or workstation, but cannot be easily accessed by others in your organization.\n\nScale up with Dash Enterprise when your Dash app is ready for department or company-wide consumption. Or, launch your initiative with Dash Enterprise from the start to unlock developer productivity gains and hands-on acceleration from Plotly's team.\n\nML Ops Features: A one-stop shop for ML Ops: Horizontally scalable hosting, deployment, and authentication for your Dash apps. No IT or DevOps required.\n- [**App manager**](https://plotly.com/dash/app-manager/) Deploy & manage Dash apps without needing IT or a DevOps team. App Manager gives you point & click control over all aspects of your Dash deployments.\n- [**Kubernetes scaling**](https://plotly.com/dash/kubernetes/) Ensure high availability of Dash apps and scale horizontally with Dash Enterprise’s Kubernetes architecture. No IT or Helm required.\n- [**No code auth**](https://plotly.com/dash/authentication/) Control Dash app access in a few clicks. Dash Enterprise supports LDAP, AD, PKI, Okta, SAML, OpenID Connect, OAuth, SSO, and simple email authentication.\n- [**Job Queue**](https://plotly.com/dash/job-queue/) The Job Queue is the key to building scalable Dash apps. Move heavy computation from synchronous Dash callbacks to the Job Queue for asynchronous background processing.\n\nLow-Code Features: Low-code Dash app capabilities that supercharge developer productivity.\n- [**Design Kit**](https://plotly.com/dash/design-kit/) Design like a pro without writing a line of CSS. Easily arrange, style, brand, and customize your Dash apps.\n- [**Snapshot Engine**](https://plotly.com/dash/snapshot-engine/) Save & share Dash app views as links or PDFs. Or, run a Python job through Dash and have Snapshot Engine email a report when the job is done.\n- [**Dashboard Toolkit**](https://plotly.com/dash/toolkit/) Drag & drop layouts, chart editing, and crossfilter for your Dash apps.\n- [**Embedding**](https://plotly.com/dash/embedding/) Natively embed Dash apps in an existing web application or website without the use of IFrames.\n\nEnterprise AI Features: Everything that your data science team needs to rapidly deliver AI/ML research and business initiatives.\n- [**AI App Marketplace**](https://plotly.com/dash/ai-and-ml-templates/) Dash Enterprise ships with dozens of Dash app templates for business problems where AI/ML is having the greatest impact.\n- [**Big Data for Pything**](https://plotly.com/dash/big-data-for-python/) Connect to Python's most popular big data back ends: Dask, Databricks, NVIDIA RAPIDS, Snowflake, Postgres, Vaex, and more.\n- [**GPU & Dask Acceleration**](https://plotly.com/dash/gpu-dask-acceleration/) Dash Enterprise puts Python’s most popular HPC stack for GPU and parallel CPU computing in the hands of business users.\n- [**Data Science Workspaces**](https://plotly.com/dash/workspaces/) Be productive from Day 1. Write and execute Python, R, & Julia code from Dash Enterprise's onboard code editor.\n\nSee [https://plotly.com/contact-us/](https://plotly.com/contact-us/) to get in touch.\n\n![Dash Enterprise](https://user-images.githubusercontent.com/2678795/161155614-21c54a22-f821-4dda-b910-ee27e27fb5f2.png)\n",20914,bioinformatics,Python,7,Python,JavaScript,CSS,HTML,TypeScript,Shell,Less,,,,,,,,,,,,,,,,,,,,,,1088,237,804,47,43,195,0,163826,2014,1731,1142,589,c847882b6bb8d5b86af5df6dad97cab61c0ae2c2,Merge pull request #2923 from plotly/rm-polyfill,2024-07-16T18:27:03Z,Philippe Duval,t4rk@outlook.com,T4rk1n,Dash v2.17.1,## Fixed\r\n\r\n- [#2860](https://github.com/plotly/dash/pull/2860) Fix dcc.Loading to apply overlay_style only to the children and not the spinner. Fixes [#2858](https://github.com/plotly/dash/issues/2858)\r\n- [#2854](https://github.com/plotly/dash/pull/2854) Fix dcc.Dropdown resetting empty values to null and triggering callbacks. Fixes [#2850](https://github.com/plotly/dash/issues/2850)\r\n- [#2859](https://github.com/plotly/dash/pull/2859) Fix base patch operators. fixes [#2855](https://github.com/plotly/dash/issues/2855)\r\n- [#2856](https://github.com/plotly/dash/pull/2856) Fix multiple consecutive calls with same id to set_props only keeping the last props. Fixes [#2852](https://github.com/plotly/dash/issues/2852)\r\n- [#2867](https://github.com/plotly/dash/pull/2867) Fix clientside no output callback. Fixes [#2866](https://github.com/plotly/dash/issues/2866)\r\n- [#2876](https://github.com/plotly/dash/pull/2876) Fix pattern matching in callback running argument. Fixes [#2863](https://github.com/plotly/dash/issues/2863),v2.17.1,Philippe Duval,,T4rk1n,MIT License,dash,plotly,70,dash,plotly,data-visualization,data-science,gui-framework,flask,react,python,finance,bioinformatics,technical-computing,charting,plotly-dash,web-app,productivity,modeling,r,rstats,jupyter,julia,/plotly/dash,105,419,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/PlasmaPy/PlasmaPy,https://github.com/PlasmaPy/PlasmaPy,0.5,not a project,0,0,1,1,0,0,0,1,0,0,0,0,An open source Python package for plasma research and education,"<div align=""center""><img src=""https://raw.githubusercontent.com/PlasmaPy/PlasmaPy-logo/main/exports/with-text-dark.png"" width=""600""/></div>\n\n# PlasmaPy\n\n[![PyPI version](https://img.shields.io/pypi/v/plasmapy?style=flat&logo=pypi)](https://pypi.org/project/plasmapy/)\n[![Conda version](https://img.shields.io/conda/v/conda-forge/plasmapy?style=flat&logo=anaconda)](https://img.shields.io/conda/v/conda-forge/plasmapy)\n[![PyPI version](https://img.shields.io/pypi/pyversions/plasmapy?style=flat&logo=python)](https://img.shields.io/pypi/pyversions/plasmapy?style=plastic)\n[![License](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](./LICENSE.md)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](https://docs.plasmapy.org/en/latest/CODE_OF_CONDUCT.html)\n\n[![Matrix](https://img.shields.io/badge/Matrix-join%20chat-blueviolet?style=flat&logo=matrix)](https://app.element.io/#/room/#plasmapy:openastronomy.org)\n<a rel=""me"" href=""https://fosstodon.org/@plasmapy"">![Mastodon](https://img.shields.io/badge/Mastodon-plasmapy%40fosstodon.org-blue?logo=mastodon&style=fla)</a>\n[![YouTube](https://img.shields.io/badge/YouTube%20-subscribe-red?style=flat&logo=youtube)](https://www.youtube.com/channel/UCSH6qzslhqIZKTAJmHPxIxw)\n\n[![CI](https://github.com/PlasmaPy/PlasmaPy/actions/workflows/ci.yml/badge.svg)](https://github.com/PlasmaPy/PlasmaPy/actions/workflows/ci.yml)\n[![weekly tests](https://github.com/PlasmaPy/PlasmaPy/actions/workflows/weekly.yml/badge.svg)](https://github.com/PlasmaPy/PlasmaPy/actions/workflows/weekly.yml)\n[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/PlasmaPy/PlasmaPy/main.svg)](https://results.pre-commit.ci/latest/github/PlasmaPy/PlasmaPy/main)\n[![codecov](https://codecov.io/gh/PlasmaPy/PlasmaPy/branch/main/graph/badge.svg)](https://codecov.io/gh/PlasmaPy/PlasmaPy)\n[![Read the Docs Status](https://readthedocs.org/projects/plasmapy/badge/?version=latest)](http://plasmapy.readthedocs.io/en/latest/?badge=latest)\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1436011.svg)](https://doi.org/10.5281/zenodo.1436011)\n[![astropy](http://img.shields.io/badge/powered%20by-AstroPy-orange.svg?style=flat&logo=astropy)](http://www.astropy.org/)\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)\n\n[Astropy]: https://www.astropy.org\n[authors and credits]: https://docs.plasmapy.org/en/latest/about/credits.html\n[3-clause BSD license]: ./LICENSE.md\n[calendar]: https://calendar.google.com/calendar/embed?src=c_sqqq390s24jjfjp3q86pv41pi8%40group.calendar.google.com&ctz=America%2FNew_York\n[cite PlasmaPy]: https://docs.plasmapy.org/en/latest/about/citation.html\n[code of conduct]: http://docs.plasmapy.org/en/latest/CODE_OF_CONDUCT.html\n[community meetings]: https://www.plasmapy.org/meetings/weekly\n[contributor guide]: https://docs.plasmapy.org/en/latest/development/index.html\n[Department of Energy]: https://www.energy.gov\n[example gallery]: https://docs.plasmapy.org/en/stable/examples.html\n[GitHub discussions]: https://github.com/PlasmaPy/PlasmaPy/discussions\n[Gitter]: https://gitter.im/PlasmaPy/Lobby\n[good first issues]: https://github.com/PlasmaPy/PlasmaPy/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22\n[Google Summer of Code]: https://summerofcode.withgoogle.com\n[**installing PlasmaPy**]: https://docs.plasmapy.org/en/stable/install.html\n[GitHub repository]: https://github.com/PlasmaPy/PlasmaPy\n[mailing list]: https://groups.google.com/forum/#!forum/plasmapy\n[Matrix]: https://app.element.io/#/room/#plasmapy:openastronomy.org\n[meetings]: https://www.plasmapy.org/meetings/weekly\n[NASA]: https://www.nasa.gov/\n[National Science Foundation]: https://nsf.gov\n[office hours]: http://www.plasmapy.org/meetings/office_hours\n[PlasmaPy Community on Zenodo]: https://zenodo.org/communities/plasmapy\n[PlasmaPy]: https://www.plasmapy.org\n[**documentation**]: https://docs.plasmapy.org\n[protections against software patents]: ./PATENT.md\n[Python]: https://www.python.org\n[Smithsonian Institution]: https://www.si.edu\n[submit a bug report]: https://github.com/PlasmaPy/PlasmaPy/issues/new?assignees=&labels=Bug&template=bug_report.yml\n[submit a feature request]: https://github.com/PlasmaPy/PlasmaPy/issues/new?assignees=&labels=Feature+request&template=feature_request.yml\n[team@plasmapy.org]: mailto:team@plasmapy.org\n[this video]: https://youtu.be/E8RwQF5wcXM\n[Zoom]: https://zoom.us/j/91633383503?pwd=QWNkdHpWeFhrYW1vQy91ODNTVG5Ndz09\n\n[PlasmaPy] is an open source, community-developed [Python] package for\nplasma research and education. PlasmaPy intends to be for plasma\nscience what [Astropy] is for astronomy — a collection of\nfunctionality commonly needed by plasma scientists and researchers\nglobally, running within and leveraging the open source scientific\nPython ecosystem. The goals of PlasmaPy are more thoroughly described\nin [this video]. Many of our recent presentations are available from\nthe [PlasmaPy Community on Zenodo].\n\n## Documentation\n\nPlease check out our online [**documentation**] to learn more about\nPlasmaPy's capabilities.\n\nIf you would like an idea of what PlasmaPy can do, go to our [example\ngallery] of Jupyter notebooks. To learn more about how to contribute,\ncheck out PlasmaPy's [contributor guide].\n\n## Installing PlasmaPy\n\nPlasmaPy's online documentation has detailed instructions on [**how to\ninstall PlasmaPy**].\n\nTo install PlasmaPy on macOS or Linux, open a terminal and run:\n```Shell\npython -m pip install plasmapy\n```\n\n> [!NOTE]\n> On some systems, it might be necessary to specify the Python version\n> number, for example by using `python3` or `python3.12` instead of\n> `python`.\n\nTo install PlasmaPy on Windows, open a terminal and run\n```Shell\npy -3.12 -m pip install plasmapy\n```\nThe `3.12` may be replaced by any version of Python that is installed\nand supported by PlasmaPy.\n\n## Citing PlasmaPy\n\nIf you use PlasmaPy for research resulting in a publication, please\n[cite PlasmaPy]. It really helps support the project! Citing software\nused in research provides credit to its authors, promotes open science &\nscientific reproducibility, and helps open source projects demonstrate\nto funding agencies that continued development should be supported.\n\nPlease check out the [PlasmaPy community on Zenodo] for prior releases\nof PlasmaPy and other resources.\n\n## Requesting features\n\nPlease [submit a feature request] in our [GitHub repository] if you\nhave an idea for new or improved functionality. PlasmaPy is\ncommunity-driven, and feature requests really help guide the future of\nthe project.\n\n## Submitting bug reports\n\nPlease [submit a bug report] on PlasmaPy's GitHub repository if you\nnotice any problems. We really appreciate it!\n\n## Contributing\n\nIf you are interested in contributing, please check out our [contributor\nguide] and [code of conduct]. There are a number of [good first issues]\nin our GitHub repository. New contributors are very welcome!\n\n## Events\n\nPlasmaPy has several [meetings] that are on our [calendar]. Events are\nusually held on PlasmaPy's [Zoom] room.\n\nLast-minute changes are usually announced on the [Matrix]/[Gitter]\nchat room. The most up-to-date information about these meetings is on\nthe [meetings] page of PlasmaPy's website.\n\n### Office hours\n\nOur weekly informal [office hours] are an opportunity to chat with\nactive members of the PlasmaPy community about topics related to\nPython and plasma science. If you'd like to learn more about PlasmaPy,\nour office hours are one of the best places to start. As of July\n2024, our office hours are on most Thursdays at 3 pm Eastern. Please\nfeel free to come by!\n\n### Community meetings\n\nPlasmaPy's weekly [community meetings] are a place to talk about code\ndevelopment. If you have an idea for a new feature or would like to\nmake a code contribution, community meetings are a good place to go\nto. As of July 2024, our community meetings are on most Tuesdays at 2 pm\nEastern.\n\n<!--\n### Project meetings\n\nPlasmaPy's weekly project meetings are a place to discuss education,\noutreach, and project coordination. Topics might range from creating\neducational notebooks to organizing community events. As of July\n2024, project meetings are held on most Wednesdays at 3 pm Eastern.\n-->\n\n<!--\n### Working group meetings\n\nPlasmaPy has started several working groups, including on diagnostics,\ndispersion relations, and simulation. These working groups usually\nmeet fortnightly, and their meeting times can be found in PlasmaPy's\nevent [calendar]. If you would like to join a PlasmaPy working group\nor even start a new one, please email us at [team@plasmapy.org]!\n-->\n\n## Community\n\n### Matrix chat\n\nIf you have any questions, the quickest way to get a response is to\nask on our [Matrix]/[Gitter] channel. Both of these are the same chat\nchannel; Gitter uses a bridge to link the two.\n\n### GitHub discussions\n\nWe're trying out [GitHub discussions] as a place to suggest ideas,\nbring up discussion topics, and ask questions.\n\n### Mailing list\n\nYou can subscribe to PlasmaPy's low-volume [mailing list] to receive\nPlasmaPy newsletters and other announcements.\n\n## Contact information\n\nPlease feel free to reach out to us at [team@plasmapy.org] or stop by\nour [office hours] with any ideas, questions, and/or puns about\ncomputational magnetohydrodynamics.\n\nPlease use these links to [submit a feature request] and to [submit a\nbug report] on PlasmaPy's GitHub repository.\n\n## License\n\nPlasmaPy is permissively licensed under a [3-clause BSD license] with\nadded [protections against software patents].\n\n## Acknowledgments\n\nDevelopment of PlasmaPy has been supported in part by the [National\nScience Foundation], [Department of Energy], [NASA], and the\n[Smithsonian Institution]. For more details, please see PlasmaPy's\ndocumentation page on [authors and credits].\n",548,physics,Python,2,Python,Jupyter Notebook,,,,,,,,,,,,,,,,,,,,,,,,,,,1835,248,1521,66,12,128,0,33422,311,900,588,312,9601792715d53d75e48fdf187023054ae4a4d923,Update requirements with 'nox -s requirements' (#2776),2024-07-19T20:14:05Z,plasmapy-requirements-bot[bot],134649236+plasmapy-requirements-bot[bot]@users.noreply.github.com,plasmapy-requirements-bot[bot],v2024.5.0,"## What's Changed\r\n* Pull changes from the `v2024.2.0` back into `main` by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2500\r\n* Drop python 3.9 support by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2501\r\n* Alphabetize authors in `CITATION.cff` by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2503\r\n* Apply ruff rules `UP006` and `UP007` by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2504\r\n* Update `@particle_input` docstring to use type union syntax instead of `Optional` by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2505\r\n* Update release checklist by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2469\r\n* Revise sections of changelog and add new `internal` category by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2441\r\n* Fix typo in gyroradius formula by @Bzero in https://github.com/PlasmaPy/PlasmaPy/pull/2512\r\n* Update pinned requirements by @plasmapy-requirements-bot in https://github.com/PlasmaPy/PlasmaPy/pull/2513\r\n* Silence some require_quantities warnings in particle_tracker by @pheuer in https://github.com/PlasmaPy/PlasmaPy/pull/2519\r\n* Update pinned requirements by @plasmapy-requirements-bot in https://github.com/PlasmaPy/PlasmaPy/pull/2521\r\n* Add a ""See Also"" to the `formulary.radiation` docs on `astropy.modeling.BlackBody` by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2518\r\n* Update type hints for `int` and `float` arguments by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2520\r\n* Refactor script for checking `CITATION.cff` by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2524\r\n* Remove references to twitter account by @jwreep in https://github.com/PlasmaPy/PlasmaPy/pull/2522\r\n* Update per-file error codes for mypy to ignore by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2527\r\n* Loosen requirements for `pytest` and `sphinx` and drop `pytest` as runtime dependency by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2525\r\n* Drop the `py310-conda` tox environment by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2526\r\n* Minor updates of docstrings by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2528\r\n* Update pinned requirements by @plasmapy-requirements-bot in https://github.com/PlasmaPy/PlasmaPy/pull/2534\r\n* Resolve deprecation warnings in tests by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2533\r\n* Add `nucleus` attribute to `Particle` by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2538\r\n* Fix `kappa_velocity_3D` docstring by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2541\r\n* Refactor gyroradius to reduce cognitive complexity by @Bzero in https://github.com/PlasmaPy/PlasmaPy/pull/2542\r\n* Add test for when `lorentzfactor` and multiple particles are provided to `gyroradius` by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2539\r\n* Bump pypa/gh-action-pypi-publish from 1.8.11 to 1.8.12 by @dependabot in https://github.com/PlasmaPy/PlasmaPy/pull/2545\r\n* Update type hints and docstrings in `plasmapy.formulary` by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2543\r\n* Update release checklist by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2511\r\n* Create universal wheel in tox environments by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2546\r\n* Use `uv` to regenerate pinned requirements to be used in CI for multiple versions of Python by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2523\r\n* Update dependabot settings by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2551\r\n* Speed up tests by caching `.tox` in GitHub Actions and update tox environments by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2552\r\n* Turn off `sphinx-codeautolink` in some docstrings to avoid warnings in repeat builds by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2554\r\n* Fix GitHub Action to regenerate requirements by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2556\r\n* Update pinned requirements by @plasmapy-requirements-bot in https://github.com/PlasmaPy/PlasmaPy/pull/2557\r\n* Remove `setup.py` by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2558\r\n* Parametrize tests of `Debye_length` by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2509\r\n* Decorate potentially flaky tests with `@pytest.mark.flaky` from `pytest-rerunfailures`  by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2483\r\n* Add known limitations to docstring of `@particle_input` by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2516\r\n* Improve type hint annotations to `plasmapy.particles` by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2458\r\n* Fix sentence in gyroradius documentation by @jwreep in https://github.com/PlasmaPy/PlasmaPy/pull/2560\r\n* Make `common_isotopes`, `known_isotopes`, and `stable_isotopes` each return a `ParticleList` by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2559\r\n* Add `sphinx-lint` to pre-commit configuration by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2561\r\n* Bump pypa/gh-action-pypi-publish from 1.8.12 to 1.8.14 in /.github/workflows by @dependabot in https://github.com/PlasmaPy/PlasmaPy/pull/2564\r\n* Update pre-commit with new version of ruff and update version of mypy by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2566\r\n* Fix Codecov Workflow by @JaydenR2305 in https://github.com/PlasmaPy/PlasmaPy/pull/2568\r\n* Update pinned requirements by @plasmapy-requirements-bot in https://github.com/PlasmaPy/PlasmaPy/pull/2569\r\n* Reorganize and improve naming consistency of tests by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2571\r\n* Switch to a GitHub Action for validating CITATION.cff by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2576\r\n* Bump actions/checkout from 3 to 4 in /.github/workflows by @dependabot in https://github.com/PlasmaPy/PlasmaPy/pull/2577\r\n* Explicitly specify UTF-8 for citation generation by @JaydenR2305 in https://github.com/PlasmaPy/PlasmaPy/pull/2578\r\n* Update documentation guide on how to locally build docs by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2565\r\n* Update pinned requirements by @plasmapy-requirements-bot in https://github.com/PlasmaPy/PlasmaPy/pull/2586\r\n* Enable a `tox` extension to use `uv` instead of `pip` for installing all the things by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2584\r\n* Fix typo in docs by @jwreep in https://github.com/PlasmaPy/PlasmaPy/pull/2588\r\n* Add some new ruff rules by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2593\r\n* Update pinned requirements by @plasmapy-requirements-bot in https://github.com/PlasmaPy/PlasmaPy/pull/2597\r\n* Update versions of Python in weekly tests by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2599\r\n* Create a class to manage local and online resource files by @pheuer in https://github.com/PlasmaPy/PlasmaPy/pull/2570\r\n* Update pinned requirements by @plasmapy-requirements-bot in https://github.com/PlasmaPy/PlasmaPy/pull/2602\r\n* [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/PlasmaPy/PlasmaPy/pull/2603\r\n* Moved charge/mass number to Other Parameters by @Spedi in https://github.com/PlasmaPy/PlasmaPy/pull/2604\r\n* Update project settings by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2601\r\n* Loosen voila requirement and update pinned requirements by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2609\r\n* Include coverage in a weekly test by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2612\r\n* Fix coverage setup in weekly tests by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2613\r\n* Update name of Codecov configuration file by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2615\r\n* Include `CODECOV_TOKEN` in the GitHub workflows that it is used in by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2616\r\n* Update Codecov settings and usage by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2617\r\n* Added Tests for Verify correctness in MaxwellianCollisionFrequencies by @daran9 in https://github.com/PlasmaPy/PlasmaPy/pull/2614\r\n* Put token for Codecov back in configuration file by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2624\r\n* Modify the way settings are passed to the _spectral_density_model function  by @pheuer in https://github.com/PlasmaPy/PlasmaPy/pull/2623\r\n* Update pinned requirements by @plasmapy-requirements-bot in https://github.com/PlasmaPy/PlasmaPy/pull/2626\r\n* Add examples to `thermal_bremsstrahlung` docstring by @jwreep in https://github.com/PlasmaPy/PlasmaPy/pull/2618\r\n* Use pinned requirements for Python 3.12 in Read the Docs build 📍 by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2622\r\n* Fix bug when giving `thermal_bremsstrahlung multiple` density values by @jwreep in https://github.com/PlasmaPy/PlasmaPy/pull/2627\r\n* Switch to `src` layout with separate `tests` directory by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2598\r\n* Discuss consequences of switch to `src` layout in `README.md` by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2633\r\n* Update pinned requirements by @plasmapy-requirements-bot in https://github.com/PlasmaPy/PlasmaPy/pull/2635\r\n* Several improvements to OTS fitting by @pheuer in https://github.com/PlasmaPy/PlasmaPy/pull/2636\r\n* Update pinned requirements by @plasmapy-requirements-bot in https://github.com/PlasmaPy/PlasmaPy/pull/2645\r\n* Remove upper limit on Sphinx by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2646\r\n* Update pinned requirements by @plasmapy-requirements-bot in https://github.com/PlasmaPy/PlasmaPy/pull/2649\r\n* Fix workflow to comment on pull requests by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2652\r\n* Update Codecov settings by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2653\r\n* Update `.git-blame-ignore-revs` to ignore recent repository-wide changes by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2651\r\n* Add nox configuration file with environments for building documentation and running mypy by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2654\r\n* Use nox for some testing environments in GitHub workflows by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2656\r\n* Update requirements setup by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2650\r\n* Add `particlewise` keyword to `particle_collections.is_category` by @jwreep in https://github.com/PlasmaPy/PlasmaPy/pull/2648\r\n* Update pinned requirements by @plasmapy-requirements-bot in https://github.com/PlasmaPy/PlasmaPy/pull/2665\r\n* [pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in https://github.com/PlasmaPy/PlasmaPy/pull/2669\r\n* Update the dependency version support policy in the coding guide by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2670\r\n* Update contributor guide discussions about tox and nox by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2590\r\n* Update changelog entries prior to v2024.5.0 release by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2671\r\n* Update requirements for binder by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2672\r\n* Make `@particle_input` apply categorization criteria when creating a `ParticleList` by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2594\r\n* Update pinned requirements by @plasmapy-requirements-bot in https://github.com/PlasmaPy/PlasmaPy/pull/2674\r\n* Alphabetize author list in `CITATION.cff` by @namurphy in https://github.com/PlasmaPy/PlasmaPy/pull/2676\r\n\r\n## New Contributors\r\n* @Bzero made their first contribution in https://github.com/PlasmaPy/PlasmaPy/pull/2512\r\n* @jwreep made their first contribution in https://github.com/PlasmaPy/PlasmaPy/pull/2522\r\n* @Spedi made their first contribution in https://github.com/PlasmaPy/PlasmaPy/pull/2604\r\n* @daran9 made their first contribution in https://github.com/PlasmaPy/PlasmaPy/pull/2614\r\n\r\n**Full Changelog**: https://github.com/PlasmaPy/PlasmaPy/compare/v2024.2.0...v2024.5.0",v2024.5.0,Nick Murphy,,namurphy,"BSD 3-Clause ""New"" or ""Revised"" License",PlasmaPy,PlasmaPy,12,python,science,heliophysics,plasma,solar,space-physics,hedp,high-energy-density-physics,astronomy,astrophysics,atomic-physics,fusion,particles,physics,,,,,,,/PlasmaPy/PlasmaPy,27,34,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/PixiEditor/PixiEditor,https://github.com/PixiEditor/PixiEditor,0,,,0,0,0,0,0,0,1,1,0,0,0,PixiEditor is a lightweight pixel art editor made with .NET 7,"<img src=""https://user-images.githubusercontent.com/25402427/102633463-c1d3bb80-4150-11eb-8262-535e568fa781.png"" width=""700"">\n\n---\n\n**PixiEditor** is a Pixel art editing software. Create beautiful sprites for your games, animations (coming soon!), and edit images. All packed in eye-friendly dark theme.\n\n[![Build Status](https://img.shields.io/azure-devops/build/flabbet/PixiEditor/6/master)](https://dev.azure.com/flabbet/PixiEditor/_build?definitionId=6) \n[![CodeFactor](https://www.codefactor.io/repository/github/pixieditor/pixieditor/badge)](https://www.codefactor.io/repository/github/pixieditor/pixieditor)\n[![Release](https://img.shields.io/github/v/release/flabbet/PixiEditor)](https://github.com/flabbet/PixiEditor/releases) \n[![Downloads](https://img.shields.io/github/downloads/PixiEditor/PixiEditor/total)](https://github.com/flabbet/PixiEditor/releases)\n[![Discord Server](https://badgen.net/badge/discord/join%20chat/7289DA?icon=discord)](https://discord.gg/qSRMYmq)\n[![Subreddit subscribers](https://img.shields.io/reddit/subreddit-subscribers/PixiEditor?label=%20r%2FPixiEditor&logoColor=%23e3002d)](https://reddit.com/r/PixiEditor)\n[![contributions](https://img.shields.io/badge/contributions-open-brightgreen)](https://github.com/flabbet/PixiEditor/pulls)\n\n### Check out our website [pixieditor.net](https://pixieditor.net)\n\n## About PixiEditor\n\nWant to create beautiful pixel art for your games? PixiEditor can help you! Our goal is to create a fully open-source, fast, and feature-rich pixel art creator. \n\n### Familiar interface\n\nHave you ever used Photoshop or Gimp? Reinventing the wheel is unnecessary, we wanted users to get familiar with the tool quickly and with ease. \n\n![](https://user-images.githubusercontent.com/45312141/235351211-e00bcaea-9c63-4ecd-a2ee-e4fb2b2c9651.png)\n\n### Fast\n\nPixiEditor is fast, drawing feels smooth on any canvas size, we've developed original chunk-based system and adaptive rendering to minimize pixel processing time.\n\n### Active development\n\nPixiEditor started in 2018 and it's been actively developed since. We continuously improve code quality to ensure the best experience and performance.\n\n\n## Installation\n\n<a href='//www.microsoft.com/store/apps/9NDDRHS8PBRN?cid=storebadge&ocid=badge'><img src='https://developer.microsoft.com/store/badges/images/English_get-it-from-MS.png' alt='Microsoft Store badge' width=""184""/></a>\n\nGet it on Steam now!\n\n[![Get PixiEditor on Steam](https://user-images.githubusercontent.com/121322/228988640-32fe5bd3-9dd0-4f3b-a8f2-f744bd9b50b5.png)](https://store.steampowered.com/app/2218560/PixiEditor__Pixel_Art_Editor?utm_source=GitHub)\n\n**Or**\n\nFollow these instructions to get PixiEditor working on your machine.\n\n1. Download the zipped installer from our [official website](https://pixieditor.net/download)\n2. Extract the installer from the archive\n3. Launch it\n4. Follow the steps in the installer to finish the installation\n\n\n## Featured content\n\n### PixiEditor 1.0 Trailer\n\n[![Trailer](https://img.youtube.com/vi/UK8HnrAQhCo/0.jpg)](https://www.youtube.com/watch?v=UK8HnrAQhCo)\n\n### Pixel Art Timelapse - ""Bog Landscape"" | PixiEditor\n\n[![Landscape timelapse](https://img.youtube.com/vi/bzC-wy6HCB8/0.jpg)](https://www.youtube.com/watch?v=bzC-wy6HCB8)\n\n### Gallery\n\nCheck out some pixel arts made with PixiEditor [here](https://github.com/PixiEditor/PixiEditor/wiki/Gallery).\n\n\n## Support\n\nStruggling with something? You can find support in a few places:\n\n* Check out [documentation](https://github.com/flabbet/PixiEditor/wiki)\n\n* Ask on [Discord](https://discord.gg/qSRMYmq)\n* Open new [Issue](https://github.com/flabbet/PixiEditor/issues)\n* Check out the [FAQ](https://github.com/PixiEditor/PixiEditor/wiki/FAQ). \n\n\n\n## Building from source\n\n### Software Requirements\n\n* .NET 7\n\n### Instructions\n\n1. Clone Repository\n\n2. Open PixiEditor/src/PixiEditor/PixiEditor.sln in Visual Studio\n\n3. Build solution\n\n## Contributing \n\nPlease read [CONTRIBUTING.md](https://github.com/flabbet/PixiEditor/blob/master/CONTRIBUTING.md) for details on our code of conduct, and the process for submitting pull requests to us.\n\n## License\n\nThis project is licensed under the LGPLv3 License - see the [LICENSE.md](https://github.com/flabbet/PixiEditor/blob/master/LICENSE) - file for details\n",1328,graphics,C#,5,C#,Inno Setup,PowerShell,Smalltalk,Shell,,,,,,,,,,,,,,,,,,,,,,,,278,23,247,8,42,21,0,16317,101,342,262,80,36608d8fb3b0718238967b29e9bbceb271fce19b,release things,2024-05-22T11:14:49Z,flabbet,flubbet@gmail.com,flabbet,,\n\n## Changes:\n\n* 440cffb4a0db81fea9be4603d0c36fc2b239bd28 Merge branch 'master' into release\n* e2e670052ff5b93e22e561c5666f381302fddea7 1.2.5.0\n* 932020ae0da37025da2dd12b8cd9071ccb45143a Merge pull request #616 from PixiEditor/palette-fix\n* 56a622f68b99b072f768a03525d83dbe2f64b0b5 Added a appropriate comment\n* 4fb9f592873caed50c49a199b4c6ecd51cbb9939 Added a webhook handler if updated is null and refresh entire palette list\n* 3791380e0f5e5c83a4a77cf4720874e9c1bac3ef Fixed non-distinct palettes in browser\n* 7a10c4e5b4099ad2db383f42c4950b7a82d38db9 Merge pull request #607 from warrengalyen/fix-jasc-palette-parser\n* 29e1ed68e70031fb011426f660b71be98aaeb719 Add PspPalette extension\n\nThis list of changes was [auto generated](https://dev.azure.com/flabbet/9b8d67ab-e542-46a5-9415-f5f980ffbdbe/_release?releaseId=105&_a=release-summary).,1.2.5.0,Krzysztof Krysiński,,flabbet,GNU Lesser General Public License v3.0,PixiEditor,PixiEditor,31,csharp,wpf,raster-graphics,editor,dotnet-core,dotnetcore,graphics-editor,pixel-art,graphics,pixi,draw,sprites,2d,games,tabs,pixel-arts,discord,game-development,dotnet7,,/PixiEditor/PixiEditor,31,23,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/pioneerspacesim/pioneer,https://github.com/pioneerspacesim/pioneer,0,,,0,0,0,0,0,0,1,0,0,0,0,A game of lonely space adventure,"[![Build](https://github.com/pioneerspacesim/pioneer/workflows/Build%20Pioneer/badge.svg)](https://github.com/pioneerspacesim/pioneer/actions)\n[![License GPLv3](https://img.shields.io/badge/license-GPL_v3-green.svg)](http://www.gnu.org/licenses/gpl-3.0.html)\n[![#pioneer on Libera.Chat](https://img.shields.io/badge/LiberaChat-%23pioneer-brightgreen.svg)](https://kiwiirc.com/client/irc.libera.chat/pioneer)\n[![Github All Releases](https://img.shields.io/github/downloads/pioneerspacesim/pioneer/latest/total)]()\n\n\n# Pioneer Space Simulator\n\n![](https://github.com/pioneerspacesim/pioneer/blob/master/data/icons/badge.png)\n\nPioneer is a space adventure game set in the Milky Way galaxy at the turn of\nthe 31st century.\n\nThe game is open-ended, and you are free to explore the millions of star\nsystems in the game. You can land on planets, slingshot past gas giants, and\nburn yourself to a crisp flying between binary star systems. You can try your\nhand at piracy, make your fortune trading between systems, or do missions for\nthe various factions fighting for power, freedom or self-determination.\n\nFor more information, see:\n  http://pioneerspacesim.net/\n\n\n## Community\n\nCome by #pioneer at irc.libera.chat and say hi to the team:\n  https://kiwiirc.com/client/irc.libera.chat/pioneer\n\nBugs? Please log an issue:\n  https://github.com/pioneerspacesim/pioneer/issues\n\nFollow Pioneer on Twitter:\n  https://twitter.com/pioneerspacesim/\n\nPioneer wiki\n  https://wiki.pioneerspacesim.net\n\nJoin the player's forum:\n  http://spacesimcentral.com/community/pioneer/\n\nJoin the development forum:\n  http://forum.pioneerspacesim.net\n\n\n## Manual\n\nManual can be found at:\n  https://wiki.pioneerspacesim.net/wiki/Manual\n\nBasic flight:\n  https://wiki.pioneerspacesim.net/wiki/Basic_flight\n\nKeyboard and mouse control is found at:\n  https://wiki.pioneerspacesim.net/wiki/Keyboard_and_mouse_controls\n\n\n## FAQ\n\nFor frequently asked questions, please see\n  https://wiki.pioneerspacesim.net/wiki/FAQ\n\n\n## BUG Reporting\n\nPlease see the section of the FAQ pertaining to bugs, crashes and reporting other issues: [Bug Reporting FAQs](https://wiki.pioneerspacesim.net/wiki/FAQ#How.2Fwhere_do_I_report_my_bug.2Fcrash).\n\nPlease do your best to fill out the issue template as completely as possible, especially when you're reporting a crash bug or a graphical issue. Having system information including graphics drivers and the method you used to install Pioneer helps immensely to diagnose and fix these kinds of issues.\n\n## Contributing\n\nIf you are hungry to contribute, more information can be found here:\n  https://wiki.pioneerspacesim.net/wiki/How_you_can_contribute\n\nIf you have a contribution you want to share, and want to learn how to make a\npull request, see:\n  https://dev.pioneerspacesim.net/contribute/git-and-github\n\nPioneer development documentation\n  https://dev.pioneerspacesim.net/\n\n## Localization\n\nLocalization for Pioneer is handled trough Transifex, and pulled to the source from there automatically. Because of this please don't make pull requests for translations. [You can find the localization project here.](https://www.transifex.com/pioneer/pioneer/dashboard/)\nYou need to register at transifex to be able to access the translations.\n\nIf you want a new language introduced, [please request it on the Libera IRC channel of Pioneer](https://web.libera.chat/#pioneer), or here by making an issue for it.\n\n## Getting Pioneer\n\nLatest build is available at\n  https://pioneerspacesim.net/page/download/\n\nFor compiling from source, please see [COMPILING.txt](https://github.com/pioneerspacesim/pioneer/blob/master/COMPILING.txt)\n\n\n## Changelog\n\nPlease see [Changelog.txt](https://github.com/pioneerspacesim/pioneer/blob/master/Changelog.txt)\n",1600,physics,C++,12,Shell,C++,C,Lua,GLSL,CSS,HTML,Nix,Python,Perl,CMake,MoonScript,,,,,,,,,,,,,,,,,3365,445,2887,33,1,132,263,1229004,362,2501,2230,271,8a84b93e361d462baf6e0fc688679c0bc1ba7078,Merge pull request #5861 from impaktor/fix-debug-commodities-tab,2024-07-18T18:15:27Z,Webster Sheets,Web-eWorks@users.noreply.github.com,Web-eWorks,Pioneer 2024-07-10,"This is the second bug-fix release since the February release. The most notable bug that made it into previous bug fix release was a save compatibility issue where the game did not allow the player to load saves from the previous version (the player could circumvent the bug by force loading the save, by holding down Ctrl-key when clicking Load).\r\n\r\nAnother notable fix is the loud white noise-bug, that some have encountered.\r\n\r\nFor those playing a localized non-English version, we have identified and fixed hundreds of broken strings in the following languages: Brazilian Portuguese (pt_BR), Chinese (zh), Czech (cs), Danish (da), Dutch (nl) French (fr), German (de), Hungarian (hu), Italian (it), Polish (pl), Portuguese (pt), Russian (ru), Spanish (es), and Swedish (sv). Unfortunately, the fix also accidentally marked all languages as translated, over on the transifex project page. This will be addressed shortly.\r\n\r\nAfter this release, we will return our focus on merging new features. Brave pilots daring bugs while enjoying latest new features are encouraged to keep an eye on master branch, and report any encountered bugs.\r\n\r\n## Bugfixes / Tweaks\r\n* Fix backwards save compatibility with v90 saves (#5798)\r\n* Fix loud white noise after mouse steering (#5812)\r\n* Fix bad placeholders in translated strings (#5852)\r\n* Improve surface scan missions on high-pressure worlds (#5841)\r\n* Fix mission list error when opened during hyperspace (#5843)\r\n* Fix location recovery in save files (#5801)\r\n* Use variable ""dist"" (SAR mission) for the distance in the BB if specified (#5818)\r\n* Fix speed limiter button positions overlapping (#5802)\r\n* Fix News Events not modifying commodity demand (#5859)\r\n* Sort commodity names by translated string (#5846)\r\n* Fix custom systems with zero stars (#5858)\r\n* Fix Military Drive 2 not for sale & tweaks (#5856)\r\n* Fix lots of missing ship labels (#5842)\r\n* Fixing misaligned little parts of the Xylophis cockpit (#5834)\r\n* Removing stray triangles from Bluenose (#5835)\r\n* Fixing flipping landing gear animation of Bowfin (#5814)\r\n* Police Coronatrix texture fix (#5816)\r\n* Fixing Coronatrix label naming (#5855)\r\n* Fix so .desktop file points to correct installed path (#5847)\r\n* Fix DLLs not being installed on Windows builds (#5850)\r\n\r\n## Internal Changes\r\n* Add missing algorithm include / copy_new complaint (#5833)\r\n* Documented minimal compiler version needed updating (#5810)\r\n* Include stdlib.h instead of alloca.h for alloca(3) on FreeBSD (#5827)\r\n* Include <algorithm> for std::max (#5828)\r\n* Do not install libfmt by default (#5808)\r\n* Renderer: Support running under Wayland (#5817)\r\n",20240710,Karl F,,impaktor,,pioneer,pioneerspacesim,11,game,space,simulation,simulator,exploration,elite,frontier,3d,newtonian,physics,procedural-generation,,,,,,,,,,/pioneerspacesim/pioneer,342,109,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/pha4ge/hAMRonization,https://github.com/pha4ge/hAMRonization,0.5,,1,1,1,1,1,0,0,0,0,0,0,1,Parse multiple Antimicrobial Resistance Analysis Reports into a common data structure,"![Python package](https://github.com/pha4ge/hAMRonization/workflows/test_package/badge.svg)\n[![DOI](https://zenodo.org/badge/248040662.svg)](https://zenodo.org/badge/latestdoi/248040662)\n[![Docs English](https://img.shields.io/badge/Documentation-English-blue)](https://github.com/pha4ge/hAMRonization/blob/master/docs/subgrant/PHA4GE_AMR_SubGrant_Documentation.pdf)\n[![Docs English](https://img.shields.io/badge/Documentation-Español-blue)](https://github.com/pha4ge/hAMRonization/blob/master/docs/subgrant/PHA4GE_hAMRonization_espan%CC%83ol.pdf)\n\n\n# hAMRonization \n\nThis repo contains the hAMRonization module and CLI parser tools combine the outputs of \n18 (as of 2022-09-25) disparate antimicrobial resistance gene detection tools into a single unified format.\n\nThis is an implementation of the [hAMRonization AMR detection specification scheme](docs/hAMRonization_specification_details.csv) which supports gene presence/absence resistance and mutational resistance (if supported by the underlying tool).\n\nThis supports a variety of summary options including an [interactive summary](https://finlaymagui.re/assets/interactive_report_demo.html).\n\n![hAMRonization overview](https://github.com/pha4ge/hAMRonization/blob/master/docs/overview_figure.png?raw=true)\n\n\n## Installation\n\nThis tool requires python>=3.7 and [pandas](https://pandas.pydata.org/)\nand the latest release can be installed directly from pip, conda, docker, this repository, or from the galaxy toolshed:\n```\npip install hAMRonization\n```\n[![PyPI version](https://badge.fury.io/py/hamronization.svg)](https://badge.fury.io/py/hamronization)\n[![PyPI downloads](https://img.shields.io/pypi/dm/hAMRonization.svg)](https://img.shields.io/pypi/dm/hAMRonization)\n\nOr\n\n```\nconda create --name hamronization --channel conda-forge --channel bioconda --channel defaults hamronization\n```\n![version-on-conda](https://anaconda.org/bioconda/hamronization/badges/version.svg)\n![conda-download](https://anaconda.org/bioconda/hamronization/badges/downloads.svg)\n![last-update-on-conda](https://anaconda.org/bioconda/hamronization/badges/latest_release_date.svg)\n\n\nOr to install using docker:\n```\ndocker pull finlaymaguire/hamronization:latest\n```\n\nOr to install the latest development version:\n```\ngit clone https://github.com/pha4ge/hAMRonization\npip install hAMRonization\n```\n\nAlternatively, hAMRonization can also be installed and used in [galaxy](https://galaxyproject.org/) via the [galaxy toolshed](https://toolshed.g2.bx.psu.edu/view/iuc/suite_hamronization/904ab154f8f4).\n\n## Usage\n\n**NOTE**: Only the output format used in the ""last updated"" version of the AMR prediction tool has been tested for accuracy. Older tool versions or updates which lead to a change in output format may not work. \nIn theory, this should only be a problem with major version changes but not all tools follow semantic versioning.\nIf you encounter any issues with newer tool versions then please create an issue in this repository.\n\n```\nusage: hamronize <tool> <options>\n\nConvert AMR gene detection tool output(s) to hAMRonization specification format\n\noptions:\n  -h, --help            show this help message and exit\n  -v, --version         show program's version number and exit\n\nTools with hAMRonizable reports:\n  {abricate,amrfinderplus,amrplusplus,ariba,csstar,deeparg,fargene,groot,kmerresistance,resfams,resfinder,mykrobe,pointfinder,rgi,srax,srst2,staramr,tbprofiler,summarize}\n    abricate            hAMRonize abricate's output report i.e., OUTPUT.tsv\n    amrfinderplus       hAMRonize amrfinderplus's output report i.e., OUTPUT.tsv\n    amrplusplus         hAMRonize amrplusplus's output report i.e., gene.tsv\n    ariba               hAMRonize ariba's output report i.e., OUTDIR/OUTPUT.tsv\n    csstar              hAMRonize csstar's output report i.e., OUTPUT.tsv\n    deeparg             hAMRonize deeparg's output report i.e.,\n                        OUTDIR/OUTPUT.mapping.ARG\n    fargene             hAMRonize fargene's output report i.e., retrieved-\n                        genes-*-hmmsearched.out\n    groot               hAMRonize groot's output report i.e., OUTPUT.tsv (from `groot\n                        report`)\n    kmerresistance      hAMRonize kmerresistance's output report i.e., OUTPUT.res\n    resfams             hAMRonize resfams's output report i.e., resfams.tblout\n    resfinder           hAMRonize resfinder's output report i.e.,\n                        ResFinder_results_tab.txt\n    mykrobe             hAMRonize mykrobe's output report i.e., OUTPUT.json\n    pointfinder         hAMRonize pointfinder's output report i.e.,\n                        PointFinder_results.txt\n    rgi                 hAMRonize rgi's output report i.e., OUTPUT.txt or\n                        OUTPUT_bwtoutput.gene_mapping_data.txt\n    srax                hAMRonize srax's output report i.e., sraX_detected_ARGs.tsv\n    srst2               hAMRonize srst2's output report i.e., OUTPUT_srst2_report.tsv\n    staramr             hAMRonize staramr's output report i.e., resfinder.tsv\n    tbprofiler          hAMRonize tbprofiler's output report i.e., OUTPUT.results.json\n    summarize           Provide a list of paths to the reports you wish to summarize\n```\n\nTo look at a specific tool e.g. `abricate`:\n```\n>hamronize abricate -h \nusage: hamronize abricate <options>\n\nApplies hAMRonization specification to output from abricate (OUTPUT.tsv)\n\npositional arguments:\n  report                Path to tool report\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --format FORMAT       Output format (tsv or json)\n  --output OUTPUT       Output location\n  --analysis_software_version ANALYSIS_SOFTWARE_VERSION\n                        Input string containing the analysis_software_version for abricate\n  --reference_database_version REFERENCE_DATABASE_VERSION\n                        Input string containing the reference_database_version for abricate\n\n```\n\nTherefore, hAMRonizing abricates output:\n```\nhamronize abricate ../test/data/raw_outputs/abricate/report.tsv --reference_database_version 3.2.5 --analysis_software_version 1.0.0 --format json\n```\n\nTo parse multiple reports from the same tool at once just give a list of reports as the argument,\nand they will be concatenated appropriately (i.e. only one header for tsv)\n\n```\nhamronize rgi --input_file_name rgi_report --analysis_software_version 6.0.0 --reference_database_version 3.2.5 test/data/raw_outputs/rgi/rgi.txt test/data/raw_outputs/rgibwt/Kp11_bwtoutput.gene_mapping_data.txt\n```\n\nYou can summarize hAMRonized reports regardless of format using the 'summarize'\nfunction:\n\n```\n> hamronize summarize -h\nusage: hamronize summarize <options> <list of reports>\n\nConcatenate and summarize AMR detection reports\n\npositional arguments:\n  hamronized_reports    list of hAMRonized reports\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -t {tsv,json,interactive}, --summary_type {tsv,json,interactive}\n                        Which summary report format to generate\n  -o OUTPUT, --output OUTPUT\n                        Output file path for summary\n```\n\nThis will take a list of report and create single sorted report in the \nspecified format just containing the unique entries across input reports.\nThis can handle mixed json and tsv hamronized report formats.\n\n```\nhamronize summarize -o combined_report.tsv -t tsv abricate.json ariba.tsv\n```\n\nThe [interactive summary](https://finlaymagui.re/assets/interactive_report_demo.html) option will produce an html file that can be opened within the browser for navigable data exploration (feature developed\nwith @alexmanuele).\n\n### Using within scripts\n\nAlternatively, hAMRonization can be used within scripts (the metadata must contain the mandatory metadata that is not included in that tool's output, this can be checked by looking at the CLI flags in `hamronize <tool> --help`):\n\n```\nimport hAMRonization\nmetadata = {""analysis_software_version"": ""1.0.1"", ""reference_database_version"": ""2019-Jul-28""}\nparsed_report = hAMRonization.parse(""abricate_report.tsv"", metadata, ""abricate"")\n```\n\nThe `parsed_report` is then a generator that yields hAMRonized result objects from the parsed report:\n\n```\nfor result in parsed_report:\n      print(result)\n```\n\nAlternatively, you can use the `.write` attribute to export all results left in the generator to a file (if a filepath isn't provided, this will write to stdout).\n\n```parsed_report.write('hAMRonized_abricate_report.tsv')```\n\nYou can also output a `json` formatted hAMRonized report:\n\n`parsed_report.write('all_hAMRonized_abricate_report.json', output_format='json')`\n\nIf you want to write multiple reports to one file, this `.write` method can accept `append_mode=True` to append rather than overwrite the output file and not include the header (in tsv format).\n\n`parsed_report.write('all_hAMRonized_abricate_report.tsv', append_mode=True)`\n\n\n### Implemented Parsers\n\nCurrently implemented parsers and the last tool version for which they have been validated:\n\n1. [abricate](hAMRonization/AbricateIO.py): last updated for v1.0.0\n2. [amrfinderplus](hAMRonization/AmrFinderPlusIO.py): last updated for v3.10.40\n3. [amrplusplus](hAMRonization/AmrPlusPlusIO.py): last updated for c6b097a\n4. [ariba](hAMRonization/AribaIO.py): last updated for v2.14.6\n5. [csstar](hAMRonization/CSStarIO.py): last updated for v2.1.0\n6. [deeparg](hAMRonization/DeepArgIO.py): last updated for v1.0.2\n7. [fargene](hAMRonization/FARGeneIO.py): last updated for v0.1\n8. [groot](hAMRonization/GrootIO.py): last updated for v1.1.2\n9. [kmerresistance](hAMRonization/KmerResistanceIO.py): late updated for v2.2.0\n10. [mykrobe](test/data/raw_outputs/mykrobe/report.json): last updated for v0.8.1\n11. [pointfinder](hAMRonization/PointFinderIO.py): last updated for v4.1.0\n12. [resfams](hAMRonization/ResFamsIO.py): last updated for hmmer v3.3.2\n13. [resfinder](hAMRonization/ResFinderIO.py): last updated for v4.1.0\n14. [rgi](hAMRonization/RgiIO.py) (includes RGI-BWT) last updated for v5.2.0\n15. [srax](hAMRonization/SraxIO.py): last updated for v1.5\n16. [srst2](hAMRonization/Srst2IO.py): last updated for v0.2.0\n17. [staramr](hAMRonization/StarAmrIO.py): last updated for v0.8.0\n18. [tbprofilder](test/data/raw_outputs/tbprofiler/tbprofiler.json): last updated for v3.0.8\n\n## Implementation Details\n\n### hAMRonizedResult Data Structure\n\nThe hAMRonization specification is implemented in the [hAMRonizedResult dataclass](https://github.com/pha4ge/harmonized-amr-parsers/blob/master/hAMRonization/hAMRonization/hAMRonizedResult.py#L6).\n\nThis is a simple datastructure that uses positional and key-word args to distinguish mandatory from optional hAMRonization fields. \nIt also uses type-hinting to validate the supplied values are of the correct type\n\n\nEach parser follows a similar strategy, using a common interface.\nThis has been designed to match the `biopython` `SeqIO` `parse` function \n\n    >>> import hAMRonization\n    >>> filename = ""abricate_report.tsv""\n    >>> metadata = {""analysis_software_version"": ""1.0.1"", ""reference_database_version"": ""2019-Jul-28""}\n    >>> for result in hAMRonization.parse(filename, metadata, ""abricate""):\n    ...    print(result)\n\nWhere the final argument to the `hAMRonization.parse` command is whichever tool is being parsed.\n\n### hAMRonizedResultIterator\n\nAn abstract iterator is then implemented to ingest a given AMR tool's report\n(via the appropriate subclassed implementation), hAMRonize results i.e. translate the \noriginal inputs to the fields in the hAMRonization specification, and yield a stream of \nhAMRonizedResult dataclasses.\n\nThis iterator also implements a write function to enable outputting the contents \nto a output stream or filehandle in either tsv or json format.\n\n### Tool-specific Iterators\n\nEach tool has a specific subclass of this abstract hAMRonizedResultIterator e.g. `AbricateIO.AbricateIterator`.\n\nThese include an attribute containing the mapping of the tools original output report fields to the hAMRonized specification fields (`self.field_mapping`), as well as handling specifying any additional required metadata.\n\nThe `parse` method of these subclasses then implements the tool-specific parsing logic required.\nThis is typically a simple `csv.DictReader` but can be more complex such as the json parsing of `resfinder` output, \nor the modification of output fields required to better fit some tools into the hAMRonization specification.\n\n\n## Contributing \n\nWe welcome contributions for users in any form (from github issues flagging problems/requests) to pull requests of bug fixes or adding new parsers.\n\n## Setting up a Development Environment\n\nFirst fork this repository and set up a development environment (replacing `YOURUSERNAME` with your github username:\n\n```\ngit clone https://github.com/YOURUSERNAME/hAMRonization\nconda create -n hAMRonization \nconda activate hAMRonization\ncd hAMRonization\npip install pytest flake8\npip install -e .\n\n```\n## Testing and Linting\n\nOn every commit github actions automatically runs tests and linting to check\nthe code. \nYou can manually run these in your development environment as well.\n\nTo run a full set of integration tests:\n\n    pushd test\n    bash run_integration_test.sh\n    popd\n\nTo run unit tests that verify parsing validity for each tool\nas well as generation of valid summaries you can use pytest:\n\n    pip install pytest\n    pushd test\n    pytest\n    popd\n\nFinally to run linting and check whether your code matches the project\ncode style:\n\n    pushd hAMRonization\n    flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n    flake8 . --count --exit-zero --max-complexity=20 --max-line-length=127 --statistics\n    popd\n\n## Adding a new parser\n\nIf you wish to add a parser for a new tool here are the main steps required:\n\n1. Add an entry into `_RequiredToolMetadata` and `_FormatToIterator` in `hAMRonziation/__init__.py` which points to the appropriate `ToolNameIO.py` containing the tool's Iterator subclass\n\n2. In `ToolNameIO.py` add a `required_metadata` list containing any mandatory fields not implemented by the tool\n\n3. Then add a class `ToolNameIterator(hAMRonizedResultIterator)` and implement the `__init__` methods with the approriate mapping (`self.field_mapping`), and metadata (`self.metadata`).\n\n4. To this class, add a `parse` method which reads an opened file stream into a dictionary per line/result (matching the keys of `self.field_mapping`) and yields the output of `self.hAMRonize` being applied to that dictionary.\n\n5. To add a CLI parser for the tool, create a python file in the `parsers` directory:\n\n    ```\n    from hAMRonization import Interfaces\n    if __name__ == '__main__': \n        Interfaces.cli_parser('toolname')\n    ```\n\nAlternatively, the `hAMRonized_parser.py` can be used as a common script interface to all implemented parsers. \n\n6. Finally, following the template in `test/test_parsing_validity.py`, please generate a unit test that ensures the parser is working as you intend it to!\n\nIf you have any questions about any of this or need any help, please file an issue.\n\n## FAQ\n\n* What's the difference between an Antimicrobial Resistance 'Result' and 'Report'?\n  * For the purposes of this project, a 'Report' is an output file (or collection of files) from an AMR analysis tool.\n    A 'Result' is a single entry in a report. For example, a single line in an abricate report file is a single Antimicrobial\n    Resistance 'Result'.\n    \n### Known Issues\n\nHere are some known issues that we would welcome input on trying to solve!\n\n#### Limitations of specification\n\n- mandatory fields: `gene_symbol` and `gene_name` are confusing and not usually both present (only consistently used in AFP). Means tools either need 1:2 mapping i.e. single output field maps to both `gene_symbol` and `gene_name` OR have fragile text splitting of single field that won't be robust to databases changes.  Current solution is 1:2 mapping e.g. staramr\n\n- inconsistent nomenclature of terms being used in specification fields: target, query, subject, reference. Need to stick to one name for sequence with which the database is being searched, and one the hit that results from that search.\n\n- `sequence_identity`: is sequence type specific %id amino acids != %id nucleotide but does this matter?\n\n- `coverage_depth` seems to include both tool fields that are average depth of read and just plain overall read-count, \n\n- `contig_id` isn't general enough when some tools this ID naturally corresponds to a `read_name` (deepARG), individual ORF (resfams), or protein sequence (AFP with protein input): *change to `query_id_name` or similar?*\n\n",132,bioinformatics,Python,3,Python,Shell,Dockerfile,,,,,,,,,,,,,,,,,,,,,,,,,,33,5,28,0,3,14,2,5665,25,53,46,7,481e0b969d29c3a8648f3885fccadf9e6f1685f6,Correct version numbering,2024-05-21T21:11:33Z,Finlay Maguire,finlaymaguire@gmail.com,fmaguire,v1.1.7,Correct release and version numbering to match one another in repositories.,v1.1.7,Finlay Maguire,,fmaguire,GNU Lesser General Public License v3.0,hAMRonization,pha4ge,13,bioinformatics,antimicrobial-resistance,parsers,data-harmonization,,,,,,,,,,,,,,,,,/pha4ge/hAMRonization,13,18,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/peridyno/peridyno,https://github.com/peridyno/peridyno,0,,,0,0,1,1,0,0,0,0,0,0,0,An AI-targeted physical simulation platform.,"\n\n\n\n![](screenshots/Logo.png)\n\n\n\n# Overview\n\nPeriDyno is a CUDA-based, highly parallal physics engine targeted at providing real-time simulation of physical environments for intelligent agents. \n\n# Installation\n\n\n## Platforms\n\n**Windows 10/11**: fully tested\n\n**Linux**: should work as well, yet not guranteed.\n\n### Prerequisites:\n\nIDE:\n\n- Visual studio 2019+\n\nCUDA:\n\n- Latest tests were done based on CUDA Toolkit 12.2, should be compatible will other old versions.\n\nGraphics:\n\n- glad: https://github.com/Dav1dde/glad.git\n- glfw: https://github.com/glfw/glfw\n- imgui: https://github.com/ocornut/imgui\n\nOptional:\n\n- Qt(5.13+): https://download.qt.io/\n- Wt(4.10.2+): https://www.webtoolkit.eu/wt/\n- VTK: https://github.com/Kitware/VTK\n- Alembic: https://github.com/alembic/alembic\n- Imath: https://github.com/AcademySoftwareFoundation/Imath\n\n### Installation:\n\nAside from those optional, other libraries are integrated inside the project to simplify the installation. Use the following git command to download the project as well as other dependences.\n\n```\ngit clone --recursive https://github.com/peridyno/peridyno.git\n```\n\n### Build the project:\n\nCheck whether CMake has been installed on your system, if not, visit https://cmake.org/download/ to download the lastest version. \n\n**Preferred**: Run cmake-gui.exe, set the top two entries with the source code and binary directories.  **Configure** the libararies you want to build, then click the **Generate** button to build the project. \n\nA more convient way to build the project with a default setting is as follows\n\n```\ncd peridyo/build \ncmake ..\n```\n\n# Applications\n\nWith a scene moded by PeriDyno, it can either be run as a GFLW application, Qt application or even a web application,  you don't need to change any code when switching between those applications.\n\n- GLFW application\n\n[<img src=""screenshots/glfwapp.png"" style=""zoom:80%;"" />](https://github.com/peridyno/peridyno/tree/master/examples/Cuda/SemiAnalytical/Semi_Barricade)\n\n- Qt application\n\n[<img src=""screenshots/qtapp.png"" style=""zoom:80%;"" />](https://github.com/peridyno/peridyno/assets/66506655/466ba7ee-851b-489c-aa7a-4493b3552476.mp4)\n\n- Web application\n\n[<img src=""screenshots/wtapp.png"" style=""zoom:80%;"" />](https://github.com/peridyno/peridyno/tree/master/examples/Cuda/WtGUI/Wt_Barricade)\n\n\n\n# Other resources\n\n- Documentation: www.peridyno.com\n- API: https://peridyno.com/doxygen/html/index.html\n- Courses: https://www.bilibili.com/video/BV15M4y1U76M/\n\n# License\n\nPeridyno's default license is the Apache 2.0 (See [LICENSE](https://github.com/peridyno/peridyno/blob/master/LICENSE)). \n\nExternal libraries are distributed under their own terms.\n",220,fluid-dynamics,C++,6,CMake,Python,C++,C,Cuda,GLSL,,,,,,,,,,,,,,,,,,,,,,,52,10,41,1,9,20,0,638363,42,14,11,3,0b9ab5b04c01cfa3da30979220afb9dfe31cf34f,Add RigidSandCoupling,2024-07-14T09:20:09Z,clouddon,clouddon@sina.com,clouddon,v0.9.0,Support Qt 6,v0.9.0,,,clouddon,Apache License 2.0,peridyno,peridyno,3,fluid-dynamics,rigid-body-dynamics,large-deformations,real-time-simulation,,,,,,,,,,,,,,,,,/peridyno/peridyno,3,10,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/penrose/penrose,https://github.com/penrose/penrose,0,,,0,0,0,0,0,0,1,1,0,0,0,Create beautiful diagrams just by typing notation in plain text.,"# Penrose [![npm (scoped)](https://img.shields.io/npm/v/@penrose/core)](https://www.npmjs.com/package/@penrose/core) [![license](https://img.shields.io/github/license/penrose/penrose)](LICENSE) [![Build](https://github.com/penrose/penrose/actions/workflows/build.yml/badge.svg)](https://github.com/penrose/penrose/actions/workflows/build.yml) [![Discord](https://dcbadge.vercel.app/api/server/a7VXJU4dfR?style=flat)](https://discord.gg/a7VXJU4dfR) [![Twitter: @UsePenrose](https://img.shields.io/badge/follow-%40UsePenrose-1DA1F2?logo=twitter&style=social)](https://twitter.com/UsePenrose)\n\n[Penrose](https://penrose.cs.cmu.edu/) is a platform that enables people to\n**create beautiful diagrams just by typing notation in plain\ntext.** The goal is to make it easy for non-experts to create and explore\nhigh-quality diagrams and provide deeper insight into challenging technical\nconcepts. We aim to democratize the process of creating visual intuition.\n\n## Usage\n\nYou can [try Penrose in your browser](https://penrose.cs.cmu.edu/try/index.html)\nwithout any installation. For a more detailed step-by-step introduction, check\nout our [tutorials](https://penrose.cs.cmu.edu/docs/tutorial/welcome). Or, for\nmore reference-style information, take a look at our\n[documentation](https://penrose.cs.cmu.edu/docs/ref).\n\n## Example\n\nHere's a simple Penrose visualization in the domain of set theory.\n\n<img src=""docs/assets/output.svg"" width=500>\n\nIt's specified by the following trio of Domain, Substance, and Style programs\n(with variation `MonsoonCaterpillar95943`):\n\n- `setTheory.domain`:\n\n  ```\n  type Set\n  \n  predicate Disjoint(Set s1, Set s2)\n  predicate Intersecting(Set s1, Set s2)\n  predicate Subset(Set s1, Set s2)\n  ```\n\n- `tree.substance`:\n\n  ```\n  Set A, B, C, D, E, F, G\n  \n  Subset(B, A)\n  Subset(C, A)\n  Subset(D, B)\n  Subset(E, B)\n  Subset(F, C)\n  Subset(G, C)\n  \n  Disjoint(E, D)\n  Disjoint(F, G)\n  Disjoint(B, C)\n  \n  AutoLabel All\n  ```\n\n- `euler.style`:\n\n  ```\n  canvas {\n    width = 800\n    height = 700\n  }\n  \n  forall Set x {\n    shape x.icon = Circle { }\n    shape x.text = Equation {\n      string : x.label\n      fontSize : ""32px""\n    }\n    ensure contains(x.icon, x.text)\n    encourage norm(x.text.center - x.icon.center) == 0\n    layer x.text above x.icon\n  }\n  \n  forall Set x; Set y\n  where Subset(x, y) {\n    ensure disjoint(y.text, x.icon, 10)\n    ensure contains(y.icon, x.icon, 5)\n    layer x.icon above y.icon\n  }\n  \n  forall Set x; Set y\n  where Disjoint(x, y) {\n    ensure disjoint(x.icon, y.icon)\n  }\n  \n  forall Set x; Set y\n  where Intersecting(x, y) {\n    ensure overlapping(x.icon, y.icon)\n    ensure disjoint(y.text, x.icon)\n    ensure disjoint(x.text, y.icon)\n  }\n  ```\n\n## Contributing\n\nSee [`CONTRIBUTING.md`](CONTRIBUTING.md).\n\n## License\n\nThis repository is licensed under the [MIT License](LICENSE).\n",6725,mathematics,TypeScript,11,HTML,Shell,CSS,Emacs Lisp,TypeScript,JavaScript,Nearley,Vim Script,Vue,Mathematica,Python,,,,,,,,,,,,,,,,,,1066,118,934,14,67,43,1290,1116378,280,752,595,157,9be26b240c96c930d031d5eb6253845fda3746f8,fix: fixed mod err in substance (#1834),2024-07-19T19:44:18Z,Kyle Lee,85892844+KyleleeSea@users.noreply.github.com,KyleleeSea,v3.2.0,"## What's Changed\r\n* docs: fix anchors for functions by @logan12358 in https://github.com/penrose/penrose/pull/1570\r\n* docs: render markdown in function param description by @wodeni in https://github.com/penrose/penrose/pull/1575\r\n* chore: enable strict TS checking in `roger` by @wodeni in https://github.com/penrose/penrose/pull/1576\r\n* docs: fix single quotation mark by @liangyiliang in https://github.com/penrose/penrose/pull/1579\r\n* feat: Update 3D Spectral Graphs by @jiriminarcik in https://github.com/penrose/penrose/pull/1550\r\n* chore: remove redundant TSConfig stuff for Roger by @samestep in https://github.com/penrose/penrose/pull/1581\r\n* style: replace `./..` with just `..` by @samestep in https://github.com/penrose/penrose/pull/1582\r\n* chore: go from `.eslintrc.cjs` to `.eslintrc.json` by @samestep in https://github.com/penrose/penrose/pull/1585\r\n* feat: `numberof` and `nameof` by @liangyiliang in https://github.com/penrose/penrose/pull/1583\r\n* feat: diagrams for selected and generic Alloy models by @liangyiliang in https://github.com/penrose/penrose/pull/1584\r\n* fix: Alloy example dining-philosophers use `numberof` and `nameof` by @liangyiliang in https://github.com/penrose/penrose/pull/1587\r\n* test: add `eslint-plugin-unary-minus` by @samestep in https://github.com/penrose/penrose/pull/1589\r\n* docs: Update README.md by @keenancrane in https://github.com/penrose/penrose/pull/1580\r\n* docs: allow TeX in Style function docs by @wodeni in https://github.com/penrose/penrose/pull/1592\r\n* docs: blog post for tailoring graph domain by @rjainrjain in https://github.com/penrose/penrose/pull/1590\r\n* chore: graphs blog post fixes by @rjainrjain in https://github.com/penrose/penrose/pull/1594\r\n* feat: Substance indexed sets by @wodeni in https://github.com/penrose/penrose/pull/1572\r\n* fix: bounding boxes and points for rotated rects by @liangyiliang in https://github.com/penrose/penrose/pull/1600\r\n* feat: Add common matrix functions to standard library by @keenancrane in https://github.com/penrose/penrose/pull/1538\r\n* docs: render Style function docstrings at build-time by @wodeni in https://github.com/penrose/penrose/pull/1602\r\n* feat: Add dinoshade example to registry by @keenancrane in https://github.com/penrose/penrose/pull/1601\r\n* feat: row-indexing of matrices by @liangyiliang in https://github.com/penrose/penrose/pull/1599\r\n* build: add rust-toolchain.toml for automatic toolchain installation by @jhvst in https://github.com/penrose/penrose/pull/1593\r\n* build: exclude test files from `core` npm pack by @wodeni in https://github.com/penrose/penrose/pull/1604\r\n* build: revert ""build: add rust-toolchain.toml for automatic toolchain installation"" by @samestep in https://github.com/penrose/penrose/pull/1608\r\n* build: experimental `core` bundle by @wodeni in https://github.com/penrose/penrose/pull/1607\r\n* feat: Penrose logo by @keenancrane in https://github.com/penrose/penrose/pull/1605\r\n* chore: bump version to 3.2.0 by @wodeni in https://github.com/penrose/penrose/pull/1609\r\n\r\n\r\n**Full Changelog**: https://github.com/penrose/penrose/compare/v3.1.0...v3.2.0",v3.2.0,"Wode ""Nimo"" Ni",,wodeni,MIT License,penrose,penrose,13,visualization,domain-specific-language,mathematics,diagrams,programming-language,,,,,,,,,,,,,,,,/penrose/penrose,21,111,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/PacificBiosciences/ccs,https://github.com/PacificBiosciences/ccs,0,"tool seems to be scientific, but repository has only docs. Worth looking into other repos of the same author",0,0,0,0,0,0,0,0,0,0,1,0,CCS: Generate Highly Accurate Single-Molecule Consensus Reads (HiFi Reads),"<a href=""https://ccs.how/"">\n  <img src=""docs/img/ccs2022.png"" alt=""CCS logo"" width=""200px"" align=""right""/>\n</a>\n<h1 align=""center"">CCS - Generate PacBio HiFi data</h1>\n<p align=""center"">Latest documentation on <a href=""https://ccs.how/"">ccs.how</a></p>\n\n***\n",113,bioinformatics,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,42,10,32,0,1,18,89,35769,31,0,0,0,0b50312ab6a5c8d2eb1210c1505fe9979e7f13c6,Aux files for Revio,2024-03-18T13:41:39Z,Armin Toepfer,atoepfer@pacificbiosciences.com,armintoepfer,Version 6.4.0,Version 6.4.0,v6.4.0,Armin Töpfer,,armintoepfer,BSD 3-Clause Clear License,ccs,PacificBiosciences,13,bioinformatics,consensus,pacbio,variant-calling,ccs,machine-learning,,,,,,,,,,,,,,,/PacificBiosciences/ccs,16,24,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/owlcollab/owltools,https://github.com/owlcollab/owltools,0.5,"Tool collection, wrapper to the main project, not sure",0,0,0,0,0,0,0,1,0,0,0,0,OWLTools,"[![Build Status](https://travis-ci.org/owlcollab/owltools.svg?branch=master)](https://travis-ci.org/owlcollab/owltools)\n[![DOI](https://zenodo.org/badge/13996/owlcollab/owltools.svg)](https://zenodo.org/badge/latestdoi/13996/owlcollab/owltools)\n\nFor full documentation,\n\n * [see the Wiki](https://github.com/owlcollab/owltools/wiki).\n * [see the java API docs](https://owlcollab.github.io/owltools)\n\n## OWLTools Build Instructions\n\nThe OWLTools use maven as a build tool.\n\nThese instructions assume that a valid maven installation is available. The recommended maven version is 3.0.x, whereby x denotes the latest release for this branch.\n\nUpdate: OWLTools also requires `git`. Only a proper clone via git, will allow the build to complete.\n\nDuring the build process, we extract the git version and branch information. These details (and the build date) will be added to the manifest of the jar. If the `.git` folder is not available the build process will fail.\n\n### Building OWLTools\n\n#### Prerequiste: Get source from Git\n\n`git clone https://github.com/owlcollab/owltools.git`\n\n#### Option 1: Command line\n\n1) Change into to the folder of `OWLTools-Parent`\n\n2a) Run command: `mvn clean install`: This will trigger a complete build of all OWLTools projects and generate the required jars for execution. Remark: As part of the build the tests are executed. Any failed test will stop the build.\n\n2b) Build without test execution (Not Recommended): Run command: mvn clean install -DskipTests\n\n#### Option 2: Eclipse\n\nRequires either:\n* Eclipse 3.7\n* Eclipse 3.6 with installed maven plugin m2e\n\nUse the provided Eclipse launch configurations to trigger the build. The configuration are located in the `OWLTools-Parent/eclipse-configs` folder.\n\n## Downloading OWLTools\n\nYou can download a pre-built JAR and script-wrapper instead of building owltools from the Java source. You can find releases on the [OWLTools GitHub releases page](https://github.com/owlcollab/owltools/releases).\n\nNote: you may need to make this file executable. Occasionally when downloaded files lose the ""executable"" permisison. You can do this with `chmod +x owltools`.\n\nIf you add this to your [PATH](https://en.wikipedia.org/wiki/PATH_(variable)), you can run owltools on the command line:\n\n```\nexport PATH=$PATH:directory/to/where/you/downloaded/owltools/\nowltools -h\n```\n\n## Running OWLTools (Unix and MacOS)\n\nRunning OWLTools requires a successful build, as described in the previous section.\n\n+ OWLTools Command-Line Tools: The build produces a combined executable bash script and jar, to be found in `OWLTools-Runner/bin`\nor in the `OWLTools-Runner/target` directory\n\n+ OORT: The executables and the generated jar are both located in `OWLTools-Oort/bin`\n",106,bioinformatics,Java,8,Java,Perl,Shell,HTML,JavaScript,Makefile,R,Batchfile,,,,,,,,,,,,,,,,,,,,,92,11,64,17,61,18,0,76109,32,235,109,126,50e2d32c505d3498907a5909d821fe73fb928562,Merge pull request #327 from owlcollab/owlapi-upgrade,2024-06-12T17:31:45Z,Jim Balhoff,jim@balhoff.org,balhoff,Release 2024-06-12,"This release updates to the newest OWL API, which includes some enhancements to the OBO file format (https://github.com/owlcs/owlapi/pull/1102).\r\n\r\n## What's Changed\r\n* Test on openjdk by @balhoff in https://github.com/owlcollab/owltools/pull/296\r\n* Update many dependencies by @balhoff in https://github.com/owlcollab/owltools/pull/295\r\n* Upgrading jetty by @cmungall in https://github.com/owlcollab/owltools/pull/266\r\n* Bump jetty-server from 7.5.4.v20111024 to 9.4.17.v20190418 in /OWLTools-Web by @dependabot in https://github.com/owlcollab/owltools/pull/297\r\n* Filter out signatures from jars merged into uber-jar. by @balhoff in https://github.com/owlcollab/owltools/pull/299\r\n* Don't include signature files in owltools-oort-all.jar by @alexhenrie in https://github.com/owlcollab/owltools/pull/306\r\n* Add correct release location to README. by @balhoff in https://github.com/owlcollab/owltools/pull/309\r\n* Use https for berkeleybop maven repo, for #315 by @balhoff in https://github.com/owlcollab/owltools/pull/317\r\n* Upgrade to OWL API 4.5.29. by @balhoff in https://github.com/owlcollab/owltools/pull/327\r\n\r\n## New Contributors\r\n* @dependabot made their first contribution in https://github.com/owlcollab/owltools/pull/297\r\n* @alexhenrie made their first contribution in https://github.com/owlcollab/owltools/pull/306\r\n\r\n**Full Changelog**: https://github.com/owlcollab/owltools/compare/2020-04-06...2024-06-12",12.06.2024,Jim Balhoff,,balhoff,"BSD 3-Clause ""New"" or ""Revised"" License",owltools,owlcollab,3,web-ontology-language,owl-api,bioinformatics,ontology,build-tool,api,,,,,,,,,,,,,,,/owlcollab/owltools,4,18,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/owkin/PyDESeq2,https://github.com/owkin/PyDESeq2,0,,,0,1,1,0,0,0,0,0,0,0,0,A Python implementation of the DESeq2 pipeline for bulk RNA-seq DEA.,"<img src=""docs/source/_static/pydeseq2_logo_green.png"" width=""600"">\n\n#\n[![pypi version](https://img.shields.io/pypi/v/pydeseq2)](https://pypi.org/project/pydeseq2)\n[![pypiDownloads](https://static.pepy.tech/badge/pydeseq2)](https://pepy.tech/project/pydeseq2)\n[![condaDownloads](https://img.shields.io/conda/dn/bioconda/pydeseq2?logo=Anaconda)](https://anaconda.org/bioconda/pydeseq2)\n[![license](https://img.shields.io/pypi/l/pydeseq2)](LICENSE)\n\nPyDESeq2 is a python implementation of the [DESeq2](https://bioconductor.org/packages/release/bioc/html/DESeq2.html) \nmethod [1] for differential expression analysis (DEA) with bulk RNA-seq data, originally in R.\nIt aims to facilitate DEA experiments for python users.\n\nAs PyDESeq2 is a re-implementation of [DESeq2](https://bioconductor.org/packages/release/bioc/html/DESeq2.html) from \nscratch, you may experience some differences in terms of retrieved values or available features.\n\nCurrently, available features broadly correspond to the default settings of DESeq2 (v1.34.0) for single-factor and \nmulti-factor analysis (with categorical or continuous factors) using Wald tests.\nWe plan to implement more in the future.\nIn case there is a feature you would particularly like to be implemented, feel free to open an issue.\n\n## Table of Contents\n- [PyDESeq2](#pydeseq2)\n  - [Table of Contents](#table-of-contents)\n  - [Installation](#installation)\n    - [Requirements](#requirements)\n  - [Getting started](#getting-started)\n    - [Documentation](#documentation)\n    - [Data](#data)\n  - [Contributing](#contributing)\n    - [1 - Download the repository](#1---download-the-repository)\n    - [2 - Create a conda environment](#2---create-a-conda-environment)\n  - [Development roadmap](#development-roadmap)\n  - [Citing this work](#citing-this-work)\n  - [References](#references)\n  - [License](#license)\n\n## Installation\n\n### PyPI\n\n`PyDESeq2` can be installed from PyPI using `pip`:\n\n`pip install pydeseq2`\n\nWe recommend installing within a conda environment:\n\n```\nconda create -n pydeseq2\nconda activate pydeseq2\nconda install pip\npip install pydeseq2\n```\n\n### Bioconda\n\n`PyDESeq2` can also be installed from Bioconda with `conda`:\n\n`conda install -c bioconda pydeseq2`\n\nIf you're interested in contributing or want access to the development version, please see the [contributing](#contributing) section.\n\n### Requirements\n\nThe list of package version requirements is available in `setup.py`.\n\nFor reference, the code is being tested in a github workflow (CI) with python\n3.9 to 3.11 and the following package versions:\n```\n- anndata 0.8.0\n- numpy 1.23.0\n- pandas 1.4.3\n- scikit-learn 1.1.1\n- scipy 1.11.0\n```\n\nPlease don't hesitate to open an issue in case you encounter any issue due to possible deprecations.\n\n\n## Getting started\n\nThe [Getting Started](https://pydeseq2.readthedocs.io/en/latest/auto_examples/index.html) section of the documentation\ncontains downloadable examples on how to use PyDESeq2.\n\n\n### Documentation\n\nThe documentation is hosted [here on ReadTheDocs](https://pydeseq2.readthedocs.io/en/latest/). \nIf you want to have the latest version of the documentation, you can build it from source.\nPlease go to the dedicated [README.md](https://github.com/owkin/PyDESeq2/blob/main/docs/README.md) for information on how to do so.\n\n### Data\n\nThe quick start examples use synthetic data, provided in this repo (see [datasets](https://github.com/owkin/PyDESeq2/blob/main/datasets/README.md).)\n\nThe experiments described in our [preprint](https://www.biorxiv.org/content/10.1101/2022.12.14.520412v1) rely on data\nfrom [The Cancer Genome Atlas](https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga),\nwhich may be obtained from this [portal](https://portal.gdc.cancer.gov/).\n\n## Contributing\n\nPlease the [Contributing](https://pydeseq2.readthedocs.io/en/latest/usage/contributing.html) section of the\ndocumentation to see how you can contribute to PyDESeq2.\n\n### 1 - Download the repository\n\n`git clone https://github.com/owkin/PyDESeq2.git`\n\n### 2 - Create a conda environment\n\nRun `conda create -n pydeseq2 python=3.9` (or higher python version) to create the `pydeseq2` environment and then activate it:\n`conda activate pydeseq2`.\n\n`cd` to the root of the repo and run `pip install -e .""[dev]""` to install in developer mode.\n\nThen, run `pre-commit install`.\n\nThe `pre-commit` tool will automatically run [ruff](https://docs.astral.sh/ruff/), [black](https://black.readthedocs.io/en/stable/), and [mypy](https://mypy.readthedocs.io/en/stable/).\n\nPyDESeq2 is a living project and any contributions are welcome! Feel free to open new PRs or issues.\n\n## Development Roadmap\n\nHere are some of the features and improvements we plan to implement in the future:\n\n- [x] Integration to the [scverse](https://scverse.org/) ecosystem:\n  * [x] Refactoring to use the [AnnData](https://anndata.readthedocs.io/) data structure\n  * [x] Submitting a PR to be listed as an [scverse ecosystem](https://github.com/scverse/ecosystem-packages/) package\n- [x] Variance-stabilizing transformation\n- [ ] Improving multi-factor analysis:\n  * [x] Allowing n-level factors\n  * [x] Support for continuous covariates\n  * [ ] Implementing interaction terms\n\n\n## Citing this work\n\n```\n@article{muzellec2023pydeseq2,\n  title={PyDESeq2: a python package for bulk RNA-seq differential expression analysis},\n  author={Muzellec, Boris and Telenczuk, Maria and Cabeli, Vincent and Andreux, Mathieu},\n  year={2023},\n  doi = {10.1093/bioinformatics/btad547},\n  journal={Bioinformatics},\n}\n```\n\n## References\n\n[1] Love, M. I., Huber, W., & Anders, S. (2014). ""Moderated estimation of fold\n        change and dispersion for RNA-seq data with DESeq2."" Genome biology, 15(12), 1-21.\n        <https://genomebiology.biomedcentral.com/articles/10.1186/s13059-014-0550-8>\n\n[2] Zhu, A., Ibrahim, J. G., & Love, M. I. (2019).\n        ""Heavy-tailed prior distributions for sequence count data:\n        removing the noise and preserving large differences.""\n        Bioinformatics, 35(12), 2084-2092.\n        <https://academic.oup.com/bioinformatics/article/35/12/2084/5159452>\n\n## License\n\nPyDESeq2 is released under an [MIT license](https://github.com/owkin/PyDESeq2/blob/main/LICENSE).\n\n",549,bioinformatics,Python,2,Python,Shell,,,,,,,,,,,,,,,,,,,,,,,,,,,185,11,169,5,11,24,0,1269,59,117,86,31,90350387af62b5b4ede1965390ebc0734b97c144,[pre-commit.ci] pre-commit autoupdate (#301),2024-07-09T07:53:51Z,pre-commit-ci[bot],66853113+pre-commit-ci[bot]@users.noreply.github.com,pre-commit-ci[bot],v0.4.10,"## What's Changed\r\n* BUG initialize `beta_init` when the design matrix is not full rank by @BorisMuzellec in https://github.com/owkin/PyDESeq2/pull/281\r\n* MAINT Replace np.NaN with np.nan -> numpy 2.0 compat by @Zethson in https://github.com/owkin/PyDESeq2/pull/282\r\n* BUG enable vst be used ""blindly"" and to fit its own dispersion by @laudmt in https://github.com/owkin/PyDESeq2/pull/268\r\n* CHORE update code owners by @mandreux-owkin in https://github.com/owkin/PyDESeq2/pull/289\r\n* MAINT summary function uses quiet to print by @yihming in https://github.com/owkin/PyDESeq2/pull/286\r\n* ENH Poscount implementation & diag(XXT) optimization by @asistradition in https://github.com/owkin/PyDESeq2/pull/284\r\n* BUG Inference n_cpus overwritten issue by @yihming in https://github.com/owkin/PyDESeq2/pull/293\r\n* DOC specify what happens if n_cpus and inference are set by @umarteauowkin in https://github.com/owkin/PyDESeq2/pull/294\r\n* DOC fix text format in DeseqStats docstring by @BorisMuzellec in https://github.com/owkin/PyDESeq2/pull/297\r\n* ENH Remove count DataFrame from calculate_cooks by @asistradition in https://github.com/owkin/PyDESeq2/pull/292\r\n* MAINT cast counts to int when initialising a DeseqDataSet from anndata by @BorisMuzellec in https://github.com/owkin/PyDESeq2/pull/299\r\n\r\n## New Contributors\r\n* @Zethson made their first contribution in https://github.com/owkin/PyDESeq2/pull/282\r\n* @laudmt made their first contribution in https://github.com/owkin/PyDESeq2/pull/268\r\n* @yihming made their first contribution in https://github.com/owkin/PyDESeq2/pull/286\r\n* @asistradition made their first contribution in https://github.com/owkin/PyDESeq2/pull/284\r\n* @umarteauowkin made their first contribution in https://github.com/owkin/PyDESeq2/pull/294\r\n\r\n**Full Changelog**: https://github.com/owkin/PyDESeq2/compare/v0.4.9...v0.4.10",v0.4.10,Boris Muzellec,,BorisMuzellec,MIT License,PyDESeq2,owkin,21,bioinformatics,differential-expression,python,rna-seq,transcriptomics,,,,,,,,,,,,,,,,/owkin/PyDESeq2,21,10,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/oscar-system/Oscar.jl,https://github.com/oscar-system/Oscar.jl,0,,,0,1,0,0,0,0,1,0,0,0,0,"A comprehensive open source computer algebra system for computations in algebra, geometry, and number theory.","# Oscar.jl\n\n| **Documentation**                                                         | **Build Status**                                      |\n|:-------------------------------------------------------------------------:|:-----------------------------------------------------:|\n| [![][docs-stable-img]][docs-stable-url] [![][docs-dev-img]][docs-dev-url] | [![][ga-img]][ga-url] [![][codecov-img]][codecov-url] |\n\n\nWelcome to the OSCAR project, a visionary new computer algebra system\nwhich combines the capabilities of four cornerstone systems: GAP,\nPolymake, Antic and Singular.\n\n## Installation\n\nOSCAR requires Julia 1.6 or newer. In principle it can be installed and used\nlike any other Julia package; doing so will take a couple of minutes:\n\n```\njulia> using Pkg\njulia> Pkg.add(""Oscar"")\njulia> using Oscar\n```\n\nHowever, some of OSCAR's components have additional requirements.\nFor more detailed information, please consult the [installation\ninstructions](https://www.oscar-system.org/install/) on our website.\n\n## Contributing to OSCAR\n\nPlease read the [introduction for new developers](https://docs.oscar-system.org/dev/DeveloperDocumentation/new_developers/)\nin the OSCAR manual to learn more on how to contribute to OSCAR.\n\n## Examples of usage\n\n```\njulia> using Oscar\n  ___   ____   ____    _    ____\n / _ \ / ___| / ___|  / \  |  _ \   |  Combining ANTIC, GAP, Polymake, Singular\n| | | |\___ \| |     / _ \ | |_) |  |  Type ""?Oscar"" for more information\n| |_| | ___) | |___ / ___ \|  _ <   |  Manual: https://docs.oscar-system.org\n \___/ |____/ \____/_/   \_\_| \_\  |  Version 1.2.0-DEV\njulia> k, a = quadratic_field(-5)\n(Imaginary quadratic field defined by x^2 + 5, sqrt(-5))\n\njulia> zk = maximal_order(k)\nMaximal order of Imaginary quadratic field defined by x^2 + 5\nwith basis AbsSimpleNumFieldElem[1, sqrt(-5)]\n\njulia> factorizations(zk(6))\n2-element Vector{Fac{AbsSimpleNumFieldOrderElem}}:\n -1 * -3 * 2\n -1 * (-sqrt(-5) - 1) * (-sqrt(-5) + 1)\n\njulia> Qx, x = polynomial_ring(QQ, [:x1,:x2])\n(Multivariate polynomial ring in 2 variables over QQ, QQMPolyRingElem[x1, x2])\n\njulia> R = grade(Qx, [1,2])[1]\nMultivariate polynomial ring in 2 variables over QQ graded by\n  x1 -> [1]\n  x2 -> [2]\n\njulia> f = R(x[1]^2+x[2])\nx1^2 + x2\n\njulia> degree(f)\n[2]\n\njulia> F = free_module(R, 1)\nFree module of rank 1 over R\n\njulia> s = sub(F, [f*F[1]])[1]\nSubmodule with 1 generator\n1 -> (x1^2 + x2)*e[1]\nrepresented as subquotient with no relations.\n\njulia> H, mH = hom(s, quo(F, s)[1])\n(hom of (s, Subquotient of\n1 -> e[1]\nby\n1 -> (x1^2 + x2)*e[1]), Map: H -> set of all homomorphisms from s to subquotient of Submodule with 1 generator\n1 -> e[1]\nby Submodule with 1 generator\n1 -> (x1^2 + x2)*e[1])\n\njulia> mH(H[1])\nMap with following data\nDomain:\n=======\nSubmodule with 1 generator\n1 -> (x1^2 + x2)*e[1]\nrepresented as subquotient with no relations.\nCodomain:\n=========\nSubquotient of Submodule with 1 generator\n1 -> e[1]\nby Submodule with 1 generator\n1 -> (x1^2 + x2)*e[1]\n```\n\nOf course, the cornerstones are also available directly:\n\n```\njulia> C = Polymake.polytope.cube(3);\n\njulia> C.F_VECTOR\npm::Vector<pm::Integer>\n8 12 6\n\njulia> RP2 = Polymake.topaz.real_projective_plane();\n\njulia> RP2.HOMOLOGY\npm::Array<topaz::HomologyGroup<pm::Integer> >\n({} 0)\n({(2 1)} 0)\n({} 0)\n```\n\n## Citing OSCAR\n\nIf you have used OSCAR in the preparation of a paper please cite it as described below:\n\n    [OSCAR]\n        OSCAR -- Open Source Computer Algebra Research system, Version 1.2.0-DEV,\n        The OSCAR Team, 2024. (https://www.oscar-system.org)\n    [OSCAR-book]\n        Wolfram Decker, Christian Eder, Claus Fieker, Max Horn, Michael Joswig, eds.\n        The Computer Algebra System OSCAR: Algorithms and Examples,\n        Algorithms and Computation in Mathematics, Springer, 2024.\n\nIf you are using BibTeX, you can use the following BibTeX entries:\n\n    @misc{OSCAR,\n      key          = {OSCAR},\n      organization = {The OSCAR Team},\n      title        = {OSCAR -- Open Source Computer Algebra Research system,\n                      Version 1.2.0-DEV},\n      year         = {2024},\n      url          = {https://www.oscar-system.org},\n      }\n\n    @book{OSCAR-book,\n      editor = {Decker, Wolfram and Eder, Christian and Fieker, Claus and Horn, Max and Joswig, Michael},\n      title = {The {C}omputer {A}lgebra {S}ystem {OSCAR}: {A}lgorithms and {E}xamples},\n      year = {2024},\n      publisher = {Springer},\n      series = {Algorithms and {C}omputation in {M}athematics},\n      volume = {32},\n      edition = {1},\n      url = {https://link.springer.com/book/9783031621260},\n      month = {8},\n      issn = {1431-1550},\n    }\n\n## Funding\n\nThe development of this Julia package is supported by the Deutsche\nForschungsgemeinschaft DFG within the\n[Collaborative Research Center TRR 195](https://www.computeralgebra.de/sfb/).\n\n[docs-dev-img]: https://img.shields.io/badge/docs-dev-blue.svg\n[docs-dev-url]: https://docs.oscar-system.org/dev/\n\n[docs-stable-img]: https://img.shields.io/badge/docs-stable-blue.svg\n[docs-stable-url]: https://docs.oscar-system.org/stable/\n\n[ga-img]: https://github.com/oscar-system/Oscar.jl/actions/workflows/CI.yml/badge.svg?branch=master&event=push\n[ga-url]: https://github.com/oscar-system/Oscar.jl/actions?query=workflow%3A%22Run+tests%22\n\n[codecov-img]: https://codecov.io/gh/oscar-system/Oscar.jl/branch/master/graph/badge.svg?branch=master\n[codecov-url]: https://codecov.io/gh/oscar-system/Oscar.jl\n",314,mathematics,Julia,4,Julia,Shell,GAP,Fantom,,,,,,,,,,,,,,,,,,,,,,,,,2944,258,2650,36,38,91,5578,806976,113,957,717,240,a8d2911ac07134e26f657bb4b84105ddd7adeb09,Sets poly ring caching to false for modules (#3872),2024-07-19T16:34:05Z,ederc,ederc@mathematik.uni-kl.de,ederc,v1.1.1,## Oscar 1.1.1\r\n\r\n### Backported changes\r\n* catch a corner case for elliptic surfaces #3880\r\n* speedup elliptic surfaces #3884\r\n\r\n**Full Changelog**: https://github.com/oscar-system/Oscar.jl/compare/v1.1.0...v1.1.1,v1.1.1,,,github-actions[bot],Other,Oscar.jl,oscar-system,34,abstract-algebra,computer-algebra,computer-algebra-system,julia,julia-package,math,mathematics,maths,,,,,,,,,,,,,/oscar-system/Oscar.jl,34,20,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/OSC/ondemand,https://github.com/OSC/ondemand,0,,,0,0,0,0,0,0,1,1,0,0,0,"Supercomputing. Seamlessly. Open, Interactive HPC Via the Web","# Open OnDemand\n![GitHub Release](https://img.shields.io/github/release/osc/ondemand.svg?color=informational)\n[![Build Status](https://github.com/osc/ondemand/workflows/Tests/badge.svg)](https://github.com/OSC/ondemand/actions?query=workflow%3ATests)\n[![GitHub License](https://img.shields.io/badge/license-MIT-green.svg?color=success)](https://opensource.org/licenses/MIT)\n[![Paper DOI](http://joss.theoj.org/papers/10.21105/joss.00622/status.svg)](https://doi.org/10.21105/joss.00622)\n[![Source DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6323791.svg)](https://doi.org/10.5281/zenodo.6323791)\n> Supercomputing. Seamlessly. Open, Interactive HPC Via the Web\n\n- Website: https://openondemand.org/\n- Website repo: https://github.com/OSC/openondemand.org\n- Documentation: https://osc.github.io/ood-documentation/latest/\n- Main code repo: https://github.com/OSC/ondemand\n- Core library repo: https://github.com/OSC/ood_core\n- Slack: [Open OnDemand Slack]\n- Discourse: [Discourse]\n\nThis work is supported by the National Science Foundation of the United States under the award [NSF SI2-SSE-1534949](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1534949) and [NSF CSSI-Frameworks-1835725](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1835725).\n\n## Overview\nOpen OnDemand is an NSF-funded open-source HPC portal. The goal of Open OnDemand is to provide an easy way for system administrators to provide web access to their HPC resources, including, but not limited to:\n\n* Plugin-free web experience\n* Easy file management\n* Command-line shell access\n* Job management and monitoring across different batch servers and resource managers\n* Graphical desktop environments and desktop applications\n\n## Demo\n\n![Open ondemand demo demonstrating integration with Open XDMOD; interactive jobs with desktops, Jupyter and visual studio code; file browsing, creation, editing and deletion.](docs/imgs/open_ondemand_demo.gif)\n\n## Installation\nInstalling Open OnDemand simple, use our `.rpm` or `.deb` packages. Get started by visiting the [installation instructions] in our documentation.\n\n### Container demos\n\nYou can use the [hpc toolset tutorial] to demonstrate Open OnDemand before installing on your systems. This `docker-compose` project\nhas a full suite of applications like Slurm, Coldfront and of course Open OnDemand.  It also includes tutorials on how to use\nand update the applications.\n\n## Architecture\nLearn more about Open OnDemand's system architecture and request lifecycle by visiting our <a href=""https://osc.github.io/ood-documentation/latest/architecture.html"">documentation</a>.\n\n## Community\nOpen OnDemand has an active and growing community! Don't hesitate to reach out to the developers via our [Discourse] instance if you would like more information or need help installing or configuring Open OnDemand.\n<br/>\n<br/>\n<a href=""https://discourse.osc.edu""><img src=""https://upload.wikimedia.org/wikipedia/commons/1/17/Discourse_icon.svg"" width=150></a>\n\n## Contributing\n\nBug reports and pull requests are welcome on GitHub at\nhttps://github.com/OSC/ondemand. Please read our [contributing guide] to get started, or find us on our [Discourse] instance if you have any questions about contributing!\n\n## License\n\nThe code is available as open source under the terms of the [MIT License].\n\n## Maintained by OSC\nThis project is maintained by the <a href=""https://www.osc.edu"">Ohio Supercomputer Center (OSC)</a>, a member of the <a href=""https://www.oh-tech.org/"">Ohio Technology Consortium</a>, the technology and information division of the <a href=""https://education.ohio.gov/"">Ohio Department of Higher Education.</a>\n\n[MIT License]: http://opensource.org/licenses/MIT\n[Open OnDemand Documentation]: https://osc.github.io/ood-documentation/latest/\n[installation instructions]: https://osc.github.io/ood-documentation/latest/requirements.html\n[contributing guide]: CONTRIBUTING.md\n[Discourse]: https://discourse.osc.edu\n[hpc toolset tutorial]: https://github.com/ubccr/hpc-toolset-tutorial/\n[Open OnDemand Slack]: http://openondemand.org/slack\n",264,hpc-applications,JavaScript,12,Ruby,Shell,Lua,HTML,JavaScript,CoffeeScript,CSS,Dockerfile,SCSS,Handlebars,TeX,Makefile,,,,,,,,,,,,,,,,,2101,308,1784,9,30,68,0,32414,100,1568,1106,462,90300cbcd6f6f4fff8aed89f75c4e05bcc906af3,move ace dependency to yarn (#3629),2024-07-18T18:09:28Z,Jeff Ohrstrom,johrstrom@osc.edu,johrstrom,v3.1.7,## What's Changed\r\n* Removes class 'w-100' from Dashboard logo (#3568) by @johrstrom in https://github.com/OSC/ondemand/pull/3632\r\n* climate_control needs to be production gem by @johrstrom in https://github.com/OSC/ondemand/pull/3641\r\n\r\n\r\n**Full Changelog**: https://github.com/OSC/ondemand/compare/v3.1.6...v3.1.7,v3.1.7,Jeff Ohrstrom,,johrstrom,MIT License,ondemand,OSC,179,hpc,hpc-applications,gateway,hacktoberfest,,,,,,,,,,,,,,,,,/OSC/ondemand,196,16,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/Orillusion/orillusion,https://github.com/Orillusion/orillusion,0,,,0,0,1,0,0,0,0,1,0,0,0,Orillusion is a pure Web3D rendering engine which is fully developed based on the WebGPU standard.,"![Cover Art](https://github.com/Orillusion/orillusion-webgpu-samples/blob/main/logo_new.png)     \n## Orillusion\n\n[![Test](https://github.com/Orillusion/orillusion/actions/workflows/ci.yml/badge.svg)](https://github.com/Orillusion/orillusion/actions/workflows/ci.yml)\n[![npm](https://img.shields.io/npm/v/@orillusion/core)](https://www.npmjs.com/package/@orillusion/core)\n\n`Orillusion`  is a pure Web3D rendering engine which is fully developed based on the `WebGPU` standard. It aims to achieve desktop-level rendering effects and supports 3D rendering of complex scenes in the browser.\n\n## Need to know\nBeta version,  **NOT**  recommended for any commercial application.\n\n## Install\n\n### NPM\nWe recommend using front-end build tools for developing Web3D applications, such  [Vite](https://vitejs.dev/) or [Webpack](https://webpack.js.org/).\n\n- Install dependencies:\n```text\nnpm install @orillusion/core --save\n```\n- Import on-demand:\n```javascript\nimport { Engine3D, Camera3D } from '@orillusion/core'\n```\n- Import globally:\n```javascript\nimport * as Orillusion from '@orillusion/core'\n```\n\n### CDN\nIn order to use the engine more conveniently, we support to use native `<script>` tag to import `Orillusion`. Three different ways to import using the official `CDN` link:\n\n- **Global Build:** You can use `Orillusion` directly from a CDN via a script tag:\n```html\n<script src=""https://unpkg.com/@orillusion/core/dist/orillusion.umd.js""></script>\n<script>  \n    const { Engine3D, Camera3D } = Orillusion  \n</script>\n```\nThe above link loads the global build of `Orillusion`, where all top-level APIs are exposed as properties on the global `Orillusion` object.\n\n-  **ESModule Build:** We recommend using the [ESModule](https://developer.mozilla.org/docs/Web/JavaScript/Guide/Modules) way for development. As most browsers have supported `ES` module, we just need to import the `ES` build version of `orillusion.es.js`\n```html\n<script type=""module"">  \n    import { Engine3D, Camera3D } from ""https://unpkg.com/@orillusion/core/dist/orillusion.es.js"" \n</script>\n```\n\n- **Import Maps:** In order to manage the name of dependencies, we recommend using [Import Maps](https://caniuse.com/import-maps)\n\n```html\n<!-- Define the name or address of ES Module -->  \n<script  type=""importmap"">  \n{  \n    ""imports"": { ""@orillusion/core"": ""https://unpkg.com/@orillusion/core/dist/orillusion.es.js"" }  \n}  \n</script>  \n<!-- Customerized names could be imported -->  \n<script  type=""module"">  \n    import { Engine3D, Camera3D } from ""@orillusion/core""\n</script>\n```\n\n## Usage\n### Create Engine3D instance\n\nAt the beginning, we need to use `Engine3D.init()` and then the instance `Engine3D` will be created for further use\n\n```javascript\nimport { Engine3D } from '@orillusion/core' \nEngine3D.init().then(()=>{  \n    // Next\n})\n```\nAs `Engine3D.init()` is asynchronous, we recommend using `async/await` in the code\n```javascript\nimport { Engine3D } from '@orillusion/core'  \nasync function demo(){  \n    await Engine3D.init();  \n    // Next \n}  \ndemo()\n```\n### Create canvas\nIn default, `Engine3D.init()`will create a `canvas` the same size with the window. Also, we could create a `canvas` manually using tag `<canvas>` with a `id`\n\n```html\n<canvas id=""canvas"" width=""800"" height=""500"" />\n```\nNext, we need to get the `<canvas>` via `id` and then init engine by passing the `<canvas>` to `canvasConfig`\n\n```javascript\nimport { Engine3D } from '@orillusion/core';  \nlet canvas = document.getElementById('canvas')  \n\nawait Engine3D.init({  \n    canvasConfig: { canvas }  \n})\n```\nPlease read the [Docs](https://www.orillusion.com/guide/) to Learn More.\n\n## Platform\n**Windows/Mac/Linux:**\n- Chrome 113+\n- Edge: 113+\n\n**Android (Behind the `enable-unsafe-webgpu` flag):** \n- Chrome Canary 113+ \n- Edge Canary 113+\n\n## Useful links\n- [Official Web Site](https://www.orillusion.com/)\n- [Documentation](https://www.orillusion.com/guide/)\n- [Forum](https://forum.orillusion.com/)\n\n## Dev and Contribution\nPlease make sure to read the [Contributing Guide](.github/contributing.md) before developing or making a pull request.\n\n## License \n\nOrillusion engine is released under the [MIT](https://opensource.org/licenses/MIT) license. \n",3946,graphics,TypeScript,4,HTML,JavaScript,TypeScript,WGSL,,,,,,,,,,,,,,,,,,,,,,,,,268,44,223,1,2,17,0,3149,461,154,95,59,4902f045e8c438410a19f2ab7d6b9b3cff523235,release v0.8,2024-07-10T14:32:44Z,ShuangLiu,liu.shuang@huasheng.io,lslzl3000,v0.8.1,"## [0.8.1](https://github.com/Orillusion/orillusion/compare/v0.7.2...v0.8.1) (2024-07-10)\r\n\r\n\r\n### Bug Fixes\r\n\r\n* **canvas:** fix external canvas resize on dpi change ([2e54053](https://github.com/Orillusion/orillusion/commit/2e54053efbffbd1d70d2a1b7c2e2f62ca672c4e9))\r\n* **effect:** fix grass get uniform data ([588721f](https://github.com/Orillusion/orillusion/commit/588721f52013fb9cadbe5d8156a41d2110636ac7))\r\n* **effect:** update windSpeed ([538ec2d](https://github.com/Orillusion/orillusion/commit/538ec2df976e1a45d13e3deaf15ef7c15fe5b409))\r\n* Error when lineJoin is set to round ([#366](https://github.com/Orillusion/orillusion/issues/366)) ([1ab8718](https://github.com/Orillusion/orillusion/commit/1ab87183c6910c2fc3e61d940b0183a2d5597b08))\r\n* fix issue of [#387](https://github.com/Orillusion/orillusion/issues/387) ([#394](https://github.com/Orillusion/orillusion/issues/394)) ([6271c37](https://github.com/Orillusion/orillusion/commit/6271c3748a1520cd444431200f7ec35111af049e))\r\n* **GlobalUniformGroup:** missing property for shadow camera ([1f90393](https://github.com/Orillusion/orillusion/commit/1f903935fc50be16763067b888c0c37cae860c5c))\r\n* **loaderFunctions:** onUrl on loadGltf ([65bda50](https://github.com/Orillusion/orillusion/commit/65bda50eac61694bb4e8354ee2f1744c876bd7ba))\r\n* object is disabled after removeChild  ([#381](https://github.com/Orillusion/orillusion/issues/381)) ([51ff3ee](https://github.com/Orillusion/orillusion/commit/51ff3ee84fb8aae46819d5c7db914b3dc873062f))\r\n* **objparser:** loadObj crash [#372](https://github.com/Orillusion/orillusion/issues/372) ([b3e9194](https://github.com/Orillusion/orillusion/commit/b3e9194630c9d0f3ab5a1cbc92dc7b760dd58f8b))\r\n* **picker:** missing normal in pickFire ([4e05c04](https://github.com/Orillusion/orillusion/commit/4e05c04dd22cccfb44e63c8dd2af859d4ff01c86))\r\n* **pick:** fix normal in pickInfo ([5197317](https://github.com/Orillusion/orillusion/commit/519731748ba046dd28891d99899e68e61a77c409))\r\n* Solve the issues mentioned in Issue367 ([#368](https://github.com/Orillusion/orillusion/issues/368)) ([7ab2f48](https://github.com/Orillusion/orillusion/commit/7ab2f489dfca66b6cb2cb84111097b430bb87c34))\r\n* **transform:** fix wrong localRotQuat ([8c5e2b3](https://github.com/Orillusion/orillusion/commit/8c5e2b3606378009045a460edf841e5d36142de8))\r\n\r\n\r\n### Features\r\n\r\n* **Animator:** Unified skeleton animation and morph animation to AnimatorComponent ([#405](https://github.com/Orillusion/orillusion/issues/405)) ([4cf51f3](https://github.com/Orillusion/orillusion/commit/4cf51f34937da6800f6cde2487defe12fe87ba8f))\r\n* **buffer:** return promise result ([590b213](https://github.com/Orillusion/orillusion/commit/590b213d41dd26ca86e1780376a7e04ece8a5166))\r\n* **GBuff:** compressed GBuff data. ([#412](https://github.com/Orillusion/orillusion/issues/412)) ([4649add](https://github.com/Orillusion/orillusion/commit/4649addc066cd53a5ee286940c1e254d64bde89e))\r\n* **orbit:** pan at xz plane ([52383f5](https://github.com/Orillusion/orillusion/commit/52383f5c60da7f6f2e3407de6bda355946f61fed))\r\n* **sample:** add camera path animation sample ([#385](https://github.com/Orillusion/orillusion/issues/385)) ([d447cd1](https://github.com/Orillusion/orillusion/commit/d447cd18e6e85763706e67c12e19d6a02e5e1dc4))\r\n* **sample:** add EatTheBox sample,add ShootTheBox sample ([#391](https://github.com/Orillusion/orillusion/issues/391)) ([e925d1f](https://github.com/Orillusion/orillusion/commit/e925d1f743ade799dac9d33c01ad829bcef386cb))",v0.8.1,ShuangLiu,,lslzl3000,MIT License,orillusion,Orillusion,14,webgpu,3d,graphics,html5,typescript,web3d,orillusion,javascript,wgsl,,,,,,,,,,,,/Orillusion/orillusion,14,287,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/Orama-Interactive/Pixelorama,https://github.com/Orama-Interactive/Pixelorama,0,,,0,0,0,0,0,0,1,1,0,0,0,"Unleash your creativity with Pixelorama, a powerful and accessible open-source pixel art multitool. Whether you want to create sprites, tiles, animations, or just express yourself in the language of pixel art, this software will realize your pixel-perfect dreams with a vast toolbox of features. Available on Windows, Linux, macOS and the Web!","<p align=""center"">\n    <h1 align = ""center"">Pixelorama - pixelate your dreams!</h1>\n</p>\n<p align=""center"">\n    Unleash your creativity with Pixelorama, a powerful and accessible open-source pixel art multitool. Whether you want to create sprites, tiles, animations, or just express yourself in the language of pixel art, this software will realize your pixel-perfect dreams with a vast toolbox of features.\n</p>\n<p align=""center"">\n    <a href=""https://github.com/Orama-Interactive/Pixelorama/actions"">\n        <img src=""https://github.com/Orama-Interactive/Pixelorama/workflows/dev-desktop-builds/badge.svg"" alt=""Build Passing"" />\n    </a>\n    <a href=""https://orama-interactive.github.io/Pixelorama/early_access/"">\n        <img src=""https://github.com/Orama-Interactive/Pixelorama/workflows/dev-web/badge.svg"" alt=""Build Passing"" />\n    </a>\n    <a href=""https://github.com/Orama-Interactive/Pixelorama"">\n        <img src=""https://img.shields.io/github/languages/code-size/Orama-Interactive/Pixelorama.svg"" alt=""Code Size"" />\n    </a>\n    <a href=""https://github.com/Orama-Interactive/Pixelorama"">\n        <img src=""https://img.shields.io/github/repo-size/Orama-Interactive/Pixelorama.svg"" alt=""Repository size"" />\n    </a>\n    <a href=""https://github.com/Orama-Interactive/Pixelorama/blob/master/LICENSE"">\n        <img src=""https://img.shields.io/github/license/Orama-Interactive/Pixelorama.svg"" alt=""License"" />\n    </a>\n</p>\n<p align=""center"">\n    <a href=""https://github.com/Orama-Interactive/Pixelorama/releases"">\n        <img src=""https://img.shields.io/github/downloads/Orama-Interactive/Pixelorama/total?color=lightgreen"" alt=""Downloads"" />\n    </a>\n    <a href=""https://discord.gg/GTMtr8s"">\n        <img src=""https://discord.com/api/guilds/645793202393186339/embed.png"" alt=""Discord Chat"" />\n    </a>\n    <a href=""https://crowdin.com/project/pixelorama"">\n        <img src=""https://badges.crowdin.net/pixelorama/localized.svg"" alt=""Crowdin Localized %"" />\n    </a>\n    <a href=""https://github.com/godotengine/awesome-godot"">\n        <img src=""https://awesome.re/mentioned-badge.svg"" alt=""Mentioned in Awesome Godot"" />\n    </a>\n</p>\n \n[![Pixelorama's UI](https://shared.akamai.steamstatic.com/store_item_assets/steam/apps/2779170/ss_54395040c25b243cb82a3bd68778e19e04b43ade.1920x1080.jpg?t=1719424898)](https://youtu.be/--ZcztkvWUQ)\n\nJoin our Discord community server where we can discuss about Pixelorama and all our other projects! https://discord.gg/GTMtr8s\n\nIf you like, consider helping us by sponsoring this project! It would enable us to focus more on Pixelorama, and make more projects in the future!\n\n[![Become a Patron!](https://c5.patreon.com/external/logo/become_a_patron_button.png)](https://patreon.com/OramaInteractive)\n\n## Download\nStable versions:\n- [Steam (available soon on Windows & Linux)](https://store.steampowered.com/app/2779170?utm_source=github)\n- [Itch.io (Windows, Linux, Mac & Web)](https://orama-interactive.itch.io/pixelorama)\n- [GitHub Releases (Windows, Linux & Mac)](https://github.com/Orama-Interactive/Pixelorama/releases)\n- [GitHub Pages (Web)](https://orama-interactive.github.io/Pixelorama/)\n- [Flathub (Linux)](https://flathub.org/apps/details/com.orama_interactive.Pixelorama)\n- [Snap Store (Linux)](https://snapcraft.io/pixelorama)\n\nYou can also find early access builds in the [GitHub Actions page](https://github.com/Orama-Interactive/Pixelorama/actions). There's also a [Web version available](https://orama-interactive.github.io/Pixelorama/early_access/).\nKeep in mind that these versions will have bugs and are unstable. Unless you're interested in testing the main branch of Pixelorama, it's recommended that you stick to a stable version.\n\n## Documentation\nYou can find online Documentation for Pixelorama here: https://orama-interactive.github.io/Pixelorama-Docs\n\nIt's still work in progress so there are some pages missing. If you want to contribute, you can do so in [Pixelorama-Docs' GitHub Repository](https://github.com/Orama-Interactive/Pixelorama-Docs).\n\n## Cloning Instructions\nPixelorama uses Godot 4.2, so you will need to have it in order to run the project. Older versions will not work.\nAs of right now, most of the code is written using GDScript, so the mono version of Godot is not required, but Pixelorama should also work with it.\n\n## Features:\n- A variety of different tools to help you create, with the ability to dynamically map each one on the left and the right mouse buttons with a single click.\n- Animation support with a timeline composed of layers and frames, with onion skinning, frame tags and the ability to draw while the animation is playing.\n- Pixel perfect mode for perfect pixel lines.\n- Pre-made palettes as well as many palette importing options.\n- Multiple image manipulation effects.\n- A powerful drawing canvas with guides, a rectangular and an isometric grid, and tile mode for easier seamless pattern creation.\n- Autosave support, with data recovery in case of a software crash.\n- Comprehensive user interface with many customizability options.\n- Export to PNG, as well as spritesheets, GIFs and animated PNGs.\n- Import spritesheets and multiple images as separate frames.\n- Various rotation and scaling algorithms tailored for pixel art, such as [cleanEdge](http://torcado.com/cleanEdge/), OmniScale and rotxel.\n- Fully open source with free updates, forever!\n- Multi-language localization support! See our [Crowdin page](https://crowdin.com/project/pixelorama) for more details.\n\n\n## Special thanks to\n- All [Godot](https://github.com/godotengine/godot) contributors! Without Godot, Pixelorama would not exist.\n- https://github.com/gilzoide/godot-dockable-container - the plugin Pixelorama's UI system uses for dockable containers.\n- https://github.com/Orama-Interactive/Keychain - the plugin Pixelorama's shortcut system uses for extensive customizability.\n- https://github.com/jegor377/godot-gdgifexporter - the gif exporter Pixelorama uses.\n- https://github.com/Pukkah/HTML5-File-Exchange-for-Godot - responsible for file exchange in Pixelorama's HTML5 (Web) version.\n- https://github.com/aBARICHELLO/godot-ci - for creating a Godot Docker image that lets us export Pixelorama automatically using GitHub Actions.\n- The entire Pixelorama community! Contributors, donors, translators, users, you all have a special place in our hearts! <3\n",6473,graphics,GDScript,4,GDScript,NSIS,Python,Shell,,,,,,,,,,,,,,,,,,,,,,,,,564,62,496,6,7,62,1591,60574,350,288,252,36,6b23c51e126bd0038332deb50568cb49a606c4b7,Fix some typos,2024-07-19T16:41:31Z,Emmanouil Papadeas,35376950+OverloadedOrama@users.noreply.github.com,OverloadedOrama,,"## [v0.11.4] - 2024-04-17\r\nThis update has been brought to you by the contributions of:\r\nFayez Akhtar ([@Variable-ind](https://github.com/Variable-ind))\r\n\r\nBuilt using Godot 3.5.2\r\n\r\n### Added\r\n- Exporting palettes to png files is now possible.\r\n- Progressive Web App (PWA) has been enabled for the Web version.\r\n\r\n### Changed\r\n- When quitting, multiple save confirmation dialogs now appear, each for every project that has changes.\r\n- Loop through frames when clicking on go to previous/next frame buttons on the timeline.\r\n- High res display is now enabled on macOS. [#936](https://github.com/Orama-Interactive/Pixelorama/issues/936)\r\n- Make cloned frames only select a cel if its corresponding original cel was selected as well. [#941](https://github.com/Orama-Interactive/Pixelorama/pull/941)\r\n- All of the timeline buttons now have the same size (24x24).\r\n\r\n### Fixed\r\n- Memory usage has been greatly optimized when doing operations such as drawing, image effects, selecting, transforming, etc, as the images stored in memory are now compressed. [#883](https://github.com/Orama-Interactive/Pixelorama/issues/883)\r\n- Fixed memory leak when applying image effects. [7235617db7c21837edc7ba7b95f2e7eeb1140691](https://github.com/Orama-Interactive/Pixelorama/commit/7235617db7c21837edc7ba7b95f2e7eeb1140691)\r\n- Fixed memory leak when previewing layouts in the Manage Layouts dialog. [b2f511c45be61cd26f01e134bf7a6a55109f46ad](https://github.com/Orama-Interactive/Pixelorama/commit/b2f511c45be61cd26f01e134bf7a6a55109f46ad)\r\n- Attempting to load an invalid pxo file no longer crashes the application. [3f6e1385e06cd7801fe12fbf90a9649557ea8f2e](https://github.com/Orama-Interactive/Pixelorama/commit/3f6e1385e06cd7801fe12fbf90a9649557ea8f2e)\r\n- Tool shortcuts can now work with <kbd>Control</kbd>. [#935](https://github.com/Orama-Interactive/Pixelorama/issues/935)\r\n- Optimize canvas drawing by only updating it when the image(s) have changed. [ac6a4db43d9296ebc03e639d8199dd3878a25d86](https://github.com/Orama-Interactive/Pixelorama/commit/ac6a4db43d9296ebc03e639d8199dd3878a25d86)\r\n- Fix bug where using shortcuts to switch between frames also moved the selection, causing deletions.\r\n- Pxo files can now be loaded from the Open menu option in the Web version. [3dcc51705a999145e53a8e6d4de217dc03b0f147](https://github.com/Orama-Interactive/Pixelorama/commit/3dcc51705a999145e53a8e6d4de217dc03b0f147)\r\n- The same frames are no longer being exported multiple times when ""Selected frames"" is selected, and multiple cels of the same frames are currently selected on the timeline. [#1001](https://github.com/Orama-Interactive/Pixelorama/issues/1001)\r\n- Fixed crash due to division by zero when locking two or three ValueSliders, and one of them has the value of 0 and the user attempts to change it. [3b8c63c4a6a3325707ef624942ea50834634e45c](https://github.com/Orama-Interactive/Pixelorama/commit/3b8c63c4a6a3325707ef624942ea50834634e45c)\r\n- Fixed exporting selected layers not including the non-selected frames. [324e21776de853e6ea24703d5724a491547371ab](https://github.com/Orama-Interactive/Pixelorama/commit/324e21776de853e6ea24703d5724a491547371ab)\r\n- Fix bug where images with width or height 1 are being completely cleared by image effects. [fcfc606861d247856db5473b702628ebd71df43f](https://github.com/Orama-Interactive/Pixelorama/commit/fcfc606861d247856db5473b702628ebd71df43f)\r\n- Made the color picker not select fully transparent pixels that are not black. [#999](https://github.com/Orama-Interactive/Pixelorama/issues/999)\r\n- Brushes are no longer being drawn outside the selection, if the selection is outside the canvas. [5f43a3e2829a7119d18d0762796222f20170f410](https://github.com/Orama-Interactive/Pixelorama/commit/5f43a3e2829a7119d18d0762796222f20170f410)\r\n- Bucket ""similar color"" and ""whole selection"" modes and image effects no longer affect pixels outside the selection area, if the selection is outside the canvas. [436406a527f0db67c5e2b58a90b43597b3168600](https://github.com/Orama-Interactive/Pixelorama/commit/436406a527f0db67c5e2b58a90b43597b3168600)\r\n- The ellipse tool no longer produces gaps with large sizes. [4f3a7a305a264e0d2fe86c201af76eca4b2fea0a](https://github.com/Orama-Interactive/Pixelorama/commit/4f3a7a305a264e0d2fe86c201af76eca4b2fea0a)\r\n- Fix ""visible layers"" option on the export dialog producing wrong results. [346d1f071a8c6b1defb1072d39aea9c642f1ef59](https://github.com/Orama-Interactive/Pixelorama/commit/346d1f071a8c6b1defb1072d39aea9c642f1ef59)\r\n- Random brushes now work again. [1317e40ffa5e9f01a9d214221bb5133db20a1de9](https://github.com/Orama-Interactive/Pixelorama/commit/1317e40ffa5e9f01a9d214221bb5133db20a1de9)\r\n- Fixed issue where the override.cfg file would be created at the wrong location, if Pixelorama is launched through a shortcut. [0c6566de761a683a0e8a781131024a1dedb9734f](https://github.com/Orama-Interactive/Pixelorama/commit/0c6566de761a683a0e8a781131024a1dedb9734f)\r\n- The gizmo in the rotation image effect dialog is now accurately following the mouse.\r\n- Fixed the size label not being updated on the Export dialog's spritesheet tab when the direction changes. [9a5eb9720d2328f914f8efc3b9aa605dadca99b0](https://github.com/Orama-Interactive/Pixelorama/commit/9a5eb9720d2328f914f8efc3b9aa605dadca99b0)\r\n- The ""Export dimensions"" label in the export dialog no longer shows fractions as the final image's size.",v0.11.4,,,github-actions[bot],MIT License,Pixelorama,Orama-Interactive,24,godot-engine,gamedev,art,pixel-art,godot,gdscript,pixel-art-maker,pixel-editor,paint,godotengine,graphics,sprites,sprite-animation,draw,spritesheet,animation,game-development,pixel,pixelart,pixelorama,/Orama-Interactive/Pixelorama,24,76,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/openwdl/wdl,https://github.com/openwdl/wdl,0,,0,0,1,1,0,0,0,1,0,0,0,0,Workflow Description Language - Specification and Implementations,"# Workflow Description Language (WDL)\n\nThe **Workflow Description Language (WDL)** is an open standard for describing data processing workflows with a human-readable and writeable syntax.\nWDL makes it straightforward to define analysis tasks, connect them together in workflows, and parallelize their execution.\nThe language strives to be accessible and understandable to all manner of users, including programmers, analysts, and operators of a production system.\nThe language enables common patterns, such as scatter-gather and conditional execution, to be expressed simply.\nWDL is designed for portability, and there are several [implementations](#execution-engines-and-platforms) to choose from that run in a variety of environments, including HPC systems and cloud platforms.\n\n## Versioning\n\nWDL versioning follows [semantic versioning](https://semver.org) conventions.\n\nThe WDL *language* has a two-number version (e.g., `1.2`).\nAn increase in the minor (second) version number (e.g., `1.1` to `1.2`) indicates the addition of, or non-breaking changes to, the language or standard library functions.\nAn increase in the major (first) version number (e.g., `1.0` to `2.0`) indicates that breaking changes have been made.\n\nThe WDL *specification* has a three-number version (e.g., `1.2.0`).\nThe specification version tracks the language version, but there may also be patch releases (indicated by a change to the patch, or third, version number) that include fixes for typos, additional examples, or non-breaking clarifications of ambiguous language.\n\n## Language Specifications\n\nThe WDL specification contains all relevant information for users and developers, including those wanting to implement an execution engine.\nThis GitHub project uses the branch for the current version of the specification as its primary branch, so you will always see the current version of the specification so long as you visit this project's [root URL](https://github.com/openwdl/wdl).\nUsers are strongly encouraged to use the current version of the specification unless absolutely necessary.\n\nThis branch is for version **1.2** of the [WDL language specification](https://github.com/openwdl/wdl/blob/wdl-1.2/SPEC.md).\nAll development of new *non-breaking* features should be done against this branch.\n\nPrevious versions of the spec can be found here:\n\n- [1.1](https://github.com/openwdl/wdl/blob/wdl-1.1/SPEC.md)\n- [1.0](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md)\n\nThere are a number of draft versions that correspond to initial efforts at creating WDL.\nWhile these are functional specifications, they should not be considered feature complete, and they contain many bugs and irregularities.\n\n- [draft-3](https://github.com/openwdl/wdl/blob/main/versions/draft-3/SPEC.md)\n- [draft-2](https://github.com/openwdl/wdl/blob/main/versions/draft-2/SPEC.md)\n- [draft-1](https://github.com/openwdl/wdl/blob/main/versions/draft-1/SPEC.md)\n\nThe next *major* version of the specification is [2.0](https://github.com/openwdl/wdl/blob/wdl-2.0/SPEC.md).\nAll development of new *breaking* features should be done against that branch.\n\n## Community and Support\n\nThe WDL community depends on your involvement to thrive.\nYou are encouraged to ask questions, help other users, and make contributions where you can.\nInteractions occur primarily on [GitHub](https://github.com/openwdl/wdl) and [Slack](https://join.slack.com/t/openwdl/shared_invite/zt-ctmj4mhf-cFBNxIiZYs6SY9HgM9UAVw).\nThe WDL community also has an official [blog](https://dev.to/openwdl) where announcements are made.\n\n### Asking a Question\n\n- Search in the [discussions](https://github.com/openwdl/wdl/discussions) to see if the question has been asked already; if not, start a new discussion.\n- Join our [Slack](https://join.slack.com/t/openwdl/shared_invite/zt-ctmj4mhf-cFBNxIiZYs6SY9HgM9UAVw) and ask in the `#support` channel.\n- Search the [Bioinformatics Stack Exchange](https://bioinformatics.stackexchange.com/search?q=wdl) for previously answered questions, or ask a new question. \n- Search the [Google Group](https://groups.google.com/a/openwdl.org/forum/#!forum/community) for previously answered questions. This group is largely inactive, so you're encoraged to ask new questions in one of the above places instead.\n\n### Bugs and New Features\n\n- Search for an existing [issue](https://github.com/openwdl/wdl/issues). If your issue has not already been reported, create a new one.\n- For feature reqeusts, you are encoraged to first start a discussion at one of the places listed above to get feedback from the community.\n- If you'd like to provide a fix/implementation for an issue, please read about [contributing](#contributing) before submitting a [pull request](https://github.com/openwdl/wdl/pulls).\n\n### Documentation\n\n- [wdl-docs](https://docs.openwdl.org/en/stable/)\n- [learn-wdl](https://github.com/openwdl/learn-wdl)\n- [WDL Resources](https://support.terra.bio/hc/en-us/sections/360007274612-WDL-Resources) provided by Terra (a product of the Broad Institute)\n\n## Published Workflows \n\nThe following are collections of open-source WDL workflows.\nThe WDL task or workflow you need may already be available in one of these repositories, or you may find a similar workflow and customize it to your needs.\n\n- [Dockstore](https://dockstore.org/search?entryType=workflows&descriptorType=WDL&searchMode=files)\n- [BioWDL](https://github.com/biowdl)\n- [Broad Institute WARP](https://broadinstitute.github.io/warp/docs/get-started/)\n- [GATK Workflows](https://github.com/gatk-workflows/)\n- [ENCODE](https://www.encodeproject.org/pipelines/)\n- [Terra](https://app.terra.bio) (requires registration)\n\n## Software and Tools\n\n### Execution Engines and Platforms\n\nWDL does not have an official implementation.\nThird parties are relied upon to provide installable software or hosted platforms that interpret and execute WDL workflows and tasks.\nAlthough WDL does not yet have an official compliance program or certification process, implementers are expected to design their tools according to the specification so as to maximize the portability of workflows across implementations.\nNonetheless, implementers may provide additional optional features specific.\nPlease see the documentation associated with each tool/platform for information on available execution options and support.\n\n| Implementation                                                                 | Requires Installation | Local Execution | HPC   | Cloud                 |\n| ------------------------------------------------------------------------------ | --------------------- | --------------- | ----- | --------------------- |\n| [AWS HealthOmics](https://docs.aws.amazon.com/omics/latest/dev/workflows.html) | Yes                   | No              | No    | AWS                   |\n| [Cromwell](https://github.com/broadinstitute/cromwell) *                       | Yes                   | Yes             | Many  | AWS Batch, Azure, GCP |\n| [dxCompiler](https://github.com/dnanexus/dxCompiler)                           | Yes                   | No              | No    | DNAnexus              |\n| [MiniWDL](https://github.com/chanzuckerberg/miniwdl)                           | Yes                   | Yes             | SLURM | AWS Batch             |\n| [Terra](https://terra.bio/)                                                    | No                    | No              | No    | Azure, GCP            |\n\n\* Also see [WDL Runner](https://github.com/broadinstitute/wdl-runner), a script for launch WDL workflows on GCP using Cromwell\n\n### Grammars, Parsers, and Language Support\n\n- The WDL [parsers repository](https://github.com/openwdl/wdl-parsers/) provides grammar definitions in various formats and generated parsers for various languages.\n- [MiniWDL](https://github.com/chanzuckerberg/miniwdl) provides python bindings for WDL as well as commandline tools for validation, linting, and generating workflow input templates.\n- [WOMTool](https://cromwell.readthedocs.io/en/stable/WOMtool/) is a standalone Java tool for WDL parsing, validating, linting, and diagramming.\n- [wdlTools](https://github.com/dnanexus/wdlTools) - provides 1) a parser Java/Scala library, based on the  [ANTLR4 grammars](https://github.com/openwdl/wdl-parsers) grammars, for WDL draft-2, 1.0, 1.1, and 2.0; and 2) command-line tools for sytanx checking, type-checking, linting, code formatting (including upgrading from older to newer WDL versions), generating documentation, and executing WDL tasks locally.\n- [sprocket](https://github.com/stjude-rust-labs/sprocket) a package manager and linter for WDL files.\n\n### IDE Support\n\n| IDE                | Tool                                                                                             |\n| ------------------ | ------------------------------------------------------------------------------------------------ |\n| Emacs              | [poly-wdl](https://github.com/jmonlong/poly-wdl)                                                 |\n| Emacs              | [wdl-mode](https://github.com/zhanxw/wdl-mode)                                                   |\n| JetBrains          | [Winstanly](https://plugins.jetbrains.com/plugin/8154-winstanley-wdl)                            |\n| Sublime            | [WDL Syntax Highlighter](https://github.com/broadinstitute/wdl-sublime-syntax-highlighter)       |\n| Vim                | [vim-wdl](https://github.com/broadinstitute/vim-wdl)                                             |\n| Visual Studio Code | [WDL Syntax Highlighter](https://marketplace.visualstudio.com/items?itemName=broadinstitute.wdl) |\n\n### Documentation\n\n- [wdl-aid](https://github.com/biowdl/wdl-aid) generates documentation for the inputs of WDL workflows, based on the parameter_meta information defined in the WDL file.	\n- [wdldoc](https://github.com/stjudecloud/wdldoc)\n\n### Testing\n\n- [wdl-tests](https://github.com/openwdl/wdl-tests) is a collection of test cases for WDL implementations. A specification is provided for writing new tests that are compatible with automated testing frameworks.\n- [Pytest-workflow](https://github.com/LUMC/pytest-workflow) is a implementation-agnostic workflow tester. It can be used with both Cromwell and MiniWDL. Uses pytest as the underlying test framework. Tests can be specified in either YAML format or python code.\n- [Pytest-wdl](https://github.com/EliLillyCo/pytest-wdl) is a plugin for the pytest unit testing framework that enables testing of WDL tasks and workflows. Tests can be specified in either YAML format or python code.\n\n### Packaging\n\n- [miniwdl zip](https://miniwdl.readthedocs.io/en/latest/zip.html) generates a ZIP file including a given WDL source code file and any other WDL files it imports. The ZIP file can be supplied directly to miniwdl run, which can extract it automatically.\n- [wdl-packager](https://github.com/biowdl/wdl-packager) packages a workflow and all of its imports into a zip file. The zip can be used as an imports zip package for cromwell. The utility can add non-WDL files (such as the license) to the zip package and provides options to package the zip in a binary reproducible way.\n\n## Contributing\n\nWDL only advances through community contributions.\nWhile participating in discussions and submitting issues are great ways to be involved, help is also needed to implement changes to the specification.\nFor more information on how you can contribute, please read the [Contributing](CONTRIBUTING.md) guide.\n\n### RFC Process\n\nSubmitted [pull requests](https://github.com/openwdl/wdl/pulls) are subject to the [RFC Process](RFC.md).\nPlease review and familiarize yourself with the process if you would like to see changes submitted to the specification.\n\n## Governance\n\nThe WDL specification is entirely community driven; however, it is overseen by a [Governance committee](GOVERNANCE.md).\nIf you are interested in being involved in WDL governance, please join the [Slack](https://join.slack.com/t/openwdl/shared_invite/zt-ctmj4mhf-cFBNxIiZYs6SY9HgM9UAVw) and post a message in the `#general` channel.\n",754,bioinformatics,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,352,100,248,4,20,54,0,4903,306,292,246,46,664adc323f536ab1fac0c41bf3fc85f4f15eb52b,fix example,2024-05-29T19:51:10Z,jdidion,github@didion.net,jdidion,WDL 1.2,"## What's New\r\n\r\n### Files and Directories\r\n+ Added `Directory` type. [PR 641](https://github.com/openwdl/wdl/pull/641)\r\n+ Added JSON extended file/directory input/output format. [PR 643] (https://github.com/openwdl/wdl/pull/643)\r\n+ Input files and directories should be treated as read-only. [PR 642](https://github.com/openwdl/wdl/pull/642)\r\n+ Local paths are always used when evaluating input/private/command expressions.\r\n+ Clarified the meaning of a remote parent folder for the purposes of localization.\r\n\r\n### Requirements and Hints\r\n+ The `requirements` and `hints` sections (#540 and #541) replace `runtime`.\r\n+ Workflow `hints` section (#543).\r\n+ `fpga` requirement and reserved hint for requesting FPGA resources.\r\n+ `disks` and `gpu` reserved hints for requesting specific resources.\r\n+ Implicit `task` variable that provides access the actual values of `requirements`, `meta`, and `parameter_meta` at runtime.\r\n\r\n### Struct Improvements\r\n+ Structs can now have `meta` and `parameter_meta` sections.\r\n+ Relaxed the requirements on coercing object/map to struct - extra keys are allowed and ignored. Note that this *may* constitute a breaking change if you rely on a task to fail when coercing an object/map with extra keys.\r\n+ Allow conversion between `Struct` types when certain criteria are met.\r\n\r\n### Standard Library Functions\r\n\r\n#### New\r\n+ `contains_key`: whether a Map or Object contain a specific member. [PR 603](https://github.com/openwdl/wdl/pull/603)\r\n+ `values`: get the values from a `Map`.\r\n+ `find`: search for a regular expression in a string.\r\n+ `matches`: whether a string match a regular expression.\r\n+ `chunk`: split an array into sub-arrays.\r\n+ `join_paths`: join two or more paths.\r\n+ `contains`: whether an array contains a specified value.\r\n\r\n#### Improved\r\n+ Generalized `size` function to take any compound value.\r\n+ Added optional `default` parameter to `select_first`.\r\n+ Generalized `length` function to also accept `Map`, `Object`, and `String` arguments.\r\n+ Added the `Array[String] keys(Struct|Object)` function variant for getting the member names for a struct or object.\r\n+ Added parameters to `read_tsv` that enable it to read field names from a header row or an `Array[String]` and return an `Array[Object]`. [PR 627](https://github.com/openwdl/wdl/pull/627)\r\n\r\n### Other\r\n+ Exponentiation operator (`**`).\r\n+ Multi-line strings. [PR 602](https://github.com/openwdl/wdl/pull/602).\r\n+ Clarify that accessing a non-existent member of an object, struct, or call is an error.\r\n+ Inputs with defaults are implicitly optional [PR 464](https://github.com/openwdl/wdl/pull/464) by @mlin\r\n+ `input:` is optional in call bodies. [PR 524](https://github.com/openwdl/wdl/pull/524) by @mlin.\r\n+ The concept of ""scoped types"", to support the use of object-like values within the `hints` section while still keeping the `Object` type as deprecated.\r\n\r\n## What's Deprecated\r\n+ `runtime` section (use `requirements` and `hints` instead).\r\n+ Specifying `allowNestedInputs` in the workflow `meta` section (put it in workflow `hints` instead).\r\n+ The previously allowed behavior implied by setting `allowNestedInputs: true` where required task/subworkflow inputs could be left unsatisfied. Now all inputs either need to have a default value or have their value specified in the call inputs. Only optional task/subworkflow inputs that are not explicitly set in the call inputs may have their value set at runtime if the `allow_nested_inputs` hint is `true`.\r\n\r\n## New Contributors\r\n* @wleepang made their first contribution in https://github.com/openwdl/wdl/pull/604\r\n* @markjschreiber made their first contribution in https://github.com/openwdl/wdl/pull/605\r\n\r\n**Full Changelog**: https://github.com/openwdl/wdl/compare/release-1.1.0...release-1.2.0",release-1.2.0,John Didion,,jdidion,"BSD 3-Clause ""New"" or ""Revised"" License",wdl,openwdl,4,wdl,workflow,cromwell,bioinformatics,reproducible-science,reproducibility,cloud,openwdl,,,,,,,,,,,,,/openwdl/wdl,10,88,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/opentk/opentk,https://github.com/opentk/opentk,0,,,0,0,0,0,0,0,1,1,0,0,0,"The Open Toolkit library is a fast, low-level C# wrapper for OpenGL, OpenAL & OpenCL. It also includes windowing, mouse, keyboard and joystick input and a robust and fast math library, giving you everything you need to write your own renderer or game engine. OpenTK can be used standalone or inside a GUI on Windows, Linux, Mac.","OpenTK\n======\n\nOpenTK is a large project. There are many components to work on, and we'd welcome almost any contribution. The community is friendly, welcoming and always ready to help you get your PRs merged!\n\nWe have a very active discord server, if you need help, want to help, or are just curious, come join us!\n\n[![Discord](https://discordapp.com/api/guilds/337627185248468993/widget.png)](https://discord.gg/6HqD48s)\n\nThe Open Toolkit library is a fast, low-level C# binding for OpenGL, OpenGL ES, OpenAL, and OpenCL. It runs on all major platforms and powers hundreds of apps, games and scientific research.\n\nUse OpenTK to add cross-platform 3d graphics, audio, compute and haptics to your C# application. Integrate it into your existing user interface or use it standalone without any external dependencies.\n\nOpenTK comes with simple and easy to follow tutorials for learning *modern* OpenGL. These are written by the community and represent all of the best practices to get you started.\n\n#### Learn how to use OpenTK here: https://opentk.net/learn/index.html\n\nA separate github repo with code is available here: https://github.com/opentk/LearnOpenTK\nThe code is similar to the tutorial but not an exact replica.\n\nProject website: https://opentk.net\n\nOfficial git repository: https://github.com/opentk/opentk\n\nBuild Status\n========\n\n| Platform       | Status         |\n| -------------- | -------------- |\n| Windows        | [![Build status](https://ci.appveyor.com/api/projects/status/c9b9754wa0v1p9rb?svg=true)](https://ci.appveyor.com/project/varon/opentk) |\n| Mono/Linux     | [![Build status](https://travis-ci.org/opentk/opentk.svg?branch=master)](https://travis-ci.org/opentk/opentk) |\n\n\nFeatures\n========\n\n- Create cutting-edge graphics with OpenGL 4.6 and OpenGL ES 3.0\n- Spice up your GUI with 3d acceleration\n- Improve your code flow with strong types and inline documentation\n- Windowing systems to help get you started\n- Input, and other game essentials.\n- Performant, highly optimized and reliable linear algebra library\n- Write once run everywhere\n\nOpenTK is available for Windows, Linux, Mac OS X, *BSD and SteamOS. It can be used standalone or integrated into a GUI (Windows.Forms, WPF, GTK+, Qt, VTK, ...)\n\nAn old, unsupported version of OpenTK 1.0 may be included in the [Xamarin](https://docs.microsoft.com/en-us/xamarin/graphics-games/game-development/) Android and iOS distribution. The Xamarin fork is not supported or maintained by this project.\n\nAdding support for mobile again is a future goal of the OpenTK project, but is somewhat frustrated by Apple's deprecation of OpenGL. Contributions to re-add support for these platforms are welcomed.\n\nInstructions\n============\n\nOpenTK is available as a [NuGet Package](http://www.nuget.org/packages/OpenTK/).\n\nAs of OpenTK 4.8 ""Visual C++ Redistributable 2015"" is not longer required to run OpenTK 4 on windows.\nFor earlier versions of OpenTK 4 you can install any version of `OpenTK.redist.glfw >= 3.3.8.35` to remove the ""Visual C++ Redistributable 2015"" requirement.\nSee https://github.com/opentk/glfw-redist for more details.\n\nNews\n===\n\n### 2022-09-24\n\nOpenTK 5.0.0 is still under active development and usage.\n\nPreview releases are running non-trivial applications successfully on .Net 6 across all platforms.\n\nPlease drop by the discord for the latest updates!\n\n### 2020-10-02\n\nOpenTK 4.0.0 is released with full availability.\n\nKey changes:\n * Full support for .Net Core 3.1\n * Brand new GLFW-based windowing system\n * Brand new GLFW-based input system\n * Removed all platform-specific backends (and fixed every xplat bug!)\n * Math library performance improvements and fixes\n * All new OpenAL bindings\n * All new OpenCL Bindings\n * Total restructure of all packages into a modular system with a number of packages. The OpenTK Nuget package is now a metapackage that will automatically download all of these for you.\n\nOpenTK 4.0.0 is entirely MIT licensed.\n\n\nWe're excited to see what you can build with this!\n\nhttps://www.nuget.org/packages/OpenTK\n\n\n### 2020-04-06\n\nOops! Forgot to update the news! OpenTK 4.0.0 PREVIEW is now available on Nuget.\n\nhttps://www.nuget.org/packages/OpenTK\n\n\n### 2020-04-06\n\nOpenTK 3.2.0 is available. \n\nThis adds bindings for the wgl_dx_interop extension and support for joysticks with > 64 buttons.\n\nhttps://www.nuget.org/packages/OpenTK/3.2.0\n\n\n### 2019-08-04\n\nOpenTK 3.1.0 is available.\n\nhttps://www.nuget.org/packages/OpenTK/3.1.0\n\nhttps://www.nuget.org/packages/OpenTK.GLControl/3.1.0\n\n\n\n### 2018-10-19\n\nWork is well underway on OpenTK 4.0, which targets .netstandard 2.0.\n\nWe would welcome any contributions!\n\n[Click here](https://github.com/opentk/opentk/issues/823) to view the tracking issue.\n\n### 2018-06-07\n\nOpenTK 3.0.1 is available.\n\nhttps://www.nuget.org/packages/OpenTK/3.0.1\n\nhttps://www.nuget.org/packages/OpenTK.GLControl/3.0.1\n\n### 2018-01-05\n\nOpenTK 3.0.0 is available.\n\nhttps://www.nuget.org/packages/OpenTK/3.0.0\n\nhttps://www.nuget.org/packages/OpenTK.GLControl/3.0.0\n\n\nBuilding from source\n============\n\nTo build OpenTK from source you just need to clone the git repo and open `OpenTK.sln` in Visual Studio 2019 or later.\n\n```\ngit clone https://github.com/opentk/opentk   # Download source code from git\ncd opentk                                    # Enter the source directory\n# open OpenTK.sln\n```\n\nAlternatively if you want to build using our buildscript you can use `build.cmd / build.sh` which require .net 6 to be installed. \n\n```\ngit clone https://github.com/opentk/opentk   # Download source code from git\ncd opentk                                    # Enter the source directory\n./build.cmd / ./build.sh                     # Run the build script for your platform\n```\n\nTo specify a specific target run:\n```\n./build.cmd -t <Target>\n```\n\nContributing\n============\n\nOpenTK uses and encourages [Early Pull Requests](https://medium.com/practical-blend/pull-request-first-f6bb667a9b6). Please don't wait until you're done to open a PR!\n\n1. Install [Git](https://git-scm.com/downloads) and the [.Net Core SDK](https://www.microsoft.com/net/download)\n1. [Fork OpenTK](https://github.com/opentk/opentk/fork)\n1. Create a branch on your fork.\n1. Add an empty commit to start your work off (and let you open a PR): `git commit --allow-empty -m ""start of [thing you're working on]""`\n1. Open a Pull request with `[WIP]` in the title. Do this **before** you actually start working.\n1. Make your commits in small, incremental steps with clear descriptions.\n1. Tag a maintainer when you're done and ask for a review!\n\n[Click here for good first issues.](https://github.com/opentk/opentk/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22)\n\n[Click here for everything we need help with.](https://github.com/opentk/opentk/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)\n\n\nRequirements\n============\n\n- Windows (7/8,10), Linux, Mac OS X, *BSD, SteamOS, Android or iOS\n- For graphics, OpenGL drivers or a suitable emulator, such as [ANGLE](https://github.com/opentk/opentk/tree/develop/Dependencies/Readme.txt)\n- For audio, OpenAL drivers or OpenAL Soft\n- To develop desktop applications: Visual Studio, Rider, or the command line tools.\n- To develop Android applications: Visual Studio and Xamarin\n- To develop iOS applications: Visual Studio, Xamarin and XCode\n\nDocumentation\n=============\n\nAPI Documentation is available on the [official website](https://opentk.net) or inline from favourite IDE.\n\nYou can also browse the full API on the official website\n\nAdditional information can be found in the [OpenTK Manual](http://web.archive.org/web/20150325224427/http://www.opentk.com/doc).\n\nTechnical documentation about the implementation of OpenTK can be found in the [Technical Wiki](https://github.com/opentk/opentk/wiki).\n\n\nNeed Help?\n==========\n\nCome chat with us on [Discord](https://discord.gg/6HqD48s).\n\nWe're happy to help with anything from learning OpenGL to advanced OpenTK questions.\n\n\nLicense\n=======\n\nThe Open Toolkit is distributed under the permissive MIT/X11 license and is absolutely free.\n",3150,graphics,C#,7,C#,F#,Batchfile,PowerShell,Tcl,Ruby,Python,,,,,,,,,,,,,,,,,,,,,,898,156,725,17,4,117,0,134328,629,820,780,40,38d4e664ef69633657ec2598779d99256163b6d1,Fix wglDXCloseDeviceNV argument name.,2024-07-05T14:11:16Z,Julius Häger,julius_hager@hotmail.com,NogginBops,04.08.2002,"FIX: Fixed issue where setting `NativeWindow.WindowState = WindowState.Normal` while fullscreen would not exit fullscreen. (@Th3Dilli)\r\nAPI: Added `NativeWindow.FramebufferSize`, `NativeWindow.OnFramebufferResize`, and `NativeWindow.FramebufferResize` to be able to properly get the framebuffer size of the window (useful on macos where framebuffer size is not equal to client size). (@NogginBops)\r\nAPI: Added `GameWindowSettings.Win32SuspendTimerOnDrag` propery that allows the internal timer to be suspended while the window is moved or resized. Without this dragging the window will cause an abnormally large time delta since the last frame. (@MV10)\r\nAPI: Made `NativeWindowSettings.Size` obsolete in favour of `NativeWindowSettings.ClientSize` (`NativeWindowSettings.Size` has always set the client size of the window, this namechange just makes this clear). (@MV10)\r\nAPI: Added ability to override OpenAL and OpenCL native library search path by setting `OpenALLibraryNameContainer.OverridePath` or `OpenCLLibraryNameContainer.OverridePath`. (@NogginBops)\r\nAPI: Added `NativeWindowSettings.AutoIconify` and `NativeWindow.AutoIconify` property to control if the window gets minimized if it loses focus while fullscreen. (@MV10)\r\nAPI: Added missing enums to `GetPName`. (@BoyBaykiller)\r\nFIX: Fixed an issue where creating a `NativeWindow` on wayland would freeze in the constructor. (@Th3Dilli)\r\nBREAKING: Re-implemented `Box2/3.Inflate` to work like `System.Drawing.Rectangle.Inflate`. The old behaviour can be found using `Box2/3.Extend`, as the previous obsoletion message stated. (@MV10, @NogginBops)\r\nAdd example of how to implement `IBindingsContext` in the documentation comments on `IBindingsContext`. (@utkumaden)\r\nInternal changes to make the build system simpler and easier to modify. (@NogginBops)\r\nInternal changes from `Math` to `MathF` where appropriate. (@MV10)",04.08.2002,Julius Häger,,NogginBops,Other,opentk,opentk,65,c-sharp,opengl,opengl-es,graphics-library,game-development,math-library,scientific-visualization,openal,opencl,graphics,game-engine,,,,,,,,,,/opentk/opentk,82,150,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/OpenSpace/OpenSpace,https://github.com/OpenSpace/OpenSpace,1,Visualization of the universe?,,1,1,1,0,0,0,0,0,0,0,0,"This is the official GitHub repository for OpenSpace: an open source astrovisualization project. For instructions on how to build and run OpenSpace, see the Getting Started Guides on the wiki page at http://docs.openspaceproject.com.","![OpenSpace Logo](/data/openspace-horiz-logo-crop.png)\n[OpenSpace](http://openspaceproject.com) is an open source, non-commercial, and freely available interactive data visualization software designed to visualize the entire known universe and portray our ongoing efforts to investigate the cosmos. Bringing the latest techniques from data visualization research to the general public, OpenSpace supports interactive presentation of dynamic data from observations, simulations, and space mission planning and operations. The software works on multiple operating systems (Windows, Linux, MacOS) with an extensible architecture capable of powering both personal computers and also high resolution tiled displays and planetarium domes. In addition, OpenSpace enables simultaneous connections across the globe creating opportunity for shared experiences among audiences worldwide. The target audience of the software reaches from the general public who wishes to explore our universe, enthusiasts interested in hacking the underlying components in OpenSpace to create unique experiences, informal science institutions wishing to create a low-cost, yet powerful exhibition piece, but also scientists desiring to visualize their datasets in a contextualized, powerful software.\n\n[![License](https://img.shields.io/badge/License-MIT-purple.svg?style=flat-square)](LICENSE)\n[![Download](https://img.shields.io/github/v/tag/OpenSpace/OpenSpace?label=Version&color=maroon&style=flat-square)](https://www.openspaceproject.com/installation)\n![Size](https://img.shields.io/github/repo-size/OpenSpace/OpenSpace?style=flat-square&color=red)\n\n[![System Paper](https://img.shields.io/badge/System%20Paper-10.1109%2FTVCG.2019.2934259-blue?style=flat-square)](https://doi.org/10.1109/TVCG.2019.2934259)\n[![GlobeBrowsing Paper](https://img.shields.io/badge/GlobeBrowsing%20Paper-https%3A%2F%2Fdoi.org%2F10.1109%2FTVCG.2017.2743958-blue?style=flat-square)](https://doi.org/10.1109/TVCG.2017.2743958)\n\n![Contributors](https://img.shields.io/github/contributors/OpenSpace/OpenSpace?style=flat-square)\n![Commits](https://img.shields.io/github/commit-activity/m/OpenSpace/OpenSpace?color=green&style=flat-square)\n\n![Image](https://docs.openspaceproject.com/en/latest/_static/images/collection.jpg)\n\n\n# Background\nOpenSpace started as a collaboration between Sweden's [Linköping University](https://immvis.github.io) (LiU) and the [American Museum of Natural History](https://www.amnh.org) (AMNH). Development of the software began several years ago through a close collaboration with NASA Goddard's [Community Coordinated Modeling Center](https://ccmc.gsfc.nasa.gov) (CCMC) to model space weather forecasting and continued with visualizations of NASA's New Horizons mission to Pluto and ESA's Rosetta mission to 67P/Churyumov-Gerasimenko. This promising set of preliminary work provided a foundation for continued funding from NASA, the Swedish eScience Research Centre, and the Knut and Alice Wallenberg foundation, which has extended the collaboration to include the University of Utah's [Scientific Computing and Imaging](https://www.sci.utah.edu) (SCI) Institute, [New York University](https://www.nyu.edu)'s Tandon School of Engineering, multiple informal science institutions across the world, and multiple, international vendors.\n\n![Image](https://docs.openspaceproject.com/en/latest/_static/images/presentation.jpg)\n\n\n# Features\nSome of the high-level features supported in OpenSpace are:\n  - AMNH's Digital Universe catalog of extrasolar datasets (stars, galaxies, quasars, ...)\n  - High-resolution planetary images for major objects in the solar system (Earth, Moon, Mars, Venus, ...)\n  - Animated 3D models representing space missions (ISS, New Horizons, JWST, ...)\n  - Support for custom profiles with arbitrary user-defined content\n  - Ability to drive any type of display environment (flat screen, multi-projector, planetariums, ...)\n  - Lua and JavaScript interface into the engine allowing highly customized controls\n  - Native support to export an interactive sessions as individual frames for video export\n  - much much more (see our [Changelog](http://wiki.openspaceproject.com/docs/general/releases))\n\nOpenSpace requires at least support for [OpenGL](https://www.opengl.org/) version 3.3, some custom components require at least version 4.2.\n\n![Image](https://docs.openspaceproject.com/en/latest/_static/images/display-systems.jpg)\n\n\n# Getting Started\nThis repository contains the source code and example profiles for OpenSpace, but does not contain any data. To build and install the application, please check out the [Documentation](https://docs.openspaceproject.com). Here, you will find the build instructions for all operating systems. Please note that the Apple Silicon series of chips do not support OpenGL natively and Metal 2 does not support `double` precision accuracy (see [here](https://developer.apple.com/metal/Metal-Shading-Language-Specification.pdf) Section 2.1), therefore only the Intel processors for MacOS are supported and maintained.\n\nRequirements for compiling are:\n  - CMake version 3.25 or above\n  - C++ compiler supporting C++20/C++23 (MSVC 19.39, GCC13, Clang17, AppleClang 15.0.0)\n  - [Boost](http://www.boost.org/)\n  - [Qt](http://www.qt.io/download)\n\n\n## :bulb: Asking Questions\nFeel free to create issues for missing features, bug reports, or compile problems or contact us via [email](mailto:support@openspaceproject.com?subject=OpenSpace:). Regarding any issues, you are very welcome on our [Slack support channel](https://openspacesupport.slack.com) to which you can freely [sign-up](https://join.slack.com/t/openspacesupport/shared_invite/zt-24uhn3wvo-gCGHgjg2m9tHzKUEb_FyMQ).\n\n## :heart: Contributing\nAny contributions to the software are very welcome and can take a multitude of forms, from reporting a bug, fixing bugs, creating new content, writing new features, and even creating and sharing images and videos you have made with the software. Please feel free to share anything you want in the #sharespace channel on the Slack.\n\n![Image](https://docs.openspaceproject.com/en/latest/_static/images/himalaya-nkpg-dome.jpg)\n\n\n# License\nThe contents of this repository provided under an [MIT license](https://github.com/OpenSpace/OpenSpace/blob/master/LICENSE.md).\n\n\n# Support\nOpenSpace is supported by the following institutions:\n\n![Image](https://docs.openspaceproject.com/en/latest/_static/logos/sponsors.png)\n",746,astronomy,C++,10,C++,CMake,Lua,GLSL,CSS,Python,JavaScript,Objective-C++,Handlebars,Batchfile,,,,,,,,,,,,,,,,,,,876,50,821,5,86,62,0,445859,120,2464,1808,656,04fc890879b54975cec7893307c7abb5801c7830,Update Ghoul (Temporary hack to fix osmodel reader),2024-07-18T15:33:05Z,Malin E,malin.ejdbo@gmail.com,Roxeena,0.20.1,"You can download pre-built binaries from [here](https://data.openspaceproject.com/release/0.20.1). Or find more information on our [homepage](https://openspaceproject.com/).\r\n\r\nYou can find a list of breaking changes against 0.19 [here](https://docs.openspaceproject.com/en/latest/about/releases/changelogs/0.20.0/index.html).\r\n\r\n# Features\r\n  - 2024 Digital Universe Data Update\r\n  - Add new TileProvider to select tile providers based on the date and apply to VIIRS Joint Polar Satellite System (#3350)\r\n  - Provide better error messages when failing to load an asset due to verification failures\r\n  - Add support for model vertex colors (#3346)\r\n  - Reduce the number of capture threads used by SGCT to optimize image sequence framerate\r\n  - Add server name to parallelpeer authentication, used as unique identifier on wormhole server\r\n  - Add the ability to scale the debug statistics graphs\r\n  - Move the statistics and frame info rendering from the RenderEngine into the debugging module ( #1248)\r\n  - Update Ghoul to get more information on when WMI queries fail (#3330)\r\n\r\n## UI\r\n  - Add possiblity to sort SGNs in GUI based on numerical value\r\n  - Small Node Popover menu refactor to provider easier access to focus controls\r\n\r\n# Content\r\n## New Assets\r\n  - Add an example asset to show the current in-game time in a screenspace object (#3312)\r\n  - Add new advanced example assets for the point cloud rendering\r\n\r\n## Updates to existing Assets/Profiles\r\n  - Add Down and Up keybinds to set the time to realtime and ""now"" respectively (#3275)\r\n  - Fix spelling mistake in Haumea model\r\n  - Fix a warning from hdf.asset due to a missing dataset parameter (#3343)\r\n  - Removed the version numbers from asset files as they were unused and inconsistent\r\n  - Remove the large data files from the URLSynchronization example files to make the file loadable\r\n  - Use the correct way to scale the Eiffel tower educational asset\r\n\r\n## Content creation\r\n  - Add the ability to specify DashboardItems for ScreenSpaceDashboards in assets.\r\n  - Make the 'Layer's specification in RenderableGlobe optional\r\n  - Improve error messages and export DashboardItem documentation\r\n  - Adds a task to generate a raw volume (used in for example RenderableTimeVaryingVolume) from a CSV file.\r\n  - The default for actions is now to have them not local (#3194)\r\n\r\n### Lua\r\n  - Add Lua function to calculate the number of seconds between dates and use it in assets (#3332)\r\n  - Add a new Lua function to create debug axes for the current focus node\r\n\r\n# Bug Fixes\r\n  - Fix a multithreaded access to SPICE that could cause a crash on startup (#3345)\r\n  - Fix a bug where the keyboard shortcuts would no longer be displayed\r\n  - Correctly consume char-based keyboard callbacks in the CEF module (#3290)\r\n  - Prevent crash when starting without any enabled audio devices (#3329)\r\n  - Fix bug with resizing the WorldWideTelescope window\r\n  - Fix an issue with the point cloud rendering texture not being rendered when the texture file path is updated during runtime\r\n  - Fix a faulty example GUI path in the point cloud example files\r\n  - Generate the framebuffer for screenspace rendering without mipmapping as it handles transparency poorly\r\n  - Fix parsing TLE files with CRLF line endings on unix (#3326)\r\n  - A non-main dashboard is now correctly displayed when adding it as a propertyowner to a ScreenSpaceDashboard\r\n  - Fix an issue where the scale for ScreenSpaceDashboards was always enforced to be 1\r\n",releases/v0.20.1,Alexander Bock,,alexanderbock,Other,OpenSpace,OpenSpace,22,visualization,astronomy,data-visualization,planetarium,space,science,universe,,,,,,,,,,,,,,/OpenSpace/OpenSpace,42,59,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/openslide/openslide,https://github.com/openslide/openslide,0,,,0,1,0,0,0,0,1,0,0,0,0,C library for reading virtual slide images,"# OpenSlide\n\nOpenSlide is a C library for reading whole slide image files (also known as\nvirtual slides).  It provides a consistent and simple API for reading files\nfrom multiple vendors.\n\n\n## Features\n\nOpenSlide can read brightfield whole slide images in [several formats][]:\n\n* [Aperio][] (`.svs`, `.tif`)\n* [DICOM][] (`.dcm`)\n* [Hamamatsu][] (`.ndpi`, `.vms`, `.vmu`)\n* [Leica][] (`.scn`)\n* [MIRAX][] (`.mrxs`)\n* [Philips][] (`.tiff`)\n* [Sakura][] (`.svslide`)\n* [Trestle][] (`.tif`)\n* [Ventana][] (`.bif`, `.tif`)\n* [Zeiss][] (`.czi`)\n* [Generic tiled TIFF][] (`.tif`)\n\nOpenSlide can also provide access to ICC profiles, textual metadata, and\nassociated images such as a slide label and thumbnail.\n\n[several formats]: https://openslide.org/formats/\n[Aperio]: https://openslide.org/formats/aperio/\n[DICOM]: https://openslide.org/formats/dicom/\n[Hamamatsu]: https://openslide.org/formats/hamamatsu/\n[Leica]: https://openslide.org/formats/leica/\n[MIRAX]: https://openslide.org/formats/mirax/\n[Philips]: https://openslide.org/formats/philips/\n[Sakura]: https://openslide.org/formats/sakura/\n[Trestle]: https://openslide.org/formats/trestle/\n[Ventana]: https://openslide.org/formats/ventana/\n[Zeiss]: https://openslide.org/formats/zeiss/\n[Generic tiled TIFF]: https://openslide.org/formats/generic-tiff/\n\n\n## Documentation\n\nThe [API reference][API] is available on the web, and is also included as\n`doc/html/openslide_8h.html` in the source tarball.  [Additional\ndocumentation][docs] is available on the [OpenSlide website][website].\n\n[API]: https://openslide.org/api/openslide_8h.html\n[docs]: https://openslide.org/#documentation\n[website]: https://openslide.org/\n\n\n## License\n\nOpenSlide is released under the terms of the [GNU Lesser General Public\nLicense, version 2.1](https://openslide.org/license/).\n\nOpenSlide is distributed in the hope that it will be useful, but WITHOUT ANY\nWARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\nFOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for\nmore details.\n\n\n## Compiling\n\nTo build OpenSlide, you will need:\n\n- Meson\n- cairo ≥ 1.2\n- GDK-PixBuf\n- glib ≥ 2.56\n- libdicom ≥ 1.0 (automatically built if missing)\n- libjpeg\n- libpng\n- libtiff ≥ 4.0\n- libxml2\n- OpenJPEG ≥ 2.1\n- SQLite ≥ 3.14\n- zlib\n- Zstandard\n\nThen:\n\n```\nmeson setup builddir\nmeson compile -C builddir\nmeson install -C builddir\n```\n\n\n## Acknowledgements\n\nOpenSlide has been developed by Carnegie Mellon University and other\ncontributors.\n\nOpenSlide has been supported by the National Institutes of Health and\nthe Clinical and Translational Science Institute at the University of\nPittsburgh.\n\nDevelopment of DICOM and ICC functionality was supported by NCI Imaging\nData Commons and has been funded in whole or in part with Federal funds\nfrom the National Cancer Institute, National Institutes of Health, under\nTask Order No. HHSN26110071 under Contract No. HHSN261201500003l.\n",397,whole-slide-imaging,C,6,C,Shell,Python,MATLAB,Roff,Meson,,,,,,,,,,,,,,,,,,,,,,,266,48,198,20,1,11,0,4146,216,356,268,88,8fbf30e1c41687e3feddd3a18f0db87a557f51a6,Merge pull request #620 from bgilbert/workflow-sequence,2024-06-11T01:41:35Z,Benjamin Gilbert,bgilbert@cs.cmu.edu,bgilbert,OpenSlide 4.0.0,"[OpenSlide](https://openslide.org/) is a C library that provides a simple way to read whole-slide images, also known as virtual slides. OpenSlide 4.0.0, the first new OpenSlide release in 8 years, is a major release that adds support for DICOM WSI slides, ICC color profiles, tile cache customization, adds the `slidetool` command-line utility, removes deprecated APIs, and improves format compatibility.\r\n\r\nOpenSlide is released under the terms of the [GNU Lesser General Public License, version 2.1](https://openslide.org/license/).\r\n\r\n## DICOM WSI support\r\n\r\nOpenSlide can now read [slide images in DICOM format](https://openslide.org/formats/dicom/) using [libdicom](https://github.com/ImagingDataCommons/libdicom) (see [demo](https://openslide.org/demo/)). It supports uncompressed, JPEG, and JPEG 2000 images, full and sparse tiling, associated images, and DICOM metadata properties.\r\n\r\n![DICOM slide in OpenSlide Python example viewer](https://github.com/openslide/openslide/assets/361374/787f0107-e31a-4a71-ac8e-bcffea6301aa)\r\n\r\nOpenSlide aims to support most popular DICOM WSI variants. If you have a sample file which does not work well, please [open an issue](https://github.com/openslide/openslide/issues/new/choose).\r\n\r\n## ICC color profile support\r\n\r\nSeveral scanner vendors now include ICC color profiles in their slide formats. These profiles can be used to improve the color rendering of slide images.  OpenSlide now has APIs to read these profiles:\r\n\r\n\r\n> [`openslide_get_icc_profile_size(osr)`](https://openslide.org/api/openslide_8h.html#a323af3836966ea1973d09b1a8ccf771f)\r\n[`openslide_read_icc_profile(osr, dest)`](https://openslide.org/api/openslide_8h.html#acaf491a585651d634735d0a61cf7fae2)\r\n[`openslide_get_associated_image_icc_profile_size(osr, name)`](https://openslide.org/api/openslide_8h.html#a4a31de687aefba75055769fba6e80b35)\r\n[`openslide_read_associated_image_icc_profile(osr, name, dest)`](https://openslide.org/api/openslide_8h.html#a26322f1638fb8c4c683a08ca1b5d3d4a)\r\n\r\n`openslide-write-png` and `slidetool` now attach ICC profiles to PNG images they write.  ICC profiles are currently supported by the Aperio, DICOM, Ventana, and generic TIFF vendor drivers.\r\n\r\n## Tile cache customization\r\n\r\nOpenSlide has new APIs to share tile caches between OpenSlide objects and to configure their capacity:\r\n\r\n>[`openslide_cache_create(capacity)`](https://openslide.org/api/openslide_8h.html#a611c06edc3173aa7f5fa60544a5b3a18)\r\n[`openslide_set_cache(osr, cache)`](https://openslide.org/api/openslide_8h.html#a15904ac24a0beb733f43667aab17b2a5)\r\n[`openslide_cache_release(cache)`](https://openslide.org/api/openslide_8h.html#a870e374a309aaf96cbd36ba49c55ec8a)\r\n\r\nBy default, OpenSlide objects continue to use non-shared 32 MiB caches.\r\n\r\n## New `slidetool` utility \r\n\r\nOpenSlide 4.0 introduces the `slidetool` command-line program, with subcommands providing access to all OpenSlide functionality.  The `openslide-quickhash1sum`, `openslide-show-properties`, and `openslide-write-png` commands are still provided but will not receive additional functionality in the future.\r\n\r\n## Breaking API changes\r\n\r\nOpenSlide 4.0 introduces several API changes, all of which are unlikely to affect applications:\r\n\r\n- Removed nine API functions which have been deprecated since 2014\r\n- [`openslide_read_associated_image()`](https://openslide.org/api/openslide_8h.html#a71ace07f87c9aeab328d3a6efcd2f983) now clears the `dest` buffer on error, and no longer accepts a NULL `dest`\r\n- The GLib log domain changed from `Openslide` to `OpenSlide`\r\n\r\nThe [soname](https://en.wikipedia.org/wiki/Soname) has been changed, which renames the library file to `libopenslide.so.1` on Linux, `libopenslide.1.dylib` on macOS, and `libopenslide-1.dll` on Windows.  Dependent packages will need to be rebuilt, and any code that dynamically loads OpenSlide at runtime will need to use the new filenames.\r\n\r\n## Building from source\r\n\r\nThe Autotools-based build system has been replaced with [Meson](https://mesonbuild.com/).  OpenSlide 4.0 can be built with these commands:\r\n\r\n```\r\nmeson setup builddir\r\nmeson compile -C builddir\r\nmeson test -C builddir          # optional\r\nsudo meson install -C builddir\r\n```\r\n\r\n## Getting binaries\r\n\r\nOpenSlide [Windows build](https://openslide.org/download/#windows-binaries) 20231011 now provides a single DLL containing OpenSlide and all its dependencies.  In addition, the DLL now uses the [Universal C Runtime](https://learn.microsoft.com/en-us/cpp/windows/universal-crt-deployment) (UCRT) rather than MSVCRT.\r\n\r\nOpenSlide now provides a [Fedora Copr](https://copr.fedorainfracloud.org/coprs/g/openslide/openslide/) and an [Ubuntu PPA](https://launchpad.net/~openslide/+archive/ubuntu/openslide), enabling users of Fedora, Ubuntu, and RHEL-compatible enterprise Linux to easily install the latest OpenSlide and OpenSlide Python releases before they reach the official repositories.  See the [download page](https://openslide.org/download/#distribution-packages) for instructions on enabling these repos.\r\n\r\n## How to help\r\n\r\nIf you have a slide scanner that can produce [files we don't have](https://github.com/openslide/openslide/wiki/DesiredTestData), or ones OpenSlide can't read, please consider [contributing a sample](https://openslide.org/submit/).  If you grant us permission to redistribute your sample under the [Creative Commons Zero](https://creativecommons.org/public-domain/cc0/) license, we can use it in automated tests and share it with other developers working on open source WSI support!\r\n\r\n## Acknowledgements\r\n\r\nDevelopment of DICOM and ICC functionality was supported by NCI Imaging Data Commons and has been funded in whole or in part with Federal funds from the National Cancer Institute, National Institutes of Health, under Task Order No. HHSN26110071 under Contract No. HHSN261201500003l.\r\n\r\n## Full changelog\r\n\r\n### Breaking changes\r\n\r\n* Update soname to `libopenslide.so.1`\r\n* Remove all deprecated functions\r\n* Clear `openslide_read_associated_image()` output buffer on error\r\n* Remove undocumented NULL `dest` support in `openslide_read_associated_image()`\r\n* Change GLib log domain to `OpenSlide`\r\n* Convert build system to Meson (thanks, Jan Harkes)\r\n\r\n### New features\r\n\r\n* New format: DICOM WSI (thanks, John Cupitt and Jim O'Donnell)\r\n* Add APIs to read ICC color profiles (thanks, John)\r\n* Add APIs to configure tile cache size and share caches between slides\r\n* Add properties for associated image metadata\r\n* generic-tiff: Set MPP properties if available\r\n* philips: Set objective power property if available\r\n* Add `slidetool` command-line tool which supports all OpenSlide features\r\n* Combine all command-line tools into the same binary\r\n* Add ICC profile to PNG images written by command-line tools\r\n* Add self-test that doesn't require sample data (run with `meson test`)\r\n\r\n### Changes\r\n\r\n* Require libtiff ≥ 4, OpenJPEG ≥ 2.1, GLib ≥ 2.56, SQLite ≥ 3.14\r\n* Require libdicom ≥ 1.0, with build-time fallback for now\r\n* Remove support for including `openslide.h` in Visual Studio \< 2013\r\n* Fail `openslide_open()` with a broken pixman 0.38.x\r\n* Avoid extra buffer copy in `openslide_read_region()`\r\n* Replace `goto`-based cleanup with `g_autoptr`\r\n* Stop using deprecated GLib slice allocator\r\n* Use internal wrappers for file I/O\r\n* Documentation improvements\r\n\r\n### Bug fixes\r\n\r\n* Use UTF-8 filenames on Windows\r\n* Improve `openslide-write-png` performance for very large regions\r\n* Fix assertions on JPEG decode errors when compiled with Clang\r\n* Portability fixes (thanks, Billy Robert O'Neal III and Kleis Auke Wolthuizen)\r\n* aperio: Set objective power property even if floating point\r\n* hamamatsu: Fix `Restart marker not found` on VMS slides with multiple Z-layers\r\n* hamamatsu: Fix integer overflow in VMS parsing (thanks, Adam Goode)\r\n* mirax: Fix `Expected 1 value` error\r\n* philips: Don't cache missing tiles\r\n* sakura: Fix memory leak reading missing tile",v4.0.0,Benjamin Gilbert,,bgilbert,GNU Lesser General Public License v2.1,openslide,openslide,36,pathology,whole-slide-imaging,c,,,,,,,,,,,,,,,,,,/openslide/openslide,36,36,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/openrndr/openrndr,https://github.com/openrndr/openrndr,0,,,0,0,0,0,0,0,1,1,0,0,0,"OPENRNDR. A Kotlin/JVM library for creative coding, real-time and interactive graphics","# OPENRNDR\n\n[![Download](https://maven-badges.herokuapp.com/maven-central/org.openrndr/openrndr-application/badge.svg)](https://mvnrepository.com/artifact/org.openrndr/openrndr-core)\n![Build status](https://github.com/openrndr/openrndr/actions/workflows/tests.yml/badge.svg)\n\nA Kotlin/JVM and Kotlin/JS library for creative coding, real-time and interactive graphics. Can currently be used on Windows, macOS and Linux/x64 to create standalone graphical applications.\n\nBasics and use are further explained in the [OPENRNDR guide](https://guide.openrndr.org).\n\n## Repository structure\n\n| module                                       | description                                      |\n|----------------------------------------------|--------------------------------------------------|\n| [openrndr-animatable](openrndr-animatable)   | Tooling for interactive animations               |\n| [openrndr-application](openrndr-application) | Application and Program classes                  |\n| [openrndr-binpack](openrndr-binpack)         | Binpacking algorithm used for texture atlasses   |\n| [openrndr-color](openrndr-color)             | Color spaces                                     |\n| [openrndr-dds](openrndr-dds)                 | DirectDraw Surface file (.dds) loader            |\n| [openrndr-demos](openrndr-demos)             | A collection of small in-repository demos        |\n| [openrndr-draw](openrndr-draw)               | Drawing primitives                               |\n| [openrndr-event](openrndr-event)             | Event classes                                    |\n| [openrndr-extensions](openrndr-extensions)   | Built-in OPENRNDR extensions                     |\n| [openrndr-filter](openrndr-filter)           | Built-in filters                                 |\n| [openrndr-js](openrndr-js)                   | Kotlin/JS specific modules                       |\n| [openrndr-jvm](openrndr-jvm)                 | Kotlin/JVM specific modules                      |\n| [openrndr-math](openrndr-math)               | Math functions and classes                       |\n| [openrndr-nullgl](openrndr-nullgl)           | Mock graphics back-end                           |\n| [openrndr-shape](openrndr-shape)             | Classes and functions for working with 2D shapes |\n| [openrndr-svg](openrndr-svg)                 | Loading and saving SVG                           |\n| [openrndr-utils](openrndr-utils)             | Assorted utilities                               |\n\n## Using OPENRNDR\n\nYou are advised to use the [OPENRNDR template](https://github.com/openrndr/openrndr-template) which provides a quick start to using the library.\n\nOPENRNDR's Javascript/WebGL is still experimental and under development. However, if you feel like trying it you should use the \n[OPENRNDR JS template](https://github.com/openrndr/openrndr-js-template).\n\n\n## Building OPENRNDR\n\nAfter cloning the repository, make sure you have Java 11 or newer installed and run the following command:\n\n```sh\n./gradlew build\n```\n\nThis should start the build process, which will take some time to complete.\n\nNote that OPENRNDR does not depend on anything that is not on Maven Central, builds should be easy and predictable.\n\n## Installing OPENRNDR as Maven artifacts\n\nIn order to use the OPENRNDR build from your applications one has to install OPENRNDR's Maven artifacts in the local Maven repository.\n\n```sh\n./gradlew publishToMavenLocal snapshot\n```\n\nA more detailed walk-through of building, publishing to Maven local and contributing can be found in the [wiki](https://github.com/openrndr/openrndr/wiki/Building-OPENRNDR-and-ORX).\n\n## Community\n\nVisit the [OPENRNDR website](https://openrndr.org) for the latest news on OPENRNDR, showcases and events \n\nJoin us on the [OPENRNDR forum](https://openrndr.discourse.group) for questions, tutorials and showcases.\n\nReach us more directly on the [OPENRNDR Slack](https://join.slack.com/t/openrndr/shared_invite/zt-avkbk0as-AZEsN7kb4UNIpfmYfbAemw).\n",852,graphics,Kotlin,4,Kotlin,GLSL,CSS,HTML,,,,,,,,,,,,,,,,,,,,,,,,,241,24,213,4,3,25,832,11897,71,155,133,22,38f54525b726f096a6bfb4391602c5e644ac1509,"Upgrade to kotlin-logging 7.0.0, kotlinx-serialization 1.7.1, kotest …",2024-07-17T09:37:29Z,Edwin Jakobs,edwin@rndr.studio,edwinRNDR,OPENRNDR 0.4.3,## Changes since 0.4.2\r\n* [Upgrade kluent to 1.73](https://github.com/openrndr/openrndr/commit/b341b5c387f2e4b9ddc5288c6c9884030a684f65)\r\n* [Upgrade kotlinx.coroutines to 1.7.0](https://github.com/openrndr/openrndr/commit/4a0f5bf4bf423da3d6f7c1f2646e0c487b6d0756)\r\n * [Upgrade to jsoup 1.16.1](https://github.com/openrndr/openrndr/commit/8078c30ccfa87dc4075a2969f3dc647d02e1525d)\r\n * [Upgrade to Kotlin 1.8.21](https://github.com/openrndr/openrndr/commit/9ee1227490f893e060bd2c8303255d8398572243)\r\n* [Ugrade Gradle to 8.1.1](https://github.com/openrndr/openrndr/commit/2d60668ea2c86229bb88f7a9be7824611d46e847) (https://github.com/openrndr/openrndr/pull/368)\r\n* [Upgrade to Gradle 8.1](https://github.com/openrndr/openrndr/commit/d8c3b744010c914940ef63d3c62f1a790ff866ee)\r\n* [Upgrade Kotest to 5.6.1](https://github.com/openrndr/openrndr/commit/8b0cffd06e192659b789f1c1d8b7c4b92b30575c)\r\n\r\n### openrndr-application \r\n * [Add MouseTracker](https://github.com/openrndr/openrndr/commit/6e99c39c8acee1e66b4c40398bbd6d043ddada69) (https://github.com/openrndr/openrndr/pull/367)\r\n * [Add Clock interface](https://github.com/openrndr/openrndr/commit/32face9aafb29107a02cdd4a6ed4bd74b1ce3084)\r\n\r\n### openrndr-draw\r\n * [Add support for array of struct parameters](https://github.com/openrndr/openrndr/commit/cf2cc0a93b9882660dc4d41f32ae56e6236064a8)\r\n * [Implement struct support for shade styles](https://github.com/openrndr/openrndr/commit/774a1cb451bb0f0b7c3b30c6d459506f709eb7fd)\r\n\r\n\r\n### openrndr-gl3\r\n * [Expose main constants before including preambles](https://github.com/openrndr/openrndr/commit/941b1ba5b19e4e121f1373f215b585c34d8826c1)\r\n * [Dereference shadow on vertex buffer destruction](https://github.com/openrndr/openrndr/commit/fea2b5326600b08a2e6258f0a95e5c8ea8402b43)\r\n * [Fix DDS image reader.](https://github.com/openrndr/openrndr/commit/0ee5625dfa21fbe015d8f2bfc958dfd337908b64) [Resolves](https://github.com/openrndr/openrndr/commit/0ee5625dfa21fbe015d8f2bfc958dfd337908b64) https://github.com/openrndr/orx/issues/315\r\n * [Add ArrayTextureImageBinding support to ComputeShader](https://github.com/openrndr/openrndr/commit/863c33cfce639d25eedd4aca768de16724bc3af3) (https://github.com/openrndr/openrndr/pull/369)\r\n### openrndr-utils\r\n * [Implement textFromURL](https://github.com/openrndr/openrndr/commit/3b7ea9cd11adf17642c7c5abc3a9d888248d0676)\r\n * [Enable debug context in gl3 tests](https://github.com/openrndr/openrndr/commit/2cf9e00434481e577752b03634e761ae7efc5be5)\r\n\r\n\r\n,v0.4.3,Edwin Jakobs,,edwinRNDR,Other,openrndr,openrndr,29,creative-coding,jvm,kotlin,shaders,graphics,webgl,,,,,,,,,,,,,,,/openrndr/openrndr,181,26,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/openMVG/openMVG,https://github.com/openMVG/openMVG,0,"big project, has complex calculations, but not really scientific software",,0,0,0,0,0,0,1,1,0,0,0,open Multiple View Geometry library. Basis for 3D computer vision and Structure from Motion.,"OpenMVG (open Multiple View Geometry)\n=====================================\n\n<p align=""center"">\n<img src=""https://github.com/openMVG/openMVG/raw/master/logo/openMVG_Logo.png"">\n<p/>\n\n| **License** | **Documentation** | Continuous Integration (Linux/MacOs/Windows) |Build  | Code Quality | Chat |\n|:-:|:-:|:-:|:-:|:-:|:-:|\n| [![GitHub license](https://img.shields.io/badge/license-MPL2-blue)](https://github.com/openMVG/openMVG/blob/master/LICENSE) |   [![doc](https://img.shields.io/badge/doc-readthedocs-blueviolet)](http://openmvg.readthedocs.org/en/latest) <br> [Wiki](https://github.com/openMVG/openMVG/wiki) | [![CI](https://github.com/openMVG/openMVG/actions/workflows/compile_and_run_test.yml/badge.svg?branch=develop)](https://github.com/openMVG/openMVG/actions/workflows/compile_and_run_test.yml) | [local/docker build tutorial ](https://github.com/openMVG/openMVG/blob/master/BUILD.md)|  [![CodeQL](https://github.com/openMVG/openMVG/actions/workflows/codeql.yml/badge.svg)](https://github.com/openMVG/openMVG/actions/workflows/codeql.yml) <br> [![CodeFactor](https://www.codefactor.io/repository/github/openmvg/openmvg/badge)](https://www.codefactor.io/repository/github/openmvg/openmvg) | [![Join the chat](https://img.shields.io/badge/chat-on%20gitter-green)](https://gitter.im/openMVG/Lobby) |\n\n**Our Mission**\n- Extend awareness of the power of 3D reconstruction from images/photogrammetry by developing a C++ framework.\n\n**Our Vision**\n- Simplify reproducible research with easy-to-read and accurate implementation of state of the art and ""classic"" algorithms.\n\n**Our Credo**\n- ""Keep it simple, keep it maintainable"".\n  - OpenMVG is designed to be easy to read, learn, modify and use.\n  - Thanks to its strict test-driven development and samples, the library allows to build trusted larger systems.\n\n**Our codebase and pipeline**\n\nOpenMVG provides an end-to-end 3D reconstruction from images framework compounded of libraries, binaries, and pipelines.\n- **The libraries** provide easy access to features like: images manipulation, features description and matching, feature tracking, camera models, multiple-view-geometry, robust-estimation, structure-from-motion algorithms, ...\n- **The binaries** solve unit tasks that a pipeline could require: scene initialization, feature detection & matching and structure-from-motion reconstruction, export the reconstructed scene to others Multiple-View-Stereovision framework to compute dense point clouds or textured meshes.\n- **The pipelines** are created by chaining various binaries to compute image matching relation, solve the Structure from Motion problem (reconstruction, triangulation, localization) and ...\n\nOpenMVG is developed in C++ and runs on Android, iOS, Linux, macOS, and Windows.\n\n<p align=""center"">\n<img src=""./docs/sphinx/rst/openMVG/sfm/pipeline_simple.png"">\n<p/>\n\n**Tutorials**\n- See [Wiki](https://github.com/openMVG/openMVG/wiki)\n  - [OpenMVG Data Structure](https://github.com/openMVG/openMVG/wiki/OpenMVG-data-structures)\n  - [Using OpenMVG as a library](https://github.com/openMVG/openMVG/blob/develop/BUILD.md#using-openmvg-as-a-third-party-library-dependency-with-cmake)\n  - [Using OpenMVG on your image dataset](https://github.com/openMVG/openMVG/wiki/OpenMVG-on-your-image-dataset)\n  - ...\n\n**More information**\n- [Authors](#authors)\n- [Contact](#contact)\n- [Citations](#citations)\n- [Acknowledgements](#acknowledgements)\n\n## Authors\n\nSee [Authors](https://github.com/openMVG/openMVG/raw/master/AUTHORS) text file\n\n## Contact\n\nopenmvg-team[AT]googlegroups.com\n\n\n## Citations\n\nWe are recommending citing `OpenMVG` if you are using the whole library or the adequate paper if you use only a submodule `AContrario Ransac [3], AContrario\nSfM [1], GlobalSfM [4] or Tracks [2]`:\n\n```\n@inproceedings{moulon2016openmvg,\n  title={Open{MVG}: Open multiple view geometry},\n  author={Moulon, Pierre and Monasse, Pascal and Perrot, Romuald and Marlet, Renaud},\n  booktitle={International Workshop on Reproducible Research in Pattern Recognition},\n  pages={60--74},\n  year={2016},\n  organization={Springer}\n}\n```\n\n[1] Moulon Pierre, Monasse Pascal and Marlet Renaud. ACCV 2012.\n[Adaptive Structure from Motion with a contrario model estimation.](http://hal.archives-ouvertes.fr/index.php?halsid=1n2qdqiv2a0l5eq7qpos9us752&view_this_doc=hal-00769266&version=1)\n```\n@inproceedings{Moulon2012,\n  doi = {10.1007/978-3-642-37447-0_20},\n  year  = {2012},\n  publisher = {Springer Berlin Heidelberg},\n  pages = {257--270},\n  author = {Pierre Moulon and Pascal Monasse and Renaud Marlet},\n  title = {Adaptive Structure from Motion with a~Contrario Model Estimation},\n  booktitle = {Proceedings of the Asian Computer Vision Conference (ACCV 2012)}\n}\n```\n\n[2] Moulon Pierre and Monasse Pascal. CVMP 2012.\n[Unordered feature tracking made fast and easy.](http://hal.archives-ouvertes.fr/index.php?halsid=ggdarhl8cv1j6ohq2073eok8q3&view_this_doc=hal-00769267&version=1)\n```\n@inproceedings{moulon2012unordered,\n  title={Unordered feature tracking made fast and easy},\n  author={Moulon, Pierre and Monasse, Pascal},\n  booktitle={CVMP 2012},\n  pages={1},\n  year={2012}\n}\n```\n\n[3] Moisan Lionel, Moulon Pierre and Monasse Pascal. IPOL 2012.\n[Automatic Homographic Registration of a Pair of Images, with A Contrario Elimination of Outliers.](http://dx.doi.org/10.5201/ipol.2012.mmm-oh)\n```\n@article{moisan2012automatic,\n  title={Automatic homographic registration of a pair of images, with a contrario elimination of outliers},\n  author={Moisan, Lionel and Moulon, Pierre and Monasse, Pascal},\n  journal={Image Processing On Line},\n  volume={2},\n  pages={56--73},\n  year={2012}\n}\n```\n\n[4] Moulon Pierre, Monasse Pascal, and Marlet Renaud. ICCV 2013.\n[Global Fusion of Relative Motions for Robust, Accurate and Scalable Structure from Motion.](http://imagine.enpc.fr/~moulonp/publis/iccv2013/index.html)\n\n```\n@inproceedings{moulon2013global,\n  title={Global fusion of relative motions for robust, accurate and scalable structure from motion},\n  author={Moulon, Pierre and Monasse, Pascal and Marlet, Renaud},\n  booktitle={Proceedings of the IEEE International Conference on Computer Vision},\n  pages={3248--3255},\n  year={2013}\n}\n```\n\n## Acknowledgements\n\nopenMVG authors would like to thanks libmv authors for providing an inspiring\nbase to design openMVG. Authors also would like to thanks [Mikros Image](http://www.mikrosimage.eu/)\nand [LIGM-Imagine laboratory](http://imagine.enpc.fr/) for support and authorization to make this\nlibrary an opensource project.\n",5591,geometry,C++,8,Python,C++,C,CMake,JavaScript,HTML,CSS,Dockerfile,,,,,,,,,,,,,,,,,,,,,408,182,190,36,13,100,0,31261,1657,1884,1651,233,1923faf0f2ee603d00e99bc26817daeba8e4c223,[C++] Remove a redundant copying in saveDescsToBinFile (#2334),2024-07-04T05:56:33Z,Minmin Gong,gongminmin@msn.com,gongminmin,v2.1 Sablefish,"OpenMVG v2.1 ""Sablefish"" is out\r\n\r\nWhat's new:\r\n* 42 PR has been handled.\r\n* 45 issues/features enhancement has been completed.\r\n\r\n### OpenMVG core:\r\n\r\n**Core**\r\n- Add implementation of linear global translation (LiGT) - Global translations from global rotations and track observations #2065\r\n- Support for Ceres > 2.0 API #2162 \r\n- SVG module refactoring #2157\r\n\r\n**Build**\r\n- Improve support for non-x86 (Mac Silicon)\r\n- Add [pixi](https://pixi.sh/) build pixi.toml rules to build OpenMVG in a local folder without contamination of your system 20ef483700c14b31cc011a6b3680c5d4f433e516\r\n\r\n**Software**\r\n- Add python examples\r\n  - to use [Kornia](https://kornia.github.io/) ML features with openMVG 8df4af8e0af11b354ab4482724a85a00ba9924c9\r\n  - to export `sfm_data.json` scene to rerun #2245\r\n\r\n**Continuous Integration**\r\n- Add CodeQL workflow #2135\r\n\r\n\r\n## New Contributors\r\n* @melhashash made their first contribution in https://github.com/openMVG/openMVG/pull/2010\r\n* @rfabbri made their first contribution in https://github.com/openMVG/openMVG/pull/2038\r\n* @p12tic made their first contribution in https://github.com/openMVG/openMVG/pull/2092\r\n* @alkavan made their first contribution in https://github.com/openMVG/openMVG/pull/2101\r\n* @BrentHuang made their first contribution in https://github.com/openMVG/openMVG/pull/2124\r\n* @pwntester made their first contribution in https://github.com/openMVG/openMVG/pull/2135\r\n* @StefanBruens made their first contribution in https://github.com/openMVG/openMVG/pull/2147\r\n* @yanxke made their first contribution in https://github.com/openMVG/openMVG/pull/2159\r\n* @kenshi84 made their first contribution in https://github.com/openMVG/openMVG/pull/2167\r\n* @Sanheiii made their first contribution in https://github.com/openMVG/openMVG/pull/2168\r\n* @awarebayes made their first contribution in https://github.com/openMVG/openMVG/pull/2257\r\n* @konstantysz-samsung made their first contribution in https://github.com/openMVG/openMVG/pull/2261\r\n* @jujimeizuo made their first contribution in https://github.com/openMVG/openMVG/pull/2259\r\n\r\n## What's Changed\r\n* Fix ComputeMatches argument parser by @erikreed in https://github.com/openMVG/openMVG/pull/1950\r\n* [build] Fix MSVC std::tolower error with std::locale by @pmoulon in https://github.com/openMVG/openMVG/pull/1952\r\n* Fix main_SfM intrinsics flag usage by @erikreed in https://github.com/openMVG/openMVG/pull/1956\r\n* VLAD Improvements by @rjanvier in https://github.com/openMVG/openMVG/pull/1964\r\n* Improve P3P Nordberg (lambdatwist) by @rjanvier in https://github.com/openMVG/openMVG/pull/1972\r\n* [Build] Modernize cmake openmp usage by @pmoulon in https://github.com/openMVG/openMVG/pull/1978\r\n* Develop non-x86 - testCI by @pmoulon in https://github.com/openMVG/openMVG/pull/1979\r\n* Develop cereal - test CI by @pmoulon in https://github.com/openMVG/openMVG/pull/1980\r\n* Develop modernize openmp usage by @pmoulon in https://github.com/openMVG/openMVG/pull/1983\r\n* Ensure we don't lose precision when exporting to txt by @melhashash in https://github.com/openMVG/openMVG/pull/2010\r\n* builds on Mac OS 12 AppleClang + OpenMP by @rfabbri in https://github.com/openMVG/openMVG/pull/2038\r\n* Add utm transfom support for software registration_to_exif_gps_position by @qingzhengzhuma in https://github.com/openMVG/openMVG/pull/1993\r\n* Pull Request cxx20 by @pmoulon in https://github.com/openMVG/openMVG/pull/2049\r\n* software: store relative paths in MVS scene by @cdcseacave in https://github.com/openMVG/openMVG/pull/2055\r\n* Add implementation of linear global translation (LiGT) - Global translations from global rotations and track observations by @pmoulon in https://github.com/openMVG/openMVG/pull/2065\r\n* Fix build againt cereal>1.3.0 by @bartoszek in https://github.com/openMVG/openMVG/pull/2071\r\n* fix build error by @cdcseacave in https://github.com/openMVG/openMVG/pull/2085\r\n* (trivial) Remove using namespace std and add std:: qualifications in few places where needed by @p12tic in https://github.com/openMVG/openMVG/pull/2092\r\n* Adding missing GoPro cameras HERO8, HERO9, and HERO10. by @alkavan in https://github.com/openMVG/openMVG/pull/2101\r\n* Fix inconsistency in spherical Intrinsic_Spherical projection functor by @rjanvier in https://github.com/openMVG/openMVG/pull/2087\r\n* fix(image): fix png operation memleak by @BrentHuang in https://github.com/openMVG/openMVG/pull/2124\r\n* Develop eigen sparse by @pmoulon in https://github.com/openMVG/openMVG/pull/2104\r\n* Add CodeQL workflow by @pwntester in https://github.com/openMVG/openMVG/pull/2135\r\n* Prefer CMake Config when looking for Flann by @StefanBruens in https://github.com/openMVG/openMVG/pull/2147\r\n* Fix library install directories by @StefanBruens in https://github.com/openMVG/openMVG/pull/2148\r\n* fix an error in colorHarmonize about eigen by @Yannnnnnnnnnnn in https://github.com/openMVG/openMVG/pull/1287\r\n* [sfm] Update API to fit ceres::manifold for constant parameter group #2152 by @pmoulon in https://github.com/openMVG/openMVG/pull/2154\r\n* [Samples] Add parameter to choose the relative pose solver by @pmoulon in https://github.com/openMVG/openMVG/pull/2155\r\n* Fix inconsistency in spherical Intrinsic_Spherical projection functor by @pmoulon in https://github.com/openMVG/openMVG/pull/2156\r\n* Add more cameras to DB by @yanxke in https://github.com/openMVG/openMVG/pull/2159\r\n* [SVG] module refactoring by @pmoulon in https://github.com/openMVG/openMVG/pull/2157\r\n* [3rParty] Update eigen to 3.4.0 rc1 to official release by @pmoulon in https://github.com/openMVG/openMVG/pull/2161\r\n* [sfm] Update for ceres2.0 API and shared loss_function #1715 by @pmoulon in https://github.com/openMVG/openMVG/pull/2162\r\n* [Experimental] Add a new SfM pipeline based on stellar reconstruction by @pmoulon in https://github.com/openMVG/openMVG/pull/2070\r\n* [software/import] Add support of Multiface dataset by @kenshi84 in https://github.com/openMVG/openMVG/pull/2167\r\n* Export masks to openMVS by @Sanheiii in https://github.com/openMVG/openMVG/pull/2168\r\n* Add to camera db. by @yanxke in https://github.com/openMVG/openMVG/pull/2181\r\n* Update Dockerfile by @awarebayes in https://github.com/openMVG/openMVG/pull/2257\r\n* software: store relative paths for masks in MVS scene and fix global space IDs in https://github.com/openMVG/openMVG/pull/2193\r\n* Add Samsung SM-S901B by @konstantysz-samsung in https://github.com/openMVG/openMVG/pull/2261\r\n* fix: compilation for ubuntu22.04 arm64 by @jujimeizuo in https://github.com/openMVG/openMVG/pull/2259\r\n* [Visualization] Add a ReRun sfm_data.json visualizer by @pmoulon in https://github.com/openMVG/openMVG/pull/2245\r\n\r\n\r\n\r\n**Full Changelog**: https://github.com/openMVG/openMVG/compare/v2.0...v2.1",v2.1,Pierre Moulon,,pmoulon,Mozilla Public License 2.0,openMVG,openMVG,16,geometry,computer-vision,openmvg,structure-from-motion,sfm,multiple-view-geometry,drone,photogrammetry,3d-reconstruction,,,,,,,,,,,,/openMVG/openMVG,19,274,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/OpenMS/OpenMS,https://github.com/OpenMS/OpenMS,1,,,1,1,1,1,0,0,0,0,0,0,1,The codebase of the OpenMS project,"OpenMS\n=======\n\n[![License (3-Clause BSD)](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg?logo=data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz4NCjwhLS0gR2VuZXJhdG9yOiBBZG9iZSBJbGx1c3RyYXRvciAyNC4xLjEsIFNWRyBFeHBvcnQgUGx1Zy1JbiAuIFNWRyBWZXJzaW9uOiA2LjAwIEJ1aWxkIDApICAtLT4NCjxzdmcgdmVyc2lvbj0iMS4xIiBpZD0iTGF5ZXJfMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeD0iMHB4IiB5PSIwcHgiDQoJIHZpZXdCb3g9IjAgMCA1MTIgNTEyIiBzdHlsZT0iZW5hYmxlLWJhY2tncm91bmQ6bmV3IDAgMCA1MTIgNTEyOyIgeG1sOnNwYWNlPSJwcmVzZXJ2ZSI+DQo8Zz4NCgk8cGF0aCBkPSJNMCwyNjcuMkMyLjQsMTI2LjksMTAwLjYsMjcuMSwyMjAuOCwxMUMzNjQuMS04LjIsNDg0LjcsODkuMyw1MDcuOSwyMTguNmMyMiwxMjIuNy00NS40LDIzNy41LTE1Ni41LDI4Mi45DQoJCWMtOS42LDMuOS0xNC45LDEuOC0xOC42LTcuOWMtMTguNC00Ny44LTM2LjgtOTUuNy01NS4xLTE0My41Yy0zLjItOC40LTEtMTMuNiw3LjItMTcuNGMyNS0xMS40LDQwLjYtMzAuNCw0NC43LTU3LjYNCgkJYzMuMS0yMC4yLTIuMy00MC44LTE0LjktNTYuOWMtMTIuNi0xNi4xLTMxLjQtMjYuMi01MS44LTI4Yy00MC4zLTMuNS03NC4xLDI0LjUtODAsNjEuNmMtNS40LDM0LjEsMTEuNSw2NS44LDQzLjMsODAuMg0KCQljOS45LDQuNSwxMS45LDguOSw4LjEsMTljLTE4LjUsNDguMS0zNyw5Ni4zLTU1LjQsMTQ0LjVjLTIuNyw3LjEtOC42LDkuNi0xNiw2LjdjLTU0LjMtMjEtMTA0LjctNjMtMTM1LjEtMTIyLjkNCgkJQzIsMzI4LjYsMS43LDI4OC44LDAsMjY3LjJMMCwyNjcuMnogTTIxLjYsMjY1LjJDMjIsMjcyLDIyLjIsMjgwLDIyLjksMjg4YzYuNSw3NC4zLDUxLjIsMTQ4LjIsMTM1LjMsMTg5LjENCgkJYzMuMywxLjUsNC41LDAuOCw1LjgtMi40YzE1LjQtNDAuNCwzMC45LTgwLjgsNDYuNS0xMjEuMWMxLjMtMy40LDAuNi01LTIuNS02LjljLTMyLjYtMjAuNi00OC44LTUwLjEtNDcuMS04OC44DQoJCWMxLTIyLjMsOS42LTQxLjgsMjQuNi01OC4xYzMxLTMzLjgsNzkuNS00MS4xLDExOS4zLTE4LjJjMzIuOSwxOSw1MS4yLDU1LjcsNDYuNyw5My4zYy0zLjcsMzEuNi0xOS45LDU1LjctNDcuMiw3Mi4xDQoJCWMtMi44LDEuNy0zLjYsMy0yLjQsNi4yYzE1LjcsNDAuNSwzMS4zLDgxLDQ2LjcsMTIxLjVjMS4yLDMuMiwyLjUsMy45LDUuOCwyLjRjMzYuNy0xNy4xLDY3LjMtNDEuNiw5MS03NC4zDQoJCUM0ODEuMiwzNTMsNDk2LDI5Ny41LDQ4OSwyMzYuNUM0NzQuOCwxMTUuMSwzNjUuNywxNC43LDIyNS4xLDMyQzExNS42LDQ1LjQsMjMuNSwxMzcuOCwyMS42LDI2NS4yTDIxLjYsMjY1LjJ6Ii8+DQo8L2c+DQo8L3N2Zz4NCg==&style=flat-square)](http://opensource.org/licenses/BSD-3-Clause)\n[![Project Stats](https://www.openhub.net/p/open-ms/widgets/project_thin_badge.gif)](https://www.openhub.net/p/open-ms)\n[![Discord Shield](https://img.shields.io/discord/832282841836159006?style=flat-square&message=Discord&color=5865F2&logo=Discord&logoColor=FFFFFF&label=Discord)](https://discord.gg/4TAGhqJ7s5)\n[![Gitter](https://img.shields.io/static/v1?style=flat-square&message=on%20Gitter&color=ED1965&logo=Gitter&logoColor=FFFFFF&label=Chat)](https://gitter.im/OpenMS/OpenMS?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n[![Install with bioconda](https://img.shields.io/conda/v/bioconda/pyopenms?color=44A833&logo=Anaconda&style=flat-square&label=Install%20with%20bioconda)](http://bioconda.github.io/recipes/openms-meta/README.html)\n[![Install with conda](https://img.shields.io/conda/v/openms/pyopenms?color=44A833&label=Install%20with%20conda%3A%3Aopenms&logo=Anaconda&style=flat-square)](https://anaconda.org/openms)\n[![Documentation](https://img.shields.io/static/v1?style=flat-square&message=ReadTheDocs&color=2C4AA8&logo=ReadTheDocs&logoColor=FFFFFF&label=Documentation)](https://openms.readthedocs.io)\n[![API docs](https://img.shields.io/static/v1?style=flat-square&message=Doxygen&color=2C4AA8&logo=ReadTheDocs&logoColor=FFFFFF&label=API%20docs)](https://abibuilder.cs.uni-tuebingen.de/archive/openms/Documentation/release/latest/html/index.html)\n[![Gitpod Ready-to-Code](https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?style=flat-square&logo=gitpod)](https://gitpod.io/#https://github.com/OpenMS/OpenMS) \n\n[OpenMS](http://www.openms.org/)\nis an open-source software C++ library for LC-MS data management and\nanalyses. It offers an infrastructure for rapid development of mass\nspectrometry related software. OpenMS is free software available under the\nthree clause BSD license and runs under Windows, macOS, and Linux.\n\nIt comes with a vast variety of pre-built and ready-to-use tools for proteomics\nand metabolomics data analysis (TOPPTools) as well as powerful 1D, 2D and 3D\nvisualization (TOPPView).\n\nOpenMS offers analyses for various quantitation protocols, including label-free\nquantitation, SILAC, iTRAQ, TMT, SRM, SWATH, etc.\n\nIt provides built-in algorithms for de-novo identification and database search,\nas well as adapters to other state-of-the art tools like X!Tandem, Mascot,\nComet, etc. It supports easy integration of OpenMS built tools into workflow\nengines like KNIME, Galaxy, WS-Pgrade, and TOPPAS via the TOPPtools concept and\na unified parameter handling via a 'common tool description' (CTD) scheme.\n\nWith pyOpenMS, OpenMS offers Python bindings to a large part of the OpenMS API\nto enable rapid algorithm development. OpenMS supports the Proteomics Standard\nInitiative (PSI) formats for MS data. The main contributors of OpenMS are\ncurrently the Eberhard-Karls-Universität in Tübingen, the Freie Universität\nBerlin, and the ETH Zürich.\n\nFeatures\n--------\n- Core C++ library under three-clause BSD licence using modern C++17\n- Python bindings to the C++ API through pyOpenMS\n- Major community file formats supported (mzML, mzXML, mzIdentXML, pepXML, mzTab, etc.)\n- Over 150+ individual analysis tools (TOPP Tools), covering most MS and LC-MS data processing and mining tasks\n- Powerful 1D, 2D and 3D visualization tools (TOPPView)\n- Support for most MS identification and quantification workflows (targeted, DIA, label-free, isobaric and stable isotope)\n- Support for all major platforms (Windows [XP, 7, 8, 10], macOS and Linux)\n\nDocumentation\n-------------\n\nUsers and developers should start by reading the [OpenMS documentation](https://openms.readthedocs.io/en/latest). OpenMS\nAPI reference documentation and advanced developer doxygen documentation can be browsed [here](https://abibuilder.cs.uni-tuebingen.de/archive/openms/Documentation/release/latest/html/index.html).\n\nopenms.readthedocs.io documentation aims at being an entry point for users and developers alike. It is trying to be mostly version-independent and therefore\nonly consists of one main branch. We may introduce tags for older releases in the future.\n\nThe OpenMS API reference has several endpoints:\n\n1. [`nightly`](https://abibuilder.cs.uni-tuebingen.de/archive/openms/Documentation/nightly/html/index.html): OpenMS API reference and advanced developer documentation of nightly releases.\n2. [`release/latest`](https://abibuilder.cs.uni-tuebingen.de/archive/openms/Documentation/release/latest/html/index.html) : OpenMS API reference and advanced developer documentation of latest stable release.\n3. [`release/${version}`](https://abibuilder.cs.uni-tuebingen.de/archive/openms/Documentation/release/latest/html/index.html) : OpenMS API reference and advanced developer documentation of an older version.\n\nDocumentation for the Python bindings pyOpenMS can be found on the [pyOpenMS online documentation](https://pyopenms.readthedocs.io).\n\nCitation\n--------\nPlease cite:\n\nPfeuffer, J., Bielow, C., Wein, S. et al. OpenMS 3 enables reproducible analysis of large-scale mass spectrometry data, Nat Methods 21, 365–367 (2024). https://doi.org/10.1038/s41592-024-02197-7\n\nThe file [AUTHORS](AUTHORS) contains a list of all authors who worked on OpenMS.\n\nLicence\n-------\nOpenMS is released under the [three clause BSD licence](LICENSE).\n",471,metabolomics,C++,16,CMake,Shell,Ruby,C++,Python,R,XSLT,HTML,C,Batchfile,NSIS,PHP,Perl,MAXScript,Cython,JetBrains MPS,,,,,,,,,,,,,4304,452,3802,50,99,112,0,644358,311,3205,2612,593,c1dccb3a874787d4156e4cd3694f9caef37d0b00,Merge pull request #7547 from cbielow/boost174,2024-07-13T20:54:15Z,Chris Bielow,chris.bielow@fu-berlin.de,cbielow,OpenMS 3.0.0,"# Dear OpenMS-Users,\r\n\r\nwe are proud to announce the release of OpenMS 3.0.0 (June 2023).\r\n\r\nThe **source code** and precompiled installers (**Windows, Linux, MacOSX**) are available at the [bottom of this page](#bottom-anchor) or [here](https://abibuilder.cs.uni-tuebingen.de/archive/openms/OpenMSInstaller/release/3.0.0).\r\n\r\nFor detailed installation instructions and more options, please visit https://openms.readthedocs.io/en/release3.0.0/openms-applications-and-tools/installation.html.\r\n\r\nThis is a full release, i.e. a major version increase from 2.0 to 3.0. Highlights of these changes are listed at the [end of this page](#major-release-compare-2vs3).\r\n\r\nNote: The Windows-Installer has been updated on Nov 16th 2023 to allow installation on systems with a lot of other software preinstalled (potentially leading to errors during installation of OpenMS).\r\n\r\n## Important changes to the [previous version 2.8 from Feb 2022](https://github.com/OpenMS/OpenMS/releases/tag/Release2.8.0):\r\n\r\n------------------------------------------------------------------------------------------\r\n\r\n#### New Tools:\r\n- FLASHDeconv -- Ultra-fast high-quality deconvolution enables online processing of top-down MS data (TOPP)\r\n- FLASHDeconvWizard -- A GUI assistant for FLASHDeconv execution.\r\n \r\n#### New Features:\r\ne.g.\r\n- TMT18plex support (#6390)\r\n- ProteinQuantifier supports iBAQ (#6107)\r\n- OpenSwath: Add support for diaPASEF data with overlapping m/z and IM windows, and add new outputs on ion mobility features (delta_im), IM calibration (#5911, #6234, #6268)\r\n- OpenSwathDecoyGenerator speed improvement and remove duplicates (#6054) \r\n- NucleicAcidSearchEngine (NASE): user defined ribonucleotides with phosphorothioate linkages (#6337), JSON based ribonucleotides and updated to latest Modomics database (#6482)\r\n- TargetedSpectraExtractor: more features (#6106)\r\n- TOPPView: TheoreticalSpectrumGenerationDialog now supports generation of isotope patterns for metabolites (#6023); faster loading of external drag'n'drop data (#6837)\r\n- colored commandline/console on all platforms (#6275)\r\n- support for 'no cleavage' for XTandemAdapter and CometAdapter (#6133).\r\n- Percolator pin file reader (#6824)\r\n- JSON export for OMS files(SQLite) (#6114)\r\n- ParamEditor with more convenient StringList editing  (#5135)\r\n- load parameter values from a JSON formatted .json file. (Accessible via -ini. This will be\r\n  helpful for Common Workflow Language users and others)\r\n- FileFilter can remove convex hulls of features and consensusFeatures to reduce file size (#6140)\r\n- Faster compile time (#6618)\r\n- Improving code quality by fixing lots of linting warnings and leaks (e.g. #6839, #6831, #6829)\r\n\r\n\r\n### Documentation\r\n - website redesign (visit openms.org)\r\n - OpenMS **user documentation** is moved to [openms.readthedocs.io/en/latest](https://openms.readthedocs.io/en/latest/). \r\n - OpenMS **API reference** and advanced developer documentation remains inside OpenMS doxygen\r\ndocumentation (https://abibuilder.cs.uni-tuebingen.de/archive/openms/Documentation/release/)\r\n - pyopenms: pyopenms-extra is renamed to pyopenms-docs.\r\n\r\n### Bug fixes\r\ne.g.\r\n- GaussFilter when using ppm as width (#6830)\r\n- NASE a-B ion masses (#6718), ID-Mapper for TMT data (#6758)\r\n- FeatureFinderMetaboliteIdentification speed improvements (#6619)\r\n- IDRipper speed improvements (#6584)\r\n- Honor MissedCleavages in SimpleSearchEngine (#6889)\r\n- TOPPView: fixed lots of display glitches, e.g. axis labels, goto dialog and easier re-use of components, etc (#6673, #6616, #6592, #6703, #6793) \r\n- mzTab fixes for empty IDs (#6445)\r\n- Fix GNPS error for empty scans in Bruker files (#6898)\r\n- PrecursorPurity: handle unknown charge (#6283)\r\n- OpenSwath: Fix duplicated transition error when multiple genes map to a single peptide (#5653)\r\n- Fixed race condition when logging messages.\r\n\r\nRemoved tools:\r\n- InspectAdapter\r\n- OMSSAAdapter\r\n- MyriMatchAdapter\r\n- CruxAdapter\r\n\r\nSupported compilers (when building from source):\r\n + g++ (7.0 or later, tested up to v13.0)\r\n + clang (?, tested up to v16)\r\n + Visual Studio (2019(v16.8.4) or later)\r\n\r\nFull changelog: [OpenMS 2.8 &rarr; 3.0](https://github.com/OpenMS/OpenMS/compare/Release2.8.0...Release3.0.0)\r\n\r\n------------------------------------------------------------------------------------------\r\n<a id=""major-release-compare-2vs3""></a>  <!-- needed for jumping here -->\r\n## OpenMS 2.0 &rarr; 3.0 highlights\r\n\r\n- FLASHDeconv and FLASHDeconvWizard: Harness ultra-fast high-quality deconvolution of top-down MS data. These tools enable rapid and accurate deconvolution, ensuring efficient and precise data analysis.\r\n\r\n- NucleicAcidSearchEngine: Enter the world of RNA analysis with NucleicAcidSearchEngine, a tool that annotates nucleic acid identifications to MS/MS spectra.\r\n\r\n- OpenPepXL, OpenPepXLLF and related tools: Perform comprehensive protein-protein cross-link analysis covering a wide range of chemical cross-linkers.\r\n\r\n- Epifany: Take your protein inference to new heights with Epifany, a Bayesian protein inference tool that offers accurate and reliable results for complex proteomics analyses.\r\n\r\n- FeatureFinderMetaboIdent: Detect features in MS1 data based on metabolite identifications with FeatureFinderMetaboIdent. This tool enhances metabolomics analyses by accurately identifying features for further investigation. Check out our Umetaflow workflow or the pyopenms documentation for details.\r\n\r\n- GNPSExport: Export consensus features into MGF format with GNPSExport. This tool simplifies data sharing and collaboration, streamlining your data analysis workflow.\r\n\r\n- ProteomicsLFQ: Explore a standard proteomics LFQ pipeline in a single tool: ProteomicsLFQ. A tool designed for reliable quantification of proteins.\r\n\r\n- QualityControl: Compute various QC metrics from input files with QualityControl, providing essential quality assessments for your data. This versatile tool offers a comprehensive view of data quality.\r\n\r\n- Optimized RT alignment with MapAlignerTreeGuided, a new tool that aligns maps through hierarchical clustering based on shared IDs.\r\n\r\n- MetaProSIP, the tool for protein-SIP experiments gained support for Deuterium and heavy Oxygen labeling.\r\n\r\nGeneral Enhancements: Streamlining your developer experience\r\nThe OpenMS 3.0 codebase has been updated to C++17. Additionally, the configuration storage path on Linux has changed to ~/.config/, making it easier to manage user-specific configurations.\r\n\r\nLibrary and Performance Improvements: Enhancing Data Analysis\r\nThe OpenMS library has undergone significant updates, resulting in improved mass calculations, peak integration, and isotope distributions. The library now supports more precise peak integration methods, ensuring the accuracy and reliability of your data analysis.\r\n\r\nUser Interface and Usability: Enhancing Your Workflow\r\nSeveral GUI tools have received notable improvements in OpenMS 3.0. TOPPView, TOPPAS, and ParamEditor have been enhanced for better usability and additional data visualization capabilities.\r\n\r\nModule Removals and Deprecated Tools: Streamlined and Focused\r\nOpenMS 3.0 removes several deprecated modules and tools. This focused approach ensures a more efficient and effective user experience.\r\n\r\nFor an exhaustive list of changes between OpenMS 2.0 - OpenMS 3.0 please refer to the changelogs.\r\n\r\n\r\nBest regards,\r\nThe OpenMS-Developers\r\n\r\n<a id=""bottom-anchor""></a>  <!-- needed for jumping to the assets/downloads, which are just below here -->\r\n\r\n",Release3.0.0,,,openms-jenkins-bot,Other,OpenMS,OpenMS,11,openms,c-plus-plus,ms-data,mass-spectrometry,linux,macos,windows,proteomics,analyses,algorithms,metabolomics,3-clause-bsd,hacktoberfest,,,,,,,,/OpenMS/OpenMS,26,31,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/OpenGene/fastp,https://github.com/OpenGene/fastp,0.5,Processes biological sequences. Not sure if it can be considered as a standalone project?,1,1,1,1,1,0,0,0,0,0,0,1,An ultra-fast all-in-one FASTQ preprocessor (QC/adapters/trimming/filtering/splitting/merging...),"[![install with conda](\nhttps://anaconda.org/bioconda/fastp/badges/version.svg)](https://anaconda.org/bioconda/fastp)\n[![install with conda](\nhttps://anaconda.org/bioconda/fastp/badges/downloads.svg)](https://anaconda.org/bioconda/fastp)\n[![DebianBadge](\nhttps://badges.debian.net/badges/debian/unstable/fastp/version.svg)](https://packages.debian.org/unstable/fastp)\n[![fastp ci](https://github.com/OpenGene/fastp/actions/workflows/ci.yml/badge.svg?branch=master)](https://github.com/OpenGene/fastp/actions/workflows/ci.yml)\n\n# fastp\nA tool designed to provide fast all-in-one preprocessing for FastQ files. This tool is developed in C++ with multithreading supported to afford high performance.\n\nCitation: Shifu Chen. 2023. Ultrafast one-pass FASTQ data preprocessing, quality control, and deduplication using fastp. iMeta 2: e107. https://doi.org/10.1002/imt2.107\n\n- [fastp](#fastp)\n- [features](#features)\n- [simple usage](#simple-usage)\n- [examples of report](#examples-of-report)\n- [get fastp](#get-fastp)\n  - [install with Bioconda](#install-with-bioconda)\n  - [or download the latest prebuilt binary for Linux users](#or-download-the-latest-prebuilt-binary-for-linux-users)\n  - [or compile from source](#or-compile-from-source)\n    - [Step 1: download and build libisal](#step-1-download-and-build-libisal)\n    - [step 2: download and build libdeflate](#step-2-download-and-build-libdeflate)\n    - [Step 3: download and build fastp](#step-3-download-and-build-fastp)\n- [input and output](#input-and-output)\n  - [output to STDOUT](#output-to-stdout)\n  - [input from STDIN](#input-from-stdin)\n  - [store the unpaired reads for PE data](#store-the-unpaired-reads-for-pe-data)\n  - [store the reads that fail the filters](#store-the-reads-that-fail-the-filters)\n  - [process only part of the data](#process-only-part-of-the-data)\n  - [do not overwrite exiting files](#do-not-overwrite-exiting-files)\n  - [split the output to multiple files for parallel processing](#split-the-output-to-multiple-files-for-parallel-processing)\n  - [merge PE reads](#merge-pe-reads)\n- [filtering](#filtering)\n  - [quality filter](#quality-filter)\n  - [length filter](#length-filter)\n  - [low complexity filter](#low-complexity-filter)\n  - [Other filter](#other-filter)\n- [adapters](#adapters)\n- [per read cutting by quality score](#per-read-cutting-by-quality-score)\n- [base correction for PE data](#base-correction-for-pe-data)\n- [global trimming](#global-trimming)\n- [polyG tail trimming](#polyg-tail-trimming)\n- [polyX tail trimming](#polyx-tail-trimming)\n- [unique molecular identifier (UMI) processing](#unique-molecular-identifier-umi-processing)\n  - [UMI example](#umi-example)\n- [output splitting](#output-splitting)\n  - [splitting by limiting file number](#splitting-by-limiting-file-number)\n  - [splitting by limiting the lines of each file](#splitting-by-limiting-the-lines-of-each-file)\n- [overrepresented sequence analysis](#overrepresented-sequence-analysis)\n- [merge paired-end reads](#merge-paired-end-reads)\n- [duplication rate and deduplication](#duplication-rate-and-deduplication)\n  - [duplication rate evaluation](#duplication-rate-evaluation)\n  - [deduplication](#deduplication)\n- [all options](#all-options)\n- [citations](#citations)\n\n# features\n0. comprehensive quality profiling for both before and after filtering data (quality curves, base contents, KMER, Q20/Q30, GC Ratio, duplication, adapter contents...)\n1. filter out bad reads (too low quality, too short, or too many N...)\n2. cut low quality bases for per read in its 5' and 3' by evaluating the mean quality from a sliding window (like Trimmomatic but faster).\n3. trim all reads in front and tail\n4. cut adapters. Adapter sequences can be automatically detected, which means you don't have to input the adapter sequences to trim them.\n5. correct mismatched base pairs in overlapped regions of paired end reads, if one base is with high quality while the other is with ultra low quality\n6. trim polyG in 3' ends, which is commonly seen in NovaSeq/NextSeq data. Trim polyX in 3' ends to remove unwanted polyX tailing (i.e. polyA tailing for mRNA-Seq data)\n7. preprocess unique molecular identifier (UMI) enabled data, shift UMI to sequence name.\n8. report JSON format result for further interpreting.\n9. visualize quality control and filtering results on a single HTML page (like FASTQC but faster and more informative).\n10. split the output to multiple files (0001.R1.gz, 0002.R1.gz...) to support parallel processing. Two modes can be used, limiting the total split file number, or limitting the lines of each split file.\n11. support long reads (data from PacBio / Nanopore devices).\n12. support reading from STDIN and writing to STDOUT\n13. support interleaved input\n14. support ultra-fast FASTQ-level deduplication\n15. ...\n\nIf you find a bug or have additional requirement for `fastp`, please file an issue:https://github.com/OpenGene/fastp/issues/new\n\n# simple usage\n* for single end data (not compressed)\n```\nfastp -i in.fq -o out.fq\n```\n* for paired end data (gzip compressed)\n```\nfastp -i in.R1.fq.gz -I in.R2.fq.gz -o out.R1.fq.gz -O out.R2.fq.gz\n```\nBy default, the HTML report is saved to `fastp.html` (can be specified with `-h` option), and the JSON report is saved to `fastp.json` (can be specified with `-j` option).\n\n# examples of report\n`fastp` creates reports in both HTML and JSON format.\n* HTML report: http://opengene.org/fastp/fastp.html\n* JSON report: http://opengene.org/fastp/fastp.json\n\n# get fastp\n## install with Bioconda\n[![install with conda](\nhttps://anaconda.org/bioconda/fastp/badges/version.svg)](https://anaconda.org/bioconda/fastp)\n```shell\n# note: the fastp version in bioconda may be not the latest\nconda install -c bioconda fastp\n```\n## or download the latest prebuilt binary for Linux users\nThis binary was compiled on CentOS, and tested on CentOS/Ubuntu\n```shell\n# download the latest build\nwget http://opengene.org/fastp/fastp\nchmod a+x ./fastp\n\n# or download specified version, i.e. fastp v0.23.4\nwget http://opengene.org/fastp/fastp.0.23.4\nmv fastp.0.23.4 fastp\nchmod a+x ./fastp\n```\n## or compile from source\n`fastp` depends on `libdeflate` and `libisal`, while `libisal` is not compatible with gcc 4.8. If you use gcc 4.8, your fastp will fail to run. Please upgrade your gcc before you build the libraries and fastp.\n\n### Step 1: download and build libisal\nSee https://github.com/intel/isa-l\n`autoconf`, `automake`, `libtools`, `nasm (>=v2.11.01)` and `yasm (>=1.2.0)` are required to build this isal\n```shell\ngit clone https://github.com/intel/isa-l.git\ncd isa-l\n./autogen.sh\n./configure --prefix=/usr --libdir=/usr/lib64\nmake\nsudo make install\n```\n\n### step 2: download and build libdeflate\nSee https://github.com/ebiggers/libdeflate\n```shell\ngit clone https://github.com/ebiggers/libdeflate.git\ncd libdeflate\ncmake -B build\ncmake --build build\ncmake --install build\n```\n\n### Step 3: download and build fastp\n```shell\n# get source (you can also use browser to download from master or releases)\ngit clone https://github.com/OpenGene/fastp.git\n\n# build\ncd fastp\nmake\n\n# Install\nsudo make install\n```\nYou can add `-j8` option to `make/cmake` to use 8 threads for the compilation. \n\n# input and output\n`fastp` supports both single-end (SE) and paired-end (PE) input/output.\n* for SE data, you only have to specify read1 input by `-i` or `--in1`, and specify read1 output by `-o` or `--out1`.\n* for PE data, you should also specify read2 input by `-I` or `--in2`, and specify read2 output by `-O` or `--out2`.\n* if you don't specify the output file names, no output files will be written, but the QC will still be done for both data before and after filtering.\n* the output will be gzip-compressed if its file name ends with `.gz`\n## output to STDOUT\n`fastp` supports streaming the passing-filter reads to STDOUT, so that it can be passed to other compressors like `bzip2`, or be passed to aligners like `bwa` and `bowtie2`.\n* specify `--stdout` to enable this mode to stream output to STDOUT\n* for PE data, the output will be interleaved FASTQ, which means the output will contain records like `record1-R1 -> record1-R2 -> record2-R1 -> record2-R2 -> record3-R1 -> record3-R2 ... `\n## input from STDIN\n* specify `--stdin` if you want to read the STDIN for processing.\n* if the STDIN is an interleaved paired-end stream, specify `--interleaved_in` to indicate that.\n## store the unpaired reads for PE data\n* you can specify `--unpaired1` to store the reads that read1 passes filters but its paired read2 doesn't, as well as `--unpaired2` for unpaired read2.\n* `--unpaired1` and `--unpaired2` can be the same, so the unpaired read1/read2 will be written to the same single file.\n## store the reads that fail the filters\n* give `--failed_out` to specify the file name to store the failed reads.\n* if one read failed and is written to `--failed_out`, its `failure reason` will be appended to its read name. For example, `failed_quality_filter`, `failed_too_short` etc.\n* for PE data, if unpaired reads are not stored (by giving --unpaired1 or --unpaired2), the failed pair of reads will be put together. If one read passes the filters but its pair doesn't, the `failure reason` will be `paired_read_is_failing`.\n## process only part of the data\nIf you don't want to process all the data, you can specify `--reads_to_process` to limit the reads to be processed. This is useful if you want to have a fast preview of the data quality, or you want to create a subset of the filtered data.\n## do not overwrite exiting files\nYou can enable the option `--dont_overwrite` to protect the existing files not to be overwritten by `fastp`. In this case, `fastp` will report an error and quit if it finds any of the output files (read1, read2, json report, html report) already exists before.\n## split the output to multiple files for parallel processing\nSee [output splitting](#output-splitting)\n## merge PE reads\nSee [merge paired-end reads](#merge-paired-end-reads)\n\n# filtering\nMultiple filters have been implemented.\n## quality filter\nQuality filtering is enabled by default, but you can disable it by `-Q` or `disable_quality_filtering`. Currently it supports filtering by limiting the N base number (`-n, --n_base_limit`),  and the percentage of unqualified bases.  \n\nTo filter reads by its percentage of unqualified bases, two options should be provided:\n* `-q, --qualified_quality_phred`       the quality value that a base is qualified. Default 15 means phred quality >=Q15 is qualified.\n* `-u, --unqualified_percent_limit`    how many percents of bases are allowed to be unqualified (0~100). Default 40 means 40%\n\nYou can also filter reads by its average quality score\n* `-e, --average_qual`   if one read's average quality score <avg_qual, then this read/pair is discarded. Default 0 means no requirement (int [=0])\n\n## length filter\nLength filtering is enabled by default, but you can disable it by `-L` or `--disable_length_filtering`. The minimum length requirement is specified with `-l` or `--length_required`.\n\nFor some applications like small RNA sequencing, you may want to discard the long reads. You can specify `--length_limit` to discard the reads longer than `length_limit`. The default value 0 means no limitation.\n\n## low complexity filter\nLow complexity filter is disabled by default, and you can enable it by `-y` or `--low_complexity_filter`. The complexity is defined as the percentage of base that is different from its next base (base[i] != base[i+1]). For example:\n```\n# a 51-bp sequence, with 3 bases that is different from its next base\nseq = 'AAAATTTTTTTTTTTTTTTTTTTTTGGGGGGGGGGGGGGGGGGGGGGCCCC'\ncomplexity = 3/(51-1) = 6%\n```\nThe threshold for low complexity filter can be specified by `-Y` or `--complexity_threshold`. It's range should be `0~100`, and its default value is 30, which means 30% complexity is required.\n\n## Other filter\nNew filters are being implemented. If you have a new idea or new request, please file an issue.\n\n# adapters\nAdapter trimming is enabled by default, but you can disable it by `-A` or `--disable_adapter_trimming`. Adapter sequences can be automatically detected for both PE/SE data.\n* For SE data, the adapters are evaluated by analyzing the tails of first ~1M reads. This evaluation may be inacurrate, and you can specify the adapter sequence by `-a` or `--adapter_sequence` option. If adapter sequence is specified, the auto detection for SE data will be disabled.\n* For PE data, the adapters can be detected by per-read overlap analysis, which seeks for the overlap of each pair of reads. This method is robust and fast, so normally you don't have to input the adapter sequence even you know it. But you can still specify the adapter sequences for read1 by `--adapter_sequence`, and for read2 by `--adapter_sequence_r2`. If `fastp` fails to find an overlap (i.e. due to low quality bases), it will use these sequences to trim adapters for read1 and read2 respectively.\n* For PE data, the adapter sequence auto-detection is disabled by default since the adapters can be trimmed by overlap analysis. However, you can specify `--detect_adapter_for_pe` to enable it.\n* For PE data, `fastp` will run a little slower if you specify the sequence adapters or enable adapter auto-detection, but usually result in a slightly cleaner output, since the overlap analysis may fail due to sequencing errors or adapter dimers.\n* The most widely used adapter is the Illumina TruSeq adapters. If your data is from the TruSeq library, you can add `--adapter_sequence=AGATCGGAAGAGCACACGTCTGAACTCCAGTCA --adapter_sequence_r2=AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT` to your command lines, or enable auto detection for PE data by specifing `detect_adapter_for_pe`.\n* `fastp` contains some built-in known adapter sequences for better auto-detection. If you want to make some adapters to be a part of the built-in adapters, please file an issue.\n\nYou can also specify `--adapter_fasta` to give a FASTA file to tell `fastp` to trim multiple adapters in this FASTA file. Here is a sample of such adapter FASTA file:\n```\n>Illumina TruSeq Adapter Read 1\nAGATCGGAAGAGCACACGTCTGAACTCCAGTCA\n>Illumina TruSeq Adapter Read 2\nAGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT\n>polyA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n```\n\nThe adapter sequence in this file should be at least 6bp long, otherwise it will be skipped. And you can give whatever you want to trim, rather than regular sequencing adapters (i.e. polyA).\n\n`fastp` first trims the auto-detected adapter or the adapter sequences given by `--adapter_sequence | --adapter_sequence_r2`, then trims the adapters given by `--adapter_fasta` one by one.\n\nThe sequence distribution of trimmed adapters can be found at the HTML/JSON reports.\n\n# per read cutting by quality score\n`fastp` supports per read sliding window cutting by evaluating the mean quality scores in the sliding window. From `v0.19.6`, `fastp` supports 3 different operations, and you enable one or all of them:\n* `-5, --cut_front`             move a sliding window from front (5') to tail, drop the bases in the window if its mean quality is below cut_mean_quality, stop otherwise. Default is disabled. The leading N bases are also trimmed. Use `cut_front_window_size` to set the widnow size, and `cut_front_mean_quality` to set the mean quality threshold. If the window size is 1, this is similar as the Trimmomatic `LEADING` method.\n* `-3, --cut_tail`              move a sliding window from tail (3') to front, drop the bases in the window if its mean quality is below cut_mean_quality, stop otherwise. Default is disabled. The trailing N bases are also trimmed. Use `cut_tail_window_size` to set the widnow size, and `cut_tail_mean_quality` to set the mean quality threshold. If the window size is 1, this is similar as the Trimmomatic `TRAILING` method.\n* `-r, --cut_right`             move a sliding window from front to tail, if meet one window with mean quality < threshold, drop the bases in the window and the right part, and then stop. Use `cut_right_window_size` to set the widnow size, and `cut_right_mean_quality` to set the mean quality threshold.  This is similar as the Trimmomatic `SLIDINGWINDOW` method.\n\n\n***WARNING: all these three operations will interfere deduplication for SE data, and `--cut_front` or `--cut_right` may also interfere deduplication for PE data. The deduplication algorithms rely on the exact matchment of coordination regions of the grouped reads/pairs.***\n\nIf `--cut_right` is enabled, then there is no need to enable `--cut_tail`, since the former is more aggressive. If `--cut_right` is enabled together with `--cut_front`, `--cut_front` will be performed first before `--cut_right` to avoid dropping whole reads due to the low quality starting bases.\n\nPlease be noted that `--cut_front` will interfere deduplication for both PE/SE data, and `--cut_tail` will interfere deduplication for SE data, since the deduplication algorithms rely on the exact matchment of coordination regions of the grouped reads/pairs.\n\nIf you don't set window size and mean quality threshold for these function respectively, `fastp` will use the values from `-W, --cut_window_size` and `-M, --cut_mean_quality `\n\n# base correction for PE data\n`fastp` perform `overlap analysis` for PE data, which try to find an overlap of each pair of reads. If an proper overlap is found, it can correct mismatched base pairs in overlapped regions of paired end reads, if one base is with high quality while the other is with ultra low quality. If a base is corrected, the quality of its paired base will be assigned to it so that they will share the same quality.   \n\nThis function is not enabled by default, specify `-c` or `--correction` to enable it. This function is based on overlapping detection, which has adjustable parameters `overlap_len_require (default 30)`, `overlap_diff_limit (default 5)` and `overlap_diff_percent_limit (default 20%)`. Please note that the reads should meet these three conditions simultaneously.\n\n# global trimming\n`fastp` supports global trimming, which means trim all reads in the front or the tail. This function is useful since sometimes you want to drop some cycles of a sequencing run.\n\nFor example, the last cycle of Illumina sequencing is uaually with low quality, and it can be dropped with `-t 1` or `--trim_tail1=1` option.\n\n* For read1 or SE data, the front/tail trimming settings are given with `-f, --trim_front1` and `-t, --trim_tail1`.\n* For read2 of PE data, the front/tail trimming settings are given with `-F, --trim_front2` and `-T, --trim_tail2`. But if these options are not specified, they will be as same as read1 options, which means `trim_front2 = trim_front1` and `trim_tail2 = trim_tail1`.\n* If you want to trim the reads to maximum length, you can specify `-b, --max_len1` for read1, and `-B, --max_len2` for read2. If `--max_len1` is specified but `--max_len2` is not, `--max_len2` will be same as `--max_len1`. For example, if `--max_len1` is specified and read1 is longer than `--max_len1`, `fastp` will trim read1 at its tail to make it as long as `--max_len1`.\n\nPlease note that the trimming for `--max_len` limitation will be applied at the last step. Following are fastp's processing steps that may orderly affect the read lengthes:\n```\n1, UMI preprocessing (--umi)\n2, global trimming at front (--trim_front)\n3, global trimming at tail (--trim_tail)\n4, quality pruning at 5' (--cut_front)\n5, quality pruning by sliding window (--cut_right)\n6, quality pruning at 3' (--cut_tail)\n7, trim polyG (--trim_poly_g, enabled by default for NovaSeq/NextSeq data)\n8, trim adapter by overlap analysis (enabled by default for PE data)\n9, trim adapter by adapter sequence (--adapter_sequence, --adapter_sequence_r2. For PE data, this step is skipped if last step succeeded)\n10, trim polyX (--trim_poly_x)\n11, trim to max length (---max_len)\n```\n\n# polyG tail trimming\nFor Illumina NextSeq/NovaSeq data, `polyG` can happen in read tails since `G` means no signal in the Illumina two-color systems. `fastp` can detect the polyG in read tails and trim them. This feature is enabled for NextSeq/NovaSeq data by default, and you can specify `-g` or `--trim_poly_g` to enable it for any data, or specify `-G` or `--disable_trim_poly_g` to disable it. NextSeq/NovaSeq data is detected by the machine ID in the FASTQ records.  \n\nA minimum length can be set with `<poly_g_min_len>` for `fastp` to detect polyG. This value is 10 by default.\n\n# polyX tail trimming\nThis feature is similar as polyG tail trimming, but is disabled by default. Use `-x` or `--trim_poly_x` to enable it. A minimum length can be set with `<poly_x_min_len>` for `fastp` to detect polyX. This value is 10 by default.\n\nWhen `polyG tail trimming` and `polyX tail trimming` are both enabled, fastp will perform `polyG trimming` first, then perform `polyX trimming`. This setting is useful for trimming the tails having `polyX (i.e. polyA) ` before `polyG`. `polyG` is usually caused by sequencing artifacts, while `polyA` can be commonly found from the tails of mRNA-Seq reads.\n\n# unique molecular identifier (UMI) processing\nUMI is useful for duplication elimination and error correction based on generating consensus of reads originated from a same DNA fragment. It's usually used in deep sequencing applications like ctDNA sequencing. Commonly for Illumina platforms, UMIs can be integrated in two different places: `index` or head of `read`.  \nTo enable UMI processing, you have to enable `-U` or `--umi` option in the command line, and specify `--umi_loc`  to specify the UMI location, it can be one of:\n* `index1` the first index is used as UMI. If the data is PE, this UMI will be used for both read1/read2.\n* `index2` the second index is used as UMI. PE data only, this UMI will be used for both read1/read2.\n* `read1` the head of read1 is used as UMI. If the data is PE, this UMI will be used for both read1/read2.\n* `read2` the head of read2 is used as UMI. PE data only, this UMI will be used for both read1/read2.\n* `per_index` `index1_index2` is used as UMI for both read1/read2.\n* `per_read` define `umi1` as the head of read1, and `umi2` as the head of read2. `umi1_umi2` is used as UMI for both read1/read2.\n\nIf `--umi_loc` is specified with `read1`, `read2` or `per_read`, the length of UMI should specified with `--umi_len`.\n\n`fastp` will extract the UMIs, and append them to the first part of read names, so the UMIs will also be presented in SAM/BAM records. If the UMI is in the reads, then it will be shifted from read so that the read will become shorter. If the UMI is in the index, it will be kept.\n\nA prefix can be specified with `--umi_prefix`. If prefix is specified, an underline will be used to connect it and UMI. For example, UMI=AATTCCGG, prefix=UMI, then the final string presented in the name will be `UMI_AATTCCGG`.\n\nIf the UMI location is read1/read2/per_read, fastp can skip some bases after UMI to trim the UMI separator and A/T tailing. Specify `--umi_skip` to enable the number of bases to skip. By default it is not enabled.\n\n## UMI example\nThe original read:\n```\n@NS500713:64:HFKJJBGXY:1:11101:1675:1101 1:N:0:TATAGCCT+GACCCCCA\nAAAAAAAAGCTACTTGGAGTACCAATAATAAAGTGAGCCCACCTTCCTGGTACCCAGACATTTCAGGAGGTCGGGAAA\n+\n6AAAAAEEEEE/E/EA/E/AEA6EE//AEE66/AAE//EEE/E//E/AA/EEE/A/AEE/EEA//EEEEEEEE6EEAA\n```\nAfter it's processed with command: `fastp -i R1.fq -o out.R1.fq -U --umi_loc=read1 --umi_len=8`:  \n```\n@NS500713:64:HFKJJBGXY:1:11101:1675:1101:AAAAAAAA 1:N:0:TATAGCCT+GACCCCCA\nGCTACTTGGAGTACCAATAATAAAGTGAGCCCACCTTCCTGGTACCCAGACATTTCAGGAGGTCGGGAAA\n+\nEEE/E/EA/E/AEA6EE//AEE66/AAE//EEE/E//E/AA/EEE/A/AEE/EEA//EEEEEEEE6EEAA\n```\n\n# output splitting\nFor parallel processing of FASTQ files (i.e. alignment in parallel), `fastp` supports splitting the output into multiple files. The splitting can work with two different modes: `by limiting file number` or `by limiting lines of each file`. These two modes cannot be enabled together.   \n\nThe file names of these split files will have a sequential number prefix, adding to the original file name specified by `--out1` or `--out2`, and the width of the prefix is controlled by the `-d` or `--split_prefix_digits` option. For example, `--split_prefix_digits=4`, `--out1=out.fq`, `--split=3`, then the output files will be `0001.out.fq`,`0002.out.fq`,`0003.out.fq`\n\n## splitting by limiting file number\nUse `-s` or `--split` to specify how many files you want to have. `fastp` evaluates the read number of a FASTQ by reading its first ~1M reads. This evaluation is not accurate so the file sizes of the last several files can be a little differnt (a bit bigger or smaller). For best performance, it is suggested to specify the file number to be a multiple of the thread number.\n\n## splitting by limiting the lines of each file\nUse `-S` or `--split_by_lines` to limit the lines of each file. The last files may have smaller sizes since usually the input file cannot be perfectly divided. The actual file lines may be a little greater than the value specified by `--split_by_lines` since `fastp` reads and writes data by blocks (a block = 1000 reads).\n\n# overrepresented sequence analysis\nOverrepresented sequence analysis is disabled by default, you can specify `-p` or `--overrepresentation_analysis` to enable it. For consideration of speed and memory, `fastp` only counts sequences with length of 10bp, 20bp, 40bp, 100bp or (cycles - 2 ).\n\nBy default, fastp uses 1/20 reads for sequence counting, and you can change this settings by specifying `-P` or `--overrepresentation_sampling` option. For example, if you set `-P 100`, only 1/100 reads will be used for counting, and if you set `-P 1`, all reads will be used but it will be extremely slow. The default value 20 is a balance of speed and accuracy.\n\n`fastp` not only gives the counts of overrepresented sequence, but also gives the information that how they distribute over cycles. A figure is provided for each detected overrepresented sequence, from which you can know where this sequence is mostly found.\n\n# merge paired-end reads\nFor paired-end (PE) input, fastp supports stiching them by specifying the `-m/--merge` option. In this `merging` mode:\n\n* `--merged_out` shouuld be given to specify the file to store merged reads, otherwise you should enable `--stdout` to stream the merged reads to STDOUT. The merged reads are also filtered.\n* `--out1` and `--out2` will be the reads that cannot be merged successfully, but both pass all the filters.\n* `--unpaired1` will be the reads that cannot be merged, `read1` passes filters but `read2` doesn't.\n* `--unpaired2` will be the reads that cannot be merged, `read2` passes filters but `read1` doesn't.\n* `--include_unmerged` can be enabled to make reads of `--out1`, `--out2`, `--unpaired1` and `--unpaired2` redirected to `--merged_out`. So you will get a single output file. This option is disabled by default.\n\n`--failed_out` can still be given to store the reads (either merged or unmerged) failed to passing filters.\n\nIn the output file, a tag like `merged_xxx_yyy`will be added to each read name to indicate that how many base pairs are from read1 and from read2, respectively. For example, `\n@NB551106:9:H5Y5GBGX2:1:22306:18653:13119 1:N:0:GATCAG merged_150_15`\nmeans that 150bp are from read1, and 15bp are from read2. `fastp` prefers the bases in read1 since they usually have higher quality than read2.\n\nSame as the [base correction feature](#base-correction-for-pe-data), this function is also based on overlapping detection, which has adjustable parameters `overlap_len_require (default 30)`, `overlap_diff_limit (default 5)` and `overlap_diff_percent_limit (default 20%)`. Please note that the reads should meet these three conditions simultaneously.\n\n# duplication rate and deduplication\nFor both SE and PE data, fastp supports evaluating its duplication rate and removing duplicated reads/pairs. fastp considers one read as duplicated only if its all base pairs are identical as another one. This meas if there is a sequencing error or an N base, the read will not be treated as duplicated.\n\n## duplication rate evaluation\nBy default, fastp evaluates duplication rate, and this module may use 1G memory and take 10% ~ 20% more running time. If you don't need the duplication rate information, you can set `--dont_eval_duplication` to disable the duplication evaluation. But please be noted that, if deduplication (`--dedup`) option is enabled, then `--dont_eval_duplication` option is ignored.\n\nfastp uses a hash algorithm to find the identical sequences. Due to the possible hash collision, about 0.01% of the total reads may be wrongly recognized as deduplicated reads. Normally this may not impact the downstream analysis. The accuracy of calculating duplication can be improved by increasing the hash buffer number or enlarge the buffer size. The option `--dup_calc_accuracy` can be used to specify the level (1 ~ 6). The higher level means more memory usage and more running time. Please refer to following table:\n\n| dup_calc_accuracy level | hash  buffer number | buffer size | memory usage | speed | |\n|- | - | - | - | - | - |\n| 1 | 1 | 1G | 1G | ultra-fast | default for no-dedup mode |\n| 2 | 1 | 2G | 2G | fast | |\n| 3 | 2 | 2G | 4G | fast | default for dedup|\n| 4 | 2 | 4G | 8G | fast | |\n| 5 | 2 | 8G | 12G | fast | |\n| 6 | 3 | 8G | 24G | moderate | |\n\n## deduplication\nSince `v0.22.0`, fastp supports deduplication for FASTQ data. Specify `-D` or `--dedup` to enable this option. When `--dedup` is enabled, the `dup_calc_accuracy` level is default to `3`, and it can be changed to any value of 1 ~ 6.\n\n\n# all options\n```shell\nusage: fastp -i <in1> -o <out1> [-I <in1> -O <out2>] [options...]\noptions:\n  # I/O options\n  -i, --in1                          read1 input file name (string)\n  -o, --out1                         read1 output file name (string [=])\n  -I, --in2                          read2 input file name (string [=])\n  -O, --out2                           read2 output file name (string [=])\n      --unpaired1                      for PE input, if read1 passed QC but read2 not, it will be written to unpaired1. Default is to discard it. (string [=])\n      --unpaired2                      for PE input, if read2 passed QC but read1 not, it will be written to unpaired2. If --unpaired2 is same as --unpaired1 (default mode), both unpaired reads will be written to this same file. (string [=])\n      --failed_out                     specify the file to store reads that cannot pass the filters. (string [=])\n      --overlapped_out                 for each read pair, output the overlapped region if it has no any mismatched base. (string [=])\n  -m, --merge                          for paired-end input, merge each pair of reads into a single read if they are overlapped. The merged reads will be written to the file given by --merged_out, the unmerged reads will be written to the files specified by --out1 and --out2. The merging mode is disabled by default.\n      --merged_out                     in the merging mode, specify the file name to store merged output, or specify --stdout to stream the merged output (string [=])\n      --include_unmerged               in the merging mode, write the unmerged or unpaired reads to the file specified by --merge. Disabled by default.\n  -6, --phred64                      indicate the input is using phred64 scoring (it'll be converted to phred33, so the output will still be phred33)\n  -z, --compression                  compression level for gzip output (1 ~ 9). 1 is fastest, 9 is smallest, default is 4. (int [=4])\n      --stdin                          input from STDIN. If the STDIN is interleaved paired-end FASTQ, please also add --interleaved_in.\n      --stdout                         output passing-filters reads to STDOUT. This option will result in interleaved FASTQ output for paired-end input. Disabled by default.\n      --interleaved_in                 indicate that <in1> is an interleaved FASTQ which contains both read1 and read2. Disabled by default.\n      --reads_to_process             specify how many reads/pairs to be processed. Default 0 means process all reads. (int [=0])\n      --dont_overwrite               don't overwrite existing files. Overwritting is allowed by default.\n      --fix_mgi_id                     the MGI FASTQ ID format is not compatible with many BAM operati",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/openframeworks/openFrameworks,https://github.com/openframeworks/openFrameworks,0,,0,0,1,0,0,0,0,1,0,0,0,0,openFrameworks is a community-developed cross platform toolkit for creative coding in C++.,"[openFrameworks](http://openframeworks.cc/)\n================\n\nopenFrameworks is a C++ toolkit for creative coding.  If you are new to OF, welcome!\n\n- [Grab the Nightly Release (to avoid submodule cloning)](https://github.com/openframeworks/openFrameworks/releases)\n- [Setup guides for different Platforms](https://openframeworks.cc/download/)\n- [Join Our Slack](https://join.slack.com/t/openframeworks/shared_invite/zt-1r2brqms0-dZMMFZgZhFTgomjJ0vlCjA)\n- [Discuss on the Forum](https://forum.openframeworks.cc) \n- [Follow OF on Mastodon](https://fosstodon.org/@openframeworks)\n\n## Build status\n\n* The **master** branch contains the newest, most recently updated code. This code is packaged and available for download in the ""Nightly Builds"" section of [openframeworks.cc/download](https://openframeworks.cc/download/).\n* The **stable** branch contains the code corresponding to the last stable openFrameworks release. This stable code is packaged and available for download at [openframeworks.cc/download](https://openframeworks.cc/download/).\n\nPlatform                     | Master branch  | Stable branch\n-----------------------------|:---------|:---------\nWindows MSYS2        | [![Build status](https://github.com/openframeworks/openFrameworks/workflows/build-msys2/badge.svg)](https://github.com/openframeworks/openFrameworks/actions) | [![Build status](https://github.com/openframeworks/openFrameworks/workflows/build-msys2/badge.svg?branch=stable)](https://github.com/openframeworks/openFrameworks/actions)\nWindows Visual Studio  | [![Build status](https://github.com/openframeworks/openFrameworks/workflows/build-vs/badge.svg)](https://github.com/openframeworks/openFrameworks/actions) | [![Build status](https://github.com/openframeworks/openFrameworks/workflows/build-vs/badge.svg?branch=stable)](https://github.com/openframeworks/openFrameworks/actions)\nLinux 64 & Arm                    | [![Linux Build Status](https://github.com/openframeworks/openFrameworks/workflows/build-linux64-and-arm/badge.svg)](https://github.com/openframeworks/openFrameworks/actions) | [![Linux Build Status](https://github.com/openframeworks/openFrameworks/workflows/build-linux64-and-arm/badge.svg?branch=stable)](https://github.com/openframeworks/openFrameworks/actions)\nEmscripten                   | [![Emscripten Build Status](https://github.com/openframeworks/openFrameworks/workflows/build-emscripten/badge.svg)](https://github.com/openframeworks/openFrameworks/actions) | [![Emscripten Build Status](https://github.com/openframeworks/openFrameworks/workflows/build-emscripten/badge.svg?branch=stable)](https://github.com/openframeworks/openFrameworks/actions) \nmacos                        | [![macos Build Status](https://github.com/openframeworks/openFrameworks/workflows/build-macos/badge.svg)](https://github.com/openframeworks/openFrameworks/actions) | [![macos Build Status](https://github.com/openframeworks/openFrameworks/workflows/build-macos/badge.svg?branch=stable)](https://github.com/openframeworks/openFrameworks/actions)\niOS & tvOS                         | [![iOS tvOS Build Status](https://github.com/openframeworks/openFrameworks/workflows/build-ios-tvos/badge.svg)](https://github.com/openframeworks/openFrameworks/actions) | [![iOS tvOS Build Status](https://github.com/openframeworks/openFrameworks/workflows/build-ios-tvos/badge.svg?branch=stable)](https://github.com/openframeworks/openFrameworks/actions)\n\n<!-- Android Arm7                 | [![Android Build Status](https://github.com/openframeworks/openFrameworks/workflows/build-android/badge.svg)](https://github.com/openframeworks/openFrameworks/actions) | [![Android Build Status](https://github.com/openframeworks/openFrameworks/workflows/build-android/badge.svg?branch=stable)](https://github.com/openframeworks/openFrameworks/actions) -->\n\n\n\n## folder structure\n\nThis release of OF comes with several folders:\n\n* addons\n* apps\n* docs\n* examples\n* export (on some systems)\n* libs\n* other\n* scripts\n* projectGenerator\n\n\n`docs` has some documentation around OF usage, per platform things to consider, etc. You should definitely take a look in there; for example, if you are on OSX, read the osx.md.   `apps` and `examples` are where projects go -- `examples` contains a variety of projects that show you how to use OF, and `apps` is where your own projects will go.  `libs` contains the libraries that OF uses, including the openframeworks core itself.  `addons` are for additional functionality that's not part of the core.  `export` is for DLLs and dylibs that need to be put in each compiled project.  The `scripts` folder has the templates and small scripts for automating OF per platform. `project generator` is a GUI based tool for making new projects - this folder is only there in packaged releases.  \n\nOne idea that's important is that OF releases are designed to be self-contained.  You can put them anywhere on your hard drive, but it's not possible to mix different releases of OF together, so please keep each release (0.8.0, 0.8.1) separate.  Projects may generally work from release to release, but this is not guaranteed.  Because OF is self-contained, there's extensive use of local file paths (ie, ../../../) throughout OF.  It's important to be aware of how directories are structured.  A common error is to take a project and move it so that it's a level below or above where it used to be compared to the root of OF.  This means that links such as ../../../libs will break.  \n\n## Get involved\n\nThe openframeworks forum:\n\n[http://forum.openframeworks.cc/](http://forum.openframeworks.cc/)\n\nis a warm and friendly place.  Please ask or answer a question.  The most important part of this project is that it's a community, more than just a tool, so please join us!  Also, this is free software, and we learn so much about what is hard, what doesn't make sense, what is useful, etc. The most basic questions are acceptable here!  Don't worry, just join the conversation.  Learning in OF is social, it's hard to do it alone, but together we can get far!\n\nOur GitHub site is active:\n\n[https://github.com/openframeworks/openFrameworks](https://github.com/openframeworks/openFrameworks)\n\nif you have bugs or feature requests, consider opening an issue.  If you are a developer and want to help, pull requests are warmly welcome.  Please read the contributing guide for guidelines:\n\n[https://github.com/openframeworks/openFrameworks/blob/master/CONTRIBUTING.md](https://github.com/openframeworks/openFrameworks/blob/master/CONTRIBUTING.md\n)\n\nWe also have a developer's mailing list, which is useful for discussing issues around the development and future of OF.\n\n## Developers\n\nTo grab a copy of openFrameworks for your platform, check the [download page](http://openframeworks.cc/download) on the main site.  \n\nIf you are working with the Git repository, the `stable` branch of the OF repository corresponds to the most recent release, with a few important differences:  \n\n1. The release includes a simple openFrameworks project generator.\n2. This GitHub repository contains code and libs for all the platforms, but the releases are done on a per-platform basis.\n3. This GitHub repository has no project files for the different examples. They are generated automatically for each release using a tool in `apps/projectGenerator/`.\n4. There are no external dependencies in this repository, you can download them using the download_libs.sh script for each platform in the particular platform folder inside scripts.\n\nIf you want to work with the openFrameworks GitHub repository, you need to download the external dependencies and you should use the project generator to create project files for all the code in `examples/`.  To generate the project files with the project generator enable the 'Advanced Options' in the settings tab, then use 'Update Multiple' to update the projects for the `examples/` folder path in the repo.\n\nTo set up the project generator submodule within the OF repo, use the command `git submodule init` then `git submodule update` whilst inside the openFrameworks repo.\n\nFor more info on working with the Project Generator, for per-platform readmes, and more information, see the [documentation](docs/table_of_contents.md).\n\n## Versioning\n\nopenFrameworks uses [Semantic Versioning](http://semver.org/), although strict adherence will only come into effect at version 1.0.0.\n",9843,graphics,C++,16,C++,Java,C,Makefile,JavaScript,Shell,Objective-C++,Objective-C,GLSL,CSS,HTML,Python,Batchfile,QML,PowerShell,CMake,,,,,,,,,,,,,3880,742,3009,129,10,319,0,2203191,2555,4109,3144,965,cbb57f2b70385269cc6dd745f2bb8ed3e5a54b43,Fix dangling pointer in ofXml (#8055),2024-07-19T14:22:39Z,Olivier XILLO,oxillo@users.noreply.github.com,oxillo,0.12.0,update pg,0.12.0,,,github-actions[bot],Other,openFrameworks,openframeworks,5,openframeworks,osx,windows,linux,raspberry-pi,android,ios,creative-coding,video,audio,opengl,graphics,computer-vision,opencv,emscripten,,,,,,/openframeworks/openFrameworks,40,515,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/OpenChemistry/avogadrolibs,https://github.com/OpenChemistry/avogadrolibs,1,,,1,1,1,1,0,0,0,0,0,0,1,"Avogadro libraries provide 3D rendering, visualization, analysis and data processing useful in computational chemistry, molecular modeling, bioinformatics, materials science, and related areas.","# ![Avogadro 2][Avogadro2Logo] Avogadro 2\n\n[![Latest Release](https://img.shields.io/github/v/release/openchemistry/avogadrolibs)](https://github.com/OpenChemistry/avogadrolibs/releases) [![BSD License](https://img.shields.io/github/license/openchemistry/avogadrolibs)](https://github.com/OpenChemistry/avogadrolibs/blob/master/LICENSE) [![Build Status](https://img.shields.io/github/actions/workflow/status/openchemistry/avogadrolibs/build_cmake.yml?branch=master)](https://github.com/OpenChemistry/avogadrolibs/actions) [![Codacy Badge](https://app.codacy.com/project/badge/Grade/44bb12662c564ed8a27ee8a7fd89ed50)](https://app.codacy.com/gh/OpenChemistry/avogadrolibs/dashboard?utm_source=gh&utm_medium=referral&utm_content=&utm_campaign=Badge_grade)  \n[![Download Count](https://avogadro.cc/downloads.svg?readme)](https://github.com/OpenChemistry/avogadrolibs/releases) [![Citation Count](https://avogadro.cc/citations.svg?readme)](http://doi.org/10.1186/1758-2946-4-17)  \n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat)](http://makeapullrequest.com) [![GitHub contributors](https://img.shields.io/github/contributors/openchemistry/avogadrolibs.svg?style=flat&color=0bf)](https://github.com/OpenChemistry/avogadrolibs/graphs/contributors)  [![OpenCollective Backers](https://img.shields.io/opencollective/all/open-chemistry)](https://opencollective.com/open-chemistry)\n\n## Introduction\n\nAvogadro is an advanced molecular editor designed for cross-platform use in\ncomputational chemistry, molecular modeling, bioinformatics, materials science,\nand related areas. It offers flexible rendering and a powerful plugin\narchitecture.\n\nCore features and goals of the Avogadro project include:\n\n* Open source distributed under the liberal 3-clause BSD license\n* Cross platform with builds on Linux, Mac OS X and Windows\n* Intuitive interface designed to be useful to whole community\n* Fast and efficient embracing the latest technologies\n* Extensible, making extensive use of a plugin architecture\n* Flexible supporting a range of chemical data formats and packages\n\nThe code in this repository is a rewrite of Avogadro with source\ncode split across a\n[libraries repository](https://github.com/openchemistry/avogadrolibs)\nand an [application repository](https://github.com/openchemistry/avogadroapp).\nThe new code architecture provides a high-performance rendering engine, modern\ncode development, and significantly improved speed and stability.\n\nAvogadro 2 is being developed as part of the [Open Chemistry][OpenChemistry]\nproject by an open community, and was started at [Kitware][Kitware] as\nan open source community project. The Avogadro 1.x series currently has more\nfeatures, and can be found [here][Avogadro1]. We are actively porting more\nfeatures to the Avogadro 2 code base, and making regular releases to get\nfeedback from the community.\n\n## Installing\n\nWe provide nightly binaries built by GitHub actions for:\n\n* [Linux AppImage](https://nightly.link/OpenChemistry/avogadrolibs/workflows/build_cmake/master/Avogadro2.AppImage.zip)\n* [MacOS](https://nightly.link/OpenChemistry/avogadrolibs/workflows/build_cmake/master/macOS.dmg.zip)\n* [Windows 64-bit](https://nightly.link/OpenChemistry/avogadrolibs/workflows/build_cmake/master/Win64.exe.zip)\n\nIf you would like to build from source we recommend that you\nfollow our [building Open Chemistry][Build] guide that will take care of\nbuilding most dependencies.\n\n## Contributing\n\nWe welcome *all* kinds of contributions as a community project, from bug\nreports, feature suggestions, language translations, Python plugins,\nand C++ code development.\n\nOur project uses the standard GitHub pull request process for code review\nand integration. Please check our [contribution][Contribution] guide for more\ndetails on developing and contributing to the project. The [GitHub issue\ntracker](https://github.com/openchemistry/avogadrolibs/issues/)\ncan be used to report bugs, make feature requests, etc. Our API is\n[documented online][API] with updated documentation generated nightly.\n\nTo introduce yourself, ask for help, or general discussion, we welcome everyone\nto our [forum](https://discuss.avogadro.cc/)\n\nContributors Hall of Fame:\n<a href=""https://github.com/openchemistry/avogadrolibs/graphs/contributors"">\n  <img src=""https://contrib.rocks/image?repo=openchemistry/avogadrolibs"" />\n</a>\n\n  [Avogadro2Logo]: https://raw.githubusercontent.com/OpenChemistry/avogadrolibs/master/docs/avogadro2_64.png ""Avogadro2""\n  [OpenChemistry]: http://openchemistry.org/ ""Open Chemistry Project""\n  [OpenChemistryLogo]: https://raw.githubusercontent.com/OpenChemistry/avogadrolibs/master/docs/OpenChemistry128.png ""Open Chemistry""\n  [Kitware]: http://kitware.com/ ""Kitware, Inc.""\n  [Avogadro1]: http://avogadro.cc/ ""Avogadro 1""\n  [Build]: https://two.avogadro.cc/install/build.html ""Building Avogadro""\n  [Contribution]: https://two.avogadro.cc/contrib/ ""Contribution guide""\n  [API]: https://two.avogadro.cc/api/ ""API documentation""\n",421,chemistry,C++,7,CMake,C++,Python,GLSL,Shell,Perl,C,,,,,,,,,,,,,,,,,,,,,,1216,155,1046,15,3,139,0,58275,163,464,355,109,866646752ccb9cd5890446733f768a43b115bf4a,Merge pull request #1681 from weblate/weblate-avogadro-avogadrolibs,2024-07-05T13:47:40Z,Geoff Hutchison,geoff.hutchison@gmail.com,ghutchis,Avogadro 1.99.0,"## 🌟 Highlights (tldr)\r\n\r\n- Further improvements to the new optimization framework, including default integrated Open Babel force fields (MMFF94, UFF, GAFF)\r\n- New toolbar icons with light / dark theme from @matterhorn103\r\n- Significantly faster molecular and orbital surfaces\r\n- Vibrational spectra plotting\r\n- Support for installing Python packages with plugins via `pip` or `conda`\r\n- Improved selection of `conda` environments\r\n- Conformer search dialog through Open Babel\r\n- Improved template tool for inserting ligands and functional groups\r\n	- Significant improvements from @nbehrnd for the ligand library\r\n- Logging debugging / error messages to a file for Windows users\r\n\r\n## ✨ Features\r\n\r\n- Add optional energy / optimize code that links Open Babel @ghutchis (#1591)\r\n- Update tool plugins to set icon as light / dark theme @ghutchis (#1578)\r\n- Modern tool icons @matterhorn103 (#1576)\r\n- Add confirmation dialog to install with conda or pip @ghutchis (#1559)\r\n- Add support for first launch dialog and conda environments @ghutchis (#1562)\r\n- Add red = a, blue = b, green = c color axes for unit cells @ghutchis (#1542)\r\n- Vibrational spectra plot @ghutchis (#1429)\r\n- Add UniqueID atom label type @ghutchis (#1569)\r\n- Add ethylene, ethyne, and phosphate standard functional groups @ghutchis (#1557)\r\n- Improved python configure dialog @ghutchis (#1555)\r\n- Adding support for bond label in base classes and CJSON @ghutchis (#1495)\r\n- Add ""…"" indicator for display types with settings @ghutchis (#1541)\r\n- Split ligands and functional groups in the template tool @ghutchis (#1516)\r\n- Added copy and export feature to property tables @Surajjalpun2002 (#1515)\r\n- Add initial support for copy from tables @ghutchis (#1506)\r\n- Add conformer search box @ghutchis (#1507)\r\n- Allow scripts to add properties (orbitals, vibrations, cubes) @ghutchis (#1479)\r\n- Add support for navigator commands - rotate, translate, zoom @ghutchis (#1472)\r\n- Add template library to insert ligands or functional groups @ghutchis (#1456)\r\n- Add support for Hall number and space group to CJSON read/write @ghutchis (#1440)\r\n- Add new MessagePack version of CJSON @ghutchis (#1452)\r\n- Add a message handler to grab debug / warnings on Windows @ghutchis\r\n- Save and load the camera modelView and projection matrix @ghutchis\r\n\r\n## 🐛 Bug Fixes\r\n\r\n- Fixed typo in setDefaultPythonInterpretor() @matterhorn103 (#1583)\r\n- Fix Mac builds @ghutchis (#1522)\r\n- Fix quantum surface max cutoff for diffuse functions @ghutchis (#1556)\r\n- Change ""Insert fragment"" ⇒ ""Insert molecule"" by popular request @ghutchis (#1519)\r\n- Fix parsing XYZ files with tabs between columns @ghutchis (#1513)\r\n- Fix bug with incorrect Unicode characters added to labels @ghutchis (#1588)\r\n- Fix for Wayland @matterhorn103 (#1577)\r\n- Fixed crash with angle properties on an empty molecule @secretkontributer (#1566)\r\n- Fix density color crash @ghutchis (#1537)\r\n- Fix crash in forcefields - check if method is valid before using it @ghutchis (#1526)\r\n- Fix for first item in ordered plugin dialog being empty @matterhorn103 (#1523)\r\n- Fix crash with ""copy as"" and an empty molecule @ghutchis (#1521)\r\n- Turn off Color Opacity Map unless a VTK widget is active @ghutchis (#1509)\r\n- Make sure to automatically load the ""Meshes"" display type @ghutchis (#1508)\r\n- If no partial charges are assigned, set them @ghutchis (#1502)\r\n- Fix potential crashes in selection commands @ghutchis (#1499)\r\n- Fix vibration animation @ghutchis (#1487)\r\n- Generate the density matrix if needed for the electron density surface @ghutchis (#1482)\r\n- Fix empty window showing up for commands without an option dialog @ghutchis (#1468)\r\n- Save partial charges and properly read them from CJSON @ghutchis (#1467)\r\n- Don't show PNG files in the filter dialog @ghutchis (#1462)\r\n- Fix GFN-FF energy - redirect GFN-FF output through a hack @ghutchis (#1454)\r\n- Fixup PDB reading with non-standard MD files (no element column) @ghutchis (#1450)\r\n- Fix atomic numbers from Orca - it would read electrons not symbols @ghutchis (#1451)\r\n- Fix linear molecular template @ghutchis (#1474)\r\n- Avoid segfaulting while manipulating carbon bonds/hydrogens @Makiah (#1493)\r\n- Reduce ambiguity of ""Export"" toolbar button @matterhorn103 (#442)\r\n- Ensure that title bar correctly displays active molecule file @Makiah (#440)\r\n\r\n## 🐍 Scripting / Plugin Improvements\r\n\r\n- Windows: Standardize plugin location based on forum feedback @ghutchis (#1605)\r\n- Add some additional Python classes including cjson and connect @ghutchis (#1427)\r\n- Always supply cjson to scripts @ghutchis (#1465)\r\n- If userOptions specifies an order, use that to sort the form @ghutchis (#1503)\r\n- Check script --menupath for {} priority numbers @ghutchis (#1501)\r\n- Add a ""text"" option for scripts to add text labels / help @ghutchis (#1488)\r\n\r\n## 🚀 Performance Improvements\r\n\r\n- Skip calculating orbital / surface points too far apart (e.g., negligible) @ghutchis (#1551)\r\n    - Leads to 2-3x faster surface generation\r\n\r\n## 🧰 Builds / Maintenance\r\n\r\n- Package OpenSSL for Windows @ghutchis\r\n- Update Eigen cmake header references @ghutchis (#1544)\r\n- Add Qt6 build tests @ghutchis (#1547)\r\n- Update to cibuildwheel for Python 3.12 @ghutchis (#1546)\r\n- Fix include of avogadrocoreexport.h @ghutchis (#1504)\r\n- Add specialization for char\* and MatrixXf types @ghutchis (#1494)\r\n- Install ligand and functional group fragments @ghutchis (#1460)\r\n- Make sure to copy libopenbabel on Mac for CPack @ghutchis\r\n\r\n## 📚 Translations\r\n\r\n- Correct typo @Acylation (#1553)\r\n- Update Australian and Canadian localization from GB version @ghutchis (#1453)\r\n- Translations update from Hosted Weblate @weblate\r\n- Automated translation updates @github-actions\r\n- Fix a few remaining cases of .. instead of ellipsis character @ghutchis (#1458)\r\n\r\n## Credits\r\n\r\nThanks to many contributors, including: @Acylation, @Azaathooth, @IagoEmanuel15, @ImgBotApp, @Makiah, @NorwayFun, @Surajjalpun2002, @alchemistcai, @bitigchi, @dependabot, @dependabot[bot], @ghutchis, @github-actions, @hakkikonu, @imgbot, @imitrichev, @joerivan, @koenr, @matterhorn103, @nbehrnd, @ovari, @research11111, @secretkontributer, @tacitcoast, @weblate, Alejandro Díaz-Moscoso, Eisuke Kawashima, Ivanushka, LibreTranslate, Remus-Gabriel Chelu, Weblate Translation Memory and gallegonovato",1.99.0,,,github-actions[bot],"BSD 3-Clause ""New"" or ""Revised"" License",avogadrolibs,OpenChemistry,11,chemistry,scientific-computing,open-source,open-science,opengl,visualization,desktop,compchem,avogadro,computational-chemistry,openchemistry,hacktoberfest,cross-platform,qt5,,,,,,,/OpenChemistry/avogadrolibs,21,27,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/OpenChemistry/avogadroapp,https://github.com/OpenChemistry/avogadroapp,1,,,1,1,1,1,0,0,0,0,0,0,1,"Avogadro is an advanced molecular editor designed for cross-platform use in computational chemistry, molecular modeling, bioinformatics, materials science, and related areas.","# ![Avogadro 2][Avogadro2Logo] Avogadro 2\n\n[![Latest Release](https://img.shields.io/github/v/release/openchemistry/avogadrolibs)](https://github.com/OpenChemistry/avogadrolibs/releases) [![BSD License](https://img.shields.io/github/license/openchemistry/avogadrolibs)](https://github.com/OpenChemistry/avogadrolibs/blob/master/LICENSE) [![Build Status](https://img.shields.io/github/workflow/status/openchemistry/avogadrolibs/CMake%20Build%20Matrix)](https://github.com/OpenChemistry/avogadrolibs/actions) [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) [![GitHub contributors](https://img.shields.io/github/contributors/openchemistry/avogadrolibs.svg?style=flat)](https://github.com/OpenChemistry/avogadrolibs/graphs/contributors)  [![OpenCollective Backers](https://img.shields.io/opencollective/all/open-chemistry)](https://opencollective.com/open-chemistry)\n\n## Introduction\n\nAvogadro is an advanced molecular editor designed for cross-platform use in\ncomputational chemistry, molecular modeling, bioinformatics, materials science,\nand related areas. It offers flexible rendering and a powerful plugin\narchitecture.\n\nCore features and goals of the Avogadro project include:\n\n* Open source distributed under the liberal 3-clause BSD license\n* Cross platform with builds on Linux, Mac OS X and Windows\n* Intuitive interface designed to be useful to whole community\n* Fast and efficient embracing the latest technologies\n* Extensible, making extensive use of a plugin architecture\n* Flexible supporting a range of chemical data formats and packages\n\nThe code in this repository is a rewrite of Avogadro with source\ncode split across a\n[libraries repository](https://github.com/openchemistry/avogadrolibs)\nand an [application repository](https://github.com/openchemistry/avogadroapp).\nThe new code architecture provides a high-performance rendering engine, modern\ncode development, and significantly improved speed and stability.\n\nAvogadro 2 is being developed as part of the [Open Chemistry][OpenChemistry]\nproject by an open community, and was started at [Kitware][Kitware] as\nan open source community project. The Avogadro 1.x series currently has more\nfeatures, and can be found [here][Avogadro1]. We are actively porting more\nfeatures to the Avogadro 2 code base, and making regular releases to get\nfeedback from the community.\n\nWe are actively working to finish Avogadro 2.0 in 2022.\n\n## Installing\n\nWe provide nightly binaries built by GitHub actions for:\n\n* [Linux AppImage](https://nightly.link/OpenChemistry/avogadrolibs/workflows/build_cmake/master/Avogadro2.AppImage.zip)\n* [MacOS](https://nightly.link/OpenChemistry/avogadrolibs/workflows/build_cmake/master/macOS.dmg.zip)\n* [Windows 64-bit](https://nightly.link/OpenChemistry/avogadrolibs/workflows/build_cmake/master/Win64.exe.zip)\n\nIf you would like to build from source we recommend that you\nfollow our [building Open Chemistry][Build] guide that will take care of\nbuilding most dependencies.\n\n## Contributing\n\nWe welcome *all* kinds of contributions as a community project, from bug\nreports, feature suggestions, language translations, Python plugins,\nand C++ code development.\n\nOur project uses the standard GitHub pull request process for code review\nand integration. Please check our [contribution][Contribution] guide for more\ndetails on developing and contributing to the project. The [GitHub issue\ntracker](https://github.com/openchemistry/avogadrolibs/issues/)\ncan be used to report bugs, make feature requests, etc. Our API is\n[documented online][API] with updated documentation generated nightly.\n\nTo introduce yourself, ask for help, or general discussion, we welcome everyone\nto our [forum](https://discuss.avogadro.cc/)\n\nContributors Hall of Fame:\n<a href=""https://github.com/openchemistry/avogadrolibs/graphs/contributors"">\n  <img src=""https://contrib.rocks/image?repo=openchemistry/avogadrolibs"" />\n</a>\n\n  [Avogadro2Logo]: https://raw.githubusercontent.com/OpenChemistry/avogadrolibs/master/docs/avogadro2_64.png ""Avogadro2""\n  [OpenChemistry]: http://openchemistry.org/ ""Open Chemistry Project""\n  [OpenChemistryLogo]: https://raw.githubusercontent.com/OpenChemistry/avogadrolibs/master/docs/OpenChemistry128.png ""Open Chemistry""\n  [Kitware]: http://kitware.com/ ""Kitware, Inc.""\n  [Avogadro1]: http://avogadro.cc/ ""Avogadro 1""\n  [Build]: https://two.avogadro.cc/install/build.html ""Building Avogadro""\n  [Contribution]: https://two.avogadro.cc/contrib/ ""Contribution guide""\n  [API]: https://two.avogadro.cc/api/ ""API documentation""\n",181,chemistry,C++,6,CMake,C++,Python,Shell,Objective-C++,Perl,,,,,,,,,,,,,,,,,,,,,,,378,49,321,8,3,74,0,8573,69,85,75,10,a8790e5350b62db5b50a1584ddbc4af13d95e3f5,Merge pull request #495 from weblate/weblate-avogadro-avogadroapp,2024-07-01T19:01:11Z,Geoff Hutchison,geoff.hutchison@gmail.com,ghutchis,Avogadro 1.99.0,"## 🌟 Highlights (tldr)\r\n\r\n- Further improvements to the new optimization framework, including default integrated Open Babel force fields (MMFF94, UFF, GAFF)\r\n- New toolbar icons with light / dark theme from @matterhorn103\r\n- Significantly faster molecular and orbital surfaces\r\n- Vibrational spectra plotting\r\n- Support for installing Python packages with plugins via `pip` or `conda`\r\n- Improved selection of `conda` environments\r\n- Conformer search dialog through Open Babel\r\n- Improved template tool for inserting ligands and functional groups\r\n	- Significant improvements from @nbehrnd for the ligand library\r\n- Logging debugging / error messages to a file for Windows users\r\n\r\n## ✨ Features\r\n\r\n- Add optional energy / optimize code that links Open Babel @ghutchis (#1591)\r\n- Update tool plugins to set icon as light / dark theme @ghutchis (#1578)\r\n- Modern tool icons @matterhorn103 (#1576)\r\n- Add confirmation dialog to install with conda or pip @ghutchis (#1559)\r\n- Add support for first launch dialog and conda environments @ghutchis (#1562)\r\n- Add red = a, blue = b, green = c color axes for unit cells @ghutchis (#1542)\r\n- Vibrational spectra plot @ghutchis (#1429)\r\n- Add UniqueID atom label type @ghutchis (#1569)\r\n- Add ethylene, ethyne, and phosphate standard functional groups @ghutchis (#1557)\r\n- Improved python configure dialog @ghutchis (#1555)\r\n- Adding support for bond label in base classes and CJSON @ghutchis (#1495)\r\n- Add ""…"" indicator for display types with settings @ghutchis (#1541)\r\n- Split ligands and functional groups in the template tool @ghutchis (#1516)\r\n- Added copy and export feature to property tables @Surajjalpun2002 (#1515)\r\n- Add initial support for copy from tables @ghutchis (#1506)\r\n- Add conformer search box @ghutchis (#1507)\r\n- Allow scripts to add properties (orbitals, vibrations, cubes) @ghutchis (#1479)\r\n- Add support for navigator commands - rotate, translate, zoom @ghutchis (#1472)\r\n- Add template library to insert ligands or functional groups @ghutchis (#1456)\r\n- Add support for Hall number and space group to CJSON read/write @ghutchis (#1440)\r\n- Add new MessagePack version of CJSON @ghutchis (#1452)\r\n- Add a message handler to grab debug / warnings on Windows @ghutchis\r\n- Save and load the camera modelView and projection matrix @ghutchis\r\n\r\n## 🐛 Bug Fixes\r\n\r\n- Fixed typo in setDefaultPythonInterpretor() @matterhorn103 (#1583)\r\n- Fix Mac builds @ghutchis (#1522)\r\n- Fix quantum surface max cutoff for diffuse functions @ghutchis (#1556)\r\n- Change ""Insert fragment"" ⇒ ""Insert molecule"" by popular request @ghutchis (#1519)\r\n- Fix parsing XYZ files with tabs between columns @ghutchis (#1513)\r\n- Fix bug with incorrect Unicode characters added to labels @ghutchis (#1588)\r\n- Fix for Wayland @matterhorn103 (#1577)\r\n- Fixed crash with angle properties on an empty molecule @secretkontributer (#1566)\r\n- Fix density color crash @ghutchis (#1537)\r\n- Fix crash in forcefields - check if method is valid before using it @ghutchis (#1526)\r\n- Fix for first item in ordered plugin dialog being empty @matterhorn103 (#1523)\r\n- Fix crash with ""copy as"" and an empty molecule @ghutchis (#1521)\r\n- Turn off Color Opacity Map unless a VTK widget is active @ghutchis (#1509)\r\n- Make sure to automatically load the ""Meshes"" display type @ghutchis (#1508)\r\n- If no partial charges are assigned, set them @ghutchis (#1502)\r\n- Fix potential crashes in selection commands @ghutchis (#1499)\r\n- Fix vibration animation @ghutchis (#1487)\r\n- Generate the density matrix if needed for the electron density surface @ghutchis (#1482)\r\n- Fix empty window showing up for commands without an option dialog @ghutchis (#1468)\r\n- Save partial charges and properly read them from CJSON @ghutchis (#1467)\r\n- Don't show PNG files in the filter dialog @ghutchis (#1462)\r\n- Fix GFN-FF energy - redirect GFN-FF output through a hack @ghutchis (#1454)\r\n- Fixup PDB reading with non-standard MD files (no element column) @ghutchis (#1450)\r\n- Fix atomic numbers from Orca - it would read electrons not symbols @ghutchis (#1451)\r\n- Fix linear molecular template @ghutchis (#1474)\r\n- Avoid segfaulting while manipulating carbon bonds/hydrogens @Makiah (#1493)\r\n- Reduce ambiguity of ""Export"" toolbar button @matterhorn103 (#442)\r\n- Ensure that title bar correctly displays active molecule file @Makiah (#440)\r\n\r\n## 🐍 Scripting / Plugin Improvements\r\n\r\n- Windows: Standardize plugin location based on forum feedback @ghutchis (#1605)\r\n- Add some additional Python classes including cjson and connect @ghutchis (#1427)\r\n- Always supply cjson to scripts @ghutchis (#1465)\r\n- If userOptions specifies an order, use that to sort the form @ghutchis (#1503)\r\n- Check script --menupath for {} priority numbers @ghutchis (#1501)\r\n- Add a ""text"" option for scripts to add text labels / help @ghutchis (#1488)\r\n\r\n## 🚀 Performance Improvements\r\n\r\n- Skip calculating orbital / surface points too far apart (e.g., negligible) @ghutchis (#1551)\r\n    - Leads to 2-3x faster surface generation\r\n\r\n## 🧰 Builds / Maintenance\r\n\r\n- Package OpenSSL for Windows @ghutchis\r\n- Update Eigen cmake header references @ghutchis (#1544)\r\n- Add Qt6 build tests @ghutchis (#1547)\r\n- Update to cibuildwheel for Python 3.12 @ghutchis (#1546)\r\n- Fix include of avogadrocoreexport.h @ghutchis (#1504)\r\n- Add specialization for char\* and MatrixXf types @ghutchis (#1494)\r\n- Install ligand and functional group fragments @ghutchis (#1460)\r\n- Make sure to copy libopenbabel on Mac for CPack @ghutchis\r\n\r\n## 📚 Translations\r\n\r\n- Correct typo @Acylation (#1553)\r\n- Update Australian and Canadian localization from GB version @ghutchis (#1453)\r\n- Translations update from Hosted Weblate @weblate\r\n- Automated translation updates @github-actions\r\n- Fix a few remaining cases of .. instead of ellipsis character @ghutchis (#1458)\r\n\r\n## Credits\r\n\r\nThanks to many contributors, including: @Acylation, @Azaathooth, @IagoEmanuel15, @ImgBotApp, @Makiah, @NorwayFun, @Surajjalpun2002, @alchemistcai, @bitigchi, @dependabot, @dependabot[bot], @ghutchis, @github-actions, @hakkikonu, @imgbot, @imitrichev, @joerivan, @koenr, @matterhorn103, @nbehrnd, @ovari, @research11111, @secretkontributer, @tacitcoast, @weblate, Alejandro Díaz-Moscoso, Eisuke Kawashima, Ivanushka, LibreTranslate, Remus-Gabriel Chelu, Weblate Translation Memory and gallegonovato",1.99.0,,,github-actions[bot],"BSD 3-Clause ""New"" or ""Revised"" License",avogadroapp,OpenChemistry,11,avogadro,openchemistry,chemistry,open-science,visualization,compchem,desktop,cross-platform,qt5,hacktoberfest,,,,,,,,,,,/OpenChemistry/avogadroapp,18,16,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/openbiox/weekly,https://github.com/openbiox/weekly,0,No idea how to read that,,0,0,0,0,0,0,0,0,1,0,0,生信爱好者周刊（每周日发布）,"# 生信爱好者周刊 <img src=""https://raw.githubusercontent.com/openbiox/wiki/master/static/img/logo-long.png"" align=""right"" width=""200""/>\n\n[![Open Source Love svg1](https://badges.frapsoft.com/os/v1/open-source.svg?v=103)](https://github.com/ellerbrock/open-source-badges/)\n[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://GitHub.com/openbiox/weekly/graphs/commit-activity)\n[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FShixiangWang%2Fweekly&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false)](https://hits.seeyoufarm.com)\n\n本项目由「Openbiox 生信周刊」运维小队维护。\n\n成员：\n\n- [`@ShixiangWang`](https://github.com/ShixiangWang) - 王诗翔\n- [`@kkjtmac`](https://github.com/kkjtmac) - 阚科佳\n- [`@NiEntropy`](https://github.com/NiEntropy) - 赵启祥 \n- [`@He-Kai-fly`](https://github.com/He-Kai-fly) - 何凯\n- [`@JnanZhang`](https://github.com/JnanZhang) - 张佳楠\n- [`@Tomcxf`](https://github.com/Tomcxf) - 陈啸枫 \n- [`@wangdepin`](https://github.com/wangdepin) - 王德品\n- [`@kongjianyang`](https://github.com/kongjianyang) - 空间阳\n- [`@donghongyu2020`](https://github.com/donghongyu2020) - 董弘禹\n- [`@DrRobinLuo`](https://github.com/DrRobinLuo) - 罗鹏\n\n轮值负责排班表见 <https://github.com/openbiox/weekly/issues/1352>\n\n\n--------------\n\n记录每周值得分享的生信相关内容，周日发布。\n\n欢迎投稿，推荐或自荐文章/软件/资源，请[提交 issue](https://github.com/openbiox/weekly/issues) 。\n\n## 如何搜索\n\n1、使用 [Sourcegraph.com](https://sourcegraph.com/github.com/openbiox/weekly) 进行搜索。（推荐方法）\n\n2、使用 GitHub 自带的搜索，但只支持搜索英文单词。\n\n3、将这个仓库克隆到本地，然后在仓库目录使用下面的命令。\n\n```bash\n$ grep -nri [搜索词] issues | cat --number\n```\n\n比如，搜索 bash 相关内容。\n\n```bash\n$ grep -nri bash issues | cat --number\n```\n\n## 2024\n### 七月\n- 第 131 期：[2023诺贝尔生理医学奖给了mRNA疫苗技术，没有终身教职的她拿奖了](issues/issue-131.md)\n- 第 130 期：[走出还原论，拥抱复杂性](issues/issue-130.md)\n\n### 六月\n- 第 129 期：[怎么才算真正拥有科研思维？](issues/issue-129.md)\n- 第 128 期：[什么事情使你放弃了学术理想？](issues/issue-128.md)\n- 第 127 期: [高校“青椒”之死：困在“非升即走”里的海归博士](issues/issue-127.md)\n- 第 126 期：[为什么细菌不能变得更大？或者更小？](issues/issue-126.md)\n- 第 125 期：[一周是一年的2%](issues/issue-125.md)\n\n\n### 五月\n\n- 第 124 期：[《Cell》癌症研究五十年：十大要点总结](issues/issue-124.md)\n- 第 123 期：[Hello GPT-4o](issues/issue-123.md)\n- 第 122 期：[他山之石——开启个性化医疗的基因组探索](issues/issue-122.md)\n\n### 四月\n\n- 第 121 期：[不唯论文，在中国可行吗？](issues/issue-121.md)\n- 第 120 期：[技术写作的首要诀窍](issues/issue-120.md)\n- 第 119 期：[科学家首次改造了真核生物超过50%的基因组](issues/issue-119.md)\n- 第 118 期：[滴血验癌](issues/issue-118.md)\n\n### 三月\n\n- 第 117 期：[半个世纪的争论: 分子演化论中的中性论/选择论之争](issues/issue-117.md)\n- 第 116 期：[我国科研影响力是否已达到世界顶尖水平？](issues/issue-116.md)\n- 第 115 期：[罗莎琳德·富兰克林对DNA双螺旋结构的真实贡献](issues/issue-115.md)\n- 第 114 期：[从复杂系统中，抓住奇妙的普适性](issues/issue-114.md)\n- 第 113 期：[空间分析技术照亮肿瘤微环境研究](issues/issue-113.md)\n\n### 二月\n\n- 第 112 期：[龙腾虎跃、砥砺前行](issues/issue-112.md)\n- 第 111 期：[三十多岁博士后对于生活的挣扎](issues/issue-111.md)\n\n### 一月\n\n- 第 110 期：[至少20年不过时的研究方向在哪里？](issues/issue-110.md)\n- 第 109 期：[说说你今年用的顺手的工具吧？](issues/issue-109.md)\n- 第 108 期：[肿瘤微生物组是污染，还是新突破](issues/issue-108.md)\n- 第 107 期：[生命起源与复杂性](issues/issue-107.md)\n\n## 2023\n\n<details>\n \n<summary>2023 列表</summary>\n\n### 十二月\n\n- 第 106 期：[哪些技能/知识使生物信息学不可替代？](issues/issue-106.md)\n- 第 105 期：[你愿不愿意在一个小县城待一辈子？](issues/issue-105.md)\n- 第 104 期：[百度文心一言和GPT的差距有多大？](issues/issue-104.md)\n- 第 103 期：[取消论文发表硬性规定，读博变轻松了吗？](issues/issue-103.md)\n- 第 102 期：[中国为什么没有世界一流的研究型医科大学](issues/issue-102.md)\n\n### 十一月\n\n- 第 101 期：[一生不被允许gap的中国人](issues/issue-101.md)\n- 第 100 期：[朋友好](issues/issue-100.md)\n- 第 99 期：[发论文还是生孩子，女性在学术界会遭遇什么？](issues/issue-99.md)\n- 第 98 期：[自然选择主要作用于基因上吗？](issues/issue-98.md)\n\n\n### 十月\n\n- 第 97 期：[失败的读博经历-如何从跌倒中爬起来？](issues/issue-97.md)\n- 第 96 期：[如何取得杰出成就](issues/issue-96.md)\n- 第 95 期：[中国人群泛基因组联盟](issues/issue-95.md)\n\n### 九月\n\n- 第 94 期：[非线性的世界，线性的你](issues/issue-94.md)\n- 第 93 期：[来自妈妈的Y染色体](issues/issue-93.md)\n- 第 92 期：[医疗反腐的困境和选项](issues/issue-92.md)\n- 第 91 期：[探索的动机](issues/issue-91.md)\n\n### 八月\n\n- 第 90 期：[性别视角下的中国科研人员画像](issues/issue-90.md)\n- 第 89 期：[视频学习胜过读书吗？](issues/issue-89.md)\n- 第 88 期：[Vim之父因病离世，一生写下Vim传奇](issues/issue-88.md)\n- 第 87 期：[耿美玉的971是真药的可能性](issues/issue-87.md)\n\n### 七月\n\n- 第 86 期：[如何做亮眼的研究生？](issues/issue-86.md)\n- 第 85 期：[大学是否选生物：适合普通家庭、智力一般的学生](issues/issue-85.md)\n- 第 84 期：[认识自己的缺点](issues/issue-84.md)\n- 第 83 期：[2022 Science年度十大科学突破](issues/issue-83.md)\n- 第 82 期：[一种新的数字表示方法 Posits](issues/issue-82.md)\n\n### 六月\n\n- 第 81 期：[好人情节](issues/issue-81.md)\n- 第 80 期：[生活就像一个鱼缸](issues/issue-80.md)\n- 第 79 期：[四千周](issues/issue-79.md)\n- 第 78 期：[霸凌是平庸科学家登上顶峰的手段](issues/issue-78.md)\n\n### 五月\n\n- 第 77 期：[科研成果被截胡抢发](issues/issue-77.md)\n- 第 76 期：[人生是一个长板问题](issues/issue-76.md)\n- 第 75 期：[学术需要批判氛围](issues/issue-75.md)\n\n### 四月\n\n- 第 74 期：[新技术的最大风险](issues/issue-74.md)\n- 第 73 期：[迄今为止最开放、成果最多的大队列是如何建成的](issues/issue-73.md)\n- 第 72 期：[把时间当作朋友](issues/issue-72.md)\n- 第 71 期：[博士生真的需要一天看20篇文献吗？](issues/issue-71.md)\n\n### 三月\n\n- 第 70 期：[多彩多姿的科学家](issues/issue-70.md)\n- 第 69 期：[如何引导年轻科研工作者？](issues/issue-69.md)\n- 第 68 期：[颠覆性大滑坡，科研还能有实质创新吗？](issues/issue-68.md)\n- 第 67 期：[你是如何活用ChatGPT给你打工的？](issues/issue-67.md)\n\n### 二月\n\n- 第 66 期：[退休越晚、寿命越短？有关系么](issues/issue-66.md)\n- 第 65 期：[125个科学问题：探索与发现](issues/issue-65.md)\n- 第 64 期：[“讨好型人格”：越是乞求，越是被推开](issues/issue-64.md)\n- 第 63 期：[停止寻找的最佳时间](issues/issue-63.md)\n\n### 一月\n\n- 第 62 期：[回望与兔年寄语](issues/issue-62.md)\n- 第 61 期：[基因对寿命的影响](issues/issue-61.md)\n- 第 60 期：[孟德尔诞辰 200 周年](issues/issue-60.md)\n- 第 59 期：[AlphaCode 编程大赛卷趴一半程序员](issues/issue-59.md)\n\n</details>\n\n## 2022\n\n<details>\n \n<summary>2022 列表</summary>\n\n### 十二月\n\n- 第 58 期：[说说你是怎么度过🐑了的日子？](issues/issue-58.md)\n- 第 57 期：[深度学习并非“简单的统计”，二者距离已越来越远](issues/issue-57.md)\n- 第 56 期：[2022诺贝尔奖的点击化学或可作为单细胞多组学开发的有力工具](issues/issue-56.md)\n- 第 55 期：[科学创新四十年，我们可能还没搞明白科学和技术的基本概念](issues/issue-55.md)\n\n\n### 十一月\n\n- 第 54 期：[人类和人生的意义](issues/issue-54.md)\n- 第 53 期：[为什么现在的中国大学生普遍焦虑内卷？](issues/issue-53.md)\n- 第 52 期：[真正的“科技与狠活”：全球首个人工“优选基因”的“完美婴儿”马上2岁啦！](issues/issue-52.md)\n- 第 51 期：[职业对性格的改变](issues/issue-51.md)\n\n\n### 十月\n\n- 第 50 期：[顶级1区期刊宣布：明年起将不再拒稿！](issues/issue-50.md)\n- 第 49 期：[面对知识孤岛，你会怎么处理？](issues/issue-49.md)\n- 第 48 期：[人生不能只有一个支点](issues/issue-48.md)\n- 第 47 期：[RStudio 改名 Posit](issues/issue-47.md)\n\n### 九月\n\n- 第 46 期：[你的苹果M系列芯片电脑跑生信顺利么？](issues/issue-46.md)\n- 第 45 期：[读博还是择业？](issues/issue-45.md)\n- 第 44 期：[为何动物的寿命差异那么大？](issues/issue-44.md)\n- 第 43 期：[RNA-seq差异分析究竟应该用什么？](issues/issue-43.md)\n\n### 八月\n\n- 第 42 期：[极简主义的胜利](issues/issue-42.md)\n- 第 41 期：[人体是一个共生生态系统](issues/issue-41.md)\n- 第 40 期：[bTMB指导肿瘤免疫治疗临床研究](issues/issue-40.md)\n- 第 39 期：[人生不短](issues/issue-39.md)\n\n### 七月\n\n- 第 38 期：[选人不选项目的「基石项目」能否走向成功？](issues/issue-38.md)\n- 第 37 期：[抛弃“影响”因子，计算颠覆因子！](issues/issue-37.md)\n- 第 36 期：[“费钱、费力、不费脑”是中国该提倡的科研吗？](issues/issue-36.md)\n- 第 35 期：[生物信息行业的经济生态](issues/issue-35.md)\n\n### 六月\n\n- 第 34 期：[中国百万人群大队列，何去何从？](issues/issue-34.md)\n- 第 33 期：[科研与生活](issues/issue-33.md)\n- 第 32 期：[有害的同义突变](issues/issue-32.md)\n- 第 31 期：[Openbiox 生物信息学社区 2022 拟开展项目，正式招募 ！](issues/issue-31.md)\n\n### 五月\n\n- 第 30 期：[生信的核心修炼道路在哪里？](issues/issue-30.md)\n- 第 29 期：[Hiplot开发库开源](issues/issue-29.md)\n- 第 28 期：[华大Stereo-seq系列成果揭秘超高分辨率生命全景时空图谱](issues/issue-28.md)\n\n### 四月\n\n- 第 27 期：[真与假的界限在哪里](issues/issue-27.md)\n- 第 26 期：[CRISPR的专利权](issues/issue-26.md)\n- 第 25 期：[从事生信工作，究竟是远见者，还是工具人？](issues/issue-25.md)\n- 第 24 期：[从有隙到无间，首个人类完整基因组发布](issues/issue-24.md)\n\n### 三月\n\n- 第 23 期：[从美国博德研究所成功之道看生命科学前沿创新](issues/issue-23.md)\n- 第 22 期：[为什么生产率在提高而我们却越来越忙](issues/issue-22.md)\n- 第 21 期：[科研与爱好](issues/issue-21.md)\n- 第 20 期：[科研苦行](issues/issue-20.md)\n\n### 二月\n\n- 第 19 期：[2022年值得关注的7大前沿技术](issues/issue-19.md)\n- 第 18 期：[过去50年最重要的统计学思想是什么？](issues/issue-18.md)\n\n### 一月\n\n- 第 17 期：[Cox比例风险模型著作者离世](issues/issue-17.md)\n- 第 16 期：[癌症新特征](issues/issue-16.md)\n- 第 15 期：[科学家的层次](issues/issue-15.md)\n\n</details>\n\n## 2021\n\n<details>\n \n<summary>2021 列表</summary>\n\n### 十二月\n\n- 第 14 期：[为什么有些朋友，走着走着就散了](issues/issue-14.md)\n- 第 13 期：[他开发了基因界的百科全书，贡献却少有人知](issues/issue-13.md)\n- 第 12 期：[你的饮食模式需要改变吗？](issues/issue-12.md)\n- 第 11 期：[中科院近20年院士增选之数据分析](issues/issue-11.md)\n\n### 十一月\n\n- 第 10 期：[开放科学](issues/issue-10.md)\n- 第 9 期：[统计建模之道和术](issues/issue-9.md)\n- 第 8 期：[《沙丘》编剧、《权游》作者使用MS-DOS创作](issues/issue-8.md)\n\n### 十月\n\n- 第 7 期：[为何年轻便科研至死](issues/issue-7.md)\n- 第 6 期：[你会买“炸场”Macbook Pro搞生信吗？](issues/issue-6.md)\n- 第 5 期：[相关非因果](issues/issue-5.md)\n- 第 4 期：[生信有一天可以得诺贝尔奖吗](issues/issue-4.md)\n\n### 九月\n\n- 第 3 期：[百年杨振宁](issues/issue-3.md)\n- 第 2 期：[生信的境界与道路](issues/issue-2.md)\n- 第 1 期：[生信是什么](issues/issue-1.md)\n\n</details>\n \n\n\n## 许可协议\n\n本周刊开源，以「署名（BY）-相同方式共享（SA）」协议发行。\n\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/CC-BY-SA_icon.svg/100px-CC-BY-SA_icon.svg.png)\n\n## 致谢\n\n本周刊受阮一峰[《科技爱好者周刊》](https://github.com/ruanyf/weekly)启发创办，仓库初始化也采用其作为模板。特此致谢。\n\n感谢以下个人/组织的长期/大额赞赏：\n\n- [Openbiox](https://github.com/openbiox)\n- [曾健明](https://github.com/jmzeng1314)\n- iCanHelp\n- 李浩\n\n## 赞赏\n\n![](https://cdn.nlark.com/yuque/0/2022/png/471931/1648291334186-bd3390be-c83c-4396-aabd-ca39f588c15d.png?x-oss-process=image%2Fresize%2Cw_1290%2Climit_0)\n\n",330,bioinformatics,HTML,1,HTML,,,,,,,,,,,,,,,,,,,,,,,,,,,,5,0,5,0,5,16,402,5887,24,2373,2171,202,78198dc90574ece87900146277929f115b56549a,up,2024-07-16T08:27:33Z,ShixiangWang,wangshx@shanghaitech.edu.cn,ShixiangWang,Issue 131,- https://openbiox.github.io/weekly/issue-131/,issue-131,Shixiang Wang (王诗翔),,ShixiangWang,,weekly,openbiox,79,bioinformatics,free-journals,weekly,,,,,,,,,,,,,,,,,,/openbiox/weekly,85,15,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/openbabel/openbabel,https://github.com/openbabel/openbabel,1,,,1,1,1,1,0,0,0,0,0,0,1,Open Babel is a chemical toolbox designed to speak the many languages of chemical data.,"Open Babel\n----------\n\n[![GitHub release](https://img.shields.io/github/release/openbabel/openbabel.svg?maxAge=86400)](https://github.com/openbabel/openbabel/releases)\n[![Download Open Babel](https://img.shields.io/sourceforge/dt/openbabel.svg?maxAge=86400)](https://github.com/openbabel/openbabel/releases)\n[![Travis CI](https://img.shields.io/travis/openbabel/openbabel.svg)](https://travis-ci.org/openbabel/openbabel)\n[![Google Scholar Citations](https://openbabel.org/citations.svg?maxAge=86400)](https://scholar.google.com/scholar?oi=bibs&hl=en&cites=13319995025871922899&as_sdt=5)\n\nOpen Babel is a chemical toolbox designed to speak the many languages\nof chemical data. It's an open, collaborative project allowing anyone\nto search, convert, analyze, or store data from molecular modeling,\nchemistry, solid-state materials, biochemistry, or related areas.\n\n* Ready-to-use programs, and complete programmer's toolkit\n* Read, write and convert over 90 chemical file formats\n* Filter and search molecular files using SMARTS and other methods\n* Generate 2D and 3D coordinates for SMILES, InChI and other formats\n* Supports molecular modeling, cheminformatics, bioinformatics,\n  organic chemistry, inorganic chemistry, solid-state materials,\n  nuclear chemistry...\n\nOpen Babel is distributed under the GNU General Public License (GPL).\nThis program is free software; you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation version 2 of the License. Full details\ncan be found in the file ""COPYING"" which should be included in your\ndistribution.\n\nFor more information, check the [Open Babel website](http://openbabel.org/).\n",1024,chemistry,C++,17,CMake,C++,Perl,Shell,XSLT,C,R,Java,PHP,HTML,Python,Ruby,Scala,Batchfile,POV-Ray SDL,C#,SWIG,,,,,,,,,,,,791,88,662,41,19,118,0,75962,403,1915,1309,606,7acf50c9d73f8016e15eb2a45ce74c8909152883,Merge pull request #2702 from merkys/libxml2-2.12.7,2024-06-21T12:37:08Z,Geoff Hutchison,geoff.hutchison@gmail.com,ghutchis,Open Babel 3.1.1,This version primarily reflects fixes for packaging on Linux and FreeBSD relative to 3.1.0. No features or significant bug fixes were involved.\r\n,openbabel-3-1-1,Geoff Hutchison,,ghutchis,GNU General Public License v2.0,openbabel,openbabel,7,chemistry,chemical-data,cheminformatics,c-plus-plus,chemical-toolbox,,,,,,,,,,,,,,,,/openbabel/openbabel,24,70,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/open2c/cooler,https://github.com/open2c/cooler,0,,,0,0,0,0,0,0,1,0,0,0,0,A cool place to store your Hi-C,"# Cooler\n\n<a href=""https://open2c.github.io/cooler""><img width=""25%"" src=""https://github.com/open2c/cooler/raw/master/docs/cooler_logo.png"" alt=""Cooler""></a>\n\n<table>\n    <tr>\n        <td>Latest Release</td>\n        <td>\n            <a href=""https://pypi.org/project/cooler/"">\n                <img src=""https://img.shields.io/pypi/v/cooler?color=blue&label=PyPI%20package"" alt=""latest release pypi"">\n            </a>\n            <a href=""https://bioconda.github.io/recipes/cooler/README.html"">\n                <img src=""https://img.shields.io/conda/vn/bioconda/cooler?color=blue"" alt=""latest release bioconda"">\n            </a>\n        </td>\n    </tr>\n    <tr>\n        <td>License</td>\n        <td>\n            <a href=""https://github.com/open2c/cooler/blob/master/LICENSE"">\n                <img src=""https://img.shields.io/badge/license-BSD-green"" alt=""license"">\n                <!-- <img src=""https://img.shields.io/pypi/l/cooler.svg"" alt=""license""> -->\n            </a>\n        </td>\n    </tr>\n    <tr>\n        <td>Build Status</td>\n        <td>\n            <a href=""https://github.com/open2c/cooler/blob/master/.github/workflows/ci.yml"">\n                <img src=""https://github.com/open2c/cooler/actions/workflows/ci.yml/badge.svg"" alt=""build status"">\n            </a>\n        </td>\n    </tr>\n    <tr>\n      <td>Pre-commit Status</td>\n      <td>\n        <a href=""https://results.pre-commit.ci/repo/github/49553222"">\n        <img src=""https://results.pre-commit.ci/badge/github/open2c/cooler/master.svg"" alt=""pre-commit status"" />\n        </a>\n      </td>\n    </tr>\n    <tr>\n        <td>Docs Status</td>\n        <td>\n            <a href=""http://cooler.readthedocs.org/en/latest/"">\n                <img src=""https://readthedocs.org/projects/cooler/badge/?version=latest"">\n            </a>\n        </td>\n    </tr>\n    <tr>\n        <td>Coverage</td>\n        <td>\n            <a href=""https://codecov.io/gh/open2c/cooler"">\n                <img src=""https://codecov.io/gh/open2c/cooler/branch/master/graph/badge.svg"" alt=""coverage"">\n            </a>\n        </td>\n    </tr>\n    <tr>\n        <td>Downloads</td>\n        <td>\n            <a href=""https://pypi.org/project/cooler"">\n                <img src=""https://static.pepy.tech/personalized-badge/cooler?period=total&units=international_system&left_color=grey&right_color=blue&left_text=PyPI%20downloads"" alt=""pypi downloads"">\n            </a>\n            <a href=""http://bioconda.github.io/recipes/cooler/README.html"">\n                <img src=""https://img.shields.io/conda/dn/bioconda/cooler.svg?style=flat&label=Bioconda downloads"" alt=""bioconda downloads"">\n            </a>\n        </td>\n    </tr>\n    <tr>\n        <td>Citation</td>\n        <td>\n            <a href=""https://doi.org/10.1093/bioinformatics/btz540"">\n                <img src=""https://img.shields.io/badge/DOI-10.1093%2Fbioinformatics%2Fbtz540-blue"" alt=""paper doi"">\n            </a>\n            <a href=""https://zenodo.org/badge/latestdoi/49553222"">\n                <img src=""https://zenodo.org/badge/49553222.svg"" alt=""zenodo doi"">\n            </a>\n        </td>\n    </tr>\n    <tr>\n        <td>Community</td>\n        <td>\n            <a href=""https://bit.ly/open2c-slack"">\n                <img src=""https://img.shields.io/badge/chat-slack-%233F0F3F?logo=slack"" alt=""slack"">\n            </a>\n            <a href=""https://www.numfocus.org/"">\n                <img src=""https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A"" alt=""numfocus"">\n            </a>\n        </td>\n    </tr>\n</table>\n\n## A cool place to store your Hi-C\n\nCooler is a support library for a **sparse, compressed, binary** persistent storage [format](http://cooler.readthedocs.io/en/latest/schema.html), also called cooler, used to store genomic interaction data, such as Hi-C contact matrices.\n\nThe cooler file format is an implementation of a genomic matrix data model using [HDF5](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) as the container format. The `cooler` package includes a suite of [command line tools](http://cooler.readthedocs.io/en/latest/cli.html) and a [Python API](http://cooler.readthedocs.io/en/latest/api.html) to facilitate creating, querying and manipulating cooler files.\n\nTo get started:\n\n- [Install](#Installation) cooler\n- Read the [documentation](http://cooler.readthedocs.org/en/stable/) and see the Jupyter Notebook [walkthrough](https://github.com/open2c/cooler-binder).\n- _cool_ files from published Hi-C data sets are available [here](https://usgs2.osn.mghpcc.org/cooler01/index.html) or via s3 (bucket `s3://cooler01 --endpoint-url https://usgs2.osn.mghpcc.org --no-sign-request`).\n- Many more multires (_mcool_) files are available on the [4DN data portal](https://data.4dnucleome.org/visualization/index).\n\n### Installation\n\nInstall from PyPI using pip.\n```sh\n$ pip install cooler\n```\n\nIf you are using `conda`, you can alternatively install `cooler` from the [bioconda](https://bioconda.github.io/index.html) channel.\n```sh\n$ conda install -c conda-forge -c bioconda cooler\n```\n\n### Citing\n\nAbdennur, N., and Mirny, L.A. (2020). Cooler: scalable storage for Hi-C data and other genomically labeled arrays. _Bioinformatics_. doi: [10.1093/bioinformatics/btz540](https://doi.org/10.1093/bioinformatics/btz540).\n\n```bibtex\n@article{cooler2020,\n    author = {Abdennur, Nezar and Mirny, Leonid A},\n    title = ""{Cooler: scalable storage for Hi-C data and other genomically labeled arrays}"",\n    journal={Bioinformatics},\n    volume={36},\n    number={1},\n    pages={311--316},\n    year={2020},\n    doi = {10.1093/bioinformatics/btz540},\n    url = {https://doi.org/10.1093/bioinformatics/btz540},\n}\n```\n\n### Contributing\n\nInterested in contributing to cooler? That's great! To get started, check out the [contributing guide](https://github.com/open2c/cooler/blob/master/CONTRIBUTING.md).\n\n\n### Related projects\n\n- See other Open2C tools to process Hi-C data ([pairtools](https://github.com/open2c/pairtools), [distiller](https://github.com/open2c/distiller-nf)) and analyze Hi-C data ([cooltools](https://github.com/open2c/cooltools))!\n- Visualize your cooler data with [HiGlass](http://higlass.io)!\n- Check out this list of [3D genomics tools and papers](https://github.com/mdozmorov/HiC_tools), most of which accept cooler files.\n\n### Affiliations and Acknowledgements\n\n* Cooler is an Affiliated Project of [NumFOCUS](https://www.numfocus.org/).\n* Cooler development has received support from the NIH [4D Nucleome](https://www.4dnucleome.org/) Consortium.\n* We are grateful for a storage allocation from NSF's [ACCESS Cyberinfrastucture](https://access-ci.org/) Open Storage Network to host example cooler data.\n",197,bioinformatics,Python,1,Python,,,,,,,,,,,,,,,,,,,,,,,,,,,,109,10,96,3,10,21,12,92424,52,285,255,30,f43a526629acc2876319fb372bf3e437f599ade3,[pre-commit.ci] pre-commit autoupdate,2024-06-17T20:13:08Z,pre-commit-ci[bot],66853113+pre-commit-ci[bot]@users.noreply.github.com,pre-commit-ci[bot],v0.10.2,## Maintenance\r\n\r\n* [NumPy 2.0 was released](https://numpy.org/doc/stable/release/2.0.0-notes.html). Pin `numpy < 2` until we migrate.\r\n,v0.10.2,Nezar Abdennur,,nvictus,"BSD 3-Clause ""New"" or ""Revised"" License",cooler,open2c,44,contact-matrix,hdf5,python,sparse,genomics,hi-c,bioinformatics,file-format,cooler,3d-genome,chromatin,ngs,,,,,,,,,/open2c/cooler,46,14,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/open2c/bioframe,https://github.com/open2c/bioframe,0,,,0,1,0,0,0,0,1,0,0,0,0,Genomic interval operations on Pandas DataFrames,"# Bioframe: Operations on Genomic Interval Dataframes\n\n<img src=""https://github.com/open2c/bioframe/raw/main/docs/figs/bioframe-logo.png"" width=75%>\n\n![CI](https://github.com/open2c/bioframe/actions/workflows/ci.yml/badge.svg)\n[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/open2c/bioframe/main.svg)](https://results.pre-commit.ci/latest/github/open2c/bioframe/main)\n[![Docs status](https://readthedocs.org/projects/bioframe/badge/)](https://bioframe.readthedocs.io/en/latest/)\n[![Paper](https://img.shields.io/badge/DOI-10.1093%2Fbioinformatics%2Fbtae088-blue)](https://doi.org/10.1093/bioinformatics/btae088)\n[![Zenodo](https://zenodo.org/badge/69901992.svg)](https://zenodo.org/badge/latestdoi/69901992)\n[![Slack](https://img.shields.io/badge/chat-slack-%233F0F3F?logo=slack)](https://bit.ly/open2c-slack)\n[![NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://www.numfocus.org)\n\nBioframe enables flexible and scalable operations on genomic interval dataframes in Python.\n\nBioframe is built directly on top of [Pandas](https://pandas.pydata.org/). Bioframe provides:\n\n* A variety of genomic interval operations that work directly on dataframes.\n* Operations for special classes of genomic intervals, including chromosome arms and fixed-size bins.\n* Conveniences for diverse tabular genomic data formats and loading genome assembly summary information.\n\nRead the [documentation](https://bioframe.readthedocs.io/en/latest/), including the [guide](https://bioframe.readthedocs.io/en/latest/guide-intervalops.html), as well as the [publication](https://doi.org/10.1093/bioinformatics/btae088) for more information.\n\nBioframe is an Affiliated Project of [NumFOCUS](https://www.numfocus.org).\n\n## Installation\n\nBioframe is available on [PyPI](https://pypi.org/project/bioframe/) and [bioconda](https://bioconda.github.io/recipes/bioframe/README.html):\n\n```sh\npip install bioframe\n```\n\n## Contributing\n\nInterested in contributing to bioframe? That's great! To get started, check out the [contributing guide](https://github.com/open2c/bioframe/blob/main/CONTRIBUTING.md). Discussions about the project roadmap take place on the [Open2C Slack](https://bit.ly/open2c-slack) and regular developer meetings scheduled there. Anyone can join and participate!\n\n\n## Interval operations\n\nKey genomic interval operations in bioframe include:\n- `overlap`: Find pairs of overlapping genomic intervals between two dataframes.\n- `closest`: For every interval in a dataframe, find the closest intervals in a second dataframe.\n- `cluster`: Group overlapping intervals in a dataframe into clusters.\n- `complement`: Find genomic intervals that are not covered by any interval from a dataframe.\n\nBioframe additionally has functions that are frequently used for genomic interval operations and can be expressed as combinations of these core operations and dataframe operations, including: `coverage`, `expand`, `merge`, `select`, and `subtract`.\n\nTo `overlap` two dataframes, call:\n```python\nimport bioframe as bf\n\nbf.overlap(df1, df2)\n```\n\nFor these two input dataframes, with intervals all on the same chromosome:\n\n<img src=""https://github.com/open2c/bioframe/raw/main/docs/figs/df1.png"" width=60%>\n<img src=""https://github.com/open2c/bioframe/raw/main/docs/figs/df2.png"" width=60%>\n\n`overlap` will return the following interval pairs as overlaps:\n\n<img src=""https://github.com/open2c/bioframe/raw/main/docs/figs/overlap_inner_0.png"" width=60%>\n<img src=""https://github.com/open2c/bioframe/raw/main/docs/figs/overlap_inner_1.png"" width=60%>\n\n\nTo `merge` all overlapping intervals in a dataframe, call:\n```python\nimport bioframe as bf\n\nbf.merge(df1)\n```\n\nFor this input dataframe, with intervals all on the same chromosome:\n\n<img src=""https://github.com/open2c/bioframe/raw/main/docs/figs/df1.png"" width=60%>\n\n`merge` will return a new dataframe with these merged intervals:\n\n<img src=""https://github.com/open2c/bioframe/raw/main/docs/figs/merge_df1.png"" width=60%>\n\nSee the [guide](https://bioframe.readthedocs.io/en/latest/guide-intervalops.html) for visualizations of other interval operations in bioframe.\n\n## File I/O\n\nBioframe includes utilities for reading genomic file formats into dataframes and vice versa. One handy function is `read_table` which mirrors pandas’s read_csv/read_table but provides a [`schema`](https://github.com/open2c/bioframe/blob/main/bioframe/io/schemas.py) argument to populate column names for common tabular file formats.\n\n```python\njaspar_url = 'http://expdata.cmmt.ubc.ca/JASPAR/downloads/UCSC_tracks/2022/hg38/MA0139.1.tsv.gz'\nctcf_motif_calls = bioframe.read_table(jaspar_url, schema='jaspar', skiprows=1)\n```\n\n## Tutorials\nSee this [jupyter notebook](https://github.com/open2c/bioframe/tree/master/docs/tutorials/tutorial_assign_motifs_to_peaks.ipynb) for an example of how to assign TF motifs to ChIP-seq peaks using bioframe.\n\n\n## Citing\n\nIf you use ***bioframe*** in your work, please cite:\n\n```bibtex\n@article{bioframe_2024,\nauthor = {Open2C and Abdennur, Nezar and Fudenberg, Geoffrey and Flyamer, Ilya M and Galitsyna, Aleksandra A and Goloborodko, Anton and Imakaev, Maxim and Venev, Sergey},\ndoi = {10.1093/bioinformatics/btae088},\njournal = {Bioinformatics},\ntitle = {{Bioframe: Operations on Genomic Intervals in Pandas Dataframes}},\nyear = {2024}\n}\n```\n",169,bioinformatics,Python,1,Python,,,,,,,,,,,,,,,,,,,,,,,,,,,,108,5,99,4,10,20,0,3744,27,108,78,30,19470b44775d6ec90ccfa74176c70e20c48ce38c,maint: RF101 Bugbear lint checks must be selected,2024-06-20T08:54:22Z,Nezar Abdennur,nabdennur@gmail.com,nvictus,v0.7.2,## API changes\r\n* `read_alignment` function introduced in v0.7.0 has been pluralized to `read_alignments`\r\n\r\n## Maintenance\r\n* Skip `read_alignments` tests on big-endian architectures by @nvictus in https://github.com/open2c/bioframe/pull/216\r\n\r\n**Full Changelog**: https://github.com/open2c/bioframe/compare/v0.7.1...v0.7.2,v0.7.2,Nezar Abdennur,,nvictus,MIT License,bioframe,open2c,29,bioinformatics,dataframes,genomic-intervals,genomic-ranges,genomics,ngs-analysis,numpy,pandas,python,spatial-join,,,,,,,,,,,/open2c/bioframe,29,12,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/ome/bioformats,https://github.com/ome/bioformats,0,,,0,0,0,0,0,0,1,0,0,0,0,Bio-Formats is a Java library for reading and writing data in life sciences image file formats. It is developed by the  Open Microscopy Environment. Bio-Formats is released under the  GNU General Public License (GPL); commercial licenses are available from  Glencoe Software.,"# Bio-Formats\n\n[![Actions Status](https://github.com/ome/bioformats/workflows/Ant/badge.svg)](https://github.com/ome/bioformats/actions)\n[![Actions Status](https://github.com/ome/bioformats/workflows/Maven/badge.svg)](https://github.com/ome/bioformats/actions)\n\nBio-Formats is a standalone Java library for reading and writing life sciences\nimage file formats. It is capable of parsing both pixels and metadata for a\nlarge number of formats, as well as writing to several formats.\n\nIf you are having an issue with Bio-Formats and need support, please see the\n[support page](./SUPPORT.md).\n\nPurpose\n-------\n\nBio-Formats' primary purpose is to convert proprietary microscopy data into \nan open standard called the OME data model, particularly into the OME-TIFF \nfile format. See [About Bio-Formats](https://docs.openmicroscopy.org/latest/bio-formats/about/index.html)\nfor further information.\n\nSupported formats\n-----------------\n\nBio-Formats supports [more than a hundred file\nformats](https://docs.openmicroscopy.org/latest/bio-formats/supported-formats.html).\n\n\nFor users\n---------\n\n[Many software packages](https://docs.openmicroscopy.org/latest/bio-formats/users/index.html)\nuse Bio-Formats to read and write microscopy formats.\n\n\nFor developers\n--------------\n\nYou can use Bio-Formats to easily [support these formats in your software](https://docs.openmicroscopy.org/latest/bio-formats/developers/java-library.html).\n\n\nMore information\n----------------\n\nFor more information, see the [Bio-Formats web\nsite](https://www.openmicroscopy.org/bio-formats).\n\nPull request testing\n--------------------\n\nWe welcome pull requests from anyone, but ask that you please verify the\nfollowing before submitting a pull request:\n\n * verify that the branch merges cleanly into ```develop```\n * verify that the branch compiles with the ```clean jars tools``` Ant targets\n * verify that the branch compiles using Maven\n * verify that the branch does not use syntax or API specific to Java 1.8+\n * run the unit tests (```ant test```) and correct any failures\n * test at least one file in each affected format, using the ```showinf```\n   command\n * internal developers only: [run the data\n   tests](https://github.com/ome/bio-formats-documentation/blob/master/sphinx/developers/commit-testing.rst)\n   against directories corresponding to the affected format(s)\n * make sure that your commits contain the correct authorship information and,\n   if necessary, a signed-off-by line\n * make sure that the commit messages or pull request comment contains\n   sufficient information for the reviewer(s) to understand what problem was\n   fixed and how to test it\n",375,whole-slide-imaging,Java,10,Java,Shell,HTML,C++,JavaScript,PostScript,MATLAB,Python,Batchfile,Dockerfile,,,,,,,,,,,,,,,,,,,3727,336,3374,17,7,105,0,279306,241,476,291,185,adf3c52843883efc4dea7f3b7550129ac6e46c85,Merge pull request #4208 from dgault/8.0.0-SNAPSHOT-bump,2024-07-11T10:19:31Z,David Gault,d.gault@dundee.ac.uk,dgault,,,v7.3.1,,,github-actions[bot],GNU General Public License v2.0,bioformats,ome,11,bio-formats,java,image,life-sciences-image,format-reader,format-converter,metadata,whole-slide-imaging,wsi,lightsheet,,,,,,,,,,,/ome/bioformats,190,30,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/OGRECave/ogre-next,https://github.com/OGRECave/ogre-next,0,,,0,0,0,0,0,0,1,1,0,0,0,"aka ogre v2 - scene-oriented, flexible 3D C++ engine","# OGRE-Next 3D (Object-Oriented Graphics Rendering Engine Next Generation)\n\nOgre-Next is a 3D graphics rendering engine. Not to be confused with a game engine which provides Networking, Sound, Physics, etc.\n\nOgre-Next 3.0 has had a substantial overhaul to focus on high performance graphics using Data Oriented Design with:\n * Cache friendly Entity and Node layout\n * Threaded batch processing of Nodes, Frustum Culling and other techniques such as Forward Clustered\n * SIMD processing using AoSoA (Array of Structures of Arrays) memory layout\n * Texture loaded via background streaming\n\nThis makes Ogre-Next suitable for projects aiming to have a **large number of objects on screen, or have tight rendering budgets such as VR.**\n\nThis is the repository where the 2.x branch is actively developed on.\nActive development of the 1.x branch happens in https://github.com/OGRECave/ogre\n\nBoth branches are in active development. See [What version to choose?](https://www.ogre3d.org/about/what-version-to-choose) to understand the differences between 1.x and 2.x\n\nBoth repositories are compatible for merging, but have been split in separate ways as their\ndifferences have diverged long enough.\n\n| Build | Status (github) |\n|-------|-----------------|\n| Linux |[![CI](https://github.com/OGRECave/ogre-next/actions/workflows/linux.build.yml/badge.svg)](https://github.com/OGRECave/ogre-next/actions/workflows/linux.build.yml)|\n| MSVC | [![Build status](https://ci.appveyor.com/api/projects/status/github/OGRECave/ogre-next?branch=master&svg=true)](https://ci.appveyor.com/project/MatiasNGoldberg/ogre-next/branch/master)|\n| Clang-Format Style Consistency |[![CI](https://github.com/OGRECave/ogre-next/actions/workflows/clang-format.yml/badge.svg)](https://github.com/OGRECave/ogre-next/actions/workflows/clang-format.yml)|\n\n## Supported Backends\n\n * Direct3D 11\n * OpenGL 3.3+\n * Metal\n * Vulkan\n\n## Supported Platforms\n\n * Windows (7, 8, 10)\n * Linux\n * macOS\*\n * iOS\n * Android\*\*\n\n(\*) Metal Backend is highly recommended. GL backend is supported in macOS, but the window subsystem hasn't been ported to 3.0 yet.<br/>\n(\*\*) Device must be Vulkan-capable. Android 7.0+ is supported; but Android 8.0+ is strongly recommended due to lots of driver bugs in older versions.<br/>\n\n## Supported Compilers\n\n * Clang 3.3 or newer\n * GCC 5 or newer\n * VS2013 or newer\n \n## Samples\nFor a list of samples and their demonstrated features, refer to the [samples section in the manual.](https://ogrecave.github.io/ogre-next/api/latest/_samples.html) \n\n# Who's using it?\n\n## [Yoy Simulators](https://www.yoy.cl/)\n\n![](./Docs/frontpage/YoySimulators.jpg)\n\n## [Skyline Game Engine](https://aurasoft-skyline.co.uk/)\n\n![](./Docs/frontpage/SkylineGameEngineEditorFull.jpg)\n\n## [Racecraft](https://store.steampowered.com/app/346610/Racecraft/)\n\n![](./Docs/frontpage/Racecraft.jpg)\n\n## [Sunset Rangers](https://store.steampowered.com/app/559340/Sunset_Rangers/)\n\n![](./Docs/frontpage/SunsetRangers.jpg)\n\n## [Stunt Rally 3](https://stuntrally.tuxfamily.org/)\n\n![](./Docs/frontpage/StuntRally3.jpg)\n\n\n# Features\n\n## Forward Clustered\n\n![](./Docs/frontpage/ForwardClustered.jpg)\n\n## PBS & HDR\n\n![](./Docs/frontpage/HDR.jpg)\n\n## Area Lights\n\n![](./Docs/frontpage/AreaLights.jpg)\n\n## Voxel Cone Tracing (VCT) GI\n\n![](./Docs/frontpage/VCT.jpg)\n\n## Instant Radiosity GI\n\n![](./Docs/frontpage/InstantRadiosity.jpg)\n\n## [Voxel Cone Tracing + Per Pixel Parallax Corrected Cubemap (PCC) Hybrid](https://www.ogre3d.org/2019/08/14/pcc-vct-hybrid-progress)\n\n![](./Docs/frontpage/VctPccHybrid.jpg)\n\n## [OpenVR Integration](https://www.ogre3d.org/2019/09/22/improvements-in-vr-morph-animations-moving-to-github-and-ci)\n\n![](./Docs/frontpage/OpenVR.jpg)\n\n# Dependencies\n\n* [CMake 3.x](https://cmake.org/download/)\n* Git\n* For HW & SW requirements, please visit http://www.ogre3d.org/developers/requirements\n* Our source dependencies are grouped in [ogre-next-deps](https://github.com/OGRECave/ogre-next-deps) repo\n* Python 3.x is needed to build shaderc dependency for Vulkan.\n\n# Dependencies (Windows)\n\n* Visual Studio 2013 - 2019 (2022 not tested). MinGW may work but we strongly recommend Visual Studio.\n* [DirectX June 2010 SDK](https://www.microsoft.com/en-us/download/details.aspx?id=6812). Optional. Comes with useful tools.\n* Windows 10 SDK. Contains the latest DirectX SDK, thus recommended over the DX June 2010 SDK,\n  but you may still want to install the June 2010 SDK for those tools.\n* Windows 7 or newer is highly recommended. For Windows Vista & 7, you need to have the\n  [KB2670838 update](https://support.microsoft.com/en-us/kb/2670838) installed.\n  **YOUR END USERS NEED THIS UPDATE AS WELL**.\n\n# Dependencies (Linux)\n\n* Clang >3.5 or GCC >4.0\n\nDebian-based. Run:\n\n```\nsudo apt-get install libfreetype6-dev libfreeimage-dev libzzip-dev libxrandr-dev libxaw7-dev freeglut3-dev libgl1-mesa-dev libglu1-mesa-dev libx11-xcb-dev libxcb-keysyms1-dev doxygen graphviz python-clang libsdl2-dev cmake ninja-build git\n```\n\nArch-based Run:\n\n```\npacman -S freeimage freetype2 libxaw libxrandr mesa zziplib cmake gcc\n```\n\n# Quick Start\n\nWe provide quick download-build scripts under the [Scripts/BuildScripts/output](Scripts/BuildScripts/output) folder.\n\nYou can download all of these scripts [as a compressed 7zip file](https://github.com/OGRECave/ogre-next/releases/download/bin-releases/build_ogre_scripts-master.7z).\nWe also have an archive if you're looking for [scripts to build older versions](https://github.com/OGRECave/ogre-next/releases/bin-releases/).\n\nIf you're on Linux, make sure to first install the dependencies (i.e. run the sudo apt-get above)\n\n# Download and Building manually\n\nIf for some reason you want to do it by hand, there's no script for your platform,\nor you want to learn what the scripts are actually doing, see\n[Setting Up Ogre](https://ogrecave.github.io/ogre-next/api/latest/_setting_up_ogre.html) from the Ogre manual.\n\n# Manual\n\nFor more information see the [online manual](https://ogrecave.github.io/ogre-next/api/latest/manual.html).\nThe manual can build on Linux using Doxygen:\n\n```\ncd build/Debug\nninja OgreDoc\n```\n\n# Resolving Merge Conflicts in 3.0\n\nUsers who run a customized version of Ogre-Next may found rebasing to the latest version a near impossible job due to the sheer amount of minor merge conflicts when upgrading to Ogre-Next 3.0\n\nSee [this guide](https://ogrecave.github.io/ogre-next/api/latest/_resolving_merge_conflicts30.html) on how to deal with them more easily.\n\n\n# OgreNext 3.0 changes\n\nFor details see [What's new?](https://ogrecave.github.io/ogre-next/api/latest/_ogre30_changes.html) from our manual.\n\n\n## OgreNext versioning\n\nSince OgreNext 3.0 we've modified our versioning scheme to be more consistent with other projects and what developers expect.\n\nOur [Github ticket](https://github.com/OGRECave/ogre-next/issues/259#issuecomment-1086954560) and [Forum post](https://forums.ogre3d.org/viewtopic.php?t=96660) explain it in detail. Basically it boils down to:\n\nOgreNext now uses a *modified* variation of [Semantic Versioning](https://semver.org/) (MAJOR.MINOR.PATCH).\n\n - PATCH can only be bumped as long as ABI compatibility remains 100%.\n - MINOR bumps as long as API compatibility stays >= 95% for all components and plugins, according to abi-compliance-checker. Measured against MAJOR.0.0\n   - This is not strictly in compliance to SemVer, but stays close to its spirit\n - MAJOR bumps when API compatibility is < 95%. Although MAJOR can be bumped even if compat >= 95% at the dev's discretion.\n\nAs a result:\n\n - Patch +1 bumps can be release for hot fixes. Bugs that are easy to fix without breaking ABI and deploy everywhere.\n - Minor +1 tells our users upgrading is painless. A recompilation is necessary. Build errors may appear, but they should be very quick & easy to fix.\n Projects like Ignition which have strong ABI requirements can leverage whether to move to Minor+1 if they have to when a certain bugfix is too important and can't be done without breaking OgreNext's ABI.\n Main focus for minor bumps will be for fixing bugs that can't be or are too hard to fix without breaking ABI.\n - Major +1 tells our users to prepare for larger upgrading efforts. Although not necessarily too large if we release more often.\n\nFor OgreNext <= 2.3 the versioning stays the same.\n\n# Support and Resources\n\n * [Forums](https://forums.ogre3d.org/viewforum.php?f=25)\n * [Bug Reports](https://github.com/OGRECave/ogre-next/issues)\n * [Contributing via Pull Requests](https://github.com/OGRECave/ogre-next/pulls)\n * [Documentation](https://ogrecave.github.io/ogre-next/api/latest/)\n * [Ogre 2.1+ FAQ](http://wiki.ogre3d.org/Ogre+2.1+FAQ)\n * [Older resources for interfaces carried over from 1.x](https://www.ogre3d.org/documentation)\n\n# Samples\n\nIf you want to test or evaluate Ogre, you can try the [prebuilt samples for Windows](https://github.com/OGRECave/ogre-next/releases/download/bin-releases/ogre-samples-windows-x64-vs2015.7z).\n\n# Unit Tests\n\nTo run the unit tests, go to Scripts/UnitTesting and to generate the comparison files type:\n\n```\npython3 RunUnitTests.py gl ../../build/Debug/bin/ ./JSON ../../build/UnitTestsOutput/\n```\n\nto check the diff against already generated data:\n\n```\npython3 RunUnitTests.py gl ../../build/Debug/bin/ ./JSON ../../build/UnitTestsOutput/ ../../build/UnitTestsOutput_old/\n```\n\n# License\n\nOGRE-Next (www.ogre3d.org) is made available under the MIT License.\n\nCopyright (c) 2000-present Torus Knot Software Ltd\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the ""Software""), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n",986,graphics,C++,16,CMake,Rich Text Format,C++,Objective-C++,C,Python,GLSL,Objective-C,Shell,Batchfile,HTML,Makefile,Java,MAXScript,CSS,NASL,,,,,,,,,,,,,223,29,181,13,10,179,427,769224,223,224,122,102,cea72df9f6ea1627f0266ebc65bd0b6b0dbf9232,Add regression test to check having many materials still works,2024-06-26T15:22:03Z,Matias N. Goldberg,dark_sylinc@yahoo.com.ar,darksylinc,v2.3.3 Daedalus,"# Codename Daedalus\r\n\r\nFor more information, see the [manual](https://ogrecave.github.io/ogre-next/api/2.3/)\r\n\r\nFor porting information see [What's new in Ogre 2.3](https://ogrecave.github.io/ogre-next/api/2.3/_ogre23_changes.html) ([Markdown version](https://github.com/OGRECave/ogre-next/blob/v2-3/Docs/src/manual/Ogre2.3.Changes.md)) and [Root Layouts](https://ogrecave.github.io/ogre-next/api/2.3/_root_layouts.html) and Ticket #101 \r\n\r\nSee [SW & HW Requirements](https://www.ogre3d.org/developers/requirements)\r\nSee [platforms supported](https://www.ogre3d.org/about/what-version-to-choose)\r\nSee [Ogre 2.1 FAQ](http://wiki.ogre3d.org/Ogre+2.1+FAQ)\r\nSee [What's new in Ogre 2.3](https://ogrecave.github.io/ogre-next/api/2.3/_ogre23_changes.html)\r\n\r\n*Note:* This version includes release notes from v2.3.2 too since release v2.3.3 only fixes trivial issues discovered shortly after v2.3.2 was released.\r\n\r\n**This is a maintenance release.**\r\n\r\n## What's Changed\r\n* Fix pkgconfig for HLMS PBS and Unlit by @mjcarroll in https://github.com/OGRECave/ogre-next/pull/318\r\n* Backport USE_NEW_PROJECT_NAME to 2.3 by @mjcarroll in https://github.com/OGRECave/ogre-next/pull/319\r\n* Backport HLMS exports for 2.3 by @mjcarroll in https://github.com/OGRECave/ogre-next/pull/326\r\n* Remainder of USE_NEW_PACKAGE_NAME updates by @mjcarroll in https://github.com/OGRECave/ogre-next/pull/324\r\n* Unregister Windows class when destroying by @mjcarroll in https://github.com/OGRECave/ogre-next/pull/327\r\n* GL3Plus: If GL_ARB_copy_image is not available, use GL_NV_copy_image by @traversaro in https://github.com/OGRECave/ogre-next/pull/388\r\n\r\n## New Contributors\r\n* @mjcarroll made their first contribution in https://github.com/OGRECave/ogre-next/pull/318\r\n\r\n**Full Changelog**: https://github.com/OGRECave/ogre-next/compare/v2.3.1...v2.3.3\r\n\r\n## Changelog\r\n\r\n- Fix build error in static builds on Linux\r\n- Fix build error on Static build\r\n- Fix Static build & D3D11 ignoring OGRE_USE_NEW_PROJECT_NAME (#420)\r\n- [VK] Fix crash when RenderWindow is exclusively in the resolve texture\r\n- [VK] Fix wrong heap chosen on fallback code path w/ high memory pressure\r\n- [Vk] Fix dangling pointer after destroying TextureGpu (#416)\r\n- Fix NEON versions of ArrayVector3::collapseMin & collapseMax not working\r\n- [Vk] Remove PVRTC support\r\n- EmptyProject CMake script does not copy Atmosphere DLL (#391)\r\n- List python 3 as a dependency\r\n- HlmsPbsDatablock::setUserValue never calls scheduleConstBufferUpdate()\r\n- Fix planar reflections' actor culling\r\n- [WSLg] Fix shader compiler errors under WSLg\r\n- GL3Plus: If GL_ARB_copy_image is not available, use GL_NV_copy_image\r\n- [GL3+] Fix shader compiler error on GL 3.3 drivers\r\n- [VK, HLSL] Fix spelling error of weight\r\n- [Vk] Validation errors when resizing window in MSAA (#366)\r\n- Fix crash when Vulkan drivers exist but no Vulkan-capable GPU (#364)\r\n- PixelFormatGpuUtils would confuse ASTC 8x8 with 6x5\r\n- Fix Xcode compile error caused by confusion between the two versions of OgreTagPoint.h.\r\n- [Metal, macOS] Fix flickering window in macOS Ventura\r\n- [Vk] Fix out of VRAM error after multiple alloc/dealloc rounds\r\n- Metal: honor mSeparateBlend setting\r\n- [Metal] Fix Terra's declaration of gl_ClipDistance\r\n- Merge branch 'v2-2' into v2-3\r\n- [GL3+] Fix double-free of sampler descriptors\r\n- [Vk] RenderSystem::flushCommands must actually flush in Vulkan\r\n- Unregister Windows class when destroying\r\n- Remainder of USE_NEW_PACKAGE_NAME updates (#324)\r\n- Backport HLMS exports for 2.3 (#326)\r\n- [Vk] AsyncTextureTicket may use the wrong image aspect (#322)\r\n- Backport USE_NEW_PROJECT_NAME to 2.3 (#319)\r\n- [Vk] Don't enable features we won't use.\r\n- Fix pkgconfig for HLMS PBS and Unlit\r\n- [Vk] Do not use COLOR_ATTACHMENT_WRITE_BIT for Depth textures\r\n- [GL] Fix shader compiler error on ancient drivers\r\n- Bump to 2.3.3\r\n- [Vk] Fix missing extensions when provided an external VkDevice\r\n",v2.3.3,Matias N. Goldberg,,darksylinc,Other,ogre-next,OGRECave,16,vulkan,graphics,opengl,direct3d11,metal,,,,,,,,,,,,,,,,/OGRECave/ogre-next,16,46,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/odlgroup/odl,https://github.com/odlgroup/odl,0.5,"Seems like a big project, but less of a pipeline, more of a library and a tool",0,0,1,0,0,0,0,1,0,0,0,0,Operator Discretization Library  https://odlgroup.github.io/odl/,"[![PyPI version](https://badge.fury.io/py/odl.svg)](https://badge.fury.io/py/odl)\n[![Build Status](https://travis-ci.org/odlgroup/odl.svg?branch=master)](https://travis-ci.org/odlgroup/odl?branch=master)\n[![Coverage Status](https://coveralls.io/repos/github/odlgroup/odl/badge.svg)](https://coveralls.io/github/odlgroup/odl)\n[![license](https://img.shields.io/badge/license-MPL--2.0-orange.svg)](https://opensource.org/licenses/MPL-2.0)\n[![DOI](https://zenodo.org/badge/45596393.svg)](https://zenodo.org/badge/latestdoi/45596393)\n\nODL\n===\n*Operator Discretization Library* (ODL) is a Python library that enables research in inverse problems on realistic or real data. The framework allows to encapsulate a physical model into an `Operator` that can be used like a mathematical object in, e.g., optimization methods. Furthermore, ODL makes it easy to experiment with reconstruction methods and optimization algorithms for variational regularization, all without sacrificing performance.\n\nFor more details and an introduction into the inner workings of ODL, please refer to the [documentation](https://odlgroup.github.io/odl/).\n\nHighlights\n==========\n- A versatile and efficient library of optimization routines for smooth and non-smooth problems, such as CGLS, BFGS, PDHG and Douglas-Rachford splitting.\n- Support for tomographic imaging with a unified geometry representation and bindings to external libraries for efficient computation of projections and back-projections.\n- And much more, including support for deep learning libraries, figures of merits, phantom generation, data handling, etc.\n\nInstallation\n============\nInstalling ODL should be as easy as\n\n    conda install -c odlgroup odl\n\nor\n\n    pip install odl\n\nFor more detailed instructions, check out the [Installation guide](https://odlgroup.github.io/odl/getting_started/installing.html).\n\nODL is compatible with Python 2/3 and all major platforms (GNU/Linux / Mac / Windows).\n\nResources\n=========\n- [ODL Documentation](https://odlgroup.github.io/odl/)\n- [Installation guide](https://odlgroup.github.io/odl/getting_started/installing.html)\n- [Getting Started](https://odlgroup.github.io/odl/getting_started/getting_started.html)\n- [Code Examples](examples)\n- [API reference](https://odlgroup.github.io/odl/odl.html)\n- [ODL Course Material](https://github.com/odlgroup/odlworkshop)\n\nApplications\n============\nThis is an incomplete list of articles and projects using ODL. If you want to add your project to the list, contact the maintainers or file a pull request.\n\n| Article      |  Code  |\n|------------------|--------|\n| *Learning to solve inverse problems using Wasserstein loss*. NIPS OMT Workshop 2017. [arXiv](https://arxiv.org/abs/1710.10898) | [<img src=""https://github.com/favicon.ico"" width=""24"">](https://github.com/adler-j/wasserstein_inverse_problems) |\n| *Faster PET Reconstruction with a Stochastic Primal-Dual Hybrid Gradient Method*. [Article](https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10394/103941O/Faster-PET-reconstruction-with-a-stochastic-primal-dual-hybrid-gradient/10.1117/12.2272946.full?SSO=1) |  |\n| *Stochastic Primal-Dual Hybrid Gradient Algorithm with Arbitrary Sampling and Imaging Applications*. [arXiv](https://arxiv.org/abs/1706.04957) | [<img src=""https://github.com/favicon.ico"" width=""24"">](https://github.com/mehrhardt/spdhg) |\n| *Learned Primal-Dual Reconstruction*. [arXiv](https://arxiv.org/abs/1707.06474), [blog](https://adler-j.github.io/2017/07/21/Learning-to-reconstruct.html) | [<img src=""https://github.com/favicon.ico"" width=""24"">](https://github.com/adler-j/learned_primal_dual) |\n| *Indirect Image Registration with Large Diffeomorphic Deformations*. [arXiv](https://arxiv.org/abs/1706.04048) | [<img src=""https://github.com/favicon.ico"" width=""24"">](https://github.com/chongchenmath/odl_lddmm) |\n| *High-level algorithm prototyping: an example extending the TVR-DART algorithm*. DGCI, 2017. [DOI](https://doi.org/10.1007/978-3-319-66272-5_10) | [<img src=""https://github.com/favicon.ico"" width=""24"">](https://github.com/aringh/TVR-DART) |\n| *GPUMCI, a ﬂexible platform for x-ray imaging on the GPU*. Fully3D, 2017 |  |\n| *Spectral CT reconstruction with anti-correlated noise model and joint prior*. Fully3D, 2017 | [<img src=""https://github.com/favicon.ico"" width=""24"">](https://github.com/adler-j/spectral_ct_examples) |\n| *Solving ill-posed inverse problems using iterative deep neural networks*. Inverse Problems, 2017 [arXiv](https://arxiv.org/abs/1704.04058), [DOI](https://doi.org/10.1088/1361-6420/aa9581) | [<img src=""https://github.com/favicon.ico"" width=""24"">](https://github.com/adler-j/learned_gradient_tomography) |\n| *Total variation regularization with variable Lebesgue prior*. [arXiv](https://arxiv.org/abs/1702.08807) | [<img src=""https://github.com/favicon.ico"" width=""24"">](https://github.com/kohr-h/variable_lp_paper) |\n| *Generalized Sinkhorn iterations for regularizing inverse problems using optimal mass transport*. SIAM Journal on Imaging Sciences, 2017. [arXiv](https://arxiv.org/abs/1612.02273), [DOI](https://doi.org/10.1137/17M111208X) | [<img src=""https://github.com/favicon.ico"" width=""24"">](https://github.com/aringh/Generalized-Sinkhorn-and-tomography) |\n| *A modified fuzzy C means algorithm for shading correction in craniofacial CBCT images*. CMBEBIH, 2017 | [<img src=""https://github.com/favicon.ico"" width=""24"">](https://github.com/adler-j/mfcm_article) |\n| *The MAX IV imaging concept*. [Article](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5133273/) | |\n| *Shape Based Image Reconstruction Using Linearized Deformations*. Inverse Problems, 2017. [DOI](http://iopscience.iop.org/article/10.1088/1361-6420/aa55af) | [<img src=""https://github.com/favicon.ico"" width=""24"">](https://github.com/chongchenmath/odl_ld) |\n\n| Project      |  Code  |\n|------------------|--------|\n| Multigrid CT reconstruction | [<img src=""https://github.com/favicon.ico"" width=""24"">](https://github.com/kohr-h/odl-multigrid) |\n| Inverse problems over Lie groups | [<img src=""https://github.com/favicon.ico"" width=""24"">](https://github.com/adler-j/lie_grp_diffeo) |\n| Bindings for the [EMRecon](http://www.uni-muenster.de/Sfbmobil/en/veroeffentlichungen/software/emrecon/index.html) package for PET |  [<img src=""https://github.com/favicon.ico"" width=""24"">](https://github.com/odlgroup/odlemrecon) |\n| ADF-STEM reconstruction using nuclear norm regularization | [<img src=""https://github.com/favicon.ico"" width=""24"">](https://github.com/adler-j/odl-stem-examples) |\n\n\nLicense\n-------\nMozilla Public License version 2.0 or later. See the [LICENSE](LICENSE) file.\n\nODL developers\n--------------\nDevelopment of ODL started in 2014 as part of the project ""Low complexity image reconstruction in medical imaging” by Ozan Öktem ([@ozanoktem](https://github.com/ozanoktem)), Jonas Adler ([@adler-j](https://github.com/adler-j)) and Holger Kohr ([@kohr-h](https://github.com/kohr-h)). Several others have made significant contributions, see the [contributors](CONTRIBUTORS.md) list.\n\nTo contact the developers either open an issue on the issue tracker or send an email to odl@math.kth.se.\n\nFunding\n-------\nODL has primarily been developed at [KTH Royal Institute of Technology, Stockholm](https://www.kth.se/en/sci/institutioner/math) and [Centrum Wiskunde & Informatica (CWI), Amsterdam](https://www.cwi.nl).\nIt is financially supported by the Swedish Foundation for Strategic Research as part of the project ""Low complexity image reconstruction in medical imaging"". \n\nSome development time has also been financed by [Elekta](https://www.elekta.com/).\n",356,mathematics,Python,3,Python,Batchfile,Shell,,,,,,,,,,,,,,,,,,,,,,,,,,714,61,626,27,4,24,569,44444,105,932,697,235,60a854a8391d62addbbe1c617e076668ef14d523,"Create custom COO matrix representation for product operators, with u…",2024-03-08T09:47:06Z,Justus Sagemüller,justussa@kth.se,leftaroundabout,ODL 0.7.0,"This release is a big one as it includes the cumulative work over a period of 1 1/2 years. It is planned to be the last release before version 1.0.0 where we expect to land a number of exciting new features.\r\n\r\nWhat follows are the **highlights** of the release. For a more detailed list of all changes, please refer to [the release notes in the documentation](http://odlgroup.github.io/odl/release_notes.html#odl-0-7-0-release-notes-2018-09-09).\r\n\r\n## Native multi-indexing of ODL space elements\r\n\r\nThe `DiscreteLpElement` and `Tensor` (renamed from `FnBaseVector`) data structures now natively support almost all kinds of Numpy ""fancy"" indexing. \r\nAt the same time, the spaces `DiscreteLp` and `Tensorspace` (renamed from `FnBase`) have more advanced indexing capabilities as well. Up to few exceptions, `elem[indices] in space[indices]` is always fulfilled.\r\nAlongside, `ProductSpace` and its elements also support more advanced indexing, in particular in the case of power spaces.\r\n\r\nFurthermore, integration with Numpy has been further improved with the implementation of the `__array_ufunc__` interface. This allows to transparently use ODL objects in calls to Numpy UFuncs, e.g., `np.cos(odl_obj, out=odl_obj)` or `np.add.reduce(odl_in, axis=0, out=odl_out)` — both these examples were not possible with the `__array__` and `__array_wrap__` interfaces.\r\n\r\nUnfortunately, this changeset makes the `odlcuda` plugin unusable since it only supports linear indexing. A much more powerful replacement based on CuPy will be added in version 1.0.0.\r\n\r\n## Integration with deep learning frameworks\r\n\r\nODL is now integrated with three major deep learning frameworks: [TensorFlow](https://www.tensorflow.org/), [PyTorch](https://pytorch.org/) and [Theano](http://www.deeplearning.net/software/theano/). In particular, ODL `Operator` and `Functional` objects can be used as layers in neural networks, with support for automatic differentiation and backpropagation. This makes a lot of (inverse) problems that ODL can handle well, e.g., tomography, accessible to the computation engines of the deep learning field, and opens up a wide range of possibilities to combine the two.\r\n\r\nThe implementation of this functionality and examples of its usage can be found in the packages [`tensorflow`](https://github.com/odlgroup/odl/tree/master/odl/contrib/tensorflow), [`torch`](https://github.com/odlgroup/odl/tree/master/odl/contrib/torch) and [`theano`](https://github.com/odlgroup/odl/tree/master/odl/contrib/theano) in the `odl.contrib` sub-package (see below).\r\n\r\n## New `contrib` sub-package\r\n\r\nThe core ODL library is intended to stay focused on general-purpose classes and data structures, and good code quality is a major goal. This implies that contributions need to undergo scrutiny in a review process, and that some contributions might not be a good fit if they are too specific for certain applications.\r\n\r\nFor this reason, we have created a new [`contrib`](https://github.com/odlgroup/odl/tree/master/odl/contrib) sub-package that is intended for exactly this kind of code. As of writing this, `contrib` already contains a number of highly useful modules:\r\n\r\n- [`datasets`](https://github.com/odlgroup/odl/tree/master/odl/contrib/datasets): Loaders and utility code for publicly available datasets (currently FIPS CT, Mayo clinic human CT, Tu Graz MRI and some image data)\r\n- [`fom`](https://github.com/odlgroup/odl/tree/master/odl/contrib/fom): Implementations of Figures-of-Merit for image quality assessment\r\n- [`mrc`](https://github.com/odlgroup/odl/tree/master/odl/contrib/mrc): Reader and writer for the MRC 2014 data format in electron microscopy\r\n- [`param_opt`](https://github.com/odlgroup/odl/tree/master/odl/contrib/param_opt): Optimization strategies for method hyperparameters\r\n- [`pyshearlab`](https://github.com/odlgroup/odl/tree/master/odl/contrib/pyshearlab): Integration of the [`pyshearlab`](https://github.com/stefanloock/pyshearlab) Python library for shearlet decomposition and analysis\r\n- [`shearlab`](https://github.com/odlgroup/odl/tree/master/odl/contrib/shearlab): Integration of the [`Shearlab.jl`](https://github.com/arsenal9971/Shearlab.jl) Julia shearlet library\r\n- [`solvers`](https://github.com/odlgroup/odl/tree/master/odl/contrib/solvers): More exotic functionals and optimization methods than in the core ODL library\r\n- [`tomo`](https://github.com/odlgroup/odl/tree/master/odl/contrib/tomo): Vendor- or application-specific geometries (currently Elekta ICON and XIV)\r\n- [`tensorflow`](https://github.com/odlgroup/odl/tree/master/odl/contrib/tensorflow): Integration of ODL with TensorFlow\r\n- [`theano`](https://github.com/odlgroup/odl/tree/master/odl/contrib/theano): Integration of ODL with Theano\r\n- [`torch`](https://github.com/odlgroup/odl/tree/master/odl/contrib/torch): Integration of ODL with PyTorch\r\n\r\n## Overhaul of tomographic geometries\r\n\r\nThe classes for representing tomographic geometries in `odl.tomo` have undergone a major update, resulting in a consistent definition of coordinate systems across all cases, [proper documentation](https://odlgroup.github.io/odl/guide/geometry_guide.html), vectorization and broadcasting semantics in all methods that compute vectors, and significant speed-up of backprojection due to better axis handling.\r\nAdditionally, factory functions `cone_beam_geometry` and `helical_geometry` have been added as a simpler and more accessible way to create cone beam geometries.\r\n",v0.7.0,Holger Kohr,,kohr-h,Mozilla Public License 2.0,odl,odlgroup,15,mathematics,prototyping,optimization,inverse-problems,tomography,discretization,python,imaging,,,,,,,,,,,,,/odlgroup/odl,15,26,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/NVIDIA/modulus-sym,https://github.com/NVIDIA/modulus-sym,0,,,0,0,0,0,0,0,1,0,0,0,0,"Framework providing pythonic APIs, algorithms and utilities to be used with Modulus core to physics inform model training as well as higher level abstraction for domain experts","<!-- markdownlint-disable -->\n# Modulus Symbolic (Beta)\n\n\n[![Project Status: Active – The project has reached a stable, usable state and is being actively developed.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n[![GitHub](https://img.shields.io/github/license/NVIDIA/modulus-sym)](https://github.com/NVIDIA/modulus-sym/blob/master/LICENSE.txt)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\nModulus Symbolic (Modulus Sym) provides pythonic APIs, algorithms and utilities to be used with Modulus core, to explicitly physics inform the model training. This includes symbolic APIs for PDEs, domain sampling and PDE-based residuals.  \n\nIt also provides higher level abstraction to compose a training loop from specification of the geometry, PDEs and constraints like boundary conditions using simple symbolic APIs. \nPlease refer to the [Lid Driven cavity](https://docs.nvidia.com/deeplearning/modulus/modulus-sym/user_guide/basics/lid_driven_cavity_flow.html) that illustrates the concept.\nAdditional information can be found in the [Modulus documentation](https://docs.nvidia.com/modulus/index.html#sym).\n\nUsers of Modulus versions older than 23.05 can refer to the [migration guide](https://docs.nvidia.com/deeplearning/modulus/migration-guide/index.html)\nfor updating to the latest version.\n\n## Modulus Packages\n\n- [Modulus (Beta)](https://github.com/NVIDIA/modulus): Open-source deep-learning framework for building, training, and fine-tuning deep learning models using state-of-the-art Physics-ML methods.\n- [Modulus Symbolic (Beta)](https://github.com/NVIDIA/modulus-sym): Framework providing pythonic APIs, algorithms and utilities to be used with Modulus core to physics inform model training as well as higher level abstraction for domain experts.\n\n### Domain Specific Packages\n\n- [Earth-2 MIP (Beta)](https://github.com/NVIDIA/earth2mip): Python framework to enable climate researchers and scientists to explore and experiment with AI models for weather and climate.\n\n## Installation\n\n### PyPi\n\nThe recommended method for installing the latest version of Modulus Symbolic is using PyPi:\n\n```bash\npip install nvidia-modulus.sym\n```\n\nNote, the above method only works for x86/amd64 based architectures. For installing Modulus Sym on Arm based systems using pip, \nInstall VTK from source as shown [here](https://gitlab.kitware.com/vtk/vtk/-/blob/v9.2.6/Documentation/dev/build.md?ref_type=tags#python-wheels) and then install Modulus-Sym and other dependencies\n\n```bash\npip install nvidia-modulus.sym --no-deps\npip install ""hydra-core>=1.2.0"" ""termcolor>=2.1.1"" ""chaospy>=4.3.7"" ""Cython==0.29.28"" ""numpy-stl==2.16.3"" ""opencv-python==4.5.5.64"" \\n    ""scikit-learn==1.0.2"" ""symengine>=0.10.0"" ""sympy==1.12"" ""timm>=1.0.3"" ""torch-optimizer==0.3.0"" ""transforms3d==0.3.1"" \\n    ""typing==3.7.4.3"" ""pillow==10.0.1"" ""notebook==6.4.12"" ""mistune==2.0.3"" ""pint==0.19.2"" ""tensorboard>=2.8.0""\n```\n\n### Container\n\nThe recommended Modulus docker image can be pulled from the [NVIDIA Container Registry](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/modulus/containers/modulus):\n\n```bash\ndocker pull nvcr.io/nvidia/modulus/modulus:24.04\n```\n\n## From Source\n\n### Package\n\nFor a local build of the Modulus Symbolic Python package from source use:\n\n```Bash\ngit clone git@github.com:NVIDIA/modulus-sym.git && cd modulus-sym\n\npip install --upgrade pip\npip install .\n```\n\n### Source Container\n\nTo build release image insert next tag and run below:\n\n```bash\ndocker build -t modulus-sym:deploy \\n    --build-arg TARGETPLATFORM=linux/amd64 --target deploy -f Dockerfile .\n```\n\nCurrently only `linux/amd64` and `linux/arm64` platforms are supported.\n\n## Contributing\n\nFor guidance on making a contribution to Modulus, see the [contributing guidelines](https://github.com/NVIDIA/modulus-sym/blob/main/CONTRIBUTING.md).\n\n## Communication\n\n- Github Discussions: Discuss architectures, implementations, Physics-ML research, etc.\n- GitHub Issues: Bug reports, feature requests, install issues, etc.\n- Modulus Forum: The [Modulus Forum](https://forums.developer.nvidia.com/c/physics-simulation/modulus-physics-ml-model-framework)\nhosts an audience of new to moderate level users and developers for general chat, online\ndiscussions, collaboration, etc.\n\n## License\n\nModulus Symbolic is provided under the Apache License 2.0, please see\n[LICENSE.txt](./LICENSE.txt) for full license text.\n",142,physics,Python,4,Dockerfile,Python,Shell,Makefile,,,,,,,,,,,,,,,,,,,,,,,,,99,7,86,6,5,21,0,204383,58,67,18,49,478b69a33c64ad819bbce30fbba148fbc3ebdd87,Update to alpha version in __init__.py,2024-07-09T19:39:11Z,Kaustubh Tangsali,71059996+ktangsali@users.noreply.github.com,ktangsali,v1.5.0,Modulus Symbolic general release v1.5.0\r\n\r\n## Added\r\n\r\n- Added reservoir examples using GenAI and CCUS workflows.\r\n\r\n## Security\r\n\r\n- Update OpenCV and Pillow versions to fix security,v1.5.0,Nicholas Geneva,,NickGeneva,Apache License 2.0,modulus-sym,NVIDIA,6,deep-learning,machine-learning,nvidia-gpu,physics,pytorch,,,,,,,,,,,,,,,,/NVIDIA/modulus-sym,6,13,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/NVIDIA/modulus,https://github.com/NVIDIA/modulus,0,,,0,0,1,1,0,0,0,0,0,0,0,"Open-source deep-learning framework for building, training, and fine-tuning deep learning models using state-of-the-art Physics-ML methods","# Modulus (Beta)\n\n<!-- markdownlint-disable -->\n[![Project Status: Active - The project has reached a stable, usable state and is being actively developed.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n[![GitHub](https://img.shields.io/github/license/NVIDIA/modulus)](https://github.com/NVIDIA/modulus/blob/master/LICENSE.txt)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n<!-- markdownlint-enable -->\n\nModulus is an open source deep-learning framework for building, training, and fine-tuning\ndeep learning models using state-of-the-art Physics-ML methods.\n\nWhether you are exploring the use of Neural operators like Fourier Neural Operators or\ninterested in Physics informed Neural Networks or a hybrid approach in between, Modulus\nprovides you with the optimized stack that will enable you to train your models at real\nworld scale.\n\nThis package is the core module that provides the core algorithms, network architectures\nand utilities that cover a broad spectrum of physics-constrained and data-driven\nworkflows to suit the diversity of use cases in the science and engineering disciplines.\n\nDetailed information on features and capabilities can be found in the [Modulus documentation](https://docs.nvidia.com/modulus/index.html#core).\n\n<!-- markdownlint-disable -->\n<p align=""center"">\n  <img src=https://raw.githubusercontent.com/NVIDIA/modulus/main/docs/img/Modulus-850x720.svg alt=""Modulus""/>\n</p>\n<!-- markdownlint-enable -->\n\n## Modulus Packages\n\n- [Modulus (Beta)](https://github.com/NVIDIA/modulus): Open-source deep-learning\n  framework for building, training, and fine-tuning deep learning models using\n  state-of-the-art Physics-ML methods.\n- [Modulus Symbolic (Beta)](https://github.com/NVIDIA/modulus-sym): Framework\n  providing pythonic APIs, algorithms and utilities to be used with Modulus\n  core to physics inform model training as well as higher level abstraction\n  for domain experts.\n\n### Domain Specific Packages\n\n- [Earth-2 MIP (Beta)](https://github.com/NVIDIA/earth2mip): Python framework\n  to enable climate researchers and scientists to explore and experiment with\n  AI models for weather and climate.\n  \n## Installation\n\n### PyPi\n\nThe recommended method for installing the latest version of Modulus is using PyPi:\n\n```Bash\npip install nvidia-modulus\n```\n\nThe installation can be verified by running a simple python code snippet as shown below:\n\n```python\npython\n>>> import torch\n>>> from modulus.models.mlp.fully_connected import FullyConnected\n>>> model = FullyConnected(in_features=32, out_features=64)\n>>> input = torch.randn(128, 32)\n>>> output = model(input)\n>>> output.shape\ntorch.Size([128, 64])\n```\n\n#### Optional dependencies\n\nModulus has many optional dependencies that are used in specific components.\nWhen using pip, all dependencies used in Modulus can be installed with\n`pip install nvidia-modulus[all]`. If you are developing Modulus, developer dependencies\ncan be installed using `pip install nvidia-modulus[dev]`. Otherwise, additional dependencies\ncan be installed on a case by case basis. A detailed information on installing the\noptional dependencies can be found in the\n[Getting Started Guide](https://docs.nvidia.com/deeplearning/modulus/getting-started/index.html).\n\n### NVCR Container\n\nThe recommended Modulus docker image can be pulled from the\n[NVIDIA Container Registry](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/modulus/containers/modulus):\n\n```Bash\ndocker pull nvcr.io/nvidia/modulus/modulus:24.04\n```\n\nInside the container you can clone the Modulus git repositories and get started with the\nexamples. Below command show the instructions to launch the modulus container and run an\nexamples from this repo.\n\n```bash\ndocker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 --runtime nvidia \\n--rm -it nvcr.io/nvidia/modulus/modulus:24.04 bash\ngit clone https://github.com/NVIDIA/modulus.git\ncd modulus/examples/cfd/darcy_fno/\npip install warp-lang # install NVIDIA Warp to run the darcy example\npython train_fno_darcy.py\n```\n\n## From Source\n\n### Package\n\nFor a local build of the Modulus Python package from source use:\n\n```Bash\ngit clone git@github.com:NVIDIA/modulus.git && cd modulus\n\npip install --upgrade pip\npip install .\n```\n\n### Source Container\n\nTo build Modulus docker image:\n\n```bash\ndocker build -t modulus:deploy \\n    --build-arg TARGETPLATFORM=linux/amd64 --target deploy -f Dockerfile .\n```\n\nAlternatively, you can run `make container-deploy`\n\nTo build CI image:\n\n```bash\ndocker build -t modulus:ci \\n    --build-arg TARGETPLATFORM=linux/amd64 --target ci -f Dockerfile .\n```\n\nAlternatively, you can run `make container-ci`.\n\nCurrently only `linux/amd64` and `linux/arm64` platforms are supported. If using\n`linux/arm64`, some dependencies like `warp-lang` might not install correctly.\n\n## Contributing\n\nModulus is an open source collaboration and its success is rooted in community\ncontribution to further the field of Physics-ML. Thank you for contributing to the\nproject so others can build on your contribution.\nFor guidance on making a contribution to Modulus, please refer to the\n[contributing guidelines](https://github.com/NVIDIA/modulus/blob/main/CONTRIBUTING.md).\n\n## Communication\n\n- Github Discussions: Discuss new architectures, implementations, Physics-ML research, etc.\n- GitHub Issues: Bug reports, feature requests, install issues, etc.\n- Modulus Forum: The [Modulus Forum](https://forums.developer.nvidia.com/c/physics-simulation/modulus-physics-ml-model-framework)\nhosts an audience of new to moderate level users and developers for general chat, online\ndiscussions, collaboration, etc.\n\n## License\n\nModulus is provided under the Apache License 2.0, please see [LICENSE.txt](./LICENSE.txt)\nfor full license text.\n",811,physics,Python,4,Python,Shell,Makefile,Dockerfile,,,,,,,,,,,,,,,,,,,,,,,,,360,43,300,17,10,48,0,88585,180,247,133,114,3247e833792107ff22b652bb0ed7af41f7c0fcdf,Fix mail links (#595),2024-07-19T17:35:37Z,Kaustubh Tangsali,71059996+ktangsali@users.noreply.github.com,ktangsali,v0.6.0,Modulus (core) general release v0.6.0\r\n\r\n## Added\r\n\r\n- Added citation file\r\n- Link to the CWA dataset\r\n- ClimateDatapipe: an improved datapipe for HDF5/NetCDF4 formatted climate data\r\n- Performance optimizations to CorrDiff\r\n- Physics-Informed Nonlinear Shallow Water Equations example\r\n- Warp neighbor search routine with a minimal example\r\n- Strict option for loading Modulus checkpoints\r\n- Regression only or diffusion only inference for CorrDiff\r\n- Support for organization level model files on NGC file system\r\n- Physics-Informed Magnetohydrodynamics example\r\n\r\n## Changed\r\n\r\n- Updated Ahmed Body and Vortex Shedding examples to use Hydra config\r\n- Added more config options to FCN AFNO example\r\n- Moved posiitonal embedding in CorrDiff from the dataloader to network architecture\r\n\r\n## Deprecated\r\n\r\n- `modulus.models.diffusion.preconditioning.EDMPrecondSR`. Use `EDMPecondSRV2` instead\r\n\r\n## Removed\r\n\r\n- Pickle dependency for CorrDiff\r\n\r\n## Fixed\r\n\r\n- Consistent handling of single GPU runs in DistributedManager\r\n- Output location of objects downloaded with NGC file system\r\n- Bug in scaling the conditional input in CorrDiff deterministic sampler\r\n\r\n## Dependencies\r\n\r\n- Updated DGL build in Dockerfile\r\n- Updated default base image\r\n- Moved Onnx from optional to required dependencies\r\n- Optional Makani dependency required for SFNO model,v0.6.0,Nicholas Geneva,,NickGeneva,Apache License 2.0,modulus,NVIDIA,7,deep-learning,machine-learning,nvidia-gpu,physics,pytorch,,,,,,,,,,,,,,,,/NVIDIA/modulus,7,37,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/numbas/Numbas,https://github.com/numbas/Numbas,0,,,0,0,0,0,0,0,1,0,0,0,0,"A completely browser-based e-assessment/e-learning system, with an emphasis on mathematics","<img src=""http://numbas.org.uk/numbas-logo.svg"" width=""100%"" alt=""Numbas logo"">\r\n\r\n**Numbas** is an open-source system for creating tests which run entirely in the browser. It has been developed by [Newcastle University's School of Mathematics, Statistics and Physics](http://www.ncl.ac.uk/maths-physics).\r\n\r\nFor more information about Numbas and what it does, see our website at [numbas.org.uk](http://www.numbas.org.uk).\r\n\r\n### How to use Numbas\r\n\r\nDocumentation for Numbas users is at [numbas-editor.readthedocs.org](http://numbas-editor.readthedocs.org).\r\n\r\n### Installation\r\n\r\nThis repository contains the Numbas compiler, which runs as standalone Python 3, but the most convenient way to use Numbas is through the web-based editor. \r\n\r\nA publicly-available editor, requiring no set-up, is available at [numbas.mathcentre.ac.uk](http://numbas.mathcentre.ac.uk). Or, you can follow [our instructions for Windows, Mac, or Ubuntu to install your own instance](http://numbas.github.io/editor).\r\n\r\nIf you decide to run your own installation, install the compiler's dependencies with `pip install -r requirements.txt`.\r\n\r\nThis repository is just one part of the Numbas ecosystem. See [the numbas organisation](http://github.com/numbas) for the other pieces, including the web-based editor, extensions, and VLE integrations.\r\n\r\n### Contributing to Numbas\r\n\r\nNumbas is open source, and we welcome contributions of any sort. Bug reports or feature suggestions can be added to [the GitHub issue tracker](https://github.com/numbas/Numbas/issues), or emailed to numbas@ncl.ac.uk. \r\n\r\nSee [our page on contributing to Numbas](http://www.numbas.org.uk/contributing-to-numbas/) for more information on how you can help.\r\n\r\nWe keep a list of tasks specifically for new contributors, under the [good-first-issue label](https://github.com/numbas/Numbas/labels/good%20first%20issue). There's [a corresponding list in the editor repository](https://github.com/numbas/editor/labels/good%20first%20issue), too. These tasks should be fairly straightforward to implement without much knowledge of how all the code fits together.\r\n\r\n### Development\r\n\r\nThis tool runs on the command line: run `python bin/numbas.py` to see the options. You can give it the name of a `.exam` file or pipe one in.\r\n\r\nWhen making changes to the JavaScript runtime, it's a good idea to run the unit tests in the `tests` directory. These can run in a browser, or on the command-line.\r\n\r\n<hr/>\r\n\r\n#### Running tests in a browser\r\n\r\nStart a local web server with `python -m http.server` and go to http://localhost:8000/tests. The tests under `tests/jme` contain tests to do with the JME system, and `tests/parts` contains tests to do with the part marking algorithms. \r\n\r\n#### Running tests on the command-line\r\n\r\nYou can run the tests from the command-line using node.js:\r\n\r\nInstall the dependencies:\r\n  \r\n```bash\r\ncd tests\r\nnpm install\r\n```\r\n\r\nThen run the tests with:\r\n\r\n```bash\r\nnpm test\r\n```\r\n\r\n<hr/>\r\n\r\nIf you make a change, please try to add unit tests to confirm that Numbas behaves as expected.\r\n\r\nThe Makefile in this repository collects together scripts to run the unit tests, and builds the API documentation. Linux and Mac OS have built-in support Makefiles, but Windows doesn't. On Windows, [cygwin](https://www.cygwin.com/) provides `make`.\r\n\r\nAPI documentation for developers is at [numbas.github.io/Numbas](https://numbas.github.io/Numbas).\r\nThis is generated using [JSDoc](http://usejsdoc.org), with [a custom template](http://github.com/numbas/numbas-jsdoc-template).\r\nRun `make docs` to rebuild the API documentation into `../numbas-docs`.\r\n\r\n### Copyright\r\n\r\n> Copyright 2011-18 Newcastle University\r\n> \r\n> Licensed under the Apache License, Version 2.0 (the ""License"");\r\n> you may not use this file except in compliance with the License.\r\n> You may obtain a copy of the License at\r\n> \r\n> http://www.apache.org/licenses/LICENSE-2.0\r\n> \r\n> Unless required by applicable law or agreed to in writing, software\r\n> distributed under the License is distributed on an ""AS IS"" BASIS,\r\n> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n> See the License for the specific language governing permissions and\r\n> limitations under the License.\r\n\r\nYou can see a plain-English explanation of the license and what it allows at [tl;drLegal](https://tldrlegal.com/license/apache-license-2.0-%28apache-2.0%29)\r\n   \r\nCopyright in the content produced using Numbas resides with the author.\r\n",199,mathematics,JavaScript,7,Python,JavaScript,CSS,HTML,XSLT,Makefile,Shell,,,,,,,,,,,,,,,,,,,,,,94,36,55,3,12,33,876,41956,118,987,877,110,3692928323b44f3a8c0eecafab4fac3a6fc589cb,update unit tests,2024-07-10T07:26:38Z,Christian Lawson-Perfect,christianperfect@gmail.com,christianp,v7.3,This release includes a variety of bug fixes and a couple of small new features. \r\n\r\nThere's [more information on the Numbas blog](https://www.numbas.org.uk/blog/2024/03/development-update-march-2024/).,v7.3,Christian Lawson-Perfect,,christianp,Apache License 2.0,Numbas,numbas,22,javascript,mathematics,e-assessment,algebra,computer-algebra,quiz,education,assessment,scorm,newcastle-university,up-for-grabs,numbas,,,,,,,,,/numbas/Numbas,26,21,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/NGSolve/ngsolve,https://github.com/NGSolve/ngsolve,1,,,1,1,1,1,0,0,0,0,0,0,1,"  Netgen/NGSolve is a high performance multiphysics finite element software. It is widely used to analyze models from solid mechanics, fluid dynamics and electromagnetics. Due to its flexible Python interface new physical equations and solution algorithms can be implemented easily.",\nMulti-purpose finite element library\n\nFind the Open Source Community on https://ngsolve.org\n\nSupport & Services: https://cerbsim.com\n,410,fluid-dynamics,C++,15,CMake,C++,Python,C,Cuda,Tcl,GLSL,Processing,Shell,Jupyter Notebook,Batchfile,PowerShell,Dockerfile,JavaScript,BitBake,,,,,,,,,,,,,,27,17,9,1,8,31,0,58179,76,45,35,10,e0d429178aed3f6efc0b83362775aa7cef9fee45,don't use template type deduction,2024-07-18T17:32:17Z,Joachim Schoeberl,joachim.schoeberl@tuwien.ac.at,JSchoeberl,,,v6.2.2304,Matthias Hochsteger,,mhochsteger,GNU Lesser General Public License v2.1,ngsolve,NGSolve,28,finite-elements,fluid-dynamics,electromagnetics,solid-mechanics,fem,parallel-computing,,,,,,,,,,,,,,,/NGSolve/ngsolve,61,33,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/ngless-toolkit/ngless,https://github.com/ngless-toolkit/ngless,0.5,Dedicated lanuage?,0,0,0,1,0,0,0,1,0,0,0,0,NGLess: NGS with less work,"# NGLess: NGS Processing with Less Work\n\n![NGLess logo](NGLess-logo-128x64.png) Ngless is a domain-specific language for\nNGS (next-generation sequencing data) processing.\n\n[![Build & test](https://github.com/ngless-toolkit/ngless/actions/workflows/build_w_nix.yml/badge.svg)](https://github.com/ngless-toolkit/ngless/actions/workflows/build_w_nix.yml)\n[![MIT licensed](https://img.shields.io/badge/license-MIT-blue.svg)](https://raw.githubusercontent.com/hyperium/hyper/master/LICENSE)\n[![Install with Bioconda](https://anaconda.org/bioconda/ngless/badges/version.svg)](https://anaconda.org/bioconda/ngless)\n[![Install with Bioconda](https://anaconda.org/bioconda/ngless/badges/downloads.svg)](https://anaconda.org/bioconda/ngless)\n[![Citation for NGLess](https://img.shields.io/badge/CITATION-DOI%3A10.1186%252Fs40168--019--0684--8-brightgreen.svg)](https://doi.org/10.1186/s40168-019-0684-8)\n\n\nFor questions and discussions, please use the [NGLess mailing\nlist](https://groups.google.com/forum/#!forum/ngless).\n\nIf you are using NGLess, please cite:\n\n> _NG-meta-profiler: fast processing of metagenomes using NGLess, a\n> domain-specific language_ by Luis Pedro Coelho, Renato Alves, Paulo Monteiro,\n> Jaime Huerta-Cepas, Ana Teresa Freitas, Peer Bork, Microbiome (2019)\n> [https://doi.org/10.1186/s40168-019-0684-8](https://doi.org/10.1186/s40168-019-0684-8)\n\n![NGLess cartoon](docs/NGLess-cartoon.svg)\n\n## Example\n\n    ngless ""1.5""\n    input = fastq(['ctrl1.fq','ctrl2.fq','stim1.fq','stim2.fq'])\n    input = preprocess(input) using |read|:\n        read = read[5:]\n        read = substrim(read, min_quality=26)\n        if len(read) < 31:\n            discard\n\n    mapped = map(input,\n                    reference='hg19')\n    write(count(mapped, features=['gene']),\n            ofile='gene_counts.csv',\n            format={csv})\n\n\nFor more information, check [the docs](https://ngless.embl.de). We also have [a\nYouTube\ntutorial](https://www.youtube.com/playlist?list=PLn-ZqA9cHNdSsmVTojYL1lEcfh-J3Hdff)\non how to use NGLess and [SemiBin](https://semibin.rtfd.io/) together (but you\ncan learn to use NGLess independently of SemiBin).\n\n## Installing\n\nSee the [install documentation](https://ngless.embl.de/install.html) for more\ninformation.\n\n### Bioconda\n\nThe recommended way to install NGLess is through\n[bioconda](https://bioconda.github.io):\n\n    conda install -c bioconda ngless \n\n### Docker\n\nAlternatively, a docker container with NGLess is available at\n[docker hub](https://hub.docker.com/r/nglesstoolkit/ngless):\n\n    docker run -v $PWD:/workdir -w /workdir -it nglesstoolkit/ngless:1.5.0 ngless --version\n\nAdapt the mount flags (``-v``) as needed.\n\n### Linux\n\nYou can download a [statically linked version of NGless\n1.5.0](https://github.com/ngless-toolkit/ngless/releases/download/v1.5.0/NGLess-v1.5.0-Linux-static-full)\n\nThis should work across a wide range of Linux versions (please\n[report](https://github.com/ngless-toolkit/ngless/issues) any issues you encounter):\n\n    curl -L -O https://github.com/ngless-toolkit/ngless/releases/download/v1.5.0/NGLess-v1.5.0-Linux-static-full\n    chmod +x NGLess-v1.5.0-Linux-static-full\n    ./NGLess-v1.5.0-Linux-static-full\n\nThis downloaded file bundles bwa, samtools and megahit (also statically linked).\n\n### From Source\n\nInstalling/compiling from source is also possible. Clone\n[https://github.com/ngless-toolkit/ngless](https://github.com/ngless-toolkit/ngless)\n\n#### Dependencies\n\nThe simplest way to get an environment with all the dependencies is to use conda:\n\n    conda create -n ngless\n    conda activate ngless\n    conda config --add channels conda-forge\n    conda install stack cairo bzip2 gmp zlib perl wget xz pkg-config make\n\nYou should have `gcc` installed (or another C-compiler).\n\nThe following sequence of commands should download and build the software\n\n    git clone https://github.com/ngless-toolkit/ngless\n    cd ngless\n    stack setup\n    make\n\nTo install, you can use the following command (replace `<PREFIX>` with\nthe directory where you wish to install, default is `/usr/local`):\n\n    make make\n\n## Running Sample Test Scripts on Local Machine\n\nFor developers who have successfully compiled and installed NGless, running the\ntest scripts in the `tests` folder would be the next line of action to have the\noutput of sample test cases.\n\n    cd tests\n\nOnce in the `tests` directory, select any of the test folders to run NGless.\n\nFor example, here we would run the `regression-fqgz` test:\n\n    cd regression-fqgz\n    ngless ungzip.ngl\n\nAfter running this script open the newly generated folder `ungzip.ngl.output_ngless` and view the template in the **index.html** file.\n\nFor developers who have done this much more datasets for testing purposes can be referenced and used by reading these documentation links:\n**[Human Gut Metagenomics Functional & Taxonomic Profiling](https://ngless.embl.de/tutorial-gut-metagenomics.html#)**\n**[Ocean Metagenomics Functional Profiling](https://ngless.embl.de/tutorial-ocean-metagenomics.html)**\n**[Ocean Metagenomics Assembly and Gene Prediction](https://ngless.embl.de/tutorial-assembly-gp.html)**\n\n\n## More information\n\n- [Full documentation](https://ngless.embl.de/)\n- [Frequently Asked Questions (FAQ)](https://ngless.embl.de/faq.html)\n- [ngless mailing list](https://groups.google.com/forum/#!forum/ngless)\n- [What's new log](https://ngless.embl.de/whatsnew.html)\n- [NGless 1.5.0 Release Documentation](https://ngless.embl.de/whatsnew.html#version-1-5-0)\n\n## Authors\n\n- [Luis Pedro Coelho](https://luispedro.org) (email: [luispedro@big-data-biology.org](mailto:luispedro@big-data-biology.org)) (on twitter: [@luispedrocoelho](https://twitter.com/luispedrocoelho))\n- Paulo Monteiro\n-  Renato Alves\n- [Ana Teresa Freitas](https://web.tecnico.ulisboa.pt/ana.freitas/)\n-  Peer Bork\n\n",142,bioinformatics,Haskell,9,Haskell,C,Makefile,Shell,CSS,HTML,Python,Nix,C++,,,,,,,,,,,,,,,,,,,,27,3,24,0,13,8,0,14888,25,128,93,35,9570b0fea9cfc3f12b74b86c504dde9e268d22de,DOC How to run motus3 with NGLess,2024-05-08T10:09:42Z,Luis Pedro Coelho,luis@luispedro.org,luispedro,Version 1.5.0,"The two big changes are:\r\n\r\n1. the ability to use Yaml files to specify samples,\r\n2. the introduction of `run_for_all` (and `run_for_all_samples`) functions to simplify the usage of the `parallel` module.\r\n\r\nSeveral of the other changes were then to support these two features.\r\nAdditionally, some minor fixes and improvements were made.\r\n\r\n## Full ChangeLog:\r\n\r\n- Add `load_sample_list` function to load samples in YAML format.\r\n- Add `compress_level` argument to `write` function to specify the compression level.\r\n- Added `name()` method to `ReadSet` objects, so you can do:\r\n\r\n    input = load_fastq_directory(""my-sample"")\r\n    print(input.name())\r\n\r\nwhich will print `my-sample`.\r\n- Added `println` function which works like `print` but prints a newline after the output.\r\n- Make `print()` accept ints and doubles as well as strings.\r\n- Added `run_for_all` function to `parallel` module, simplifying its API.\r\n- When using the `parallel` module and a job fails, writes the log to the corresponding `.failed` file.\r\n- External modules can now use the `sequenceset` type to represent a FASTA file.\r\n- The `load_fastq_directory` function now supports `.xz` compressed files.\r\n- The `parallel` module now checks for stale locks **before** re-trying failed tasks. The former model could lead to a situation where a particular sample failed deterministically and then blocked progress even when some locks were stale.\r\n\r\n## Bugfixes\r\n\r\n- The `parallel` module should generate a `.failed` file for each failed job, but this was not happening in every case.\r\n- Fixed parsing of GFF files to support negative values (reported by Josh Sekela on the mailing-list).\r\n\r\n",v1.5.0,Luis Pedro Coelho,,luispedro,Other,ngless,ngless-toolkit,21,haskell,bioinformatics,bioinformatics-pipeline,samtools,bwa,next-generation-sequencing,fastq-format,fastq,science,ngs,metagenomics,genomics,haskell-language,,,,,,,,/ngless-toolkit/ngless,27,11,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/nf-core/tools,https://github.com/nf-core/tools,0,,,0,0,0,0,0,0,1,0,0,0,0,Python package with helper tools for the nf-core community.,,219,bioinformatics,Python,5,Python,Nextflow,Dockerfile,HTML,Jinja,,,,,,,,,,,,,,,,,,,,,,,,2038,185,1820,33,41,191,0,21399,182,1011,766,245,930ece572bf23b68c7a7c5259e918a878ba6499e,Merge pull request #2971 from nf-core/dev,2024-05-09T11:26:44Z,Matthias Hörtenhuber,mashehu@users.noreply.github.com,mashehu,v2.14.1 - Tantalum Toad - Patch,This is a patch release to fix a broken CI action.\r\n\r\n### Template\r\n\r\n- Don't cache pip in `linting.yml` ([#2961](https://github.com/nf-core/tools/pull/2961))\r\n\r\n### General\r\n\r\n- Fix update github action for components in pipeline template ([#2968](https://github.com/nf-core/tools/pull/2968))\r\n- Run sync after release on self hosted runners ([#2970](https://github.com/nf-core/tools/pull/2970)),2.14.1,Matthias Hörtenhuber,,mashehu,MIT License,tools,nf-core,48,nextflow,pipeline,workflow,bioinformatics,python,linter,linting,nf-core,,,,,,,,,,,,,/nf-core/tools,48,141,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/nf-core/taxprofiler,https://github.com/nf-core/taxprofiler,1,,,1,1,1,1,0,0,0,0,0,0,1,Highly parallelised multi-taxonomic profiling of shotgun short- and long-read metagenomic data,"<h1>\n  <picture>\n    <source media=""(prefers-color-scheme: dark)"" srcset=""docs/images/nf-core-taxprofiler_logo_custom_dark.png"">\n    <img alt=""nf-core/taxprofiler"" src=""docs/images/nf-core-taxprofiler_logo_custom_light.png.png"">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/taxprofiler/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/taxprofiler/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/taxprofiler/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/taxprofiler/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/taxprofiler/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.7728364-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.7728364)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A523.04.0-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/taxprofiler)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23taxprofiler-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/taxprofiler)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n[![Cite Preprint](https://img.shields.io/badge/Cite%20Us!-Cite%20Preprint-orange)](https://doi.org/10.1101/2023.10.20.563221)\n\n## Introduction\n\n**nf-core/taxprofiler** is a bioinformatics best-practice analysis pipeline for taxonomic classification and profiling of shotgun short- and long-read metagenomic data. It allows for in-parallel taxonomic identification of reads or taxonomic abundance estimation with multiple classification and profiling tools against multiple databases, and produces standardised output tables for facilitating results comparison between different tools and databases.\n\n## Pipeline summary\n\n![](docs/images/taxprofiler_tube.png)\n\n1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) or [`falco`](https://github.com/smithlabcode/falco) as an alternative option)\n2. Performs optional read pre-processing\n   - Adapter clipping and merging (short-read: [fastp](https://github.com/OpenGene/fastp), [AdapterRemoval2](https://github.com/MikkelSchubert/adapterremoval); long-read: [porechop](https://github.com/rrwick/Porechop))\n   - Low complexity and quality filtering (short-read: [bbduk](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/), [PRINSEQ++](https://github.com/Adrian-Cantu/PRINSEQ-plus-plus); long-read: [Filtlong](https://github.com/rrwick/Filtlong))\n   - Host-read removal (short-read: [BowTie2](http://bowtie-bio.sourceforge.net/bowtie2/); long-read: [Minimap2](https://github.com/lh3/minimap2))\n   - Run merging\n3. Supports statistics for host-read removal ([Samtools](http://www.htslib.org/))\n4. Performs taxonomic classification and/or profiling using one or more of:\n   - [Kraken2](https://ccb.jhu.edu/software/kraken2/)\n   - [MetaPhlAn](https://huttenhower.sph.harvard.edu/metaphlan/)\n   - [MALT](https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/algorithms-in-bioinformatics/software/malt/)\n   - [DIAMOND](https://github.com/bbuchfink/diamond)\n   - [Centrifuge](https://ccb.jhu.edu/software/centrifuge/)\n   - [Kaiju](https://kaiju.binf.ku.dk/)\n   - [mOTUs](https://motu-tool.org/)\n   - [KrakenUniq](https://github.com/fbreitwieser/krakenuniq)\n   - [KMCP](https://github.com/shenwei356/kmcp)\n   - [ganon](https://pirovc.github.io/ganon/)\n5. Perform optional post-processing with:\n   - [bracken](https://ccb.jhu.edu/software/bracken/)\n6. Standardises output tables ([`Taxpasta`](https://taxpasta.readthedocs.io))\n7. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n8. Plotting Kraken2, Centrifuge, Kaiju and MALT results ([`Krona`](https://hpc.nih.gov/apps/kronatools.html))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,run_accession,instrument_platform,fastq_1,fastq_2,fasta\n2612,run1,ILLUMINA,2612_run1_R1.fq.gz,,\n2612,run2,ILLUMINA,2612_run2_R1.fq.gz,,\n2612,run3,ILLUMINA,2612_run3_R1.fq.gz,2612_run3_R2.fq.gz,\n```\n\nEach row represents a fastq file (single-end), a pair of fastq files (paired end), or a fasta (with long reads).\n\nAdditionally, you will need a database sheet that looks as follows:\n\n`databases.csv`:\n\n```\ntool,db_name,db_params,db_path\nkraken2,db2,--quick,/<path>/<to>/kraken2/testdb-kraken2.tar.gz\nmetaphlan,db1,,/<path>/<to>/metaphlan/metaphlan_database/\n```\n\nThat includes directories or `.tar.gz` archives containing databases for the tools you wish to run the pipeline against.\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/taxprofiler \\n   -profile <docker/singularity/.../institute> \\n   --input samplesheet.csv \\n   --databases databases.csv \\n   --outdir <OUTDIR>  \\n   --run_kraken2 --run_metaphlan\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\n> see [docs](https://nf-co.re/usage/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/taxprofiler/usage) and the [parameter documentation](https://nf-co.re/taxprofiler/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/taxprofiler/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/taxprofiler/output).\n\n## Credits\n\nnf-core/taxprofiler was originally written by James A. Fellows Yates, Sofia Stamouli, Moritz E. Beber, and the nf-core/taxprofiler team.\n\n### Team\n\n- [James A. Fellows Yates](https://github.com/jfy133)\n- [Sofia Stamouli](https://github.com/sofstam)\n- [Moritz E. Beber](https://github.com/Midnighter)\n\nWe thank the following people for their contributions to the development of this pipeline:\n\n- [Lauri Mesilaakso](https://github.com/ljmesi)\n- [Tanja Normark](https://github.com/talnor)\n- [Maxime Borry](https://github.com/maxibor)\n- [Thomas A. Christensen II](https://github.com/MillironX)\n- [Jianhong Ou](https://github.com/jianhong)\n- [Rafal Stepien](https://github.com/rafalstepien)\n- [Mahwash Jamy](https://github.com/mjamy)\n- [Lily Andersson Lee](https://github.com/LilyAnderssonLee)\n\n### Acknowledgments\n\nWe also are grateful for the feedback and comments from:\n\n- The general [nf-core/community](https://nf-co.re/community)\n\nAnd specifically to\n\n- [Alex Hübner](https://github.com/alexhbnr)\n\n❤️ also goes to [Zandra Fagernäs](https://github.com/ZandraFagernas) for the logo.\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#taxprofiler` channel](https://nfcore.slack.com/channels/taxprofiler) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/taxprofiler for your analysis, please cite it using the following doi: [10.1101/2023.10.20.563221](https://doi.org/10.1101/2023.10.20.563221).\n\n> Stamouli, S., Beber, M. E., Normark, T., Christensen II, T. A., Andersson-Li, L., Borry, M., Jamy, M., nf-core community, & Fellows Yates, J. A. (2023). nf-core/taxprofiler: Highly parallelised and flexible pipeline for metagenomic taxonomic classification and profiling. In bioRxiv (p. 2023.10.20.563221). https://doi.org/10.1101/2023.10.20.563221\n\nFor the latest version of the code, cite the Zenodo doi: [10.5281/zenodo.7728364](https://doi.org/10.5281/zenodo.7728364)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",104,bioinformatics,Nextflow,2,HTML,Nextflow,,,,,,,,,,,,,,,,,,,,,,,,,,,317,33,279,5,11,128,0,12604,32,192,153,39,5d3ee5513a84f92773c8376c55b5f4da39835307,Merge pull request #499 from nf-core/dev,2024-06-20T15:58:34Z,James A. Fellows Yates,jfy133@gmail.com,jfy133,v1.1.8 - Augmented Akita Patch [2024-06-20],"### `Added`\r\n\r\n- [#487](https://github.com/nf-core/taxprofiler/pull/487) Updated to nf-core pipeline template v2.14.1 (added by @jfy133)\r\n\r\n### `Fixed`\r\n\r\n- [#484](https://github.com/nf-core/taxprofiler/pull/484) Improved input validation to immediately fail if run accession IDs within a given sample ID are not unique (❤️ to @sofstam for reporting, fixed by @jfy133)\r\n- [#491](https://github.com/nf-core/taxprofiler/pull/491) Added flag to publish intermediate bracken files (❤️ to @ewissel for reporting, fixed by @sofstam and @jfy133)\r\n- [#489](https://github.com/nf-core/taxprofiler/pull/489) Fix KrakenUniq classified reads output format mismatch (❤️ to @SannaAb for reporting, fixed by @jfy133)\r\n- [#495](https://github.com/nf-core/taxprofiler/pull/495) Stop TAXPASTA failures when profiles do not have exact compositionality (fixes by @Midnighter, @jfy133)\r\n\r\n### `Dependencies`\r\n\r\n| Tool     | Previous version | New version |\r\n| -------- | ---------------- | ----------- |\r\n| KMCP     | 0.9.1            | 0.9.4       |\r\n| TAXPASTA | 0.6.1            | 0.7.0       |\r\n\r\n### `Deprecated`\r\n\r\n- [#492](https://github.com/nf-core/taxprofiler/pull/492) Removed `--kmcp_mode` parameter from KMCP to allow per database specification by setting in db_params in database sheet (fixed by @jfy133)\r\n",01.01.2008,James A. Fellows Yates,,jfy133,MIT License,taxprofiler,nf-core,11,metagenomics,profiling,taxonomic-profiling,shotgun,nf-core,nextflow,workflow,pipeline,classification,microbiome,taxonomic-classification,long-reads,nanopore,bioinformatics,illumina,pathogen,,,,,/nf-core/taxprofiler,11,146,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/nf-core/sarek,https://github.com/nf-core/sarek,1,,,1,1,1,1,0,0,0,0,0,0,1,"Analysis pipeline to detect germline or somatic variants (pre-processing, variant calling and annotation) from WGS / targeted sequencing","<h1>\n  <picture>\n    <source media=""(prefers-color-scheme: dark)"" srcset=""docs/images/nf-core-sarek_logo_dark.png"">\n    <img alt=""nf-core/sarek"" src=""docs/images/nf-core-sarek_logo_light.png"">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/sarek/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/sarek/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/sarek/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/sarek/actions/workflows/linting.yml)\n[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/sarek/results)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.3476425-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.3476425)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A523.04.0-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://tower.nf/launch?pipeline=https://github.com/nf-core/sarek)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23sarek-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/sarek)\n[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)\n[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)\n[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/sarek** is a workflow designed to detect variants on whole genome or targeted sequencing data. Initially designed for Human, and Mouse, it can work on any species with a reference genome. Sarek can also handle tumour / normal pairs and could include additional relapses.\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/sarek/results).\n\nIt's listed on [Elixir - Tools and Data Services Registry](https://bio.tools/nf-core-sarek) and [Dockstore](https://dockstore.org/workflows/github.com/nf-core/sarek).\n\n<p align=""center"">\n    <img title=""Sarek Workflow"" src=""docs/images/sarek_workflow.png"" width=30%>\n</p>\n\n## Pipeline summary\n\nDepending on the options and samples provided, the pipeline can currently perform the following:\n\n- Form consensus reads from UMI sequences (`fgbio`)\n- Sequencing quality control and trimming (enabled by `--trim_fastq`) (`FastQC`, `fastp`)\n- Map Reads to Reference (`BWA-mem`, `BWA-mem2`, `dragmap` or `Sentieon BWA-mem`)\n- Process BAM file (`GATK MarkDuplicates`, `GATK BaseRecalibrator` and `GATK ApplyBQSR` or `Sentieon LocusCollector` and `Sentieon Dedup`)\n- Summarise alignment statistics (`samtools stats`, `mosdepth`)\n- Variant calling (enabled by `--tools`, see [compatibility](https://nf-co.re/sarek/latest/docs/usage#which-variant-calling-tool-is-implemented-for-which-data-type)):\n  - `ASCAT`\n  - `CNVkit`\n  - `Control-FREEC`\n  - `DeepVariant`\n  - `freebayes`\n  - `GATK HaplotypeCaller`\n  - `Manta`\n  - `mpileup`\n  - `MSIsensor-pro`\n  - `Mutect2`\n  - `Sentieon Haplotyper`\n  - `Strelka2`\n  - `TIDDIT`\n- Variant filtering and annotation (`SnpEff`, `Ensembl VEP`, `BCFtools annotate`)\n- Summarise and represent QC (`MultiQC`)\n\n<p align=""center"">\n    <img title=""Sarek Workflow"" src=""docs/images/sarek_subway.png"" width=60%>\n</p>\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\npatient,sample,lane,fastq_1,fastq_2\nID1,S1,L002,ID1_S1_L002_R1_001.fastq.gz,ID1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a pair of fastq files (paired end).\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/sarek \\n   -profile <docker/singularity/.../institute> \\n   --input samplesheet.csv \\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\n> see [docs](https://nf-co.re/usage/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/sarek/usage) and the [parameter documentation](https://nf-co.re/sarek/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/sarek/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/sarek/output).\n\n## Benchmarking\n\nOn each release, the pipeline is run on 3 full size tests:\n\n- `test_full` runs tumor-normal data for one patient from the SEQ2C consortium\n- `test_full_germline` runs a WGS 30X Genome-in-a-Bottle(NA12878) dataset\n- `test_full_germline_ncbench_agilent` runs two WES samples with 75M and 200M reads (data available [here](https://github.com/ncbench/ncbench-workflow#contributing-callsets)). The results are uploaded to Zenodo, evaluated against a truth dataset, and results are made available via the [NCBench dashboard](https://ncbench.github.io/report/report.html#).\n\n## Credits\n\nSarek was originally written by Maxime U Garcia and Szilveszter Juhos at the [National Genomics Infastructure](https://ngisweden.scilifelab.se) and [National Bioinformatics Infastructure Sweden](https://nbis.se) which are both platforms at [SciLifeLab](https://scilifelab.se), with the support of [The Swedish Childhood Tumor Biobank (Barntumörbanken)](https://ki.se/forskning/barntumorbanken).\nFriederike Hanssen and Gisela Gabernet at [QBiC](https://www.qbic.uni-tuebingen.de/) later joined and helped with further development.\n\nThe Nextflow DSL2 conversion of the pipeline was lead by Friederike Hanssen and Maxime U Garcia.\n\nMaintenance is now lead by Friederike Hanssen and Maxime U Garcia (now at [Seqera Labs](https://seqera/io))\n\nMain developers:\n\n- [Maxime U Garcia](https://github.com/maxulysse)\n- [Friederike Hanssen](https://github.com/FriederikeHanssen)\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- [Abhinav Sharma](https://github.com/abhi18av)\n- [Adam Talbot](https://github.com/adamrtalbot)\n- [Adrian Lärkeryd](https://github.com/adrlar)\n- [Alexander Peltzer](https://github.com/apeltzer)\n- [Alison Meynert](https://github.com/ameynert)\n- [Anders Sune Pedersen](https://github.com/asp8200)\n- [arontommi](https://github.com/arontommi)\n- [BarryDigby](https://github.com/BarryDigby)\n- [Bekir Ergüner](https://github.com/berguner)\n- [bjornnystedt](https://github.com/bjornnystedt)\n- [cgpu](https://github.com/cgpu)\n- [Chela James](https://github.com/chelauk)\n- [David Mas-Ponte](https://github.com/davidmasp)\n- [Edmund Miller](https://github.com/edmundmiller)\n- [Francesco Lescai](https://github.com/lescai)\n- [Gavin Mackenzie](https://github.com/GCJMackenzie)\n- [Gisela Gabernet](https://github.com/ggabernet)\n- [Grant Neilson](https://github.com/grantn5)\n- [gulfshores](https://github.com/gulfshores)\n- [Harshil Patel](https://github.com/drpatelh)\n- [James A. Fellows Yates](https://github.com/jfy133)\n- [Jesper Eisfeldt](https://github.com/J35P312)\n- [Johannes Alneberg](https://github.com/alneberg)\n- [José Fernández Navarro](https://github.com/jfnavarro)\n- [Júlia Mir Pedrol](https://github.com/mirpedrol)\n- [Ken Brewer](https://github.com/kenibrewer)\n- [Lasse Westergaard Folkersen](https://github.com/lassefolkersen)\n- [Lucia Conde](https://github.com/lconde-ucl)\n- [Malin Larsson](https://github.com/malinlarsson)\n- [Marcel Martin](https://github.com/marcelm)\n- [Nick Smith](https://github.com/nickhsmith)\n- [Nicolas Schcolnicov](https://github.com/nschcolnicov)\n- [Nilesh Tawari](https://github.com/nilesh-tawari)\n- [Nils Homer](https://github.com/nh13)\n- [Olga Botvinnik](https://github.com/olgabot)\n- [Oskar Wacker](https://github.com/WackerO)\n- [pallolason](https://github.com/pallolason)\n- [Paul Cantalupo](https://github.com/pcantalupo)\n- [Phil Ewels](https://github.com/ewels)\n- [Sabrina Krakau](https://github.com/skrakau)\n- [Sam Minot](https://github.com/sminot)\n- [Sebastian-D](https://github.com/Sebastian-D)\n- [Silvia Morini](https://github.com/silviamorins)\n- [Simon Pearce](https://github.com/SPPearce)\n- [Solenne Correard](https://github.com/scorreard)\n- [Susanne Jodoin](https://github.com/SusiJo)\n- [Szilveszter Juhos](https://github.com/szilvajuhos)\n- [Tobias Koch](https://github.com/KochTobi)\n- [Winni Kretzschmar](https://github.com/winni2k)\n\n## Acknowledgements\n\n|      [![Barntumörbanken](docs/images/BTB_logo.png)](https://ki.se/forskning/barntumorbanken)      |            [![SciLifeLab](docs/images/SciLifeLab_logo.png)](https://scilifelab.se)             |\n| :-----------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------: |\n| [![National Genomics Infrastructure](docs/images/NGI_logo.png)](https://ngisweden.scilifelab.se/) | [![National Bioinformatics Infrastructure Sweden](docs/images/NBIS_logo.png)](https://nbis.se) |\n|              [![QBiC](docs/images/QBiC_logo.png)](https://www.qbic.uni-tuebingen.de)              |                   [![GHGA](docs/images/GHGA_logo.png)](https://www.ghga.de/)                   |\n|                     [![DNGC](docs/images/DNGC_logo.png)](https://eng.ngc.dk/)                     |                                                                                                |\n\n## Contributions & Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#sarek` channel](https://nfcore.slack.com/channels/sarek) (you can join with [this invite](https://nf-co.re/join/slack)), or contact us: [Maxime U Garcia](mailto:maxime.garcia@seqera.io?subject=[GitHub]%20nf-core/sarek), [Friederike Hanssen](mailto:friederike.hanssen@qbic.uni-tuebingen.de?subject=[GitHub]%20nf-core/sarek)\n\n## Citations\n\nIf you use `nf-core/sarek` for your analysis, please cite the `Sarek` article as follows:\n\n> Friederike Hanssen, Maxime U Garcia, Lasse Folkersen, Anders Sune Pedersen, Francesco Lescai, Susanne Jodoin, Edmund Miller, Oskar Wacker, Nicholas Smith, nf-core community, Gisela Gabernet, Sven Nahnsen **Scalable and efficient DNA sequencing analysis on different compute infrastructures aiding variant discovery** _NAR Genomics and Bioinformatics_ Volume 6, Issue 2, June 2024, lqae031, [doi: 10.1093/nargab/lqae031](https://doi.org/10.1093/nargab/lqae031).\n\n> Garcia M, Juhos S, Larsson M et al. **Sarek: A portable workflow for whole-genome sequencing analysis of germline and somatic variants [version 2; peer review: 2 approved]** _F1000Research_ 2020, 9:63 [doi: 10.12688/f1000research.16665.2](http://dx.doi.org/10.12688/f1000research.16665.2).\n\nYou can cite the sarek zenodo record for a specific version using the following [doi: 10.5281/zenodo.3476425](https://doi.org/10.5281/zenodo.3476425)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n\n## CHANGELOG\n\n- [CHANGELOG](CHANGELOG.md)\n",354,bioinformatics,Nextflow,3,Nextflow,HTML,Python,,,,,,,,,,,,,,,,,,,,,,,,,,932,133,769,30,16,149,0,86777,391,655,423,232,b5b766d3b4ac89864f2fa07441cdc8844e70a79e,Merge pull request #1484 from nf-core/dev,2024-05-07T11:47:52Z,Maxime U Garcia,max.u.garcia@gmail.com,maxulysse,Sarek 3.4.2 - Sájtáristjåhkkå,## What's Changed\r\n\r\n* 3.5.0dev - back to dev by @maxulysse in https://github.com/nf-core/sarek/pull/1477\r\n* feat(nf-prov): pin nf-prov to 1.2.2 by @maxulysse in https://github.com/nf-core/sarek/pull/1482\r\n* Improve cloud testing matrix strategy by @adamrtalbot in https://github.com/nf-core/sarek/pull/1378\r\n* Add new citation by @FriederikeHanssen in https://github.com/nf-core/sarek/pull/1485\r\n* Fixing call to GATK4_HAPLOTYPECALLER by @asp8200 in https://github.com/nf-core/sarek/pull/1488\r\n* Update sentieon modules to 202308.02 by @asp8200 in https://github.com/nf-core/sarek/pull/1487\r\n* add test data in igenomes by @maxulysse in https://github.com/nf-core/sarek/pull/1489\r\n* Update mosdepth by @asp8200 in https://github.com/nf-core/sarek/pull/1490\r\n* feat(wave): add wave profile by @maxulysse in https://github.com/nf-core/sarek/pull/1493\r\n* handle multiple DOIs by @maxulysse in https://github.com/nf-core/sarek/pull/1496\r\n* Fix: Cloud Storage objects are immutable on GCP by @maxulysse in https://github.com/nf-core/sarek/pull/1494\r\n* feat(release): prepare release 3.4.2 by @maxulysse in https://github.com/nf-core/sarek/pull/1498\r\n* fix(tests): remove md5sum for all mosdepth tests + update dependency for these tests by @maxulysse in https://github.com/nf-core/sarek/pull/1499\r\n* fix: remove string none option from params.ascat_genome by @kenibrewer in https://github.com/nf-core/sarek/pull/1501\r\n* Adding Simon and Ken by @maxulysse in https://github.com/nf-core/sarek/pull/1509\r\n* Update CITATIONS.md with fgbio by @nh13 in https://github.com/nf-core/sarek/pull/1505\r\n* fix(TYPO): index_alignement should be index_alignment by @nh13 in https://github.com/nf-core/sarek/pull/1506\r\n* Release 3.4.2 by @maxulysse in https://github.com/nf-core/sarek/pull/1484\r\n\r\n## New Contributors\r\n\r\n* @kenibrewer made their first contribution in https://github.com/nf-core/sarek/pull/1501\r\n* @nh13 made their first contribution in https://github.com/nf-core/sarek/pull/1505\r\n\r\n**Full Changelog**: https://github.com/nf-core/sarek/compare/3.4.1...3.4.2\r\n\r\nhttps://github.com/nf-core/sarek/blob/master/CHANGELOG.md#342---s%C3%A1jt%C3%A1ristj%C3%A5hkk%C3%A5,03.04.2002,Maxime U Garcia,,maxulysse,MIT License,sarek,nf-core,24,nextflow,workflow,pipeline,bioinformatics,genomics,cancer,next-generation-sequencing,conda,reproducible-research,containers,germline,somatic,variant-calling,nf-core,gatk4,annotation,whole-genome-sequencing,whole-exome-sequencing,pre-processing,target-panels,/nf-core/sarek,24,128,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/nf-core/mag,https://github.com/nf-core/mag,1,,,1,1,1,1,0,0,0,0,0,0,1,Assembly and binning of metagenomes,"<h1>\n  <picture>\n    <source media=""(prefers-color-scheme: dark)"" srcset=""docs/images/mag_logo_mascot_dark.png"">\n    <img alt=""nf-core/mag"" src=""docs/images/mag_logo_mascot_light.png"">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/mag/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/mag/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/mag/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/mag/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/mag/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.3589527-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.3589527)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)[![Cite Publication](https://img.shields.io/badge/Cite%20Us!-Cite%20Publication-orange)](https://doi.org/10.1093/nargab/lqac007)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A523.04.0-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/mag)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23mag-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/mag)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/mag** is a bioinformatics best-practise analysis pipeline for assembly, binning and annotation of metagenomes.\n\n<p align=""center"">\n    <img src=""docs/images/mag_workflow.png"" alt=""nf-core/mag workflow overview"" width=""90%"">\n</p>\n\n## Pipeline summary\n\nBy default, the pipeline currently performs the following: it supports both short and long reads, quality trims the reads and adapters with [fastp](https://github.com/OpenGene/fastp) and [Porechop](https://github.com/rrwick/Porechop), and performs basic QC with [FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/), and merge multiple sequencing runs.\n\nThe pipeline then:\n\n- assigns taxonomy to reads using [Centrifuge](https://ccb.jhu.edu/software/centrifuge/) and/or [Kraken2](https://github.com/DerrickWood/kraken2/wiki)\n- performs assembly using [MEGAHIT](https://github.com/voutcn/megahit) and [SPAdes](http://cab.spbu.ru/software/spades/), and checks their quality using [Quast](http://quast.sourceforge.net/quast)\n- (optionally) performs ancient DNA assembly validation using [PyDamage](https://github.com/maxibor/pydamage) and contig consensus sequence recalling with [Freebayes](https://github.com/freebayes/freebayes) and [BCFtools](http://samtools.github.io/bcftools/bcftools.html)\n- predicts protein-coding genes for the assemblies using [Prodigal](https://github.com/hyattpd/Prodigal), and bins with [Prokka](https://github.com/tseemann/prokka) and optionally [MetaEuk](https://www.google.com/search?channel=fs&client=ubuntu-sn&q=MetaEuk)\n- performs metagenome binning using [MetaBAT2](https://bitbucket.org/berkeleylab/metabat/src/master/), [MaxBin2](https://sourceforge.net/projects/maxbin2/), and/or with [CONCOCT](https://github.com/BinPro/CONCOCT), and checks the quality of the genome bins using [Busco](https://busco.ezlab.org/), or [CheckM](https://ecogenomics.github.io/CheckM/), and optionally [GUNC](https://grp-bork.embl-community.io/gunc/).\n- Performs ancient DNA validation and repair with [pyDamage](https://github.com/maxibor/pydamage) and [freebayes](https://github.com/freebayes/freebayes)\n- optionally refines bins with [DAS Tool](https://github.com/cmks/DAS_Tool)\n- assigns taxonomy to bins using [GTDB-Tk](https://github.com/Ecogenomics/GTDBTk) and/or [CAT](https://github.com/dutilh/CAT) and optionally identifies viruses in assemblies using [geNomad](https://github.com/apcamargo/genomad), or Eukaryotes with [Tiara](https://github.com/ibe-uw/tiara)\n\nFurthermore, the pipeline creates various reports in the results directory specified, including a [MultiQC](https://multiqc.info/) report summarizing some of the findings and software versions.\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n```bash\nnextflow run nf-core/mag -profile <docker/singularity/podman/shifter/charliecloud/conda/institute> --input '*_R{1,2}.fastq.gz' --outdir <OUTDIR>\n```\n\nor\n\n```bash\nnextflow run nf-core/mag -profile <docker/singularity/podman/shifter/charliecloud/conda/institute> --input samplesheet.csv --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\n> see [docs](https://nf-co.re/usage/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/mag/usage) and the [parameter documentation](https://nf-co.re/mag/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/mag/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/mag/output).\n\n### Group-wise co-assembly and co-abundance computation\n\nEach sample has an associated group ID (see [input specifications](https://nf-co.re/mag/usage#input_specifications)). This group information can be used for group-wise co-assembly with `MEGAHIT` or `SPAdes` and/or to compute co-abundances for the binning step with `MetaBAT2`. By default, group-wise co-assembly is disabled, while the computation of group-wise co-abundances is enabled. For more information about how this group information can be used see the documentation for the parameters [`--coassemble_group`](https://nf-co.re/mag/parameters#coassemble_group) and [`--binning_map_mode`](https://nf-co.re/mag/parameters#binning_map_mode).\n\nWhen group-wise co-assembly is enabled, `SPAdes` is run on accordingly pooled read files, since `metaSPAdes` does not yet allow the input of multiple samples or libraries. In contrast, `MEGAHIT` is run for each group while supplying lists of the individual readfiles.\n\n## Credits\n\nnf-core/mag was written by [Hadrien Gourlé](https://hadriengourle.com) at [SLU](https://slu.se), [Daniel Straub](https://github.com/d4straub) and [Sabrina Krakau](https://github.com/skrakau) at the [Quantitative Biology Center (QBiC)](http://qbic.life). [James A. Fellows Yates](https://github.com/jfy133) and [Maxime Borry](https://github.com/maxibor) at the [Max Planck Institute for Evolutionary Anthropology](https://www.eva.mpg.de) joined in version 2.2.0. More recent contributors include [Jim Downie](https://github.com/prototaxites) and [Carson Miller](https://github.com/CarsonJM).\n\nLong read processing was inspired by [caspargross/HybridAssembly](https://github.com/caspargross/HybridAssembly) written by Caspar Gross [@caspargross](https://github.com/caspargross)\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- [Alexander Peltzer](https://github.com/apeltzer)\n- [Antonia Schuster](https://github.com/antoniaschuster)\n- [Phil Ewels](https://github.com/ewels)\n- [Gisela Gabernet](https://github.com/ggabernet)\n- [Harshil Patel](https://github.com/drpatelh)\n- [Johannes Alneberg](https://github.com/alneberg)\n- [Maxime Garcia](https://github.com/MaxUlysse)\n- [Michael L Heuer](https://github.com/heuermh)\n- [Alex Hübner](https://github.com/alexhbnr)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#mag` channel](https://nfcore.slack.com/channels/mag) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/mag for your analysis, please cite the preprint as follows:\n\n> **nf-core/mag: a best-practice pipeline for metagenome hybrid assembly and binning**\n>\n> Sabrina Krakau, Daniel Straub, Hadrien Gourlé, Gisela Gabernet, Sven Nahnsen.\n>\n> NAR Genom Bioinform. 2022 Feb 2;4(1):lqac007. doi: [10.1093/nargab/lqac007](https://doi.org/10.1093/nargab/lqac007).\n\nAdditionally you can cite the pipeline directly with the following doi: [10.5281/zenodo.3589527](https://doi.org/10.5281/zenodo.3589527)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",192,bioinformatics,Nextflow,5,HTML,Python,Nextflow,R,Shell,,,,,,,,,,,,,,,,,,,,,,,,377,38,334,5,14,124,0,20282,102,261,211,50,bb74c4695d7767c7094fc5411bf5c49b1caf137d,Merge pull request #635 from nf-core/dev,2024-07-04T06:36:30Z,James A. Fellows Yates,jfy133@gmail.com,jfy133,3.0.2 [2024-07-04],"\r\n### `Changed`\r\n\r\n- [#633](https://github.com/nf-core/mag/pull/633/) - Changed BUSCO to use offline mode when the database is specified by the user (reported by @ChristophKnapp and many others, fix by @jfy133)\r\n- [#632](https://github.com/nf-core/mag/pull/632) - Use default NanoLyse log of just removed reads rather than custom (by @jfy133)\r\n\r\n### `Fixed`\r\n\r\n- [#630](https://github.com/nf-core/mag/pull/630) - Fix CONCOCT empty bins killing the pipeline, and allow for true multithreading again (removing OPENBLAS loop) (reported by @maxibor, fix by @maxibor and @jfy133)\r\n\r\n### `Dependencies`\r\n\r\n| Tool     | Previous version | New version |\r\n| -------- | ---------------- | ----------- |\r\n| Porechop | 0.2.3_seqan2.1.1 | 0.2.4       |\r\n| NanoPlot | 1.26.3           | 1.41.6      |\r\n| NanoLyse | 1.1.0            | 1.2.0       |\r\n",3.0.2,James A. Fellows Yates,,jfy133,MIT License,mag,nf-core,22,workflow,nextflow,metagenomics,assembly,binning,annotation,nf-core,pipeline,bioinformatics,nanopore,nanopore-sequencing,metagenomes,long-read-sequencing,,,,,,,,/nf-core/mag,22,155,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/nf-core/eager,https://github.com/nf-core/eager,1,,,1,1,1,1,0,0,0,0,0,0,1,A fully reproducible and state-of-the-art ancient DNA analysis pipeline,"# ![nf-core/eager](docs/images/nf-core_eager_logo_outline_drop.png)\n\n**A fully reproducible and state-of-the-art ancient DNA analysis pipeline**.\n\n[![GitHub Actions CI Status](https://github.com/nf-core/eager/workflows/nf-core%20CI/badge.svg)](https://github.com/nf-core/eager/actions)\n[![GitHub Actions Linting Status](https://github.com/nf-core/eager/workflows/nf-core%20linting/badge.svg)](https://github.com/nf-core/eager/actions)\n[![Nextflow](https://img.shields.io/badge/nextflow-%E2%89%A520.07.1-brightgreen.svg)](https://www.nextflow.io/)\n[![nf-core](https://img.shields.io/badge/nf--core-pipeline-brightgreen.svg)](https://nf-co.re/)\n[![DOI](https://zenodo.org/badge/135918251.svg)](https://zenodo.org/badge/latestdoi/135918251)\n[![Published in PeerJ](https://img.shields.io/badge/peerj-published-%2300B2FF)](https://peerj.com/articles/10947/)\n\n[![install with bioconda](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg)](https://bioconda.github.io/)\n[![Docker](https://img.shields.io/docker/automated/nfcore/eager.svg)](https://hub.docker.com/r/nfcore/eager)\n![Singularity Container available](https://img.shields.io/badge/singularity-available-7E4C74.svg)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23eager-4A154B?logo=slack)](https://nfcore.slack.com/channels/eager)\n\n>[!IMPORTANT]  \n> nf-core/eager versions 2.* are only compatible with Nextflow versions up to 22.10.6!\n\n## Introduction\n\n<!-- nf-core: Write a 1-2 sentence summary of what data the pipeline is for and what it does -->\n**nf-core/eager** is a scalable and reproducible bioinformatics best-practise processing pipeline for genomic NGS sequencing data, with a focus on ancient DNA (aDNA) data. It is ideal for the (palaeo)genomic analysis of humans, animals, plants, microbes and even microbiomes.\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker containers making installation trivial and results highly reproducible. The pipeline pre-processes raw data from FASTQ inputs, or preprocessed BAM inputs. It can align reads and performs extensive general NGS and aDNA specific quality-control on the results. It comes with docker, singularity or conda containers making installation trivial and results highly reproducible.\n\n<p align=""center"">\n    <img src=""docs/images/usage/eager2_workflow.png"" alt=""nf-core/eager schematic workflow"" width=""70%""\n</p>\n\n## Quick Start\n\n1. Install [`nextflow`](https://nf-co.re/usage/installation) (`>=20.07.1` && `<=22.10.6`)\n\n2. Install any of [`Docker`](https://docs.docker.com/engine/installation/), [`Singularity`](https://www.sylabs.io/guides/3.0/user-guide/), [`Podman`](https://podman.io/), [`Shifter`](https://nersc.gitlab.io/development/shifter/how-to-use/) or [`Charliecloud`](https://hpc.github.io/charliecloud/) for full pipeline reproducibility _(please only use [`Conda`](https://conda.io/miniconda.html) as a last resort; see [docs](https://nf-co.re/usage/configuration#basic-configuration-profiles))_\n\n3. Download the pipeline and test it on a minimal dataset with a single command:\n\n    ```bash\n    nextflow run nf-core/eager -profile test,<docker/singularity/podman/shifter/charliecloud/conda/institute>\n    ```\n\n    > Please check [nf-core/configs](https://github.com/nf-core/configs#documentation) to see if a custom config file to run nf-core pipelines already exists for your Institute. If so, you can simply use `-profile <institute>` in your command. This will enable either `docker` or `singularity` and set the appropriate execution settings for your local compute environment.\n\n4. Start running your own analysis!\n\n    ```bash\n    nextflow run nf-core/eager -profile <docker/singularity/podman/conda/institute> --input '*_R{1,2}.fastq.gz' --fasta '<your_reference>.fasta'\n    ```\n\n5. Once your run has completed successfully, clean up the intermediate files.\n\n    ```bash\n    nextflow clean -f -k\n    ```\n\nSee [usage docs](https://nf-co.re/eager/usage) for all of the available options when running the pipeline.\n\n**N.B.** You can see an overview of the run in the MultiQC report located at `./results/MultiQC/multiqc_report.html`\n\nModifications to the default pipeline are easily made using various options as described in the documentation.\n\n## Pipeline Summary\n\n### Default Steps\n\nBy default the pipeline currently performs the following:\n\n* Create reference genome indices for mapping (`bwa`, `samtools`, and `picard`)\n* Sequencing quality control (`FastQC`)\n* Sequencing adapter removal, paired-end data merging (`AdapterRemoval`)\n* Read mapping to reference using (`bwa aln`, `bwa mem`, `CircularMapper`, or `bowtie2`)\n* Post-mapping processing, statistics and conversion to bam (`samtools`)\n* Ancient DNA C-to-T damage pattern visualisation (`DamageProfiler` or `mapDamage`)\n* PCR duplicate removal (`DeDup` or `MarkDuplicates`)\n* Post-mapping statistics and BAM quality control (`Qualimap`)\n* Library Complexity Estimation (`preseq`)\n* Overall pipeline statistics summaries (`MultiQC`)\n\n### Additional Steps\n\nAdditional functionality contained by the pipeline currently includes:\n\n#### Input\n\n* Automatic merging of complex sequencing setups (e.g. multiple lanes, sequencing configurations, library types)\n\n#### Preprocessing\n\n* Illumina two-coloured sequencer poly-G tail removal (`fastp`)\n* Post-AdapterRemoval trimming of FASTQ files prior mapping (`fastp`)\n* Automatic conversion of unmapped reads to FASTQ (`samtools`)\n* Host DNA (mapped reads) stripping from input FASTQ files (for sensitive samples)\n\n#### aDNA Damage manipulation\n\n* Damage removal/clipping for UDG+/UDG-half treatment protocols (`BamUtil`)\n* Damaged reads extraction and assessment (`PMDTools`)\n* Nuclear DNA contamination estimation of human samples (`angsd`)\n\n#### Genotyping\n\n* Creation of VCF genotyping files (`GATK UnifiedGenotyper`, `GATK HaplotypeCaller` and `FreeBayes`)\n* Creation of EIGENSTRAT genotyping files (`pileupCaller`)\n* Creation of Genotype Likelihood files (`angsd`)\n* Consensus sequence FASTA creation (`VCF2Genome`)\n* SNP Table generation (`MultiVCFAnalyzer`)\n\n#### Biological Information\n\n* Mitochondrial to Nuclear read ratio calculation (`MtNucRatioCalculator`)\n* Statistical sex determination of human individuals (`Sex.DetERRmine`)\n\n#### Metagenomic Screening\n\n* Low-sequenced complexity filtering (`BBduk`)\n* Taxonomic binner with alignment (`MALT`)\n* Taxonomic binner without alignment (`Kraken2`)\n* aDNA characteristic screening of taxonomically binned data from MALT (`MaltExtract`)\n\n#### Functionality Overview\n\nA graphical overview of suggested routes through the pipeline depending on context can be seen below.\n\n<p align=""center"">\n    <img src=""docs/images/usage/eager2_metromap_complex.png"" alt=""nf-core/eager metro map"" width=""70%""\n</p>\n\n## Documentation\n\nThe nf-core/eager pipeline comes with documentation about the pipeline: [usage](https://nf-co.re/eager/usage) and [output](https://nf-co.re/eager/output).\n\n1. [Nextflow installation](https://nf-co.re/usage/installation)\n2. Pipeline configuration\n    * [Pipeline installation](https://nf-co.re/usage/local_installation)\n    * [Adding your own system config](https://nf-co.re/usage/adding_own_config)\n    * [Reference genomes](https://nf-co.re/usage/reference_genomes)\n3. [Running the pipeline](https://nf-co.re/eager/usage)\n   * This includes tutorials, FAQs, and troubleshooting instructions\n4. [Output and how to interpret the results](https://nf-co.re/eager/output)\n\n## Credits\n\nThis pipeline was mostly written by Alexander Peltzer ([apeltzer](https://github.com/apeltzer)) and [James A. Fellows Yates](https://github.com/jfy133), with contributions from [Stephen Clayton](https://github.com/sc13-bioinf), [Thiseas C. Lamnidis](https://github.com/TCLamnidis), [Maxime Borry](https://github.com/maxibor), [Zandra Fagernäs](https://github.com/ZandraFagernas), [Aida Andrades Valtueña](https://github.com/aidaanva) and [Maxime Garcia](https://github.com/MaxUlysse) and the nf-core community.\n\nWe thank the following people for their extensive assistance in the development\nof this pipeline:\n\n## Authors (alphabetical)\n\n* [Aida Andrades Valtueña](https://github.com/aidaanva)\n* [Alexander Peltzer](https://github.com/apeltzer)\n* [James A. Fellows Yates](https://github.com/jfy133)\n* [Judith Neukamm](https://github.com/JudithNeukamm)\n* [Maxime Borry](https://github.com/maxibor)\n* [Maxime Garcia](https://github.com/MaxUlysse)\n* [Stephen Clayton](https://github.com/sc13-bioinf)\n* [Thiseas C. Lamnidis](https://github.com/TCLamnidis)\n* [Zandra Fagernäs](https://github.com/ZandraFagernas)\n\n## Additional Contributors (alphabetical)\n\nThose who have provided conceptual guidance, suggestions, bug reports etc.\n\n* [Alex Hübner](https://github.com/alexhbnr)\n* [Alexandre Gilardet](https://github.com/alexandregilardet)\n* Arielle Munters\n* [Åshild Vågene](https://github.com/ashildv)\n* [Asmaa Ali](https://github.com/asmaa-a-abdelwahab)\n* [Charles Plessy](https://github.com/charles-plessy)\n* [Elina Salmela](https://github.com/esalmela)\n* [Fabian Lehmann](https://github.com/Lehmann-Fabian)\n* [He Yu](https://github.com/paulayu)\n* [Hester van Schalkwyk](https://github.com/hesterjvs)\n* [Ido Bar](https://github.com/IdoBar)\n* [Irina Velsko](https://github.com/ivelsko)\n* [Işın Altınkaya](https://github.com/isinaltinkaya)\n* [Johan Nylander](https://github.com/nylander)\n* [Jonas Niemann](https://github.com/NiemannJ)\n* [Katerine Eaton](https://github.com/ktmeaton)\n* [Kathrin Nägele](https://github.com/KathrinNaegele)\n* [Kevin Lord](https://github.com/lordkev)\n* [Laura Lacher](https://github.com/neija2611)\n* [Luc Venturini](https://github.com/lucventurini)\n* [Mahesh Binzer-Panchal](https://github.com/mahesh-panchal)\n* [Marcel Keller](https://github.com/marcel-keller)\n* [Megan Michel](https://github.com/meganemichel)\n* [Pierre Lindenbaum](https://github.com/lindenb)\n* [Pontus Skoglund](https://github.com/pontussk)\n* [Raphael Eisenhofer](https://github.com/EisenRa)\n* [Roberta Davidson](https://github.com/roberta-davidson)\n* [Rodrigo Barquera](https://github.com/RodrigoBarquera)\n* [Selina Carlhoff](https://github.com/scarlhoff)\n* [Torsten Günter](https://bitbucket.org/tguenther)\n\nIf you've contributed and you're missing in here, please let us know and we will add you in of course!\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#eager` channel](https://nfcore.slack.com/channels/eager) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use `nf-core/eager` for your analysis, please cite the `eager` preprint as follows:\n\n> Fellows Yates JA, Lamnidis TC, Borry M, Valtueña Andrades A, Fagernäs Z, Clayton S, Garcia MU, Neukamm J, Peltzer A. 2021. Reproducible, portable, and efficient ancient genome reconstruction with nf-core/eager. PeerJ 9:e10947. DOI: [10.7717/peerj.10947](https://doi.org/10.7717/peerj.10947).\n\nYou can cite the eager zenodo record for a specific version using the following [doi: 10.5281/zenodo.3698082](https://zenodo.org/badge/latestdoi/135918251)\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n\nIn addition, references of tools and data used in this pipeline are as follows:\n\n* **EAGER v1**, CircularMapper, DeDup* Peltzer, A., Jäger, G., Herbig, A., Seitz, A., Kniep, C., Krause, J., & Nieselt, K. (2016). EAGER: efficient ancient genome reconstruction. Genome Biology, 17(1), 1–14. [https://doi.org/10.1186/s13059-016-0918-z](https://doi.org/10.1186/s13059-016-0918-z).  Download: [https://github.com/apeltzer/EAGER-GUI](https://github.com/apeltzer/EAGER-GUI) and [https://github.com/apeltzer/EAGER-CLI](https://github.com/apeltzer/EAGER-CLI)\n* **FastQC** Download: [https://www.bioinformatics.babraham.ac.uk/projects/fastqc/](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)\n* **AdapterRemoval v2** Schubert, M., Lindgreen, S., & Orlando, L. (2016). AdapterRemoval v2: rapid adapter trimming, identification, and read merging. BMC Research Notes, 9, 88. [https://doi.org/10.1186/s13104-016-1900-2](https://doi.org/10.1186/s13104-016-1900-2). Download: [https://github.com/MikkelSchubert/adapterremoval](https://github.com/MikkelSchubert/adapterremoval)\n* **bwa** Li, H., & Durbin, R. (2009). Fast and accurate short read alignment with Burrows-Wheeler transform. Bioinformatics , 25(14), 1754–1760. [https://doi.org/10.1093/bioinformatics/btp324](https://doi.org/10.1093/bioinformatics/btp324). Download: [http://bio-bwa.sourceforge.net/bwa.shtml](http://bio-bwa.sourceforge.net/bwa.shtml)\n* **SAMtools** Li, H., Handsaker, B., Wysoker, A., Fennell, T., Ruan, J., Homer, N., … 1000 Genome Project Data Processing Subgroup. (2009). The Sequence Alignment/Map format and SAMtools. Bioinformatics , 25(16), 2078–2079. [https://doi.org/10.1093/bioinformatics/btp352](https://doi.org/10.1093/bioinformatics/btp352). Download: [http://www.htslib.org/](http://www.htslib.org/)\n* **DamageProfiler** Neukamm, J., Peltzer, A., & Nieselt, K. (2020). DamageProfiler: Fast damage pattern calculation for ancient DNA. In Bioinformatics (btab190). [https://doi.org/10.1093/bioinformatics/btab190](https://doi.org/10.1093/bioinformatics/btab190). Download: [https://github.com/Integrative-Transcriptomics/DamageProfiler](https://github.com/Integrative-Transcriptomics/DamageProfiler)\n* **QualiMap** Okonechnikov, K., Conesa, A., & García-Alcalde, F. (2016). Qualimap 2: advanced multi-sample quality control for high-throughput sequencing data. Bioinformatics , 32(2), 292–294. [https://doi.org/10.1093/bioinformatics/btv566](https://doi.org/10.1093/bioinformatics/btv566). Download: [http://qualimap.bioinfo.cipf.es/](http://qualimap.bioinfo.cipf.es/)\n* **preseq** Daley, T., & Smith, A. D. (2013). Predicting the molecular complexity of sequencing libraries. Nature Methods, 10(4), 325–327. [https://doi.org/10.1038/nmeth.2375](https://doi.org/10.1038/nmeth.2375). Download: [http://smithlabresearch.org/software/preseq/](http://smithlabresearch.org/software/preseq/)\n* **PMDTools** Skoglund, P., Northoff, B. H., Shunkov, M. V., Derevianko, A. P., Pääbo, S., Krause, J., & Jakobsson, M. (2014). Separating endogenous ancient DNA from modern day contamination in a Siberian Neandertal. Proceedings of the National Academy of Sciences of the United States of America, 111(6), 2229–2234. [https://doi.org/10.1073/pnas.1318934111](https://doi.org/10.1073/pnas.1318934111). Download: [https://github.com/pontussk/PMDtools](https://github.com/pontussk/PMDtools)\n* **MultiQC** Ewels, P., Magnusson, M., Lundin, S., & Käller, M. (2016). MultiQC: summarize analysis results for multiple tools and samples in a single report. Bioinformatics , 32(19), 3047–3048. [https://doi.org/10.1093/bioinformatics/btw354](https://doi.org/10.1093/bioinformatics/btw354). Download: [https://multiqc.info/](https://multiqc.info/)\n* **BamUtils** Jun, G., Wing, M. K., Abecasis, G. R., & Kang, H. M. (2015). An efficient and scalable analysis framework for variant extraction and refinement from population-scale DNA sequence data. Genome Research, 25(6), 918–925. [https://doi.org/10.1101/gr.176552.114](https://doi.org/10.1101/gr.176552.114). Download: [https://genome.sph.umich.edu/wiki/BamUtil](https://genome.sph.umich.edu/wiki/BamUtil)\n* **FastP** Chen, S., Zhou, Y., Chen, Y., & Gu, J. (2018). fastp: an ultra-fast all-in-one FASTQ preprocessor. Bioinformatics , 34(17), i884–i890. [https://doi.org/10.1093/bioinformatics/bty560](https://doi.org/10.1093/bioinformatics/bty560). Download: [https://github.com/OpenGene/fastp](https://github.com/OpenGene/fastp)\n* **GATK 3.5** DePristo, M. A., Banks, E., Poplin, R., Garimella, K. V., Maguire, J. R., Hartl, C., … Daly, M. J. (2011). A framework for variation discovery and genotyping using next-generation DNA sequencing data. Nature Genetics, 43(5), 491–498. [https://doi.org/10.1038/ng.806](https://doi.org/10.1038/ng.806.).Download: [https://console.cloud.google.com/storage/browser/gatk](https://console.cloud.google.com/storage/browser/gatk)\n* **GATK 4.X** - no citation available yet. Download: [https://github.com/broadinstitute/gatk/releases](https://github.com/broadinstitute/gatk/releases)\n* **VCF2Genome** - Alexander Herbig and Alex Peltzer (unpublished). Download: [https://github.com/apeltzer/VCF2Genome](https://github.com/apeltzer/VCF2Genome)\n* **MultiVCFAnalyzer** Bos, K.I. et al., 2014. Pre-Columbian mycobacterial genomes reveal seals as a source of New World human tuberculosis. Nature, 514(7523), pp.494–497. Available at: [http://dx.doi.org/10.1038/nature13591](http://dx.doi.org/10.1038/nature13591). Download: [https://github.com/alexherbig/MultiVCFAnalyzer](https://github.com/alexherbig/MultiVCFAnalyzer)\n* **MTNucRatioCalculator** Alex Peltzter (Unpublished). Download: [https://github.com/apeltzer/MTNucRatioCalculator](https://github.com/apeltzer/MTNucRatioCalculator)\n* **Sex.DetERRmine.py** Lamnidis, T.C. et al., 2018. Ancient Fennoscandian genomes reveal origin and spread of Siberian ancestry in Europe. Nature communications, 9(1), p.5018. Available at: [http://dx.doi.org/10.1038/s41467-018-07483-5](http://dx.doi.org/10.1038/s41467-018-07483-5). Download: [https://github.com/TCLamnidis/Sex.DetERRmine.git](https://github.com/TCLamnidis/Sex.DetERRmine.git)\n* **ANGSD** Korneliussen, T.S., Albrechtsen, A. & Nielsen, R., 2014. ANGSD: Analysis of Next Generation Sequencing Data. BMC bioinformatics, 15, p.356. Available at: [http://dx.doi.org/10.1186/s12859-014-0356-4](http://dx.doi.org/10.1186/s12859-014-0356-4). Download: [https://github.com/ANGSD/angsd](https://github.com/ANGSD/angsd)\n* **bedtools** Quinlan, A.R. & Hall, I.M., 2010. BEDTools: a flexible suite of utilities for comparing genomic features. Bioinformatics , 26(6), pp.841–842. Available at: [http://dx.doi.org/10.1093/bioinformatics/btq033](http://dx.doi.org/10.1093/bioinformatics/btq033). Download: [https://github.com/arq5x/bedtools2/releases](https://github.com/arq5x/bedtools2/)\n* **MALT**. Download: [https://software-ab.informatik.uni-tuebingen.de/download/malt/welcome.html](https://software-ab.informatik.uni-tuebingen.de/download/malt/welcome.html)\n  * Vågene, Å.J. et al., 2018. Salmonella enterica genomes from victims of a major sixteenth-century epidemic in Mexico. Nature ecology & evolution, 2(3), pp.520–528. Available at: [http://dx.doi.org/10.1038/s41559-017-0446-6](http://dx.doi.org/10.1038/s41559-017-0446-6).\n  * Herbig, A. et al., 2016. MALT: Fast alignment and analysis of metagenomic DNA sequence data applied to the Tyrolean Iceman. bioRxiv, p.050559. Available at: [http://biorxiv.org/content/early/2016/04/27/050559](http://biorxiv.org/content/early/2016/04/27/050559).\n* **MaltExtract** Huebler, R. et al., 2019. HOPS: Automated detection and authentication of pathogen DNA in archaeological remains. bioRxiv, p.534198. Available at: [https://www.biorxiv.org/content/10.1101/534198v1?rss=1](https://www.biorxiv.org/content/10.1101/534198v1?rss=1). Download: [https://github.com/rhuebler/MaltExtract](https://github.com/rhuebler/MaltExtract)\n* **Kraken2** Wood, D et al., 2019. Improved metagenomic analysis with Kraken 2. Genome Biology volume 20, Article number: 257. Available at: [https://doi.org/10.1186/s13059-019-1891-0](https://doi.org/10.1186/s13059-019-1891-0). Download: [https://ccb.jhu.edu/software/kraken2/](https://ccb.jhu.edu/software/kraken2/)\n* **endorS.py** Aida Andrades Valtueña (Unpublished). Download: [https://github.com/aidaanva/endorS.py](https://github.com/aidaanva/endorS.py)\n* **Bowtie2**  Langmead, B. and Salzberg, S. L. 2012 Fast gapped-read alignment with Bowtie 2. Nature methods, 9(4), p. 357–359. doi: [10.1038/nmeth.1923](https:/dx.doi.org/10.1038/nmeth.1923).\n* **sequenceTools** Stephan Schiffels (Unpublished). Download: [https://github.com/stschiff/sequenceTools](https://github.com/stschiff/sequenceTools)\n* **EigenstratDatabaseTools** Thiseas C. Lamnidis (Unpublished). Download: [https://github.com/TCLamnidis/EigenStratDatabaseTools.git](https://github.com/TCLamnidis/EigenStratDatabaseTools.git)\n* **mapDamage** Jónsson, H., et al 2013. mapDamage2.0: fast approximate Bayesian estimates of ancient DNA damage parameters. Bioinformatics , 29(13), 1682–1684. [https://doi.org/10.1093/bioinformatics/btt193](https://doi.org/10.1093/bioinformatics/btt193)\n* **BBduk** Brian Bushnell (Unpublished). Download: [https://sourceforge.net/projects/bbmap/](sourceforge.net/projects/bbmap/)\n\n## Data References\n\nThis repository uses test data from the following studies:\n\n* Fellows Yates, J. A. et al. (2017) ‘Central European Woolly Mammoth Population Dynamics: Insights from Late Pleistocene Mitochondrial Genomes’, Scientific reports, 7(1), p. 17714. [doi: 10.1038/s41598-017-17723-1](https://doi.org/10.1038/s41598-017-17723-1).\n* Gamba, C. et al. (2014) ‘Genome flux and stasis in a five millennium transect of European prehistory’, Nature communications, 5, p. 5257. [doi: 10.1038/ncomms6257](https://doi.org/10.1038/ncomms6257).\n* Star, B. et al. (2017) ‘Ancient DNA reveals the Arctic origin of Viking Age cod from Haithabu, Germany’, Proceedings of the National Academy of Sciences of the United States of America, 114(34), pp. 9152–9157. [doi: 10.1073/pnas.1710186114](https://doi.org/10.1073/pnas.1710186114).\n* de Barros Damgaard, P. et al. (2018). '137 ancient human genomes from across the Eurasian steppes.', Nature, 557(7705), 369–374. [doi: 10.1038/s41586-018-0094-2](https://doi.org/10.1038/s41586-018-0094-2)\n",129,bioinformatics,Nextflow,5,HTML,Python,Nextflow,Dockerfile,Groovy,,,,,,,,,,,,,,,,,,,,,,,,652,81,567,4,11,123,0,67273,78,426,383,43,65055295800d081fb5c5b9ae14267906c7c55b80,Merge pull request #1075 from nf-core/patch,2024-06-28T12:11:42Z,Thiseas C. Lamnidis,thisseass@gmail.com,TCLamnidis,[2.5.2] - Bopfingen (Patch) - 2024-06-28,"## [2.5.2] - 2024-06-28\r\n\r\n### `Added`\r\n\r\n- [#1079](https://github.com/nf-core/eager/issues/1079) - Added the `lanemerging` output directory in the output documentation (♥ to @TessaZei for reporting, fix by @TCLamnidis).\r\n\r\n### `Fixed`\r\n\r\n- [#1037](https://github.com/nf-core/eager/issues/1073) - Fixed post-adapterremoval trimmed FastQC results not being displayed in MultiQC (♥ to @kieren-j-mitchell for reporting, fix by @jfy133 and @TCLamnidis)\r\n\r\n### `Dependencies`\r\n\r\n### `Deprecated`\r\n",02.05.2002,Thiseas C. Lamnidis,,TCLamnidis,MIT License,eager,nf-core,29,nextflow,ancientdna,adna,pipeline,pathogen-genomics,population-genetics,nf-core,workflow,metagenomics,bioinformatics,genome,ancient-dna-analysis,,,,,,,,,/nf-core/eager,29,153,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/nextflow-io/nextflow,https://github.com/nextflow-io/nextflow,0,not scientific,,0,1,1,0,0,0,0,0,0,0,0,A DSL for data-driven computational pipelines,"<picture>\n  <source media=""(prefers-color-scheme: dark)"" srcset=""docs/_static/nextflow-logo-bg-dark.png"">\n  <source media=""(prefers-color-scheme: light)"" srcset=""docs/_static/nextflow-logo-bg-light.png"">\n  <img alt=""Nextflow Logo"" src=""docs/_static/nextflow-logo-bg-light.png"">\n</picture>\n\n*""Dataflow variables are spectacularly expressive in concurrent programming""*\n<br>[Henri E. Bal , Jennifer G. Steiner , Andrew S. Tanenbaum](https://dl.acm.org/doi/abs/10.1145/72551.72552)\n\n[![Nextflow CI](https://github.com/nextflow-io/nextflow/workflows/Nextflow%20CI/badge.svg)](https://github.com/nextflow-io/nextflow/actions/workflows/build.yml?query=branch%3Amaster+event%3Apush)\n[![Nextflow version](https://img.shields.io/github/release/nextflow-io/nextflow.svg?colorB=58bd9f&style=popout)](https://github.com/nextflow-io/nextflow/releases/latest)\n[![Nextflow Twitter](https://img.shields.io/twitter/url/https/nextflowio.svg?colorB=58bd9f&&label=%40nextflow&style=popout)](https://twitter.com/nextflowio)\n[![Nextflow Publication](https://img.shields.io/badge/Published-Nature%20Biotechnology-26af64.svg?colorB=58bd9f&style=popout)](https://www.nature.com/articles/nbt.3820)\n[![install with bioconda](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?colorB=58bd9f&style=popout)](http://bioconda.github.io/recipes/nextflow/README.html)\n[![Nextflow license](https://img.shields.io/github/license/nextflow-io/nextflow.svg?colorB=58bd9f&style=popout)](https://github.com/nextflow-io/nextflow/blob/master/COPYING)\n\nNextflow is a workflow system for creating scalable, portable, and reproducible workflows. It is based on the dataflow programming model, which greatly simplifies the writing of parallel and distributed pipelines, allowing you to focus on the flow of data and computation. Nextflow can deploy workflows on a variety of execution platforms, including your local machine, HPC schedulers, AWS Batch, Azure Batch, Google Cloud Batch, and Kubernetes. Additionally, it supports many ways to manage your software dependencies, including Conda, Spack, Docker, Podman, Singularity, and more.\n\n## Quick start\n\nInstall Nextflow with a single command:\n\n```bash\ncurl -fsSL https://get.nextflow.io | bash\n```\n\nIt creates the `nextflow` executable file in the current directory. You can then move it to a directory in your `$PATH` to run it from anywhere.\n\nNextflow can also be installed from Bioconda:\n\n```bash\nconda install -c bioconda nextflow\n```\n\n## Documentation\n\nThe Nextflow documentation is available for the latest [stable](https://nextflow.io/docs/latest/) and [edge](https://nextflow.io/docs/edge/) releases.\n\n## Community\n\nYou can post questions and get help in the [Nextflow community forum](https://community.seqera.io) or the [Nextflow Slack](https://www.nextflow.io/slack-invite.html). Bugs and feature requests should be reported as [GitHub issues](https://github.com/nextflow-io/nextflow/issues/new/choose).\n\nThe Nextflow community is highly active with regular community meetings, events, a podcast and more. You can view much of this material on the [Nextflow](https://www.youtube.com/@Nextflow) and [nf-core](https://www.youtube.com/@nf-core) YouTube channels.\n\nThe [nf-core](https://nf-co.re/) project is a community effort aggregating high quality Nextflow workflows which can be used by everyone.\n\n## Contributing\n\nContributions are more than welcome. See the [CONTRIBUTING](CONTRIBUTING.md) file for details.\n\n## License\n\nNextflow is released under the Apache 2.0 license. Nextflow is a [registered trademark](https://github.com/nextflow-io/trademark).\n\n## Citations\n\nIf you use Nextflow in your work, please cite:\n\nP. Di Tommaso, et al. Nextflow enables reproducible computational workflows. Nature Biotechnology 35, 316–319 (2017) doi:[10.1038/nbt.3820](http://www.nature.com/nbt/journal/v35/n4/full/nbt.3820.html)\n\n## Credits\n\nNextflow is built on two \*great* open-source software projects, <a href='http://groovy-lang.org' target='_blank'>Groovy</a>\nand <a href='http://www.gpars.org/' target='_blank'>GPars</a>.\n\n<a href='http://www.yourkit.com' target='_blank'>YourKit</a> is kindly supporting Nextflow with its fully-featured Java Profiler.\n",2629,bioinformatics,Groovy,10,Shell,Groovy,Java,HTML,Smarty,Makefile,JavaScript,Nextflow,Dockerfile,Perl,,,,,,,,,,,,,,,,,,,1325,415,850,60,191,170,0,57219,608,3136,2792,344,e562ce061c0133ab2d16d8601127d86e1aba8d55,Make Google Batch auto retry codes configurable (#5148),2024-07-16T17:55:52Z,Paolo Di Tommaso,paolo.ditommaso@gmail.com,pditommaso,Version 24.04.3,- Add ability to override failOnError setting default via env variable (#5117) [ci fast] [6852429c]\r\n- Fix normalization of consecutive slashes in uri path (#5114) [3f366b7e]\r\n- Fix executions hangs on finalisation exception (#5070) [4c207c23]\r\n- Bump nf-google@1.13.2-patch1 [55ec5ec5],v24.04.3,Paolo Di Tommaso,,pditommaso,Apache License 2.0,nextflow,nextflow-io,265,bioinformatics,workflow-engine,pipeline,pipeline-framework,nextflow,cloud,groovy,sge,slurm,aws,docker,singularity,hpc,singularity-containers,reproducible-science,reproducible-research,dataflow,hello,,,/nextflow-io/nextflow,296,87,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/NeuroDiffGym/neurodiffeq,https://github.com/NeuroDiffGym/neurodiffeq,0.5,,0,0,1,1,0,0,0,0,0,0,0,0,"A library for solving differential equations using neural networks based on PyTorch, used by multiple research groups around the world, including at Harvard IACS.","# neurodiffeq\n\n[![Downloads](https://static.pepy.tech/badge/neurodiffeq)](https://pepy.tech/project/neurodiffeq)\n[![Codacy Badge](https://api.codacy.com/project/badge/Grade/eada52ca726e4919923e213b81ee6420)](https://app.codacy.com/gh/odegym/neurodiffeq?utm_source=github.com&utm_medium=referral&utm_content=odegym/neurodiffeq&utm_campaign=Badge_Grade_Settings)\n![PyPI](https://img.shields.io/pypi/v/neurodiffeq?color=blueviolet&label=PyPI&logoColor=blueviolet) ![GitHub issues](https://img.shields.io/github/issues/NeuroDiffGym/neurodiffeq?color=green) [![Build Status](https://app.travis-ci.com/NeuroDiffGym/neurodiffeq.svg?branch=master)](https://app.travis-ci.com/NeuroDiffGym/neurodiffeq) [![codecov](https://codecov.io/gh/NeuroDiffGym/neurodiffeq/branch/master/graph/badge.svg)](https://codecov.io/gh/NeuroDiffGym/neurodiffeq) [![Documentation Status](https://readthedocs.org/projects/neurodiffeq/badge/?version=latest)](https://neurodiffeq.readthedocs.io/en/latest/?badge=latest) [![DOI](https://joss.theoj.org/papers/10.21105/joss.01931/status.svg)](https://doi.org/10.21105/joss.01931)\n\n# Citation\n\n```\n@article{chen2020neurodiffeq,\n  title={NeuroDiffEq: A Python package for solving differential equations with neural networks},\n  author={Chen, Feiyu and Sondak, David and Protopapas, Pavlos and Mattheakis, Marios and Liu, Shuheng and Agarwal, Devansh and Di Giovanni, Marco},\n  journal={Journal of Open Source Software},\n  volume={5},\n  number={46},\n  pages={1931},\n  year={2020}\n}\n```\n\n------\n\n**🔥🔥🔥Did you know that neurodiffeq supports solution bundles and can be used to solve reverse problems? [See here](#solution-bundle-and-reverse-problems)!**\n\n:mortar_board: **Already familiar with neurodiffeq?** :point_down: **[Jump to FAQs](#faq).**\n\n------\n\n# Introduction\n\n`neurodiffeq` is a package for solving differential equations with neural networks. Differential equations are equations that relate some function with its derivatives. They emerge in various scientific and engineering domains. Traditionally these problems can be solved by numerical methods (e.g. finite difference, finite element). While these methods are effective and adequate, their expressibility is limited by their function representation. It would be interesting if we can compute solutions for differential equations that are continuous and differentiable.\n\nAs universal function approximators, artificial neural networks have been shown to have the potential to solve ordinary differential equations (ODEs) and partial differential equations (PDEs) with certain initial/boundary conditions. The aim of `neurodiffeq` is to implement these existing techniques of using ANN to solve differential equations in a way that allow the software to be flexible enough to work on a wide range of user-defined problems.\n\n<p align='center'>\n  <a href='https://youtu.be/VDLwyFD-sXQ'>\n    <img src=""https://raw.githubusercontent.com/NeuroDiffGym/neurodiffeq/master/resources/watermark-cover.jpg"" width=""80%"">\n  </a>\n</p>\n\n# Installation\n\n## Using pip\n\nLike most standard libraries, `neurodiffeq` is hosted on [PyPI](https://pypi.org/project/neurodiffeq/). To install the latest stable relesase, \n\n```bash\npip install -U neurodiffeq  # '-U' means update to latest version\n```\n\n## Manually\n\nAlternatively, you can install the library manually to get early access to our new features. This is the recommended way for developers who want to contribute to the library.\n\n```bash\ngit clone https://github.com/NeuroDiffGym/neurodiffeq.git\ncd neurodiffeq && pip install -r requirements\npip install .  # To make changes to the library, use `pip install -e .`\npytest tests/  # Run tests. Optional.\n```\n\n# Getting Started\n\nWe are happy to help you with any questions. In the meantime, you can checkout the [FAQs](#faq).\n\nTo view complete tutorials and documentation of `neurodiffeq`, please check [Official Documentation](https://neurodiffeq.readthedocs.io/en/latest/). \n\nIn addition to the documentations, we have recently made a quick walkthrough [Demo Video](https://youtu.be/VDLwyFD-sXQ) with [slides](https://drive.google.com/file/d/1XTbwkZ0g7ufzD7lvMB-Cl8s5nh6jKgHk/view?usp=sharing).\n\n## Example Usages\n\n### Imports\n\n```python\nfrom neurodiffeq import diff\nfrom neurodiffeq.solvers import Solver1D, Solver2D\nfrom neurodiffeq.conditions import IVP, DirichletBVP2D\nfrom neurodiffeq.networks import FCNN, SinActv\n```\n\n### ODE System Example\n\nHere we solve a non-linear system of two ODEs, known as the [Lotka–Volterra](https://en.wikipedia.org/wiki/Lotka–Volterra_equations) equations. There are two unknown functions (`u` and `v`) and a single independent variable (`t`).\n\n```python\ndef ode_system(u, v, t): \n    return [diff(u,t)-(u-u*v), diff(v,t)-(u*v-v)]\n\nconditions = [IVP(t_0=0.0, u_0=1.5), IVP(t_0=0.0, u_0=1.0)]\nnets = [FCNN(actv=SinActv), FCNN(actv=SinActv)]\n\nsolver = Solver1D(ode_system, conditions, t_min=0.1, t_max=12.0, nets=nets)\nsolver.fit(max_epochs=3000)\nsolution = solver.get_solution()\n```\n\n`solution` is a callable object, you can pass in numpy arrays or torch tensors to it like\n\n```python\nu, v = solution(t, to_numpy=True)  # t can be np.ndarray or torch.Tensor\n```\n\nPlotting `u` and `v` against their analytical solutions yields something like:\n\n![lotka–volterra-solution](resources/lotka–volterra-solution.png)\n\n### PDE System Example\n\nHere we solve a Laplace Equation with Dirichlet boundary conditions on a rectangle. Note that we choose Laplace equation for its simplicity of computing analytical solution. **In practice, you can attempt any nonlinear, chaotic PDEs**, provided you tune the solver well enough.\n\nSolving a 2-D PDE system is quite similar to solving ODEs, except there are *two* variables `x` and `y` for boundary value problems or `x` and `t` for initial boundary value problems, both of which are supported.\n\n```python\ndef pde_system(u, x, y):\n    return [diff(u, x, order=2) + diff(u, y, order=2)]\n\nconditions = [\n    DirichletBVP2D(\n        x_min=0, x_min_val=lambda y: torch.sin(np.pi*y),\n        x_max=1, x_max_val=lambda y: 0,                   \n        y_min=0, y_min_val=lambda x: 0,                   \n        y_max=1, y_max_val=lambda x: 0,                   \n    )\n]\nnets = [FCNN(n_input_units=2, n_output_units=1, hidden_units=(512,))]\n\nsolver = Solver2D(pde_system, conditions, xy_min=(0, 0), xy_max=(1, 1), nets=nets)\nsolver.fit(max_epochs=2000)\nsolution = solver.get_solution()\n```\n\nThe signature of `solution` for a 2D PDE is slightly different from that of an ODE. Again, it takes in either numpy arrays or torch tensors.\n\n```python\nu = solution(x, y, to_numpy=True)\n```\nEvaluating u on `[0,1] × [0,1]` yields the following plots\n\n|                 ANN-Based Solution                  |                    Residual of PDE                           |\n| :-------------------------------------------------: | :----------------------------------------------------------: |\n| ![laplace-solution](resources/laplace-solution.png) | ![laplace-error](resources/laplace-error.png)                |\n\n### Using a Monitor\n\nA monitor is a tool for visualizing PDE/ODE solutions as well as history of loss and custom metrics during training. Jupyter Notebooks users need to run the `%matplotlib notebook` magic. For Jupyter Lab users, try `%matplotlib widget`. \n\n```python\nfrom neurodiffeq.monitors import Monitor1D\n...\nmonitor = Monitor1D(t_min=0.0, t_max=12.0, check_every=100)\nsolver.fit(..., callbacks=[monitor.to_callback()])\n```\n\nYou should see the plots update *every 100 epoch* as well as *on the last epoch*, showing two plots — one for solution visualization on the interval `[0,12]` and the other for loss history (training and validation). \n\n![monitor](resources/monitor.gif)\n\n### Custom Networks\n\nFor convenience, we have implemented an `FCNN` – fully-connected neural network, whose hidden units and activation functions can be customized. \n\n```python\nfrom neurodiffeq.networks import FCNN\n# Default: n_input_units=1, n_output_units=1, hidden_units=[32, 32], activation=torch.nn.Tanh\nnet1 = FCNN(n_input_units=..., n_output_units=..., hidden_units=[..., ..., ...], activation=...) \n...\nnets = [net1, net2, ...]\n```\n\n`FCNN` is usually a good starting point. For advanced users, solvers are compatible with any custom `torch.nn.Module`. The only constraints are:\n\n1. The modules takes in a tensor of shape `(None, n_coords)` and the outputs a tensor of shape `(None, 1)`. \n\n2. There must be a total of `n_funcs` modules in `nets` to be passed to `solver = Solver(..., nets=nets)`.\n\n![monitor](resources/nets.png)\n\n*Acutally, `neurodiffeq` has a **single_net** feature that doesn't obey the above rules, which won't be covered here.*\n\nRead the PyTorch [tutorial](https://pytorch.org/docs/stable/notes/modules.html) on building your own network (a.k.a module) architecture. \n\n### Transfer Learning\n\nTransfer learning is easily done by serializing `old_solver.nets` (a list of torch modules) to disk and then loading them and passing to a new solver:\n\n```python\nold_solver.fit(max_epochs=...)\n# ... dump `old_solver.nets` to disk\n\n# ... load the networks from disk, store them in some `loaded_nets` variable\nnew_solver = Solver(..., nets=loaded_nets)\nnew_solver.fit(max_epochs=...)\n```\n\nWe currently working on wrapper functions to save/load networks and other internal variables of Solvers. In the meantime, you can read the PyTorch [tutorial](https://pytorch.org/tutorials/beginner/saving_loading_models.html) on saving and loading your networks.\n\n### Sampling Strategies\n\nIn neurodiffeq, the networks are trained by minimizing loss (ODE/PDE residuals) evaluated on a set of points in the domain. The points are randonly resampled every time. To control the number, distribution, and bounding domain of sampled points, you can specify your own training/valiadation `generator`s.\n\n```python\nfrom neurodiffeq.generators import Generator1D\n\n# Default t_min=0.0, t_max=1.0, method='uniform', noise_std=None\ng1 = Generator1D(size=..., t_min=..., t_max=..., method=..., noise_std=...)\ng2 = Generator1D(size=..., t_min=..., t_max=..., method=..., noise_std=...)\n\nsolver = Solver1D(..., train_generator=g1, valid_generator=g2)\n```\n\nHere are  some sample distributions of a `Generator1D`.\n\n|      `Generator1D(8192, 0.0, 1.0, method='uniform')`      | `Generator1D(8192, -1.0, 0.0, method='log-spaced-noisy', noise_std=1e-3)` |\n| :-------------------------------------------------------: | :----------------------------------------------------------: |\n| ![generator1d-uniform](resources/generator1d-uniform.jpg) | ![generator1d-log-spaced-noisy](resources/generator1d-log-spaced-noisy.jpg) |\n\n\n\nNote that when both `train_generator` and `valid_generator` are specified, `t_min` and `t_max` can be omitted in `Solver1D(...)`. In fact, even if you pass `t_min`, `t_max`, `train_generator`, `valid_generator` together, the `t_min` and `t_max` will still be ignored.\n\n#### Combining Generators\n\nAnother nice feature of the generators is that you can concatenate them, for example \n\n```python\ng1 = Generator2D((16, 16), xy_min=(0, 0), xy_max=(1, 1))\ng2 = Generator2D((16, 16), xy_min=(1, 1), xy_max=(2, 2))\ng = g1 + g2\n```\n\nHere, `g` will be a generator that outputs the combined samples of `g1` and `g2`\n\n|                     `g1`                      |                     `g2`                      |                        `g1 + g2`                        |\n| :-------------------------------------------: | :-------------------------------------------: | :-----------------------------------------------------: |\n| ![generator2d-1](resources/generator2d-1.jpg) | ![generator2d-2](resources/generator2d-2.jpg) | ![generator2d-concat](resources/generator2d-concat.jpg) |\n\n#### Sampling Higher Dimensions\n\nYou can use `Generator2D`, `Generator3D`, etc. for sampling points in higher dimensions. But there's also another way\n\n```python\ng1 = Generator1D(1024, 2.0, 3.0, method='uniform')\ng2 = Generator1D(1024, 0.1, 1.0, method='log-spaced-noisy', noise_std=0.001)\ng = g1 * g2\n```\n\nHere, `g` will be a generator which yields 1024 points in a 2-D rectangle `(2,3) × (0.1,1)` every time. The x-coordinates of them are drawn from `(2,3)` using strategy `uniform` and the y-coordinate drawn from `(0.1,1)` using strategy `log-spaced-noisy`.\n\n|                      `g1`                       |                      `g2`                       |                          `g1 * g2`                           |\n| :---------------------------------------------: | :---------------------------------------------: | :----------------------------------------------------------: |\n| ![generator2d-1](resources/generator-ens-1.jpg) | ![generator2d-2](resources/generator-ens-2.jpg) | ![generator2d-concat](resources/generator-ens-ensembled.jpg) |\n\n# Solution Bundle and Reverse Problems\n\nSometimes, it is interesting to solve a ***bundle*** of equations at once. For example, you may want to solve differential equations of the form `du/dt + λu = 0` under the initial condition `u(0) = U0`. You may want to solve this for all `λ` and `U0` at once, by treating them as inputs to the neural networks. \n\nOne such application is for chemical reactions, where the reaction rate is unknown. Different reaction rates correspond to different solutions, and only one solution matches observed data points. You maybe interested in first solving for a bundle of solutions, and then determining the best reaction rates (aka equation parameters). The second step is known as the ***inverse problem***. \n\nHere's an example of how to do this using `neurodiffeq`:\n\n1. Let's say we have an equation `du/dt + λu = 0` and initial condition `u(0) = U0` where `λ` and `U0` are unknown constants. We also have a set of observations `t_obs` and `u_obs`. We first import `BundleSolver` and `BundleIVP` which is necessary to obtaining a solution bundle:\n\n   ```python\n   from neurodiffeq.conditions import BundleIVP\n   from neurodiffeq.solvers import BundleSolver1D\n   \n   import matplotlib.pyplot as plt\n   import numpy as np\n   import torch\n   from neurodiffeq import diff\n   ```\n\n2. We determine the domain of input `t`, as well as the domain of parameters  `λ` and `U0`. We also need to make a decision of the order of the parameters. Namely, which should be the first parameter, and which should be the second. **For the purpose of this demo, we choose `λ` to be the first parameter (index 0), and `U0` to be the second (index 1). It is very important to keep track of the indices of the parameters.**\n\n   ```python\n   T_MIN, T_MAX = 0, 1\n   LAMBDA_MIN, LAMBDA_MAX = 3, 5  # first parameter,  index = 0\n   U0_MIN, U0_MAX = 0.2, 0.6       # second parameter, index = 1\n   ```\n\n3. We then define the `conditions` and `solver` as usual, except that we use `BundleIVP` and `BundleSolver1D` instead of `IVP` and `Solver1D`. The interface of these two is very similar to `IVP` and `Solver1D`. You can find out more in the [API reference](https://neurodiffeq.readthedocs.io/en/latest/api.html). \n\n   ```python\n   # equation parameters comes after inputs (usually temporal and spatial coordinates)\n   diff_eq = lambda u, t, lmd: [diff(u, t) + lmd * u]\n   \n   # The keyword argument must be named ""u_0"" in BundleIVP. If you use anything else, e.g. `y0`, `u0`, etc., it won't work.\n   conditions = [\n       BundleIVP(t_0=0, u_0=None, bundle_param_lookup={'u_0': 1})  # u_0 has index 1\n   ]\n   \n   solver = BundleSolver1D(\n       ode_system=diff_eq,\n       conditions=conditions,\n       t_min=T_MIN, t_max=T_MAX, \n       theta_min=[LAMBDA_MIN, U0_MIN],  # λ has index 0; u_0 has index 1\n       theta_max=[LAMBDA_MAX, U0_MAX],  # λ has index 0; u_0 has index 1\n       eq_param_index=(0,),             # λ is the only equation parameter, which has index 0\n       n_batches_valid=1,\n   )\n   ```\n\n   Since **`λ` is a parameter in the equation** and **`U0` is a parameter in the initial condition**, we must include `λ` in the `diff_eq` and `U0` in the condition. If a parameter is present in both the equation and the condition, it must be included in both places. **All elements of `conditions` passed to `BundleSovler1D` must be `Bundle*` conditions, even if they don't have parameters.**\n\n4. Now, we can train it and obtain the solution as we normally would. \n\n   ```python\n   solver.fit(max_epochs=1000)\n   solution = solver.get_solution(best=True)\n   ```\n\n   The solution expects three inputs - `t`, `λ` and `U0`. All inputs must have the same shape. For example, if you are interested in fixing `λ=4` and `U0=0.4` and plotting the solution `u` against `t ∈ [0,1]` , you can do the following\n\n   ```python\n   t = np.linspace(0, 1)\n   lmd = 4 * np.ones_like(t)\n   u0 = 0.4 * np.ones_like(t)\n   \n   u = solution(t, lmd, u0, to_numpy=True)\n   \n   import matplotlib.pyplot as plt\n   plt.plot(t, u)\n   ```\n\n5. Once you have a bundled `solution`, you can find a set of parameters `(λ, U0)` that matches observed data points `(t_i, u_i)` most closely. This is achieved using simple gradient descent. In the following toy example, we assume there are only three data points `u(0.2) = 0.273`, `u(0.5)=0.129`, and `u(0.8) = 0.0609`. The following is classical PyTorch workflow.\n\n   ```python\n   # observed data points\n   t_obs = torch.tensor([0.2, 0.5, 0.8]).reshape(-1, 1)\n   u_obs = torch.tensor([0.273, 0.129, 0.0609]).reshape(-1, 1)\n   \n   # random intialization of λ and U0; keep track of their gradient\n   lmd_tensor = torch.rand(1) * (LAMBDA_MAX - LAMBDA_MIN) + LAMBDA_MIN\n   u0_tensor = torch.rand(1) * (U0_MAX - U0_MIN) + U0_MIN\n   adam = torch.optim.Adam([lmd_tensor.requires_grad_(True), u0_tensor.requires_grad_(True)], lr=1e-2)\n   \n   # run gradient descent for 10000 epochs\n   for _ in range(10000):\n       output = solution(t_obs, lmd_tensor * torch.ones_like(t_obs), u0_tensor * torch.ones_like(t_obs))\n       loss = ((output - u_obs) ** 2).mean()\n       loss.backward()\n       adam.step()\n       adam.zero_grad()\n      \n   print(f""λ = {lmd_tensor.item()}, U0={u0_tensor.item()}, loss = {loss.item()}"")\n   ```\n\n# FAQ\n\n#### Q: How to use GPU for training?\n\nSimple. When importing neurodiffeq, the library automatically detects if CUDA is available on your machine. Since the library is based on PyTorch, it will set default tensor type to `torch.cuda.DoubleTensor` for if a compatible GPU device is found.\n\n#### Q: How to use pretrained nets?\n\nRefer to Sections [Custom Networks](#custom-networks) and [Transfer Learning](#transfer-learning).\n\n#### Q: How to change the learning rate?\n\nThe standard PyTorch way. \n\n1. Build your networks as explained in [Custom Networks](#custom-networks): `nets = [FCNN(), FCN(), ...]`\n\n2. Instantiate a custom optimizer and pass all parameters of these networks to it\n\n   ```python\n   parameters = [p for net in nets for p in net.parameters()]  # list of paramters of all networks\n   MY_LEARNING_RATE = 5e-3\n   optimizer = torch.optim.Adam(parameters, lr=MY_LEARNING_RATE, ...)\n   ```\n\n3. Pass BOTH your `nets ` and your `optimizer` to the solver: `solver = Solver1D(..., nets=nets, optimizer=optimizer)`\n\n#### Q: I got a bad solution.\n\nUnlike traditional numerial methods (FEM, FVM, etc.), the NN-based solution requires some hypertuning. The library offers the utmost flexibility to try any combination of hyperparameters.\n\n- To use a different network architecture, you can pass in your custom `torch.nn.Module`s.\n- To use a different optimizer, you can pass in your own optimizer to `solver = Solver(..., optimizer=my_optim)`. \n- To use a different sampling distribution, you can use [built-in generators](https://neurodiffeq.readthedocs.io/en/latest/api.html#module-neurodiffeq.generators) or write your own generators from scratch.\n- To use a different sampling size, you can tweak the generators or change `solver = Solver(..., n_batches_train)`.\n- To dynamically change hyperparameters during training, checkout our [callbacks](https://neurodiffeq.readthedocs.io/en/latest/api.html#module-neurodiffeq.callbacks) feature.\n\n#### Q: Any rules of thumbs?\n\n- Don't use `ReLU` for activation, because its second-order derivative is identically 0.\n- Re-scale your PDE/ODE in dimensionless form, preferably make everything range in `[0,1]`. Working with a domain like `[0,1000000]` is prone to failure because **a)** PyTorch initializes the modules weights to be relatively small and **b)** most activation functions (like Sigmoid, Tanh, Swish) are most nonlinear near 0.\n- If your PDE/ODE is too complicated, consider trying curriculum learning. Start training your networks on a smaller domain, and then gradually expand until the whole domain is covered.\n\n# Contributing\n\nEveryone is welcome to contribute to this project.\n\nWhen contributing to this repository, we consider the following process:\n\n1. Open an issue to discuss the change you are planning to make.\n2. Go through [Contribution Guidelines](CONTRIBUTING.md).\n3. Make the change on a forked repository and update the README.md if changes are made to the interface.\n4. Open a pull request. \n\n",664,pde-solver,Python,5,Python,TeX,Dockerfile,Shell,Batchfile,,,,,,,,,,,,,,,,,,,,,,,,123,18,95,10,25,28,0,144358,87,91,68,23,2e8ef812096ccb949ccb900581cd5e33828d47de,APTx Function: Default value of gamma should be 0.5 when trainable=Fa…,2024-07-15T15:55:10Z,Ravin Kumar,16964978+mr-ravin@users.noreply.github.com,mr-ravin,Release of v0.6.3,Happy 2024 🎆 🥳 \r\n- Add support for selecting a particular device when there are multiple GPU devices.\r\n- Fix an bug when loading a solver whose `.nets` have shared instance(s) of `torch.nn.Module`.\r\n- Miscellaneous documentation improvement.\r\n- PR CI is up and running again\r\n,v0.6.3,Shuheng Liu,,shuheng-liu,MIT License,neurodiffeq,NeuroDiffGym,19,differential-equations,neural-networks,pytorch,pde-solver,odes,artificial-intelligence,deep-learning,physics-informed-neural-networks,pypi,time-series,mathematical-modelling,scientific-computing,ode,pinn,boundary-value-problem,initial-value-problem,,,,,/NeuroDiffGym/neurodiffeq,19,28,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/NetTopologySuite/NetTopologySuite.IO.GeoJSON,https://github.com/NetTopologySuite/NetTopologySuite.IO.GeoJSON,0,,,0,0,0,0,0,0,1,0,0,0,0,GeoJSON IO module for NTS.,"# GeoJSON\n\n[GeoJSON (RFC 7946)](https://geojson.org) IO module for NTS. \n\n## GeoJSON4STJ Usage\n\nThis is the package for System.Text.Json serialization and deserialization.\n\n### ASP.NET Core Example\n\nAdd the `System.Text.Json.Serializer.JsonConverterFactory`, `GeoJsonConverterFactory`, to the `JsonSerializerOptions` when you configure your controllers, MVC, etc in the `ConfigureServices` method of your `Startup.cs` class.\n\n```csharp\npublic void ConfigureServices(IServiceCollection services) {\n  services.AddControllers()\n  .AddJsonOptions(options => {\n    options.JsonSerializerOptions.Converters.Add(new NetTopologySuite.IO.Converters.GeoJsonConverterFactory());\n  });\n}\n````\n\n## GeoJSON Usage\n\n**GeoJSON to `Geometry`**:\n\n```c#\nvar geoJson = ""{\""type\"":\""Point\"",\""coordinates\"":[0.0,0.0]}"";\nGeometry geometry;\n\nvar serializer = GeoJsonSerializer.Create();\nusing (var stringReader = new StringReader(geoJson))\nusing (var jsonReader = new JsonTextReader(stringReader))\n{\n    geometry = serializer.Deserialize<Geometry>(jsonReader);\n}\n```\n\n**`Geometry` to GeoJSON**:\n\n```c#\nvar geometry = new Point(0, 0);\nstring geoJson;\n\nvar serializer = GeoJsonSerializer.Create();\nusing (var stringWriter = new StringWriter())\nusing (var jsonWriter = new JsonTextWriter(stringWriter))\n{\n    serializer.Serialize(jsonWriter, geometry);\n    geoJson = stringWriter.ToString();\n}\n```\n\n",104,geometry,C#,1,C#,,,,,,,,,,,,,,,,,,,,,,,,,,,,44,6,33,5,8,20,0,716,45,100,88,12,76cdb7db6e69e9d1c5a1aed2950913c826d88839,Deserialize MultiPolygon Array of Empty Coordinates Array (#140),2024-05-01T11:06:01Z,Matthew Dickinson,dickinsonm@gmail.com,dickinsonm,v4.0.0,"## What's Changed\r\n* Support a variant of JsonElementAttributesTable that can be modified in-place, at a cost to reading performance. by @airbreather in https://github.com/NetTopologySuite/NetTopologySuite.IO.GeoJSON/pull/118\r\n* Fix feature serialization when attributes or geometry are null (#128) by @jjanuszkiewicz in https://github.com/NetTopologySuite/NetTopologySuite.IO.GeoJSON/pull/129\r\n* fix code style violation errors by @jjanuszkiewicz in https://github.com/NetTopologySuite/NetTopologySuite.IO.GeoJSON/pull/124\r\n* add GeoJsonConverterFactory constructors that don't require a GeometryFactory by @jjanuszkiewicz in https://github.com/NetTopologySuite/NetTopologySuite.IO.GeoJSON/pull/125\r\n* Fix serialization bug by @alex2512 in https://github.com/NetTopologySuite/NetTopologySuite.IO.GeoJSON/pull/130\r\n* Homogenize exceptions in GeoJSON by @FObermaier in https://github.com/NetTopologySuite/NetTopologySuite.IO.GeoJSON/pull/132\r\n* Slightly better exception types in 4STJ by @airbreather in https://github.com/NetTopologySuite/NetTopologySuite.IO.GeoJSON/pull/134\r\n\r\n## New Contributors\r\n* @jjanuszkiewicz made their first contribution in https://github.com/NetTopologySuite/NetTopologySuite.IO.GeoJSON/pull/129\r\n* @alex2512 made their first contribution in https://github.com/NetTopologySuite/NetTopologySuite.IO.GeoJSON/pull/130\r\n\r\n**Full Changelog**: https://github.com/NetTopologySuite/NetTopologySuite.IO.GeoJSON/compare/v3.0.0...v4.0.0",v4.0.0,Felix Obermaier,,FObermaier,"BSD 3-Clause ""New"" or ""Revised"" License",NetTopologySuite.IO.GeoJSON,NetTopologySuite,17,geojson,nettopologysuite,json,geometry,library,,,,,,,,,,,,,,,,/NetTopologySuite/NetTopologySuite.IO.GeoJSON,17,13,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/Nesvilab/philosopher,https://github.com/Nesvilab/philosopher,1,,,1,1,1,1,0,0,0,0,0,0,1,"PeptideProphet, PTMProphet, ProteinProphet, iProphet, Abacus, and FDR filtering","<p align=""center"">\n  <img height=""420"" width=""593"" src=""/images/philosopher.png"">\n</p>\n\n[![Release](https://img.shields.io/github/release/nesvilab/philosopher.svg?color=purple&style=for-the-badge)](https://github.com/Nesvilab/philosopher/releases/latest)\n![Golang](https://img.shields.io/badge/Go-1.19.4-blue.svg?style=for-the-badge)\n[![Go Report Card](https://goreportcard.com/badge/github.com/Nesvilab/philosopher?style=for-the-badge&color=red&logo=appveyor)](https://goreportcard.com/report/github.com/Nesvilab/philosopher)\n![GitHub](https://img.shields.io/github/license/Nesvilab/philosopher?style=for-the-badge)\n![](https://img.shields.io/github/downloads/Nesvilab/philosopher/total.svg?color=red&style=for-the-badge)\n![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/Nesvilab/philosopher/go.yml?style=for-the-badge)\n\n#### Philosopher is a fast, easy-to-use, scalable, and versatile data analysis software for mass spectrometry-based proteomics. It is also a depencency-free wraper of Trans-Proteomic Pipeline (PeptideProphet, iProphet, PTMProphet, and ProteinProphet).\n\n- Database downloading and formatting.\n\n- Peptide assignment validation with PeptideProphet.\n\n- Multi-level integrative analysis with iProphet.\n\n- PTM site localization with PTMProphet.\n\n- Protein inference with ProteinProphet.\n\n- FDR filtering with custom algorithms.\n\n  - Two-dimensional filtering for simultaneous control of PSM and Protein FDR levels.\n  - Sequential FDR estimation for large data sets using filtered PSM and proteins lists.\n\n- Label-free quantification via spectral counting and MS1 intensities.\n\n- Label-based quantification using TMT and iTRAQ.\n\n- Multi-level detailed reports for peptides, ions, and proteins.\n\n- Support for REPRINT and MSstats.\n\n\n## How to Use\nPhilosopher is part of [FragPipe](https://fragpipe.nesvilab.org/) which has a user-friendly GUI.\n\n## Documentation\nSee the [documentation](https://github.com/Nesvilab/philosopher/wiki/Home) for more details about the available commands.\n\n## Questions, requests and bug reports\nIf you have any questions or remarks please use the [Discussion board](https://github.com/Nesvilab/philosopher/discussions). If you want to report a bug, please use the [Issue tracker](https://github.com/nesvilab/philosopher/issues).\n\n\n## How to cite\nda Veiga Leprevost F, Haynes SE, Avtonomov DM, Chang HY, Shanmugam AK, Mellacheruvu D, Kong AT, Nesvizhskii AI. [Philosopher: a versatile toolkit for shotgun proteomics data analysis](https://doi.org/10.1038/s41592-020-0912-y). Nat Methods. 2020 Sep;17(9):869-870. doi: 10.1038/s41592-020-0912-y. PMID: 32669682; PMCID: PMC7509848.\n\n## About the authors\n[Alexey Nesvizhskii's research group](http://www.nesvilab.org/)\n",109,mass-spectrometry,Go,3,Dockerfile,Go,Makefile,,,,,,,,,,,,,,,,,,,,,,,,,,69,8,61,0,2,7,196,683056,17,317,300,17,336013f17f1c1c26b6ca4ebeae67a85d0966c1b4,Bump to 5.1.1-rc13,2024-05-29T13:49:47Z,Fengchao,fcyucn@gmail.com,fcyu,Philosopher 5.1.0,"### Added\r\n- Supported IBT-16 isobaric tag\r\n- Extended support for up to 32 isobaric labels.\r\n- Added the 'minPepLen' control option for protein probability assignment\r\n- Added the dbbin option to resue the prebuilt db.bin file\r\n\r\n### Changed\r\n- Used unique+razor peptides only for PSM-protein level roll up\r\n- For plex with 0 PSMs, let philosopher print a warning and exit normally\r\n- Excluded decoys from protein quantification (protein.tsv)\r\n- Removed the warning message for running 'workspace --clean' in a clean direcotry\r\n\r\n### Fixed\r\n- Fixed a problem occurring when parsing custom database with headers starting with '>AT'\r\n- Fixed a bug related to summed intensites in protein.tsv\r\n- Fixed a bug causing empty intensities in msstats.tsv related with multiplex TMT11 data.\r\n- Fixed a crash when there are non-zero TMT intensities and removelow > 0\r\n- Updated the purity calculation",v5.1.0,Yamei Deng,,AimeeD90,GNU General Public License v3.0,philosopher,Nesvilab,37,bioinformatics,proteomics,mass-spectrometry,ms-data,go,data-analysis,,,,,,,,,,,,,,,/Nesvilab/philosopher,38,16,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/Nesvilab/FragPipe,https://github.com/Nesvilab/FragPipe,1,,,1,1,1,1,0,0,0,0,0,0,1,A cross-platform Graphical User Interface (GUI) for running MSFragger and Philosopher - powered pipeline for comprehensive analysis of shotgun proteomics data,"![Release](https://img.shields.io/github/release/Nesvilab/FragPipe.svg) ![Downloads](https://img.shields.io/github/downloads/Nesvilab/FragPipe/total.svg) ![Docker pulls](https://img.shields.io/docker/pulls/fcyucn/fragpipe) ![Downloads](https://img.shields.io/github/downloads/Nesvilab/FragPipe/latest/total.svg)\n\n<div align=""center"">\n<img src=""https://raw.githubusercontent.com/Nesvilab/FragPipe/develop/frag-pipe/images/fragpipe-01.png"" width=""350px""/>\n</div>\n\nFragPipe is a comprehensive computational platform for analyzing mass spectrometry-based proteomics data. FragPipe comes with an easy to use Java Graphical User Interface (GUI) but can also be run in the command line mode, on Windows, Linux, or in the cloud environment. It is powered by [MSFragger](https://msfragger.nesvilab.org/) - an ultrafast proteomic search engine suitable for both conventional and ""open"" (wide precursor mass tolerance) peptide identification. FragPipe includes Percolator and the [Philosopher](https://nesvilab.github.io/philosopher/) toolkit for downstream post-processing of MSFragger search results (PeptideProphet, iProphet, ProteinProphet), FDR filtering, label-based quantification, and multi-experiment summary report generation. FragPipe includes MSBooster module for deep-learning based rescoring of peptide identifications. [Crystal-C](https://www.nesvilab.org/Crystal-C/) and [PTM-Shepherd](https://github.com/Nesvilab/PTM-Shepherd) are included to aid interpretation of open search results. Also included in FragPipe binary are [TMT-Integrator](http://tmt-integrator.nesvilab.org/) for TMT/iTRAQ isobaric labeling-based quantification, [IonQuant](http://ionquant.nesvilab.org/) for label-free quantification with FDR-controlled match-between-run (MBR) functionality, spectral library building with EasyPQP, and MSFragger-DIA, DIA-Umpire SE, and diaTracer modules for direct (""library-free"") analysis of data independent acquisition (DIA) data. FragPipe includes DIA-NN for extraction of quantification from DIA data.    \n\n\n### [Download](https://github.com/Nesvilab/FragPipe/releases)\n#### [Docker image](https://hub.docker.com/r/fcyucn/fragpipe)\n\n#### FragPipe tutorials\n* [Using FragPipe](https://fragpipe.nesvilab.org/docs/tutorial_fragpipe.html) (general tutorial covering all FragPipe modules)\n* [Running FragPipe in command line interface](https://fragpipe.nesvilab.org/docs/tutorial_headless.html)\n* PTM discovery\n  * [Open search](https://fragpipe.nesvilab.org/docs/tutorial_open.html)\n  * [Mass offset search](https://fragpipe.nesvilab.org/docs/tutorial_offset.html)\n  * [Labile PTM search](https://fragpipe.nesvilab.org/docs/tutorial_labile.html)\n  * [Glycoproteomics search](https://fragpipe.nesvilab.org/docs/tutorial_glyco.html)\n  * [Custom mass offset workflow (RNA crosslinking example)](https://fragpipe.nesvilab.org/docs/tutorial_custom_mass_offset.html) \n  * [Diagnostic ion mining](https://fragpipe.nesvilab.org/docs/tutorial_diagnostic_mining.html)\n  * [FPOP](https://fragpipe.nesvilab.org/docs/tutorial_fpop.html)\n  \n* TMT/iTRAQ quantification\n  * [Single plex](https://fragpipe.nesvilab.org/docs/tutorial_tmt.html)\n  * [Multiple plexes with a pooled reference sample](https://fragpipe.nesvilab.org/docs/tutorial_tmt-2plexes.html)\n  * [Streamlined activity-based protein profiling of reactive cysteines (SLC-ABPP)](https://fragpipe.nesvilab.org/docs/tutorial_abpp.html)\n* [Label-free quantification](https://fragpipe.nesvilab.org/docs/tutorial_lfq.html)\n* [SILAC (or other MS1-labeled) data](https://fragpipe.nesvilab.org/docs/tutorial_silac.html)\n* [DIA analysis](https://fragpipe.nesvilab.org/docs/tutorial_DIA.html)\n* [Novel/variant peptide detection using two-pass search](https://fragpipe.nesvilab.org/docs/tutorial_two_pass_search.html)\n* [Group FDR estimation for novel/variant peptide analysis](https://fragpipe.nesvilab.org/docs/tutorial_group_fdr.html)\n\n#### Resources\n* [Interpreting output files](https://fragpipe.nesvilab.org/docs/tutorial_fragpipe_outputs.html)\n* [List of built-in workflows](https://fragpipe.nesvilab.org/docs/tutorial_fragpipe_workflows.html)\n* [FragPipe setup](https://fragpipe.nesvilab.org/docs/tutorial_setup_fragpipe.html)\n* [Converting LC/MS data files to mzML](https://fragpipe.nesvilab.org/docs/tutorial_convert.html)\n* [Setting up FragPipe on remote Linux server (with X forwarding)](https://fragpipe.nesvilab.org/docs/tutorial_setup_x_forwarding.html)\n\n#### Using FragPipe with other tools\n* [Running MSstats with IonQuant results](https://fragpipe.nesvilab.org/docs/tutorial_msstats.html)\n* [Importing results into Skyline](https://fragpipe.nesvilab.org/docs/tutorial_skyline.html)\n* [Importing results into Perseus](https://fragpipe.nesvilab.org/docs/tutorial_perseus.html)\n\n\n\n#### Supported instruments and file formats  \nThe table below shows the compatibility of FragPipe workflow components with different spectral file formats.\n\n_Bruker .d indicates ddaPASEF files from timsTOF, other Bruker .d files should be converted to .mzML. Please also note that timsTOF data requires [Visual C++ Redistributable for Visual Studio 2017](https://aka.ms/vs/16/release/VC_redist.x64.exe) in Windows. If you see an error saying cannot find Bruker native library, please try to install the Visual C++ redistibutable._\n\n| Workflow Step                      | .mzML | Thermo (.raw) | Bruker (.d) |  .mgf |\n|------------------------------------|:-----:|:-------------:|:-----------:|:-----:|\n| DIA-Umpire pseudo-MS/MS generation | ✔     | ✔             |             |       | \n| diaTracer pseudo-MS/MS generation  |       |               | ✔           |       | \n| MSFragger search                   | ✔     | ✔             | ✔           | ✔     | \n| MSFragger-DIA                      | ✔     | ✔             |             |       | \n| Crystal-C artifact removal         | ✔     | ✔             |             |       | \n| PTMProphet localization            | ✔     | ✔             | ✔           |       | \n| PTM-Shepherd summarization         | ✔     | ✔             | ✔           |       | \n| Label-free quantification          | ✔     | ✔             | ✔           |       | \n| SILAC/dimethyl quantification      | ✔     | ✔             | ✔           |       | \n| TMT/iTRAQ quantification           | ✔     | ✔             |             |       | \n| Spectral library generation        | ✔     | ✔             | ✔           | ✔     | \n| DIA-NN quantification              | ✔     | ✔*            | ✔           |       | \n\n_DIA data acquired with overlapping/staggered windows must be [converted to mzML with demultiplexing](https://fragpipe.nesvilab.org/docs/tutorial_convert.html#convert-thermo-dia-raw-files-with-overlappingstaggered-windows)._\n_Quantification from Thermo .raw files with DIA-NN requires installation of Thermo MS File Reader, see the [DIA-NN documentation](https://github.com/vdemichev/DiaNN#raw-data-formats) for details._\n\nPlease note TMT/iTRAQ quantification from Thermo .raw files will take longer than from .mzML files.\n\n\n\n#### Additional Documentation\nComplete MSFragger documentation can be found on the [MSFragger wiki](https://github.com/Nesvilab/MSFragger/wiki).\nFor documentation on the Philosopher toolkit see the [Philosopher wiki](https://github.com/Nesvilab/philosopher/wiki).\n\n#### Questions and Technical Support\nView previous questions/bug reports in the\n[FragPipe issue tracker](https://github.com/Nesvilab/FragPipe/issues). Please post any new questions/bug reports regarding FragPipe itself here as well.\nFor questions specific to individual components of FragPipe you can also\nuse [MSFragger issue tracker](https://github.com/Nesvilab/MSFragger/issues),\n[Philosopher issue tracker](https://github.com/Nesvilab/philosopher/issues),\n[IonQuant issue tracker](https://github.com/Nesvilab/IonQuant/issues).\nSee the MSFragger [wiki](https://github.com/Nesvilab/MSFragger/wiki) and [FAQ](https://github.com/Nesvilab/MSFragger/wiki/Frequently-Asked-Questions). \n\n\nFor other tools developed by Nesvizhskii lab, visit our website \n[nesvilab.org](http://www.nesvilab.org)\n\n#### How to Run\n- **Windows**:\n  - Double click the `FragPipe.exe` or `FragPipe.bat` from the `bin` folder\n  - Or execute the command: `java -jar FragPipe-x.x.jar`\n- **Linux**:\n  - Run the `fragpipe` shell script (can double-click to run)  \n  - Or execute the command: `java -jar FragPipe-x.x.jar`\n- **Mac OS** (command line interface only):\n  - Install docker by following the [instruction](https://docs.docker.com/desktop/install/mac-install/)\n  - Open terminal and pull the docker image by running `docker pull fcyucn/fragpipe`\n  - FragPipe is located in `/fragpipe_bin`\n  - Go to `/fragpipe_bin/fragPipe-x.x/fragpipe/bin` directory and execute `./fragpipe --help` in the terminal\n \n#### Integration\nFragPipe is open source and the output is currently supported by the following software projects:\n- [Skyline](https://skyline.ms/project/home/software/Skyline/begin.view)\n- [AlphaPeptDeep](https://github.com/MannLabs/alphapeptdeep)\n- [AlphaPeptStats](https://github.com/MannLabs/alphapeptstats)\n- [AlphaMap](https://github.com/MannLabs/alphamap)\n- [directLFQ](https://github.com/MannLabs/directlfq)\n- [DIA-NN](https://github.com/vdemichev/DiaNN)\n- [MSstats](http://msstats.org/)\n- [picked_group_fdr](https://github.com/kusterlab/picked_group_fdr)\n- [FragPipe-Analyst](http://fragpipe-analyst.nesvilab.org/)\n\n\n\n#### Key references\n##### Database search\n- Kong, A. T., Leprevost, F. V., Avtonomov, D. M., Mellacheruvu, D., & Nesvizhskii, A. I. (2017). MSFragger: ultrafast and comprehensive peptide identification in mass spectrometry–based proteomics. Nature Methods, 14(5), 513-520.\n- Yu, F., Teo, G. C., Kong, A. T., Haynes, S. E., Avtonomov, D. M., Geiszler, D. J., & Nesvizhskii, A. I. (2020). Identification of modified peptides using localization-aware open search. Nature Communications, 11(1), 1-9.\n- Yu, F., Haynes, S. E., Teo, G. C., Avtonomov, D. M., Polasky, D. A., & Nesvizhskii, A. I. (2020). Fast quantitative analysis of timsTOF PASEF data with MSFragger and IonQuant. Molecular & Cellular Proteomics, 10(9), 1575-1585.\n- Teo, G. C., Polasky, D. A., Yu, F., Nesvizhskii, A. I. (2020). A fast deisotoping algorithm and its implementation in the MSFragger search engine. Journal of Proteome Research, 20(1), 498-505.\n\n\n##### Glyco/Labile search\n- Polasky, D. A., Yu, F., Teo, G. C., & Nesvizhskii, A. I. (2020). Fast and Comprehensive N-and O-glycoproteomics analysis with MSFragger-Glyco. Nature Methods, 17, 1125-1132.\n- Polasky, D. A., Geiszler, D. J., Yu, F., & Nesvizhskii, A. I. (2022). Multiattribute Glycan Identification and FDR Control for Glycoproteomics. Molecular & Cellular Proteomics, 21(3), 100205.\n- Polasky, D. A., Geiszler, D. J., Yu, F., Kai, Li., Teo, G. C., & Nesvizhskii, A. I. (2023). MSFragger-Labile: A Flexible Method to Improve Labile PTM Analysis in Proteomics. Molecular & Cellular Proteomics, 22(5), 100538.\n\n\n##### PTM\n- Chang, H. Y., Kong, A. T., da Veiga Leprevost, F., Avtonomov, D. M., Haynes, S. E., & Nesvizhskii, A. I. (2020). Crystal-C: A computational tool for refinement of open search results. Journal of Proteome Research, 19(6), 2511-2515.\n- Geiszler, D. J., Kong, A. T., Avtonomov, D. M., Yu, F., da Veiga Leprevost, F., & Nesvizhskii, A. I. (2020). PTM-Shepherd: analysis and summarization of post-translational and chemical modifications from open search results. Molecular & Cellular Proteomics, 20, 100018.\n- Geiszler, D. J., Polasky, D. A., Yu, F., & Nesvizhskii, A. I. (2023). Detecting diagnostic features in MS/MS spectra of post-translationally modified peptides. Nature Communications, 14, 4132.\n\n\n##### DIA\n- Tsou, C. C., Avtonomov, D., Larsen, B., Tucholska, M., Choi, H., Gingras, A. C., & Nesvizhskii, A. I. (2015). DIA-Umpire: comprehensive computational framework for data-independent acquisition proteomics. Nature methods, 12(3), 258-264.\n- Yu, F, Teo, G. C., Kong, A. T., Fröhlich, K., Li, G. X. , Demichev, V, Nesvizhskii, A..I. (2023). Analysis of DIA proteomics data using MSFragger-DIA and FragPipe computational platform, Nature Communications 14:4154.\n\n\n##### DDA quantification\n- Yu, F., Haynes, S. E., & Nesvizhskii, A. I. (2021). IonQuant enables accurate and sensitive label-free quantification with FDR-controlled match-between-runs. Molecular & Cellular Proteomics, 20, 100077.\n\n\n##### Miscellaneous\n\n- da Veiga Leprevost, F., Haynes, S. E., Avtonomov, D. M., Chang, H. Y., Shanmugam, A. K., Mellacheruvu, D., Kong, A. T., & Nesvizhskii, A. I. (2020). Philosopher: a versatile toolkit for shotgun proteomics data analysis. Nature Methods, 17(9), 869-870.\n- Yang, K. L., Yu, F., Teo, G. C., Kai, L., Demichev, V., Ralser, M., & Nesvizhskii, A. I. (2023). MSBooster: improving peptide identification rates using deep learning-based features. Nature Communications, 14, 4539.\n\n\n\n#### Building from scratch\n\n1. Update build version:  \nThe version of the build is stored in 3 separate places:  \n    - File: `MSFragger-GUI/src/umich/msfragger/gui/Bundle.properties`  \n      Property: `msfragger.gui.version`\n    - File: `MSFragger-GUI/build.gradle`  \n      Property: `version`\n    - File: `MSFragger-GUI/src/umich/msfragger/gui/Bundle.properties `  \n      Property: `msfragger.gui.version`\n2. Build:  \nYou don't need to have Gradle installed, the Gradle wrapper included in this repository will be used. From the root directory of the repository issue the following commands:\n\n    ```bash\n    cd ./MSFragger-GUI\n    ./gradlew makeReleaseZipNoJre\n    ```\n    or use this version to build with Java Runtime (for Windows only):\n    \n    ```bash\n    cd ./MSFragger-GUI\n    ./gradlew makeReleaseZipWithJre\n    ```\n    \n3. The .zip output will be in `MSFragger-GUI/build/github-release`.\n",178,mass-spectrometry,Java,6,Shell,Java,Batchfile,Python,GDB,Dockerfile,,,,,,,,,,,,,,,,,,,,,,,50,10,39,1,7,16,340,3016407,37,1573,1529,44,a69064c34573ec766aa6626fe0cf495841f49d93,make PTM-S spectrum processing params visible,2024-07-12T19:50:45Z,Dan Polasky,dpolasky@umich.edu,dpolasky,FragPipe v22.0,"## Downloading\r\n- The zip (<a href='https://github.com/Nesvilab/FragPipe/releases/download/22.0/FragPipe-22.0.zip' target='_blank'>FragPipe-22.0.zip</a>) doesn't contain Java, you will need Java 9+ to run.\r\n- The other zip with `-jre-` in its name (<a href='https://github.com/Nesvilab/FragPipe/releases/download/22.0/FragPipe-jre-22.0.zip' target='_blank'>FragPipe-jre-22.0.zip</a>) contains a Java runtime **for Windows only**.\r\n- The docker image is available at <a href='https://hub.docker.com/r/fcyucn/fragpipe' target='_blank'>https://hub.docker.com/r/fcyucn/fragpipe</a>\r\n## Running\r\n- Unzip the file\r\n- In `/bin` subdirectory you will find a `shell script for Linux`, `bat file for Windows`, and an `exe file for Windows`\r\n\r\n\r\n\r\n### Changelog:\r\n\r\nv22.0:\r\n - diaTracer tool for generating pseudo-MS/MS spectra from diaPASEF data, enabling spectrum-centric, direct DIA analysis (including nonspecific and PTM searches).\r\n - Integration of Skyline in FragPipe (supporting DIA, DDA, and DDA glycoproteomics workflows).\r\n - Support for MSFragger DDA+ (full isolation window search) analysis of ddaPASEF data.\r\n - Support in MSBooster for using the Koina server for deep-learning predictions. Requires specifying the Koina URL.\r\n - Support user-specified glycans and glycan modifications for glycoproteomics searches.\r\n - Implement DIA glycoproteomics workflows.\r\n - Significantly faster loading of Thermo raw files in MSFragger and IonQuant.\r\n - Calculate and report more information useful for localizing the sites of modifications identified in open and mass-offset searches.\r\n - Add photo-affinity labeling (PAL) chemoproteomics workflow.\r\n - For DIA analysis, propagate the site localization, protein start, and protein end information to msstats.csv file.\r\n - For DIA analysis, propagate the site localization to the DIA-NN's reports.\r\n - For DIA analysis, propagate `Proteotypic`, `AllMappedProteins`, and `AllMappedGenes` columns to the DIA-NN reports.\r\n - Generate MSstatsPTM input files.\r\n - Add `nocleavage` option to the enzyme panel in the MSFragger tab.\r\n - Add `analyze filter` dropdown to the MSFragger tab to only search MS/MS scans of the specified type (ITMS or FTMS).\r\n - Change the default value of the IonQuant's minions to 1 (MaxLFQ normalization).\r\n - Overhaul MSFragger and IonQuant config panels. MSFragger, IonQuant, and diaTracer need to be in the same folder. Specify the folder in the Config tab.\r\n - Update the tool download panel\r\n - Bundle Philosopher\r\n - Upgrade the bundled JRE to version 17\r\n - Require Java 11+\r\n - Require MSFragger 4.1+\r\n - Require IonQuant 1.10.27+\r\n - Require Python 3.9, 3.10, or 3.11\r\n - Require EasyPQP 0.1.44+\r\n - Upgrade Crystal-C to 1.5.6\r\n - Upgrade MSBooster to 1.2.31\r\n - Upgrade Percolator to 3.6.5\r\n - Upgrade TMT-Integrator to 5.0.9\r\n - Various bug fixes and improvements\r\n",22,Fengchao,,fcyu,Other,FragPipe,Nesvilab,50,mass-spectrometry,proteomics,search-engine,pipeline,gui,,,,,,,,,,,,,,,,/Nesvilab/FragPipe,58,22,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/Nemocas/AbstractAlgebra.jl,https://github.com/Nemocas/AbstractAlgebra.jl,0,,,0,1,0,0,0,0,1,0,0,0,0,Generic abstract algebra functionality in pure Julia (no C dependencies),"# AbstractAlgebra\n\n[![Docs](https://img.shields.io/badge/docs-dev-blue.svg)](https://nemocas.github.io/AbstractAlgebra.jl/dev)\n[![Docs](https://img.shields.io/badge/docs-stable-blue.svg)](https://nemocas.github.io/AbstractAlgebra.jl/stable)\n[![Run tests](https://github.com/Nemocas/AbstractAlgebra.jl/actions/workflows/CI.yml/badge.svg?branch=master)](https://github.com/Nemocas/AbstractAlgebra.jl/actions/workflows/CI.yml)\n[![Codecov](https://codecov.io/github/Nemocas/AbstractAlgebra.jl/coverage.svg?branch=master&token=)](https://codecov.io/gh/Nemocas/AbstractAlgebra.jl)\n\nAbstractAlgebra is a pure Julia package for computational abstract algebra. It grew out of the Nemo project and provides all of the abstract types and generic implementations that Nemo relies on.\n\nIt was originally developed by William Hart, Tommy Hofmann, Fredrik Johansson and\nClaus Fieker with contributions from others. Current maintainers are Claus Fieker,\nTommy Hofmann and Max Horn.\n\nAbstractAlgebra currently provides:\n\n* Generic polynomial rings, matrix spaces, fraction fields, residue rings, relative and absolute power series, Laurent series\n* Finite fields, integers, rationals, permutations and characters, number fields\n\nDocumentation can be found at the following link:\n\n* <https://nemocas.github.io/AbstractAlgebra.jl/dev/index.html>\n\nProjects that depend on AbstractAlgebra include:\n\n* Nemo.jl <https://github.com/Nemocas/Nemo.jl> (optimised implementations of specific rings provided by the Flint, Arb and Antic C libraries)\n* Hecke.jl <https://github.com/thofma/Hecke.jl> (algebraic number theory)\n* Singular.jl <https://github.com/oscar-system/Singular.jl> (polynomial rings and ideals, Groebner bases and computer algebra provided by the Singular C++ library)\n",158,mathematics,Julia,1,Julia,,,,,,,,,,,,,,,,,,,,,,,,,,,,1299,72,1217,10,7,54,1112,96768,60,456,338,118,2235d6a60348cbb60f4af97332a0dbe49ae24a51,chore: tag 0.42.0 (#1756),2024-07-18T18:18:02Z,Tommy Hofmann,thofma@gmail.com,thofma,v0.42.0,"## AbstractAlgebra v0.42.0\n\n[Diff since v0.41.10](https://github.com/Nemocas/AbstractAlgebra.jl/compare/v0.41.10...v0.42.0)\n\n\n**Merged pull requests:**\n- Use type traits in linear solving (#1738) (@joschmitt)\n- Use mpoly_type rings inside of UnivPolyRing (#1748) (@lgoettgens)\n- Update deprecations for breaking release (#1755) (@lgoettgens)\n- chore: tag 0.42.0 (#1756) (@thofma)\n\n**Closed issues:**\n- Use Nemo mpolys in UniversalPolyRing, if available  (#1746)",v0.42.0,,,github-actions[bot],Other,AbstractAlgebra.jl,Nemocas,168,abstract-algebra,computer-algebra,julia,math,mathematics,maths,julia-package,,,,,,,,,,,,,,/Nemocas/AbstractAlgebra.jl,168,16,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/nebulabroadcast/nebula,https://github.com/nebulabroadcast/nebula,0,,0,0,0,0,0,0,0,1,0,0,0,0,Media asset management and broadcast automation system,"NEBULA\n======\n\n![GitHub release (latest by date)](https://img.shields.io/github/v/release/nebulabroadcast/nebula?style=for-the-badge)\n![Maintenance](https://img.shields.io/maintenance/yes/2024?style=for-the-badge)\n![Last commit](https://img.shields.io/github/last-commit/nebulabroadcast/nebula?style=for-the-badge)\n![Python version](https://img.shields.io/badge/python-3.11-blue?style=for-the-badge)\n\nNebula is an open source broadcast automation and media asset management system for television, radio and VOD platforms.\nSince 2012 Nebula has proven stable and reliable software in 24/7 broadcast environment \nand it is now used by TV and production companies worldwide.\n\nThis repository contains the source code of Nebula server - the core of the system.\nNebula server consists of a Python backend and a React frontend. The repository\nalso contains a Dockerfile for building a Docker image of the server.\n\nApart from this server repository, Nebula consists of the following other repositories:\n\n - [nebula-tutorial](https://github.com/nebulabroadcast/nebula-tutorial) - Example docker-compose.yml, default settings, plugins and other files for getting started with Nebula.\n - [nebula-worker](https://github.com/nebulabroadcast/nebula-worker) - Nebula worker is a Python application that runs on a worker machine and handles the actual media processing.\n - [firefly](https://github.com/nebulabroadcast/firefly) - Firefly is a desktop client needed for linear broadcast planning, scheduling and playout.\n\n\nKey features\n------------\n\n### Media Asset Management\n\nSimple and fast media catalog based on [EBU&nbsp;Core](https://tech.ebu.ch/MetadataEbuCore) includes a description of asset\ngenre, editorial format, atmosphere, rights, relations, and technical metadata,\nwhile its very fast search engine makes navigation among media files very easy.\n\nThe low-resolution preview allows for editorial review, trimming, and the creation of sub-clips.\n\n![Metadata editor](https://nebulabroadcast.com/static/img/nebula-metadata-editor.webp)\n\n### Video and audio cross-conversion and normalization\n\nA preliminary media analysis and normalization guarantee its standards compliance.\nThis process includes metadata extraction, aspect ratio fixing, crop and rotation detection,\nsmart frame rate and size normalization and [EBU R128](https://tech.ebu.ch/docs/r/r128.pdf) loudness correction.\n\nAutomatic cross-conversion servers transcode files for playout, web, low-res proxies, customer previews, etc.\nFor **h.264** and **HEVC**, Nebula can take advantage of NVIDIA nvenc and leverage the speed of transcoding using GPUs.\n\nIt is possible to start conversions automatically (rule-based) or trigger them from the user interface.\n\n### Linear scheduling\n\nFirefly client provides a simple and user-friendly way to schedule linear broadcasting.\nMacro- and micro-scheduling patterns are finished intuitively using drag&drop, including live events.\n\nNebula has also the ability to schedule for playback assets, which aren't finished yet.\nAs soon as a media file is created the media file and rundown item are paired automatically.\n\nThe optional [Dramatica](https://github.com/immstudios/dramatica) module makes program planning even easier;\ndepending on the particular broadcast scheme, Dramatica selects and automatically completes convenient shows, self-promotions, trailers, and fillings.\n\nIt is the way to create a playlist for a music station where an algorithm automatically creates a playlist based on a predefined scheme.\nEach clip in the rundown is picked by its editorial format, genre, tempo, atmosphere, etc.\n\n![Detail of a scheduler panel in the Firefly application](https://nebulabroadcast.com/static/img/nebula-scheduler.webp)\n\n\n### Playout control\n\nFor linear broadcasting, Nebula can control\n[CasparCG](https://casparcg.com), [VLC](https://videolan.org) or [Conti](https://github.com/immstudios/conti).\nBroadcasting can run autonomously with and option of starting blocks at a specified time.\n\nUsers - master control room operators - can interfere with the rundown using [Firefly client](https://github.com/nebulabroadcast/firefly),\nexecuting graphics or change run order until the last moment.\n\nPlayout control module offers a plug-in interface for secondary events execution such as CG, router or studio control,\nrecorders control and so on. Right at the operator's fingertips.\n\n![Detail of a rundown panel with playout control interface](https://nebulabroadcast.com/static/img/nebula-playout-control.webp)\n\n### Publishing\n\nNebula can be linked to a company website via the API.\nMedia files are automatically uploaded to the web or social networks after the planned program is broadcasted.\n\n### Statistics and reporting\n\nNebula allows generating various statistics and reports for collective rights management societies like OSA, DACS, etc. in an xls file.\n\n### Management and monitoring\n\nA simple web based interface allows various management tasks (services and jobs monitoring, user management...) as well as simplified MAM access for\neditorial work without Firefly installed.\n\nNebula provides extensive system metrics in [Prometheus](https://prometheus.io) format. [Grafana](https://grafana.com)\ndashboard can be used for their visualization and alerting in case of problems.\n\n### Reliability\n\nNebula is under active development and is in production since 2012 with no intenitons of abandoning the project,\nand we have a roadmap for several years. We listen to our customers, and we change our priorities in order to meet\nrequests from the production.\n\nWe do not attempt to include numerous features that no one will ever use.\nInstead, we have spent many years working alongside Nebula operators, learning from each other.\nWe believe that Nebula covers all common tasks in a broadcast environment.\n\nLegal\n-----\n\n*Nebula* is developed and maintained by [imm studios, z.s.](https://imm.cz)\n\n### License\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\nSee the GNU General Public License for more details.\n\nNeed help?\n----------\n\n - Join [Open Source Broadcasting](https://discord.gg/WXaaHYGQ) group on Discord\n - Professional support for Nebula is provided by [Nebula Broadcast](https://nebulabroadcast.com)\n - User documentation is available on [our website](https://nebulabroadcast.com/doc/nebula)\n - Found a bug? Please [create an issue](https://github.com/nebulabroadcast/nebula/issues)\n",212,graphics,Python,7,Makefile,Python,Shell,HTML,JavaScript,Dockerfile,Sass,,,,,,,,,,,,,,,,,,,,,,43,1,40,2,10,3,0,5706,25,26,23,3,e56bc4d73d229d229f5edbfe5d2ef89537c2ee1b,Merge pull request #66 from nebulabroadcast/65-handle-displaying-corr…,2024-05-09T20:55:13Z,Martin Wacker,martas@imm.cz,martastain,Nebula 6.0.6,## What's Changed\n* 6.0.5 by @martastain in https://github.com/nebulabroadcast/nebula/pull/58\n* Merge pull request #58 from nebulabroadcast/develop by @martastain in https://github.com/nebulabroadcast/nebula/pull/59\n* fix: pagination by @martastain in https://github.com/nebulabroadcast/nebula/pull/60\n\n\n**Full Changelog**: https://github.com/nebulabroadcast/nebula/compare/v6.0.5...v6.0.6,v6.0.6,,,github-actions[bot],GNU General Public License v3.0,nebula,nebulabroadcast,17,broadcast,multimedia,cms,automation,transcoding,video-processing,python3,asset-management,television,ffmpeg,casparcg,vod,livestream,graphics,playlist,scheduling,,,,,/nebulabroadcast/nebula,19,16,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/NASAWorldWind/WorldWindJava,https://github.com/NASAWorldWind/WorldWindJava,0,visualization?,,0,0,1,0,0,0,0,1,0,0,0,The NASA WorldWind Java SDK (WWJ) is for building cross-platform 3D geospatial desktop applications in Java.,"<img src=""https://worldwind.arc.nasa.gov/img/nasa-logo.svg"" height=""100""/>\n\n# WorldWind Java\n\n## New version of WorldWind Java released\nWorldWind Java 2.2.1 is now available on GitHub. This version of WorldWind Java is a maintenance release that addresses small fixes (typos, bad references, etc.) to various areas of the code and removes references to services that are no longer supported by the WorldWind servers.\n\nWorldWind's API remains unchanged in this release and we are committed to maintaining a consistent API in future releases.\nMore information on the release can be found at this link: [WorldWind Java 2.2.1](https://github.com/NASAWorldWind/WorldWindJava/releases).\n\nPlease direct questions to our new email address: arc-worldwind@mail.nasa.gov.\n\n[![Build Status](https://travis-ci.com/NASAWorldWind/WorldWindJava.svg?branch=develop)](https://travis-ci.com/NASAWorldWind/WorldWindJava)\n\n3D virtual globe API for desktop Java, developed by NASA. Provides a geographic context with high-resolution terrain, for visualizing geographic or geo-located information in 3D and 2D. Developers can customize the globe's terrain and imagery. Provides a collection of shapes for displaying and interacting with geographic data and representing a range of geometric objects.\n\n- [worldwind.arc.nasa.gov](https://worldwind.arc.nasa.gov) has setup instructions, developers guides, API documentation and more\n- [Apache NetBeans](https://netbeans.apache.org) is used by the NASA WorldWind development team\n\n## Releases and Roadmap\n\nOfficial WorldWind Java releases have the latest stable features, enhancements and bug fixes ready for production use.\n\n- [GitHub Releases](https://github.com/NASAWorldWind/WorldWindJava/releases/) documents official releases\n- [GitHub Milestones](https://github.com/NASAWorldWind/WorldWindJava/milestones) documents upcoming releases and the development roadmap\n- [Travis CI](https://travis-ci.com/NASAWorldWind/WorldWindJava) provides continuous integration and build automation\n\n## Run a Demo\n\nThe following options are available to run a WorldWind Java demo:\n\n###### From the Apache NetBeans IDE\n\nClone the SDK with git, open the WorldWind Java project with Apache Netbeans and run demos via the Netbeans interface. \n\n###### From a Windows Development Environment\n\n- Download and extract the [Latest Release](https://github.com/NASAWorldWind/WorldWindJava/releases/latest)\n- Open the Command Prompt\n```bash\ncd [WorldWind release]\nrun-demo.bat\n```\n\n###### From a Linux or macOS Development Environment\n\n- Download and extract the [Latest Release](https://github.com/NASAWorldWind/WorldWindJava/releases/latest)\n- Open the Terminal app\n```bash\ncd [WorldWind release]\nsh run-demo.bash\n```\n\n###### Troubleshooting\n\nWorldWind requires a modern graphics card with a current driver. Most display problems are caused by out-of-date\ngraphics drivers. On Windows, visit your graphics card manufacturer's web site for the latest driver: NVIDIA, ATI or\nIntel. The drivers are typically under a link named Downloads or Support. If you're using a laptop, the latest drivers\nare found at the laptop manufacturer's web site.\n\n## JOGL Native Binaries\n\nJOGL performs runtime extraction of native binaries. Some deployment situations may not allow this because it extracts\nthe binaries to the application user’s temp directory. Runtime extraction can be avoided by by modifying WorldWind\nJava's JOGL distribution to load native binaries directly from the library path instead of dynamically using the native\nbinary JAR files as follows:\n\n1. Extract the GlueGen and JOGL native binary JAR files for the desired platform.\n   These JAR files follow the naming pattern gluegen-rt-natives-PLATFORM.jar and jogl-all-natives-PLATFORM.jar\n2. Place the extracted native binaries either in the program's working directory or in a location specified as the\n   library path. The following JOGL user's guide page outlines supported library path variables:\n   https://jogamp.org/jogl/doc/userguide/index.html#traditionallibraryloading\n3. Remove the GlueGen and JOGL native binary JAR files from your application's workspace.\n   JOGL attempts to use the native binary JAR files before loading from the library path, so these files must not be\n   deployed with the application.\n4. When running, specify the JVM argument -Djogamp.gluegen.UseTempJarCache=false\n\n## License\n\nCopyright 2006-2009, 2017, 2020 United States Government, as represented by the\nAdministrator of the National Aeronautics and Space Administration.\nAll rights reserved.\n\nThe NASA World Wind Java (WWJ) platform is licensed under the Apache License,\nVersion 2.0 (the ""License""); you may not use this file except in compliance\nwith the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed\nunder the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR\nCONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nNASA World Wind Java (WWJ) also contains the following 3rd party Open Source\nsoftware:\n\n    Jackson Parser – Licensed under Apache 2.0\n    GDAL – Licensed under MIT\n    JOGL – Licensed under  Berkeley Software Distribution (BSD)\n    Gluegen – Licensed under Berkeley Software Distribution (BSD)\n\nA complete listing of 3rd Party software notices and licenses included in\nNASA World Wind Java (WWJ)  can be found in the WorldWindJava-v2.2 3rd-party\nnotices and licenses PDF found in code directory.\n",714,graphics,Java,9,HTML,Java,Shell,Batchfile,PHP,Objective-C,C++,C,Makefile,,,,,,,,,,,,,,,,,,,,105,30,41,34,20,11,2,128461,324,175,59,116,2543c234349c76f4a290e318b49f071ac156d2a0,Add the ability to retrieve geoid offsets from an EGM 2008 model.  (#…,2024-04-15T23:14:51Z,Mark Peterson,markpet@yahoo.com,markpet49,v2.2.1,"This version of WorldWind Java is a maintenance release that addresses small fixes (updated formatting, typos, bad references, etc.) to various areas of the code and removes references to services that are no longer supported by the WorldWind servers.\r\n\r\nWorldWind's API remains unchanged in this release and we are committed to maintaining a consistent API in future releases.\r\n\r\n## What's Changed\r\n* Add Collada example by @markpet49 in https://github.com/NASAWorldWind/WorldWindJava/pull/214\r\n* Formatting by @markpet49 in https://github.com/NASAWorldWind/WorldWindJava/pull/225\r\n* Readme update by @markpet49 in https://github.com/NASAWorldWind/WorldWindJava/pull/230\r\n* Removed outdated references to goworldwind.org website by @Beak-man in https://github.com/NASAWorldWind/WorldWindJava/pull/235\r\n* Remove configs for retired services by @markpet49 in https://github.com/NASAWorldWind/WorldWindJava/pull/239\r\n* Release changes 2.21 by @markpet49 in https://github.com/NASAWorldWind/WorldWindJava/pull/246\r\n* Version 2.2.1 Release Changes by @markpet49 in https://github.com/NASAWorldWind/WorldWindJava/pull/247\r\n",v2.2.1,Mark Peterson,,markpet49,,WorldWindJava,NASAWorldWind,9,nasa,worldwind,java,opengl,geospatial,earth,3d,sdk,terrain,imagery,maps,globe,graphics,,,,,,,,/NASAWorldWind/WorldWindJava,10,96,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/NASA-AMMOS/3DTilesRendererJS,https://github.com/NASA-AMMOS/3DTilesRendererJS,0,,,0,0,0,0,0,0,1,1,0,0,0,Renderer for 3D Tiles in Javascript using three.js,"# 3d-tiles-renderer\n\n[![npm version](https://img.shields.io/npm/v/3d-tiles-renderer.svg?style=flat-square)](https://www.npmjs.com/package/3d-tiles-renderer)\n[![build](https://img.shields.io/github/actions/workflow/status/NASA-AMMOS/3DTilesRendererJS/node.js.yml?style=flat-square&label=build&branch=master)](https://github.com/NASA-AMMOS/3DTilesRendererJS/actions)\n\n![](./images/header-mars.png)\n\nThree.js renderer implementation for the [3D Tiles format](https://github.com/AnalyticalGraphicsInc/3d-tiles/blob/master/specification/). The renderer supports most of the 3D Tiles spec features with a few exceptions. See the [Feature Complete Milestone](https://github.com/NASA-AMMOS/3DTilesRendererJS/milestone/1) for information on which features are not yet implemented.\n\nIf a tile set or geometry does not load or render properly please make an issue! Example data is needed for adding and testing features.\n\n**Examples**\n\n[Dingo Gap Mars dataset with multiple tile sets](https://nasa-ammos.github.io/3DTilesRendererJS/example/bundle/mars.html)\n\n[Kitchen sink example with all options here](https://nasa-ammos.github.io/3DTilesRendererJS/example/bundle/index.html)\n\n[Rendering in VR example here](https://nasa-ammos.github.io/3DTilesRendererJS/example/bundle/vr.html)\n\n**External Tiles Providers**\n\n_Personal [Google Tiles API Key](https://developers.google.com/maps/documentation/tile/3d-tiles) or [Cesium Ion API Key](https://cesium.com/platform/cesium-ion/) required_\n\n[Cesium Ion 3D Tiles](https://nasa-ammos.github.io/3DTilesRendererJS/example/bundle/ionExample.html)\n\n[Google Photorealistic Tiles](https://nasa-ammos.github.io/3DTilesRendererJS/example/bundle/googleMapsAerial.html)\n\n[Google Globe Tiles](https://nasa-ammos.github.io/3DTilesRendererJS/example/bundle/googleMapsExample.html)\n\n**Customization**\n\n[Custom material example](https://nasa-ammos.github.io/3DTilesRendererJS/example/bundle/customMaterial.html)\n\n[Rendering shadows from offscreen tiles example](https://nasa-ammos.github.io/3DTilesRendererJS/example/bundle/offscreenShadows.html)\n\n[Alterate texture overlays](https://nasa-ammos.github.io/3DTilesRendererJS/example/bundle/landformSiteOverlay.html)\n\n**Plugins**\n\n[Tile Metadata](https://nasa-ammos.github.io/3DTilesRendererJS/example/bundle/metadata.html)\n\n[Tile LoD Fade Transition](https://nasa-ammos.github.io/3DTilesRendererJS/example/bundle/fadingTiles.html)\n\n**Debug Pages**\n\n[B3DM Loading](https://nasa-ammos.github.io/3DTilesRendererJS/example/bundle/b3dmExample.html)\n\n[I3DM Loading](https://nasa-ammos.github.io/3DTilesRendererJS/example/bundle/i3dmExample.html)\n\n[PNTS Loading](https://nasa-ammos.github.io/3DTilesRendererJS/example/bundle/pntsExample.html)\n\n[Ellipsoid Region Bounds](https://nasa-ammos.github.io/3DTilesRendererJS/example/bundle/ellipsoid.html)\n\n\n# Use\n\n## Installation\n\n```\nnpm install 3d-tiles-renderer --save\n```\n\n## Basic TilesRenderer\n\nSetting up a basic application with a 3D Tile Set.\n\n```js\nimport { TilesRenderer } from '3d-tiles-renderer';\n\n// ... initialize three scene ...\n\nconst tilesRenderer = new TilesRenderer( './path/to/tileset.json' );\ntilesRenderer.setCamera( camera );\ntilesRenderer.setResolutionFromRenderer( camera, renderer );\ntilesRenderer.addEventListener( 'load-tile-set', () => {\n\n	// optionally center the tile set in case it's far off center\n	const sphere = new Sphere();\n	tilesRenderer.getBoundingSphere( sphere );\n	tilesRenderer.group.position.copy( sphere.center ).multiplyScalar( - 1 );\n\n} );\n\nscene.add( tilesRenderer.group );\n\nrenderLoop();\n\nfunction renderLoop() {\n\n	requestAnimationFrame( renderLoop );\n\n	// The camera matrix is expected to be up to date\n	// before calling tilesRenderer.update\n	camera.updateMatrixWorld();\n	tilesRenderer.update();\n	renderer.render( scene, camera );\n\n}\n```\n\n## Custom Material\n\nSetting up a 3D Tile Set using a custom material.\n\n```js\nconst tilesRenderer = new TilesRenderer( './path/to/tileset.json' );\ntilesRenderer.setCamera( camera );\ntilesRenderer.setResolutionFromRenderer( camera, renderer );\ntilesRenderer.onLoadModel = function ( scene ) {\n\n	// create a custom material for the tile\n	scene.traverse( c => {\n\n		if ( c.material ) {\n\n			c.originalMaterial = c.material;\n			c.material = new MeshBasicMaterial();\n\n		}\n\n	} );\n\n};\n\ntilesRenderer.onDisposeModel = function ( scene ) {\n\n	// dispose of any manually created materials\n	scene.traverse( c => {\n\n		if ( c.material ) {\n\n			c.material.dispose();\n\n		}\n\n	} );\n\n};\nscene.add( tilesRenderer.group );\n```\n\n## Multiple TilesRenderers with Shared Caches and Queues\n\nUsing multiple tiles renderers that share LRUCache and PriorityQueue instances to cut down on memory and correctly prioritize downloads.\n\n```js\n// create multiple tiles renderers\nconst tilesRenderer = new TilesRenderer( './path/to/tileset.json' );\ntilesRenderer.setCamera( camera );\ntilesRenderer.setResolutionFromRenderer( camera, renderer );\n\nconst tilesRenderer2 = new TilesRenderer( './path/to/tileset2.json' );\ntilesRenderer2.setCamera( camera );\ntilesRenderer2.setResolutionFromRenderer( camera, renderer );\n\n// set the second renderer to share the cache and queues from the first\ntilesRenderer2.lruCache = tilesRenderer.lruCache;\ntilesRenderer2.downloadQueue = tilesRenderer.downloadQueue;\ntilesRenderer2.parseQueue = tilesRenderer.parseQueue;\n\n// add them to the scene\nscene.add( tilesRenderer.group );\nscene.add( tilesRenderer2.group );\n```\n\n## Adding DRACO Decompression Support\n\nAdding support for DRACO decompression within the GLTF files that are transported in B3DM and I3DM formats. The same approach can be used to add support for KTX2 and DDS textures.\n\n```js\n\n// Note the DRACO compression files need to be supplied via an explicit source.\n// We use unpkg here but in practice should be provided by the application.\nconst tilesRenderer = new TilesRenderer( './path/to/tileset.json' );\n\nconst dracoLoader = new DRACOLoader();\ndracoLoader.setDecoderPath( 'https://unpkg.com/three@0.123.0/examples/js/libs/draco/gltf/' );\n\nconst loader = new GLTFLoader( tilesRenderer.manager );\nloader.setDRACOLoader( dracoLoader );\n\ntilesRenderer.manager.addHandler( /\.gltf$/, loader );\n```\n\nAdding support for DRACO decompression within the PNTS files.\n\n```js\n\n// Note the DRACO compression files need to be supplied via an explicit source.\n// We use unpkg here but in practice should be provided by the application.\nconst dracoLoader = new DRACOLoader();\ndracoLoader.setDecoderPath( 'https://unpkg.com/three@0.123.0/examples/js/libs/draco/gltf/' );\n\n\nconst tilesRenderer = new TilesRenderer( './path/to/tileset.json' );\ntilesRenderer.manager.addHandler( /\.drc$/, loader );\n```\n\n\n## Loading from Cesium Ion\n\nLoading from Cesium Ion requires some extra fetching of the ion url endpoint, as well as a temporary bearer access token. A full example is found in the ionExample.js file in the examples folder.\n\nSet the desired assetId as well as your Ion AccessToken. [More reading is provided by the Cesium REST API documentation](https://cesium.com/docs/rest-api/).\n\n```js\n// fetch a temporary token for the Cesium Ion asset\nconst url = new URL( `https://api.cesium.com/v1/assets/${ assetId }/endpoint` );\nurl.searchParams.append( 'access_token', accessToken );\n\nfetch( url, { mode: 'cors' } )\n	.then( res => res.json() )\n	.then( json => {\n\n		url = new URL( json.url );\n\n		const version = url.searchParams.get( 'v' );\n		tiles = new TilesRenderer( url );\n		tiles.fetchOptions.headers = {};\n		tiles.fetchOptions.headers.Authorization = `Bearer ${json.accessToken}`;\n\n		// Prefilter each model fetch by setting the cesium Ion version to the search\n		// parameters of the url.\n		tiles.preprocessURL = uri => {\n\n			uri = new URL( uri );\n			uri.searchParams.append( 'v', version );\n			return uri.toString();\n\n		};\n\n	} );\n```\n\n## Render On Change\n\nThe tile set and model load callbacks can be used to detect when the data has changed and a new render is necessary.\n\n```js\nlet needsRerender = true;\nconst tilesRenderer = new TilesRenderer( './path/to/tileset.json' );\ntilesRenderer.onLoadTileSet = () => needsRerender = true;\ntilesRenderer.onLoadModel = () => needsRerender = true;\n\nfunction renderLoop() {\n\n	requestAnimationFrame( renderLoop );\n	if ( needsRerender ) {\n\n		needsRerender = false;\n		camera.updateMatrixWorld();\n		tilesRenderer.update();\n		renderer.render( scene, camera );\n\n	}\n\n}\nrenderLoop();\n```\n\n## Read Batch Id and Batch Table Data\n\nHow to find the batch id and batch table associated with a mesh and read the data.\n\n```js\nconst tilesRenderer = new TilesRenderer( './path/to/tileset.json' );\n\n// ...checking intersections...\n\nconst intersects = raycaster.intersectObject( scene, true );\nif ( intersects.length ) {\n\n	const { face, object } = intersects[ 0 ];\n	const batchidAttr = object.geometry.getAttribute( '_batchid' );\n\n	if ( batchidAttr ) {\n\n		// Traverse the parents to find the batch table.\n		let batchTableObject = object;\n		while ( ! batchTableObject.batchTable ) {\n\n			batchTableObject = batchTableObject.parent;\n\n		}\n\n		// Log the batch data\n		const batchTable = batchTableObject.batchTable;\n		const hoveredBatchid = batchidAttr.getX( face.a );\n		const batchData = batchTable.getData( 'BatchTableKey' );\n		if ( batchData ) {\n\n			console.log( batchData[ hoveredBatchid ] );\n\n		}\n\n	}\n\n}\n```\n\n# API\n\nSee the [plugins documentation](./PLUGINS.md) for GLTFLoader extension plugins, TilesRenderer plugins, and extra classes.\n\n## TilesRenderer\n\n_extends `THREE.EventDispatcher`` & [TilesRendererBase](https://github.com/NASA-AMMOS/3DTilesRendererJS/blob/master/src/base/TilesRendererBase.js), which can be used to implement a 3d tiles renderer in other engines_\n\n### events\n\n```js\n// fired when a new root or child tile set is loaded\n{ type: 'load-tile-set', tileSet: Object, url: String }\n\n// fired when a tile model is loaded\n{ type: 'load-model', scene: THREE.Group, tile: Object }\n\n// fired when a tile model is disposed\n{ type: 'dispose-model', scene: THREE.Group, tile: Object }\n\n// fired when a tiles visibility changes\n{ type: 'tile-visibility-change', scene: THREE.Group, tile: Object }\n```\n\n### .fetchOptions\n\n```js\nfetchOptions = {} : Object\n```\n\nOptions passed to `fetch` when loading tile set and model data.\n\n### .errorTarget\n\n```js\nerrorTarget = 6 : Number\n```\n\nThe target screenspace error in pixels to target when updating the geometry. Tiles will not render if they have below this level of screenspace error. See the [""geometric error"" section in the 3d tiles specification](https://github.com/CesiumGS/3d-tiles/tree/master/specification#geometric-error) for more information.\n\n### .errorThreshold\n\n```js\nerrorThreshold = Infinity : Number\n```\n\nValue used to compute the threshold `errorTarget * errorThreshold` above which tiles will not load or render. This is used to enable traversal to skip loading and rendering parent tiles far from the cameras current screenspace error requirement. If `errorThreshold` is set to `Infinity` then all parent tiles will be loaded and rendered. If it's set to `0` then no parent tiles will render and only the tiles that are being rendered will be loaded.\n\nNote that if the camera position zooms in or out dramatically setting this to a value other than `Infinity` could result in tiles flickering if the renderer updates to display tiles that were previously outside the error threshold. As such this setting is best suited for when camera movement is limited smaller movement scales such as real world movement speeds.\n\n### .maxDepth\n\n```js\nmaxDepth = Infinity : Number\n```\n\nThe max depth to which tiles will be loaded and rendered. Setting it to `1` will only render the root tile. If the tile at depth `maxDepth` is an empty tile then the next set of visible children will be rendered.\n\n### .loadSiblings\n\n```js\nloadSiblings = true : Boolean\n```\n\nIf true then all sibling tiles will be loaded, as well, to ensure coherence when moving the camera. If false then only currently viewed tiles will be loaded.\n\n### .displayActiveTiles\n\n```js\ndisplayActiveTiles = false : Boolean\n```\n\n""Active tiles"" are those that are loaded and available but not necessarily visible. If [loadSiblings](#loadSiblings) is true then the tiles loaded up to the extents of the tile set will be considered active even outside the camera view. These tiles are useful for raycasting off camera or for casting shadows.\n\nActive tiles not currently visible in a camera frustum are removed from the scene as an optimization. Setting `displayActiveTiles` to true will keep them in the scene to be rendered from an outside camera view not accounted for by the tiles renderer.\n\n### .autoDisableRendererCulling\n\n```js\nautoDisableRendererCulling = true : Boolean\n```\n\nIf true then all tile meshes automatically have their [frustumCulled](https://threejs.org/docs/index.html#api/en/core/Object3D.frustumCulled) field set to false. This is useful particularly when using one camera because the tiles renderer automatically performs it's own frustum culling on visible tiles. If [displayActiveTiles](#displayActiveTiles) is true or multiple cameras are being used then you may consider setting this to false.\n\n### .optimizeRaycast\n\n```js\noptimizeRaycast = true : Boolean\n```\n\nIf true then the `raycast` functions of the loaded tile objects are overriden to disable raycasting and the `TilesRenderer.group` raycast function is used to perform a raycast over all visible tiles. This enables an optimized traversal for raycasting against tiles. If `raycaster.firstHitOnly = true` then as well as a more optimal traversal of tiles the raycast will end early as soon as the closest intersction is found.\n\nIf you would like to manage raycasting against tiles yourself this behavior can be disabled if needed by setting `optizeRaycast` to false.\n\n### .preprocessURL\n\n```js\npreprocessURL = null : ( uri : string | URL ) => string | URL;\n```\n\nFunction to preprocess the url for each individual tile geometry or child tile set to be loaded. If null then the url is used directly.\n\n### .lruCache\n\n```js\nlruCache = new LRUCache() : LRUCache\n```\n\n_NOTE: This cannot be set once [update](#update) is called for the first time._\n\n### .downloadQueue\n\n```js\ndownloadQueue = new PriorityQueue : PriorityQueue\n```\n\n_NOTE: This cannot be set once [update](#update) is called for the first time._\n\n### .parseQueue\n\n```js\nparseQueue = new PriorityQueue : PriorityQueue\n```\n\n_NOTE: This cannot be modified once [update](#update) is called for the first time._\n\n### .group\n\n```js\ngroup : Group\n```\n\nThe container group for the 3d tiles. Add this to the three.js scene in order to render it.\n\nWhen raycasting a higher performance traversal approach is used (see [optimizeRaycast](#optimizeRaycast)).\n\n### .manager\n\n```js\nmanager : LoadingManager\n```\n\nThe manager used when loading tile geometry.\n\n### .constructor\n\n```js\nconstructor( url : String )\n```\n\nTakes the url of the `tileset.json` for the tile set to be rendered.\n\n### .update\n\n```js\nupdate() : void\n```\n\nUpdates the tiles to render and kicks off loads for the appropriate tiles in the 3d tile set.\n\nBoth `group.matrixWorld` and all cameras world matrices are expected to be up to date before this is called.\n\n### .resetFailedTiles\n\n```js\nresetFailedTiles() : void\n```\n\nIf any tiles failed to load due to server or network issues then they will not be retried by automatically. This function clears all failed tile states so unloaded tiles can be retried again.\n\n### .getBoundingBox\n\n```js\ngetBoundingBox( box : Box3 ) : boolean\n```\n\nSets `box` to the axis aligned root bounding box of the tile set in the [group](#group) frame. Returns `false` if the tile root is not loaded and the bounding box cannot be set.\n\n### .getOrientedBoundingBox\n\n```js\ngetOrientedBoundingBox( box : Box3, boxTransform : Matrix4 ) : boolean;\n```\n\nSets `box` and `boxTransform` to the bounds and matrix that describe the oriented bounding box that encapsulates the root of the tile set. Returns `false` if the tile root is not loaded and the bounding box cannot be set.\n\n### .getBoundingSphere\n\n```js\ngetBoundingSphere( sphere : Sphere ) : boolean;\n```\n\nSets `sphere` to the bounding sphere that encapsulates the root of the tile set. Returns `false` if the tile root is not loaded and the bounding sphere cannot be set.\n\n### .hasCamera\n\n```js\nhasCamera( camera : Camera ) : boolean\n```\n\nReturns `true` if the camera has already been set on the renderer.\n\n### .setCamera\n\n```js\nsetCamera( camera : Camera ) : boolean\n```\n\nAdds the camera to the camera to be accounted for when traversing the tile set. Returns `false` if the camera is already being tracked. Returns `true` otherwise.\n\n### .deleteCamera\n\n```js\ndeleteCamera( camera : Camera ) : boolean\n```\n\nRemoves the given camera from being accounted for when traversing the tile set. Returns `false` if the camera was not tracked.\n\n### .setResolution\n\n```js\nsetResolution( camera : Camera, resolution : Vector2 ) : boolean\nsetResolution( camera : Camera, x : number, y : number ) : boolean\n```\n\nSets the resolution being rendered to for the given camera. Returns `false` if the camera is not being tracked.\n\n### .setResolutionFromRenderer\n\n```js\nsetResolutionFromRenderer( camera : Camera, renderer : WebGLRenderer ) : boolean\n```\n\nSets the resolution being rendered to for the given camera via renderer which accounts for canvas size and current pixel ratio. Returns `false` if the camera is not being tracked.\n\n### .forEachLoadedModel\n\n```js\nforEachLoadedModel( callback : ( scene : Object3D, tile : object ) => void ) : void\n```\n\nFires the callback for every loaded scene in the hierarchy with the associatd tile as the second argument. This can be used to update the materials of all loaded meshes in the tile set.\n\n### .onLoadTileSet\n\n```js\nonLoadTileSet = null : ( tileSet : Tileset ) => void\n```\n\nCallback that is called whenever a tile set is loaded.\n\n### .onLoadModel\n\n```js\nonLoadModel = null : ( scene : Object3D, tile : Tile ) => void\n```\n\nCallback that is called every time a model is loaded. This can be used in conjunction with [.forEachLoadedModel](#forEachLoadedModel) to set the material of all load and still yet to load meshes in the tile set.\n\n### .onDisposeModel\n\n```js\nonDisposeModel = null : ( scene : Object3D, tile : Tile ) => void\n```\n\nCallback that is called every time a model is disposed of. This should be used in conjunction with [.onLoadModel](#onLoadModel) to dispose of any custom materials created for a tile. Note that the textures, materials, and geometries that a tile loaded in with are all automatically disposed of even if they have been removed from the tile meshes.\n\n### .onTileVisibilityChange\n\n```js\nonTileVisibilityChange = null : ( scene : Object3D, tile : Tile, visible : boolean ) => void\n```\n\nCallback that is called when a tile's visibility changed. The parameter `visible` is `true` when the tile is visible\n\n### .registerPlugin\n\n```js\nregisterPlugin( plugin : TilesPlugin ) : void\n```\n\nRegister a plugin to the TilesRenderer. See the [plugins documentation](./PLUGINS.md) for more information.\n\n### .getPluginByName\n\n```js\ngetPluginByName( name : string ) : TilesPlugin\n```\n\nReturns the plugin with the given name if it has been registered. Returns the first one if multiple have been registered.\n\n### .dispose\n\n```js\ndispose() : void\n```\n\nDisposes of all the tiles in the renderer. Calls dispose on all materials, textures, and geometries that were loaded by the renderer and subsequently calls [onDisposeModel](#onDisposeModel) for any loaded tile model.\n\n## DebugTilesRenderer\n\n_extends [TilesRenderer](#TilesRenderer)_\n\nSpecial variant of TilesRenderer that includes helpers for debugging and visualizing the various tiles in the tile set. Material overrides will not work as expected with this renderer. The debug renderer includes additional logic and initialization code which can cause performance loss so it's recommended to only use this when needed.\n\n### .colorMode\n\n```js\ncolorMode = NONE : ColorMode\n```\n\nWhich color mode to use when rendering the tile set. The following exported enumerations can be used:\n\n```js\n// No special color mode. Uses the default materials.\nNONE\n\n// Render the screenspace error from black to white with errorTarget\n// being the maximum value.\nSCREEN_ERROR\n\n// Render the geometric error from black to white with maxDebugError\n// being the maximum value.\nGEOMETRIC_ERROR\n\n// Render the distance from the camera to the tile as black to white\n// with maxDebugDistance being the maximum value.\nDISTANCE\n\n// Render the depth of the tile relative to the root as black to white\n// with maxDebugDepth being the maximum value.\nDEPTH\n\n// Render the depth of the tile relative to the nearest rendered parent\n// as black to white with maxDebugDepth being the maximum value.\nRELATIVE_DEPTH\n\n// Render leaf nodes as white and parent nodes as black.\nIS_LEAF\n\n// Render the tiles with a random color to show tile edges clearly.\nRANDOM_COLOR\n\n// Render every individual mesh in the scene with a random color.\nRANDOM_NODE_COLOR\n\n// Sets a custom color using the customColorCallback call back.\nCUSTOM_COLOR\n```\n### .customColorCallback\n\n```js\ncustomColorCallback: (tile: Tile, child: Object3D) => void\n```\n\nThe callback used if `debugColor` is set to `CUSTOM_COLOR`. Value defaults to `null` and must be set explicitly.\n\n### .displayBoxBounds\n\n```js\ndisplayBoxBounds = false : Boolean\n```\n\nDisplay wireframe bounding boxes from the tiles `boundingVolume.box` (or derived from the region bounds) for every visible tile.\n\n### .displaySphereBounds\n\n```js\ndisplaySphereBounds = false : Boolean\n```\n\nDisplay wireframe bounding boxes from the tiles `boundingVolume.sphere` (or derived from the bounding box / region bounds) for every visible tile.\n\n### .displayRegionBounds\n\n```js\ndisplayRegionBounds = false : Boolean\n```\n\nDisplay wireframe bounding rgions from the tiles `boundingVolume.region` for every visible tile if it exists.\n\n### .maxDebugDepth\n\n```js\nmaxDebugDepth = - 1 : Number\n```\n\nThe depth value that represents white when rendering with `DEPTH` or `RELATIVE_DEPTH` [colorMode](#colorMode). If `maxDebugDepth` is `-1` then the maximum depth of the tile set is used.\n\n### .maxDebugError\n\n```js\nmaxDebugError = - 1 : Number\n```\n\nThe error value that represents white when rendering with `GEOMETRIC_ERROR` [colorMode](#colorMode). If `maxDebugError` is `-1` then the maximum geometric error in the tile set is used.\n\n### .maxDebugDistance\n\n```js\nmaxDebugDistance = - 1 : Number\n```\n\nThe distance value that represents white when rendering with `DISTANCE` [colorMode](#colorMode). If `maxDebugDistance` is `-1` then the radius of the tile set is used.\n\n### .getDebugColor\n\n```js\ngetDebugColor : ( val : Number, target : Color ) => void\n```\n\nThe function used to map a [0, 1] value to a color for debug visualizations. By default the color is mapped from black to white.\n\n## PriorityQueue\n\nPiority-sorted queue to prioritize file downloads and parsing.\n\n### .maxJobs\n\n```js\nmaxJobs = 6 : number\n```\n\nThe maximum number of jobs to be processing at once.\n\n### .priorityCallback\n\n```js\npriorityCallback = null : ( itemA, itemB ) => Number\n```\n\nFunction to derive the job priority of the given item. Higher priority values get processed first.\n\n### .schedulingCallback\n\n```js\nschedulingCallback = requestAnimationFrame : ( cb : Function ) => void\n```\n\nA function used for scheduling when to run jobs next so more work doesn't happen in a single frame than there is time for -- defaults to the next frame. This should be overriden in scenarios where requestAnimationFrame is not reliable, such as when running in WebXR. See the VR demo for one example on how to handle this with WebXR.\n\n## GoogleTilesRenderer\n\n_extends [TilesRenderer](#TilesRenderer)_\n\nVariant of the TilesRenderer designed to easily support [Google's Photorealistic 3D Tiles API](https://cloud.google.com/blog/products/maps-platform/create-immersive-3d-map-experiences-photorealistic-3d-tiles). Handles adding api key to all requests, reading tile credits, and initializes tile set traversal options to reasonable defaults for the globe.\n\n### constructor\n\n```js\nconstructor( apiKey: String )\n```\n\nTakes the Google Photorealistic Tiles API Key.\n\n### .getCreditsString\n\n```js\ngetCreditsString(): String;\n```\n\nReturns a string of unique credits for all the tiles currently displayed.\n\n### .setLatLonToYUp\n\n```js\nsetLatLonToYUp( lat: Number, lon: Number ): void;\n```\n\nRotates and positions the local transformation of the tile group object so the surface of the globe ellipsoid at the specified latitude and longitude faces Y+, X+ points north, and Z+ points east and is centered at 0, 0, 0.\n\n## CesiumIonTilesRenderer\n\n_extends [TilesRenderer](#TilesRenderer)_\n\nVariant of TilesRenderer designed to easily support the [Cesium Ion API](https://cesium.com/learn/ion/rest-api/#section/Authentication). Handles initial url resolution, access tokens in the header, and query parameter additions.\n\n### constructor\n\n```js\nconstructor( ionAssetId: String | Number, ionAccessToken: String )\n```\n\nTakes the Ion asset id and access token.\n\n## LRUCache\n\nUtility class for the TilesRenderer to keep track of currently used items so rendered items will not be unloaded.\n\n### .maxSize\n\n```js\nmaxSize = 800 : number\n```\n\nThe maximum cached size. If that current amount of cached items is equal to this value then no more items can be cached.\n\n### .minSize\n\n```js\nminSize = 600 : number\n```\n\nThe minimum cache size. Above this cached data will be unloaded if it's unused.\n\n### .unloadPercent\n\n```js\nunloadPercent = 0.05 : number\n```\n\nThe maximum percentage of [minSize](#minSize) to unload during a given frame.\n\n### .unloadPriorityCallback\n\n```js\nunloadPriorityCallback = null : ( item ) => Number\n```\n\nFunction to derive the unload priority of the given item. Higher priority values get unloaded first.\n\n## BatchTable\n\n### .getKeys\n\n```js\ngetKeys() : Array<String>\n```\n\nReturns the keys of all the data in the batch table.\n\n### .getData\n\n```js\ngetData(\n	key : String,\n	defaultComponentType = null : String | null,\n	defaultType = null : String | null,\n) : Array | TypedArray | null\n```\n\nReturns the data associated with the `key` passed into the function. If the component and type are specified in the batch table contents then those values are used otherwise the values in `defaultComponentType` and `defaultType` are used. Returns null if the key is not in the table.\n\n`defaultComponentType` can be set to `BYTE`, `UNSIGNED_BYTE`, `SHORT`, `UNSIGNED_SHORT`, `INT`, `UNSIGNED_INT`, `FLOAT`, or `DOUBLE`. `defaultType` can be set to `SCALAR`, `VEC2`, `VEC3`, or `VEC4`.\n\n# LICENSE\n\nThe software is available under the [Apache V2.0 license](../LICENSE.txt).\n\nCopyright © 2020 California Institute of Technology. ALL RIGHTS\nRESERVED. United States Government Sponsorship Acknowledged.\nNeither the name of Caltech nor its operating division, the\nJet Propulsion Laboratory, nor the names of its contributors may be\nused to endorse or promote products derived from this software\nwithout specific prior written permission.\n",1477,graphics,JavaScript,2,JavaScript,HTML,,,,,,,,,,,,,,,,,,,,,,,,,,,270,23,246,1,7,38,800,54168,270,355,295,60,e44f2d2b70464da11a5305648cc6dcaf6d315292,Update PLUGINS.md,2024-07-19T15:30:38Z,Garrett Johnson,garrett.kjohnson@gmail.com,gkjohnson,,### Fixed\r\n- Lint rules causing build failures.,v0.3.35,Garrett Johnson,,gkjohnson,Apache License 2.0,3DTilesRendererJS,NASA-AMMOS,54,3d-tiles,3dtiles,graphics,b3dm,threejs,rendering,geometry,terrain,cesium,gltf,tile-set,tileset,gis,,,,,,,,/NASA-AMMOS/3DTilesRendererJS,56,37,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/mypaint/libmypaint,https://github.com/mypaint/libmypaint,0,,,0,0,0,0,0,0,1,1,0,0,0,"libmypaint, a.k.a. ""brushlib"", is a library for making brushstrokes which is used by MyPaint and other projects.","# libmypaint - MyPaint brush engine library\n\n[![Translation status](https://hosted.weblate.org/widgets/mypaint/-/libmypaint/svg-badge.svg)](https://hosted.weblate.org/engage/mypaint/?utm_source=widget)\n[![Travis Build Status](https://travis-ci.org/mypaint/libmypaint.svg?branch=master)](https://travis-ci.org/mypaint/libmypaint)\n[![Appveyor Build Status](https://ci.appveyor.com/api/projects/status/github/mypaint/libmypaint?branch=master&svg=true)](https://ci.appveyor.com/project/jonnor/libmypaint)\n\nThis is the brush library used by MyPaint. A number of other painting\nprograms use it too.\n\nLicense: ISC, see [COPYING](./COPYING) for details.\n\n## Dependencies\n\n* All configurations and builds:\n  - [json-c](https://github.com/json-c/json-c/wiki) (>= 0.11)\n  - C compiler, `make` etc.\n* Most configurations (all except `--disable-introspection --without-glib`):\n  - [GObject-Introspection](https://live.gnome.org/GObjectIntrospection)\n  - [GLib](https://wiki.gnome.org/Projects/GLib)\n* When building from `git` (developer package names vary by distribution):\n  - [Python](http://python.org/)\n  - [autotools](https://en.wikipedia.org/wiki/GNU_Build_System)\n  - [intltool](https://freedesktop.org/wiki/Software/intltool/)\n  - [gettext](https://www.gnu.org/software/gettext/gettext.html)\n* For `--enable-gegl` (GIMP *does not* require this):\n  - [GEGL + BABL](http://gegl.org/)\n\n### Install dependencies (Debian and derivatives)\n\nOn recent Debian-like systems, you can type the following\nto get started with a standard configuration:\n\n    # apt install -y build-essential\n    # apt install -y libjson-c-dev libgirepository1.0-dev libglib2.0-dev\n\nWhen building from git:\n\n    # apt install -y python autotools-dev intltool gettext libtool\n    \nYou might also try using your package manager:\n\n    # apt build-dep mypaint # will get additional deps for MyPaint (GUI)\n    # apt build-dep libmypaint  # may not exist; included in mypaint\n\n### Install dependencies (Red Hat and derivatives)\n\nThe following works on a minimal CentOS 7 installation:\n\n    # yum install -y gcc gobject-introspection-devel json-c-devel glib2-devel\n\nWhen building from git, you'll want to add:\n\n    # yum install -y git python autoconf intltool gettext libtool\n    \nYou might also try your package manager:\n\n    # yum builddep libmypaint\n\n### Install dependencies (OpenSUSE)\n\nWorks with a fresh OpenSUSE Tumbleweed Docker image:\n\n    # zypper install gcc13 gobject-introspection-devel libjson-c-devel glib2-devel\n\nWhen building from git:\n\n    # zypper install git python311 autoconf intltool gettext-tools libtool\n\nPackage manager:\n\n    # zypper install libmypaint0\n\n## Build and install\n\nMyPaint and libmypaint benefit dramatically from autovectorization and other compiler optimizations.\nYou may want to set your CFLAGS before compiling (for gcc):\n\n    $ export CFLAGS='-Ofast -ftree-vectorize -fopt-info-vec-optimized -march=native -mtune=native -funsafe-math-optimizations -funsafe-loop-optimizations'\n\nThe traditional setup works just fine.\n\n    $ ./autogen.sh    # Only needed when building from git.\n    $ ./configure\n    # make install\n    # ldconfig\n\n### Maintainer mode\n\nWe don't ship a `configure` script in our git repository. If you're\nbuilding from git, you have to kickstart the build environment with:\n\n    $ git clone https://github.com/mypaint/libmypaint.git\n    $ cd libmypaint\n    $ ./autogen.sh\n\nThis script generates `configure` from `configure.ac`, after running a\nfew checks to make sure your build environment is broadly OK. It also\nregenerates certain important generated headers if they need it.\n\nFolks building from a release tarball don't need to do this: they will\nhave a `configure` script from the start.\n\n### Configure\n\n    $ ./configure\n    $ ./configure --prefix=/tmp/junk/example\n\nThere are several MyPaint-specific options.\nThese can be shown by running\n\n    $ ./configure --help\n\n### Build\n\n    $ make\n\nOnce MyPaint is built, you can run the test suite and/or install it.\n\n### Test\n\n    $ make check\n\nThis runs all the unit tests.\n\n### Install\n\n    # make install\n\nUninstall libmypaint with `make uninstall`.\n\n### Check availability\n\nMake sure that pkg-config can see libmypaint before trying to build with it.\n\n    $ pkg-config --list-all | grep -i mypaint\n\nIf it's not found, you'll need to add the relevant pkgconfig directory to\nthe `pkg-config` search path. For example, on CentOS, with a default install:\n\n    # sh -c ""echo 'PKG_CONFIG_PATH=/usr/local/lib/pkgconfig' >>/etc/environment""\n\nMake sure ldconfig can see libmypaint as well\n\n    # ldconfig -p |grep -i libmypaint\n\nIf it's not found, you'll need to add the relevant lib directory to\nthe LD_LIBRARY_PATH:\n    \n    $ export LD_LIBRARY_PATH=/usr/local/lib\n    # sh -c ""echo 'LD_LIBRARY_PATH=/usr/local/lib' >>/etc/environment\n\nAlternatively, you may want to enable /usr/local for libraries.  Arch and Redhat derivatives:\n\n    # sh -c ""echo '/usr/local/lib' > /etc/ld.so.conf.d/usrlocal.conf""\n    # ldconfig\n\n## Contributing\n\nThe MyPaint project welcomes and encourages participation by everyone.\nWe want our community to be skilled and diverse,\nand we want it to be a community that anybody can feel good about joining.\nNo matter who you are or what your background is, we welcome you.\n\nPlease note that MyPaint is released with a\n[Contributor Code of Conduct](CODE_OF_CONDUCT.md).\nBy participating in this project you agree to abide by its terms.\n\nPlease see the file [CONTRIBUTING.md](CONTRIBUTING.md)\nfor details of how you can begin contributing.\n\n## Making releases\n\nThe distribution release can be generated with:\n\n    $ make dist\n\nAnd it should be checked before public release with:\n\n    $ make distcheck\n\n## Localization\n\nContribute translations here: <https://hosted.weblate.org/engage/mypaint/>.\n\nThe list of languages is maintained in [po/LINGUAS](po/LINGUAS).\nCurrently this file lists all the languages we have translations for.\nIt can be regenerated with:\n\n    $ ls po/*.po | sed 's$^.*po/\([^.]*\).po$\1$' | sort > po/LINGUAS\n\nYou can also disable languages by removing them from the list if needed.\n\nA list of files where localizable strings can be found is maintained\nin `po/POTFILES.in`.\n\n### Strings update\n\nYou can update the .po files when translated strings in the code change\nusing:\n\n    $ cd po && make update-po\n\nWhen the results of this are pushed, Weblate translators will see the\nnew strings immediately.\n\n## Documentation\n\nFurther documentation can be found in the libmypaint wiki:\n<https://github.com/mypaint/libmypaint/wiki>.\n\n\n## Software using libmypaint\n\n* [MyPaint](https://github.com/mypaint/mypaint)\n* [GIMP](https://gitlab.gnome.org/GNOME/gimp/)\n* [OpenToonz](https://github.com/opentoonz/opentoonz/)\n* [enve](https://github.com/MaurycyLiebner/enve/)\n",299,graphics,C,6,Python,C,C++,Makefile,M4,Shell,,,,,,,,,,,,,,,,,,,,,,,91,29,57,5,6,131,0,3183,88,105,85,20,032797ab1180f13e51b084d1353f9b5cc29b2260,Merge pull request #195 from weblate/weblate-mypaint-libmypaint,2024-02-24T10:40:31Z,Albert Westra,contact@odysseywestra.com,odysseywestra,1.6.1 - libtool `-release` flag no longer used,":warning: NOTE: This release contains no code changes. If you have already upgraded to [1.6.0](https://github.com/mypaint/libmypaint/releases/tag/v1.6.0), there is no need to upgrade to 1.6.1 for other reasons than avoiding future unnecessary rebuilds of reverse dependencies.\r\n\r\nThis release marks the end of including the MAJOR.MINOR API version fields in the library names. The abi fields have also been cleared, since they weren't that useful when used in conjunction with the libtool `-release` flag.\r\n\r\n----\r\n\r\n## Actual runtime compatibility (for reference)\r\n\r\nWhere `a --> b` means that a is backwards abi-compatible with b:\r\n\r\n1.6.1 --> 1.6.0 --> 1.5.1 --> 1.5.0 --> 1.3.0\r\n\r\n1.4.0 is not abi-compatible with any other release.\r\n\r\n\r\n",v1.6.1,Jesper Lloyd,,jplloyd,Other,libmypaint,mypaint,13,graphics,painting,library,libmypaint,c,mypaint,brush,,,,,,,,,,,,,,/mypaint/libmypaint,14,33,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/MultiQC/MultiQC,https://github.com/MultiQC/MultiQC,0,,0,0,0,1,1,0,0,0,0,0,0,0,Aggregate results from bioinformatics analyses across many samples into a single report.,"<h1>\n<picture>\n  <source media=""(prefers-color-scheme: dark)"" srcset=""https://github.com/MultiQC/MultiQC/raw/main/docs/images/MultiQC_logo_darkbg.png"">\n  <source media=""(prefers-color-scheme: light)"" srcset=""https://github.com/MultiQC/MultiQC/raw/main/docs/images/MultiQC_logo.png"">\n  <img src=""https://github.com/MultiQC/MultiQC/raw/main/docs/images/MultiQC_logo.png"" alt=""MultiQC"">\n</picture>\n</h1>\n\n### Aggregate bioinformatics results across many samples into a single report\n\n##### Find [documentation](http://multiqc.info/docs) and [example reports](https://multiqc.info/example-reports/) at [http://multiqc.info](http://multiqc.info)\n\n[![PyPI Version](https://img.shields.io/pypi/v/multiqc)](https://pypi.python.org/pypi/multiqc/)\n[![Bioconda Version](https://img.shields.io/conda/v/bioconda/multiqc?label=bioconda)](https://bioconda.github.io/recipes/multiqc/README.html)\n[![DOI](https://img.shields.io/badge/DOI-10.1093%2Fbioinformatics%2Fbtw354-red.svg)](http://dx.doi.org/10.1093/bioinformatics/btw354)\n\n---\n\nMultiQC is a tool to create a single report with interactive plots for multiple bioinformatics analyses across many samples.\n\nReports are generated by scanning given directories for recognised log files.\nThese are parsed and a single HTML report is generated summarising the statistics\nfor all logs found. MultiQC reports can describe multiple analysis steps and\nlarge numbers of samples within a single plot, and multiple analysis tools making\nit ideal for routine fast quality control.\n\nA very large number of Bioinformatics tools are supported by MultiQC. Please see the MultiQC website for a [complete list](https://multiqc.info/modules/).\nMultiQC can also easily parse data from custom scripts, if correctly formatted / configured - a feature called [Custom Content](https://multiqc.info/docs/custom_content/).\n\nMore modules are being written all the time. Please suggest any ideas as a new\n[issue](https://github.com/MultiQC/MultiQC/issues) _(please include example log files)_.\n\n## Installation\n\nYou can install MultiQC from [PyPI](https://pypi.python.org/pypi/multiqc/)\nusing `pip` as follows:\n\n```bash\npip install multiqc\n```\n\nAlternatively, you can install using [Conda](http://anaconda.org/)\nfrom [Bioconda](https://bioconda.github.io/) ([set up your channels](https://bioconda.github.io/#usage) first):\n\n```bash\nconda install multiqc\n```\n\nIf you would like the development version from GitHub instead, you can install it with `pip`:\n\n```bash\npip install --upgrade --force-reinstall git+https://github.com/MultiQC/MultiQC.git\n```\n\nMultiQC is also available via Docker and Singularity images, Galaxy wrappers, and\nmany more software distribution systems.\nSee [the documentation](https://multiqc.info/docs/getting_started/installation/) for details.\n\n## Usage\n\nOnce installed, you can use MultiQC by navigating to your analysis directory\n(or a parent directory) and running the tool:\n\n```bash\nmultiqc .\n```\n\nThat's it! MultiQC will scan the specified directory (`.` is the current dir)\nand produce a report detailing whatever it finds.\n\n<!-- RICH-CODEX fake_command: ""multiqc ."" -->\n\n![`cd test-data/data/modules/fastqc/v0.10.1 && multiqc .`](https://github.com/MultiQC/MultiQC/raw/main/docs/images/screenshots/fastqc-run.svg)\n\nThe report is created in `multiqc_report.html` by default. Tab-delimited data\nfiles are also created in `multiqc_data/`, containing extra information.\nThese can be easily inspected using Excel (use `--data-format` to get `yaml`\nor `json` instead).\n\nFor more detailed instructions, run `multiqc -h` or see the\n[documentation](http://multiqc.info/docs/#running-multiqc).\n\n## Citation\n\nPlease consider citing MultiQC if you use it in your analysis.\n\n> **MultiQC: Summarize analysis results for multiple tools and samples in a single report.** <br> _Philip Ewels, Måns Magnusson, Sverker Lundin and Max Käller_ <br>\n> Bioinformatics (2016) <br>\n> doi: [10.1093/bioinformatics/btw354](http://dx.doi.org/10.1093/bioinformatics/btw354) <br>\n> PMID: [27312411](http://www.ncbi.nlm.nih.gov/pubmed/27312411)\n\n```BibTeX\n@article{doi:10.1093/bioinformatics/btw354,\n author = {Ewels, Philip and Magnusson, Måns and Lundin, Sverker and Käller, Max},\n title = {MultiQC: summarize analysis results for multiple tools and samples in a single report},\n journal = {Bioinformatics},\n volume = {32},\n number = {19},\n pages = {3047},\n year = {2016},\n doi = {10.1093/bioinformatics/btw354},\n URL = { + http://dx.doi.org/10.1093/bioinformatics/btw354},\n eprint = {/oup/backfile/Content_public/Journal/bioinformatics/32/19/10.1093_bioinformatics_btw354/3/btw354.pdf}\n}\n```\n\n## Contributions & Support\n\nContributions and suggestions for new features are welcome, as are bug reports!\nPlease create a new [issue](https://github.com/MultiQC/MultiQC/issues) for any\nof these, including example reports where possible.\nPull-requests for fixes and additions are very welcome.\nPlease see the [contributing notes](https://github.com/MultiQC/MultiQC/blob/main/.github/CONTRIBUTING.md) for more information about how the process works.\n\nMultiQC has extensive [documentation](http://multiqc.info/docs/development/)\ndescribing how to write new modules, plugins and templates.\n\nIf in doubt, feel free to get in touch with the author directly:\n[@ewels](https://github.com/ewels) (phil.ewels@seqera.io)\n\n### Contributors\n\nMultiQC is developed and maintained by Phil Ewels ([@ewels](https://github.com/ewels)) at [Seqera Labs](https://seqera.io/).\nIt was originally written at the [National Genomics Infrastructure](https://ngisweden.scilifelab.se/), part of [SciLifeLab](https://www.scilifelab.se/) in Sweden.\n\nA huge thank you to all code contributors - there are a lot of you!\nSee the [Contributors Graph](https://github.com/MultiQC/MultiQC/graphs/contributors) for details.\n\nMultiQC is released under the GPL v3 or later licence.\n",1190,bioinformatics,JavaScript,7,Python,HTML,CSS,JavaScript,Shell,Dockerfile,Nix,,,,,,,,,,,,,,,,,,,,,,1157,111,1007,39,44,218,3,35238,591,1545,1338,207,06432f56447d9c05894e9aeff9a34ee56a788ba6,Fix re-loading explicit user configs in interactive sessions (#2704),2024-07-18T13:47:45Z,Vlad Savelyev,vladislav.sav@gmail.com,vladsavelyev,v1.23,"## [MultiQC v1.23](https://github.com/MultiQC/MultiQC/releases/tag/v1.23)\r\n\r\nBug fixes, integration of `pytest` and `mypy`, and one new module.\r\n\r\nFrom the user perspective, this is mostly a maintenance release, containing several\r\nimportant bugfixes, plus minor improvements and a new module - Glimpse.\r\n\r\nFor developers, there are two significant additions to the CI workflow:\r\n\r\n- [pytest](https://docs.pytest.org/), along with unit tests covering the core library,\r\n- and [mypy](https://mypy-lang.org/), along with ensuring that the core codebase is fully\r\n  type-annotated.\r\n\r\nThe core unit tests are located in the `multiqc/tests` folder, and the module tests are\r\nlocated in the corresponding `multiqc/modules/*/tests` subfolders.\r\nThe [CI workflows](https://github.com/MultiQC/MultiQC/tree/main/.github/workflows)\r\nare refactored to separate the integration tests and the unit tests, to improve the\r\ngranularity and parallelization. The tests are discovered and executed with pytest,\r\nand the coverage is reported by [codecov](https://app.codecov.io/gh/MultiQC/MultiQC).\r\n\r\nThe `multiqc/tests` subfolder has several test files that cover most of the core library.\r\nIt also has a [test_modules_run.py](https://github.com/MultiQC/MultiQC/blob/main/tests/test_modules_run.py)\r\ntests that checks that every module didn't crash when being run on the corresponding data\r\nin [test-data](https://github.com/MultiQC/test-data), and added _something_ into the report.\r\nThat is somewhat of a blanket test for modules, that doesn't check if the modules logic\r\nworked correctly. For that reason, the users are encouraged to write more comprehensive\r\ntests that take the specific module logic into account, and place them in\r\n`multiqc/modules/*/tests`. For some initial examples, consider checking:\r\n\r\n- The [samtools flagstat](https://github.com/MultiQC/MultiQC/blob/main/multiqc/modules/samtools/tests/test_flagstat.py)\r\n  test that verifies some logic in the `flagstat` submodule of the `samtools` module;\r\n- The [picard tools](https://github.com/MultiQC/MultiQC/blob/main/multiqc/modules/picard/tests/test_picard.py)\r\n  test that checks that every submodule for each Picard tool worked correctly.\r\n\r\n### Other changes & fixes\r\n\r\n#### Fixes\r\n\r\n- Custom content""\r\n  - Multiple fixes of the custom content parsing logic ([#2674](https://github.com/MultiQC/MultiQC/pull/2674))\r\n  - Fix parsing custom content submodules with a custom config ([#2654](https://github.com/MultiQC/MultiQC/pull/2654))\r\n  - Line plot: fix spreading `extra_series` between multiple datasets ([#2684](https://github.com/MultiQC/MultiQC/pull/2684))\r\n  - Line plot series: allow pass lists instead of x/y tuples to work properly with YAML ([#2683](https://github.com/MultiQC/MultiQC/pull/2683))\r\n- Re-enabling the `software_version` module section ([#2670](https://github.com/MultiQC/MultiQC/pull/2670))\r\n- When `--no-ansi` is set, disable colors in `rich_click` too ([#2678](https://github.com/MultiQC/MultiQC/pull/2678))\r\n- Support CWD path filters ( `./path/...`) in config ([#2676](https://github.com/MultiQC/MultiQC/pull/2676))\r\n- Fix writing report to stdout with `--filename stdout`, log to stderr ([#2672](https://github.com/MultiQC/MultiQC/pull/2672))\r\n- Interactive use:\r\n  - Reset all config values in `config.reset()`, even those that are not in `config_default.yaml` ([#2660](https://github.com/MultiQC/MultiQC/pull/2660))\r\n  - Reset config between `multiqc.run()` calls ([#2655](https://github.com/MultiQC/MultiQC/pull/2655))\r\n  - Better handling of calling `write_report` twice ([#2688](https://github.com/MultiQC/MultiQC/pull/2688))\r\n\r\n#### Updates\r\n\r\n- Run mypy on core library ([#2665](https://github.com/MultiQC/MultiQC/pull/2665))\r\n- Add tests for plot export ([#2682](https://github.com/MultiQC/MultiQC/pull/2682))\r\n- Add tests for command line use, including for passing `TMPDIR` ([#2677](https://github.com/MultiQC/MultiQC/pull/2677))\r\n- Custom content: allow hash-fenced table columns ([#2649](https://github.com/MultiQC/MultiQC/pull/2649))\r\n- Software versions: parse for sorting, but preserve the original strings ([#2671](https://github.com/MultiQC/MultiQC/pull/2671))\r\n- Allow both table-level and column-level custom plot config for table ([#2662](https://github.com/MultiQC/MultiQC/pull/2662))\r\n\r\n#### New modules\r\n\r\n- Glimpse ([#2492](https://github.com/MultiQC/MultiQC/pull/2492))\r\n\r\n#### Module fixes\r\n\r\n- Fix parsing kraken vs. bracken: respect `num_lines` in search patterns ([#2657](https://github.com/MultiQC/MultiQC/pull/2657))\r\n- Fix the `bbmap/qchist` search pattern ([#2661](https://github.com/MultiQC/MultiQC/pull/2661))\r\n\r\n#### Module updates\r\n\r\n- Picard HsMetrics: support any custom X coverage metrics ([#2663](https://github.com/MultiQC/MultiQC/pull/2663))\r\n- Samtools coverage: avoid hard crash for invalid file contents ([#2664](https://github.com/MultiQC/MultiQC/pull/2664))\r\n\r\n#### Refactoring\r\n\r\n- Abstract code related to temporary directory creation into a separate module ([#2675](https://github.com/MultiQC/MultiQC/pull/2675))\r\n\r\n#### Infrastructure\r\n\r\n- Use pull-request labels and milestones for changelog generation ([#2691](https://github.com/MultiQC/MultiQC/pull/2691))\r\n",v1.23,Vlad Savelyev,,vladsavelyev,GNU General Public License v3.0,MultiQC,MultiQC,39,bioinformatics,analysis,pypi,bioconda,multiqc,python,data-visualization,quality-control,reporting,seqera,vizualisation,,,,,,,,,,/MultiQC/MultiQC,39,37,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/MouseLand/cellpose,https://github.com/MouseLand/cellpose,1,,,1,1,1,0,0,0,0,0,0,0,0,a generalist algorithm for cellular segmentation with human-in-the-loop capabilities,"# <p>  <b>Cellpose </b> </p>\n<img src=""http://www.cellpose.org/static/images/logo.png?raw=True"" width=""250"" title=""cellpose"" alt=""cellpose"" align=""right"" vspace = ""50"">\n\n[![Documentation Status](https://readthedocs.org/projects/cellpose/badge/?version=latest)](https://cellpose.readthedocs.io/en/latest/?badge=latest)\n![tests](https://github.com/mouseland/cellpose/actions/workflows/test_and_deploy.yml/badge.svg)\n[![codecov](https://codecov.io/gh/MouseLand/cellpose/branch/main/graph/badge.svg?token=9FFo4zNtYP)](https://codecov.io/gh/MouseLand/cellpose)\n[![PyPI version](https://badge.fury.io/py/cellpose.svg)](https://badge.fury.io/py/cellpose)\n[![Downloads](https://pepy.tech/badge/cellpose)](https://pepy.tech/project/cellpose)\n[![Downloads](https://pepy.tech/badge/cellpose/month)](https://pepy.tech/project/cellpose)\n[![Python version](https://img.shields.io/pypi/pyversions/cellpose)](https://pypistats.org/packages/cellpose)\n[![Licence: GPL v3](https://img.shields.io/github/license/MouseLand/cellpose)](https://github.com/MouseLand/cellpose/blob/master/LICENSE)\n[![Contributors](https://img.shields.io/github/contributors-anon/MouseLand/cellpose)](https://github.com/MouseLand/cellpose/graphs/contributors)\n[![website](https://img.shields.io/website?url=https%3A%2F%2Fwww.cellpose.org)](https://www.cellpose.org)\n[![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&url=https%3A%2F%2Fforum.image.sc%2Ftags%2Fcellpose.json&query=%24.topic_list.tags.0.topic_count&colorB=brightgreen&suffix=%20topics&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tag/cellpose)\n[![repo size](https://img.shields.io/github/repo-size/MouseLand/cellpose)](https://github.com/MouseLand/cellpose/)\n[![GitHub stars](https://img.shields.io/github/stars/MouseLand/cellpose?style=social)](https://github.com/MouseLand/cellpose/)\n[![GitHub forks](https://img.shields.io/github/forks/MouseLand/cellpose?style=social)](https://github.com/MouseLand/cellpose/)\n\nA generalist algorithm for cell and nucleus segmentation (v1.0) that can be optimized for your own data (v2.0) and (**NEW**) perform image restoration (v3.0).\n\nCellpose was written by Carsen Stringer and Marius Pachitariu. To learn about Cellpose3 (image restoration), read the [paper](https://www.biorxiv.org/content/10.1101/2024.02.10.579780v1). To learn about Cellpose 2.0 (human-in-the-loop), read the [paper](https://www.nature.com/articles/s41592-022-01663-4) or watch the [talk](https://www.youtube.com/watch?v=3ydtAhfq6H0). To learn about Cellpose 1.0, read the [paper](https://t.co/kBMXmPp3Yn?amp=1) or watch the [talk](https://t.co/JChCsTD0SK?amp=1). For support, please open an [issue](https://github.com/MouseLand/cellpose/issues). \n\nPlease see install instructions [below](README.md/#Installation), and also check out the detailed documentation at [**cellpose.readthedocs.io**](https://cellpose.readthedocs.io/en/latest/) for more information.\n\n### :star2: v3 (Feb 2024) :star2:\n\nCellpose3 enables image restoration in the GUI, API and CLI (saved to `_seg.npy`). To learn more...\n* Check out the [paper](https://www.biorxiv.org/content/10.1101/2024.02.10.579780v1).\n* API documentation [here](https://cellpose.readthedocs.io/en/latest/restore.html)\n* Example google colab notebook for image restoration: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MouseLand/cellpose/blob/main/notebooks/run_cellpose3.ipynb), using new `CellposeDenoiseModel`.\n* Example google colab notebook with new super-generalist ""cyto3"" model: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MouseLand/cellpose/blob/main/notebooks/run_cyto3.ipynb). Try out the new `cyto3` super-generalist Cellpose model with `model_type=""cyto3""`.\n\ndenoising             |  deblurring                  |  upsampling\n:-------------------------:|:-------------------------:|:-------------------------:\n<img src=""http://www.cellpose.org/static/images/poisson_cp3.gif?raw=True"" width=""350"" title=""cellpose denoising"" alt=""cellpose denoising examples""> | <img src=""http://www.cellpose.org/static/images/blur_cp3.gif?raw=True"" width=""350"" title=""cellpose deblurring"" alt=""cellpose deblurring examples""> | <img src=""http://www.cellpose.org/static/images/downsample_cp3.gif?raw=True"" width=""350"" title=""cellpose upsampling"" alt=""cellpose upsampling examples"">\n\n### CITATION\n\n**If you use Cellpose 1, 2 or 3, please cite the Cellpose 1.0 [paper](https://t.co/kBMXmPp3Yn?amp=1):**  \nStringer, C., Wang, T., Michaelos, M., & Pachitariu, M. (2021). Cellpose: a generalist algorithm for cellular segmentation. <em>Nature methods, 18</em>(1), 100-106.\n\n**If you use the human-in-the-loop training, please also cite the Cellpose 2.0 [paper](https://www.nature.com/articles/s41592-022-01663-4):**  \nPachitariu, M. & Stringer, C. (2022). Cellpose 2.0: how to train your own model. <em>Nature methods</em>, 1-8.\n\n**If you use the new image restoration models or cyto3, please also cite the Cellpose3 [paper](https://www.biorxiv.org/content/10.1101/2024.02.10.579780v1):**  \nStringer, C. & Pachitariu, M. (2024). Cellpose3: one-click image restoration for improved segmentation. <em>bioRxiv</em>.\n\n:triangular_flag_on_post: All models in Cellpose, except `yeast_BF_cp3`, `yeast_PhC_cp3`, and `deepbacs_cp3`, are trained on some amount of data that is **CC-BY-NC**. The Cellpose annotated dataset is also CC-BY-NC.\n\n### :star2: v2.0 (April 2022) :star2:\n\nCellpose 2.0 allows human-in-the-loop training of models! To learn more...\n* Check out the twitter [thread](https://twitter.com/marius10p/status/1511415409047650307?s=20&t=umTVIG1CFKIWHYMrQqFKyQ) for an overview.\n* Check out the [paper](https://www.nature.com/articles/s41592-022-01663-4) for more details on the algorithm and the performance. Also, there's a short review of the paper available [here](https://www.nature.com/articles/s41592-022-01664-3).\n* Watch the short overview [talk](https://youtu.be/wB7XYh4QRiI) and watch the longer [tutorial talk](https://youtu.be/5qANHWoubZU) which goes through running Cellpose 2.0 in the GUI and a jupyter notebook.\n* Check out the full human-in-the-loop [video](https://youtu.be/3Y1VKcxjNy4). \n* Check out the colab notebook to get cloud access to a GPU to train your models or run your custom models: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MouseLand/cellpose/blob/main/notebooks/run_cellpose_2.ipynb).\n* See how to use it yourself in the [docs](https://cellpose.readthedocs.io/en/latest/gui.html#training-your-own-cellpose-model) and also check out the help info in the `Models` menu in the GUI.\n\nMxnet is no longer supported in cellpose. To use mxnet, please use v1.0.2 (not recommended).\n\n# Installation\n\nYou can install cellpose using conda or with native python if you have python3.8+ on your machine. \n\n## Local installation (< 2 minutes)\n\n### System requirements\n\nLinux, Windows and Mac OS are supported for running the code. For running the graphical interface you will need a Mac OS later than Yosemite. At least 8GB of RAM is required to run the software. 16GB-32GB may be required for larger images and 3D volumes. The software has been heavily tested on Windows 10 and Ubuntu 18.04 and less well-tested on Mac OS. Please open an issue if you have problems with installation.\n\n### Dependencies\ncellpose relies on the following excellent packages (which are automatically installed with conda/pip if missing):\n- [pytorch](https://pytorch.org/)\n- [pyqtgraph](http://pyqtgraph.org/)\n- [PyQt5](http://pyqt.sourceforge.net/Docs/PyQt5/)\n- [numpy](http://www.numpy.org/) (>=1.16.0)\n- [numba](http://numba.pydata.org/numba-doc/latest/user/5minguide.html)\n- [scipy](https://www.scipy.org/)\n- [natsort](https://natsort.readthedocs.io/en/master/)\n\n### Option 1: Installation Instructions with conda \n\nIf you have an older `cellpose` environment you can remove it with `conda env remove -n cellpose` before creating a new one.\n\nIf you are using a GPU, make sure its drivers and the cuda libraries are correctly installed.\n\n1. Install an [Anaconda](https://www.anaconda.com/products/distribution) distribution of Python. Note you might need to use an anaconda prompt if you did not add anaconda to the path.\n2. Open an anaconda prompt / command prompt which has `conda` for **python 3** in the path\n3. Create a new environment with `conda create --name cellpose python=3.8`. We recommend python 3.8, but python 3.9 and 3.10 will likely work as well.\n4. To activate this new environment, run `conda activate cellpose`\n5. To install the minimal version of cellpose, run `python -m pip install cellpose`.  \n6. To install cellpose and the GUI, run `python -m pip install cellpose[gui]`. If you're on a zsh server, you may need to use ' ' around the cellpose[gui] call: `python -m pip install 'cellpose[gui]'`.\n\nTo upgrade cellpose (package [here](https://pypi.org/project/cellpose/)), run the following in the environment:\n\n~~~sh\npython -m pip install cellpose --upgrade\n~~~\n\nNote you will always have to run `conda activate cellpose` before you run cellpose. If you want to run jupyter notebooks in this environment, then also `python -m pip install notebook` and `python -m pip install matplotlib`.\n\nYou can also try to install cellpose and the GUI dependencies from your base environment using the command\n\n~~~~sh\npython -m pip install cellpose[gui]\n~~~~\n\nIf you have **issues** with installation, see the [docs](https://cellpose.readthedocs.io/en/latest/installation.html) for more details. You can also use the cellpose environment file included in the repository and create a cellpose environment with `conda env create -f environment.yml` which may solve certain dependency issues.\n\nIf these suggestions fail, open an issue.\n\n### Option 2: Installation Instructions with python's venv\n\nVenv ([tutorial](https://docs.python-guide.org/dev/virtualenvs/#lower-level-virtualenv), for those interested) is a built-in tool in python for creating virtual environments. It is a good alternative if you don't want to install conda and already have python3 on your machine. The main difference is that you will need to choose where to install the environment and the packages. Cellpose will then live in this environment and not be accessible from other environments. You will need to navigate to the environment directory and activate it each time before running cellpose. The steps are similar to the conda installation:\n\nIf you are using a GPU, make sure its drivers and the cuda libraries are correctly installed.\n\n1. Install python3.8 or later from [python.org](https://www.python.org/downloads/). This will be the version of python that will be used in the environment. You can check your python version with `python --version`.\n2. Navigate to the directory where you want to create the environment and run `python3 -m venv cellpose` to create a new environment called `cellpose`.\n3. Activate the environment with `source cellpose/bin/activate` on Mac/Linux or `cellpose\Scripts\activate` on Windows. A prefix `(cellpose)` should appear in the terminal.\n4. Install cellpose into the `cellpose` venv using pip with `python -m pip install cellpose`.\n5. Install the cellpose GUI, with `python -m pip install cellpose[gui]`. Depending on your terminal software, you may need to use quotes like this: `python -m pip install 'cellpose[gui]'`.\n6. You can now run cellpose from this environment with `python -m cellpose` or `cellpose` if you are in the cellpose directory.\n7. To deactivate the environment, run `deactivate`. \n\n### GPU version (CUDA) on Windows or Linux\n\nIf you plan on running many images, you may want to install a GPU version of *torch* (if it isn't already installed).\n\nTo use your NVIDIA GPU with python, you will first need to install the NVIDIA driver for your GPU, check out this [website](https://www.nvidia.com/Download/index.aspx?lang=en-us) to download it. You can also install the CUDA toolkit, or use the pytorch cudatoolkit (installed below with conda). If you have trouble with the below install, we recommend installing the CUDA toolkit yourself, choosing one of the 11.x releases [here](https://developer.nvidia.com/cuda-toolkit-archive).\n\nNext we need to remove the CPU version of torch:\n~~~\npip uninstall torch\n~~~\n\nTo install the GPU version of torch, follow the instructions [here](https://pytorch.org/get-started/locally/). The conda install is strongly recommended, and then choose the CUDA version that is supported by your GPU (newer GPUs may need newer CUDA versions > 10.2). For instance this command will install the 11.6 version on Linux and Windows (note the `torchvision` and `torchaudio` commands are removed because cellpose doesn't require them):\n~~~\nconda install pytorch pytorch-cuda=11.6 -c pytorch -c nvidia\n~~~\n\nIf the latest CUDA versions don't work, try an older version like cuda 11.3:\n~~~\nconda install pytorch==1.12.0 cudatoolkit=11.3 -c pytorch\n~~~~\nInfo on how to install several older versions is available [here](https://pytorch.org/get-started/previous-versions/). After install you can check `conda list` for `pytorch`, and its version info should have `cuXX.X`, not `cpu`.\n\n### Installation of github version\n\nFollow steps from above to install the dependencies. Then run \n~~~\npip install git+https://www.github.com/mouseland/cellpose.git\n~~~\n\nIf you want edit ability to the code, in the github repository folder, run `pip install -e .`. If you want to go back to the pip version of cellpose, then say `pip install cellpose`.\n\n## Run cellpose 1.0 without local python installation\n\nYou can quickly try out Cellpose on the [website](https://www.cellpose.org) first (many features disabled). \n\nYou can also run Cellpose in google colab with a GPU: \n* a code-based notebook: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MouseLand/cellpose/blob/main/notebooks/run_cellpose_GPU.ipynb)\n* a more user-friendly notebook for 2D segmentation written by [@pr4deepr](https://github.com/pr4deepr): [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MouseLand/cellpose/blob/main/notebooks/Cellpose_cell_segmentation_2D_prediction_only.ipynb)\n* a user-friendly [ZeroCostDL4Mic](https://github.com/HenriquesLab/ZeroCostDL4Mic) notebook that includes training cellpose models, written by [@guijacquemet](https://github.com/guijacquemet): [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/Beta%20notebooks/Cellpose_2D_ZeroCostDL4Mic.ipynb)\n\nThe colab notebooks are recommended if you have issues with MKL or run speed on your local computer (and are running 3D volumes). Colab does not allow you to run the GUI, but you can save `*_seg.npy` files in colab that you can download and open in the GUI.\n\n**Executable file**: You can download an executable file for [*Windows 10*](http://www.cellpose.org/windows) or for [*Mac OS*](http://www.cellpose.org/mac) (High Sierra or greater) that were made using PyInstaller on Intel processors (MKL acceleration works, but no GPU support). Note in both cases it will take a few seconds to open.\n\n* The [*Mac OS*](https://www.cellpose.org/mac) file will download as `cellpose_mac` OR `cellpose_mac.dms`. You will need to make it into an executable file and run it through the terminal:\n1. Open a terminal and run `cd ~/Downloads/`.\n2. Run `chmod 777 cellpose_mac` OR `chmod 777 cellpose_mac.dms` to make the file executable.\n3. Run `./cellpose_mac` OR `./cellpose_mac.dms` to open the cellpose GUI. Messages from cellpose will be printed in the terminal.\n4. You can also run using the command line interface, e.g. as `./cellpose_mac --dir ~/Pictures/ --chan 2 --save_png`.\n\n* The [*Windows 10*](https://www.cellpose.org/windows) file is an exe and you can click on it to run the GUI. You can also run using the command line interface, e.g. as `cellpose.exe --dir Pictures/ --chan 2 --save_png`\n\n# Run cellpose locally\n\nThe quickest way to start is to open the GUI from a command line terminal. You might need to open an anaconda prompt if you did not add anaconda to the path:\n~~~~\npython -m cellpose\n~~~~\n\nThe first time cellpose runs it downloads the latest available trained model weights from the website.\n\nYou can now **drag and drop** any images (*.tif, *.png, *.jpg, *.gif) into the GUI and run Cellpose, and/or manually segment them. When the GUI is processing, you will see the progress bar fill up and during this time you cannot click on anything in the GUI. For more information about what the GUI is doing you can look at the terminal/prompt you opened the GUI with. For example data, see [website](http://www.cellpose.org) or this [zip file](https://www.cellpose.org/static/images/demo_images.zip). For best accuracy and runtime performance, resize images so cells are less than 100 pixels across. \n\nFor 3D data, with multi-Z, please use the 3D version of the GUI with:\n~~~~\npython -m cellpose --Zstack\n~~~~\n\n\n## Step-by-step demo\n\n1. Download this [folder](http://cellpose.org/static/images/demo_images.zip) of images and unzip it. These are a subset of the test images from the paper.\n2. Start the GUI with `python -m cellpose`.\n3. Drag an image from the folder into the GUI.\n4. Set the model (in demo all are `cyto`) and the channel you want to segment (in demo all are `green`). Optionally set the second channel if you are segmenting `cyto` and have an available nucleus channel.\n5. Click the `calibrate` button to estimate the size of the objects in the image. Alternatively (RECOMMENDED) you can set the `cell diameter` by hand and press ENTER. You will see the size you set as a red disk at the bottom left of the image.\n6. Click the `run segmentation` button. If MASKS ON is checked, you should see masks drawn on the image.\n7. Now you can click the LEFT/RIGHT arrow keys to move through the folder and segment another image.\n\nTo draw ROIs on the image you can right-click then hover to complete the ROI (do not right-click and drag). To remove ROIs left-click while holding down CTRL. See more details [here](https://cellpose.readthedocs.io/en/latest/gui.html).\n\nOn the demo images each of these steps should run in less than a few seconds on a standard laptop or desktop (with mkl working).\n\n### 3D segmentation\n\nFor multi-channel, multi-Z tiff's, the expected format is Z x channels x Ly x Lx.\n\n### Download of pretrained models\n\nThe models will be downloaded automatically from the [website](https://www.cellpose.org) when you first run a pretrained model in cellpose. If you are having issues with the downloads, you can download them from this [google drive zip file](https://drive.google.com/file/d/1zHGFYCqRCTwTPwgEUMNZu0EhQy2zaovg/view?usp=sharing), unzip the file and put the models in your home directory under the path .cellpose/models/, e.g. on Windows this would be C:/Users/YOUR_USERNAME/.cellpose/models/ or on Linux this would be /home/YOUR_USERNAME/.cellpose/models/, so /home/YOUR_USERNAME/.cellpose/models/cyto_0 is the full path to one model for example. If you cannot access google drive, the models are also available on baidu: Link：https://pan.baidu.com/s/1CARpRGCBHIYaz7KeyoX-fg ; Fetch code：pose ; thanks to @qixinbo!\n\n# Older software releases\n\n### UPDATE v1.0 (Jan 2022)\n\nCellpose has been relatively stable for a while now. Small bugs will continue to be fixed, but we are now releasing a reference 1.0 version. Larger updates to Cellpose will go towards a new 2.0 candidate version to be released soon.  \n\nThis update fixes bugs in GUI and plotting. It also stops model weight reloading to improve speed. `resample=True` is default again as in earlier releases, turn off with `--no_resample`. Now logging is turned off by default. Turn on in CLI with `--verbose` flag or in a script/notebook by\n```\nfrom cellpose.io import logger_setup\nlogger_setup();\n```\n\nTo install this version please use\n```\npip install cellpose==1.0.2\n```\n\n### UPDATE v0.7 (Nov 2021)\n\nCheck out [Omnipose](https://github.com/kevinjohncutler/omnipose), an extension of Cellpose for long filamentous bacteria. Omnipose was written by Kevin Cutler ([@kevinjohncutler](https://github.com/kevinjohncutler)). To learn about Omnipose, read the [paper](http://biorxiv.org/content/early/2021/11/04/2021.11.03.467199).\n\n### UPDATE v0.6 (Dec 2020)\n\nPytorch is now the default deep neural network software for cellpose. Mxnet will still be supported. To install mxnet (CPU), run `pip install mxnet-mkl`. To use mxnet in a notebook, declare `torch=False` when creating a model, e.g. `model = models.Cellpose(torch=False)`. To use mxnet on the command line, add the flag `--mxnet`, e.g. `python -m cellpose --dir ~/images/ --mxnet`. The pytorch implementation is 20% faster than the mxnet implementation when running on the GPU and 20% slower when running on the CPU. \n\nDynamics are computed using bilinear interpolation by default instead of nearest neighbor interpolation. Set `interp=False` in `model.eval` to turn off. The bilinear interpolation will be slightly slower on the CPU, but it is faster than nearest neighbor if using torch and the GPU is enabled.\n\n\n### Timing (v0.6)\n\nYou can check if cellpose is running the MKL version (if you are using the CPU not the GPU) by adding the flag `--check_mkl`. If you are not using MKL cellpose will be much slower. Here are Cellpose run times divided into the time it takes to run the deep neural network (DNN) and the time for postprocessing (gradient tracking, segmentation, quality control etc.). The DNN runtime is shown using either a GPU (Nvidia GTX 1080Ti) or a CPU (Intel 10-core 7900X), with or without network ensembling (4net vs 1net). The postprocessing runtime is similar regardless of ensembling or CPU/GPU version. Runtime is shown for different image sizes, all with a cell diameter of 30 pixels (the average from our training set).\n\n|   | 256 pix | 512 pix | 1024 pix |\n|----|-------|------|----------|\n| DNN (1net, GPU) | 0.054 s | 0.12 s | 0.31 s  |\n| DNN (1net, CPU) | 0.30 s | 0.65 s | 2.4 s  |\n| DNN (4net, GPU) | 0.23 s | 0.41 s | 1.3 s |\n| DNN (4net, CPU) | 1.3 s | 2.5 s | 9.1 s  |\n|  | |  |  |\n| Postprocessing (CPU) | 0.32 s | 1.2 s | 6.1 s  |\n",1264,cell-segmentation,Jupyter Notebook,3,Python,Jupyter Notebook,HTML,,,,,,,,,,,,,,,,,,,,,,,,,,133,39,81,13,13,35,0,112715,364,842,758,84,84344b0ceb6eb91e759301fbdad5c9b77a1c881b,Update README.md,2024-06-20T19:44:01Z,carsen-stringer,carsen.stringer@gmail.com,carsen-stringer,Cellpose v3.0.10,* fix bug in diameter changing when rerunning the model after training -- should make Cellpose 3 training parameters now equivalent to Cellpose 2\r\n* fix bug with nimg_per_epoch being fixed to 8 when training in the GUI (#925),v3.0.10,,,carsen-stringer,"BSD 3-Clause ""New"" or ""Revised"" License",cellpose,MouseLand,22,segmentation,cell-segmentation,cell-biology,,,,,,,,,,,,,,,,,,/MouseLand/cellpose,47,29,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/MonoGame/MonoGame,https://github.com/MonoGame/MonoGame,0,,,0,0,0,0,0,0,1,0,0,0,0,One framework for creating powerful cross-platform games.,"﻿# MonoGame\n\nMonoGame is a simple and powerful .NET framework for creating games for desktop PCs, video game consoles, and mobile devices using the C# programming language. It has been successfully used to create games such as [Streets of Rage 4](https://store.steampowered.com/app/985890/Streets_of_Rage_4/), [Carrion](https://store.steampowered.com/app/953490/CARRION/), [Celeste](https://store.steampowered.com/app/504230/Celeste/), [Stardew Valley](https://store.steampowered.com/app/413150/Stardew_Valley/), and [many others](https://monogame.net/showcase/).\n\nIt is an open-source re-implementation of the discontinued [Microsoft's XNA Framework](https://msdn.microsoft.com/en-us/library/bb200104.aspx).\n\n[![Join the chat at https://discord.gg/monogame](https://img.shields.io/discord/355231098122272778?color=%237289DA&label=MonoGame&logo=discord&logoColor=white)](https://discord.gg/monogame)\n\n* [Build Status](#build-status)\n* [Supported Platforms](#supported-platforms)\n* [Support and Contributions](#support-and-contributions)\n* [Source Code](#source-code)\n* [Helpful Links](#helpful-links)\n* [License](#license)\n\n## Build Status\n\nWe use [GitHub Actions](https://github.com/MonoGame/MonoGame/actions) to automate builds and packages distribution of the latest MonoGame changes. We also rely on a [build server](http://teamcity.monogame.net/?guest=1) to run tests in order to avoid regressions. The table below shows the current build status for the ```develop``` branch.\n\n| Name | Status |\n|:---- | ------ |\n| Builds | [![Build](https://github.com/MonoGame/MonoGame/actions/workflows/main.yml/badge.svg?branch=develop)](https://github.com/MonoGame/MonoGame/actions/workflows/main.yml) |\n\n## Supported Platforms\n\nWe support a growing list of platforms across the desktop, mobile, and console space. If there is a platform we don't support, please [make a request](https://github.com/MonoGame/MonoGame/issues) or [come help us](CONTRIBUTING.md) add it.\n\n* Desktop PCs\n  * Windows 8.1 and up (OpenGL & DirectX)\n  * Linux (OpenGL)\n  * macOS 10.15 and up (OpenGL)\n* Mobile/Tablet Devices\n  * Android 6.0 and up (OpenGL)\n  * iPhone/iPad 10.0 and up (OpenGL)\n* Consoles (for registered developers)\n  * PlayStation 4\n  * PlayStation 5\n  * Xbox One (XDK only) (GDK coming soon)\n  * Nintendo Switch\n  * Google Stadia\n\n## Support and Contributions\n\nIf you think you have found a bug or have a feature request, use our [issue tracker](https://github.com/MonoGame/MonoGame/issues). Before opening a new issue, please search to see if your problem has already been reported. Try to be as detailed as possible in your issue reports.\n\nIf you need help using MonoGame or have other questions we suggest you post on our [community forums](http://community.monogame.net). Please do not use the GitHub issue tracker for personal support requests.\n\nIf you are interested in contributing fixes or features to MonoGame, please read our [contributors guide](CONTRIBUTING.md) first.\n\n### Subscription\n\nIf you'd like to help the project by supporting us financially, consider supporting us via a subscription for the price of a monthly coffee.\n\nMoney goes towards hosting, new hardware and if enough people subscribe a dedicated developer.\n\nThere are several options on our [Donation Page](https://monogame.net/donate/).\n\n## Source Code\n\nThe full source code is available here from GitHub:\n\n* Clone the source: `git clone https://github.com/MonoGame/MonoGame.git`\n* Set up the submodules: `git submodule update --init`\n* Open the solution for your target platform to build the game framework.\n* Open the Tools solution for your development platform to build the pipeline and content tools.\n\nFor the prerequisites for building from source, please look at the [Requirements](REQUIREMENTS.md) file.\n\nA high level breakdown of the components of the framework:\n\n* The game framework is found in [MonoGame.Framework](MonoGame.Framework).\n* The content pipeline is located in [MonoGame.Framework.Content.Pipeline](MonoGame.Framework.Content.Pipeline).\n* Project templates are in [Templates](Templates).\n* See [Tests](Tests) for the framework unit tests.\n* See [Tools/Tests](Tools/MonoGame.Tools.Tests) for the content pipeline and other tool tests.\n* [mgcb](Tools/MonoGame.Content.Builder) is the command line tool for content processing.\n* [mgfxc](Tools/MonoGame.Effect.Compiler) is the command line effect compiler tool.\n* The [mgcb-editor](Tools/MonoGame.Content.Builder.Editor) tool is a GUI frontend for content processing.\n\n## Helpful Links\n\n* The official website is [monogame.net](http://www.monogame.net).\n* Our [issue tracker](https://github.com/MonoGame/MonoGame/issues) is on GitHub.\n* Use our [community forums](http://community.monogame.net/) for support questions.\n* You can [join the Discord server](https://discord.gg/monogame) and chat live with the core developers and other users.\n* The [official documentation](https://docs.monogame.net/articles/index.html) is on our website.\n* Download [release](https://github.com/MonoGame/MonoGame/releases) and [development](https://github.com/orgs/MonoGame/packages) packages.\n* Follow [@MonoGameTeam](https://twitter.com/monogameteam) on Twitter.\n\n## License\n\nThe MonoGame project is under the [Microsoft Public License](https://opensource.org/licenses/MS-PL) except for a few portions of the code. See the [LICENSE.txt](LICENSE.txt) file for more details. Third-party libraries used by MonoGame are under their own licenses. Please refer to those libraries for details on the license they use.\n",11131,graphics,C#,8,C#,XSLT,Smalltalk,Batchfile,HLSL,Rich Text Format,Shell,PowerShell,,,,,,,,,,,,,,,,,,,,,4572,979,3550,43,5,369,0,93873,2868,3628,2944,684,e3d9f77eeb011ced6d5638f8ec837b92a6f9a1b1,UWP Deprecation (#8407),2024-07-18T11:41:28Z,Simon (Darkside) Jackson,darkside@zenithmoon.com,SimonDarksideJ,MonoGame v3.8.1_HOTFIX,This is the official MonoGame v3.8.1 (hotfix) release source code and Visual Studio 2022 extensions for installing templates.\r\n\r\nThis release fixes some critical bugs found in the original 3.8.1 release. Please make sure to upgrade to 3.8.1.303.\r\n\r\nRead [our post](https://community.monogame.net/t/monogame-3-8-1/17786) for more details.,v3.8.1_HOTFIX,,,github-actions[bot],Other,MonoGame,MonoGame,19,xna,monogame,game-framework,gamedev,csharp,cross-platform,open-source,graphics,game-engine,dotnet,c-sharp,game-development,3d,,,,,,,,/MonoGame/MonoGame,19,471,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/mono/SkiaSharp.Extended,https://github.com/mono/SkiaSharp.Extended,0,,,0,0,0,0,0,0,1,1,0,0,0,"SkiaSharp is a cross-platform, comprehensive 2D graphics API for all .NET platforms. And, here is where you will find all sorts of extras that you can use with it.","# SkiaSharp.Extended\n\n[![Build Status](https://dev.azure.com/devdiv/DevDiv/_apis/build/status/Xamarin/Components/SkiaSharp.Extended?branchName=main)](https://dev.azure.com/devdiv/DevDiv/_build/latest?definitionId=10846&branchName=main)  [![Build Status](https://dev.azure.com/xamarin/public/_apis/build/status/mono/SkiaSharp/SkiaSharp.Extended%20(Public)?branchName=main)](https://dev.azure.com/xamarin/public/_build/latest?definitionId=6&branchName=main)\n\n**SkiaSharp.Extended** is a collection some cool libraries that may be\nuseful to some apps. There are several repositories that may have\ninteresting projects:\n\n - [SkiaSharp][skiasharp] _(the engine)_\n - [SkiaSharp.Extended][extended] _(additional APIs)_\n - [SkiaSharp.Extended.UI.Forms][ui-forms] _(additional Xamarin.Forms controls)_\n - [SkiaSharp.Extended.UI.Maui][ui-maui] _(additional .NET MAUI controls)_\n\n## Using\n\nThere is an early access feed that you can use to get the latest and greatest, before it goes out to the public:\n\n```\nhttps://aka.ms/skiasharp-eap/index.json\n```\n\n## Building\n\nTo build the projects and samples, just open `SkiaSharp.Extended.sln` \nin Visual Studio.\n\nThe CI server just runs `dotnet cake` and outputs all the packages,\nassemblies and test results. This can also be used to build everything\nlocally.\n\n## License\n\nThe code in this repository is licensed under the [MIT License][license].\n\n[license]: https://github.com/mono/SkiaSharp.Extended/blob/main/LICENSE\n[netcore]: https://www.microsoft.com/net/core\n\n[skiasharp]: https://github.com/mono/SkiaSharp\n[extended]: https://mono.github.io/SkiaSharp.Extended/api/extended\n[ui-forms]: https://mono.github.io/SkiaSharp.Extended/api/ui-forms\n[ui-maui]: https://mono.github.io/SkiaSharp.Extended/api/ui-maui\n",218,graphics,C#,1,C#,,,,,,,,,,,,,,,,,,,,,,,,,,,,163,63,98,2,7,26,95,8718,69,110,86,24,ba76f96a8bcf294c70cca6eab27cd6962925e319,Bump coverlet.collector from 6.0.0 to 6.0.2 (#278),2024-04-23T11:55:40Z,dependabot[bot],49699333+dependabot[bot]@users.noreply.github.com,dependabot[bot],Version 2.0,"## What's Changed\r\n* Development by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/52\r\n* Merging #44 into master by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/55\r\n* read gradientTransform Attribute by @inforithmics in https://github.com/mono/SkiaSharp.Extended/pull/54\r\n* Disable log compression in build.groovy by @akoeplinger in https://github.com/mono/SkiaSharp.Extended/pull/58\r\n* Fixup for PR #58 by @akoeplinger in https://github.com/mono/SkiaSharp.Extended/pull/59\r\n* [build] Added Azure DevOps & NuGet Signing by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/63\r\n* Update SkiaSharp + MaterialDesignIcons by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/64\r\n* Added tests to make sure the fill is not preserved by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/67\r\n* Adding some more code / tests for gradients and strokes by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/66\r\n* Dev by @leonardors in https://github.com/mono/SkiaSharp.Extended/pull/7\r\n* [SVG] Add support for `fill-rule=""evenodd""` by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/69\r\n* Switch to cake so that we can add more complex tasks by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/70\r\n* Use the signing template by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/71\r\n* Update Xamrin.Forms and Android Support by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/78\r\n* Link Code of Conduct by @terrajobst in https://github.com/mono/SkiaSharp.Extended/pull/90\r\n* Add files via upload by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/94\r\n* Add the data for the SKBlurHash demo by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/95\r\n* Re-work the projects and add the UI and Controls by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/97\r\n* Add a BlurHash Implementation by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/98\r\n* Add a confetti view by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/92\r\n* Add support for converting ImageSource to SKImage by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/99\r\n* Add the new docs website by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/104\r\n* Update NuGet packages by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/108\r\n* Modernize the entire repo again and add .NET MAUI by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/111\r\n* Added the MAUI sample by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/112\r\n* Split the base out of the confetti view by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/113\r\n* Update SkiaSharp to the latest of everything by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/114\r\n* Fix Unit Tests by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/116\r\n* A few enhancements to the base types by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/115\r\n* Several improvements and changes to SKGeometry by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/117\r\n* Make the build run in a single job by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/119\r\n* Update SkiaSharp by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/120\r\n* Move tests into a shared folder by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/121\r\n* Add a SKLottieView to play Lottie animations by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/118\r\n* Fixed typo in the SKLottieView Docs by @jfversluis in https://github.com/mono/SkiaSharp.Extended/pull/122\r\n* Update SkiaSharp by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/124\r\n* Do not load the animation directly by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/123\r\n* Reset properties before firing events by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/126\r\n* Rename IsRunning to IsAnimationEnabled on SKAnimatedSurfaceView by @bijington in https://github.com/mono/SkiaSharp.Extended/pull/125\r\n* Rename IsCompleted to IsRunning by @bijington in https://github.com/mono/SkiaSharp.Extended/pull/127\r\n* Revert ""Rename IsCompleted to IsRunning"" by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/129\r\n* Change default binding mode of readonly properties by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/130\r\n* Update SkiaSharp by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/131\r\n* Add an a image comparer by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/137\r\n* Update SkiaSharp to the stable versions by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/138\r\n* Enable CodeQL by @Redth in https://github.com/mono/SkiaSharp.Extended/pull/150\r\n* Enable TSA for CodeQL by @mdh1418 in https://github.com/mono/SkiaSharp.Extended/pull/152\r\n* Update Tools by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/186\r\n* Bump the skiasharp group with 9 updates by @dependabot in https://github.com/mono/SkiaSharp.Extended/pull/191\r\n* Add more docs for repeat mode by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/194\r\n* Bump Microsoft.SourceLink.GitHub from 1.0.0 to 1.1.1 by @dependabot in https://github.com/mono/SkiaSharp.Extended/pull/180\r\n* Fix the maui sample by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/197\r\n* Bump the xunit group with 2 updates by @dependabot in https://github.com/mono/SkiaSharp.Extended/pull/196\r\n* Support VS Code by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/199\r\n* Allow for a slightly delayed theme registration by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/200\r\n* Bump Svg.Skia from 0.5.0 to 1.0.0 by @dependabot in https://github.com/mono/SkiaSharp.Extended/pull/201\r\n* Bump Microsoft.NET.Test.Sdk from 16.5.0 to 17.7.1 by @dependabot in https://github.com/mono/SkiaSharp.Extended/pull/203\r\n* Make some lottie image source members public by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/198\r\n* Bump coverlet.collector from 3.0.2 to 6.0.0 by @dependabot in https://github.com/mono/SkiaSharp.Extended/pull/202\r\n* Update Dependencies and Android Target Version by @KannanKrish in https://github.com/mono/SkiaSharp.Extended/pull/204\r\n* Bump Svg.Skia from 1.0.0 to 1.0.0.1 by @dependabot in https://github.com/mono/SkiaSharp.Extended/pull/207\r\n* Bump Microsoft.NET.Test.Sdk from 17.7.1 to 17.7.2 by @dependabot in https://github.com/mono/SkiaSharp.Extended/pull/208\r\n* Add AnimationFinished event to SKLottieView by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/209\r\n* Bump the xunit group with 2 updates by @dependabot in https://github.com/mono/SkiaSharp.Extended/pull/210\r\n* Bump Svg.Skia from 1.0.0.1 to 1.0.0.2 by @dependabot in https://github.com/mono/SkiaSharp.Extended/pull/211\r\n* Bump the skiasharp group with 9 updates by @dependabot in https://github.com/mono/SkiaSharp.Extended/pull/227\r\n* Bump Svg.Skia from 1.0.0.2 to 1.0.0.3 by @dependabot in https://github.com/mono/SkiaSharp.Extended/pull/229\r\n* Bump the xunit group with 2 updates by @dependabot in https://github.com/mono/SkiaSharp.Extended/pull/231\r\n* Bump the xunit group with 1 update by @dependabot in https://github.com/mono/SkiaSharp.Extended/pull/233\r\n* Bump Microsoft.NET.Test.Sdk from 17.7.2 to 17.8.0 by @dependabot in https://github.com/mono/SkiaSharp.Extended/pull/235\r\n* Migrate to 1ES PT by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/258\r\n* Bump cake.tool from 3.1.0 to 4.0.0 by @dependabot in https://github.com/mono/SkiaSharp.Extended/pull/242\r\n* Bump Microsoft.SourceLink.GitHub from 1.1.1 to 8.0.0 by @dependabot in https://github.com/mono/SkiaSharp.Extended/pull/239\r\n* Increase APIScan timeout by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/260\r\n* Package Updates by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/259\r\n* Fix APIScan by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/265\r\n* Update to .NET 8 for the tests and sample by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/266\r\n* Added more details in the event args by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/268\r\n* Avoid memory leaks in the timers by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/267\r\n* Apply styles to derived types by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/270\r\n* Fix a crash when closing the app when running by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/269\r\n* Add a README and Icon to the nugets by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/271\r\n* Update Svg.Skia by @mattleibow in https://github.com/mono/SkiaSharp.Extended/pull/272\r\n\r\n## New Contributors\r\n* @inforithmics made their first contribution in https://github.com/mono/SkiaSharp.Extended/pull/54\r\n* @akoeplinger made their first contribution in https://github.com/mono/SkiaSharp.Extended/pull/58\r\n* @terrajobst made their first contribution in https://github.com/mono/SkiaSharp.Extended/pull/90\r\n* @jfversluis made their first contribution in https://github.com/mono/SkiaSharp.Extended/pull/122\r\n* @bijington made their first contribution in https://github.com/mono/SkiaSharp.Extended/pull/125\r\n* @Redth made their first contribution in https://github.com/mono/SkiaSharp.Extended/pull/150\r\n* @mdh1418 made their first contribution in https://github.com/mono/SkiaSharp.Extended/pull/152\r\n* @dependabot made their first contribution in https://github.com/mono/SkiaSharp.Extended/pull/191\r\n* @KannanKrish made their first contribution in https://github.com/mono/SkiaSharp.Extended/pull/204\r\n\r\n**Full Changelog**: https://github.com/mono/SkiaSharp.Extended/compare/v1.60.0...v2.0.0",v2.0.0,Matthew Leibowitz,,mattleibow,MIT License,SkiaSharp.Extended,mono,15,xamarin,graphics,dot-net,ios,android,windows,macos,cross-platform,skia,skiasharp,,,,,,,,,,,/mono/SkiaSharp.Extended,15,31,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/mono/SkiaSharp,https://github.com/mono/SkiaSharp,0,,,0,0,0,0,0,0,1,1,0,0,0,"SkiaSharp is a cross-platform 2D graphics API for .NET platforms based on Google's Skia Graphics Library. It provides a comprehensive 2D API that can be used across mobile, server and desktop models to render images.","# SkiaSharp\n\n[![SkiaSharp](https://img.shields.io/nuget/vpre/SkiaSharp.svg?cacheSeconds=3600&label=SkiaSharp%20nuget)](https://www.nuget.org/packages/SkiaSharp)\n[![HarfBuzzSharp](https://img.shields.io/nuget/vpre/HarfBuzzSharp.svg?cacheSeconds=3600&label=HarfBuzzSharp%20nuget)](https://www.nuget.org/packages/HarfBuzzSharp)\n\n[![SkiaSharp.Views](https://img.shields.io/nuget/vpre/SkiaSharp.Views.svg?cacheSeconds=3600&label=SkiaSharp.Views%20nuget)](https://www.nuget.org/packages/SkiaSharp.Views)\n[![SkiaSharp.Views.Maui.Controls](https://img.shields.io/nuget/vpre/SkiaSharp.Views.Maui.Controls.svg?cacheSeconds=3600&label=SkiaSharp.Views.Maui.Controls%20nuget)](https://www.nuget.org/packages/SkiaSharp.Views.Maui.Controls)\n[![SkiaSharp.Views.Uno](https://img.shields.io/nuget/vpre/SkiaSharp.Views.Uno.svg?cacheSeconds=3600&label=SkiaSharp.Views.Uno%20nuget)](https://www.nuget.org/packages/SkiaSharp.Views.Uno) \n\n[![discord](https://img.shields.io/badge/chat-.NET%20Discord-E60256.svg)](https://aka.ms/dotnet-discord)\n[![SkiaSharp API Docs](https://img.shields.io/badge/docs-skiasharp-1faece.svg)](https://docs.microsoft.com/dotnet/api/SkiaSharp)\n[![HarfBuzzSharp API Docs](https://img.shields.io/badge/docs-harfbuzzsharp-1faece.svg)](https://docs.microsoft.com/dotnet/api/SkiaSharp)\n[![SkiaSharp Guides](https://img.shields.io/badge/docs-guides-1faece.svg)](https://docs.microsoft.com/xamarin/graphics-games/skiasharp/)\n\n[![Build Status](https://dev.azure.com/devdiv/DevDiv/_apis/build/status/Xamarin/Components/SkiaSharp?branchName=main)](https://dev.azure.com/devdiv/DevDiv/_build/latest?definitionId=10789&branchName=main)\n[![Build Status](https://dev.azure.com/xamarin/public/_apis/build/status/mono/SkiaSharp/SkiaSharp%20(Public)?branchName=main)](https://dev.azure.com/xamarin/public/_build/latest?definitionId=4&branchName=main)\n\nSkiaSharp is a cross-platform 2D graphics API for .NET platforms based on Google's\nSkia Graphics Library ([skia.org](https://skia.org/)). It provides a comprehensive 2D API that can\nbe used across mobile, server and desktop models to render images.\n\nSkiaSharp provides cross-platform bindings for:\n\n - .NET Standard 1.3\n - .NET Core\n - .NET 6\n - Tizen\n - Android\n - iOS\n - tvOS\n - macOS\n - Mac Catalyst\n - WinUI 3 (Windows App SDK / Uno Platform)\n - Windows Classic Desktop (Windows.Forms / WPF)\n - Web Assembly (WASM)\n - Uno Platform (iOS / macOS / Android / WebAssembly)\n\nThe [API Documentation](https://docs.microsoft.com/en-us/dotnet/api/SkiaSharp/) is\navailable on the web to browse.\n\n## Using SkiaSharp\n\nSkiaSharp is available as a convenient NuGet package, to use install the package like this:\n\n```\nnuget install SkiaSharp\n```\n\n_Because there are multiple distros of Linux, and we cannot possibly support them all, we have a separate NuGet package that will contain the supported binaries for a few distros: [SkiaSharp.NativeAssets.Linux](https://www.nuget.org/packages/SkiaSharp.NativeAssets.Linux). ([distros](https://github.com/mono/SkiaSharp/issues/453)) ([more info](https://github.com/mono/SkiaSharp/issues/312))_\n\nThere is also a early access feed that you can use to get the latest and greatest, before it goes out to the public:\n\n```\nhttps://aka.ms/skiasharp-eap/index.json\n```\n\n## Building SkiaSharp\n\nBuilding SkiaSharp is mostly straight forward. The main issue is the multiple dependencies for each platform.\n\nHowever, these are easy to install as they are found on the various websites. If you are just working on managed code, it is even easier as there mays to skip all the native builds.\n\n - To get started building, [go here](https://github.com/mono/SkiaSharp/wiki/Building-SkiaSharp).\n - If you are just wanting a custom Linux build, [go here](https://github.com/mono/SkiaSharp/wiki/Building-on-Linux)\n\n## Compare Code\n\nHere are some links to show the differences in our code as compared to Google's code.\n\nWhat version are we on? [**m116**](https://github.com/google/skia/tree/chrome/m116)  \nAre we up-to-date with Google? [Compare](https://github.com/mono/skia/compare/skiasharp...google:chrome/m116)  \nWhat have we added? [Compare](https://github.com/google/skia/compare/chrome/m116...mono:skiasharp)  \n",4271,graphics,C#,12,C#,PowerShell,Shell,Makefile,C,Dockerfile,HTML,JavaScript,TypeScript,Python,C++,Batchfile,,,,,,,,,,,,,,,,,980,182,776,22,168,105,0,54491,530,1763,1191,572,e3ab2e458011dbd8fffece03e40070b39a2e2d94,Build ANGLE separately (#2950),2024-07-19T17:38:02Z,Matthew Leibowitz,mattleibow@live.com,mattleibow,Version 2.88.9 (Preview 1),## What's Changed\r\n* Work around annoying fake warning in the IDE by @mattleibow in https://github.com/mono/SkiaSharp/pull/2844\r\n* Use new license expressions by @mattleibow in https://github.com/mono/SkiaSharp/pull/2877\r\n* [release/2.x] fix: XamlRoot may be null when the SKXamlCanvas is unloaded by @github-actions in https://github.com/mono/SkiaSharp/pull/2884\r\n* [release/2.x] Fix the GetKerningPairAdjustments API. by @github-actions in https://github.com/mono/SkiaSharp/pull/2886\r\n* [release/2.x] Fix SKXamlCanvas on Uno Skia to use Bgra8888 by @github-actions in https://github.com/mono/SkiaSharp/pull/2919\r\n* [release/2.x] Authenticate Docker by @mattleibow in https://github.com/mono/SkiaSharp/pull/2929\r\n* Update the signing template by @mattleibow in https://github.com/mono/SkiaSharp/pull/2932\r\n\r\n\r\n**Full Changelog**: https://github.com/mono/SkiaSharp/compare/v2.88.8...v2.88.9-preview.1.1,v2.88.9-preview.1.1,Matthew Leibowitz,,mattleibow,MIT License,SkiaSharp,mono,126,xamarin,graphics,dot-net,ios,android,windows,macos,cross-platform,skia,dotnet,skiasharp,hacktoberfest,,,,,,,,,/mono/SkiaSharp,129,124,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/MolSSI/QCEngine,https://github.com/MolSSI/QCEngine,0,,,0,1,0,0,0,0,1,0,0,0,0,Quantum chemistry program executor and IO standardizer (QCSchema).,"QCEngine\n========\n[![Build Status](https://github.com/MolSSI/QCEngine/workflows/CI/badge.svg?branch=master)](https://github.com/MolSSI/QCEngine/actions?query=workflow%3ACI)\n[![codecov](https://img.shields.io/codecov/c/github/MolSSI/QCEngine.svg?logo=Codecov&logoColor=white)](https://codecov.io/gh/MolSSI/QCEngine)\n[![Documentation Status](https://img.shields.io/github/actions/workflow/status/MolSSI/QCEngine/CI.yml?label=docs&logo=readthedocs&logoColor=white)](https://molssi.github.io/QCEngine/)\n[![Conda (channel only)](https://img.shields.io/conda/vn/conda-forge/qcengine?color=blue&logo=anaconda&logoColor=white)](https://anaconda.org/conda-forge/qcengine)\n[![Chat on Slack](https://img.shields.io/badge/chat-on_slack-808493.svg?longCache=true&style=flat&logo=slack)](https://join.slack.com/t/qcarchive/shared_invite/enQtNDIzNTQ2OTExODk0LTE3MWI0YzBjNzVhNzczNDM0ZTA5MmQ1ODcxYTc0YTA1ZDQ2MTk1NDhlMjhjMmQ0YWYwOGMzYzJkZTM2NDlmOGM)\n![python](https://img.shields.io/badge/python-3.7+-blue.svg)\n\n<!--[![Azure Build Status](https://dev.azure.com/MolSSI/QCArchive/_apis/build/status/MolSSI.QCEngine?branchName=master)](https://dev.azure.com/MolSSI/QCArchive/_build/latest?definitionId=5&branchName=master)-->\nQuantum chemistry program executor and IO standardizer ([QCSchema](https://github.com/MolSSI/QCSchema)) for quantum chemistry.\n\n# Example\n\nA simple example of QCEngine's capabilities is as follows:\n\n```python\n>>> import qcengine as qcng\n>>> import qcelemental as qcel\n\n>>> mol = qcel.models.Molecule.from_data(""""""\nO  0.0  0.000  -0.129\nH  0.0 -1.494  1.027\nH  0.0  1.494  1.027\n"""""")\n\n>>> inp = qcel.models.AtomicInput(\n    molecule=mol,\n    driver=""energy"",\n    model={""method"": ""SCF"", ""basis"": ""sto-3g""},\n    keywords={""scf_type"": ""df""}\n    )\n```\n\nThese input specifications can be executed with the ``compute`` function along with a program specifier:\n\n```python\n>>> ret = qcng.compute(inp, ""psi4"")\n```\n\nThe results contain a complete record of the computation:\n\n\n```python\n>>> ret.return_result\n-74.45994963230625\n\n>>> ret.properties.scf_dipole_moment\n[0.0, 0.0, 0.6635967188869244]\n\n>>> ret.provenance.cpu\nIntel(R) Core(TM) i7-7820HQ CPU @ 2.90GHz\n```\n\nSee the [documentation](https://molssi.github.io/QCEngine/) for more information.\n\n# License\n\nBSD-3C. See the [License File](LICENSE) for more information.\n",162,chemistry,Python,3,Python,Shell,Makefile,,,,,,,,,,,,,,,,,,,,,,,,,,338,35,284,19,5,51,76,11124,78,111,71,40,3b9ed2aee662424df6be12d9e7e23f51b9c6b6eb,test CI with changelog alone (#449),2024-06-25T20:44:59Z,Lori A. Burns,lori.burns@gmail.com,loriab,v0.30.0,https://github.com/MolSSI/QCEngine/blob/master/docs/source/changelog.rst#v0300--2024-06-25,v0.30.0,Lori A. Burns,,loriab,"BSD 3-Clause ""New"" or ""Revised"" License",QCEngine,MolSSI,43,quantum-chemistry,python3,computational-chemistry,standards,chemistry,,,,,,,,,,,,,,,,/MolSSI/QCEngine,48,15,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/MolSSI/QCElemental,https://github.com/MolSSI/QCElemental,0,,,0,0,0,0,0,0,1,0,0,0,0,"Periodic table, physical constants, and molecule parsing for quantum chemistry.","# QCElemental\n\n[![Build Status](https://github.com/MolSSI/QCElemental/workflows/CI/badge.svg?branch=master)](https://github.com/MolSSI/QCElemental/actions?query=workflow%3ACI)\n[![codecov](https://img.shields.io/codecov/c/github/MolSSI/QCElemental.svg?logo=Codecov&logoColor=white)](https://codecov.io/gh/MolSSI/QCElemental)\n[![Documentation Status](https://img.shields.io/github/actions/workflow/status/MolSSI/QCElemental/CI.yaml?label=docs&logo=readthedocs&logoColor=white)](https://molssi.github.io/QCElemental/)\n[![Chat on Slack](https://img.shields.io/badge/chat-on_slack-green.svg?longCache=true&style=flat&logo=slack)](https://join.slack.com/t/qcarchive/shared_invite/enQtNDIzNTQ2OTExODk0LTE3MWI0YzBjNzVhNzczNDM0ZTA5MmQ1ODcxYTc0YTA1ZDQ2MTk1NDhlMjhjMmQ0YWYwOGMzYzJkZTM2NDlmOGM)\n![python](https://img.shields.io/badge/python-3.7+-blue.svg)\n\n**Documentation:** [GitHub Pages](https://molssi.github.io/QCElemental/)\n\nCore data structures for Quantum Chemistry. QCElemental also contains physical constants and periodic table data from NIST and molecule handlers.\n\nPeriodic Table and Physical Constants data are pulled from NIST srd144 and srd121, respectively ([details](raw_data/README.md)) in a renewable manner (class around NIST-published JSON file).\n\nThis project also contains a generator, validator, and translator for [Molecule QCSchema](https://molssi-qc-schema.readthedocs.io/en/latest/auto_topology.html).\n\n## ✨ Getting Started\n\n- Installation. QCElemental supports Python 3.7+.\n\n  ```sh\n  python -m pip install qcelemental\n  ```\n\n- To install QCElemental with molecule visualization capabilities (useful in iPython or Jupyter notebook environments):\n\n  ```sh\n  python -m pip install 'qcelemental[viz]`\n  ```\n\n- To install QCElemental with various alignment capabilities using `networkx`\n\n  ```sh\n  python -m pip install 'qcelemental[align]`\n  ```\n\n- Or install both:\n\n  ```sh\n  python -m pip install 'qcelemental[viz,align]`\n  ```\n\n- See [documentation](https://molssi.github.io/QCElemental/)\n\n### Periodic Table\n\nA variety of periodic table quantities are available using virtually any alias:\n\n```python\n>>> import qcelemental as qcel\n>>> qcel.periodictable.to_E('KRYPTON')\n'Kr'\n>>> qcel.periodictable.to_element(36)\n'Krypton'\n>>> qcel.periodictable.to_Z('kr84')\n36\n>>> qcel.periodictable.to_A('Kr')\n84\n>>> qcel.periodictable.to_A('D')\n2\n>>> qcel.periodictable.to_mass('kr', return_decimal=True)\nDecimal('83.9114977282')\n>>> qcel.periodictable.to_mass('kr84')\n83.9114977282\n>>> qcel.periodictable.to_mass('Kr86')\n85.9106106269\n```\n\n### Physical Constants\n\nPhysical constants can be acquired directly from the [NIST CODATA](https://physics.nist.gov/cuu/Constants/Table/allascii.txt):\n\n```python\n>>> import qcelemental as qcel\n>>> qcel.constants.Hartree_energy_in_eV\n27.21138602\n>>> qcel.constants.get('hartree ENERGY in ev')\n27.21138602\n>>> pc = qcel.constants.get('hartree ENERGY in ev', return_tuple=True)\n>>> pc.label\n'Hartree energy in eV'\n>>> pc.data\nDecimal('27.21138602')\n>>> pc.units\n'eV'\n>>> pc.comment\n'uncertainty=0.000 000 17'\n```\n\nAlternatively, with the use of the [Pint unit conversion package](https://pint.readthedocs.io/en/latest/), arbitrary\nconversion factors can be obtained:\n\n```python\n>>> qcel.constants.conversion_factor(""bohr"", ""miles"")\n3.2881547429884475e-14\n```\n\n### Covalent Radii\n\nCovalent radii are accessible for most of the periodic table from [Alvarez, Dalton Transactions (2008) doi:10.1039/b801115j](https://doi.org/10.1039/b801115j) ([details](qcelemental/data/alvarez_2008_covalent_radii.py.py)).\n\n```python\n>>> import qcelemental as qcel\n>>> qcel.covalentradii.get('I')\n2.626719314386381\n>>> qcel.covalentradii.get('I', units='angstrom')\n1.39\n>>> qcel.covalentradii.get(116)\nTraceback (most recent call last):\n...\nqcelemental.exceptions.DataUnavailableError: ('covalent radius', 'Lv')\n>>> qcel.covalentradii.get(116, missing=4.0)\n4.0\n>>> qcel.covalentradii.get('iodine', return_tuple=True).dict()\n{'numeric': True, 'label': 'I', 'units': 'angstrom', 'data': Decimal('1.39'), 'comment': 'e.s.d.=3 n=451', 'doi': 'DOI: 10.1039/b801115j'}\n```\n\n### van der Waals Radii\n\nVan der Waals radii are accessible for most of the periodic table from [Mantina, J. Phys. Chem. A (2009) doi: 10.1021/jp8111556](https://pubs.acs.org/doi/10.1021/jp8111556) ([details](qcelemental/data/mantina_2009_vanderwaals_radii.py)).\n\n```python\n>>> import qcelemental as qcel\n>>> qcel.vdwradii.get('I')\n3.7416577284064996\n>>> qcel.vdwradii.get('I', units='angstrom')\n1.98\n>>> qcel.vdwradii.get(116)\nTraceback (most recent call last):\n...\nqcelemental.exceptions.DataUnavailableError: ('vanderwaals radius', 'Lv')\n>>> qcel.vdwradii.get('iodine', return_tuple=True).dict()\n{'numeric': True, 'label': 'I', 'units': 'angstrom', 'data': Decimal('1.98'), 'doi': 'DOI: 10.1021/jp8111556'}\n```\n",135,chemistry,Python,2,Python,Shell,,,,,,,,,,,,,,,,,,,,,,,,,,,263,15,231,17,12,41,45,29246,70,78,46,32,b4c3bd369cb03f7637e7de06e38e8cf7db35a0b4,update version,2024-06-21T13:25:27Z,Lori A. Burns,lori.burns@gmail.com,loriab,v0.28.0,https://github.com/MolSSI/QCElemental/blob/master/docs/changelog.rst#0280--2024-06-21,v0.28.0,Lori A. Burns,,loriab,"BSD 3-Clause ""New"" or ""Revised"" License",QCElemental,MolSSI,48,python,chemistry,physical-constant,periodic-table,standards,nist,codata,computational-chemistry,quantum-chemistry,,,,,,,,,,,,/MolSSI/QCElemental,50,11,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/MolecularAI/REINVENT4,https://github.com/MolecularAI/REINVENT4,0,aI based,1,1,1,1,1,0,0,0,0,0,0,1,"AI molecular design tool for de novo design, scaffold hopping, R-group replacement, linker design and molecule optimization.","REINVENT 4\n==========\n\n\nDescription\n-----------\n\nREINVENT is a molecular design tool for de novo design, scaffold hopping,\nR-group replacement, linker design, molecule optimization, and other small\nmolecule design tasks.  REINVENT uses a Reinforcement Learning\n(RL) algorithm to generate optimized molecules compliant with a user defined\nproperty profile defined as a multi-component score.  Transfer Learning (TL)\ncan be used to create or pre-train a model that generates molecules closer\nto a set of input molecules. \n\nA paper describing the software has been published as Open Access in the\nJournal of Cheminformatics:\n[Reinvent 4: Modern AI–driven generative molecule design](https://link.springer.com/article/10.1186/s13321-024-00812-5?utm_source=rct_congratemailt&utm_medium=email&utm_campaign=oa_20240221&utm_content=10.1186/s13321-024-00812-5).\nSee AUTHORS.md for references to previous papers.\n\n\nRequirements\n------------\n\nREINVENT is being developed on Linux and supports both GPU and CPU.  The Linux\nversion is fully validated.  REINVENT on Windows and MacOSX supports\nboth GPU and CPU but is only partially tested on these platforms and therefore\nsupport is limited.\n\nThe code is written in Python 3 (>= 3.10).  The list of\ndependencies can be found in the repository (see also Installation below).\n\nA GPU is not strictly necessary but strongly recommended for performance\nreasons especially for transfer learning and model training.  Reinforcement\nlearning (RL) requires the computation of scores where most scoring\ncomponents run on the CPU.  Thus, a GPU is less important for RL (depending\non how much time is spent on the CPU).\n\nNote that if no GPU is installed in your computer the code will run on the\nCPU automatically.  REINVENT [supports](https://pytorch.org/get-started/locally/) NVIDIA GPUs and also some AMD GPUs.\nFor most design tasks a memory of about 8 GiB for both CPU main memory and\nGPU memory is sufficient.\n\n\nInstallation\n------------\n\n1. Clone this Git repository.\n1. Install a compatible version of Python, for example with [Conda](https://conda.io/projects/conda/en/latest/index.html) (other virtual environments like Docker, pyenv, or the system package manager work too).\n    ```shell\n    conda create --name reinvent4 python=3.10\n    conda activate reinvent4\n    ```\n1. Change directory to the repository and install the dependencies from the lockfile:\n    ```shell\n    pip install -r requirements-linux-64.lock\n    ```\n   1. _Optional_: if you want to use **AMD GPUs** on Linux you would need to install the [ROCm PyTorch version](https://pytorch.org/get-started/locally/) manually _after_ installation of the dependencies in point 3, e.g.\n        ```shell\n        pip install torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/rocm5.7\n        ```\n   1. _Optional_: use requirements file `requirements-macOS.lock` for MacOSX. \n1. Install the tool. The dependencies were already installed in the previous step, so there is no need to install them again (flag `--no-deps).  If you want to install in editable mode (changes to the code are automatically available), add -e before the dot.\n    ```shell\n    pip install --no-deps .\n    ```\n1. Test the tool. The installer has added a script `reinvent` to your PATH.\n    ```shell\n    reinvent --help\n    ```\n\nBasic Usage\n-----------\n\nREINVENT is a command line tool and works principally as follows\n```shell\nreinvent -l sampling.log sampling.toml\n```\n\nThis writes logging information to the file `sampling.log`.  If you wish to write\nthis to the screen, leave out the `-l sampling.log` part. `sampling.toml` is the\nconfiguration file.  The main user format is [TOML](https://toml.io/en/) as it tends to be more\nuse friendly.  JSON can be used too, add `-f json`, but a specialised editor is\nrecommended as the format is very sensitive to minor changes.\n\nSample configuration files for all run modes are\nlocated in `config/toml` in the repository and file paths in these files would need to be\nadjusted to your local installation.  In particular, ready made prior models are\nlocated in `priors` and you would choose a model and the\nappropriate run mode depending on the research problem you are trying to address.\nThere is additional information in `config/toml` in several `*.md` files with\ninstructions on how to configure the TOML file.  Internal priors can be referenced with a\ndot notation (see `reinvent/prior_registry.py`).\n\n\nTutorials / `Jupyter` notebooks\n-------------------------------\n\nBasic instructions can be found in the comments in the config examples in `config/toml`.\n\nNotebooks are provided in the `notebook/` directory.  Please note that we\nprovide the notebooks in jupytext ""light script"" format.  To work with the light\nscripts you will need to install jupytext.  A few other packages will come in handy too.\n\n```shell\npip install jupytext mols2grid seaborn\n```\n\nThe Python files in `notebook/` can then be converted to a notebook e.g.\n\n```shell\njupytext -o Reinvent_demo.ipynb Reinvent_demo.py\n```\n\n\nUpdating dependencies\n---------------------\n\nUpdate the lock files with [pip-tools](https://pypi.org/project/pip-tools/) (please, do not edit the files manually):\n```shell\npip-compile --extra-index-url=https://download.pytorch.org/whl/cu121 --extra-index-url=https://pypi.anaconda.org/OpenEye/simple --resolver=backtracking pyproject.toml\n```\nTo update a single package, use `pip-compile --upgrade-package somepackage`\n(see the documentation for pip-tools).\n\n\nScoring Plugins\n---------------\n\nThe scoring subsystem uses a simple plugin mechanism (Python\n[native namespace packages](https://packaging.python.org/en/latest/guides/packaging-namespace-packages/#native-namespace-packages)).  If you\nwish to write your own plugin, follow the instructions below.\nThere is no need to touch any of the REINVENT code. The public\nrepository contains a [contrib](https://github.com/MolecularAI/REINVENT4/tree/main/contrib/reinvent_plugins/components) directory with some useful examples.\n\n1. Create `/top/dir/somewhere/reinvent\_plugins/components` where `/top/dir/somewhere` is a convenient location for you.\n1. Do **not** place a `__init__.py` in either `reinvent_plugins` or `components` as this would break the mechanism.  It is fine to create normal packages within `components` as long as you import those correctly.\n1. Place a file whose name starts with `comp_*` into `reinvent_plugins/components`.   Files with different names will be ignored i.e. not imported. The directory will be searched recursively so structure your code as needed but directory/package names must be unique.\n1. Tag the scoring component class(es) in that file with the @add\_tag decorator.  More than one component class can be added to the same *comp\_* file. See existing code.\n1. Tag at most one dataclass for parameters in the same file, see existing code.  This is optional.\n1. Set or add `/top/dir/somewhere` to the `PYTHONPATH` environment variable or use any other mechanism to extend `sys.path`.\n1. The scoring component should now automatically be picked up by REINVENT.\n\n\nUnit and Integration Tests \n--------------------------\n\nThis is primarily for developers and admins/users who wish to ensure that the\ninstallation works.  The information here is not relevant to the\npractical use of REINVENT.  Please refer to _Basic Usage_ for instructions on\nhow to use the `reinvent` command.\n\nThe REINVENT project uses the `pytest` framework for its tests.  Before you run\nthem you first have to create a configuration file for the tests.\n\nIn the project directory, create a `config.json` file in the `configs/` directory.\nYou can use the example config `example.config.json` as a base.  Make sure that\nyou set `MAIN_TEST_PATH` to a non-existent directory.  That is where temporary\nfiles will be written during the tests.  If it is set to an existing directory,\nthat directory will be removed once the tests have finished.\n\nSome tests require a proprietary OpenEye license.  You have to set up a few\nthings to make the tests read your license.  The simple way is to just set the\n`OE_LICENSE` environment variable to the path of the file containing the\nlicense.  \n\nOnce you have a configuration and your license can be read, you can run the tests.\n\n```\n$ pytest tests --json /path/to/config.json --device cuda\n```\n",272,chemistry,Python,2,Python,Shell,,,,,,,,,,,,,,,,,,,,,,,,,,,12,2,10,0,1,3,0,1334195,64,79,74,5,ca28563a5e13a5329518539b3192b5d5a9f2120a,fixed config error,2024-07-12T12:40:33Z,"Löffler, Hannes",hannes.loffler@astrazeneca.com,,Release 4.4,New in REINVENT 4.4\r\n===================\r\n\r\nFor details see CHANGELOG.md.\r\n\r\n* Transformer based Libinvent\r\n* Prior registry to load internal priors more easily\r\n* Strict validation of input configuration to ensure consistency\r\n* Better JSON configuration file writing\r\n* Metadata writing for all created RL and TL models\r\n* Import functionality for scoring runmode\r\n* Stages in staged learning can have their own diversity filters\r\n* More memory efficient transformer models to handle larger numbers of input SMILES\r\n* Additional (fragment) SMILES written to staged learning CSV\r\n* TanimotoDistance renamed to TanimotoSimilarity\r\n* Support for ChemProp multitask models: requires param.target\_column\r\n* Allow dot SMILES fragment separator for Lib/Linkinvent input\r\n* Optional [scheduler] section for TL\r\n* Example support script for RAScore\r\n* A more complete RL/TL demo notebook\r\n* Experimental data pipeline to preprocess SMILES for prior creation\r\n* Various code improvements and fixes,v4.4.22,Hannes Loeffler,,halx,Apache License 2.0,REINVENT4,MolecularAI,5,ai,cheminformatics,chemistry,denovo-design,generative-ai,ml,molecule-generation,astrazeneca,deep-learning,neural-networks,reinforcement-learning,transfer-learning,drug-design,drug-discovery,,,,,,,/MolecularAI/REINVENT4,8,21,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/mojs/mojs,https://github.com/mojs/mojs,0,,,0,0,0,0,0,0,1,1,0,0,0,The motion graphics toolbelt for the web,"# mo · js – [![npm](https://img.shields.io/npm/v/@mojs/core.svg)](https://www.npmjs.com/package/@mojs/core) [![ci](https://img.shields.io/github/actions/workflow/status/mojs/mojs/ci.yml?branch=main)](https://github.com/mojs/mojs/actions?query=workflow:""CI"") [![Coverage Status](https://coveralls.io/repos/mojs/mojs/badge.svg?branch=main)](https://coveralls.io/r/mojs/mojs?branch=main) [![Slack](https://img.shields.io/badge/style-join-ff69b4.svg?label=slack)](https://join.slack.com/t/mojs/shared_invite/zt-dlyxhupt-VR7eV2uwCnvu3Cx~Yc_L9Q)\n\nThe motion graphics toolbelt for the web.\n\n[![mo · js](logo.svg ""mo · js"")](https://mojs.github.io/)\n\n## Intro\n**mo · js** is a javascript motion graphics library that is a **fast, retina ready, modular and open source**. In comparison to other libraries, it has a different syntax and code animation structure approach. The declarative API provides you a complete control over the animation, making it customizable with ease.\n\nThe library provides **built-in components** to start animating from scratch like html, shape, swirl, burst and stagger, but also bring you tools to help craft your animation in a most natural way. Using mojs on your site will enhance the user experience, enrich your content visually and create **delightful animations** precisely.\n\n## Install\n### Use with a bundler\n\nMojs is published on the **NPM registry**, so you can install it through the command line interpreter using your favorite package manager. This is the best way to install the library if you are comfortable with **javascript bundlers** like `webpack` or `rollup`.\n\n```sh\n# npm\nnpm install @mojs/core\n\n# yarn\nyarn add @mojs/core\n```\n\nThen **import it like any other module** inside your build:\n\n```js\nimport mojs from '@mojs/core';\n\nnew mojs.Html({\n  // ...\n});\n```\n\n> Using a bundler has **many advantages** like output compression, code splitting, tree shaking, etc., so we encourage you to use this kind of tool with mojs.\n\n### Use with a CDN\n\nTo rapidly **include the minified production file** in your web page, load the latest build from your favorite CDN using a generic script markup:\n\n```html\n<!-- unpkg -->\n<script src=""https://unpkg.com/@mojs/core""></script>\n\n<!-- jsdelivr -->\n<script src=""https://cdn.jsdelivr.net/npm/@mojs/core""></script>\n```\n\nThen instanciate using:\n\n```html\n<script>\n  new mojs.Html({\n    // ...\n  });\n</script>\n```\n\n> By default, if no one is specified, the CDN will automatically target the **@latest** version of mojs and load the **UMD build** from `dist/mo.umd.js`.\n\n## User guide\nThe base documentation you need to get started with mojs.\n- [Get started](https://mojs.github.io/tutorials/getting-started.html)\n- [@mojs/player](https://github.com/mojs/mojs-player/)\n- [@mojs/curve-editor](https://github.com/mojs/mojs-curve-editor/)\n- [@mojs/timeline-editor](https://github.com/mojs/mojs-timeline-editor/)\n\n## Learn\nDiscover the amazing things that mojs can do!\n- [Shape & Swirl](https://mojs.github.io/tutorials/shape-swirl/) _(Tutorial)_\n- [Burst](https://mojs.github.io/tutorials/burst/) _(Tutorial)_\n- [Icon animations powered by mo.js](https://tympanus.net/codrops/2016/02/23/icon-animations-powered-by-mo-js/) _(Codrops tutorial)_\n- [An Introduction to mo.js](https://css-tricks.com/introduction-mo-js/) _(CSS tricks tutorial)_\n- [Playing with @mojs/player and @mojs/curve-editor](https://vimeo.com/185587462) _(Vimeo video)_\n- [Web animations and mo.js](https://www.youtube.com/watch?v=yRxWa8lXasI) _(Youtube video)_\n\n## Developer\nGet technical informations, open an issue/pull request or join the (amazing) community!\n- [API documentation](https://mojs.github.io/api/)\n- [Github](https://github.com/mojs/mojs/)\n- [Slack workspace](https://mojs.slack.com) _(Not in the workspace yet? Use the [invite link](https://join.slack.com/t/mojs/shared_invite/zt-dlyxhupt-VR7eV2uwCnvu3Cx~Yc_L9Q) 🔓)_\n\n## Showcase\n- [Motion Graphics for the Web](https://codepen.io/sol0mka/full/ogOYJj/)\n- [Bubble Layout](https://codepen.io/sol0mka/full/yNOage/)\n- [Sleepy Mole](https://codepen.io/sol0mka/full/OyzBXR/)\n- [Animocons](https://tympanus.net/Development/Animocons/)\n- [Love or Hate Modal](https://codepen.io/sol0mka/full/812699ce32c9a7aeb70c9384b32a533a/)\n- [Mograph](https://codepen.io/sol0mka/full/39427561a8a0b15d7896480a7d96d3d1/)\n- [Word Reveal](https://codepen.io/sol0mka/full/c94452fb65dbf676b0ae8a12d4267473/)\n- [Jump and Squash](https://codepen.io/sol0mka/full/pEagoL/)\n- [Physical Balls](https://codepen.io/sol0mka/full/7315f4364360ec87a6655d33782702fe/)\n- [Dust Trail](https://codepen.io/sol0mka/full/633e6aa52d40691cca2f2cda91650bae/)\n- [Bubble Modal](https://codepen.io/sol0mka/full/3c49de2d7d0ca3e92bf5db5bf7a2687d/)\n- [Bubbles](https://codepen.io/sol0mka/full/2ef10ed42ff535182c31cd1dbb81e453/)\n- [Blast](https://codepen.io/sol0mka/full/699cfc8716a13e0e1c15105af2b6fb95/) _(click to see)_\n- [Simple Burst](https://codepen.io/sol0mka/full/6caf96461207a5caa9226fbd2631569d/) _(click to see)_\n- [Dusty Burst](https://codepen.io/sol0mka/full/03e9d8f2fbf886aa1505c61c81d782a0/) _(click to see)_\n- [Twitter Fav](https://codepen.io/sol0mka/full/wWdRLk/) _(click to see)_\n- [Twitter Fav (stars)](https://codepen.io/sol0mka/full/PzmAym/) _(click to see)_\n- [Twitter Fav Firework](https://codepen.io/sol0mka/full/xOAKKA/) _(click to see)_\n- [Simple Ripple](https://codepen.io/sol0mka/full/XKdWJg/) _(click to see)_\n\n## Browser support\n- Chrome 49+\n- Firefox 70+\n- Opera 36+\n- Safari 8+\n- Edge 79+\n\n> Many other browsers may work, but are not extensively tested.\n\n## Maintainers\nSince 2019, mojs ecosystem is **maintained and developed** by:\n- [Xavier Foucrier](https://github.com/xavierfoucrier)\n- [Jonas Sandstedt](https://github.com/Sandstedt)\n\n## Contribute\nIf you want to report a bug or request a new feature/improvement, please **read the project [contributors guidelines](.github/CONTRIBUTING.md) before**. Thanks for taking time to contribute.\n",18323,graphics,CoffeeScript,4,CSS,JavaScript,HTML,CoffeeScript,,,,,,,,,,,,,,,,,,,,,,,,,61,16,45,0,4,22,0,140733,893,195,158,37,51ed2454b0ef9f573b3f176c059d7eab72eebe7a,Update eslint-webpack-plugin to 4.2.0,2024-06-07T21:57:23Z,Xavier Foucrier,xavier.foucrier@gmail.com,xavierfoucrier,@mojs/core 1.7.1,- Fix missing `dist` folder in the published package,v1.7.1,Xavier Foucrier,,xavierfoucrier,MIT License,mojs,mojs,28,web,graphics,motion,toolbelt,mojs,javascript,animation,library,es6,webpack,modern,svg,design,motion-graphics,burst,swirl,shape,timeline,mo-js,,/mojs/mojs,28,394,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/milaboratory/mixcr,https://github.com/milaboratory/mixcr,1,,,1,1,1,1,0,0,0,0,0,0,1,MiXCR is an ultimate software platform for analysis of Next-Generation Sequencing (NGS) data for immune profiling.,"[![image](https://img.shields.io/badge/documentation-v4.x-brightgreen)](https://docs.milaboratories.com)\n\n\n![MiXCR logo](./doc/_static/MiXCR_logo_dark.png#gh-light-mode-only)\n![MiXCR logo](./doc/_static/MiXCR_logo_white.png#gh-dark-mode-only)\n\nMiXCR is a universal software for fast and accurate analysis of raw T- or B- cell receptor repertoire sequencing data. It works with any kind of sequencing data:\n - Bulk repertoire sequencing data with or without UMIs\n - Single cell sequencing data including but not limited to 10x Genomics protocols \n - RNA-Seq or any other kind of fragmented/shotgun data which may contain just a tiny fraction of target sequences\n - and any other kind of sequencing data containing TCRs or BCRs \n\nPowerful downstream analysis tools allow to obtain vector plots and tabular results for multiple measures. Key features include:\n - Ability to group samples by metadata values and compare repertoire features between groups \n - Comprehensive repertoire normalization and filtering \n - Statistical significance tests with proper p-value adjustment\n - Repertoire overlap analysis\n - Vector plots output (.svg / .pdf)\n - Tabular outputs\n\nOther key features:\n\n- Clonotype assembly by arbitrary gene feature, including *full-length* variable region\n- PCR / Sequencing error correction with or without aid of UMI or Cell barcodes\n- Robust and dedicated aligner algorithms for maximum extraction with zero false-positive rate\n- Supports any custom barcode sequences architecture (UMI / Cell)\n- _Human_, _Mice_, _Rat_, _Spalax_, _Alpaca_, _Monkey_\n- Support IMGT reference\n- Barcodes error-correction\n- Adapter trimming\n- Optional CDR3 reconstruction by assembling overlapping fragmented sequencing reads into complete CDR3-containing contigs when the read position is floating (e.g. shotgun-sequencing, RNA-Seq etc.)\n- Optional contig assembly to build longest possible TCR/IG sequence from available data (with or without aid of UMI or Cell barcodes) \n- Comprehensive quality control reports provided at all the steps of the pipeline\n- Regions not covered by the data may be imputed from germline\n- Exhaustive output information for clonotypes and alignments:\n    - nucleotide and amino acid sequences of all immunologically relevant regions (FR1, CDR1, ..., CDR3, etc..)\n    - identified V, D, J, C genes\n    - comprehensive information on nucleotide and amino acid mutations\n    - positions of all immunologically relevant points in output sequences\n    - and many more informative columns\n- Ability to backtrack fate of each raw sequencing read through the whole pipeline \n\nSee full documentation at [https://docs.milaboratories.com](https://docs.milaboratories.com).\n\n## Who uses MiXCR \nMiXCR is used by 8 out of 10 world leading pharmaceutical companies in the R&D for:\n- Vaccine development\n- Antibody discovery\n- Cancer immunotherapy research\n\nWidely adopted by academic community with 1000+ citations in peer-reviewed scientific publications.\n\n## Installation / Download\n\n### Using Homebrew on Mac OS X or Linux (linuxbrew)\n\n    brew install milaboratory/all/mixcr\n    \nto upgrade already installed MiXCR to the newest version:\n\n    brew update\n    brew upgrade mixcr\n\n### Conda\n\nWe maintain [Anaconda repository](https://anaconda.org/milaboratories/mixcr) to simplify installation of MiXCR using `conda` package manager. To install latest stable MiXCR build with conda run:\n\n```\nconda install -c milaboratories mixcr\n```\n\nto install a specific version run:\n\n```\nconda install -c milaboratories mixcr=3.0.12\n```\n\n`mixcr` package specifies `openjdk` as a dependency, if you already have Java installed on your system, it might be a good idea to prevent conda from installing another copy of JDK, to do that use `--no-deps` flag:\n\n```\nconda install -c milaboratories mixcr --no-deps\n```\n\n### Docker\n\nOfficial MiXCR Docker repository is hosted on the GitHub along with this repo.\n\nExample:\n\n```\ndocker run --rm \\n    -e MI_LICENSE=""...license-token..."" \\n    -v /path/to/raw/data:/raw:ro \\n    -v /path/to/put/results:/work \\n    ghcr.io/milaboratory/mixcr/mixcr:latest \\n    align -s hs /raw/data_R1.fastq.gz /raw/data_R2.fastq.gz alignments.vdjca \n```\n\n#### Tags\n\nThe docker repo provides pre-built docker images for all release versions of MiXCR starting from 1.1. Images come in two flavours: ""mixcr only"" (i.e tag `4.0.0`) and co-bundled ""mixcr + imgt reference"" (i.e. tag `4.0.0-imgt`), for the latter please see the license note below. All bundled versions before and including `4.0.0` contain IMGT reference version `202214-2` from [here](https://github.com/repseqio/library-imgt/releases/tag/v8), this might be different from the images from the previous official docker registry on Docker Hub (which is now deprecated and planned for removal).\n\nSee [docker packages](https://github.com/milaboratory/mixcr/pkgs/container/mixcr%2Fmixcr) page for the full list of tags including development builds.\n\n#### Setting the license\n\nThere are several ways to pass the license for mixcr when executed inside a container:\n\n1. Using environment variable:\n   \n   ```\n   docker run \\n       -e MI_LICENSE=""...license-token..."" \\n       ....\n   ```\n\n2. Using license file:\n\n   ```\n   docker run \\n       -v /path/to/mi.license:/opt/mixcr/mi.license:ro \\n       ....\n   ```\n\n3. If it is hard to mount `mi.license` file into already populated folder `/opt/mixcr/` (i.e. in Kubernetes or with other container orchestration tools), you can tell MiXCR where to look for it:\n\n   ```\n   docker run \\n       -v /path/to/folder_with_mi_license:/secrets:ro \\n       -e MI_LICENSE_FILE=""/secrets/milicense.txt"" \\n       ....\n   ```\n\n#### Migration from the previous docker images\n\nNew docker images define `mixcr` startup script as an entrypoint of the image, compared to the previous docker repo where `bash` was used instead. So, what previously was executed this way:\n\n```\ndocker run ... old/mixcr/image/address:with_tag mixcr align ...\n```\n\nnow will be\n\n```\ndocker run ... new/mixcr/image/address:with_tag align ...\n```\n\nFor those who rely on other tools inside the image, beware, new build relies on a different base image and has slightly different layout.\n\n`mixcr` startup script is added to `PATH` environment variable, so even if you specify custom entrypoint, there is no need in using of full path to run `mixcr`. \n\n#### License notice for IMGT images\n\nImages with IMGT reference library contain data imported from IMGT and is subject to terms of use listed on http://www.imgt.org site.\n\nData coming from IMGT server may be used for academic research only, provided that it is referred to IMGT&reg;, and cited as ""IMGT&reg;, the international ImMunoGeneTics information system&reg; http://www.imgt.org (founder and director: Marie-Paule Lefranc, Montpellier, France).""\n\nReferences to cite: Lefranc, M.-P. et al., Nucleic Acids Research, 27, 209-212 (1999) Cover of NAR; Ruiz, M. et al., Nucleic Acids Research, 28, 219-221 (2000); Lefranc, M.-P., Nucleic Acids Research, 29, 207-209 (2001); Lefranc, M.-P., Nucleic Acids Res., 31, 307-310 (2003); Lefranc, M.-P. et al., In Silico Biol., 5, 0006 (2004) [Epub], 5, 45-60 (2005); Lefranc, M.-P. et al., Nucleic Acids Res., 33, D593-D597 (2005) Full text, Lefranc, M.-P. et al., Nucleic Acids Research 2009 37(Database issue): D1006-D1012; doi:10.1093/nar/gkn838 Full text.\n\n### Manual install (any OS)\n\n* download the latest stable MiXCR build from [release page](https://github.com/milaboratory/mixcr/releases/latest)\n* unzip the archive\n* add resulting folder to your ``PATH`` variable\n  * or add symbolic link for ``mixcr`` script to your ``bin`` folder\n  * or use MiXCR directly by specifying full path to the executable script\n\n#### Requirements\n\n* Any OS with Java support (Linux, Windows, Mac OS X, etc..)\n* Java 1.8 or higher\n\n## Obtaining a license\n\nTo run MiXCR one needs a license file. MiXCR is free for academic users with no commercial funding. We are committed to support academic community and provide our software free of charge for scientists doing non-profit research.\n\nAcademic users can quickly get a license at https://licensing.milaboratories.com.\n\nCommercial trial license may be requested at https://licensing.milaboratories.com or by email to licensing@milaboratories.com.\n\nTo activate the license do one of the following:\n\n- put `mi.license` to\n    - `~/.mi.license`\n    - `~/mi.license`\n    - directory with `mixcr.jar` file\n    - directory with MiXCR executable\n    - to any place and specify it in `MI_LICENSE_FILE` environment variable\n- put `mi.license` content to `MI_LICENSE` environment variable\n- run `mixcr activate-license` and paste `mi.license` content to the command prompt\n\n## Usage & documentation\n\nSee usage examples and detailed documentation at https://docs.milaboratories.com\n\nIf you haven't found the answer to your question in the docs, or have any suggestions concerning new features, feel free to create an issue here, on GitHub, or write an email to support@milaboratory.com .\n\n## License\n\nCopyright (c) 2014-2022, MiLaboratories Inc. All Rights Reserved\n\nBefore downloading or accessing the software, please read carefully the\nLicense Agreement available at:\nhttps://github.com/milaboratory/mixcr/blob/develop/LICENSE\n\nBy downloading or accessing the software, you accept and agree to be bound\nby the terms of the License Agreement. If you do not want to agree to the terms\nof the Licensing Agreement, you must not download or access the software.\n\n\n## Cite\n\n* Dmitriy A. Bolotin, Stanislav Poslavsky, Igor Mitrophanov, Mikhail Shugay, Ilgar Z. Mamedov, Ekaterina V. Putintseva, and Dmitriy M. Chudakov. ""MiXCR: software for comprehensive adaptive immunity profiling."" *Nature methods* 12, no. 5 (**2015**): 380-381.\n  \n  \\n  (Files referenced in this paper can be found [here](https://github.com/milaboratory/mixcr/blob/develop/doc/paper/paperAttachments.md).)\n\n  \n\n* Dmitriy A. Bolotin, Stanislav Poslavsky, Alexey N. Davydov, Felix E. Frenkel, Lorenzo Fanchi, Olga I. Zolotareva, Saskia Hemmers, Ekaterina V. Putintseva, Anna S. Obraztsova, Mikhail Shugay, Ravshan I. Ataullakhanov, Alexander Y. Rudensky, Ton N. Schumacher & Dmitriy M. Chudakov. ""Antigen receptor repertoire profiling from RNA-seq data."" *Nature Biotechnology* 35, 908–911 (**2017**)\n\n\n",314,bioinformatics,Kotlin,5,Java,JavaScript,Shell,Kotlin,Python,,,,,,,,,,,,,,,,,,,,,,,,573,35,530,8,37,14,24,48866,78,943,850,93,e737ddef145ed152c9a8eb1b05139d0ae5f22a2e,Merge pull request #1653 from milaboratory/archer-fix,2024-07-10T23:28:08Z,mizraelson,mizraelson@gmail.com,mizraelson,MiXCR v4.6.0,"## 🖇️ Combined Heavy+Light Somatic Hypermutation Trees from Single-Cell data\r\n\r\n- A special step is added in `findShmTrees` to combine heavy and light SHM trees utilizing information added to clonotypes by `groupClones` command. Nodes in resulting tree will contain both light and heavy chains. If there is no connection to a clone from a companion chain, a reconstructed sequence will be added.\r\n- Behaviour can be disabled with `--dont-combine-tree-by-cells` option to reconstruct separate heavy and light SHM trees\r\n- Added `exportShmSingleCellTrees` command that export one node per line. It there is several roots in a tree, data will be exported in a different columns.\r\n- Added `-subtreeId` to tree exports to differentiate part of trees from different chains\r\n- `exportShmTreesWithNodes` and `exportShmTrees` commands will export subtrees with different chains at separate rows.\r\n\r\n## 🚀 Other major upgrades\r\n\r\n### Changes in `groupClones` command\r\n\r\n- Previous algorithm replaced with a new one that have better way of working with contamination, can detect multi-mappers (when one cell barcode marks two different cells) and can work with non-functional clones\r\n- Some clones are now explicitly marked as contamination. This information is available as a separate label in `exportClones` in `groupId` column. Such clones can be filtered out from export by `--filter-out-group-types contamination`\r\n- More important algorithm performance metrics are added to the report\r\n- Fix for behaviour leading to clones with `undefiened` group being split by cell barcodes\r\n\r\n### New characteristics in SHM trees exports\r\n\r\n- `-subtreeId` for determination of different chains in the same tree\r\n- `-numberOfClonesInTree [forChain]` Number of uniq clones in the SHM tree.\r\n- `-numberOfNodesWithClones` Number of nodes with clones, i.e. nodes with different clone sequences.\r\n- `-totalReadsCountInTree [forChain]` Total sum of read counts of clones in the SHM tree.\r\n- `-totalUniqueTagCountInTree (Molecule|Cell|Sample) [forChain]` Total count of unique tags in the SHM tree with specified type.\r\n- `-chains` Chain type of the tree\r\n- `-treeHeight` Height of the tree\r\n- `-vGene`, `-jGene`, `-vFamily`, `-jFamily` - in previous version thous were exported only for nodes with clones\r\n- `-vBestIdentityPercent`, `-jBestIdentityPercent`, `-isOOF` and `-isProductive` now exported for reconstructed nodes too\r\n\r\n### New characteristics in clonotype export\r\n\r\n- `-aaLength` and `-allAALength` is available alongside `-nLength` and `-allNLength`\r\n- `-aaMutationsRate` is available alongside `-nMutationsRate`\r\n- Added optional arg `germline` in `-nFeature`, `-aaFeature`, `-nLength`, `-aaLength` in `exportClones`, `exportAlignments` and `exportCloneGroups`. It allows to export a sequence of the germline instead of a sequence of the gene.\r\n- For all mutation exports (excluding `-mutationsDetailed` ) added optional filter by mutation type: `... [(substitutions|indels|inserts|deletions)]`\r\n- Added `-nMutationsCount`, `-aaMutationsCount`, `-allNMutationsCount`, `-allAAMutationsCount` for all relatable exports\r\n- For mutation exports in `exportShmTreesWithNodes` `(germline|mrca|parent)` option is now optional. Will be export mutations from `germline` by default\r\n- Added `--export-clone-groups-sort-chains-by` mixin\r\n- Nucleotide mutations now could be exported for features that contain `VCDR3Part`, `DCDR3Part` or `JCDR3Part`\r\n- Now `-nLength`, `-nMutationsCount`, `-nMutationsRate` can be calculated for multiple gene features (e.g. `-nMutationsRate VRegionTrimmed,JRegionTrimmed`)\r\n- Added `--export-clone-groups-sort-chains-by` mixin with type of sorting of clones for determination of the primary and the secondary chains. It applies to `exportCloneGroups` command. By default, it's `Auto` (by UMI if it's available, by Read otherwise; previous default value was `Read`)\r\n- Added `--filter-out-group-types` mixin to filter-out clones having certain clone group assignment kind: `found`, `undefined` or `contamination`. It applies to `exportClones` command\r\n- Now `exportCloneGroups` by default will export groups in separate files for `IG`, `TRAB`, `TRGD` and `mixed`. This behaviour could be switched off by using `--reset-export-clone-table-splitting` or single `--export-clone-groups-for-cell-type`. In case of several `--export-clone-groups-for-cell-type` every cell type will be exported in separate file.\r\n- In case of `--export-clone-groups-for-cell-type` in `exportCloneGroups` all mixed or unmatched groups will be filtered out.\r\n- Added read and Molecule fraction columns to single cell exportClones output.\r\n\r\n## 🧬 Reference library upgrades\r\n\r\n- Previous `TRAD` meta-chain split into `TRA` and `TRD` as it should be. Chain assignment for clonotypes based on J genes.\r\n- Rebuild allelic reference for human `IGH`, `TRB`, `TRA` and `TRD` chains. Now allelic names correspond to the IUIS nomenclature.\r\n- Human `IGK` `Vend` coordinates corrected.\r\n- `UTR5Begin` coordinates added to the following mouse genes: IGKV23-1, IGKV20-101-2, IGKV14-130, IGKV8-28, TRGV2\r\n\r\n## 📚  Preset updates\r\n\r\n- The `milab-human-rna-tcr-umi-race` preset has been updated: now clones are assembled by default based on the CDR3, in line with the manufacturer's recommended read length.\r\n- The `flairr-seq-bcr` preset has been updated: now the preset sets species to `human` by default according to a built-in tag pattern with primer sequences.\r\n- The following presets have been added to cover Ivivoscribe assay panels: `invivoscribe-human-dna-trg-lymphotrack`,`invivoscribe-human-dna-trb-lymphotrack`, `invivoscribe-human-dna-igk-lymphotrack`,`invivoscribe-human-dna-ighv-leader-lymphotrack`,`invivoscribe-human-dna-igh-fr3-lymphotrack`, `invivoscribe-human-dna-igh-fr2-lymphotrack`,`invivoscribe-human-dna-igh-fr1-lymphotrack`,`invivoscribe-human-dna-igh-fr123-lymphotrack`.\r\n- The following presets have been added for mouse Thermofisher assays: `thermofisher-mouse-rna-tcb-ampliseq-sr`,`thermofisher-mouse-dna-tcb-ampliseq-sr`,`thermofisher-mouse-rna-igh-ampliseq-sr`,`thermofisher-mouse-dna-igh-ampliseq-sr`.\r\n- Preset for SMARTer Human scTCR a/b Profiling Kit: `takara-sc-human-rna-tcr-smarter`\r\n- The `milab-human-rna-ig-umi-multiplex` preset has been updated: the pattern now trims fewer nucleotides, which facilitates CDR1 identification. The splits by V and J genes have been removed as redundant due to the full-length assembling feature.\r\n\r\n## 🛠️ Minor improvements & fixes\r\n\r\n- More strict `Combining trees` step in `findShmTrees` command\r\n- Better calculation of indel mutations between nodes in process of building shm trees\r\n- Increased percent of successful alignment-aided overlaps by removing unnecessary overlap region quality sum threshold\r\n- Impossible export of germline sequence for `VJJunction` in `shmTrees` exports now produces an error\r\n- Parameter validation fix in `-nMutationsRate`\r\n- Fix for `-nMutationsRate` if region is not covered for the clone\r\n- Fix for the formal of `exportAlignmentsPretty` broken in the previous version\r\n- Fix for IllegalArgumentException in `exportAlignmentsPretty` for cases where translation can't be performed\r\n- Fix error for `analyze` executed with `-f` and `--output-not-used-reads` at the same time\r\n- Resolutions of wildcards are excluded from calculation of `-nMutationsRate` for CDR3 in `exportShmTreesWithNodes`\r\n- Fix OutOfMemory exception in command `extend` with `.vdjca` input\r\n- In `findShmTrees` filter for productive only clones now check for stop codons in all features, not only in CDR3\r\n- Change default value for filter for productive clones in `findShmTrees` to false (was true before)\r\n- Add option `--productive-only` to `findShmTrees`\r\n- Fixed parsing of `--export-clone-groups-for-cell-type` parameter\r\n- Fixed usage of `slice` command on clnx files that weren't ordered by id.\r\n- In `slice` now default behaviour is to keep original ids. Previous behaviour available with `--reassign-ids` option\r\n- Fixed parsing of composite gene features with offsets like `--assemble-clonotypes-by [VDJRegion,CBegin(0,10)]`\r\n- Fixed parent directory creation for output of `exportClonesOverlap`\r\n- Fixed `exportAirr` in case of a clone with CDR3 that don't have VCDR3Part and JCDR3Part\r\n- Optimize calculation of ranks in clone set. Speeds up export with tags and several other places.\r\n- Added `clone_id` column in `exportAirr`\r\n- Fixed `exportClones` in case of splitting file by `tag:...` if there is a clone that have several tags of requested level\r\n- Fixed calculation of `-nMutationsCount`, `-nMutationsRate`, `-aaMutationsCount` and `-aaMutationsRate`. Previously in some cases it was calculated on different region, from what was requested.\r\n- Added `CellBarcodesWithFoundGroups` for `groupClones` QC checks\r\n- New filter `--no-feature` in `exportAlignmentsPretty`\r\n- Fixed reporting in `align`, now coverage takes into account alignment-aided overlap\r\n\r\n## ❗ Breaking changes\r\n\r\n- Option `--build-from <path>` was removed from `findShmTrees` command\r\n\r\n### Deprecations of export options\r\n\r\n- `-lengthOf` now is deprecated, use `-nLength` instead\r\n- `-allLengthOf` now is deprecated, use `-allNLength` instead\r\n- `-mutationRate` now is deprecated, use `-nMutationsRate` instead\r\n",v4.6.0,,,github-actions[bot],Other,mixcr,milaboratory,60,bioinformatics,immunology,antibody,t-cell-receptor,t-cell,rep-seq,10x,rna-seq,sequencing,single-cell,,,,,,,,,,,/milaboratory/mixcr,63,31,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/microsoft/MixedReality-GraphicsTools-Unity,https://github.com/microsoft/MixedReality-GraphicsTools-Unity,0,,,0,0,0,0,0,0,1,1,0,0,0,Graphics tools and components for developing Mixed Reality applications in Unity.,"![MRTK](README/MRTKBanner.png)\n\n# What is Graphics Tools?\n\nMRTK Graphics Tools for Unity is a [Unity engine](https://unity.com/) package with code, shaders, assets, and samples created to help improve the visual fidelity of [mixed reality](https://docs.microsoft.com/windows/mixed-reality/discover/mixed-reality) applications while staying within performance budgets.\n\n![Graphics Tools](README/GTBanner.png)\n\n# Getting started with Graphics Tools\n\nGraphics Tools is normally ingested as a Unity [package](https://docs.unity3d.com/Manual/Packages.html). To import Graphics Tools into your Unity project follow the below steps:\n\n> [!NOTE]\n> The Graphics Tools package requires Unity 2021.3 and above.\n\n1. Open your Unity project and select `Window > Package Manager` from the file menu bar\n\n2. Click the `'+'` icon within the Package Manager and select `""Add package from git URL...""`\n\n    ![Package Manager Add](README/PackageManagerAdd.png)\n\n3. Paste *https://github.com/microsoft/MixedReality-GraphicsTools-Unity.git?path=/com.microsoft.mrtk.graphicstools.unity#v0.4.0* into the text field and click `""Add""`\n\n    ![Package Manager Paste](README/PackageManagerPaste.png)\n\n4. Graphics Tools will now be installed within your Unity project as an immutable package within the project's `Packages` folder named `MRTK Graphics Tools`.\n\nIt is advised you use a specific release of the Graphics Tools package to ensure your project is locked to a stable release. Release version 0.4.0 is suggested in step three above. You can also pull another version, specific branch, or git commit hash by altering the URL as demonstrated below:\n\n| Syntax           | URL example                                                                                                                                               |\n|------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Specific version | *https://github.com/microsoft/MixedReality-GraphicsTools-Unity.git?path=/com.microsoft.mrtk.graphicstools.unity#vX.Y.Z*                                   |\n| Specific branch  | *https://github.com/microsoft/MixedReality-GraphicsTools-Unity.git?path=/com.microsoft.mrtk.graphicstools.unity#my_branch*                                |\n| Git commit hash  | *https://github.com/microsoft/MixedReality-GraphicsTools-Unity.git?path=/com.microsoft.mrtk.graphicstools.unity#badc0ffee0ddf00ddead10cc8badf00d1badb002* |\n\n## Documentation\n\nDocumentation can be found on Microsoft's technical documentation website.\n\n| Documentation Category | Link                                                                                                               |\n|------------------------|--------------------------------------------------------------------------------------------------------------------|\n| Conceptual             | [MRTK Unity](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/mrtk3-graphicstools/)                                         |\n| API Reference          | [Microsoft.MixedReality.GraphicsTools](https://docs.microsoft.com/dotnet/api/Microsoft.MixedReality.GraphicsTools) |\n\n## Importing samples\n\nTo view the samples contained within Graphics Tools select `Window > Package Manager` from the Unity file menu bar. Click on the `MRTK Graphics Tools` package and expand the `Samples` list. Finally, click the `Import` button for any samples you would like to try:\n\n![Package Manager Samples](README/PackageManagerSamples.png)\n\n## Example project\n\nA Unity project named `GraphicsToolsUnityProject` lives at the root of this repository. The project serves as sandbox to test and validate the Graphics Tools package. This project can also assist in developing contributions to this repo. See [below](#updating-the-package-samples) to learn more about contributing.\n\n> [!TIP]\n> See GraphicsToolsUnityProject/ProjectSettings/ProjectVersion.txt to inspect the Unity version used for the example project.\n\n# Feedback and contributions\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Updating the package samples\n\nSamples exist under the `Samples~` subfolder as outlined by Unity's [sample recommendations](https://docs.unity3d.com/Manual/cus-samples.html). The '~' character prevents the `Samples~` folder for being imported by Unity.\n\nIf you wish to contribute changes to the samples you must make a temporary local change to the Unity project's directory structure.\n\n1. Open the `GraphicsToolsUnityProject` in Unity. The `Samples` folder will not be visible in the `MRTK Graphics Tools` package by default. To show the samples select `Window > Graphics Tools > Show Samples` from the file menu bar.\n2. Make any desired changes to the samples.\n3. **Important:** When finished and before committing your changes, remember to hide the samples. Select `Window > Graphics Tools > Hide Samples` from the file menu bar.\n\n# License agreement\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n",175,graphics,C#,4,C#,HLSL,GLSL,ShaderLab,,,,,,,,,,,,,,,,,,,,,,,,,117,9,107,1,7,29,2,56469,39,101,59,42,e657cea8228981f951c3024aeebd868aaf524095,Update CHANGELOG.md,2024-05-22T23:42:11Z,Cameron,thmicka@microsoft.com,Cameron-Micka,Graphics Tools v0.7.1,"![Logo](https://github.com/microsoft/MixedReality-GraphicsTools-Unity/blob/public/0.7.x/README/GTBanner.png)\r\n\r\n# Getting started\r\nTo help you get started using Graphics Tools v0.7.1, please see the [README](https://github.com/microsoft/MixedReality-GraphicsTools-Unity/blob/public/0.7.x/README.md).\r\n\r\n# Release notes\r\nPlease see the [CHANGELOG](https://github.com/microsoft/MixedReality-GraphicsTools-Unity/blob/public/0.7.x/com.microsoft.mrtk.graphicstools.unity/CHANGELOG.md).",v0.7.1,Cameron,,Cameron-Micka,MIT License,MixedReality-GraphicsTools-Unity,microsoft,3,hololens,unity,graphics,mixed-reality,mixedrealitytookit-unity,mixedrealitytoolkit,mrtk,graphicstools,,,,,,,,,,,,,/microsoft/MixedReality-GraphicsTools-Unity,35,13,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/microsoft/maker.js,https://github.com/microsoft/maker.js,0,,0,0,0,0,0,0,0,1,1,0,0,0,📐⚙ 2D vector line drawing and shape modeling for CNC and laser cutters.,"# Maker.js\n\nYour compass and straightedge, in JavaScript.\n\nCreate line drawings using familiar constructs from geometry and drafting. Initially designated for CNC and laser cutters, Maker.js can also help you programmatically draw shapes for any purpose. It runs in both Node.js and web browsers.\n\n2D Export formats: \n[DXF](https://maker.js.org/docs/api/modules/makerjs.exporter.html#todxf), \n[SVG](https://maker.js.org/docs/api/modules/makerjs.exporter.html#tosvg),\n[PDF](https://maker.js.org/docs/api/modules/makerjs.exporter.html#topdf),\n[Jscad CAG object](https://maker.js.org/docs/api/modules/makerjs.exporter.html#tojscadcag)\n\n3D Export formats:\n[Jscad Script](https://maker.js.org/docs/api/modules/makerjs.exporter.html#tojscadscript),\n[Jscad CSG object](https://maker.js.org/docs/api/modules/makerjs.exporter.html#tojscadcsg),\n[STL](https://maker.js.org/docs/api/modules/makerjs.exporter.html#tojscadstl)\n\n[Demos](https://maker.js.org/demos/) - [Documentation](http://maker.js.org/docs/)\n\n![Sample animation](https://maker.js.org/images/anim-wheel.gif)\n\n## Core concepts\n\n* [paths](https://maker.js.org/docs/basic-drawing/#Paths) - The primitive elements of a drawing are lines, arcs, and circles.\n* [models](https://maker.js.org/docs/basic-drawing/#Models) - Groups of paths to compose a shape.\n* [layers](https://maker.js.org/docs/advanced-drawing/#Layers) - Organization of models, such as by color or tool type.\n* [chains](https://maker.js.org/docs/working-with-chains/#content) - A series of lines and arcs that connect end-to-end continuously.\n\nLearn more in [the tutorial](https://maker.js.org/docs/basic-drawing/) or [API documentation](https://maker.js.org/docs/api/).\n\n## Features\n\n* Drawings are a [simple JavaScript object](https://maker.js.org/docs/basic-drawing/#It%27s%20Just%20JSON) which can be serialized / deserialized conventionally with JSON. This also makes a drawing easy to [clone](https://maker.js.org/docs/intermediate-drawing/#Cloning).\n\n* Other people's Models can be required the Node.js way, [modified](https://maker.js.org/docs/intermediate-drawing/#Modifying%20models), and re-exported.\n\n* Models can be [scaled](https://maker.js.org/docs/intermediate-drawing/#Scaling), [distorted](https://maker.js.org/docs/intermediate-drawing/#Distorting), [measured](https://maker.js.org/docs/api/modules/makerjs.measure.html#modelextents), and [converted to different unit systems](https://maker.js.org/docs/basic-drawing/#Units).\n\n* Paths can be [distorted](https://maker.js.org/docs/api/modules/makerjs.path.html#distort).\n\n* Models can be [rotated](https://maker.js.org/docs/intermediate-drawing/#Rotating) or [mirrored](https://maker.js.org/docs/intermediate-drawing/#Mirroring).\n\n* Find [intersection points or intersection angles](https://maker.js.org/docs/intermediate-drawing/#Intersection) of paths.\n\n* [Traverse a model tree](https://maker.js.org/docs/model-trees/#content) to reason over its children.\n\n* Detect [chains](https://maker.js.org/docs/api/modules/makerjs.model.html#findchains) formed by paths connecting end to end.\n\n* Get the [points along a path](https://maker.js.org/docs/api/modules/makerjs.path.html#topoints) or along a [chain of paths](https://maker.js.org/docs/api/modules/makerjs.chain.html#topoints).\n\n* Easily add a curvature at the joint between any 2 paths, using a [traditional or a dogbone fillet](https://maker.js.org/docs/intermediate-drawing/#Fillets).\n\n* [Combine models](https://maker.js.org/docs/advanced-drawing/#Combining%20with%20Boolean%20operations) with boolean operations to get unions, intersections, or punches.\n\n* [Expand paths](https://maker.js.org/docs/advanced-drawing/#Expanding%20paths) to simulate a stroke thickness, with the option to bevel joints.\n\n* [Outline model](https://maker.js.org/docs/advanced-drawing/#Outlining%20a%20model) to create a surrounding outline, with the option to bevel joints.\n\n* Layout clones into [rows](http://maker.js.org/docs/api/modules/makerjs.layout.html#clonetorow), [columns](https://maker.js.org/docs/api/modules/makerjs.layout.html#clonetocolumn), [grids](https://maker.js.org/docs/api/modules/makerjs.layout.html#clonetogrid), [bricks](https://maker.js.org/docs/api/modules/makerjs.layout.html#clonetobrick), or [honeycombs](https://maker.js.org/docs/api/modules/makerjs.layout.html#clonetohoneycomb)\n\n#### Built-in models\n\n* [Belt](https://maker.js.org/playground/?script=Belt)\n* [Bezier Curve](https://maker.js.org/playground/?script=BezierCurve)\n* [Bolt Circle](https://maker.js.org/playground/?script=BoltCircle)\n* [Bolt Rectangle](https://maker.js.org/playground/?script=BoltRectangle)\n* [Connect the dots](https://maker.js.org/playground/?script=ConnectTheDots)\n* [Dogbone](https://maker.js.org/playground/?script=Dogbone)\n* [Dome](https://maker.js.org/playground/?script=Dome)\n* [Ellipse](https://maker.js.org/playground/?script=Ellipse)\n* [Elliptic Arc](https://maker.js.org/playground/?script=EllipticArc)\n* [Holes](https://maker.js.org/playground/?script=Holes)\n* [Oval](https://maker.js.org/playground/?script=Oval)\n* [OvalArc](https://maker.js.org/playground/?script=OvalArc)\n* [Polygon](https://maker.js.org/playground/?script=Polygon)\n* [Rectangle](https://maker.js.org/playground/?script=Rectangle)\n* [Ring](https://maker.js.org/playground/?script=Ring)\n* [RoundRectangle](https://maker.js.org/playground/?script=RoundRectangle)\n* [S curve](https://maker.js.org/playground/?script=SCurve)\n* [Slot](https://maker.js.org/playground/?script=Slot)\n* [Square](https://maker.js.org/playground/?script=Square)\n* [Star](https://maker.js.org/playground/?script=Star)\n* [Text](https://maker.js.org/playground/?script=Text)\n\n#### Import formats\n\n* [Fonts](https://maker.js.org/playground/?script=Text) (Requires [opentype.js](https://opentype.js.org/))\n* [SVG Path Data](https://maker.js.org/docs/importing/#SVG+path+data)\n* [SVG Points](https://maker.js.org/docs/importing/#SVG+points)\n\n## Getting Started\n\n### Try it now\n\nVisit the [Maker.js Playground](https://maker.js.org/playground/) a sample app to edit and run JavaScript from your browser.\n\nEach of the [demos](https://maker.js.org/demos/#content) will also open in the playground so that you can explore and modify their code.\n\n### To use in a web browser\n\nDownload the browser-based version of Maker.js, then upload it to your website:\n[https://maker.js.org/target/js/browser.maker.js](https://maker.js.org/target/js/browser.maker.js)\n\nAdd a script tag in your HTML:\n```html\n<script src=""https://maker.js.org/target/js/browser.maker.js"" type=""text/javascript""></script>\n```\n\n*Note: You may also need [additional libraries](https://maker.js.org/docs/getting-started/#For+the+browser)*\n\nIn your JavaScript, use the require function to get a reference:\n \n```javascript\nvar makerjs = require('makerjs');\n```\n\n### To use via CDN\n\nAdd a script tag to your HTML:\n```\n<script src=""https://cdn.jsdelivr.net/npm/makerjs@0/target/js/browser.maker.js""></script>\n```\nTo work with Bezier Curves, you will also need a copy of [Bezier.js by Pomax](http://pomax.github.io/bezierjs/)\n```\n<script src=""https://cdn.jsdelivr.net/npm/bezier-js@2/bezier.js""></script>\n```\nTo work with fonts, you will need both Bezier.js(above) and a copy of [Opentype.js by Frederik De Bleser](https://github.com/nodebox/opentype.js)\n```\n<script src=""https://cdn.jsdelivr.net/npm/opentype.js@0/dist/opentype.js""></script>\n```\n\nIn your JavaScript, use the `require` function to get a reference:\n```\nvar makerjs = require('makerjs');\n```\n\n### To use in Node.js\n\nTo depend on Maker.js, run this from the command line:\n```bash\nnpm install makerjs --save\n```\n\nIn your JavaScript, use the `require` function to get a reference:\n \n```javascript\nvar makerjs = require('makerjs');\n```\n\n## Contributing\nThere are many ways to contribute to Maker.js:\n* [★ Star Maker.js on GitHub](https://github.com/Microsoft/maker.js)\n* Submit bugs and feature requests [on GitHub](https://github.com/Microsoft/maker.js/issues).\n* Create a demo for the [gallery](http://maker.js.org/demos/#content).\n* Create lessons or videos for the [documentation](http://maker.js.org/docs/#content).\n* Enhance the [website](https://github.com/Microsoft/maker.js/tree/gh-pages).\n* Add features to the [Playground app](https://maker.js.org/playground/).\n* Create a new Maker.js app, and we will link to it here.\n* Find some TODO's in the [core source code](https://github.com/Microsoft/maker.js/tree/master).\n* Create unit tests for the core.\n\nSome of these may require a [contributor agreement](https://github.com/Microsoft/maker.js/blob/master/CONTRIBUTING.md).\n\n### Credits\nMaker.js depends on:\n* [clone](https://github.com/pvorb/node-clone) by Paul Vorbach\n* [bezier-js](https://github.com/Pomax/bezierjs) by Pomax\n* [graham_scan](https://github.com/brian3kb/graham_scan_js) by Brian Barnett\n* [kdbush](https://github.com/mourner/kdbush) by Vladimir Agafonkin\n---\n\nMaker.js is a Microsoft Garage project. The Microsoft Garage turns fresh ideas into real projects. Learn more at [http://microsoft.com/garage](http://microsoft.com/garage).\n",1746,geometry,TypeScript,5,HTML,JavaScript,TypeScript,CSS,Handlebars,,,,,,,,,,,,,,,,,,,,,,,,366,6,357,3,18,19,682,15433,264,213,153,60,2ce5da2ca31b9b8f7e1099fdefa0ee7879c6178e,Bump braces from 3.0.2 to 3.0.3 (#594),2024-06-17T18:11:03Z,dependabot[bot],49699333+dependabot[bot]@users.noreply.github.com,dependabot[bot],Maker.js 0.9.17,"Minor changes from 0.9.10:\n- Environment detection\n- Rotation origin default to [0, 0]\n- Path converge to closest line endpoint\n\nBug fixes:\n- expansion\n- text centering\n- svg layer export\n",0.9.17,Dan Marshall,,danmarshall,Apache License 2.0,maker.js,microsoft,9,maker,cad,draw,cnc,vector,svg,pdf,dxf,drawing,fonts,geometry,bezier,openjscad,rectangle,circle,line,arc,laser,,,/microsoft/maker.js,9,104,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/microsoft/DXUT,https://github.com/microsoft/DXUT,0,,,0,0,0,0,0,0,1,1,0,1,0,"DXUT is a ""GLUT""-like framework for Direct3D 11.x Win32 desktop applications; primarily samples, demos, and prototypes.","![DirectX Logo](https://raw.githubusercontent.com/wiki/Microsoft/DXUT/Dx_logo.GIF)\n\n# DXUT for Direct3D 11\n\nhttp://go.microsoft.com/fwlink/?LinkId=320437\n\nCopyright (c) Microsoft Corporation.\n\n**June 13, 2023**\n\nDXUT is a ""GLUT""-like framework for Direct3D 11.x Win32 desktop applications; primarily samples, demos, and prototypes.\n\nThis code is designed to build with Visual Studio 2019 (16.11) or Visual Studio 2022. Use of the Windows 10 May 2020 Update SDK ([19041](https://walbourn.github.io/windows-10-may-2020-update-sdk/)) or later is required.\n\nThese components are designed to work without requiring any content from the legacy DirectX SDK. For details, see [Where is the DirectX SDK?](https://aka.ms/dxsdk).\n\n*This project is 'archived'. It is still available for use for legacy projects or when using older developer education materials, but use of it for new projects is not recommended.*\n\n## Disclaimer\n\nDXUT is being provided as a porting aid for older code that makes use of the legacy DirectX SDK, the deprecated D3DX9/D3DX11 library, and the DXUT11 framework. It is a cleaned up version of the original DXUT11 that will build with the Windows 8.1 / 10 SDK and does not make use of any legacy DirectX SDK or DirectSetup deployed components.\n\nThe DXUT framework is for use in Win32 desktop applications. It not usable for Universal Windows Platform apps, Windows Store apps,\nXbox, or Windows phone.\n\nThis version of DXUT only supports Direct3D 11, and therefore is not compatible with Windows XP or early versions of Windows Vista.\n\n## Documentation\n\nDocumentation is available on the [GitHub wiki](https://github.com/Microsoft/DXUT/wiki).\n\n## Notices\n\nAll content and source code for this package are subject to the terms of the [MIT License](https://github.com/microsoft/DXUT/blob/main/LICENSE).\n\nFor the latest version of DXUT for Direct3D 11, please visit the project site on [GitHub](https://github.com/microsoft/DXUT).\n\n> The legacy versions of **DXUT for DX11/DX9** and **DXUT for DX10/DX9** version are on [GitHub](https://github.com/walbourn/directx-sdk-legacy-samples). These both require using [Microsoft.DXSDK.D3DX](https://www.nuget.org/packages/Microsoft.DXSDK.D3DX).\n\n## Release Notes\n\nFOR SECURITY ADVISORIES, see [GitHub](https://github.com/microsoft/DXUT/security/advisories).\n\nFor a full change history, see [CHANGELOG.md](https://github.com/microsoft/DXUT/blob/main/CHANGELOG.md).\n\n* Starting with the July 2022 release, the ``bool forceSRGB`` parameter for DDSTextureLoader ``Ex`` functions is now a ``DDS_LOADER_FLAGS`` typed enum bitmask flag parameter. This may have a *breaking change* impact to client code. Replace ``true`` with ``DDS_LOADER_FORCE_SRGB`` and ``false`` with ``DDS_LOADER_DEFAULT``.\n\n* There are known codegen issues when mixing the library built with Visual C++ and the sample built with recent builds of clang/LLVM for Windows. Building both the library and the sample using the same complier avoids the issue.\n\n## Support\n\nFor questions, consider using [Stack Overflow](https://stackoverflow.com/questions/tagged/dxut) with the *dxut* tag.\n\n## Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Credits\n\nThe DXUT library is the work of Shanon Drone, Jason Sandlin, and David Tuft with contributions from David Cook, Kev Gee, Matt Lee, and Chuck Walbourn.\n\n## Samples\n\n* Direct3D Tutorial08 - 10\n* BasicHLSL11, EmptyProject11, SimpleSample11\n* DXUT+DirectXTK Simple Sample\n\nThese are hosted on [GitHub](https://github.com/walbourn/directx-sdk-samples)\n",405,graphics,C++,3,C++,C,CMake,,,,,,,,,,,,,,,,,,,,,,,,,,23,0,23,0,1,4,0,1477,112,0,0,0,4a004a91650d1ca3f9091314713c64519ce16034,ADO pipeline changes for OpenSSF Best Practices (#32),2024-05-17T21:32:44Z,Chuck Walbourn,walbourn@users.noreply.github.com,walbourn,June 2023,* CMake project updates\r\n\r\nThis version is available from [vcpkg](https://github.com/microsoft/vcpkg/tree/master/ports/dxut).,Jun2023,Chuck Walbourn,,walbourn,MIT License,DXUT,microsoft,32,microsoft,directx,directx-11,graphics,framework,,,,,,,,,,,,,,,,/microsoft/DXUT,34,85,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/microsoft/DirectXTK12,https://github.com/microsoft/DirectXTK12,0,,,0,0,0,0,0,0,1,0,0,0,0,The DirectX Tool Kit (aka DirectXTK12) is a collection of helper classes for writing DirectX 12 code in C++,"![DirectX Logo](https://raw.githubusercontent.com/wiki/Microsoft/DirectXTK12/X_jpg.jpg)\n\n# DirectX Tool Kit for DirectX 12\n\nhttp://go.microsoft.com/fwlink/?LinkID=615561\n\nCopyright (c) Microsoft Corporation.\n\n**June 4, 2024**\n\nThis package contains the ""DirectX Tool Kit"", a collection of helper classes for writing Direct3D 12 C++ code for Universal Windows Platform (UWP) apps for Windows 11 and Windows 10, game titles for Xbox Series X\|S and Xbox One, and Win32 desktop applications for Windows 11 and Windows 10.\n\nThis code is designed to build with Visual Studio 2019 (16.11), Visual Studio 2022, clang for Windows v12 or later, or MinGW 12.2. Use of the Windows 10 May 2020 Update SDK ([19041](https://walbourn.github.io/windows-10-may-2020-update-sdk/)) or later is required for Visual Studio.\n\nThese components are designed to work without requiring any content from the legacy DirectX SDK. For details, see [Where is the DirectX SDK?](https://aka.ms/dxsdk).\n\n## Directory Layout\n\n* ``Inc\``\n\n  + Public Header Files (in the DirectX C++ namespace):\n\n    * Audio.h - low-level audio API using XAudio2 (DirectXTK for Audio public header)\n    * BufferHelpers.h - C++ helpers for creating D3D resources from CPU data\n    * CommonStates.h - common D3D state combinations\n    * DDSTextureLoader.h - light-weight DDS file texture loader\n    * DescriptorHeap.h - helper for managing DX12 descriptor heaps\n    * DirectXHelpers.h - misc C++ helpers for D3D programming\n    * EffectPipelineStateDescription.h - helper for creating PSOs\n    * Effects.h - set of built-in shaders for common rendering tasks\n    * GamePad.h - gamepad controller helper using Windows.Gaming.Input or GameInput\n    * GeometricPrimitive.h - draws basic shapes such as cubes and spheres\n    * GraphicsMemory.h - helper for managing dynamic graphics memory allocation\n    * Keyboard.h - keyboard state tracking helper\n    * Model.h - draws meshes loaded from .CMO, .SDKMESH, or .VBO files\n    * Mouse.h - mouse helper\n    * PostProcess.h - set of built-in shaders for common post-processing operations\n    * PrimitiveBatch.h - simple and efficient way to draw user primitives\n    * RenderTargetState.h - helper for communicating render target requirements when creating PSOs\n    * ResourceUploadBatch.h - helper for managing texture resource upload to the GPU\n    * ScreenGrab.h - light-weight screen shot saver\n    * SimpleMath.h - simplified C++ wrapper for DirectXMath\n    * SpriteBatch.h - simple & efficient 2D sprite rendering\n    * SpriteFont.h - bitmap based text rendering\n    * VertexTypes.h - structures for commonly used vertex data formats\n    * WICTextureLoader.h - WIC-based image file texture loader\n    * XboxDDSTextureLoader.h - Xbox exclusive apps variant of DDSTextureLoader\n\n* ``Src\``\n\n  + DirectXTK source files and internal implementation headers\n\n* ``Audio\``\n\n  + DirectXTK for Audio source files and internal implementation headers\n\n* ``build\``\n\n  + Contains YAML files for the build pipelines along with some miscellaneous build files and scripts.\n\n> MakeSpriteFont and XWBTool can be found in the [DirectX Tool Kit for DirectX 11](https://github.com/microsoft/DirectXTK)\n\n## Documentation\n\nDocumentation is available on the [GitHub wiki](https://github.com/Microsoft/DirectXTK12/wiki).\n\n## Notices\n\nAll content and source code for this package are subject to the terms of the [MIT License](https://github.com/microsoft/DirectXTK12/blob/main/LICENSE).\n\nFor the latest version of DirectXTK12, bug reports, etc. please visit the project site on [GitHub](https://github.com/microsoft/DirectXTK12).\n\n## Comparisons to DirectX 11 Version\n\n* No support for Visual Studio Directed Graph Shader Language (DGSL) effect shaders (i.e. *DGSLEffect*). CMO files are loaded using BasicEffect or SkinnedEffect materials.\n\n* VertexTypes does not include VertexPositionNormalTangentColorTexture or VertexPositionNormalTangentColorTextureSkinning which were intended for use with the DGSL pipeline.\n\n* DirectX Tool Kit for DirectX 11 supports Feature Level 9.x, while DirectX 12 requires Direct3D Feature Level 11.0. There are no expected DirectX 12 drivers for any lower feature level devices.\n\n* The library assumes it is building for Windows 10 (aka ``_WIN32_WINNT=0x0A00``) so it makes use of XAudio 2.9 and WIC2 as well as DirectX 12.\n\n* DirectX Tool Kit for Audio, GamePad, Keyboard, Mouse, and SimpleMath are identical to the DirectX 11 version.\n\n## Release Notes\n\nFOR SECURITY ADVISORIES, see [GitHub](https://github.com/microsoft/DirectXTK12/security/advisories).\n\nFor a full change history, see [CHANGELOG.md](https://github.com/microsoft/DirectXTK12/blob/main/CHANGELOG.md).\n\n* In the June 2024 release, the defaulted parameter `initialState` for the ``CreateUploadBuffer`` function in *BufferHelpers* was removed. Per the DirectX 12 validation layer, the only valid initial state for an upload buffer is ``D3D12_RESOURCE_STATE_GENERIC_READ``.\n\n* Starting with the February 2023 release, the Mouse class implementation of relative mouse movement was updated to accumulate changes between calls to ``GetState``. By default, each time you call ``GetState`` the deltas are reset which works for scenarios where you use relative movement but only call the method once per frame. If you call it more than once per frame, then add an explicit call to ``EndOfInputFrame`` to use an explicit reset model instead.\n\n* As of the September 2022 release, the library makes use of C++11 inline namespaces for differing types that have the same names in the DirectX 11 and DirectX 12 version of the *DirectX Tool Kit*. This provides a link-unique name such as ``DirectX::DX12::SpriteBatch`` that will appear in linker output messages. In most use cases, however, there is no need to add explicit ``DX12`` namespace resolution in client code.\n\n* Starting with the June 2021 release, this library builds the HLSL shaders with Shader Model 6 via DXC. See [this wiki page](https://github.com/microsoft/DirectXTK12/wiki/Shader-Model-6) for more information. The Microsoft GDK projects have always used Shader Model 6.\n\n* Starting with the June 2020 release, this library makes use of [typed enum bitmask flags](https://walbourn.github.io/modern-c++-bitmask-types/) per the recommendation of the _C++ Standard_ section *17.5.2.1.3 Bitmask types*. This may have *breaking change* impacts to client code:\n\n  * You cannot pass the ``0`` literal as your flags value. Instead you must make use of the appropriate default enum value: ``AudioEngine_Default``, ``SoundEffectInstance_Default``, ``ModelLoader_Clockwise``, ``DDS_LOADER_DEFAULT``, or ``WIC_LOADER_DEFAULT``.\n\n  * Use the enum type instead of ``DWORD`` if building up flags values locally with bitmask operations. For example, ```WIC_LOADER_FLAGS flags = WIC_LOADER_DEFAULT; if (...) flags |= WIC_LOADER_FORCE_SRGB;```\n\n* The UWP projects and the Win10 classic desktop project include configurations for the ARM64 platform. Building these requires installing the ARM64 toolset.\n\n* When using clang/LLVM for the ARM64 platform, the Windows 11 SDK ([22000](https://walbourn.github.io/windows-sdk-for-windows-11/)) or later is required.\n\n* The ``CompileShaders.cmd`` script must have Windows-style (CRLF) line-endings. If it is changed to Linux-style (LF) line-endings, it can fail to build all the required shaders.\n\n## Support\n\nFor questions, consider using [Stack Overflow](https://stackoverflow.com/questions/tagged/directxtk) with the *directxtk* tag, or the [DirectX Discord Server](https://discord.gg/directx) in the *dx12-developers* channel.\n\nFor bug reports and feature requests, please use GitHub [issues](https://github.com/microsoft/DirectXTK12/issues) for this project.\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nTests for new features should also be submitted as a PR to the [Test Suite](https://github.com/walbourn/directxtk12test/wiki) repository.\n\n## Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more informatsion see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Credits\n\nThe _DirectX Tool Kit for DirectX 11_ is the work of Shawn Hargreaves and Chuck Walbourn, with contributions from Aaron Rodriguez Hernandez and Dani Roman.\n\nThe _DirectX Tool Kit for DirectX 12_ is the work of Pete Lewis, Justin Saunders, and Chuck Walbourn based heavily on the DirectX Tool Kit for DirectX 11.\n\nThanks to Shanon Drone for the SDKMESH file format.\n\nThanks to Adrian Tsai for the geodesic sphere implementation.\n\nThanks to Garrett Serack for his help in creating the NuGet packages for DirectX Tool Kit.\n\nThanks to Pete Lewis and Justin Saunders for the normal-mapped and PBR shaders implementation.\n\nThanks for Travis Johnson for the mGPU support.\n\nThanks to Roberto Sonnino for his help with the CMO format and the VS Starter Kit animation.\n\nThanks to Richie Meyer for their contribution of Xbox PIX custom memory and type allocation tracking events support.\n\nThanks to Andrew Farrier and Scott Matloff for their on-going help with code reviews.\n",1452,graphics,C++,7,C++,HLSL,Batchfile,C,CMake,POV-Ray SDL,PowerShell,,,,,,,,,,,,,,,,,,,,,,149,4,145,0,2,14,0,7434,371,85,75,10,0260a8fa91be0a999de6116c8f843f577e75d674,CMake updated to build ARM64EC (#238),2024-07-18T22:29:04Z,Chuck Walbourn,walbourn@users.noreply.github.com,walbourn,June 2024,* *breaking change* `CreateUploadBuffer` helper no longer takes initialState parameter as it must be a specific value\r\n* Renamed Internal namespace to ToolKitInternal for some conformance issues\r\n* Updated D3DX12 internal copy with latest changes from DirectX-Headers GitHub\r\n* Add `c_initialRead/UAVTargetState` to help with PC vs. Xbox validation warnings\r\n* CMake project updates\r\n* Retired VS 2019 projects for the UWP platform\r\n\r\nThis version is also available on NuGet as version 2024.6.5\r\n- [Windows desktop app using VS 2019 or VS 2022](https://www.nuget.org/packages/directxtk12_desktop_2019/2024.6.5.1)\r\n- [Universal Windows Platform apps using VS 2019 or VS 2022](https://www.nuget.org/packages/directxtk12_uwp/2024.6.5.1)\r\n\r\nThis version is available via [vcpkg](https://github.com/microsoft/vcpkg/tree/master/ports/directxtk12).,Jun2024,Chuck Walbourn,,walbourn,MIT License,DirectXTK12,microsoft,71,microsoft,directx,directx-12,cpp-library,graphics,xbox,uwp,directxtk,shaders,desktop,,,,,,,,,,,/microsoft/DirectXTK12,75,107,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/microsoft/DirectXTK,https://github.com/microsoft/DirectXTK,0,,,0,0,0,0,0,0,1,1,0,0,0,The DirectX Tool Kit (aka DirectXTK) is a collection of helper classes for writing DirectX 11.x code in C++,"![DirectX Logo](https://raw.githubusercontent.com/wiki/Microsoft/DirectXTK/X_jpg.jpg)\n\n# DirectX Tool Kit for DirectX 11\n\nhttp://go.microsoft.com/fwlink/?LinkId=248929\n\nCopyright (c) Microsoft Corporation.\n\n**June 4, 2024**\n\nThis package contains the ""DirectX Tool Kit"", a collection of helper classes for writing Direct3D 11 C++ code for Universal Windows Platform (UWP) apps for Windows 11, Windows 10, Xbox One, and Win32 desktop applications for Windows 7 Service Pack 1 or later.\n\nThis code is designed to build with Visual Studio 2019 (16.11), Visual Studio 2022, clang for Windows v12 or later, or MinGW 12.2. Use of the Windows 10 May 2020 Update SDK ([19041](https://walbourn.github.io/windows-10-may-2020-update-sdk/)) or later is required for Visual Studio.\n\nThese components are designed to work without requiring any content from the legacy DirectX SDK. For details, see [Where is the DirectX SDK?](https://aka.ms/dxsdk).\n\n## Directory Layout\n\n* ``Inc\``\n\n  + Public Header Files (in the DirectX C++ namespace):\n\n    * Audio.h - low-level audio API using XAudio2 (DirectXTK for Audio public header)\n    * BufferHelpers.h - C++ helpers for creating D3D resources from CPU data\n    * CommonStates.h - factory providing commonly used D3D state objects\n    * DDSTextureLoader.h - light-weight DDS file texture loader\n    * DirectXHelpers.h - misc C++ helpers for D3D programming\n    * Effects.h - set of built-in shaders for common rendering tasks\n    * GamePad.h - gamepad controller helper using XInput, Windows.Gaming.Input, or GameInput\n    * GeometricPrimitive.h - draws basic shapes such as cubes and spheres\n    * GraphicsMemory.h - helper for managing dynamic graphics memory allocation\n    * Keyboard.h - keyboard state tracking helper\n    * Model.h - draws meshes loaded from .CMO, .SDKMESH, or .VBO files\n    * Mouse.h - mouse helper\n    * PostProcess.h - set of built-in shaders for common post-processing operations\n    * PrimitiveBatch.h - simple and efficient way to draw user primitives\n    * ScreenGrab.h - light-weight screen shot saver\n    * SimpleMath.h - simplified C++ wrapper for DirectXMath\n    * SpriteBatch.h - simple & efficient 2D sprite rendering\n    * SpriteFont.h - bitmap based text rendering\n    * VertexTypes.h - structures for commonly used vertex data formats\n    * WICTextureLoader.h - WIC-based image file texture loader\n    * XboxDDSTextureLoader.h - Xbox One exclusive apps variant of DDSTextureLoader\n\n* ``Src\``\n\n  + DirectXTK source files and internal implementation headers\n\n* ``Audio\``\n\n  + DirectXTK for Audio source files and internal implementation headers\n\n* ``MakeSpriteFont\``\n\n  + Command line tool used to generate binary resources for use with SpriteFont\n\n* ``XWBTool\``\n\n  +  Command line tool for building XACT-style wave banks for use with DirectXTK for Audio's WaveBank class\n\n* ``build\``\n\n  + Contains YAML files for the build pipelines along with some miscellaneous build files and scripts.\n\n## Documentation\n\nDocumentation is available on the [GitHub wiki](https://github.com/Microsoft/DirectXTK/wiki).\n\n## Notices\n\nAll content and source code for this package are subject to the terms of the [MIT License](https://github.com/microsoft/DirectXTK/blob/main/LICENSE).\n\nFor the latest version of DirectXTK, bug reports, etc. please visit the project site on [GitHub](https://github.com/microsoft/DirectXTK).\n\n## Release Notes\n\nFOR SECURITY ADVISORIES, see [GitHub](https://github.com/microsoft/DirectXTK/security/advisories).\n\nFor a full change history, see [CHANGELOG.md](https://github.com/microsoft/DirectXTK/blob/main/CHANGELOG.md).\n\n* Starting with the February 2023 release, the Mouse class implementation of relative mouse movement was updated to accumulate changes between calls to ``GetState``. By default, each time you call ``GetState`` the deltas are reset which works for scenarios where you use relative movement but only call the method once per frame. If you call it more than once per frame, then add an explicit call to ``EndOfInputFrame`` to use an explicit reset model instead.\n\n* As of the September 2022 release, the library makes use of C++11 inline namespaces for differing types that have the same names in the DirectX 11 and DirectX 12 version of the *DirectX Tool Kit*. This provides a link-unique name such as ``DirectX::DX11::SpriteBatch`` that will appear in linker output messages. In most use cases, however, there is no need to add explicit ``DX11`` namespace resolution in client code.\n\n* Starting with the July 2022 release, the ``bool forceSRGB`` parameter for DDSTextureLoader ``Ex`` functions is now a ``DDS_LOADER_FLAGS`` typed enum bitmask flag parameter. This may have a *breaking change* impact to client code. Replace ``true`` with ``DDS_LOADER_FORCE_SRGB`` and ``false`` with ``DDS_LOADER_DEFAULT``.\n\n* As of the October 2021 release, the DGSLEffect no longer directly supports skinning. Instead, make use of **SkinnedDGSLEffect** which is derived from DGSLEffect.\n\n* Starting with the June 2020 release, this library makes use of [typed enum bitmask flags](https://walbourn.github.io/modern-c++-bitmask-types/) per the recommendation of the _C++ Standard_ section *17.5.2.1.3 Bitmask types*. This may have *breaking change* impacts to client code:\n\n  * You cannot pass the ``0`` literal as your flags value. Instead you must make use of the appropriate default enum value: ``AudioEngine_Default``, ``SoundEffectInstance_Default``, ``ModelLoader_Clockwise``, or ``WIC_LOADER_DEFAULT``.\n\n  * Use the enum type instead of ``DWORD`` if building up flags values locally with bitmask operations. For example, ```WIC_LOADER_FLAGS flags = WIC_LOADER_DEFAULT; if (...) flags |= WIC_LOADER_FORCE_SRGB;```\n\n* The UWP projects and the Win10 classic desktop project include configurations for the ARM64 platform. Building these requires installing the ARM64 toolset.\n\n* When using clang/LLVM for the ARM64 platform, the Windows 11 SDK ([22000](https://walbourn.github.io/windows-sdk-for-windows-11/)) or later is required.\n\n* The ``CompileShaders.cmd`` script must have Windows-style (CRLF) line-endings. If it is changed to Linux-style (LF) line-endings, it can fail to build all the required shaders.\n\n* Xbox One support for DirectX 11 requires the legacy Xbox One XDK. See February 2023 or earlier releases of *DirectX Tool Kit* for the required project files.\n\n## Support\n\nFor questions, consider using [Stack Overflow](https://stackoverflow.com/questions/tagged/directxtk) with the *directxtk* tag, or the [DirectX Discord Server](https://discord.gg/directx) in the *dx9-dx11-developers* channel.\n\nFor bug reports and feature requests, please use GitHub [issues](https://github.com/microsoft/DirectXTK/issues) for this project.\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nTests for new features should also be submitted as a PR to the [Test Suite](https://github.com/walbourn/directxtktest/wiki) repository.\n\n## Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Credits\n\nThe _DirectX Tool Kit_ is the work of Shawn Hargreaves and Chuck Walbourn, with contributions from Aaron Rodriguez Hernandez and Dani Roman.\n\nThanks to Shanon Drone for the SDKMESH file format.\n\nThanks to Adrian Tsai for the geodesic sphere implementation.\n\nThanks to Garrett Serack for his help in creating the NuGet packages for DirectX Tool Kit.\n\nThanks to Roberto Sonnino for his help with the ``CMO``, DGSL rendering, and the VS Starter Kit animation.\n\nThanks to Pete Lewis and Justin Saunders for the normal-mapped and PBR shaders implementation.\n\nThanks to Andrew Farrier and Scott Matloff for their on-going help with code reviews.",2524,graphics,C++,7,C++,C#,Batchfile,HLSL,CMake,PowerShell,POV-Ray SDL,,,,,,,,,,,,,,,,,,,,,,155,17,138,0,2,13,0,9043,498,299,266,33,1877bba2289df04f89f5a0318aecdfa463d246d8,CMake updated to build ARM64EC (#459),2024-07-18T22:29:13Z,Chuck Walbourn,walbourn@users.noreply.github.com,walbourn,June 2024,* Renamed Internal namespace to ToolKitInternal for some conformance issues\r\n* CMake project updates\r\n* Retired VS 2019 projects for the UWP platform\r\n\r\nThis version is also available on NuGet as version 2024.6.5\r\n- [Windows desktop app using VS 2019 or VS 2022](https://www.nuget.org/packages/directxtk_desktop_2019/2024.6.5.1)\r\n- [Windows desktop app using VS 2019 or VS 2022 for Windows 10](https://www.nuget.org/packages/directxtk_desktop_win10/2024.6.5.1)\r\n- [Universal Windows Platform apps using VS 2019 or VS 2022](https://www.nuget.org/packages/directxtk_uwp/2024.6.5.1)\r\n\r\nThis version is available via [vcpkg](https://github.com/microsoft/vcpkg/tree/master/ports/directxtk).\r\n,Jun2024,Chuck Walbourn,,walbourn,MIT License,DirectXTK,microsoft,102,microsoft,directx,directx-11,cpp-library,graphics,xbox,directxtk,uwp,desktop,,,,,,,,,,,,/microsoft/DirectXTK,105,199,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/microcharts-dotnet/Microcharts,https://github.com/microcharts-dotnet/Microcharts,0,,,0,0,0,0,0,0,1,1,0,0,0,"Create cross-platform (Xamarin, Windows, ...) simple charts.","# Microcharts\n\n[![Mac Builds](https://github.com/dotnet-ad/Microcharts/actions/workflows/CI-MacOS.yml/badge.svg)](https://github.com/dotnet-ad/Microcharts/actions/workflows/CI-MacOS.yml)\n[![Windows Builds](https://github.com/dotnet-ad/Microcharts/actions/workflows/CI-Windows.yml/badge.svg)](https://github.com/dotnet-ad/Microcharts/actions/workflows/CI-Windows.yml)\n\n[Looking for more contributors](https://github.com/microcharts-dotnet/Microcharts/discussions/274)\n\n## Version 1.0.0 Beta is now available\n\n[![NuGet](https://img.shields.io/nuget/vpre/Microcharts.Forms.svg?label=Microcharts.Forms)](https://www.nuget.org/packages/Microcharts.Forms/)\n\n[![NuGet](https://img.shields.io/nuget/v/Microcharts.Android.svg?label=Microcharts.Android)](https://www.nuget.org/packages/Microcharts.Android/)\n\n[![NuGet](https://img.shields.io/nuget/v/Microcharts.iOS.svg?label=Microcharts.iOS)](https://www.nuget.org/packages/Microcharts.iOS/)\n\n[![NuGet](https://img.shields.io/nuget/v/Microcharts.Mac.svg?label=Microcharts.Mac)](https://www.nuget.org/packages/Microcharts.Mac/)\n\n[![NuGet](https://img.shields.io/nuget/v/Microcharts.Uwp.svg?label=Microcharts.Uwp)](https://www.nuget.org/packages/Microcharts.Uwp/)\n\n\n**Microcharts** is an extremely simple charting library for a wide range of platforms (see *Compatibility* section below), with shared code and rendering for all of them!\n\nread our [wiki](https://github.com/dotnet-ad/Microcharts/wiki) to learn more about how to use this library.\n\n## About\n\nThis project is just simple drawing on top of the awesome [SkiaSharp](https://github.com/mono/SkiaSharp) library. The purpose is not to have an heavily customizable charting library. If you want so, simply fork the code, since all of this is fairly simple. Their is no interaction, nor animation at the moment.\n\n## Contributions\n\nContributions are welcome! If you find a bug please report it and if you want a feature please report it.\n\nIf you want to contribute code please file an issue and create a branch off of the current dev branch and file a pull request.\n\nMore info on how you can help can be found [here](https://github.com/dotnet-ad/Microcharts/wiki/Contributing).\n\n## Gallery\n\n![animation gallery](assets/animations.gif)\n\n![gallery](assets/Gallery.png)\n\n## Install\n\nAvailable on NuGet\n\n**NET Standard 2.0, Xamarin.iOS, Xamarin.Android, UWP**\n\n[![NuGet](https://img.shields.io/nuget/v/Microcharts.svg?label=NuGet)](https://www.nuget.org/packages/Microcharts/)\n\n\n**Xamarin.Forms (.NET Standard 2.0)**\n\n[![NuGet](https://img.shields.io/nuget/v/Microcharts.Forms.svg?label=NuGet)](https://www.nuget.org/packages/Microcharts.Forms/)\n\n**.NET MAUI**\n\nNot yet available through NuGet.\n\n> [!IMPORTANT]\n> Don't forget to call `UseMicrocharts()` on `MauiAppBuilder` in the `MauiProgram` class.\n\n## Tutorials\n\n* [Video: Charts for Xamarin Forms](https://www.youtube.com/watch?v=tmymWdmf1y4) by [@HoussemDellai](https://github.com/HoussemDellai)\n\n## Compatibility\n\nBuilt in views are provided for:\n\n* **UWP**\n* **Xamarin.Forms**, **Xamarin.iOS** and **Xamarin.Android**, **Xamarin.macOS**\n* **.NET MAUI** (Windows, Android, iOS, and macOS)\n* **WinUI** (Windows App SDK)\n* And any other **.NET Standard 2.0** [SkiaSharp](https://github.com/mono/SkiaSharp) supported platform is also compatible (see one of the included `ChartView` implementations for more details).\n\n## License\n\nMIT © [Aloïs Deniel](https://aloisdeniel.com) & [Ed Lomonaco](https://edlomonaco.dev)\n",1995,graphics,C#,2,C#,Batchfile,,,,,,,,,,,,,,,,,,,,,,,,,,,75,31,41,3,1,24,0,11471,358,251,157,94,fdf876be33746c83633ad2fd1175bb6d202a055e,Merge pull request #337 from Eilon/main,2024-05-01T03:54:14Z,Ed Lomonaco,elomonaco127@gmail.com,eman1986,0.9.5.9,Added the ability to  Change title color of value #210,0.9.5.9,Ed Lomonaco,,eman1986,MIT License,Microcharts,microcharts-dotnet,4,xamarin,skia,charts,ios,android,uwp,graphics,,,,,,,,,,,,,,/microcharts-dotnet/Microcharts,6,94,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/MFlowCode/MFC,https://github.com/MFlowCode/MFC,1,,,1,1,1,1,0,0,0,0,0,0,1,Exascale simulation of multiphase/physics fluid dynamics,"<p align=""center"">\n  <a href=""http://mflowcode.github.io/"">\n    <img src=""docs/res/readme.png"" alt=""MFC Banner"" width=""500""/>\n  </a>\n</p>\n\n<p align=""center"">\n  <a href=""http://dx.doi.org/10.1016/j.cpc.2020.107396"" target=""_blank"">\n    <img src=""https://zenodo.org/badge/doi/10.1016/j.cpc.2020.107396.svg"" />\n  </a>\n  <a href=""https://github.com/MFlowCode/MFC/actions"">\n    <img src=""https://github.com/MFlowCode/MFC/actions/workflows/test.yml/badge.svg"" />\n  </a>\n  <a href=""https://join.slack.com/t/mflowcode/shared_invite/zt-y75wibvk-g~zztjknjYkK1hFgCuJxVw"">\n    <img src=""https://img.shields.io/badge/slack-MFC-purple.svg?logo=slack"" />\n  </a>\n  <a href=""https://lbesson.mit-license.org/"">\n    <img src=""https://img.shields.io/badge/License-MIT-blue.svg"" />\n  </a>\n  <a href=""https://codecov.io/github/MFlowCode/MFC"" target=""_blank"">\n    <img src=""https://codecov.io/github/MFlowCode/MFC/graph/badge.svg?token=8SY043QND4"">\n  </a>\n</p>\n\nWelcome to the home of MFC!\nMFC simulates compressible multi-component and multi-phase flows, [amongst other things](#what-else-can-this-thing-do). \nMFC is written in Fortran and makes use of metaprogramming to keep the code short (about 20K lines).\n\nMFC is used on the latest leadership-class supercomputers.\nIt scales <b>ideally to exascale</b>; [tens of thousands of GPUs on NVIDIA- and AMD-GPU machines](#is-this-really-exascale) on Oak Ridge Summit and Frontier.\nMFC is a SPEChpc benchmark candidate, part of the JSC JUPITER Early Access Program, and made use of OLCF Frontier and LLNL El Capitan early access systems.\n  \nGet in touch with <a href=""mailto:shb@gatech.edu"">Spencer</a> if you have questions!\nWe have an [active Slack channel](https://join.slack.com/t/mflowcode/shared_invite/zt-y75wibvk-g~zztjknjYkK1hFgCuJxVw) and development team.\nMFC has high-level documentation, visualizations, and more on [its website](https://mflowcode.github.io/).\n\n## An example\n\nWe keep many examples.\nHere's one!\nMFC can execute high-fidelity simulations of shock-droplet interaction (see `examples/3d_shockdroplet`)\n\n<p align=""center"">\n    <img src=""docs/res/shockdrop.png"" alt=""Shock Droplet Example"" width=""700""/>\n</p>\n\nAnother example is the high-Mach flow over an airfoil, shown below.\n\n<p align=""center"">\n    <img src=""docs/res/airfoil.png"" alt=""Airfoil Example"" width=""700""/><br/>\n</p>\n\n\n## Getting started\n\nYou can navigate [to this webpage](https://mflowcode.github.io/documentation/md_getting-started.html) to get started using MFC!\nIt's rather straightforward.\nWe'll give a brief intro. here for MacOS.\nUsing [brew](https://brew.sh), install MFC's modest set of dependencies:\n```shell\nbrew install wget python cmake gcc@14 mpich\n```\nYou're now ready to build and test MFC!\nPut it to a convenient directory via\n```shell\ngit clone https://github.com/MFlowCode/MFC\ncd MFC\n```\nand be sure MFC knows what compilers to use by appending and sourcing your `~/.profile` file via this command\n```shell\necho -e ""export CC=gcc-14 \nexport CXX=g++-14 \nexport FC=gfortran-14"" >> ~/.profile\nsource ~/.profile\n```\nthen you can build MFC and run the test suite!\n```shell\n./mfc.sh build -j $(nproc)\n./mfc.sh test -j $(nproc)\n```\nAnd... you're done!\n\nYou can learn more about MFC's capabilities [via its documentation](https://mflowcode.github.io/documentation/index.html) or play with the examples located in the `examples/` directory (some are [shown here](https://mflowcode.github.io/documentation/md_examples.html))!\n\nThe shock-droplet interaction case above was run via\n```shell\n./mfc.sh run ./examples/3d_shockdroplet/case.py -n 8\n```\nwhere `8` is the number of cores the example will run on.\nYou can visualize the output data in `examples/3d_shockdroplet/silo_hdf5` via Paraview, Visit, or your other favorite software.\n\n## Is this _really_ exascale?\n\n[OLCF Frontier](https://www.olcf.ornl.gov/frontier/) is the first exascale supercomputer.\nThe weak scaling of MFC on this machine is below, showing near-ideal utilization. \n\n<p align=""center"">\n    <img src=""docs/res/scaling.png"" alt=""Scaling"" width=""400""/>\n</p>\n\n\n## What else can this thing do\n\nMFC has many features.\nThey are organized below. Just click the drop-downs!\n\n<details>\n<summary>Physics</summary>\n\n* 1-3D\n* Compressible\n	* Low Mach number treatment available\n* Multi- and single-component\n	* 4, 5, and 6 equation models for multi-component/phase features\n   	* Kapila and Allaire models\n* Multi- and single-phase \n	* Phase change via p, pT, and pTg schemes\n* Grids\n	* 1-3D Cartesian, cylindrical, axisymmetric. \n	* Arbitrary grid stretching for multiple domain regions.\n	* Complex/arbitrary geometries via immersed boundary methods \n	* STL geometry files supported\n* Surface tension for multiphase cases\n* Sub-grid Euler-Euler multiphase models for bubble dynamics and similar\n* Viscous effects (high-order accurate representations)\n* Ideal and stiffened gas equations of state\n* Body forces\n* Acoustic wave generation (one- and two-way sound sources)\n</details>\n\n<details>\n<summary>Numerics</summary>\n\n* Shock and interface capturing schemes\n	* First-order upwinding\n 	* WENO reconstructions of order 3 and 5\n  	* WENO variants: WENO-JS, WENO-M, WENO-Z, TENO\n   	* Monotonicity-preserving reconstructions\n	* Reliable handling of high density ratios\n* Exact and approximate (e.g., HLL, HLLC) Riemann solvers\n* Boundary conditions: Periodic, reflective, extrapolation/Neumann, slip/no-slip, non-reflecting characteristic buffers, inflows, outflows, and more\n* Runge-Kutta orders 1-3 (SSP TVD)\n* Interface sharpening (THINC-like)\n</details>\n\n<details>\n<summary>Large-scale and accelerated simulation</summary>\n\n* GPU compatible on NVIDIA (P/V/A/H100, GH200, etc.) and AMD (MI200+) hardware\n* Ideal weak scaling to 100% of the largest GPU supercomputers\n	* \>10K NVIDIA GPUs on [OLCF Summit](https://www.olcf.ornl.gov/summit/) (NV V100-based)\n	* \>66K AMD GPUs on the first exascale computer, [OLCF Frontier](https://www.olcf.ornl.gov/frontier/) (AMD MI250X-based)\n* Near compute roofline behavior\n* RDMA (remote data memory access; GPU-GPU direct communication) via GPU-aware MPI on NVIDIA (CUDA-aware MPI) and AMD GPU systems\n</details>\n\n<details>\n<summary>Software robustness and other features</summary>\n\n* [Fypp](https://fypp.readthedocs.io/en/stable/fypp.html) metaprogramming for code readability, performance, and portability\n* Continuous Integration (CI)\n	* \>100 Regression tests with each PR.\n 		* Performed with GNU, Intel, and NVIDIA compilers on NVIDIA and AMD GPUs.\n		* Line-level test coverage reports via [Codecov](https://app.codecov.io/gh/MFlowCode/MFC) and `gcov`\n	* Benchmarking to avoid performance regressions and identify speed-ups\n* Continuous Deployment (CD) of [website](https://mflowcode.github.io) and [API documentation](https://mflowcode.github.io/documentation/index.html)\n</details>\n\n\n## Citation\n\nIf you use MFC, consider citing it as:\n\n<p align=""center"">\n  <a href=""https://doi.org/10.1016/j.cpc.2020.107396"">\n    S. H. Bryngelson, K. Schmidmayer, V. Coralic, K. Maeda, J. Meng, T. Colonius (2021) Computer Physics Communications <b>266</b>, 107396\n  </a>\n</p>\n\n```bibtex\n@article{Bryngelson_2021,\n  title   = {{MFC: A}n open-source high-order multi-component, multi-phase, and multi-scale compressible flow solver},\n  author  = {S. H. Bryngelson and K. Schmidmayer and V. Coralic and J. C. Meng and K. Maeda and T. Colonius},\n  journal = {Computer Physics Communications},\n  year    = {2021},\n  volume  = {266},\n  pages   = {107396},\n  doi     = {10.1016/j.cpc.2020.107396}\n}\n```\n\n```bibtex\n@article{Radhakrishnan_2024,\n  title   = {Method for portable, scalable, and performant {GPU}-accelerated simulation of multiphase compressible flow},\n  author  = {A. Radhakrishnan and H. {Le Berre} and B. Wilfong and J.-S. Spratt and M. {Rodriguez Jr.} and T. Colonius and S. H. Bryngelson},\n  journal = {Computer Physics Communications},\n  year    = {2024},\n  volume  = {302},\n  pages   = {109238},\n  doi     = {10.1016/j.cpc.2024.109238}\n}\n```\n\n## License\n \nCopyright 2021-2024 Spencer Bryngelson and Tim Colonius.\nMFC is under the MIT license (see [LICENSE](LICENSE) for full text).\n\n## Acknowledgements\n\nMultiple federal sponsors have supported MFC development, including the US Department of Defense (DOD), National Institutes of Health (NIH), Department of Energy (DOE), and National Science Foundation (NSF).\n\nMFC computations have used many supercomputing systems. A partial list is below\n  * OLCF Frontier and Summit, and testbed systems Wombat, Crusher, and Spock (allocation CFD154, PI Bryngelson)\n  * PSC Bridges(1/2), NCSA Delta, SDSC Comet and Expanse, Purdue Anvil, TACC Stampede(1-3), and TAMU ACES via ACCESS-CI (allocations TG-CTS120005 (PI Colonius) and TG-PHY210084 (PI Bryngelson))\n  * DOD systems Onyx, Carpenter, and Nautilus via the DOD HPCMP program\n  * Sandia National Labs systems Doom and Attaway and testbed systems Weaver and Vortex\n",133,hpc-applications,Fortran,8,Python,Shell,MATLAB,Fortran,CMake,Batchfile,Dockerfile,Mako,,,,,,,,,,,,,,,,,,,,,309,37,267,5,1,19,0,473586,58,219,182,37,42cfdf0b43a259f3388ff9592b95dde3d2c3437f,Update README.md (#528),2024-07-18T22:22:38Z,Spencer Bryngelson,sbryngelson@gmail.com,sbryngelson,MFC v4.9.3,## What's Changed\r\n* Documentation typos by @ChrisZYJ in https://github.com/MFlowCode/MFC/pull/508\r\n* don't benchmark if no code is changed by @sbryngelson in https://github.com/MFlowCode/MFC/pull/510\r\n* Satiate spellchecker by @sbryngelson in https://github.com/MFlowCode/MFC/pull/511\r\n* Remove paths from test.yml by @okBrian in https://github.com/MFlowCode/MFC/pull/512\r\n* Simplify `./mfc.sh clean` to behave as most people expect. by @AiredaleDev in https://github.com/MFlowCode/MFC/pull/514\r\n* Extend time-out limit by @sbryngelson in https://github.com/MFlowCode/MFC/pull/516\r\n* Improve Only Flag for Test by @okBrian in https://github.com/MFlowCode/MFC/pull/518\r\n* Update on adaptive time stepping for sub-grid bubbles by @lee-hyeoksu in https://github.com/MFlowCode/MFC/pull/408\r\n* Fail a CI self-hosted job if frontier doesn't get a node by @sbryngelson in https://github.com/MFlowCode/MFC/pull/517\r\n* Fix 3D IBM Infinite CFL Number on GPUs by @Sam-Briney in https://github.com/MFlowCode/MFC/pull/519\r\n* Including DoD Nautilus in the list of computers for MFC by @JRChreim in https://github.com/MFlowCode/MFC/pull/523\r\n* Fully simplify and fix `mfc.sh clean` by @AiredaleDev in https://github.com/MFlowCode/MFC/pull/524\r\n* Fixes `<string>:1: SyntaxWarning: invalid escape sequence '\('` by @sbryngelson in https://github.com/MFlowCode/MFC/pull/526\r\n* Update modules for Carpenter by @lee-hyeoksu in https://github.com/MFlowCode/MFC/pull/527\r\n* Update README.md by @sbryngelson in https://github.com/MFlowCode/MFC/pull/528\r\n\r\n## New Contributors\r\n* @Sam-Briney made their first contribution in https://github.com/MFlowCode/MFC/pull/519\r\n\r\n**Full Changelog**: https://github.com/MFlowCode/MFC/compare/v4.9.2...v4.9.3,v4.9.3,Spencer Bryngelson,,sbryngelson,MIT License,MFC,MFlowCode,33,multiphase-flow,computational-fluid-dynamics,gpu,openacc,hpc-applications,exascale,fortran,amdgpu,instinct,nvidia-gpu,compressible-fluid-dynamics,,,,,,,,,,/MFlowCode/MFC,33,12,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/MethanePowered/MethaneKit,https://github.com/MethanePowered/MethaneKit,0,,,0,0,0,0,0,0,1,1,0,0,0,"🎲 Modern 3D graphics made simple with C++17 cross-platform framework and rendering abstraction API on top of DirectX 12, Metal & Vulkan","# Methane Kit <img src=""https://github.com/MethanePowered/MethaneKit/blob/master/Docs/Images/Logo/MethaneLogoNameSmall.png"" width=200 align=""right"" valign=""middle"">\n\n[![CI Build](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-build.yml/badge.svg)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-build.yml)\n[![CI CodeQL Scan](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-codeql-scan.yml/badge.svg)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-codeql-scan.yml)\n[![CI Sonar Scan](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-sonar-scan.yml/badge.svg)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-sonar-scan.yml)\n[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=methane-powered-kit-windows&metric=alert_status)](https://sonarcloud.io/dashboard?id=methane-powered-kit-windows)\n[![CodeCov](https://codecov.io/gh/MethanePowered/MethaneKit/branch/master/graph/badge.svg?token=VBY74VXWX5)](https://codecov.io/gh/MethanePowered/MethaneKit)\n[![Test Space Metric](https://methanepowered.testspace.com/spaces/190760/badge?token=196ea878d56f2b034a8d29a9b4ba6a94c968643c)](https://methanepowered.testspace.com/spaces/190760?utm_campaign=badge&utm_medium=referral&utm_source=test ""Test Cases"")\n[![Gitpod Ready-to-Code](https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/MethanePowered/MethaneKit)\n\n**Easy to use modern 3D graphics rendering abstraction API and cross-platform application framework:**\n- **Builds on top of modern native 3D graphics APIs**: DirectX 12 on Windows, Vulkan on Linux, Metal on MacOS, iOS & tvOS.\n- **Simplifies modern graphics programming** with object-oriented medium-level graphics API inspired by simplicity of Apple Metal. Common shaders code in HLSL 6 is used on all platforms.\n- **Provides cross-platform application framework** with CMake build toolchain, platform-independent application foundation classes and native-GUI layer for Windows, Linux and MacOS.\n\nDownload [release builds](https://github.com/MethanePowered/MethaneKit/releases) with pre-built samples, tutorials and tests to try them out. \nCheck latest build status, tests, code coverage and analysis results or get build artifacts from [GitHub Actions](https://github.com/MethanePowered/MethaneKit/actions) CI and [Sonar Cloud](https://sonarcloud.io/organizations/methane-powered).\nSee [Build Instructions](/Build/README.md) topic for manual build instructions and start learning [Methane Graphics RHI](/Modules/Graphics/RHI) API with [Hello Triangle](/Apps/01-HelloTriangle) and other tutorials' documentation.\n\n[![Open in Gitpod](https://gitpod.io/button/open-in-gitpod.svg)](https://gitpod.io/#https://github.com/MethanePowered/MethaneKit)\n\n| Platform                                                                                                                                        | Graphics API                                                                                                                                        | Master Build                                                                                                                                                                                                                                                                                   | Develop Build                                                                                                                                                                                                                                                                                     |\n|-------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| <img src=""https://github.com/MethanePowered/MethaneKit/blob/master/Docs/Images/Platforms/Windows.png"" width=24 valign=""middle""> **Windows x64** | <img src=""https://github.com/MethanePowered/MethaneKit/blob/master/Docs/Images/GraphicsApi/DirectX12Small.png"" width=24 valign=""middle""> DirectX 12 | [![Windows x64 Master Build](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_Win64_DX_Release_master.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-build.yml?query=branch%3Amaster) | [![Windows x64 Develop Build](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_Win64_DX_Release_develop.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-build.yml?query=branch%3Adevelop) |\n| <img src=""https://github.com/MethanePowered/MethaneKit/blob/master/Docs/Images/Platforms/Windows.png"" width=24 valign=""middle""> **Windows x86** | <img src=""https://github.com/MethanePowered/MethaneKit/blob/master/Docs/Images/GraphicsApi/DirectX12Small.png"" width=24 valign=""middle""> DirectX 12 | [![Windows x86 Master Build](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_Win32_DX_Release_master.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-build.yml?query=branch%3Amaster) | [![Windows x86 Develop Build](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_Win32_DX_Release_develop.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-build.yml?query=branch%3Adevelop) |\n| <img src=""https://github.com/MethanePowered/MethaneKit/blob/master/Docs/Images/Platforms/Windows.png"" width=24 valign=""middle""> **Windows x64** | <img src=""https://github.com/MethanePowered/MethaneKit/blob/master/Docs/Images/GraphicsApi/VulkanSmall.png"" width=24 valign=""middle""> Vulkan        | [![Windows x64 Master Build](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_Win64_VK_Release_master.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-build.yml?query=branch%3Amaster) | [![Windows x64 Develop Build](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_Win64_VK_Release_develop.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-build.yml?query=branch%3Adevelop) |\n| <img src=""https://github.com/MethanePowered/MethaneKit/blob/master/Docs/Images/Platforms/Windows.png"" width=24 valign=""middle""> **Windows x86** | <img src=""https://github.com/MethanePowered/MethaneKit/blob/master/Docs/Images/GraphicsApi/VulkanSmall.png"" width=24 valign=""middle""> Vulkan        | [![Windows x86 Master Build](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_Win32_VK_Release_master.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-build.yml?query=branch%3Amaster) | [![Windows x86 Develop Build](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_Win32_VK_Release_develop.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-build.yml?query=branch%3Adevelop) |\n| <img src=""https://github.com/MethanePowered/MethaneKit/blob/master/Docs/Images/Platforms/Linux.png"" width=24 valign=""middle""> **Linux**         | <img src=""https://github.com/MethanePowered/MethaneKit/blob/master/Docs/Images/GraphicsApi/VulkanSmall.png"" width=24 valign=""middle""> Vulkan        | [![Ubuntu Master Build](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_Ubuntu_VK_Release_master.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-build.yml?query=branch%3Amaster)     | [![Ubuntu Develop Build](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_Ubuntu_VK_Release_develop.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-build.yml?query=branch%3Adevelop)     |\n| <img src=""https://github.com/MethanePowered/MethaneKit/blob/master/Docs/Images/Platforms/MacOS.png"" width=24 valign=""middle""> **MacOS**         | <img src=""https://github.com/MethanePowered/MethaneKit/blob/master/Docs/Images/GraphicsApi/VulkanSmall.png"" width=24 valign=""middle""> Vulkan        | [![MacOS Master Build](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_MacOS_VK_Release_master.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-build.yml?query=branch%3Amaster)       | [![MacOS Develop Build](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_MacOS_VK_Release_develop.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-build.yml?query=branch%3Adevelop)       |\n| <img src=""https://github.com/MethanePowered/MethaneKit/blob/master/Docs/Images/Platforms/MacOS.png"" width=24 valign=""middle""> **MacOS**         | <img src=""https://github.com/MethanePowered/MethaneKit/blob/master/Docs/Images/GraphicsApi/MetalSmall.png"" width=24 valign=""middle""> Metal          | [![MacOS Master Build](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_MacOS_MTL_Release_master.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-build.yml?query=branch%3Amaster)      | [![MacOS Develop Build](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_MacOS_MTL_Release_develop.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-build.yml?query=branch%3Adevelop)      |\n| <img src=""https://github.com/MethanePowered/MethaneKit/blob/master/Docs/Images/Platforms/iOS.png"" width=24 valign=""middle""> **iOS** (Sim)       | <img src=""https://github.com/MethanePowered/MethaneKit/blob/master/Docs/Images/GraphicsApi/MetalSmall.png"" width=24 valign=""middle""> Metal          | [![iOS Master Build](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_iOS_Sim_MTL_Release_master.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-build.yml?query=branch%3Amaster)      | [![iOS Develop Build](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_iOS_Sim_MTL_Release_develop.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-build.yml?query=branch%3Adevelop)      |\n| <img src=""https://github.com/MethanePowered/MethaneKit/blob/master/Docs/Images/Platforms/iOS.png"" width=24 valign=""middle""> **tvOS** (Sim)      | <img src=""https://github.com/MethanePowered/MethaneKit/blob/master/Docs/Images/GraphicsApi/MetalSmall.png"" width=24 valign=""middle""> Metal          | [![tvOS Master Build](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_tvOS_Sim_MTL_Release_master.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-build.yml?query=branch%3Amaster)    | [![tvOS Develop Build](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_tvOS_Sim_MTL_Release_develop.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-build.yml?query=branch%3Adevelop)    |\n\n[Static code analysis](#static-code-analysis) scans are performed as a part of automated CI build process on master and develop branches\nwith up-to-date results published on [Sonar Cloud](https://sonarcloud.io/organizations/methane-powered).\n\n| Platform                                                                                                                                            | Sonar Quality Gate                                                                                                                                                                                         | Master Scan Status                                                                                                                                                                                                                                                                                      | Develop Scan Status                                                                                                                                                                                                                                                                                        |\n|-----------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| <img src=""https://github.com/MethanePowered/MethaneKit/blob/master/Docs/Images/Platforms/Windows.png"" width=24 valign=""middle""> **Windows** DirectX | [![Windows Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=methane-powered-kit-windows&metric=alert_status)](https://sonarcloud.io/dashboard?id=methane-powered-kit-windows) | [![Windows Master Scan Status](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_Win64_DX_SonarScan_master.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-sonar-scan.yml?query=branch%3Amaster) | [![Windows Develop Scan Status](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_Win64_DX_SonarScan_develop.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-sonar-scan.yml?query=branch%3Adevelop) |\n| <img src=""https://github.com/MethanePowered/MethaneKit/blob/master/Docs/Images/Platforms/Linux.png"" width=24 valign=""middle""> **Linux** Vulkan      | [![Linux Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=methane-powered-kit-linux&metric=alert_status)](https://sonarcloud.io/dashboard?id=methane-powered-kit-linux)       | [![Linux Master Scan Status](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_Ubuntu_VK_SonarScan_master.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-sonar-scan.yml?query=branch%3Amaster)  | [![Linux Develop Scan Status](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_Ubuntu_VK_SonarScan_develop.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-sonar-scan.yml?query=branch%3Adevelop)       |\n| <img src=""https://github.com/MethanePowered/MethaneKit/blob/master/Docs/Images/Platforms/MacOS.png"" width=24 valign=""middle""> **MacOS** Metal       | [![MacOS Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=methane-powered-kit-macos&metric=alert_status)](https://sonarcloud.io/dashboard?id=methane-powered-kit-macos)       | [![MacOS Master Scan Status](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_MacOS_MTL_SonarScan_master.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-sonar-scan.yml?query=branch%3Amaster)  | [![MacOS Develop Scan Status](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_MacOS_MTL_SonarScan_develop.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-sonar-scan.yml?query=branch%3Adevelop)       |\n\n[![Windows Maintainability Rating](https://sonarcloud.io/api/project_badges/measure?project=methane-powered-kit-windows&metric=sqale_rating)](https://sonarcloud.io/dashboard?id=methane-powered-kit-windows)\n[![Windows Reliability Rating](https://sonarcloud.io/api/project_badges/measure?project=methane-powered-kit-windows&metric=reliability_rating)](https://sonarcloud.io/dashboard?id=methane-powered-kit-windows)\n[![Windows Security Rating](https://sonarcloud.io/api/project_badges/measure?project=methane-powered-kit-windows&metric=security_rating)](https://sonarcloud.io/dashboard?id=methane-powered-kit-windows)\n[![Windows Code Smells](https://sonarcloud.io/api/project_badges/measure?project=methane-powered-kit-windows&metric=code_smells)](https://sonarcloud.io/dashboard?id=methane-powered-kit-windows)\n[![Windows Duplicated Lines (%)](https://sonarcloud.io/api/project_badges/measure?project=methane-powered-kit-windows&metric=duplicated_lines_density)](https://sonarcloud.io/dashboard?id=methane-powered-kit-windows)\n[![Windows Coverage](https://sonarcloud.io/api/project_badges/measure?project=methane-powered-kit-windows&metric=coverage)](https://sonarcloud.io/dashboard?id=methane-powered-kit-windows)\n[![Windows Lines of Code](https://sonarcloud.io/api/project_badges/measure?project=methane-powered-kit-windows&metric=ncloc)](https://sonarcloud.io/dashboard?id=methane-powered-kit-windows)\n\n![Asteroids Sample on Windows](/../../../MethaneAsteroids/blob/main/Screenshots/AsteroidsWinDirectX12.jpg)\n<p align=""center""><i><a href=""https://github.com/MethanePowered/MethaneAsteroids"">Asteroids sample</a> demonstrating multi-threaded rendering with Methane Kit</i></p>\n\n## [Build Instructions](/Build/README.md)\n\n- [Prerequisites](/Build/README.md#prerequisites)\n- [Fetch Sources](/Build/README.md#fetch-sources)\n- - [Notes](/Build/README.md#notes)\n- [First time initialization](/Build/README.md#first-time-initialization)\n- [Update sources to latest revision](/Build/README.md#update-sources-to-latest-revision)\n- [Building from Sources](/Build/README.md#building-from-sources)\n  - [Windows Build with Visual Studio](/Build/README.md#windows-build-with-visual-studio)\n  - [Linux Build with Unix Makefiles](/Build/README.md#linux-build-with-unix-makefiles)\n  - [MacOS Build with XCode](/Build/README.md#macos-build-with-xcode)\n  - [iOS and tvOS Build with XCode](/Build/README.md#ios-and-tvos-build-with-xcode)\n- [CMake Generator](/Build/README.md#cmake-generator)\n  - [CMake Options](/Build/README.md#cmake-options)\n  - [CMake Presets](/Build/README.md#cmake-presets)\n\n## Getting Started\n\n### High-Level Architecture\n\nMethane Kit architecture is clearly distributing library modules between 5 layers from low to high level of abstraction.\n![High Level Architecture](Docs/Diagrams/MethaneKit_HighLevel_Architecture.svg)\n\n### Rendering Hardware Interface (RHI)\n\n[Methane Graphics RHI](Modules/Graphics/RHI) module implements a set of public object-oriented interfaces, \nwhich make modern graphics programming easy and convenient in a platform and API independent way.\n![Graphics RHI](Docs/Diagrams/MethaneKit_Graphics_RHI.svg)\n\n### Tutorials\n\nStart learning Methane Graphics API with [Hello Triangle](/Apps/01-HelloTriangle) tutorial documentation\nand continue with others.\n\n| <pre><b>Name / Link</b></pre>                       | <pre><b>Screenshot</b></pre>                                                                               | <pre><b>Description</b>                                         </pre>                            |\n|-----------------------------------------------------|------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\n| 1. [Hello Triangle](/Apps/01-HelloTriangle)         | ![Hello Triangle on Windows](/Apps/01-HelloTriangle/Screenshots/HelloTriangleWinDirectX12.jpg)             | Colored triangle rendering in 100 lines of code.                                                  |\n| 2. [Hello Cube](/Apps/02-HelloCube)                 | ![Hello Cube on Windows](/Apps/02-HelloCube/Screenshots/HelloCubeWinDirectX12.jpg)                         | Colored cube rendering in 200 lines of code with vertex and index buffers.                        |\n| 3. [Textured Cube](/Apps/03-TexturedCube)           | ![Textured Cube on Windows](/Apps/03-TexturedCube/Screenshots/TexturedCubeWinDirectX12.jpg)                | Textured cube introduces buffers, textures and samplers usage with Phong shading.                 |\n| 4. [Shadow Cube](/Apps/04-ShadowCube)               | ![Shadow Cube on Windows](/Apps/04-ShadowCube/Screenshots/ShadowCubeWinDirectX12.jpg)                      | Shadow cube introduces multi-pass rendering with render passes.                                   |\n| 5. [Typography](/Apps/05-Typography)                | ![Typography on Windows](/Apps/05-Typography/Screenshots/TypographyWinDirectX12.jpg)                       | Typography demonstrates animated text rendering with dynamic font atlas updates using Methane UI. |\n| 6. [Cube-Map Array](/Apps/06-CubeMapArray)          | ![Cube-Map Array on Windows](/Apps/06-CubeMapArray/Screenshots/CubeMapArrayWinDirectX12.jpg)               | Cube-map array texturing along with sky-box rendering.                                            |\n| 7. [Parallel Rendering](/Apps/07-ParallelRendering) | ![Parallel Rendering on Windows](/Apps/07-ParallelRendering/Screenshots/ParallelRenderingWinDirectX12.jpg) | Parallel rendering of the textured cube instances to the single render pass.                      |\n| 8. [Console Compute](/Apps/08-ConsoleCompute)       | ![Console Compute on Windows](/Apps/08-ConsoleCompute/Screenshots/ConsoleComputeWinDirectX12.jpg)          | Conway's Game of Life implemented in Compute Shader and running in pure console application.      |\n\n### Samples\n\nMethane samples demonstrate advanced techniques and usage scenarios with more complex implementation than tutorials above.\nSamples are distributes in form of separate repositories.\n\n| <pre><b>Name / Link</b></pre>           | <pre><b>Screenshot</b></pre>                                                                        | <pre><b>Description</b>                                         </pre>                                                                                                  |\n|-----------------------------------------|-----------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [Asteroids](/../../../MethaneAsteroids) | ![Asteroids on Windows](/../../../MethaneAsteroids/blob/main/Screenshots/AsteroidsWinDirectX12.jpg) | Benchmark demonstrating parallel render commands encoding in a single render pass for the large number of heterogeneous asteroid objects processed in multiple threads. |\n\n### Features\n\n- **Cross-platform application & input classes**: Windows, MacOS and Linux are supported\n  - **CMake modules** for convenient application build configuration, adding shaders and embedded resources\n  - **HLSL-6 Shaders** serving all graphics APIs converted to native shader language and compiled in build time with SPIRV-Cross & DirectXCompiler\n  - **HLSL++ Math** library with [HLSL-like syntax](https://docs.microsoft.com/en-us/windows/desktop/direct3dhlsl/dx-graphics-hlsl-reference) in C++\n    and vector-instruction optimizations for different platforms\n- **Modern Graphics API abstractions**: based on DirectX 12, Vulkan and Metal APIs\n  - Render state and program configuration with compact initialization syntax\n  - Program binding objects implement efficient binding of shader arguments to resources\n  - Automatic resource state tracking used for automatic resource transition barriers setup\n  - Resources are automatically retained from destroying while in use on GPU with shared pointers in command list state\n  - Command list execution state tracking with optional GPU timestamps query on completion\n  - Parallel render command list for multi-threaded render commands encoding in single render pass\n  - Multiple command queues execution on GPU with synchronization using fences\n  - Private GPU resources asynchronously updated through the upload command list and shared resource\n  - Registry of named graphics objects enabling reuse of render states and graphics resources between renderer objects\n- **Graphics primitives and extensions**:\n  - Graphics application base class with per-frame resource management and frame buffers resizing enable effective triple buffering\n  - Camera primitive and interactive arc-ball camera\n  - Procedural mesh generation for quad, box, sphere, icosahedron and uber-mesh\n  - Screen-quad and sky-box rendering extension classes\n  - Texture loader (currently implemented with STB, planned for replacement with OpenImageIO)\n- **User Interface**:\n  - UI application base class with integrated HUD, logo badge and help/parameters text panels\n  - Typography library for fonts loading, dynamic atlas updating, text rendering & layout\n  - Widgets library (under development)\n- **Platform Infrastructure**:\n  - Base application with window management and input handling for Windows, MacOS and Linux\n  - Events mechanism connecting emitters and receivers via callback interfaces\n  - Animations subsystem\n  - Embedded resource providers\n- **Integrated debugging and profiling capabilities**:\n  - Library instrumentation for performance analysis with [trace profiling tools](#trace-profiling-tools)\n  - Debug names for all GPU objects and debug regions for graphics API calls for use with [frame profiling tools](#frame-profiling-and-debugging-tools)\n- **Continuous integration** with automated multi-platform builds, unit-tests and\n  [Sonar Cloud](https://sonarcloud.io/dashboard?id=methane-powered-kit-windows) static code analysis\n  in [GitHub Actions](https://github.com/MethanePowered/MethaneKit/actions)\n\nFor detailed features description and development plans please refer to [Modules documentation](Modules).\n\n## Supported Development Tools\n\n### Development Environments\n\n<a href=""https://www.jetbrains.com/?from=MethaneKit"" target=""_blank""><img src=""https://github.com/MethanePowered/MethaneKit/blob/master/Docs/Partners/JetBrains.png"" width=200 align=""right"" valign=""bottom""/></a>\n- Microsoft Visual Studio 2019\n  - Solutions and projects build (generate with [Build/Windows/Build.bat](/Build/Windows/Build.bat))\n  - CMake native build support (pre-configured with [CMakePresets.json](/CMakePresets.json))\n- Apple XCode\n  - XCode workspace and projects (generate with [Build/Unix/Build.sh](/Build/Unix/Build.sh))\n- Microsoft VS Code and [GitPod](https://gitpod.io/#https://github.com/MethanePowered/MethaneKit) (pre-configured with [CMakePresets.json](/CMakePresets.json) and [.vscode/settings.json](/.vscode/settings.json))\n- Jet Brains CLion (pre-configured with [.idea](/.idea))\n- Qt Creator with CMake native support\n\nMethane Kit is being developed with support of [Jet Brains](https://www.jetbrains.com/?from=MethaneKit) development tools.\nOpen source project development license is provided free of charge to all key contributors of Methane Kit project.\n\n### Static Code Analysis\n\nMethane Kit comes with continuous C++ static code and code coverage analysis performed as a part of automated CI ""Scan"" builds\nwith up-to-date results published on [Sonar Cloud](https://sonarcloud.io/organizations/methane-powered)\nseparately for all supported platforms.\n\n| Master Scan Results  | Windows                                                                                                                                                                                                                                                                                                 | MacOS                                                                                                                                                                                                                                                                                                  | Linux                                                                                                                                                                                                                                                                                                  |\n|----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Scan Build Status    | [![Windows Master Scan Status](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_Win64_DX_SonarScan_master.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-sonar-scan.yml?query=branch%3Amaster) | [![MacOS Master Scan Status](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_MacOS_MTL_SonarScan_master.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-sonar-scan.yml?query=branch%3Amaster) | [![Linux Master Scan Status](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/egorodet/96d788046ccd52b45b3354a99f8569c3/raw/MethaneKit_Ubuntu_VK_SonarScan_master.json)](https://github.com/MethanePowered/MethaneKit/actions/workflows/ci-sonar-scan.yml?query=branch%3Amaster) |\n| Quality Gate         | [![Windows Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=methane-powered-kit-windows&metric=alert_status)](https://sonarcloud.io/dashboard?id=methane-powered-kit-windows)                                                                                              | [![MacOS Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=methane-powered-kit-macos&metric=alert_status)](https://sonarcloud.io/dashboard?id=methane-powered-kit-macos)                                                                           ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/metagenome-atlas/atlas,https://github.com/metagenome-atlas/atlas,1,,,1,1,1,1,0,0,0,0,0,0,1,ATLAS - Three commands to start analyzing your metagenome data,"# Metagenome-Atlas\n\n[![Anaconda-Server Badge](https://anaconda.org/bioconda/metagenome-atlas/badges/latest_release_relative_date.svg)](https://anaconda.org/bioconda/metagenome-atlas)\n[![Bioconda](https://img.shields.io/conda/dn/bioconda/metagenome-atlas.svg?label=Bioconda )](https://anaconda.org/bioconda/metagenome-atlas)\n[![Documentation Status](https://readthedocs.org/projects/metagenome-atlas/badge/?version=latest)](https://metagenome-atlas.readthedocs.io/en/latest/?badge=latest)\n![Mastodon Follow](https://img.shields.io/mastodon/follow/109273833677404282?domain=https%3A%2F%2Fmstdn.science&style=social)\n<!--[![follow on twitter](https://img.shields.io/twitter/follow/SilasKieser.svg?style=social&label=Follow)](https://twitter.com/search?f=tweets&q=%40SilasKieser%20%23metagenomeAtlas&src=typd) -->\n\n\nMetagenome-atlas is a easy-to-use metagenomic pipeline based on snakemake. It handles all steps from QC, Assembly, Binning, to Annotation.\n\n![scheme of workflow](resources/images/atlas_list.png?raw=true)\n\nYou can start using atlas with three commands:\n```\n    mamba install -y -c bioconda -c conda-forge metagenome-atlas={latest_version}\n    atlas init --db-dir databases path/to/fastq/files\n    atlas run all\n```\nwhere `{latest_version}` should be replaced by [![Version](https://anaconda.org/bioconda/metagenome-atlas/badges/version.svg)](https://anaconda.org/bioconda/metagenome-atlas)\n\n\n# Webpage\n\n[metagenome-atlas.github.io](https://metagenome-atlas.github.io/)\n\n# Documentation\n\nhttps://metagenome-atlas.readthedocs.io/\n\n[Tutorial](https://github.com/metagenome-atlas/Tutorial)\n\n# Citation\n\n> ATLAS: a Snakemake workflow for assembly, annotation, and genomic binning of metagenome sequence data.  \n> Kieser, S., Brown, J., Zdobnov, E. M., Trajkovski, M. & McCue, L. A.   \n> BMC Bioinformatics 21, 257 (2020).  \n> doi: [10.1186/s12859-020-03585-4](https://doi.org/10.1186/s12859-020-03585-4)\n\n\n# Developpment/Extensions\n\nHere are some ideas I work or want to work on when I have time. If you want to contribute or have some ideas let me know via a feature request issue.\n\n- Optimized MAG recovery (e.g. [Spacegraphcats](https://github.com/spacegraphcats/spacegraphcats))\n- Integration of viruses/plasmid that live for now as [extensions](https://github.com/metagenome-atlas/virome_atlas)\n- Add statistics and visualisations as in [atlas_analyze](https://github.com/metagenome-atlas/atlas_analyze)\n- Implementation of most rules as snakemake wrapper\n- Cloud execution\n- Update to new Snakemake version and use cool reports.\n",359,snakemake,Python,5,Python,CSS,Shell,Ruby,HTML,,,,,,,,,,,,,,,,,,,,,,,,250,46,198,6,62,25,0,21274,100,423,421,2,22d78b982ba9164c1313439f4642d793615cc9c1,Merge pull request #730 from metagenome-atlas/gtdb9-atlas2,2024-06-30T08:05:32Z,Silas Kieser,SilasK@users.noreply.github.com,SilasK,v2.18.2,## [2.18.2](https://github.com/metagenome-atlas/atlas/compare/v2.18.1...v2.18.2) (2024-06-28)\n\n\n### Bug Fixes\n\n* 676 ([8b4d552](https://github.com/metagenome-atlas/atlas/commit/8b4d5522afe2b35265ea406ac2a4b7d0edf571fb))\n* 701 ([ce22404](https://github.com/metagenome-atlas/atlas/commit/ce224044ee13db9647b74a6cba726006f04ec861)),v2.18.2,,,github-actions[bot],"BSD 3-Clause ""New"" or ""Revised"" License",atlas,metagenome-atlas,58,metagenomics,annotation,snakemake,assembly,genomic-binning,functional-annotation,taxonomic-classifications,,,,,,,,,,,,,,/metagenome-atlas/atlas,79,15,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/merenlab/anvio,https://github.com/merenlab/anvio,0,,,0,1,1,1,0,0,0,0,0,0,1,An analysis and visualization platform for 'omics data,"<p align=""center""><img src=""https://github.com/merenlab/anvio/raw/master/anvio/data/interactive/images/logo-fancy.png"" height=""256"" /></p>\n\n### Releases\n\nGithub [releases page](https://github.com/merenlab/anvio/releases) lists all the stable releases of anvi'o.\n\n### Installation and tutorials\n\nThe [anvi'o project page](https://anvio.org) gives access to installation manuals, user tutorials, and other sweets.\n\n### Help on anvi'o programs and artifacts\n\n[The anvi'o help pages](https://anvio.org/help) describe individual anvi'o programs as well as artifacts they consume or produce.\n\n### Coding style considerations\n\nPlease see [relevant discussions](https://github.com/merenlab/anvio/issues?q=label%3A%22coding+style%22+).\n\n### Community chat\n\nClick [this link](https://discord.gg/C6He6mSNY4) to join the anvi'o Discord channel.\n\n### Others on anvi'o\n\nRead our [user testimonials](http://merenlab.org/2017/07/12/testimonials/).\n",415,bioinformatics,Python,8,Shell,Python,HTML,CSS,JavaScript,R,Dockerfile,Makefile,,,,,,,,,,,,,,,,,,,,,670,37,624,9,94,56,0,750956,142,1638,1498,140,b62613321d870d5261592c5ae3f1e381fefb2fa2,add --gene-caller flag to `anvi-meta-pan-genome` to address #2308,2024-07-18T16:10:30Z,Iva Veseli,iva.veseli@gmail.com,ivagljiva,"anvi'o v8, ""marie""","We are happy to announce anvi'o `v8` with the code name, ""**marie**""!\r\n\r\nAfter about 4,200 changes that introduced over 36,000 new lines of code, this stable release of anvi'o represents significant advancements over `v7`, and introduces many new features for integrated studies of microbial metabolism, genomic inversions, phylogeography of proteins, performance improvements, and fixes for known bugs.\r\n\r\nThis page intends to give you a summary of some of the notable changes that come with *marie*.\r\n\r\n---\r\n\r\n![image](https://github.com/merenlab/anvio/assets/197307/e0e09ea4-90f6-4c4c-8971-eec817e0ca22)\r\n\r\n_The code name recognizes [Marie Tharp](https://en.wikipedia.org/wiki/Marie_Tharp), an American geologist and oceanographic cartographer, who has made immense contributions to earth sciences. Marie was a pioneer in our understanding of oceans as she created the first map of the Atlantic seafloor with her colleague Bruce Heezen [1]. Her work showed that the bottom of our oceans were not only flat sediments but were also covered with canyons, ridges, and mountain ranges that spanned over 65,000 kilometers around the globe. Marie's revolutionary work emerged from her interpretation of data she was not allowed to collect since women were not allowed to be on ships during the 1950s. Marie compiled her physiographic diagrams from the data Bruce Heezen were able to collect [2]. She did not step on a ship until 1968, and the early evidence she had for seafloor features was initially dismissed as 'girl talk' [3]._\r\n\r\n[1] https://en.wikipedia.org/wiki/Marie_Tharp\r\n[2] https://www.lyellcollection.org/doi/abs/10.1144/GSL.SP.2002.192.01.11\r\n[3] https://www.youtube.com/watch?v=gsQGOJtwdv0\r\n\r\n_The code name was a [suggestion](https://twitter.com/zenanaut/status/1334200960692830209) by [Zena Cardman](https://twitter.com/zenanaut), a Marine Microbiologist and a NASA Astronaut. The release notes were written by [Meren](https://twitter.com/merenbey), [Iva Veseli](http://linkedin.com/in/iva-veseli), and [Matt Schechter](https://twitter.com/mschecht_bio), who are among the [developers of anvi'o](https://anvio.org/people/). The notes were proofread by [Katy Lambert-Slosarska](https://twitter.com/Katy_LS_), who is a MSc student at [the International Max Planck Research School of Marine Microbiology](https://marmic.mpg.de/marmic2/overview.php?section=overview) (MarMic)._\r\n\r\n---\r\n\r\n# New anvi'o programs, artifacts, and workflows\r\n\r\nThe new version of anvi'o comes with a few new programs:\r\n\r\n* [anvi-compute-functional-enrichment-across-genomes](https://anvio.org/help/8/programs/anvi-compute-functional-enrichment-across-genomes)\r\n* [anvi-compute-functional-enrichment-in-pan](https://anvio.org/help/8/programs/anvi-compute-functional-enrichment-in-pan)\r\n* [anvi-compute-metabolic-enrichment](https://anvio.org/help/8/programs/anvi-compute-metabolic-enrichment)\r\n* [anvi-delete-functions](https://anvio.org/help/8/programs/anvi-delete-functions)\r\n* [anvi-display-functions](https://anvio.org/help/8/programs/anvi-display-functions)\r\n* [anvi-get-codon-usage-bias](https://anvio.org/help/8/programs/anvi-get-codon-usage-bias)\r\n* [anvi-get-metabolic-model-file](https://anvio.org/help/8/programs/anvi-get-metabolic-model-file)\r\n* [anvi-get-pn-ps-ratio](https://anvio.org/help/8/programs/anvi-get-pn-ps-ratio)\r\n* [anvi-get-tlen-dist-from-bam](https://anvio.org/help/8/programs/anvi-get-tlen-dist-from-bam)\r\n* [anvi-merge-trnaseq](https://anvio.org/help/8/programs/anvi-merge-trnaseq)\r\n* [anvi-plot-trnaseq](https://anvio.org/help/8/programs/anvi-plot-trnaseq)\r\n* [anvi-profile-blitz](https://anvio.org/help/8/programs/anvi-profile-blitz)\r\n* [anvi-reaction-network](https://anvio.org/help/8/programs/anvi-reaction-network)\r\n* [anvi-report-inversions](https://anvio.org/help/8/programs/anvi-report-inversions)\r\n* [anvi-run-cazymes](https://anvio.org/help/8/programs/anvi-run-cazymes)\r\n* [anvi-search-palindromes](https://anvio.org/help/8/programs/anvi-search-palindromes)\r\n* [anvi-search-primers](https://anvio.org/help/8/programs/anvi-search-primers)\r\n* [anvi-search-sequence-motifs](https://anvio.org/help/8/programs/anvi-search-sequence-motifs)\r\n* [anvi-setup-cazymes](https://anvio.org/help/8/programs/anvi-setup-cazymes)\r\n* [anvi-setup-kegg-data](https://anvio.org/help/8/programs/anvi-setup-kegg-data)\r\n* [anvi-setup-modelseed-database](https://anvio.org/help/8/programs/anvi-setup-modelseed-database)\r\n* [anvi-setup-user-modules](https://anvio.org/help/8/programs/anvi-setup-user-modules)\r\n* [anvi-summarize-blitz](https://anvio.org/help/8/programs/anvi-summarize-blitz)\r\n* [anvi-tabulate-trnaseq](https://anvio.org/help/8/programs/anvi-tabulate-trnaseq)\r\n* [anvi-script-as-markdown](https://anvio.org/help/8/programs/anvi-script-as-markdown)\r\n* [anvi-script-compute-bayesian-pan-core](https://anvio.org/help/8/programs/anvi-script-compute-bayesian-pan-core)\r\n* [anvi-script-estimate-metabolic-independence](https://anvio.org/help/8/programs/anvi-script-estimate-metabolic-independence)\r\n* [anvi-script-filter-hmm-hits-table](https://anvio.org/help/8/programs/anvi-script-filter-hmm-hits-table)\r\n* [anvi-script-gen-function-matrix-across-genomes](https://anvio.org/help/8/programs/anvi-script-gen-function-matrix-across-genomes)\r\n* [anvi-script-gen-functions-per-group-stats-output](https://anvio.org/help/8/programs/anvi-script-gen-functions-per-group-stats-output)\r\n* [anvi-script-gen-genomes-file](https://anvio.org/help/8/programs/anvi-script-gen-genomes-file)\r\n* [anvi-script-gen-user-module-file](https://anvio.org/help/8/programs/anvi-script-gen-user-module-file)\r\n* [anvi-script-permute-trnaseq-seeds](https://anvio.org/help/8/programs/anvi-script-permute-trnaseq-seeds)\r\n\r\nAnd a few new artifacts:\r\n\r\n* [bam-stats-txt](https://anvio.org/help/8/artifacts/bam-stats-txt)\r\n* [bams-and-profiles-txt](https://anvio.org/help/8/artifacts/bams-and-profiles-txt)\r\n* [cazyme-data](https://anvio.org/help/8/artifacts/cazyme-data)\r\n* [contig-inspection](https://anvio.org/help/8/artifacts/contig-inspection)\r\n* [dna-sequence](https://anvio.org/help/8/artifacts/dna-sequence)\r\n* [enzymes-list-for-module](https://anvio.org/help/8/artifacts/enzymes-list-for-module)\r\n* [enzymes-txt](https://anvio.org/help/8/artifacts/enzymes-txt)\r\n* [external-structures](https://anvio.org/help/8/artifacts/external-structures)\r\n* [functions-across-genomes-txt](https://anvio.org/help/8/artifacts/functions-across-genomes-txt)\r\n* [gene-cluster-inspection](https://anvio.org/help/8/artifacts/gene-cluster-inspection)\r\n* [hmm-hits-across-genomes-txt](https://anvio.org/help/8/artifacts/hmm-hits-across-genomes-txt)\r\n* [hmm-list](https://anvio.org/help/8/artifacts/hmm-list)\r\n* [inversions-txt](https://anvio.org/help/8/artifacts/inversions-txt)\r\n* [markdown-txt](https://anvio.org/help/8/artifacts/markdown-txt)\r\n* [metabolic-independence-score](https://anvio.org/help/8/artifacts/metabolic-independence-score)\r\n* [modifications-txt](https://anvio.org/help/8/artifacts/modifications-txt)\r\n* [paired-end-fastq](https://anvio.org/help/8/artifacts/paired-end-fastq)\r\n* [palindromes-txt](https://anvio.org/help/8/artifacts/palindromes-txt)\r\n* [primers-txt](https://anvio.org/help/8/artifacts/primers-txt)\r\n* [quick-summary](https://anvio.org/help/8/artifacts/quick-summary)\r\n* [reaction-network](https://anvio.org/help/8/artifacts/reaction-network)\r\n* [reaction-network-json](https://anvio.org/help/8/artifacts/reaction-network-json)\r\n* [reaction-ref-data](https://anvio.org/help/8/artifacts/reaction-ref-data)\r\n* [seeds-non-specific-txt](https://anvio.org/help/8/artifacts/seeds-non-specific-txt)\r\n* [seeds-specific-txt](https://anvio.org/help/8/artifacts/seeds-specific-txt)\r\n* [trnaseq-contigs-db](https://anvio.org/help/8/artifacts/trnaseq-contigs-db)\r\n* [trnaseq-plot](https://anvio.org/help/8/artifacts/trnaseq-plot)\r\n* [trnaseq-profile-db](https://anvio.org/help/8/artifacts/trnaseq-profile-db)\r\n* [trnaseq-seed-txt](https://anvio.org/help/8/artifacts/trnaseq-seed-txt)\r\n* [user-metabolism](https://anvio.org/help/8/artifacts/user-metabolism)\r\n* [user-modules-data](https://anvio.org/help/8/artifacts/user-modules-data)\r\n* [variability-profile-xml](https://anvio.org/help/8/artifacts/variability-profile-xml)\r\n\r\nIn addition, this release makes available three new Snakemake workflows that are accessible via the anvi'o program [anvi-run-workflow](https://anvio.org/help/8/programs/anvi-run-workflow): [trnaseq](https://anvio.org/help/8/workflows/trnaseq/), [ecophylo](https://anvio.org/help/8/workflows/ecophylo), and [sra_download](https://anvio.org/help/8/workflows/sra-download).\r\n\r\n\r\n# A new subsystem for metabolic modeling\r\n\r\nOne of the biggest news in this release is the set of programs now anvi'o includes for metabolic modeling. These programs are emerging as a by-product of collaborative projects in C-CoMP, or [the Center for Chemical Currencies of a Microbial Planet](https://ccomp-stc.org/), and under the leadership of [Samuel Miller](https://anvio.org/people/semiller10).\r\n\r\nUsing the integrated anvi'o metabolic modeling subsystem, one can generate a biochemical reaction network suitable for metabolic modeling from the annotations in a genome or a pangenome using the new program [`anvi-reaction-network`](https://anvio.org/help/8/programs/anvi-reaction-network). This works on both individual genomes (using a [contigs-db](https://anvio.org/help/8/artifacts/contigs-db/) and pangenomes (using a [genomes-storage-db](https://anvio.org/help/8/artifacts/genomes-storage-db/)). The resulting network is stored in corresponding anvi'o database for programmatic access, and can be exported into a JSON file for inspection and downstream usage (i.e., as input into a program for flux-balance analysis) via another new program, [`anvi-get-metabolic-model-file`](https://anvio.org/help/8/programs/anvi-get-metabolic-model-file).\r\n\r\nThese programs rely on [KEGG Orthology (KO)](https://www.genome.jp/kegg/ko.html) annotations of protein-coding genes and reference data in the [ModelSEED Biochemistry database](https://github.com/ModelSEED/ModelSEEDDatabase), which can be downloaded and set up on your computer using the programs [anvi-setup-kegg-data](https://anvio.org/help/8/programs/anvi-setup-kegg-data/) and [anvi-setup-modelseed-database](https://anvio.org/help/main/programs/anvi-setup-modelseed-database/), respectively.\r\n\r\nFor additional information, please see PRs #2058, #2072, and #2123.\r\n\r\n\r\n# Substantial improvements to metabolic pathway prediction in anvi'o\r\n\r\nAnvi'o metabolism offers a full suite of integrated tools to study metabolism in microbial genomes and metagenomes, and multiple recent papers from our group (i.e., by [Watson et al](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-023-02924-x) and [Veseli et al](https://doi.org/10.7554/eLife.89862.1)) propelled a series of improvements thanks work from [Iva Veseli](https://anvio.org/people/ivagljiva). We hope these improvements summarized below will also help anvi'o users at large.\r\n\r\n## Improved data download and processing\r\n\r\nNow multiple aspects of anvi'o rely on data from [KEGG](https://www.genome.jp/kegg/), so we decided to revamp how we download it. The old program `anvi-setup-kegg-kofams` has been changed to a new program, `anvi-setup-kegg-data`. This program has multiple modes for downloading KOfam profiles, KEGG MODULE data, KEGG BRITE hierarchies (PR #1910), and modeling data for `anvi-reaction-network`. It can be multi-threaded for faster downloads. \r\n\r\nHowever, for most users we recommend the default usage of this program, which downloads a pre-processed snapshot of everything you need for downstream programs working on this data. Please see [anvi-setup-kegg-data](https://anvio.org/help/8/programs/anvi-setup-kegg-data/).\r\n\r\n## Improvements in pathway prediction\r\n\r\nThe metabolism framework in anvi'o has undergone a lot of changes in the past year, with the addition of several notable features mainly concerning the use of the program [anvi-estimate-metabolism](https://anvio.org/help/8/programs/anvi-estimate-metabolism/):\r\n\r\n###  ""Stepwise"" metrics: a new strategy for interpreting metabolic pathway definitions\r\n\r\nAs of PR #1927, we've added a new way of interpreting metabolic modules which affects how metrics like completeness and copy number are calculated. This strategy is called 'stepwise' interpretation because it considers only the major, non-redundant steps in a metabolic module. In this method, alternative enzymes, or in some cases alternative series of enzymes, are evaluated as one entity. Stepwise metrics may be appropriate for those interested in summarizing generic metabolic capacity with less focus on the specific enzymes that are required. \r\n\r\nThe former method of interpreting pathways is now referred to as the 'pathwise' strategy because it involves deconstructing the module definition into all possible unique combinations of enzymes required to catalyze the reactions in the metabolic pathway (so it considers all possible 'paths' through the module). Metrics are still calculated using this strategy and are labeled with the term 'pathwise' to distinguish them from the stepwise metrics.\r\n\r\nYou can find a description of these two strategies, along with examples, [here](https://anvio.org/help/8/programs/anvi-estimate-metabolism/#two-estimation-strategies---pathwise-and-stepwise).\r\n\r\n![image](https://github.com/merenlab/anvio/assets/197307/473e5f7b-027a-458a-87f3-c5979bdcdfd3)\r\n\r\n### Calculation of pathway copy number\r\n\r\nThis release also introduces a redundancy metric for metagenome-wide analyses - pathway copy number. This metric can be added to your output files using the `--add-copy-number` flag, and will be calculated using both the pathwise and the stepwise strategies. This metric may be most appropriate when your input data represents a multitude of organisms (as in when you input a metagenome without using the `--metagenome-mode` flag). \r\n\r\nIn our documentation, you can find an explanation of the [pathwise copy number calculation](https://anvio.org/help/8/programs/anvi-estimate-metabolism/#part-5-path-copy-number) and the [stepwise copy number calculation](https://anvio.org/help/8/programs/anvi-estimate-metabolism/#part-5-step-copy-number). This feature was added in PR #1927.\r\n\r\n### User-defined metabolic modules\r\n\r\n[anvi-estimate-metabolism](https://anvio.org/help/8/programs/anvi-estimate-metabolism/) now has the ability to work with user-defined metabolic pathways based on arbitrary functional annotation sources as of PR #1867. \r\n\r\n![image](https://github.com/merenlab/anvio/assets/197307/f1dbf833-bae5-489c-b1e4-b76bd1d00ab4)\r\n\r\nUsers wishing to define their own metabolic modules can use the new anvi'o artifact, [user-modules-data](https://anvio.org/help/8/artifacts/user-modules-data/). The files can either be written manually or generated via the script [anvi-script-gen-user-module-file](https://anvio.org/help/8/programs/anvi-script-gen-user-module-file/) (See PR #1872). The program [anvi-setup-user-modules](https://anvio.org/help/8/programs/anvi-setup-user-modules/) can then convert these module files into a database that can be used with [anvi-estimate-metabolism](https://anvio.org/help/8/programs/anvi-estimate-metabolism/) via the `--user-modules` flag, [as described here](https://anvio.org/help/8/programs/anvi-estimate-metabolism/#working-with-user-defined-metabolism-data).\r\n\r\nTo support the use of arbitrary HMMs as an annotation source for user-defined metabolic modules, the program [anvi-run-hmms](https://anvio.org/help/8/programs/anvi-run-hmms) now has a flag called `--add-to-functions-table`, which causes any HMM hits to be stored as functional annotations. [See here for details](https://anvio.org/help/8/programs/anvi-run-hmms/#adding-hmm-hits-as-a-functional-annotation-source).\r\n\r\n### Miscellaneous updates\r\n\r\nBeyond the major features described above, there are a few miscellaneous changes to the metabolism codebase.\r\n\r\n- You no longer have to rely on having contigs databases as input to [anvi-estimate-metabolism](https://anvio.org/help/8/programs/anvi-estimate-metabolism/). Thanks to help from [Antonio Fernandez-Guerra](https://anvio.org/people/genomewalker), this program now can accept a simple list of enzymes as input. See PR #1890 as well as [this help section](https://anvio.org/help/8/programs/anvi-estimate-metabolism/#estimation-for-a-set-of-enzymes).\r\n- The output options and formats for [anvi-estimate-metabolism](https://anvio.org/help/8/programs/anvi-estimate-metabolism/) are different. See [this page](https://anvio.org/help/8/artifacts/kegg-metabolism/) for details. One new output feature that may particularly help with interpretation of these data is the addition of columns related to enzymes that are unique to a given metabolic module. These are described in PR #1867.\r\n\r\n# A new anvi'o workflow to study phylogeography of any gene family\r\n\r\nExploring the ecology and evolution of microbes across environments with metagenomic data is a common task for microbiologists. What if we applied this framework to gene families? The availability of large metagenomic datasets and fast computational biology toolsets provide us a unique opportunity to explore the limits of gene diversity! To leverage this, [Matthew Schechter](https://anvio.org/people/mschecht) led the development of the [ecophylo workflow](https://anvio.org/help/8/workflows/ecophylo/), which can simultaneously profile **eco**logical and **phylo**genetic relationships between gene families and environments.\r\n\r\nThe final output of the [ecophylo workflow](https://anvio.org/help/8/workflows/ecophylo/) is an [interactive interface](https://anvio.org/help/8/artifacts/interactive) that includes **(1)** a phylogenetic analysis of all genes detected by the HMM in genomes and/or metagenomes, and **(2)** the distribution pattern of each of these genes across metagenomes if the user provided metagenomic short reads to survey.\r\n\r\nFor more details please see the [ecophylo documentation](https://anvio.org/help/8/workflows/ecophylo/).\r\n\r\n# A new anvi'o framework to identify genomic inversions and quantify their activity \r\n\r\nGenetic variants can rapidly proliferate even in populations taht grow from a single cell. One class of such variants emerge from 'inversions', a genetic phenomenon through which a microorganism can mediate the ON/OFF orientation of a promoter region regulating the expression of a downstream gene. Using paired-end short reads and quantifying their orientation upon mapping to a genomic context, one can identify and quantify inversions and their activities.\r\n\r\nThanks to [Florian Trigodet](https://anvio.org/people/FlorianTrigodet/)'s efforts, this version of anvi'o comes with a new program, [anvi-report-inversions](https://anvio.org/help/8/programs/anvi-report-inversions/) to study inversions in genomes and metagenomes across environments and to quantify the relative proportion of each inversion orientation in each sample. \r\n\r\nThe [anvi-report-inversions](https://anvio.org/help/8/programs/anvi-report-inversions/) workflow will (1) **find genomic regions of interest** (based on short-read recruitment data), (2) **find palindromic motifs in regions of interest** (where the pair of inverted repeats (IR) that surround the inversion site is found, (3) **confirm the inversion** (by going back to the BAM file and make sure the IR is the true one among multiple potential IRs that may occur in the region of interest), (4) **compute the inversion activity** (using the raw R1/R2 sequences from FASTQ files find support for activity, and (5) **generate extensive reporting** (including the genomic context, and genes that surround the inversion site). These reports will include a lot of information in text file outputs (see [inversion-txt](https://anvio.org/help/8/artifacts/inversions-txt) for details), as well as a static HTML output that does not require an anvi'o installation to browse.\r\n\r\n![image](https://github.com/merenlab/anvio/assets/197307/3c553f5f-9dcb-4369-949b-6e46f310ec76)\r\n\r\n# A new suite of programs to analyze Transfer RNA transcripts\r\n\r\nAnvi'o now includes a comprehensive (yet very experimental) software framework to support the analysis of tRNA transcript sequencing (as demonstrated [here](https://www.nature.com/articles/s41467-022-30261-3)). The 'tRex Tools', as [Samuel Miller](https://anvio.org/people/semiller10) calls them, include new programs for the identification of tRNA sequences and their modification sites in tRNA-seq results. The primary output of tRex tools in anvi'o is a set of **tRNA seeds**, each of which represents a mature tRNA sequence (minus the 3’-CCA acceptor) from the input set of samples. These capabilities are implemented in a set of programs that can be run individually or as part of the [tRNAseq workflow](https://anvio.org/help/8/workflows/trnaseq/):\r\n\r\n- The [`anvi-trnaseq` program](https://anvio.org/help/8/programs/anvi-trnaseq/) predicts tRNA sequences, structures, and modifications from a single tRNA-seq library\r\n- The [`anvi-merge-trnaseq` program](https://anvio.org/help/8/programs/anvi-merge-trnaseq/) combines the results across multiple tRNA-seq libraries and computes a final set of tRNA seeds (as well as their coverage across samples)\r\n- To analyze the taxonomy associated with tRNA sequences, there are two programs to be run in sequence: [`anvi-run-trna-taxonomy`](https://anvio.org/help/8/programs/anvi-run-trna-taxonomy/) and [`anvi-estimate-trna-taxonomy`](https://anvio.org/help/8/programs/anvi-estimate-trna-taxonomy/)\r\n- Finally, the program [`anvi-tabulate-trnaseq`](https://anvio.org/help/8/programs/anvi-tabulate-trnaseq/) exports the tRNA-associated coverage and modification data as tab-delimited files\r\n\r\nYou can also generate nice plots of the tRNA seed coverages and modification sites with [`anvi-plot-trnaseq`](https://anvio.org/help/8/programs/anvi-plot-trnaseq/).\r\n\r\nA new variant of the contigs database -- the [trnaseq-contigs-db](https://anvio.org/help/8/artifacts/trnaseq-contigs-db/), which stores tRNA seeds instead of contigs -- and a new variant of the profile database -- the [trnaseq-profile-db](https://anvio.org/help/8/artifacts/trnaseq-profile-db/), which stores modification positions and both specific and non-specific coverage of tRNA seeds -- makes the integration of these new data types possible.\r\n\r\nThis is quite an experimental workflow, and if you plan to use it, please get in touch with us.\r\n\r\n\r\n---\r\n\r\nPlease follow the latest installation instructions at https://anvio.org/install/ and come to the [anvi'o Discord channel](https://discord.gg/C6He6mSNY4) if you have any qeustions or concerns, or to simply join our community.",v8,A. Murat Eren (Meren),,meren,GNU General Public License v3.0,anvio,merenlab,29,metagenomics,metatranscriptomics,pangenomics,comparative-genomics,science,visualization,bioinformatics,phylogenomics,population-genetics,python,javascript,anvio,,,,,,,,,/merenlab/anvio,36,32,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/mbk-dev/okama,https://github.com/mbk-dev/okama,0,,,0,0,0,0,0,0,1,0,0,0,0,Investment portfolio and stocks analyzing tools for Python with free historical data,"\n[![Documentation Status](https://img.shields.io/readthedocs/okama.svg?style=popout)](http://okama.readthedocs.io/)\n[![Python](https://img.shields.io/badge/python-v3-brightgreen.svg)](https://www.python.org/)\n[![PyPI Latest Release](https://img.shields.io/pypi/v/okama.svg)](https://pypi.org/project/okama/)\n[![Coverage](https://coveralls.io/repos/github/mbk-dev/okama/badge.svg?branch=master)](https://coveralls.io/github/mbk-dev/okama?branch=master)\n[![License](https://img.shields.io/pypi/l/okama.svg)](https://opensource.org/licenses/MIT)\n[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mbk-dev/okama/blob/master/examples/01%20howto.ipynb)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\n# Okama\n\n_okama_ is a library with investment portfolio analyzing & optimization tools. CFA recommendations are used in quantitative finance.\n\n_okama_ goes with **free** «end of day» historical stock markets data and macroeconomic indicators through API.\n>...entities should not be multiplied without necessity\n>\n> -- <cite>William of Ockham (c. 1287–1347)</cite>\n\n## Table of contents\n\n- [Okama main features](#okama-main-features)\n- [Financial data and macroeconomic indicators](#financial-data-and-macroeconomic-indicators)\n  - [End of day historical data](#end-of-day-historical-data)\n  - [Macroeconomic indicators](#macroeconomic-indicators)\n  - [Other historical data](#other-historical-data)\n- [Installation](#installation)\n- [Getting started](#getting-started)\n- [Documentation](#documentation)\n- [Financial Widgets](#financial-widgets)\n- [RoadMap](#roadmap)\n- [Contributing to okama](#contributing-to-okama)\n- [Communication](#communication)\n\n## Okama main features\n\n- Investment portfolio constrained Markowitz Mean-Variance Analysis (MVA) and optimization\n- Rebalanced portfolio optimization with constraints (multi-period Efficient Frontier)\n- Investment portfolios with contributions / withdrawals cash flows (DCF)\n- Monte Carlo Simulations for financial assets and investment portfolios\n- Popular risk metrics: VAR, CVaR, semi-deviation, variance and drawdowns\n- Different financial ratios: CAPE10, Sharpe ratio, Sortino ratio, Diversification ratio \n- Forecasting models according to normal, lognormal and other popular distributions\n- Testing distribution on historical data\n- Dividend yield and other dividend indicators for stocks\n- Backtesting and comparing historical performance of broad range of assets and indexes in multiple currencies\n- Methods to track the performance of index funds (ETF) and compare them with benchmarks\n- Main macroeconomic indicators: inflation, central banks rates\n- Matplotlib visualization scripts for the Efficient Frontier, Transition map and assets risk / return performance\n\n## Financial data and macroeconomic indicators\n\n### End of day historical data\n\n- Stocks and ETF for main world markets\n- Mutual funds\n- Commodities\n- Stock indexes\n\n### Currencies\n\n- FX currencies\n- Crypto currencies\n- Central bank exchange rates\n\n### Macroeconomic indicators\nFor many countries (China, USA, United Kingdom, European Union, Russia, Israel etc.):  \n\n- Inflation\n- Central bank rates\n- CAPE10 (Shiller P/E) Cyclically adjusted price-to-earnings ratios\n\n### Other historical data\n\n- Real estate prices\n- Top bank rates\n\n## Installation\n\n`pip install okama`\n\nThe latest development version can be installed directly from GitHub:\n`git clone https://github.com/mbk-dev/okama@dev`\n\n`poetry install`\n\n\n## Getting started\n\n### 1. Compare several assets from different stock markets. Get USD-adjusted performance\n\n```python\nimport okama as ok\n\nx = ok.AssetList(['SPY.US', 'BND.US', 'DBXD.XFRA'], ccy='USD')\nx  # all examples are for Jupyter Notebook/iPython. For raw Python interpreter use 'print(x)' instead.\n\n```\n![](../images/images/readmi01.jpg?raw=true) \n\nGet the main parameters for the set:\n```python\nx.describe()\n```\n![](../images/images/readmi02.jpg?raw=true) \n\nGet the assets accumulated return, plot it and compare with the USD inflation:\n```python\nx.wealth_indexes.plot()\n```\n![](../images/images/readmi03.jpg?raw=true) \n\n### 2. Create a dividend stocks portfolio with base currency EUR\n\n```python\nweights = [0.3, 0.2, 0.2, 0.2, 0.1]\nassets = ['T.US', 'XOM.US', 'FRE.XFRA', 'SNW.XFRA', 'LKOH.MOEX']\npf = ok.Portfolio(assets, weights=weights, ccy='EUR')\npf.table\n```\n![](../images/images/readmi04.jpg?raw=true) \n\nPlot the dividend yield of the portfolio (adjusted to the base currency).\n\n```python\npf.dividend_yield.plot()\n```\n![](../images/images/readmi05.png?raw=true) \n\n### 3. Draw an Efficient Frontier for 2 popular ETF: SPY and GLD\n```python\nls = ['SPY.US', 'GLD.US']\ncurr = 'USD'\nlast_date='2020-10'\n# Rebalancing periods is one year (default value)\nfrontier = ok.EfficientFrontierReb(ls, last_date=last_date, ccy=curr, rebalancing_period='year')\nfrontier.names\n```\n![](../images/images/readmi06.jpg?raw=true) \n\nGet the Efficient Frontier points for rebalanced portfolios and plot the chart with the assets risk/CAGR points:\n```python\nimport matplotlib.pyplot as plt\n\npoints = frontier.ef_points\n\nfig = plt.figure(figsize=(12,6))\nfig.subplots_adjust(bottom=0.2, top=1.5)\nfrontier.plot_assets(kind='cagr')  # plots the assets points on the chart\nax = plt.gca()\nax.plot(points.Risk, points.CAGR) \n```\n![](../images/images/readmi07.jpg?raw=true)   \n<nowiki>*</nowiki> - *rebalancing period is one year*.\n\n### 4. Get a Transition Map for allocations\n```python\nls = ['SPY.US', 'GLD.US', 'BND.US']\nmap = ok.EfficientFrontier(ls, ccy='USD').plot_transition_map(x_axe='risk')\n```\n![](../images/images/readmi08.jpg?v23-11-2020,raw=true ""Transition map"")  \n\nMore examples are available in form of [Jupyter Notebooks](https://github.com/mbk-dev/okama/tree/master/examples).\n\n## Documentation\n\nThe official documentation is hosted on readthedocs.org: [https://okama.readthedocs.io/](https://okama.readthedocs.io/)\n\n## Financial Widgets\n[okama-dash](https://github.com/mbk-dev/okama-dash) repository has interactive financial widgets (multi-page web application) \nbuild with _okama_ package and [Dash (plotly)](https://github.com/plotly/dash) framework. Working example is available at \n[okama.io](https://okama.io/).\n\n![](https://github.com/mbk-dev/okama-dash/blob/images/images/main_page.jpg?raw=true) \n\n## RoadMap\n\nThe plan for _okama_ is to add more functions that will be useful to investors and asset managers.\n\n- Add Omega ratio to EfficientFrontier, EfficientFrontierReb and Portfolio classes.\n- Make complex withdrawals / contributions strategies in Portfolio class.\n- Make complex portfolio rebalancing strategies.\n- Add Black-Litterman asset allocation \n- Accelerate optimization for multi-period Efficient Frontier: minimize_risk and maximize_risk methods of EfficientFrontierReb class.\n- Make a single EfficientFrontier class for all optimizations: single-period or multu-period with rebalancing period as a parameter.\n- Add different utility functions for optimizers: IRR, portfolio survival period, semi-deviation, VaR, CVaR, drawdowns etc.\n- Add more functions based on suggestion of users.\n\n## Contributing to okama\n\nContributions are *most welcome*. Have a look at the [Contribution Guide](https://github.com/mbk-dev/okama/blob/master/CONTRIBUTING.md) for more.  \nFeel free to ask questions on [Discussuions](https://github.com/mbk-dev/okama/discussions).  \nAs contributors and maintainers to this project, you are expected to abide by okama' code of conduct. More information can be found at: [Contributor Code of Conduct](https://github.com/mbk-dev/okama/blob/master/CODE_OF_CONDUCT.md)\n\n## Communication\n\nFor basic usage questions (e.g., ""_Is XXX currency supported by okama?_"") and for sharing ideas please use [GitHub Discussions](https://github.com/mbk-dev/okama/discussions/3).\nRussian language community is available at [okama.io forums](https://community.okama.io).\n\n## License\n\nMIT\n",191,mathematics,Python,2,Python,Jupyter Notebook,,,,,,,,,,,,,,,,,,,,,,,,,,,27,14,13,0,8,7,0,24207,34,36,29,7,6c1d9ec33d81335eb466ca78f95555f32578a6b9,fix: import RollingWindowLengthBelowOneYearError,2024-07-06T13:39:25Z,Sergey Kikevich,sergey@rostsber.ru,chilango74,Okama 1.4.1,"Okama 1.4.1 adds custom exceptions for time period and Student's t distribution for Monte-Carlo methods in Portfolio and AssetList.\r\n\r\n## New features\r\n### Custom exceptions for time periods issues\r\n- `ShortPeriodLengthError` is raised when an asset has less then 3 months of history in AssetList, Portfolio and EfficentFrontier classes\r\n- `RollingWindowLengthBelowOneYearError` is raised when rolling windows size is below one year\r\n- `LongRollingWindowLengthError` is raised when rolling window size is more than data history depth\r\n\r\n### Student's t distribution in Monte-Carlo methods\r\n- set `distr=""t""` in methods like `Portfolio.dcf.monte_carlo_wealth` or `AssetList.kstest`\r\n\r\n### Changes in existing methods & properties\r\n- `Portfolio.dcf.monte_carlo_wealth` uses `initial_amount` value by default.\r\n\r\n## Bugs fixed\r\n- wrong formula for 0 period rate of return in `helpers.Rebalance.return_ror_ts` ",v1.4.1,Sergey Kikevich,,chilango74,MIT License,okama,mbk-dev,38,finance,investments,mathematics,efficient-frontier,asset-allocation,python,optimization,portfolio-optimization,quantitative-finance,historical-data,macroeconomic-indicators,portfolio,financial-data,assets-risk,risk-management,rebalancing,cfa,chartered-financial-analyst,time-series,,/mbk-dev/okama,46,11,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/mattools/matGeom,https://github.com/mattools/matGeom,0,,,0,0,0,0,0,0,1,1,0,0,0,Matlab geometry toolbox for 2D/3D geometric computing,"# MatGeom\nMATLAB geometry processing library in 2D/3D.\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7799184.svg)](https://doi.org/10.5281/zenodo.7799184)\n\n**MatGeom** is a library for geometry processing / geometric computing with MATLAB in 2D and 3D. \nMatGeom is a “function-based” library: it contains several hundreds of functions for the creation,\nmanipulation and display of 2D and 3D shapes such as point sets, lines, ellipses, polygons, \n3D polygonal meshes, ...\nThe official homepage for the project is http://github.com/mattools/matGeom. \n\nA [user manual](https://github.com/mattools/matGeom/releases/download/v1.2.7/matGeom-manual-1.2.7.pdf) \ncontaining a large number of illustrations and examples is available.\nStarting from February 2022, the HTML pages of the functions (obtained with m2html) are available \n[here](https://mattools.github.io/matGeom/api/index.html).\n\nThe MatGeom library corresponds to the concatenation of the \n""[geom2d](https://fr.mathworks.com/matlabcentral/fileexchange/7844-geom2d)"" \nand \n""[geom3d](https://fr.mathworks.com/matlabcentral/fileexchange/24484-geom3d)"" \nlibraries that were distributed on the FileExchange. Distribution as a single library greatly \nfacilitates the interoperability of the functions.\n\nIf you use matGeom, you might cite it as follows:\n```\nDavid Legland et al. (2024) ""MatGeom library for geometric computing with MATLAB"" DOI: 10.5281/zenodo.7799184\n```\n\n## Package organization\nThe library is organized into several modules:\n* [geom2d](https://github.com/mattools/matGeom/wiki/geom2d ""geom2d Wiki page"") - General functions in Euclidean plane\n* [polygons2d](https://github.com/mattools/matGeom/wiki/polygons2d ""polygons2d Wiki page"") - Functions operating on polygons and polylines represented as list of vertices\n* [graphs](https://github.com/mattools/matGeom/wiki/graphs ""graphs Wiki page"") - Manipulation of geometric graphs\n* [geom3d](https://github.com/mattools/matGeom/wiki/geom3d ""geom3d Wiki page"") - General functions in 3D Euclidean space\n* [meshes3d](https://github.com/mattools/matGeom/wiki/meshes3d ""meshes3d Wiki page"") - Manipulation of 3D polygonal meshes (trimesh, quadmesh, or more generic meshes)\n\n## Quick overview\nBasic functionalities comprise creation of simple geometries such as points, lines, ellipses... \nAn example is provided in the following script.\n\n    % load data\n    data = load('fisheriris');\n    pts = data.meas(:, [3 1]);\n    % display\n    figure; axis equal; hold on; axis([0 8 3 9]);\n    drawPoint(pts, 'bx');\n    % Fit line\n    line = fitLine(pts);\n    drawLine(line, 'color', 'k', 'linewidth', 2);\n    % Draw oriented box\n    obox = orientedBox(pts);\n    drawOrientedBox(obox, 'color', 'k', 'linewidth', 1);\n    % identifiy species index\n    [labels, ~, inds]= unique(str.species);\n    % for ech species, compute equivalent ellipse and display with axes\n    colors = [1 0 0; 0 0.8 0; 0 0 1];\n    for i = 1:3\n        pts_i = pts(inds == i, :);\n        drawPoint(pts_i, 'marker', 'x', 'color', colors(i,:), 'linewidth', 2);\n        elli = equivalentEllipse(pts_i);\n        drawEllipse(elli, 'color', colors(i,:), 'linewidth', 2)\n        drawEllipseAxes(elli, 'color', colors(i,:), 'linewidth', 2)\n    end\n\n![Computation of equivalent ellipses, oriented box, and fitting line from set of points](https://github.com/mattools/matGeom/blob/master/docs/images/demo_geom2d_iris.png)\n\nIt is possible to work with more complex shapes such as polygonal lines (""polylines"") or polygons.\nCommon operations comprise smoothing, simplification (retaining only a selection of vertices), \ncomputation of convex hull or of intersections with other geometric primitives. \nA summary of typical operations in presented in the following script.\n\n    % read polygon data as a numeric N-by-2 array\n    poly = load('leaf_poly.txt');\n    \n    % display the polygon using basic color option\n    figure; axis equal; hold on; axis([0 600 0 400]);\n    drawPolygon(poly, 'k');\n    \n    % Bounding box of the polygon\n    poly_bnd = boundingBox(poly);\n    drawBox(poly_bnd, 'k');\n    \n    % computes convex hull of polygon vertices\n    poly_hull = convexHull(poly);\n    drawPolygon(poly_hull, 'LineWidth', 2, 'Color', 'k');\n    \n    % applies smoothing to the original polygon.\n    poly_smooth = smoothPolygon(poly, 51);\n    drawPolygon(poly_smooth, 'color', 'b', 'linewidth', 2);\n    \n    % Computes a simplified version of the polygon\n    poly_simpl = simplifyPolygon(poly, 20);\n    drawPolygon(poly_simpl, 'color', 'r', 'linewidth', 2);\n    drawVertices(poly_simpl, 'Color', 'k', 'Marker', 's', 'MarkerFaceColor', 'w');\n    \n    % compute intersections with an arbitrary line\n    line = createLine([0 250], [600 350]);\n    drawLine(line, 'k');\n    inters = intersectLinePolygon(line, poly_simpl);\n    drawPoint(inters, 'Color', 'r', 'Marker', 'o', 'MarkerFaceColor', 'w', 'linewidth', 2);\n\n![Summary of polygon processing operations: smoothing, simplification, convex hull, intersection with lines.](https://github.com/mattools/matGeom/blob/master/docs/images/leafPoly_variousOps.png)\n",255,geometry,MATLAB,3,MATLAB,HTML,M,,,,,,,,,,,,,,,,,,,,,,,,,,139,11,128,0,4,14,10,9813,95,60,45,15,a9f273f5a65a35a6060d07757503dd82faacbd66,doc: update user manual,2024-07-19T13:51:06Z,dlegland,david.legland@inrae.fr,dlegland,MatGeom 1.2.7,"MatGeom is a library for geometry processing / geometric computing with Matlab. It contains several hundreds of functions for the creation, the manipulation and the display of 2D and 3D geometries such as point sets, lines, polygons, 3D meshes, ellipses...\r\n\r\n## Installation\r\n\r\nYou can follow one of these procedures (from the simplest one to the most technical):\r\n\r\n* From Matlab's ""Add-Ons"" menu, choose ""GetAdd-Ons...."", and look for MatGeom. Then simply install or update the toolbox.\r\n* Download the file self-installable ""matGeom-1.2.7.mltbx"" from the release, and run it from Matlab.\r\n* Download one of the ""matGeom-1.2.7.zip"" or ""matGeom-full-1.2.7.zip"" file, and uncompress in the directiory of your choice. Then, open Matlab, set the path to the directory containing the script ""setupMatGeom"", and run the script. This should install the main files of the library.\r\n\r\n## New features in 1.2.7\r\n\r\nVersion 1.2.7 is mostly a consolidation and cleanup version, containing several changes in function names. Several new functions and bug fixes are included as well.\r\n\r\n### New features\r\n\r\n* Several new functions for mesh processing: clipMeshByPlane.m, removeDuplicateVertices.m, meshBoundaryEdges.m, removeUnreferencedVertices.m, thanks to oqilipo \r\n* meshes3d/intersectPlaneMesh.m now supports intersections with open meshes\r\n* added function distancePointCircle3d.m\r\n* enhance reading of 3D meshes with OBJ format\r\n\r\n### Bug fixes\r\n\r\n* geom2d/intersectEdges.m: fixed management of colinear edges (bug #157)\r\n*  fixed drawing of multiple shapes with single call to drawXXX function (in both geom2d and geom3d modules)\r\n* fixed fillPolygon (thanks to oqilipo)\r\n* removed several obsolete functions to avoid function name clashes\r\n\r\n### Documentation\r\n* Huge effort of documentation homogeneization, especially in function headers and function names, thanks to oqilipo \r\n* Several updates in the user manual\r\n\r\nFull Changelog (from v1.2.6): https://github.com/mattools/matGeom/compare/v1.2.6...v1.2.7",v1.2.7,dlegland,,dlegland,"BSD 2-Clause ""Simplified"" License",matGeom,mattools,9,matlab,polygon,geometry,geometry-processing,mesh,matlab-geometry-toolbox,geometry-library,geometric-shapes,,,,,,,,,,,,,/mattools/matGeom,9,22,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/MannLabs/alphapept,https://github.com/MannLabs/alphapept,1,,,1,1,1,1,0,0,0,0,0,0,1,"A modular, python-based framework for mass spectrometry. Powered by nbdev.","AlphaPept\n================\n\n<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->\n\n[![nbdev\nCI](https://github.com/MannLabs/alphapept/actions/workflows/nbdev.yaml/badge.svg?branch=master)](https://github.com/MannLabs/alphapept/actions/workflows/nbdev.yaml)\n[![launch -\nrenku](https://renkulab.io/renku-badge.svg)](https://renkulab.io/projects/renku-stories/alphapept-gui-streamlit)\n[![DOI:10.1038/s41467-024-46485-4](http://img.shields.io/badge/DOI-10.1038/s41467-024-46485-4-B31B1B.svg)](https://doi.org/10.1038/s41467-024-46485-4)\n\n![](https://i.imgur.com/xkFtDff.jpg)\n\n**AlphaPept: a modern and open framework for MS-based proteomics**\n\n[Nature Communications](https://doi.org/10.1038/s41467-024-46485-4).\n\nBe sure to check out other packages of our ecosystem:\n\n- [alphatims](https://github.com/MannLabs/alphatims): Fast access to\n  TimsTOF data.\n- [alphamap](https://github.com/MannLabs/alphamap): Peptide level MS\n  data exploration.\n- [alphapeptdeep](https://github.com/MannLabs/alphapeptdeep): Predicting\n  properties from peptides.\n- [alphapeptstats](https://github.com/MannLabs/alphapeptstats):\n  Downstream analysis of MS data\n- [alphaviz](https://github.com/MannLabs/alphaviz): Vizualization of MS\n  data.\n\n## Windows Quickstart\n\n![](https://i.imgur.com/UO64YPx.jpg)\n\n1.  Download the latest installer\n    [here](https://github.com/MannLabs/alphapept/releases/latest),\n    install and click the shortcut on the desktop. A browser window with\n    the AlphaPept interface should open. In the case of Windows Firewall\n    asking for network access for AlphaPept, please allow.\n2.  In the `New Experiment`, select a folder with raw files and FASTA\n    files.\n3.  Specify additional settings such as modifications with `Settings`.\n4.  Click `Start` and run the analysis.\n\nSee also below for more detailed instructions.\n\n## Current functionality\n\n| Feature         | Implemented    |\n|-----------------|----------------|\n| Type            | DDA            |\n| Filetypes       | Bruker, Thermo |\n| Quantification  | LFQ            |\n| Isobaric labels | None           |\n| Platform        | Windows        |\n\nLinux and macOS should, in principle, work but are not heavily tested\nand might require additional work to set up (see detailed instructions\nbelow). To read Thermo files, we use Mono, which can be used on Mac and\nLinux. For Bruker files, we can use Linux but not yet macOS.\n\n## Python Installation Instructions\n\n### Requirements\n\nWe highly recommend the [Anaconda](https://www.anaconda.com) or\n[Miniconda](https://docs.conda.io/en/latest/miniconda.html) Python\ndistribution, which comes with a powerful package manager. See below for\nadditional instructions for Linux and Mac as they require additional\ninstallation of Mono to use the RawFileReader.\n\nAlphaPept can be used as an application as a whole or as a Python\nPackage where individual modules are called. Depending on the use case,\nAlphaPept will need different requirements, and you might not want to\ninstall all of them.\n\nCurrently, we have the default `requirements.txt`, additional\nrequirements to run the GUI `gui` and packages used for developing\n`develop`.\n\nTherefore, you can install AlphaPept in multiple ways:\n\n- The default `alphapept`\n- With GUI-packages `alphapept[gui]`\n- With pacakges for development `alphapept[develop]`\n  (`alphapept[develop,gui]`) respectively\n\nThe requirements typically contain pinned versions and will be\nautomatically upgraded and tested with `dependabot`. This `stable`\nversion allows having a reproducible workflow. However, in order to\navoid conflicts with package versions that are too strict, the\nrequirements are not pinned when being installed. To use the strict\nversion use the `-stable`-flag, e.g. `alphapept[stable]`.\n\nFor end-users that want to set up a processing environment in Python,\nthe `""alphapept[stable,gui-stable]""` is the `batteries-included`-version\nthat you want to use.\n\n### Python\n\nIt is strongly recommended to install AlphaPept in its own\nenvironment. 1. Open the console and create a new conda environment:\n`conda create --name alphapept python=3.8` 2. Activate the environment:\n`conda activate alphapept` 3. Install AlphaPept via pip:\n`pip install ""alphapept[stable,gui-stable]""`. If you want to use\nAlphaPept as a package without the GUI dependencies and without strict\nversion dependencies, use `pip install alphapept`.\n\nIf AlphaPept is installed correctly, you should be able to import\nAlphaPept as a package within the environment; see below.\n\n------------------------------------------------------------------------\n\n#### Linux\n\n1.  Install the build-essentials:\n    `sudo apt-get install build-essential`.\n2.  Install AlphaPept via pip:\n    `pip install ""alphapept[stable,gui-stable]""`. If you want to use\n    AlphaPept as a package withouth the GUI dependencies and strict\n    version dependencies use `pip install alphapept`.\n3.  Install libgomp.1 with `sudo apt-get install libgomp1`.\n\n##### Bruker Support\n\n4.  Copy-paste the Bruker library for feature finding to your /usr/lib\n    folder with\n    `sudo cp alphapept/ext/bruker/FF/linux64/alphapeptlibtbb.so.2 /usr/lib/libtbb.so.2`.\n\n##### Thermo Support\n\n5.  Install Mono from mono-project website [Mono\n    Linux](https://www.mono-project.com/download/stable/#download-lin).\n    NOTE, the installed mono version should be at least 6.10, which\n    requires you to add the ppa to your trusted sources!\n6.  Install pythonnet with `pip install pythonnet>=2.5.2`\n\n------------------------------------------------------------------------\n\n#### Mac\n\n1.  Install AlphaPept via pip:\n    `pip install ""alphapept[stable,gui-stable]""`. If you want to use\n    AlphaPept as a package withouth the GUI dependencies and strict\n    version dependencies use `pip install alphapept`.\n\n##### Bruker Support\n\n> Only supported for preprocessed files.\n\n##### Thermo Support\n\n2.  Install [brew](https://brew.sh) and pkg-config:\n    `brew install pkg-config`\n3.  Install Mono from mono-project website [Mono\n    Mac](https://www.mono-project.com/download/stable/)\n4.  Register the Mono-Path to your system: For macOS Catalina, open the\n    configuration of zsh via the terminal:\n\n- Type in `cd` to navigate to the home directory.\n- Type `nano ~/.zshrc` to open the configuration of the terminal\n- Add the path to your mono installation:\n  `export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig:/usr/lib/pkgconfig:/Library/Frameworks/Mono.framework/Versions/Current/lib/pkgconfig:$PKG_CONFIG_PATH`.\n  Make sure that the Path matches to your version (Here 6.12.0)\n- Save everything and execute `. ~/.zshrc`\n\n4.  Install pythonnet with `pip install pythonnet>=2.5.2`\n\n------------------------------------------------------------------------\n\n#### Developer\n\n1.  Redirect to the folder of choice and clone the repository:\n    `git clone https://github.com/MannLabs/alphapept.git`\n2.  Navigate to the alphapept folder with `cd alphapept` and install the\n    package with `pip install .` (default users) or with\n    `pip install -e .` to enable developers mode. Note that you can use\n    the different requirements here aswell\n    (e.g. `pip install "".[gui-stable]""`)\n\n#### GPU Support\n\nSome functionality of AlphaPept is GPU optimized that uses Nvidia’s\nCUDA. To enable this, additional packages need to be installed.\n\n1.  Make sure to have a working [CUDA\n    toolkit](https://developer.nvidia.com/cuda-toolkit) installation\n    that is compatible with CuPy. To check type `nvcc --version` in your\n    terminal.\n2.  Install [cupy](https://cupy.dev). Make sure to install the cupy\n    version matching your CUDA toolkit (e.g. `pip install cupy-cuda110`\n    for CUDA toolkit 11.0.\n\n### Additional Notes\n\n> To access Thermo files, we have integrated\n> [RawFileReader](https://planetorbitrap.com/rawfilereader) into\n> AlphaPept. We rely on [Mono](https://www.mono-project.com/) for\n> Linux/Mac systems.\n\n> To access Bruker files, we rely on the `timsdata`-library. Currently,\n> only Windows is supported. For feature finding, we use the Bruker\n> Feature Finder, which can be found in the `ext` folder of this\n> repository.\n\n#### Notes for NBDEV\n\n- For developing with the notebooks, install the nbdev package (see the\n  development requirements)\n- To facilitate navigating the notebooks, use jupyter notebook\n  extensions. They can be called from a running jupyter instance like\n  so: `http://localhost:8888/nbextensions`. The extensions\n  `collapsible headings` and `toc2` are very beneficial.\n\n## Standalone Windows Installer\n\nTo use AlphaPept as a stand-alone program for end-users, it can be\ninstalled on Windows machines via a one-click installer. Download the\nlatest version\n[here](https://github.com/MannLabs/alphapept/releases/latest).\n\n## Docker\n\nIt is possible to run AlphaPept in a docker container. For this, we\nprovide two Dockerfiles: `Dockerfile_thermo` and `Dockerfile_bruker`,\ndepending on which filetypes you want to analyse. They are split because\nof drastically different requirements.\n\nTo run, navigate to the AlphaPept repository and rename the dockerfile\nyou want to use, e.g. `Dockerfile_thermo` to `Dockerfile`.\n\n- Build the image with: `docker build -t docker-alphapept:latest .`\n- To run use\n  `docker run -p 8505:8505 -v /Users/username/Desktop/docker:/home/alphapept/ docker-alphapept:latest alphapept gui`\n  (Note that -v maps a local folder for convient file transfer)\n- Access the AlphaPept GUI via `localhost:8505` in your browser.\n- Note 1: The Thermo Dockerfile is built on a Jupyter image, so you can\n  also start a jupyter instance:\n  `docker run -p 8888:8888 -v /Users/username/Desktop/docker:/home/jovyan/ docker-alphapept:latest jupyter notebook --allow-root`\n\n### Docker Troubleshooting on M1-Mac\n\n- The Thermo dockerfile was tested on an M1-Mac. Resources were set to\n  18GB RAM and 2 CPUs, 200 GB disk\n- It was possible to build the Bruker dockerfile with the platform tag\n  `--platform linux/amd64`. However, it was very slow and the Bruker\n  file is not recommended for an M1-Mac. Windows worked nicely.\n\n## Additional Documentation\n\nThe documentation is automatically built based on the jupyter notebooks\n(nbs/index.ipynb) and can be found\n[here](https://mannlabs.github.io/alphapept/):\n\n## Version Performance\n\nAn overview of the performance of different versions can be found\n[here](https://charts.mongodb.com/charts-alphapept-itfxv/public/dashboards/5f671dcf-bcd6-4d90-8494-8c7f724b727b).\nWe re-run multiple tests on datasets for different versions so that\nusers can assess what changes from version to version. Feel free to\n[suggest](https://github.com/MannLabs/alphapept/discussions) a test set\nin case.\n\n## How to use\n\nAlphaPept is meant to be a framework to implement and test new ideas\nquickly but also to serve as a performant processing pipeline. In\nprinciple, there are three use-cases:\n\n- GUI: Use the graphical user interface to select settings and process\n  files manually.\n- CMD: Use the command-line interface to process files. Useful when\n  building automatic pipelines.\n- Python: Use python modules to build individual workflows. Useful when\n  building customized pipelines and using Python as a scripting language\n  or when implementing new ideas.\n\n### Windows Standalone Installation\n\nFor the [windows\ninstallation](https://github.com/MannLabs/alphapept/releases/latest),\nsimply click on the shortcut after installation. The windows\ninstallation also installs the command-line tool so that you can call\nalphapept via `alphapept` in the command line.\n\n![](https://i.imgur.com/SQikLHQ.jpg)\n\n### Python Package\n\nOnce AlphaPept is correctly installed, you can use it like any other\npython module.\n\n``` python\nfrom alphapept.fasta import get_frag_dict, parse\nfrom alphapept import constants\n\npeptide = 'PEPT'\n\nget_frag_dict(parse(peptide), constants.mass_dict)\n```\n\n    {'b1': 98.06004032687,\n     'b2': 227.10263342687,\n     'b3': 324.15539728686997,\n     'y1': 120.06551965033,\n     'y2': 217.11828351033,\n     'y3': 346.16087661033}\n\n### Using as a tool\n\nIf alphapept is installed an a conda or virtual environment, launch this\nenvironment first.\n\nTo launch the command line interface use: \* `alphapept`\n\nThis allows us to select different modules. To start the GUI use: \*\n`alphapept gui`\n\nTo run a workflow, use: \* `alphapept workflow your_own_workflow.yaml`\nAn example workflow is easily generated by running the GUI once and\nsaving the settings which can be modified on a per-project basis.\n\n### CMD / Python\n\n1.  Create a settings-file. This can be done by changing the\n    `default_settings.yaml` in the repository or using the GUI.\n2.  Run the analysis with the new settings file.\n    `alphapept run new_settings.yaml`\n\nWithin Python (i.e., Jupyter notebook) the following code would be\nrequired)\n\n    from alphapept.settings import load_settings\n    import alphapept.interface\n    settings = load_settings('new_settings.yaml')\n    r = alphapept.interface.run_complete_workflow(settings)\n\nThis also allows you to break the workflow down in indiviudal steps,\ne.g.:\n\n    settings = alphapept.interface.import_raw_data(settings)\n    settings = alphapept.interface.feature_finding(settings)\n\n## Notebooks\n\nWithin the notebooks, we try to cover most aspects of a proteomics\nworkflow:\n\n- Settings: General settings to define a workflow\n- Chem: Chemistry related functions, e.g., for calculating isotope\n  distributions\n- Input / Output: Everything related to importing and exporting and the\n  file formats used\n- FASTA: Generating theoretical databases from FASTA files\n- Feature Finding: How to extract MS1 features for quantification\n- Search: Comparing theoretical databases to experimental spectra and\n  getting Peptide-Spectrum-Matches (PSMs)\n- Score: Scoring PSMs\n- Recalibration: Recalibration of data based on identified peptides\n- Quantification: Functions for quantification, e.g., LFQ\n- Matching: Functions for Match-between-runs\n- Constants: A collection of constants\n- Interface: Code that generates the command-line-interface (CLI) and\n  makes workflow steps callable\n- Performance: Helper functions to speed up code with CPU / GPU\n- Export: Helper functions to make exports compatbile to other Software\n  tools\n- Label: Code for support isobaric label search\n- Display: Code related to displaying in the streamlit gui\n- Additional code: Overview of additional code not covered by the\n  notebooks\n- How to contribute: Contribution guidelines\n- AlphaPept workflow and files: Overview of the worfklow, files and\n  column names\n\n## Contributing\n\nIf you have a feature request or a bug report, please post it either as\nan idea in the\n[discussions](https://github.com/MannLabs/alphapept/discussions) or as\nan issue on the [GitHub issue\ntracker](https://github.com/MannLabs/alphapept/issues). Upvoting\nfeatures in the discussions page will help to prioritize what to\nimplement next. If you want to contribute, put a PR for it. You can find\nmore guidelines for contributing and how to get started\n[here](https://mannlabs.github.io/alphapept/contributing.html). We will\ngladly guide you through the codebase and credit you accordingly.\nAdditionally, you can check out the Projects page on GitHub. You can\nalso contact us via opensource@alphapept.com.\n\nIf you like the project, consider starring it!\n\n## Cite us\n\nIf you use this project in your research, please cite:\n\n> Strauss, M.T., Bludau, I., Zeng, WF. et al. AlphaPept: a modern and\n> open framework for MS-based proteomics. Nat Commun 15, 2168 (2024).\n> https://doi.org/10.1038/s41467-024-46485-4\n",166,mass-spectrometry,HTML,7,Python,Inno Setup,Batchfile,HTML,CSS,JavaScript,Dockerfile,,,,,,,,,,,,,,,,,,,,,,377,119,253,5,24,65,393,213970,29,143,125,18,c81f6f7072cad940aa95328f49d6b6cb6b252669,Bump version: 0.5.2 → 0.5.3,2024-04-19T15:51:00Z,Maximilian Strauss,straussmaximilian@gmail.com,straussmaximilian,Release v0.5.0,"# Summary:\r\n\r\n## Major\r\n### File reading changed to AlphaRaw\r\n- This allows SciEx support. FF is not yet optimized for SciEX yet\r\n- Adjusted memory limits for raw conversion, raw conversion for Bruker is now faster\r\n- Bruker legacy files are supported with the one-click installer but not for the Python version anymore. Install `python-lzf`  or install with the legacy requirements,\r\n- Working docker files \r\n\r\n\r\n## Minor\r\n- Option to change the top_n peptides\r\n- Option to select the initial score for ML scoring\r\n- Fixed a bug in peptide generation that would not consider the last AA\r\n- Added a new protease `protease_dict[""trypsin/p""] = ""[KR]""`\r\n- Small UI improvements\r\n\r\n## Performance:\r\n- Overall, Bruker runs should be 25% faster. We will expect further speed increases once the FF is exchanged\r\n- IDs dropped ~5%, but FDR estimates are well below 1% now\r\n\r\n**Full Changelog**: https://github.com/MannLabs/alphapept/compare/v0.4.9...v0.5.0",v0.5.0,,,github-actions[bot],Apache License 2.0,alphapept,MannLabs,23,mass-spectrometry,bioinformatics,proteomics,alphapept-ecosystem,,,,,,,,,,,,,,,,,/MannLabs/alphapept,24,11,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/MakieOrg/Makie.jl,https://github.com/MakieOrg/Makie.jl,0,,,0,0,0,0,0,0,1,1,0,0,0,Interactive data visualizations and plotting in Julia,"<div align=""center"">\n    <picture>\n      <source media=""(prefers-color-scheme: dark)"" \n        srcset=""/assets/makie_logo_canvas_dark.svg"" >\n      <img alt=""Makie.jl logo"" \n        src=""/assets/makie_logo_canvas.svg"" width=""350"">\n    </picture>\n</div>\n\n<div align=""center"">\n\n[![][docs-stable-img]][docs-stable-url]\n[![][docs-master-img]][docs-master-url]\n\n[![Build Status](https://github.com/MakieOrg/Makie.jl/actions/workflows/ci.yml/badge.svg?branch=master)](https://github.com/MakieOrg/Makie.jl/actions/workflows/ci.yml?query=branch%3Amaster)\n[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://github.com/MakieOrg/Makie.jl/blob/main/LICENSE)\n[![Downloads](https://img.shields.io/badge/dynamic/json?url=http%3A%2F%2Fjuliapkgstats.com%2Fapi%2Fv1%2Fmonthly_downloads%2FMakie&query=total_requests&suffix=%2Fmonth&label=Downloads)](http://juliapkgstats.com/pkg/Makie)\n\n[![JOSS][joss-img]][joss-url]\n\n[![Mastodon](https://img.shields.io/badge/-mastodon-%232B90D9?style=for-the-badge&logo=mastodon&logoColor=white)](https://julialang.social/@makie)\n\n[![chat][discord-img]][discord-url]\n\n</div>\n\nMakie is an interactive data visualization and plotting ecosystem for the [Julia programming language](https://julialang.org/), available on Windows, Linux and Mac.\nThe backend packages **GLMakie**, **WGLMakie**, **CairoMakie** and **RPRMakie** add different functionalities:\nYou can use Makie to interactively explore your data and create simple GUIs\nin native windows or web browsers, export high-quality vector graphics or even raytrace with physically accurate lighting.\n\nThe name Makie (we pronounce it Mah-kee) is derived from the japanese word [_Maki-e_](https://en.wikipedia.org/wiki/Maki-e), which is a technique to sprinkle lacquer with gold and silver powder.\nData is the gold and silver of our age, so let's spread it out beautifully on the screen!\n\nTo learn more, we invite you to visit the documentation at [docs.makie.org](http://docs.makie.org/stable/).\n\n[gitlab-img]: https://gitlab.com/JuliaGPU/Makie.jl/badges/master/pipeline.svg\n[gitlab-url]: https://gitlab.com/JuliaGPU/Makie.jl/pipelines\n[docs-stable-img]: https://img.shields.io/badge/Docs-Stable-lightgrey.svg\n[docs-stable-url]: http://docs.makie.org/stable/\n[docs-master-img]: https://img.shields.io/badge/Docs-Dev-blue.svg\n[docs-master-url]: http://docs.makie.org/dev/\n[joss-url]: https://doi.org/10.21105/joss.03349\n[joss-img]: http://joss.theoj.org/papers/10.21105/joss.03349/status.svg\n\n[discord-url]: https://discord.com/invite/2FBjYAT3cY\n[discord-img]: https://img.shields.io/discord/996787732149981214.svg?logo=discord&colorB=7289DA&label=Discord\n\n## Citing Makie\n\nIf you use Makie for a scientific publication, please acknowledge and support our work by citing [our JOSS paper](https://joss.theoj.org/papers/10.21105/joss.03349) the following way:\n\n```\nDanisch & Krumbiegel, (2021). Makie.jl: Flexible high-performance data visualization for Julia.\nJournal of Open Source Software, 6(65), 3349, https://doi.org/10.21105/joss.03349\n```\n\n\n<details>\n  <summary>BibTeX entry:</summary>\n\n```bib\n@article{DanischKrumbiegel2021,\n  doi = {10.21105/joss.03349},\n  url = {https://doi.org/10.21105/joss.03349},\n  year = {2021},\n  publisher = {The Open Journal},\n  volume = {6},\n  number = {65},\n  pages = {3349},\n  author = {Simon Danisch and Julius Krumbiegel},\n  title = {{Makie.jl}: Flexible high-performance data visualization for {Julia}},\n  journal = {Journal of Open Source Software}\n}\n```\n</details>\n\nor [Download the BibTeX file](./assets/DanischKrumbiegel2021.bibtex).\n\n## Community Channels\n\nWe are on [Discord](https://discord.com/invite/2FBjYAT3cY) and [Discourse](https://discourse.julialang.org/c/17?tags=Makie)! Community channels are a great way for you to ask questions and get help. Please join us!\n\n## Installation\n\nChoose one or more backend packages: **GLMakie** (interactive OpenGL in native OS windows), **WGLMakie** (interactive WebGL in browsers, IDEs, notebooks), **CairoMakie** (static 2D vector graphics and images) and **RPRMakie** (raytracing).\nEach backend re-exports all of Makie.jl so you don't have to install or load it explicitly.\n\nInstall:\n\n```julia\njulia>]\npkg> add GLMakie\n```\n\nCheck the installed version:\n\n```julia\n]st GLMakie\n```\n\nStart using the package:\n\n```julia\nusing GLMakie\n```\n\n## Developing Makie\n\n<details>\n  <summary><span style=""color:red""> 🔥 Click for more 🔥</span></summary>\n\nMakie and its backends all live in the Makie monorepo.\nThis makes it easier to change code across all packages.\nTherefore, dev'ing Makie almost works as with other Julia packages, just, that one needs to also dev the sub packages:\n\n```julia\n]dev --local Makie # local will clone the repository at ./dev/Makie\n]dev dev/Makie/MakieCore dev/Makie/GLMakie dev/Makie/CairoMakie dev/Makie/WGLMakie dev/Makie/RPRMakie\n```\n\nTo run the tests, you also should add:\n```julia\n]dev dev/Makie/ReferenceTests\n```\nFor more info about ReferenceTests, check out its [README](./ReferenceUpdater/README.md)\n</details>\n\n## Examples\n\nThe following examples are supposed to be self-explanatory. For further information [check out the documentation!](http://docs.makie.org/stable/)\n\n### A simple parabola\n\n```julia\nx = 1:0.1:10\nfig = lines(x, x.^2; label = ""Parabola"",\n    axis = (; xlabel = ""x"", ylabel = ""y"", title =""Title""),\n    figure = (; size = (800,600), fontsize = 22))\naxislegend(; position = :lt)\nsave(""./assets/parabola.png"", fig)\nfig\n```\n\n<img src=""./assets/parabola.png"" width=""600"">\n\n### A more complex plot with unicode characters and LaTeX strings:\n[Similar to the one on this link](<https://github.com/gcalderone/Gnuplot.jl#a-slightly-more-complex-plot-with-unicode-on-x-tics>)\n\n<details>\n  <summary>Show Code</summary>\n\n```julia\nx = -2pi:0.1:2pi\napprox = fill(0.0, length(x))\ncmap = [:gold, :deepskyblue3, :orangered, ""#e82051""]\nwith_theme(palette = (; patchcolor = cgrad(cmap, alpha=0.45))) do\n    fig, axis, lineplot = lines(x, sin.(x); label = L""sin(x)"", linewidth = 3, color = :black,\n        axis = (; title = ""Polynomial approximation of sin(x)"",\n            xgridstyle = :dash, ygridstyle = :dash,\n            xticksize = 10, yticksize = 10, xtickalign = 1, ytickalign = 1,\n            xticks = (-π:π/2:π, [""π"", ""-π/2"", ""0"", ""π/2"", ""π""])\n        ))\n    translate!(lineplot, 0, 0, 2) # move line to foreground\n    band!(x, sin.(x), approx .+= x; label = L""n = 0"")\n    band!(x, sin.(x), approx .+= -x .^ 3 / 6; label = L""n = 1"")\n    band!(x, sin.(x), approx .+= x .^ 5 / 120; label = L""n = 2"")\n    band!(x, sin.(x), approx .+= -x .^ 7 / 5040; label = L""n = 3"")\n    limits!(-3.8, 3.8, -1.5, 1.5)\n    axislegend(; position = :ct, backgroundcolor = (:white, 0.75), framecolor = :orange)\n    save(""./assets/approxsin.png"", fig, size = (800, 600))\n    fig\nend\n```\n</details>\n\n<img src=""./assets/approxsin.png"" width=""600"">\n\n### Simple layout: Heatmap, contour and 3D surface plot\n\n<details>\n  <summary>Show Code</summary>\n\n```julia\nx = y = -5:0.5:5\nz = x .^ 2 .+ y' .^ 2\ncmap = :plasma\nwith_theme(colormap = cmap) do\n    fig = Figure(fontsize = 22)\n    ax3d = Axis3(fig[1, 1]; aspect = (1, 1, 1),\n        perspectiveness = 0.5, azimuth = 2.19, elevation = 0.57)\n    ax2d = Axis(fig[1, 2]; aspect = 1, xlabel = ""x"", ylabel=""y"")\n    pltobj = surface!(ax3d, x, y, z; transparency = true)\n    heatmap!(ax2d, x, y, z; colormap = (cmap, 0.65))\n    contour!(ax2d, x, y, z; linewidth = 2, levels = 12, color = :black)\n    contour3d!(ax3d, x, y, z; linewidth = 4, levels = 12,\n        transparency = true)\n    Colorbar(fig[1, 3], pltobj; label=""z"", labelrotation=pi)\n    colsize!(fig.layout, 1, Aspect(1, 1.0))\n    colsize!(fig.layout, 2, Aspect(1, 1.0))\n    resize_to_layout!(fig)\n    save(""./assets/simpleLayout.png"", fig)\n    fig\nend\n```\n</details>\n\n<img src=""./assets/simpleLayout.png"" width=""600"">\n\nInteractive example by [AlexisRenchon](https://github.com/AlexisRenchon):\n\n![out](https://user-images.githubusercontent.com/1010467/81500379-2e8cfa80-92d2-11ea-884a-7069d401e5d0.gif)\n\nExample from [InteractiveChaos.jl](https://github.com/JuliaDynamics/InteractiveChaos.jl)\n\n[![interactive chaos](https://user-images.githubusercontent.com/1010467/81500069-ea005f80-92cf-11ea-81db-2b7bcbfea297.gif)\n](https://github.com/JuliaDynamics/InteractiveChaos.jl)\n\n\n## Sponsors\n\n<img src=""https://github.com/MakieOrg/Makie.jl/blob/master/assets/BMBF_gefoerdert_2017_en.jpg?raw=true"" width=""300""/>\nFörderkennzeichen: 01IS10S27, 2020\n",2342,graphics,Julia,6,Julia,GLSL,JavaScript,HTML,TeX,CSS,,,,,,,,,,,,,,,,,,,,,,,1555,334,1132,89,191,216,4108,552432,291,2431,1494,937,77f2cfdb15b85374f449c5cdc3ad3c108cf858e2,resolve CairoMakie insert! ambiguity (#4038),2024-07-16T15:52:30Z,Alexander Plavin,alexander@plav.in,aplavin,v0.21.5,"## Makie v0.21.5\n\n[Diff since v0.21.4](https://github.com/MakieOrg/Makie.jl/compare/v0.21.4...v0.21.5)\n\n\n**Merged pull requests:**\n- validate plot attributes later, for axis specific plot attributes (#3974) (@SimonDanisch)\n- correctly implement `resize_to` for tuples (#4009) (@SimonDanisch)\n- tag v0.21.5 (#4010) (@SimonDanisch)\n\n**Closed issues:**\n- Installation of GLMakie on Ubuntu 24.04 fails (#4002)",v0.21.5,,,github-actions[bot],MIT License,Makie.jl,MakieOrg,97,julia,plotting,visualization,julia-language,gpu,graphics,,,,,,,,,,,,,,,/MakieOrg/Makie.jl,115,27,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/madsjulia/Mads.jl,https://github.com/madsjulia/Mads.jl,0,,,0,0,1,1,0,0,0,0,0,0,0,MADS: Model Analysis & Decision Support,"# MADS (Model Analysis & Decision Support)\n\n![logo](logos/mads_black_swan_logo_big_text_new_3inch.png)\n\n[action-img]: https://github.com/madsjulia/Mads.jl/workflows/CI/badge.svg\n[action-url]: https://github.com/madsjulia/Mads.jl/actions\n\n[![coveralls.io](https://coveralls.io/repos/madsjulia/Mads.jl/badge.svg?branch=master)](https://coveralls.io/r/madsjulia/Mads.jl?branch=master)\n<!-- [![codecov.io](http://codecov.io/github/madsjulia/Mads.jl/coverage.svg?branch=master)](http://codecov.io/github/madsjulia/Mads.jl?branch=master) -->\n\n[MADS](http://madsjulia.github.io/Mads.jl) is an integrated high-performance computational framework for data/model/decision analyses.\n\n[MADS](http://madsjulia.github.io/Mads.jl) can be applied to perform:\n\n* Sensitivity Analysis\n* Parameter Estimation\n* Model Inversion and Calibration\n* Uncertainty Quantification\n* Model Selection and Averaging\n* Model Reduction and Surrogate Modeling\n* Risk Assessment\n* Decision Analysis and Support\n\n[MADS](http://madsjulia.github.io/Mads.jl) utilizes adaptive rules and techniques that allow the analyses to be performed efficiently with minimum user input.\n\n[MADS](http://madsjulia.github.io/Mads.jl) provides a series of alternative algorithms to execute various types of data-based and model-based analyses.\n\n[MADS](http://madsjulia.github.io/Mads.jl) can efficiently utilize available computational resources.\n\n[MADS](http://madsjulia.github.io/Mads.jl) has been extensively tested and verified.\n\n## Documentation\n\n[MADS](http://madsjulia.github.io/Mads.jl) documentation, including descriptions of all modules, functions, and variables, is available at:\n- [GitHub](http://madsjulia.github.io/Mads.jl) (always up-to-date)\n- [ReadtheDocs](https://mads.readthedocs.io) (outdated)\n- [LANL](https://madsjulia.lanl.gov) (outdated).\n\n[MADS](http://madsjulia.github.io/Mads.jl) information is also available at [mads.gitlab.io](http://mads.gitlab.io) and [madsjulia.github.io](http://madsjulia.github.io/Mads.jl)\n\nDetailed demontrative data analysis and model diagnostics problems are available as [Julia scripts](https://github.com/madsjulia/Mads.jl/tree/master/examples) and [Jupyter notebooks](https://github.com/madsjulia/Mads.jl/tree/master/notebooks/model_diagnostics). See also below.\n\n## Installation\n\nIn [Julia](https://julialang.org/downloads) REPL, execute:\n\n```julia\nimport Pkg; Pkg.add(""Mads"")\n```\n\nTo utilize the latest code updates, use:\n\n```julia\nimport Pkg; Pkg.add(Pkg.PackageSpec(name=""Mads"", rev=""master""))\n```\n\n## Testing\n\nExecute:\n\n```julia\nimport Mads; Mads.test()\n```\n\nor\n\n```julia\nimport Pkg; Pkg.test(""Mads"")\n```\n\n## Getting started\n\nTo explore getting-started instructions, execute:\n\n```julia\nimport Mads; Mads.help()\n```\n\n## Examples\n\nVarious examples are located in the `examples` directory of the `Mads` repository.\n\nA list of all the examples is provided by:\n\n```julia\nMads.examples()\n```\n\nA specific can be executed using:\n\n```julia\nMads.examples(""contamination"")\n```\n\nor\n\n```julia\ninclude(joinpath(Mads.dir, ""examples"", ""contamination"", ""contamination.jl""))\n```\n\nThis example will demonstrate various  analyses related to groundwater contaminant transport.\n\nTo perform Bayesian Information Gap Decision Theory (BIG-DT) analysis, execute:\n\n```julia\nMads.examples(""bigdt"")\n```\n\nor\n\n```julia\ninclude(joinpath(Mads.dir, ""examples"", ""bigdt"", ""bigdt.jl""))\n```\n\n## Notebooks\n\nTo explore available notebooks, please execute:\n\n```julia\nMads.notebooks()\n```\n\n## Docker\n\n```bash\ndocker run --interactive --tty montyvesselinov/madsjulia\n```\n\n## Related Julia Packages\n\n* [SmartTensors:\nUnsupervised and Physics-Informed Machine Learning based on Matrix/Tensor Factorization](https://github.com/SmartTensors)\n* [RegAE: Regularization with a variational autoencoder for inverse analysis](https://github.com/madsjulia/RegAE.jl)\n* [Geostatistical Inversion with randomized + sketching optimization](https://github.com/madsjulia/GeostatInversion.jl)\n\n## Publications, Presentations, Projects\n\n* [mads @ GitLab](http://mads.gitlab.io)\n* [mads @ GitHub](http://madsjulia.github.io)\n* [mads @ LANL](http://mads.lanl.gov)\n* [SmartTensors.com](https://smarttensors.com)\n* [SmartTensors @ GitHub](https://smarttensors.github.io)\n* [SmartTensors @ LANL](https://smarttensors.lanl.gov)\n* [monty @ GitLab](http://monty.gitlab.io)\n* [monty @ GitHub](http://montyvesselinov.github.io)\n* [monty @ LANL](https://www.lanl.gov/orgs/ees/staff/monty)\n",101,uncertainty-quantification,HTML,7,Julia,Smarty,Dockerfile,Shell,HTML,Jupyter Notebook,TeX,,,,,,,,,,,,,,,,,,,,,,44,39,5,0,9,15,194,191705,20,21,21,0,c69a25e01d66d4e6a0dc99ed240d4a98a249e2ce,Update README.md,2024-07-18T13:48:05Z,Velimir V Vesselinov (monty),velimir.vesselinov@gmail.com,montyvesselinov,v1.5.25,## Mads v1.5.25\n\n[Diff since v1.5.24](https://github.com/madsjulia/Mads.jl/compare/v1.5.24...v1.5.25),v1.5.25,,,github-actions[bot],GNU General Public License v3.0,Mads.jl,madsjulia,129,decision-support,model-analysis,calibration,sensitivity-analysis,uncertainty-quantification,decision-making,model-reduction,machine-learning,blind-source-separation,inversion,parameter-estimation,high-performance-computing,matrix-factorization,data-analytics,,,,,,,/madsjulia/Mads.jl,208,18,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/Macaulay2/M2,https://github.com/Macaulay2/M2,0.5,"Separate language that contains scientific algoriths, not sure if it works for us",1,1,1,1,0,0,0,0,0,0,0,0,"The primary source code repository for Macaulay2, a system for computing in commutative algebra, algebraic geometry and related fields.","Macaulay2\n=========\n\nMacaulay2 is a system for computing in commutative algebra, algebraic geometry\nand related fields.  The system was originally written by Dan Grayson and Mike\nStillman.  David Eisenbud joined the project a number of years ago, and many\nusers are writing packages for the system, and some are contributing source\ncode.  See our web page <https://macaulay2.com/> for more details and for\ndownloading binary releases.\n\nThe source code is available at <https://github.com/Macaulay2/M2>.\n\nOne uses the program `git` to access it.  For brief instructions about how to\nuse git, see\n<https://github.com/Macaulay2/M2/wiki/Git-for-Macaulay2-Contributors>.\n\nReport bugs via the github issue tracker at <https://github.com/Macaulay2/M2/>.\n\nThe directory containing this file is the top level directory of the directory\ntree that contains the following subdirectories:\n\n* `M2`\n\n    The subdirectory `M2` contains everything needed by a user to\n    build Macaulay2, and the file `M2/INSTALL` gives instructions for\n    doing so.\n\n* `bugs`\n\n    The subdirectory `bugs` is where we keep track of older bug\n    reports.\n\n-----------------------------------------------------------------------------\n\nContributions to the code of Macaulay2 are welcome.\n\nTo make a contribution, submit a ""pull request"" on github.  If the\ncontribution involves changing an existing package in a non-trivial way, we\nwill normally contact the authors to get their approval of the change.  If a\nnew package with mathematical content is submitted, it will normally be\naccepted if it can be installed with `installPackage` and the tests pass as\ndetermined by `check`, in the latest version of Macaulay2.\n\nTo start working on an existing github ""issue"", volunteer to work on it, so\nyou can get ""assigned"" to the issue, thereby preventing duplication of\neffort.\n\nTo report a bug, submit an ""issue"" on github.\n",333,mathematics,Macaulay2,24,Makefile,Shell,C,TeX,C++,CSS,Emacs Lisp,Awk,JavaScript,Scilab,HTML,Perl,Yacc,Lex,M4,Python,Roff,CMake,AppleScript,Rich Text Format,Julia,Dockerfile,Macaulay2,Vim Script,,,,,1628,143,1472,13,18,175,0,92790,228,1713,982,731,ec9e9ac60ed4a8e791448942202f077a88a87e15,Release Macaulay2 1.24.05,2024-05-14T23:18:01Z,Doug Torrance,dtorrance@piedmont.edu,d-torrance,Macaulay2 v1.24.05,"### New GitHub Contributors and [Package Authors](https://macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_authors_spof_sp__Macaulay2_sppackages.html)\r\n\r\n* Antonino Ficarra\r\n* Maya Banks\r\n* Alessio Borzì\r\n* Eduardo Torres Davila\r\n* Antonino Ficarra\r\n* Tara Gomes\r\n* Emanuele Sgroi (@EmanueleSgroi) made their first contribution in https://github.com/Macaulay2/M2/pull/3154\r\n* Prashanth Sridhar\r\n\r\n### Changelog\r\n\r\n- packages that have been published and certified:\r\n  - ![""a gold star""](https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/Macaulay2/Style/GoldStar.png) <a title=""A package for cotangent Schubert calculus"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/CotangentSchubert/html/index.html"">CotangentSchubert</a>, a package by Paul Zinn-Justin for Cotangent Schubert calculus, has been published.\r\n  - ![""a gold star""](https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/Macaulay2/Style/GoldStar.png) <a title=""a package to check whether ideals are geometrically vertex decomposable"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/GeometricDecomposability/html/index.html"">GeometricDecomposability</a>, a package by Mike Cummings and Adam Van Tuyl to check whether ideals are geometrically vertex decomposable, has been published.\r\n  - ![""a gold star""](https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/Macaulay2/Style/GoldStar.png) <a title=""invariants of group actions"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/InvariantRing/html/index.html"">InvariantRing</a>, a package by Luigi Ferraro, Federico Galetto, Francesca Gandini, Hang Huang, Thomas Hawes, Matthew Mastroeni, and Xianglong Ni for invariants of group actions, has been published.\r\n  - ![""a gold star""](https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/Macaulay2/Style/GoldStar.png) <a title=""multiplicity sequence of ideals"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/MultiplicitySequence/html/index.html"">MultiplicitySequence</a>, a package by Justin Chen, Youngsu Kim, and Jonathan Montaño for computing the multiplicity sequence of an ideal, has been published.\r\n  - ![""a gold star""](https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/Macaulay2/Style/GoldStar.png) <a title=""basic probability functions"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Probability/html/index.html"">Probability</a>, a package by Doug Torrance for basic probability functions, has been published.\r\n  - ![""a gold star""](https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/Macaulay2/Style/GoldStar.png) <a title=""Macaulay2 package for toric intersection theory using tropical geometry."" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/TropicalToric/html/index.html"">TropicalToric</a>, a package by Alessio Borzì on tropical methods for toric intersection theory, has been published.\r\n- new packages:\r\n  - <a title=""Package for working with Multigraded BGG and Differential Modules"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/MultigradedBGG/html/index.html"">MultigradedBGG</a>, a package by Maya Banks, Michael K. Brown, Tara Gomes, Prashanth Sridhar, Eduardo Torres Davila, and Sasha	Zotine for the multigraded BGG correspondence and differential modules, has been added.\r\n  - <a title=""Macaulay2 package for toric intersection theory using tropical geometry."" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/TropicalToric/html/index.html"">TropicalToric</a>, a package by Alessio Borzì on tropical methods for toric intersection theory, has been added.\r\n  - <a title=""a package for computing the v-number and v-function"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/VNumber/html/index.html"">VNumber</a>, a package by Antonino Ficarra and Emanuele Sgroi to compute v-number of homogeneous ideals and v-function of monomial ideals, has been added.\r\n- improved packages:\r\n  - Many <a title=""Interface for 4ti2"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/FourTiTwo/html/index.html"">FourTiTwo</a> methods now have a <a title=""name of an optional argument."" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/___Precision.html"">Precision</a> option for setting the integer precision used by 4ti2.\r\n  - <a title=""a package to check whether ideals are geometrically vertex decomposable"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/GeometricDecomposability/html/index.html"">GeometricDecomposability</a> has been updated to version 1.4.1 with minor updates.\r\n  - <a title=""Numerical Algebraic Geometry"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/NumericalAlgebraicGeometry/html/index.html"">NumericalAlgebraicGeometry</a> has been updated to version 1.24 with small updates.\r\n  - <a title=""checks positivity of toric vector bundles"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/PositivityToricBundles/html/index.html"">PositivityToricBundles</a> has been updated to version 1.7, adding several new methods, fixing bugs, and updating the documentation and tests. \r\n  - <a title=""cohomology computations of equivariant vector bundles on toric varieties"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/ToricVectorBundles/html/index.html"">ToricVectorBundles</a> has been updated to version 1.2 with updated documentation.\r\n  - <a title=""Computes Whitney Statifications of real and complex varieties and of algebraic maps between them."" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/WhitneyStratifications/html/index.html"">WhitneyStratifications</a> has been updated to version 2.03, adding new routines to stratify algebraic maps to the package. The update also includes several performance improvements and bug fixes.\r\n- functionality added or improved:\r\n  - It is now possible to create a fraction field of an iterated polymial ring using <a title=""construct a fraction field"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_frac.html"">frac</a>.\r\n  - A number of new operators have been added that may be used for defining methods.  See <a title=""a unary postfix operator, used for the upper shriek functor"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_%5E%21.html"">^!</a>, <a title=""a unary postfix operator, used for the lower shriek functor"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/__us%21.html"">_!</a>, <a title=""a unary postfix operator, used for sheafification"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_%5E~.html"">^~</a>, <a title=""a unary postfix operator"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/__us~.html"">_~</a>, <a title=""a binary operator, used for truncation"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_%5E_gt.html"">^></a>, <a title=""a binary operator, used for truncation"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_%5E_gt_eq.html"">^>=</a>, <a title=""a binary operator, used for truncation"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_%5E_lt.html"">^&lt;</a>, <a title=""a binary operator, used for truncation"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_%5E_lt_eq.html"">^&lt;=</a>, <a title=""a binary operator, used for truncation"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/__us_gt.html"">_></a>, <a title=""a binary operator, used for truncation"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/__us_gt_eq.html"">_>=</a>, <a title=""a binary operator, used for truncation"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/__us_lt.html"">_&lt;</a>, <a title=""a binary operator, used for truncation"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/__us_lt_eq.html"">_&lt;=</a>, <a title=""a binary operator, used for restriction to a subset"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/__vb_us.html"">|_</a>, and <a title=""augmented assignment for |_"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/__vb_us_eq.html"">|_=</a>.\r\n  - A number of improvements have been made to parallelization. In particular:\r\n    - The function <a title=""get I/O thread mode"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_get__I__O__Thread__Mode.html"">getIOThreadMode</a> has been added for determining the current I/O thread mode. \r\n    - The functions <a title=""exclusive I/O for the current thread"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_set__I__O__Exclusive.html"">setIOExclusive</a>, <a title=""synchronized I/O for threads"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_set__I__O__Synchronized.html"">setIOSynchronized</a>, and <a title=""unsynchronized I/O for threads"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_set__I__O__Un__Synchronized.html"">setIOUnSynchronized</a> now also accept a file as an argument.\r\n    - The default I/O thread mode has been changed from 0 (unsynchronized) to 1 (synchronized).\r\n    - Much of the code (e.g., reading mutable hash tables) is now thread safe.\r\n    - The output of <a title=""information about the status of the garbage collector"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/___G__Cstats.html"">GCstats</a> has been improved.\r\n    - The initial heap size used by the garbage collector has been increased and the free space divisor has been decreased.\r\n    - The <a title=""retrieve the value returned by a task"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_task__Result_lp__Task_rp.html"">taskResult</a> function now waits until a task is finished before returning its result.\r\n    - The <a title=""apply a function to each element in parallel"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_parallel__Apply.html"">parallelApply</a> function has been added for applying a function to a list in parallel.\r\n    - The output when using the <a title=""time a computation"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_time.html"">time</a> keyword now includes the time used by the current thread and in garbage collection. \r\n    - The keyword `threadVariable` has been renamed to <a title=""create a symbol whose value in one thread is not shared with others"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_thread__Local.html"">threadLocal</a>, although the former still exists as a synonym.\r\nSee <a href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_parallel_spprogramming_spwith_spthreads_spand_sptasks.html"">parallel programming with threads and tasks</a> for more.\r\n  - The hash counter for mutable hash tables increases much more slowly, decreasing the likelihood of overflowing.\r\n  - The <a title=""Take some elements from a list or sequence."" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_take.html"">take</a> function will now accept a two-element list as its second element if the class of the first element has a <a title=""get an iterator"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_iterator.html"">iterator</a> method installed.\r\n  - Broken links to the [Journal of Software for Algebra and Geometry](https://msp.org/jsag/) have been fixed in the documentation for older certified packages.\r\n  - The documentation page for each certified package now includes its DOI.\r\n  - The <a title=""computes the union of two posets"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Posets/html/_union.html"">union</a> function has been added for sets.\r\n  - The <a title=""compute an intersection"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_intersect.html"">intersect</a> and <a title=""compute an intersection"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_intersection.html"">intersection</a> functions now work for sets.\r\n  - Creating integer quotient rings using `ZZ/n` now works when n is large and/or composite.\r\n  - The syntax `g \\ f` as a synonym for `f // g` when f and g are matrices has been deprecated.  It will be replaced in the next release.\r\n  - Modules may now be compared using <a title=""less than"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/__lt.html"">&lt;</a>, <a title=""less than or equal"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/__lt_eq.html"">&lt;=</a>, <a title=""greater than"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/__gt.html"">></a>, and <a title=""greater than or equal"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/__gt_eq.html"">>=</a>, by their ranks and degrees.  In particular, lists of modules may now be sorted.\r\n  - The method <a title=""get the homomorphism from element of Hom"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_homomorphism.html"">homomorphism(Vector)</a> was added for getting the homomorphism from an element of a Hom module.\r\n  - The method <a title=""get a random vector in the module"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_random_lp__Module_rp.html"">random(Module)</a> was added for generating random elements of modules.\r\n  - The method <a title=""whether all elements of a list satisfy a specified condition"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_all_lp__Basic__List_cm__Function_rp.html"">all(BasicList)</a> was added for checking if all elements of a list of booleans are true.\r\n  - The method <a title=""the first element of a list satisfying a condition"" href=""https://www.macaulay2.com/doc/Macaulay2-1.24.05/share/doc/Macaulay2/Macaulay2Doc/html/_position.html"">position(ZZ,Function)</a> was added for finding the smallest index for which a function returns true.",release-1.24.05,Mahrud Sayrafi,,mahrud,,M2,Macaulay2,9,algebraic-geometry,commutative-algebra,symbolic-computation,mathematics,groebner-basis,homological-algebra,projective-geometry,resolutions,modules,rings,polynomials,polynomial-systems,polynomial-arithmetic,interpreted-programming-language,,,,,,,/Macaulay2/M2,83,29,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/lvgl/lvgl,https://github.com/lvgl/lvgl,0,,,0,0,0,0,0,0,1,1,0,0,0,"Embedded graphics library to create beautiful UIs for any MCU, MPU and display type. ","\n<a href=""https://github.com/sponsors/lvgl"" target=""_blank""><img align=""left"" src=""https://lvgl.io/github-assets/sponsor.png"" height=""32px""></a>\n\n<p align=""right"">\n  <b>English</b> | <a href=""./docs/README_zh.rst"">中文</a> | <a href=""./docs/README_pt_BR.rst"">Português do Brasil</a> | <a href=""./docs/README_jp.rst"">日本語</a>\n</p>\n\n<br>\n\n<p align=""center"">\n  <img src=""https://lvgl.io/github-assets/logo-colored.png"" width=300px>\n</p>\n\n  <h1 align=""center"">Light and Versatile Graphics Library</h1>\n  <br>\n<div align=""center"">\n  <img src=""https://lvgl.io/github-assets/smartwatch-demo.gif"">\n  &nbsp;\n  <img border=""1px"" src=""https://lvgl.io/github-assets/widgets-demo.gif"">\n</div>\n<br>\n<p align=""center"">\n<a href=""https://lvgl.io"" title=""Homepage of LVGL"">Website </a> |\n<a href=""https://docs.lvgl.io/"" title=""Detailed documentation with 100+ examples"">Docs</a> |\n<a href=""https://forum.lvgl.io"" title=""Get help and help others"">Forum</a> |\n<a href=""https://lvgl.io/demos"" title=""Demos running in your browser"">Demos</a> |\n<a href=""https://lvgl.io/services"" title=""Graphics design, UI implementation and consulting"">Services</a>\n</p>\n<br>\n\n## :ledger: Overview\n\n**Mature and Well-known**<br>\nLVGL is the most popular free and open source embedded graphics library to create beautiful UIs for any MCU, MPU and display type. It's supported by industry leading vendors and projects like  Arm, STM32, NXP, Espressif, Nuvoton, Arduino, RT-Thread, Zephyr, NuttX, Adafruit and many more.\n\n**Feature Rich**<br>\nIt has all the features to create modern and beautiful GUIs: 30+ built-in widgets, a powerful style system, web inspired layout managers, and a typography system supporting many languages. To integrate LVGL into your platform, all you need is at least 32kB RAM and 128 kB Flash, a C compiler, a frame buffer, and at least an 1/10 screen sized buffer for rendering.\n\n**Services**<br>\nOur team is ready to help you with graphics design, UI implementation and consulting services. Contact us if you need some support during the development of your next GUI project.\n\n## :rocket: Features\n\n**Free and Portable**\n  - A fully portable C (C++ compatible) library with no external dependencies.\n  - Can be compiled to any MCU or MPU, with any (RT)OS.\n  - Supports monochrome, ePaper, OLED or TFT displays, or even monitors. [Porting Guide](https://docs.lvgl.io/master/porting/project.html)\n  - Distributed under the MIT license, so you can easily use it in commercial projects too.\n  - Needs only 32kB RAM and 128 kB Flash, a frame buffer, and at least an 1/10 screen sized buffer for rendering.\n  - OS, External memory and GPU are supported but not required.\n\n**Widgets, Styles, Layouts and more**\n  - 30+ built-in [Widgets](https://docs.lvgl.io/master/widgets/index.html):  Button, Label, Slider, Chart, Keyboard, Meter, Arc, Table and many more.\n  - Flexible [Style system](https://docs.lvgl.io/master/overview/style.html) with  ~100 style properties to customize any part of the widgets in any state.\n  - [Flexbox](https://docs.lvgl.io/master/layouts/flex.html) and [Grid](https://docs.lvgl.io/master/layouts/grid.html)-like layouts engines to automatically size and position the widgets in a responsive way.\n  - Texts are rendered with UTF-8 encoding supporting CJK, Thai, Hindi, Arabic, Persian writing systems.\n  - Word wrapping, kerning, text scrolling, sub-pixel rendering, Pinyin-IME Chinese input, Emojis in texts.\n  - Rendering engine supporting animations, anti-aliasing, opacity, smooth scrolling, shadows, image transformation, etc  \n  - Supports Mouse, Touchpad, Keypad, Keyboard, External buttons, Encoder [Input devices](https://docs.lvgl.io/master/porting/indev.html).\n  - [Multiple display](https://docs.lvgl.io/master/overview/display.html#multiple-display-support) support.\n\n**Binding and Build Support**\n  - [MicroPython Binding](https://blog.lvgl.io/2019-02-20/micropython-bindings) exposes LVGL API\n  - [PikaScript Binding](https://blog.lvgl.io/2022-08-24/pikascript-and-lvgl) python on MCU lighter and easier.\n  - No custom build system is used. You can build LVGL as you build the other files of your project.\n  - Support for Make and [CMake](https://docs.lvgl.io/master/integration/building/cmake.html) is included out of the box.\n  - [Develop on PC](https://docs.lvgl.io/master/integration/ide/pc-simulator.html) and use the same UI code on embedded hardware.\n  - Convert the C UI code to HTML file with our [Emscripten port](https://github.com/lvgl/lv_web_emscripten).\n\n**Docs, Tools, and Services**\n  - Detailed [Documentation](https://docs.lvgl.io/) with [100+ simple examples](https://docs.lvgl.io/master/index.html)\n  - [Services](https://lvgl.io/services) such as User interface design, Implementation and Consulting to make UI development simpler and faster.\n\n## :heart: Sponsor\n\nIf LVGL saved you a lot of time and money or you just had fun using it, consider [Supporting its Development](https://github.com/sponsors/lvgl).\n\n**How do we spend the donations?**<br>\nOur goal is to provide financial compensation for people who do the most for LVGL. It means not only the maintainers but anyone who implements a great feature should get a payment from the accumulated money. We use the donations to cover our operational costs like servers and related services.\n\n**How to donate?**<br>\nWe use [GitHub Sponsors](https://github.com/sponsors/lvgl) where you can easily send one time or recurring donations. You can also see all of our expenses  in a transparent way.\n\n**How to get paid for your contribution?**<br>\nIf someone implements or fixes an issue labeled as [Sponsored](https://github.com/lvgl/lvgl/labels/Sponsored) he or she will get a payment for that work. We estimate the required time, complexity and importance of the issue and set a price accordingly. To jump in just comment on a [Sponsored](https://github.com/lvgl/lvgl/labels/Sponsored) issue saying ""Hi, I'd like to deal with it. This is how I'm planning to fix/implement it..."". A work is considered ready when it's approved and merged by a maintainer. After that you can submit and expense at [opencollective.com](https://opencollective.com/lvgl) and you will receive the payment in a few days.\n\n**Organizations supporting LVGL**<br>\n[![Sponsors of LVGL](https://opencollective.com/lvgl/organizations.svg?width=600)](https://opencollective.com/lvgl)\n\n**Individuals supporting LVGL**<br>\n[![Backers of LVGL](https://opencollective.com/lvgl/individuals.svg?width=600)](https://opencollective.com/lvgl)\n\n## :package: Packages\nLVGL is available as:\n- [Arduino library](https://docs.lvgl.io/master/integration/framework/arduino.html)\n- [PlatformIO package](https://registry.platformio.org/libraries/lvgl/lvgl)\n- [Zephyr library](https://docs.lvgl.io/master/integration/os/zephyr.html)\n- [ESP-IDF(ESP32) component](https://components.espressif.com/components/lvgl/lvgl)\n- [NXP MCUXpresso component](https://www.nxp.com/design/software/embedded-software/lvgl-open-source-graphics-library:LITTLEVGL-OPEN-SOURCE-GRAPHICS-LIBRARY)\n- [NuttX library](https://docs.lvgl.io/master/integration/os/nuttx.html)\n- [RT-Thread RTOS](https://docs.lvgl.io/master/integration/os/rt-thread.html)\n- CMSIS-Pack\n- [RIOT OS package](https://doc.riot-os.org/group__pkg__lvgl.html#details)\n\n\n## :robot: Examples\n\nSee some examples of creating widgets, using layouts and applying styles. You will find C and MicroPython code, and links to try out or edit the examples in an online MicroPython editor.\n\nFor more examples check out the [Examples](https://github.com/lvgl/lvgl/tree/master/examples) folder.\n\n\n### Hello world label\n\n![Simple Hello world label example in LVGL](https://github.com/kisvegabor/test/raw/master/readme_example_1.png)\n\n<details>\n  <summary>C code</summary>\n\n```c\n/*Change the active screen's background color*/\nlv_obj_set_style_bg_color(lv_screen_active(), lv_color_hex(0x003a57), LV_PART_MAIN);\n\n/*Create a white label, set its text and align it to the center*/\nlv_obj_t * label = lv_label_create(lv_screen_active());\nlv_label_set_text(label, ""Hello world"");\nlv_obj_set_style_text_color(label, lv_color_hex(0xffffff), LV_PART_MAIN);\nlv_obj_align(label, LV_ALIGN_CENTER, 0, 0);\n```\n</details>\n\n<details>\n  <summary>MicroPython code | <a href=""https://sim.lvgl.io/v8.3/micropython/ports/javascript/index.html?script_direct=4ab7c40c35b0dc349aa2f0c3b00938d7d8e8ac9f"" target=""_blank"">Online Simulator</a></summary>\n\n```python\n# Change the active screen's background color\nscr = lv.screen_active()\nscr.set_style_bg_color(lv.color_hex(0x003a57), lv.PART.MAIN)\n\n# Create a white label, set its text and align it to the center\nlabel = lv.label(lv.screen_active())\nlabel.set_text(""Hello world"")\nlabel.set_style_text_color(lv.color_hex(0xffffff), lv.PART.MAIN)\nlabel.align(lv.ALIGN.CENTER, 0, 0)\n```\n</details>\n<br>\n\n### Button with Click Event\n\n![LVGL button with label example](https://github.com/kisvegabor/test/raw/master/readme_example_2.gif)\n\n<details>\n  <summary>C code</summary>\n\n```c\nlv_obj_t * button = lv_button_create(lv_screen_active());                   /*Add a button to the current screen*/\nlv_obj_center(button);                                             /*Set its position*/\nlv_obj_set_size(button, 100, 50);                                  /*Set its size*/\nlv_obj_add_event_cb(button, button_event_cb, LV_EVENT_CLICKED, NULL); /*Assign a callback to the button*/\n\nlv_obj_t * label = lv_label_create(button);                        /*Add a label to the button*/\nlv_label_set_text(label, ""Button"");                             /*Set the labels text*/\nlv_obj_center(label);                                           /*Align the label to the center*/\n...\n\nvoid button_event_cb(lv_event_t * e)\n{\n  printf(""Clicked\n"");\n}\n```\n</details>\n\n<details>\n  <summary>MicroPython code | <a href=""https://sim.lvgl.io/v8.3/micropython/ports/javascript/index.html?script_startup=https://raw.githubusercontent.com/lvgl/lvgl/0d9ab4ee0e591aad1970e3c9164fd7c544ecce70/examples/header.py&script=https://raw.githubusercontent.com/lvgl/lvgl/0d9ab4ee0e591aad1970e3c9164fd7c544ecce70/examples/widgets/slider/lv_example_slider_2.py&script_direct=926bde43ec7af0146c486de470c53f11f167491e"" target=""_blank"">Online Simulator</a></summary>\n\n```python\ndef button_event_cb(e):\n  print(""Clicked"")\n\n# Create a Button and a Label\nbutton = lv.button(lv.screen_active())\nbutton.center()\nbutton.set_size(100, 50)\nbutton.add_event_cb(button_event_cb, lv.EVENT.CLICKED, None)\n\nlabel = lv.label(button)\nlabel.set_text(""Button"")\nlabel.center()\n```\n</details>\n<br>\n\n### Checkboxes with Layout\n![Checkboxes with layout in LVGL](https://github.com/kisvegabor/test/raw/master/readme_example_3.gif)\n\n<details>\n  <summary>C code</summary>\n\n```c\n\nlv_obj_set_flex_flow(lv_screen_active(), LV_FLEX_FLOW_COLUMN);\nlv_obj_set_flex_align(lv_screen_active(), LV_FLEX_ALIGN_CENTER, LV_FLEX_ALIGN_START, LV_FLEX_ALIGN_CENTER);\n\nlv_obj_t * cb;\ncb = lv_checkbox_create(lv_screen_active());\nlv_checkbox_set_text(cb, ""Apple"");\nlv_obj_add_event_cb(cb, event_handler, LV_EVENT_ALL, NULL);\n\ncb = lv_checkbox_create(lv_screen_active());\nlv_checkbox_set_text(cb, ""Banana"");\nlv_obj_add_state(cb, LV_STATE_CHECKED);\nlv_obj_add_event_cb(cb, event_handler, LV_EVENT_ALL, NULL);\n\ncb = lv_checkbox_create(lv_screen_active());\nlv_checkbox_set_text(cb, ""Lemon"");\nlv_obj_add_state(cb, LV_STATE_DISABLED);\nlv_obj_add_event_cb(cb, event_handler, LV_EVENT_ALL, NULL);\n\ncb = lv_checkbox_create(lv_screen_active());\nlv_obj_add_state(cb, LV_STATE_CHECKED | LV_STATE_DISABLED);\nlv_checkbox_set_text(cb, ""Melon\nand a new line"");\nlv_obj_add_event_cb(cb, event_handler, LV_EVENT_ALL, NULL);\n```\n\n</details>\n\n<details>\n  <summary>MicroPython code | <a href=""https://sim.lvgl.io/v8.3/micropython/ports/javascript/index.html?script_startup=https://raw.githubusercontent.com/lvgl/lvgl/0d9ab4ee0e591aad1970e3c9164fd7c544ecce70/examples/header.py&script=https://raw.githubusercontent.com/lvgl/lvgl/0d9ab4ee0e591aad1970e3c9164fd7c544ecce70/examples/widgets/slider/lv_example_slider_2.py&script_direct=311d37e5f70daf1cb0d2cad24c7f72751b5f1792"" target=""_blank"">Online Simulator</a></summary>\n\n```python\ndef event_handler(e):\n    code = e.get_code()\n    obj = e.get_target_obj()\n    if code == lv.EVENT.VALUE_CHANGED:\n        txt = obj.get_text()\n        if obj.get_state() & lv.STATE.CHECKED:\n            state = ""Checked""\n        else:\n            state = ""Unchecked""\n        print(txt + "":"" + state)\n\n\nlv.screen_active().set_flex_flow(lv.FLEX_FLOW.COLUMN)\nlv.screen_active().set_flex_align(lv.FLEX_ALIGN.CENTER, lv.FLEX_ALIGN.START, lv.FLEX_ALIGN.CENTER)\n\ncb = lv.checkbox(lv.screen_active())\ncb.set_text(""Apple"")\ncb.add_event_cb(event_handler, lv.EVENT.ALL, None)\n\ncb = lv.checkbox(lv.screen_active())\ncb.set_text(""Banana"")\ncb.add_state(lv.STATE.CHECKED)\ncb.add_event_cb(event_handler, lv.EVENT.ALL, None)\n\ncb = lv.checkbox(lv.screen_active())\ncb.set_text(""Lemon"")\ncb.add_state(lv.STATE.DISABLED)\ncb.add_event_cb(event_handler, lv.EVENT.ALL, None)\n\ncb = lv.checkbox(lv.screen_active())\ncb.add_state(lv.STATE.CHECKED | lv.STATE.DISABLED)\ncb.set_text(""Melon"")\ncb.add_event_cb(event_handler, lv.EVENT.ALL, None)\n```\n\n</details>\n<br>\n\n### Styling a Slider\n![Styling a slider with LVGL](https://github.com/kisvegabor/test/raw/master/readme_example_4.gif)\n\n\n<details>\n  <summary>C code</summary>\n\n```c\nlv_obj_t * slider = lv_slider_create(lv_screen_active());\nlv_slider_set_value(slider, 70, LV_ANIM_OFF);\nlv_obj_set_size(slider, 300, 20);\nlv_obj_center(slider);\n\n/*Add local styles to MAIN part (background rectangle)*/\nlv_obj_set_style_bg_color(slider, lv_color_hex(0x0F1215), LV_PART_MAIN);\nlv_obj_set_style_bg_opa(slider, 255, LV_PART_MAIN);\nlv_obj_set_style_border_color(slider, lv_color_hex(0x333943), LV_PART_MAIN);\nlv_obj_set_style_border_width(slider, 5, LV_PART_MAIN);\nlv_obj_set_style_pad_all(slider, 5, LV_PART_MAIN);\n\n/*Create a reusable style sheet for the INDICATOR part*/\nstatic lv_style_t style_indicator;\nlv_style_init(&style_indicator);\nlv_style_set_bg_color(&style_indicator, lv_color_hex(0x37B9F5));\nlv_style_set_bg_grad_color(&style_indicator, lv_color_hex(0x1464F0));\nlv_style_set_bg_grad_dir(&style_indicator, LV_GRAD_DIR_HOR);\nlv_style_set_shadow_color(&style_indicator, lv_color_hex(0x37B9F5));\nlv_style_set_shadow_width(&style_indicator, 15);\nlv_style_set_shadow_spread(&style_indicator, 5);\n4\n/*Add the style sheet to the slider's INDICATOR part*/\nlv_obj_add_style(slider, &style_indicator, LV_PART_INDICATOR);\n\n/*Add the same style to the KNOB part too and locally overwrite some properties*/\nlv_obj_add_style(slider, &style_indicator, LV_PART_KNOB);\n\nlv_obj_set_style_outline_color(slider, lv_color_hex(0x0096FF), LV_PART_KNOB);\nlv_obj_set_style_outline_width(slider, 3, LV_PART_KNOB);\nlv_obj_set_style_outline_pad(slider, -5, LV_PART_KNOB);\nlv_obj_set_style_shadow_spread(slider, 2, LV_PART_KNOB);\n```\n\n</details>\n\n<details>\n  <summary>MicroPython code |\n<a href=""https://sim.lvgl.io/v8.3/micropython/ports/javascript/index.html?script_startup=https://raw.githubusercontent.com/lvgl/lvgl/0d9ab4ee0e591aad1970e3c9164fd7c544ecce70/examples/header.py&script=https://raw.githubusercontent.com/lvgl/lvgl/0d9ab4ee0e591aad1970e3c9164fd7c544ecce70/examples/widgets/slider/lv_example_slider_2.py&script_direct=c431c7b4dfd2cc0dd9c392b74365d5af6ea986f0"" target=""_blank"">Online Simulator</a>\n</summary>\n\n\n```python\n# Create a slider and add the style\nslider = lv.slider(lv.screen_active())\nslider.set_value(70, lv.ANIM.OFF)\nslider.set_size(300, 20)\nslider.center()\n\n# Add local styles to MAIN part (background rectangle)\nslider.set_style_bg_color(lv.color_hex(0x0F1215), lv.PART.MAIN)\nslider.set_style_bg_opa(255, lv.PART.MAIN)\nslider.set_style_border_color(lv.color_hex(0x333943), lv.PART.MAIN)\nslider.set_style_border_width(5, lv.PART.MAIN)\nslider.set_style_pad_all(5, lv.PART.MAIN)\n\n# Create a reusable style sheet for the INDICATOR part\nstyle_indicator = lv.style_t()\nstyle_indicator.init()\nstyle_indicator.set_bg_color(lv.color_hex(0x37B9F5))\nstyle_indicator.set_bg_grad_color(lv.color_hex(0x1464F0))\nstyle_indicator.set_bg_grad_dir(lv.GRAD_DIR.HOR)\nstyle_indicator.set_shadow_color(lv.color_hex(0x37B9F5))\nstyle_indicator.set_shadow_width(15)\nstyle_indicator.set_shadow_spread(5)\n\n# Add the style sheet to the slider's INDICATOR part\nslider.add_style(style_indicator, lv.PART.INDICATOR)\nslider.add_style(style_indicator, lv.PART.KNOB)\n\n# Add the same style to the KNOB part too and locally overwrite some properties\nslider.set_style_outline_color(lv.color_hex(0x0096FF), lv.PART.KNOB)\nslider.set_style_outline_width(3, lv.PART.KNOB)\nslider.set_style_outline_pad(-5, lv.PART.KNOB)\nslider.set_style_shadow_spread(2, lv.PART.KNOB)\n```\n</details>\n<br>\n\n### English, Hebrew (mixed LTR-RTL) and Chinese texts\n\n![English, Hebrew and Chinese texts with LVGL](https://github.com/kisvegabor/test/raw/master/readme_example_5.png)\n\n<details>\n  <summary>C code</summary>\n\n```c\nlv_obj_t * ltr_label = lv_label_create(lv_screen_active());\nlv_label_set_text(ltr_label, ""In modern terminology, a microcontroller is similar to a system on a chip (SoC)."");\nlv_obj_set_style_text_font(ltr_label, &lv_font_montserrat_16, 0);\nlv_obj_set_width(ltr_label, 310);\nlv_obj_align(ltr_label, LV_ALIGN_TOP_LEFT, 5, 5);\n\nlv_obj_t * rtl_label = lv_label_create(lv_screen_active());\nlv_label_set_text(rtl_label,""מעבד, או בשמו המלא יחידת עיבוד מרכזית (באנגלית: CPU - Central Processing Unit)."");\nlv_obj_set_style_base_dir(rtl_label, LV_BASE_DIR_RTL, 0);\nlv_obj_set_style_text_font(rtl_label, &lv_font_dejavu_16_persian_hebrew, 0);\nlv_obj_set_width(rtl_label, 310);\nlv_obj_align(rtl_label, LV_ALIGN_LEFT_MID, 5, 0);\n\nlv_obj_t * cz_label = lv_label_create(lv_screen_active());\nlv_label_set_text(cz_label,\n                  ""嵌入式系统（Embedded System），\n是一种嵌入机械或电气系统内部、具有专一功能和实时计算性能的计算机系统。"");\nlv_obj_set_style_text_font(cz_label, &lv_font_simsun_16_cjk, 0);\nlv_obj_set_width(cz_label, 310);\nlv_obj_align(cz_label, LV_ALIGN_BOTTOM_LEFT, 5, -5);\n```\n\n</details>\n\n<details>\n  <summary>MicroPython code | <a href=""https://sim.lvgl.io/v8.3/micropython/ports/javascript/index.html?script_startup=https://raw.githubusercontent.com/lvgl/lvgl/0d9ab4ee0e591aad1970e3c9164fd7c544ecce70/examples/header.py&script=https://raw.githubusercontent.com/lvgl/lvgl/0d9ab4ee0e591aad1970e3c9164fd7c544ecce70/examples/widgets/slider/lv_example_slider_2.py&script_direct=18bb38200a64e10ead1aa17a65c977fc18131842"" target=""_blank"">Online Simulator</a></summary>\n\n```python\nltr_label = lv.label(lv.screen_active())\nltr_label.set_text(""In modern terminology, a microcontroller is similar to a system on a chip (SoC)."")\nltr_label.set_style_text_font(lv.font_montserrat_16, 0);\n\nltr_label.set_width(310)\nltr_label.align(lv.ALIGN.TOP_LEFT, 5, 5)\n\nrtl_label = lv.label(lv.screen_active())\nrtl_label.set_text(""מעבד, או בשמו המלא יחידת עיבוד מרכזית (באנגלית: CPU - Central Processing Unit)."")\nrtl_label.set_style_base_dir(lv.BASE_DIR.RTL, 0)\nrtl_label.set_style_text_font(lv.font_dejavu_16_persian_hebrew, 0)\nrtl_label.set_width(310)\nrtl_label.align(lv.ALIGN.LEFT_MID, 5, 0)\n\nfont_simsun_16_cjk = lv.font_load(""S:../../assets/font/lv_font_simsun_16_cjk.fnt"")\n\ncz_label = lv.label(lv.screen_active())\ncz_label.set_style_text_font(font_simsun_16_cjk, 0)\ncz_label.set_text(""嵌入式系统（Embedded System），\n是一种嵌入机械或电气系统内部、具有专一功能和实时计算性能的计算机系统。"")\ncz_label.set_width(310)\ncz_label.align(lv.ALIGN.BOTTOM_LEFT, 5, -5)\n\n```\n</details>\n\n## :arrow_forward: Get started\nThis list will guide you to get started with LVGL step-by-step.\n\n**Get Familiar with LVGL**\n\n  1. Check the [Online demos](https://lvgl.io/demos) to see LVGL in action (3 minutes)\n  2. Read the [Introduction](https://docs.lvgl.io/master/intro/index.html) page of the documentation (5 minutes)\n  3. Get familiar with the basics on the [Quick overview](https://docs.lvgl.io/master/get-started/quick-overview.html) page (15 minutes)\n\n**Start to Use LVGL**\n\n  4. Set up a [Simulator](https://docs.lvgl.io/master/integration/ide/pc-simulator.html#simulator) (10 minutes)\n  5. Try out some [Examples](https://github.com/lvgl/lvgl/tree/master/examples)\n  6. Port LVGL to a board. See the [Porting](https://docs.lvgl.io/master/porting/index.html) guide or check the ready to use [Projects](https://github.com/lvgl?q=lv_port_)\n\n**Become a Pro**\n\n  7. Read the [Overview](https://docs.lvgl.io/master/overview/index.html) page to get a better understanding of the library (2-3 hours)\n  8. Check the documentation of the [Widgets](https://docs.lvgl.io/master/widgets/index.html) to see their features and usage\n\n**Get Help and Help Others**\n\n  9. If you have questions go to the [Forum](http://forum.lvgl.io/)\n  10. Read the [Contributing](https://docs.lvgl.io/master/CONTRIBUTING.html) guide to see how you can help to improve LVGL (15 minutes)\n\n\n## :handshake: Services\nLVGL LLC was established to provide a solid background for LVGL library and to offer several type of services to help you in UI development. With 15+ years of experience in the user interface and graphics industry we can help you the bring your UI to the next level.\n\n- **Graphics design** Our in-house graphics designers are experts in creating beautiful modern designs which fit to your product and the resources of your hardware.\n- **UI implementation** We can also implement your UI based on the design you or we have created. You can be sure that we will make the most out of your hardware and LVGL. If a feature or widget is missing from LVGL, don't worry, we will implement it for you.\n- **Consulting and Support** We can support you with consulting as well to avoid pricey and time consuming mistakes during the UI development.\n- **Board certification** For companies who are offering development boards, or production ready kits we do board certification which shows how board can run LVGL.\n\n\nCheck out our [Demos](https://lvgl.io/demos) as reference. For more information take look at the [Services page](https://lvgl.io/services).\n\n[Contact us](https://lvgl.io/#contact) and tell how we can help.\n\n\n## :star2: Contributing\nLVGL is an open project and contribution is very welcome. There are many ways to contribute from simply speaking about your project, through writing examples, improving the documentation, fixing bugs or even hosting your own project under the LVGL organization.\n\nFor a detailed description of contribution opportunities visit the [Contributing](https://docs.lvgl.io/master/CONTRIBUTING.html) section of the documentation.\n\nMore than 300 people already left their fingerprint in LVGL. Be one them! See you here! :slightly_smiling_face:\n\n<a href=""https://github.com/lvgl/lvgl/graphs/contributors"">\n  <img src=""https://contrib.rocks/image?repo=lvgl/lvgl&max=48"" />\n</a>\n\n... and many other.\n",15693,graphics,C,11,C,Makefile,Python,Shell,CMake,Ruby,HTML,C++,Handlebars,Assembly,Batchfile,,,,,,,,,,,,,,,,,,3305,449,2811,45,18,445,0,344263,3094,3204,3127,77,80b8c33f219d21b57b4cef0e2f149f471490cefa,ci(sdl): add sdl build to ci test (#6505),2024-07-19T10:10:58Z,Liu Yi,lhdjply@126.com,lhdjply,Release v9.1.0,See the [CHANGELOG](https://github.com/lvgl/lvgl/blob/master/docs/CHANGELOG.rst),v9.1.0,,,github-actions[bot],MIT License,lvgl,lvgl,63,embedded,gui,c,microcontroller,mcu,graphics,tft,,,,,,,,,,,,,,/lvgl/lvgl,66,300,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/lvgl/lv_binding_rust,https://github.com/lvgl/lv_binding_rust,0,,,0,0,0,0,0,0,1,1,0,0,0,"LVGL bindings for Rust. A powerful and easy-to-use embedded GUI with many widgets, advanced visual effects (opacity, antialiasing, animations) and low memory requirements (16K RAM, 64K Flash). ","<h1 align=""center""> LVGL - Open-source Embedded GUI Library in Rust</h1>\n\n![Original LVGL demo image](lv_demo.png)\n\n<p align=""center"">\nLVGL provides everything you need to create a Graphical User Interface (GUI) on embedded systems with easy-to-use graphical elements, beautiful visual effects and low memory footprint. \n</p>\n<p align=""center"">\nLVGL is compatible with <samp>#![no_std]</samp> environments by default.\n</p>\n\n<h4 align=""center"">\n<a href=""https://github.com/rafaelcaricio/lvgl-rs-wasm"">Rust to WASM demo</a> &middot;\n<a href=""https://lvgl.io/"">Official LVGL Website </a> &middot;\n<a href=""https://github.com/littlevgl/lvgl"">C library repository</a> &middot;\n<a href=""https://lvgl.io/demos"">Official live demos</a>\n</h4>\n\n---\n\n![Rust bindings usage demo code.](demo.png)\n\n## System Build Dependencies\n\nIn order to build the `lvgl` project you will need the following system dependencies to be installed:\n\n```\n$ sudo apt install build-essential llvm clang\n```\n\nIf you want to build the examples, then you will need to install SDL2 as well.\n\n```\n$ sudo apt install libsdl2-dev\n```\n\n## Usage\n\nEdit your `Cargo.toml` file dependencies with:\n```\n$ cargo add lvgl\n```\n\nThe build requires the environment variable bellow to be set:\n\n- `DEP_LV_CONFIG_PATH`: Path to the directory containing the `lv_conf.h` header file used for configuration of LVGL library.\n- (Optional) `LVGL_FONTS_DIR`: Directory for custom fonts generated for use in LVGL. See the documentation for usage.\n- (Optional) `LVGL_INCLUDE`: C headers to include during the build if using the `drivers` feature, comma-separated. The default is `/usr/include,/usr/local/include`.\n- (Optional) `LVGL_LINK`: C libraries to link in during the build if using the `drivers` feature, comma-separated. The default is `SDL2`.\n\nWe recommend the `lv_conf.h` file to be in your project's root directory. If so, the command to build your project would be:\n```shell script\n$ DEP_LV_CONFIG_PATH=`pwd` cargo build\n```\n\n### Building for embedded environments\n\nWe make use of `bindgen` for generating the bindings to LittlevGL at build time. There is a problem in cargo when building\nfor `no_std`, so we need to use a workaround to build ""lvgl-rs"". The mainstrem issue in cargo is being tracked at\n[rust-lang/cargo#7915](https://github.com/rust-lang/cargo/issues/7915).\n\n```shell\n$ DEP_LV_CONFIG_PATH=`pwd` cargo build -Zfeatures=build_dep\n```\n\nThe `unsafe_no_autoinit` feature must also be enabled when building for baremetal targets. See its documentation in `Cargo.toml` for notes on usage.\n### LVGL Global Allocator\n\nA [global allocator](https://doc.rust-lang.org/std/alloc/trait.GlobalAlloc.html) for Rust leveraging the\n[LVGL memory allocator](https://github.com/lvgl/lvgl/blob/master/src/misc/lv_mem.h) is provided, but not enabled \nby default. Can be enabled by the feature `lvgl_alloc`. This will make all dynamic memory to be allocated by LVGL \ninternal memory manager.\n\n## Running the demo\n\n**Hint for macOS users**: Before you run the demos you need to make sure you have [libsdl](https://www.libsdl.org)\ninstalled on your machine. To install it, use HomeBrew:\n\n```shell\n$ brew install sdl2\n```\n\n[This project contains examples that can run in a desktop simulator.](./examples)\n\nFirst, make sure to pull `lvgl-rs` submodules:\n```shell\n$ git submodule init\n$ git submodule update \n```\n\nThen run the `demo` example:\n\n```shell\n$ DEP_LV_CONFIG_PATH=`pwd`/examples/include cargo run --example demo --features=""alloc""\n```\n\n## Feature Support\n\nThe bindings are still in development. There are many features of LVGL that needs to be exposed by `lvgl-rs`. In\nthis section you can check what is implemented at the moment.\n\n### Features\n\nList of LVGL features that impacts the library usage in general.\n- [x] Displays: We use [`embedded_graphics`](https://docs.rs/embedded-graphics/0.6.2/embedded_graphics/) library to\n      draw to the display, along with [`lv_drivers`](https://github.com/lvgl/lv_drivers). You can\n      use `lvgl-rs` with any of the [`embedded_graphics`](https://docs.rs/embedded-graphics/0.6.2/embedded_graphics/#supported-displays) supported\n      displays, and those supported by [`lv_drivers`](https://github.com/lvgl/lv_drivers).\n      **Note:** [`lv_drivers`](https://github.com/lvgl/lv_drivers) support is currently experimental.\n- [x] Events: You can listen and trigger events in widget objects.\n- [x] Styles: You can set styles in any exposed object. We are still missing the possibility of defining global base styles.\n- [x] Input Devices: Input devices supported by [`lv_drivers`](https://github.com/lvgl/lv_drivers)\n      can be used, and custom handlers can be specified for [`embedded_graphics`](https://docs.rs/embedded-graphics/0.6.2/embedded_graphics/). Currently, only pointer input devices are supported.\n      **Note:** [`lv_drivers`](https://github.com/lvgl/lv_drivers) support is currently experimental.\n- [x] Fonts: All fonts built-in to LVGL can be used on nightly Rust if the `nightly` feature is enabled. Custom fonts can also be encoded into a C file (see the documentation on the `font` module).\n- [x] Animations: Creating basic animations is supported entirely from Rust.\n- [ ] Images\n- [ ] File system\n- [ ] Tasks\n\n### Widgets\n\nWidgets currently implemented might have some missing features. If the widget you want to use is not exposed or\nis missing a feature you want to make use, please send a Pull Request or open an issue.\n\n[![SWUbanner](https://raw.githubusercontent.com/vshymanskyy/StandWithUkraine/main/banner2-direct.svg)](https://github.com/vshymanskyy/StandWithUkraine/blob/main/docs/README.md)\n",639,graphics,Rust,2,Rust,C,,,,,,,,,,,,,,,,,,,,,,,,,,,97,2,83,12,7,20,0,1547,64,81,47,34,3b3abb3019eef637b932fae967a8d246bf5b6f14,feat: accept additional cflags from LVGL_CFLAGS env var,2024-06-15T11:50:51Z,Stanislav Ravas,ravas@tind.sk,elrafoon,Release 0.6.2,### Fixed\r\n\r\n- Fix build in docs.rs,0.6.2,Nia,,nia-e,MIT License,lv_binding_rust,lvgl,7,embedded,graphics,gui,rust,lvgl,,,,,,,,,,,,,,,,/lvgl/lv_binding_rust,11,12,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/lume/lume,https://github.com/lume/lume,0,,,0,0,0,0,0,0,1,1,0,0,0,Create 3D web applications with HTML. Bring a new depth to your DOM!,"<!-- # LUME -->\n\n# <a href=""//lume.io""><img src=""./logo.svg"" width=""200"" alt=""LUME"" title=""LUME"" /></a>\n\n#### **A toolkit that simplifies the creation of rich and interactive 2D or 3D experiences.**\n\n<h3>\n  <a href=""//lume.io"">Home</a>&nbsp;&nbsp;·&nbsp;\n  <a href=""//lume.io/docs"">Documentation</a>&nbsp;&nbsp;·&nbsp;\n  <a href=""//lume.io/docs/#/examples/hello3d"">Examples</a>&nbsp;&nbsp;·&nbsp;\n  <a href=""//lume.community"">Forum</a>&nbsp;&nbsp;·&nbsp;\n  <a href=""//discord.gg/PgeyevP"">Chat</a>&nbsp;&nbsp;·&nbsp;\n  <a href=""//github.com/lume/lume"">Source</a>\n</h3>\n\n### `npm install lume`\n\n## Features\n\nLUME is composed of several packages that can be used individually, or\ntogether as a whole:\n\n### [`lume`](./README.md) - HTML elements for rich graphics\n\nHTML elements for easily defining rich and interactive 2D or 3D applications\npowered by CSS3D, WebGL, or a combination of both.\n\nThis package uses and re-exports features from the below packages.\n\n### [`@lume/element`](//github.com/lume/element) - System for defining HTML elements\n\nThis is a web component system that allows you to create new, fast, and\nperformant HTML elements in a simple way. It provides the foundation for\nLUME's HTML elements, and a standard pattern for building new elements that\nextend the features of LUME.\n\n### [`element-behaviors`](//github.com/lume/element-behaviors) - Mix functionalities onto HTML elements\n\nThis allows you to augment HTML elements with features called ""behaviors""\nthat are similar to custom elements: each behavior is defined as a `class`\nthat has the same lifecycle methods as custom elements. The difference is\nthat an unlimited number of behaviors can be associated with an element.\n\n### [`glas`](//github.com/lume/glas) - WebGL engine written in AssemblyScript (WIP)\n\nThis is a WebGL engine with the consistent performance of WebAssembly, written\nin [AssemblyScript](http://assemblyscript.org/) (a TypeScript-to-WebAssembly compiler).\n\n## LUMECraft\n\nLUMECraft is a collection of applications made with LUME, showing what LUME can do, and serving as forkable starting points for further customization.\n\n### [`first-person-shooter`](//github.com/LUMECraft/first-person-shooter) - First-person shooter game\n\nA first-person shooter game foundation made with LUME, Solid.js, and Meteor.\n\n## Getting involved\n\nThere are various ways to get involved!\n\n- Visit the [documentation](//lume.io/docs) and make something awesome!\n- Submit fixes or new features to any packages or the website! See the\n  [contributing](./CONTRIBUTING.md) guide.\n- Discuss LUME, get help, or help others in the [forums](//lume.community) or\n  on our Discord [chat server](//discord.gg/PgeyevP).\n\n## Status\n\n[![tests](https://github.com/lume/lume/actions/workflows/tests.yml/badge.svg)](https://github.com/lume/lume/actions/workflows/tests.yml)\n",1282,graphics,TypeScript,3,JavaScript,TypeScript,Shell,,,,,,,,,,,,,,,,,,,,,,,,,,56,14,40,2,49,8,2,130023,57,245,133,112,d3e0be2b43a33996fb7a886e73c4f8967ba9374f,accept any URL string for Scene.background,2024-07-18T08:11:55Z,Joe Pea,joe@trusktr.io,trusktr,"v0.3.0-alpha.42 - little by little, good things take time!","## What's Changed since `v0.3.0-alpha.14`\r\n\r\n### Features\r\n\r\n* **feature**: add `""l""` and `""p""` shorthands for `""literal""` and `""proportional""` sizeMode values, respectively by @trusktr in https://github.com/lume/lume/pull/250\r\n  * F.e. `<lume-mesh size-mode=""l p"" size=""100 0.5"">`\r\n* **feature**: use DOM APIs only on the client, not SSR, by avoiding using DOM APIs at the top level of any module. This allows any SSR code (f.e. Svelte apps, Next/Nuxt, etc) that might be importing `lume` to not fail. by @trusktr in https://github.com/lume/lume/pull/250\r\n* **feature**: individual instance setter methods for `<lume-instanced-mesh>` as an alternative to attributes/props to avoid recalculating every instance matrix when updating only a single instance by @keywizzle in https://github.com/lume/lume/pull/265\r\n  * `el.setInstanceRotation(index, x, y, z)`\r\n  * `el.setInstanceScale(index, x, y, z)`\r\n  * `el.setInstancePosition(index, x, y, z)`\r\n  * `el.setInstanceColor(index, r, g, b)`\r\n* **feature**: remove inertial slowdown for `ScrollFling` because modern touchpads already have inertial slowdown, and instead add a lerp towards the current position to smooth it out for mouse wheels by @trusktr in https://github.com/lume/lume/pull/292\r\n* **feature**, **deprecation**: rename `Node` (which conflicts with the global `Node`) to `Element3D`, but leave a deprecated `Node` class for backwards compat for now by @trusktr in https://github.com/lume/lume/pull/250\r\n\r\n\r\n### Docs\r\n\r\n* **docs**: Update README.md by @UdayKharatmol in https://github.com/lume/lume/pull/282\r\n* **docs**: Update README.md by @bhargavshirin in https://github.com/lume/lume/pull/286\r\n\r\n### Infra\r\n\r\n* **infra**: update to Yarn 4.0 by @trusktr in https://github.com/lume/lume/pull/284\r\n* **infra**: update lume cli to run tests in the amazing `@web/test-runner` … by @trusktr in https://github.com/lume/lume/pull/287\r\n\r\n### Internal\r\n\r\n* **internal**: replace `Promise.resolve().then()` with the newer `queueMicrotask()` by @trusktr in https://github.com/lume/lume/pull/250\r\n* **internal**: legacy code cleanup 🎉: remove `DeclarativeBase`, split into `CompositionTracker` and `ChildTracker` classes, move remaining logic to the `ImperativeBase` base class by @trusktr in https://github.com/lume/lume/pull/250\r\n* **internal**: rename `ImperativeBase` to `SharedAPI`. by @trusktr in https://github.com/lume/lume/pull/250\r\n  - **BREAKING**: If you relied on this class (not recommended) import statements should be renamed.\r\n* **internal**: remove DOM  `transform` property backfill 🎉 by @trusktr in https://github.com/lume/lume/pull/250\r\n  * This provides an overall better zoom functionality across all browser and OS combinations.\r\n* **internal**: add InstancedMesh tests, and some internal updates fixes by @trusktr in https://github.com/lume/lume/pull/267\r\n\r\n\r\n## New Contributors\r\n* @keywizzle made their first contribution in https://github.com/lume/lume/pull/265\r\n* @UdayKharatmol made their first contribution in https://github.com/lume/lume/pull/282\r\n* @bhargavshirin made their first contribution in https://github.com/lume/lume/pull/286\r\n\r\n**Full Changelog**: https://github.com/lume/lume/compare/v0.3.0-alpha.14...v0.3.0-alpha.42",v0.3.0-alpha.42,Joe Pea,,trusktr,MIT License,lume,lume,16,javascript,graphics,graphics-engine,graphics-programming,three-d,three-dimensions,scene,custom-elements,ui,threejs,webgl,css3d,html,3d,3d-graphics,3d-models,3d-engine,web-components,dom-elements,lume,/lume/lume,45,36,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/loosolab/TOBIAS,https://github.com/loosolab/TOBIAS,1,,,1,1,1,1,0,0,0,0,0,0,1,Transcription factor Occupancy prediction By Investigation of ATAC-seq Signal,"TOBIAS - Transcription factor Occupancy prediction By Investigation of ATAC-seq Signal \n=======================================\n\n\n[![PyPI Version](https://img.shields.io/pypi/v/tobias.svg?style=plastic)](https://pypi.org/project/tobias/)\n[![PyPI download month](https://img.shields.io/pypi/dm/tobias.svg?style=plastic)](https://pypi.python.org/pypi/tobias/)\n[![install with bioconda](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?style=plastic)](http://bioconda.github.io/recipes/tobias/README.html)\n[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg?style=plastic)](https://GitHub.com/loosolab/TOBIAS/graphs/commit-activity)\n[![publication](https://img.shields.io/badge/Publication-NatComm-blue.svg?style=plastic)](https://doi.org/10.1038/s41467-020-18035-1)\n\nIntroduction \n------------\n\nATAC-seq (Assay for Transposase-Accessible Chromatin using high-throughput sequencing) is a sequencing assay for investigating genome-wide chromatin accessibility. The assay applies a Tn5 Transposase to insert sequencing adapters into accessible chromatin, enabling mapping of regulatory regions across the genome. Additionally, the local distribution of Tn5 insertions contains information about transcription factor binding due to the visible depletion of insertions around sites bound by protein - known as _footprints_. \n\n**TOBIAS** is a collection of command-line bioinformatics tools for performing footprinting analysis on ATAC-seq data, and includes:\n\n<img align=""right"" width=150 src=""/figures/tobias.png"">\n\n- Correction of Tn5 insertion bias\n- Calculation of footprint scores within regulatory regions\n- Estimation of bound/unbound transcription factor binding sites\n- Visualization of footprints within and across different conditions\n\nFor information on each tool, please see the [wiki](https://github.com/loosolab/TOBIAS/wiki/).\n\nInstallation\n------------\nTOBIAS is written as a python package and can be quickly installed via pip:\n```bash\n$ pip install tobias\n```\n\nTOBIAS is also available as a conda package on the Bioconda channel:\n```bash\n$ conda install tobias -c bioconda\n```\nPlease see the [installation](https://github.com/loosolab/TOBIAS/wiki/installation) page for more info.\n\nUsage\n------------\nAll tools are available through the command-line as ```TOBIAS <TOOLNAME>```, for example:\n``` \n$ TOBIAS ATACorrect\n__________________________________________________________________________________________\n\n                                   TOBIAS ~ ATACorrect\n__________________________________________________________________________________________\n\nATACorrect corrects the cutsite-signal from ATAC-seq with regard to the underlying\nsequence preference of Tn5 transposase.\n\nUsage:\nTOBIAS ATACorrect --bam <reads.bam> --genome <genome.fa> --peaks <peaks.bed>\n\nOutput files:\n- <outdir>/<prefix>_uncorrected.bw\n- <outdir>/<prefix>_bias.bw\n- <outdir>/<prefix>_expected.bw\n- <outdir>/<prefix>_corrected.bw\n- <outdir>/<prefix>_atacorrect.pdf\n\n(...)\n```\n\nOverview and command-line examples\n-------------\n\n* [ATACorrect](https://github.com/loosolab/TOBIAS/wiki/ATACorrect): Bias correction of ATAC-seq reads in open chromatin\n* [ScoreBigwig](https://github.com/loosolab/TOBIAS/wiki/ScoreBigwig): Calculate footprint scores from corrected cutsites\n* [BINDetect](https://github.com/loosolab/TOBIAS/wiki/BINDetect): Estimation of differentially bound motifs based on scores, sequence and motifs\n* [PlotAggregate](https://github.com/loosolab/TOBIAS/wiki/PlotAggregate): Plot aggregated ATAC-seq signals in combinations of .bed/.bw to visualize footprints\n* [PlotHeatmap](https://github.com/loosolab/TOBIAS/wiki/PlotHeatmap): Plot heatmaps and aggregates of ATAC-seq signals in combinations of .bed/.bw to visualize footprints\n* [PlotTracks](https://github.com/loosolab/TOBIAS/wiki/PlotTracks): Plot IGV-style genomic signals such as cutsites and footprints across a selection of regions\n* [FormatMotifs](https://github.com/loosolab/TOBIAS/wiki/FormatMotifs): A utility to convert and join/split across different motif-file formats\n* [ClusterMotifs](https://github.com/loosolab/TOBIAS/wiki/Additional) : Cluster motifs and create consensus motifs based on similarity\n* [CreateNetwork](https://github.com/loosolab/TOBIAS/wiki/CreateNetwork): Create TF-TF binding network from annotated TFBS\n* [FilterFragments](https://github.com/loosolab/TOBIAS/wiki/Additional): Filter fragments from a .bam-file using a .bed-file of regions\n* [Additional utility tools](https://github.com/loosolab/TOBIAS/wiki/Additional)\n\n\nPipelines\n----------------\nWhile each TOBIAS tool can be run independently, they are developed to be run as part of an analysis pipeline. We provide ready-made pipelines for performing bias-correction, footprinting, differential binding and visualization for multiple conditions automatically.\n\n**Snakemake pipeline**  \nWe provide a pre-set snakemake workflow which is found [here](https://github.com/loosolab/TOBIAS_snakemake).\n\n**Nextflow pipeline**  \nYou can also run the TOBIAS tool as a nextflow pipeline. The pre-set workflow can be found [here](https://github.molgen.mpg.de/loosolab/TOBIAS-nextflow).\n\n**Nextflow kubernetes/de.NBI cloud aware pipeline**  \nWe also provide the TOBIAS nextflow pipeline for a cloud computing environment. One version utilizes a [kubernetes framework](https://github.molgen.mpg.de/loosolab/TOBIAS-nextflow/tree/master/TOBIAS_MAPOKS), and a second version utilizing a webbased job scheduler, started automatically within a local TOBIAS run, making use of the de.NBI [cloud](https://github.molgen.mpg.de/loosolab/TOBIAS-nextflow/tree/master/TOBIAS_MACSEK).\n\nHelp \n--------\nIn case of any issues/questions/comments, please check out the [FAQ](https://github.com/loosolab/TOBIAS/wiki/FAQ). Otherwise, please write an issue [here](https://github.com/loosolab/TOBIAS/issues).\n\nHow to cite\n------------\n\nBentsen, M., Goymann, P., Schultheis, H. et al. ATAC-seq footprinting unravels kinetics of transcription factor binding during zygotic genome activation. Nat Commun 11, 4267 (2020). \n\nDOI: https://doi.org/10.1038/s41467-020-18035-1\n\nLicense\n------------\nThis project is licensed under the [MIT license](LICENSE). \n",181,bioinformatics,Python,3,Python,Shell,Cython,,,,,,,,,,,,,,,,,,,,,,,,,,20,0,20,0,4,9,0,27935,38,254,234,20,50d0b55522538b29726ea888f7196fec638f693a,Merge pull request #280 from loosolab/dev,2024-07-15T12:27:04Z,Moritz Hobein,81377794+mohobein@users.noreply.github.com,mohobein,v0.14.0,,v0.14.0,"Mette Bentsen, PhD",,msbentsen,MIT License,TOBIAS,loosolab,4,bioinformatics,atac-seq,footprinting,,,,,,,,,,,,,,,,,,/loosolab/TOBIAS,4,16,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/LooseLab/readfish,https://github.com/LooseLab/readfish,0,,0,0,0,0,0,0,0,1,0,0,0,0,CLI tool for flexible and fast adaptive sampling on ONT sequencers,,164,bioinformatics,Python,1,Python,,,,,,,,,,,,,,,,,,,,,,,,,,,,75,13,59,3,9,5,15,5054,31,264,249,15,8e5210e9d7e72d9b7e9d57e9a14b3e4f232fb7d1,Dorado plugin (#344),2024-05-09T14:20:43Z,mattloose,matt.loose@nottingham.ac.uk,mattloose,v2024.2.0 - Dorado >= 7.3.9 support,"## What's new\r\n### Dorado 7.3.9 support\r\nAs discussed in #347, due to the move to `ont_pybasecall_client_lib` by ONT with the release of Dorado 7.3.9, readfish was unable to connect to the basecall socket, as it was using `ont_pyguppy_client_lib`.\r\n\r\nThis release supports both connections to `dorado` 7.3.9 and upwards with the `dorado` plugin, (which also handles the new parameter `sample_rate` on `package_reads` smartly) and `dorado` < 7.3.9 with the `guppy plugin`.\r\n\r\nTo use the new plugin, simply switch the `caller_settings` table in your toml from `.guppy` to `.dorado`, leaving other parameters the same, like so:\r\n```toml\r\n[caller_settings.dorado]\r\n```\r\n\r\n### MinKNOW version checking\r\nReadfish also now lists a tested MinKNOW version range, and will raise a warning if the MinKNOW version being connected to is outside the range for the given version of readfish. \r\nHowever, readfish will still attempt to run adaptive sampling, and will most likely succeed!\r\nAs recommended in the logged warning, if a newer version of readfish doesn't exist, we recommend opening an issue on the github.\r\n\r\n> [!WARNING]\r\n> The Guppy plugin will likely be deprecated in the next major release of `readfish`.\r\n\r\n## What's Changed\r\n* Dorado plugin by @mattloose and @Adoni5 in https://github.com/LooseLab/readfish/pull/344\r\n* Minknow version checking by @mattloose and @Adoni5 https://github.com/LooseLab/readfish/pull/351\r\n\r\n**Full Changelog**: https://github.com/LooseLab/readfish/compare/2024.1.0...2024.2.0",2024.2.0,Rory Munro,,Adoni5,GNU General Public License v3.0,readfish,LooseLab,7,adaptive-sampling,bioinformatics,genomics,ont,oxford-nanopore,sequencing,,,,,,,,,,,,,,,/LooseLab/readfish,9,14,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/locationtech/jts,https://github.com/locationtech/jts,0,,0,0,0,0,0,0,0,1,0,0,0,0,The JTS Topology Suite is a Java library for creating and manipulating vector geometry.,"JTS Topology Suite\n==================\n\nThe JTS Topology Suite is a Java library for creating and manipulating vector geometry.  It also provides a comprehensive set of geometry test cases, and the TestBuilder GUI application for working with and visualizing geometry and JTS functions.\n\n![JTS logo](jts_logo.png)\n\n[![Travis Build Status](https://api.travis-ci.org/locationtech/jts.svg)](http://travis-ci.org/locationtech/jts) [![GitHub Action Status](https://github.com/locationtech/jts/workflows/GitHub%20CI/badge.svg)](https://github.com/locationtech/jts/actions) \n\n[![Join the chat at https://gitter.im/locationtech/jts](https://badges.gitter.im/locationtech/jts.svg)](https://gitter.im/locationtech/jts?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n\nJTS is a project in the [LocationTech](http://www.locationtech.org) working group of the Eclipse Foundation.\n\n![LocationTech](locationtech_mark.png) \n\n## Requirements\n\nCurrently JTS targets Java 1.8 and above.\n\n## Resources\n\n### Code\n* [GitHub Repo](https://github.com/locationtech/jts)\n* [Maven Central group](https://mvnrepository.com/artifact/org.locationtech.jts)\n\n### Websites\n* [LocationTech Home](https://locationtech.org/projects/technology.jts)\n* [GitHub web site](https://locationtech.github.io/jts/)\n\n### Communication\n* [Mailing List](https://accounts.eclipse.org/mailing-list/jts-dev)\n* [Gitter Channel](https://gitter.im/locationtech/jts)\n\n### Forums\n* [Stack Overflow](https://stackoverflow.com/questions/tagged/jts)\n* [GIS Stack Exchange](https://gis.stackexchange.com/questions/tagged/jts-topology-suite)\n\n## License\n\nJTS is open source software.  It is dual-licensed under:\n\n* [Eclipse Public License 2.0](https://www.eclipse.org/legal/epl-v20.html)\n* [Eclipse Distribution License 1.0](http://www.eclipse.org/org/documents/edl-v10.php) (a BSD Style License)\n\nSee also:\n\n* [License details](LICENSES.md)\n* Licensing [FAQ](FAQ-LICENSING.md)\n\n## Documentation\n\n* [**Javadoc**](https://locationtech.github.io/jts/javadoc) for the latest version of JTS\n* [**FAQ**](https://locationtech.github.io/jts/jts-faq.html) - Frequently Asked Questions \n* [**User Guide**](USING.md) - Installing and using JTS \n* [**Tools**](doc/TOOLS.md) - Guide to tools included with JTS\n* [**Developing Guide**](DEVELOPING.md) - how to build and develop for JTS\n* [**Upgrade Guide**](MIGRATION.md) - How to migrate from previous versions of JTS\n\n## History\n\n* [**Version History**](https://github.com/locationtech/jts/blob/master/doc/JTS_Version_History.md)\n* History from the previous JTS SourceForge repo is in the branch [`_old/history`](https://github.com/locationtech/jts/tree/_old/history)\n* Older versions of JTS can be found on SourceForge\n* There is an archive of distros of older versions [here](https://github.com/dr-jts/jts-versions)\n\n## Contributing\n\nIf you are interested in contributing to JTS please read the [**Contributing Guide**](CONTRIBUTING.md).\n\n## Downstream Projects\n\n### Derivatives (ports to other languages)\n* [**GEOS**](https://trac.osgeo.org/geos) - C++\n* [**NetTopologySuite**](https://github.com/NetTopologySuite/NetTopologySuite) - .NET\n* [**JSTS**](https://github.com/bjornharrtell/jsts) - JavaScript\n* [**dart_jts**](https://github.com/moovida/dart_jts) - Dart\n\n### Via GEOS\n* [**Shapely**](https://github.com/Toblerity/Shapely) - Python wrapper of GEOS\n* [**R-GEOS**](https://cran.r-project.org/web/packages/rgeos/index.html) - R wrapper of GEOS\n* [**rgeo**](https://github.com/rgeo/rgeo) - Ruby wrapper of GEOS\n* [**GEOSwift**](https://github.com/GEOSwift/GEOSwift)- Swift library using GEOS\n\nThere are many projects using GEOS - for a list see the [GEOS wiki](https://trac.osgeo.org/geos/wiki/Applications).\n\n\n",1903,geometry,Java,5,Batchfile,Shell,Java,HTML,Ruby,,,,,,,,,,,,,,,,,,,,,,,,603,108,462,33,11,54,29,39977,437,442,271,171,46326d37442a4c9fe834c22f5471192be6694438,Update JTS_Version_History.md,2024-07-05T17:06:20Z,Martin Davis,mtnclimb@gmail.com,dr-jts,JTS Release 1.19.0,"*Release Date: 06/21/2022*\r\n\r\n### New Features\r\n\r\n* Add `ConstrainedDelaunayTriangulator` and `PolygonTriangulator` (#775, #862)\r\n* Add `Tri` data structure for representing triangulations (#775)\r\n* Add `DiscreteFrechetDistance` (#764, #783)\r\n* Add `OffsetCurve` class (#810, #816)\r\n* Add `ConcaveHull` class for points (#823, #829)\r\n* Add `ConcaveHullOfPolygons` class (#870)\r\n* Add `PolygonHullSimplifier` class (#861, #880)\r\n* TWKB read and write implementation (#854)\r\n\r\n### Functionality Improvements\r\n\r\n* Improve `GeometryFixer` behaviour for holes outside polygons (#772)\r\n* Simplify and fix logic of `BufferParameters.setQuadSegs` (#778)\r\n* Improve `KdTree` query code to avoid recursion (#779)\r\n* Add `KdTree` seeding to`SnappingNoder` (#780)\r\n* Add `GeometryFixer` option to preserve `Multi` geometry types when collapses occur (#791)\r\n* Make `QuadTree` thread-safe (#792)\r\n* Allow specifying a fixed `PrecisionModel` via grid size (#804)\r\n* Improve `Densifier` to interpolate Z values (#835)\r\n* Add support for GeoJSON `Feature` and `FeatureCollection` types (#837)\r\n* Add `WKTReader.setFixStructure` to fix WKT input (#848)\r\n* Improve `LineSegment.hashCode` to reduce collisions (#872)\r\n\r\n### Performance Improvements\r\n\r\n* Improve performance of `CoveageUnion` by using boundary chains (#891)\r\n\r\n### Bug Fixes\r\n\r\n* Fix `WKTReader` geometry typename parsing (#786)\r\n* Fix `CoordinateArrays.reverse` to handle zero-length arrays #787\r\n* Fix `GeometryFixer` to appply `isKeepCollapsed` flag to `GeometryCollection` elements (#790)\r\n* Fix `RectangleIntersects` to handle XYZM geometry (#794)\r\n* Fix various operations to handle XYZM geometry (#795)\r\n* Fix `SnapRoundingNoder` to use tolerance in noding (also fixes `GeometryPrecisionReducer`) (#802)\r\n* Fix `MaximumInscribedCircle` to avoid infinite-looping on flat collapsed input (#807)\r\n* Add `OverlayNG` result area heuristic check (#812)\r\n* Fix the buffers generated for mitred joins (#818)\r\n* Fix `WKTReader` to produce correct XY coordinate dimension for POLYGON EMPTY (#828)\r\n* Fix `RelateOp` for a snapped line boundary point (#839)\r\n* Fix IsValidOp for repeated node points (#845)\r\n* Fix `IsSimpleOp` for repeated endpoints (#851)\r\n* Fix `GeometryFixer` via noding check for zero-distance buffers (#867)\r\n* Fix `MinimumDiameter.minimumRectangle` for flat inputs (#875)\r\n* Fix `BufferOp` inverted ring check optimization (#878)\r\n* Fix `STRtree` nearest-neighbour queries on empty trees to avoid NPE (#886)\r\n* Remove transitive compile dependency on junit from jts-io-commmon (#855)\r\n",1.19.0,Jody Garnett,,jodygarnett,Other,jts,locationtech,12,jts-topology-suite,jts,geometry,geometry-algorithms,geometry-library,java,java-library,computational-geometry,gis,ogc,ogc-wkt,geometric-algorithms,triangulation,voronoi,,,,,,,/locationtech/jts,13,70,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/LibreCAD/LibreCAD,https://github.com/LibreCAD/LibreCAD,0,,,0,0,1,0,0,0,0,0,0,0,0,"LibreCAD is a cross-platform 2D CAD program written in C++17. It can read DXF/DWG files and can write DXF/PDF/SVG files. It supports point/line/circle/ellipse/parabola/spline primitives. The user interface is highly customizable, and has dozens of translations.","# LibreCAD [![Build Status](https://travis-ci.org/LibreCAD/LibreCAD.svg?branch=master)](https://travis-ci.org/LibreCAD/LibreCAD) \n\n[→ Download ←](https://github.com/LibreCAD/LibreCAD/wiki/Download)\n\n[LibreCAD](https://www.librecad.org) is a 2D CAD drawing tool\nbased on the community edition of [QCAD](https://www.qcad.org).\nLibreCAD uses the cross-platform framework [Qt](https://www.qt.io/download-open-source/),\nwhich means it works with most operating systems.  \nThe user interface is translated in over 30 languages.  https://translate.librecad.org\n\nLibreCAD is free software; you can redistribute it and/or modify  \nit under the terms of the [GNU General Public License version 2](https://www.gnu.org/licenses/gpl-2.0.html) (GPLv2)  \nas published by the Free Software Foundation.  \nPlease read the [LICENSE](LICENSE) file for additional information.\n\nThe master branch represents the latest pre-release code,  \nand now requires Qt 6.4.0 or newer.  \nThe 2.2.1 branch requires Qt 5.15.0 or newer.\nThe 2.2 branch requires Qt 5.2.1 or newer.\nThe 2.1 branch will be the last to support Qt4.  \nThe 2.0 branch will be the last to support the QCAD toolbar. [![Build Status](https://travis-ci.org/LibreCAD/LibreCAD.svg?branch=2.0)](https://travis-ci.org/LibreCAD/LibreCAD) \n\n## DXF Converter\nLibreCAD can be used as dxf to a pdf, png or svg converter. For example, to convert a foo.dxf to foo.pdf, foo.png or foo.svg:\n```bash\n$ librecad dxf2pdf foo.dxf\n$ librecad dxf2png foo.dxf\n$ librecad dxf2svg foo.dxf\n```\n## Releases and Milestones\n\n- [Releases and Prereleases](https://github.com/LibreCAD/LibreCAD/releases)\n- [Milestones](https://github.com/LibreCAD/LibreCAD/milestones)\n\n## libdxfrw\n[libdxfrw](https://github.com/LibreCAD/libdxfrw) is an associated project that allows LibreCAD to read DXF and DWG files.\n\n#\n**Requests and Bug reports**\n\n- [GitHub issues (preferred)](https://github.com/LibreCAD/LibreCAD/issues)\n- [SourceForge tickets (disabled)](https://sourceforge.net/p/librecad/_list/tickets?source=navbar)\n\n**Users Documentation**\n\n- [Users Manual](https://librecad.readthedocs.io/)\n- [Wiki Main Page](https://dokuwiki.librecad.org/)\n\n**Questions or Comments**\n\n- [LibreCAD's Forum](https://forum.librecad.org/)\n- IRC: [#librecad](https://web.libera.chat/#librecad) at libera.chat\n\n**Building**\n\nRequirements:\n\n- [Qt](https://www.qt.io/download-open-source/) 6.4.0+ (MinGW version on Windows)\n- [Boost](https://www.boost.org/) 1.55.0+\n\nMore information: [Build from source](https://github.com/LibreCAD/LibreCAD/wiki/Build-from-source)\n\n**Contributing**\n\n[Git and GitHub](https://github.com/LibreCAD/LibreCAD/wiki/Git-and-GitHub)\n\n[Becoming a developer](https://github.com/LibreCAD/LibreCAD/wiki/Becoming-a-developer)\n\nThere is a [resources repository](https://github.com/LibreCAD/Resources) for people that want to indirectly  \ncontribute to the project by supplying icons, stylesheets, documentation, templates...\n\nAssociated downloads: <https://sourceforge.net/projects/librecad/files/Resources/>\n",4220,geometry,C++,11,Shell,QMake,C++,C,Redcode,HTML,CSS,NSIS,Batchfile,Roff,CMake,,,,,,,,,,,,,,,,,,725,152,553,20,9,111,0,98632,988,1092,715,377,3590452b8dfce2d657c238ecd8aa2e962f9ddf9e,Update README.md: libdxfrw url changed to GitHub,2024-07-18T18:59:00Z,Dongxu Li,dongxuli2011@gmail.com,dxli,Bugfix release 2.2.0.2,"## Bugfix release 2.2.0.2\r\nThis is a bugfix release for official stable release 2.2.0.\r\n\r\nIt fixes 3 minor issue:\r\n\r\n  * An undetected vulnerability, opening malformed LFF font files caused a crash\r\n  * Format issues in bundled fonts\r\n  * A regression, finding nearest points on ellipses caused a crash\r\n\r\n\r\nMD5 sums:\r\n```\r\n747e2e9986801d87b94e67d76a5807e9  LibreCAD-2.2.0.2-x86_64.appimage\r\n39f31210941ae2f94f757ff3d677e889  LibreCAD-2.2.0.2.dmg\r\nf73213a4c8036f46ef0a397efd5d7b16  LibreCAD-Installer-2.2.0.2.exe\r\n```\r\nSHA256 hashes:\r\n```\r\ne5f32dce953b856f357bb3ccaaffa30cad99459bbd0d474ab07b16065249c7fb  LibreCAD-2.2.0.2-x86_64.appimage\r\n552e2ac63fca297c617511c3983be7477bc050e8f774841abb7db5ce81ce935b  LibreCAD-2.2.0.2.dmg\r\n996014cc38e0e98d274fbbb89e4f6ff92455e487e3a06ba46a38feb7c575a9d2  LibreCAD-Installer-2.2.0.2.exe\r\n```\r\n",2.2.0.2,,,github-actions[bot],Other,LibreCAD,LibreCAD,20,cad,qt,dxf,drawing,2d,geometry,,,,,,,,,,,,,,,/LibreCAD/LibreCAD,59,195,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/liblava/liblava,https://github.com/liblava/liblava,0,,,0,0,0,0,0,0,1,1,0,0,0,Modern and easy-to-use library for Vulkan,"<br />\n\n<a href=""https://liblava.dev"">\n    <img align=""left"" src=""docs/assets/liblava_200px.png"" width=""110"">\n</a>\n\n<br />\n\n<a href=""https://liblava.dev""><img src=""docs/assets/liblava.svg""></a>\n\n**A modern and easy-to-use library for the Vulkan® API**\n\n<br />\n\n[![version](https://img.shields.io/badge/2024_preview-0.8.1-cf1020)](https://github.com/liblava/liblava/tags) [![License](https://img.shields.io/github/license/liblava/liblava)](#license) [![CodeFactor](https://img.shields.io/codefactor/grade/github/liblava/liblava)](https://www.codefactor.io/repository/github/liblava/liblava) &nbsp; [![Discord](https://img.shields.io/discord/439508141722435595)](https://discord.liblava.dev) [![Donate](https://img.shields.io/badge/donate-PayPal-3b7bbf.svg)](https://donate.liblava.dev) [![Twitter Follow](https://img.shields.io/badge/follow-@liblava-00acee)](https://twitter.com/liblava)\n\n<br />\n\n**lava**  provides **essentials** for **low-level graphics** - suited for **prototyping**, **tooling**, **profiling** and **education**. \n\n> **This lean framework** is written in **neat C++23** and it strives for a **modular rolling release** as far as possible. We *don't* want to promise too much... but **lava runs** really smoothly **on Windows** and **Linux**.\n\n<br />\n\n➜ &nbsp; [Download](https://github.com/liblava/liblava/releases) &nbsp; • &nbsp; [Documentation](https://docs.liblava.dev) (Tutorial + Guide) &nbsp; • &nbsp; [Projects](#projects) &nbsp; • &nbsp; [Modules](#modules) &nbsp; • &nbsp; [Collaborate](#collaborate)\n\n<br />\n\n# In a nutshell\n\n<a href=""https://vulkan.org/"">\n    <img align=""right"" src=""docs/assets/Vulkan_RGB_Dec16.svg"" width=""270"">\n</a>\n\n* **liblava** is written in **modern C++** with latest **Vulkan support**\n* Provides **run loop** abstraction for **window** and **input handling**\n* Plain **renderer** and  **command buffer model**\n* Batteries included ➜ runtime **shader compilation**\n* **Texture** and **mesh** loading from **virtual file system**\n* **Camera**, **imgui**, **logger** and much more...\n\n<br />\n\n[![engine](https://img.shields.io/badge/lava-engine-brightgreen.svg)](#lava-engine) [![app](https://img.shields.io/badge/lava-app-brightgreen.svg)](#lava-app) [![frame](https://img.shields.io/badge/lava-frame-brightgreen.svg)](#lava-frame) &nbsp; [![block](https://img.shields.io/badge/lava-block-red.svg)](#lava-block) [![asset](https://img.shields.io/badge/lava-asset-red.svg)](#lava-asset) [![resource](https://img.shields.io/badge/lava-resource-red.svg)](#lava-resource) [![base](https://img.shields.io/badge/lava-base-red.svg)](#lava-base) &nbsp; [![file](https://img.shields.io/badge/lava-file-blue.svg)](#lava-file) [![util](https://img.shields.io/badge/lava-util-blue.svg)](#lava-util) [![core](https://img.shields.io/badge/lava-core-blue.svg)](#lava-core)\n\n<br />\n\n# Take a look\n\n```c++\n#include ""liblava/lava.hpp""\n#include ""imgui.h""\n\nint main(int argc, char* argv[]) {\n\n    lava::engine app(""imgui demo"", { argc, argv });\n    if (!app.setup())\n        return lava::error::not_ready;\n\n    app.imgui.layers.add(""demo window"", []() {\n        ImGui::ShowDemoWindow();\n    });\n\n    return app.run();\n}\n```\n\n<br />\n\n# Demos\n\n|||\n|:-|:-|\n| ![demo](res/demo/screenshot.png) | [![lava demo](https://img.shields.io/badge/lava-demo-brightgreen.svg)](liblava-demo/main.cpp)<br />**free download on ➜ [itch.io](https://demo.liblava.dev)**<br /><br /> The collection includes all stages to play around. - *You can easily switch between them.* |\n|||\n\n## Stages\n\n|||\n|:-|:-|\n| ![light](res/light/screenshot.png) | [![spawn](https://img.shields.io/badge/lava-light-brightgreen.svg)](liblava-demo/light.cpp)<br />**deferred shading + offscreen rendering**<br /><br />Small demo that showcases how to render to an offscreen framebuffer and sample from it. - *It is a challenge in itself and also a compact solution.* |\n| ![spawn](res/spawn/screenshot.png) | [![light](https://img.shields.io/badge/lava-spawn-brightgreen.svg)](liblava-demo/spawn.cpp)<br />**uniform buffer + camera**<br /><br />This loads a very large mesh from file and simply textures it. - *Use your gamepad to control the camera if there is one around.* |\n| ![lamp](res/lamp/screenshot.png) | [![lamp](https://img.shields.io/badge/lava-lamp-brightgreen.svg)](liblava-demo/lamp.cpp)<br />**push constants to shader**<br /><br />Classic lamp to relax and where colors can be easily switched. - *Unfortunately it also consumes power - so be aware!* |\n| ![shapes](res/shapes/screenshot.png) | [![shapes](https://img.shields.io/badge/lava-shapes-brightgreen.svg)](liblava-demo/shapes.cpp)<br />**generating primitives**<br /><br />Switch between basic shapes and use the camera to fly around. - *A great start for your next interactive application.* |\n| ![generics](res/generics/screenshot.png) | [![generics](https://img.shields.io/badge/lava-generics-brightgreen.svg)](liblava-demo/generics.cpp)<br />**float, double & int meshes**<br /><br />This demo shows how to check GPU features and render mesh data with custom vertex layout. - *There is a chapter about it in the Guide.* |\n| ![triangle](res/triangle/screenshot.png) | [![triangle](https://img.shields.io/badge/lava-triangle-brightgreen.svg)](liblava-demo/triangle.cpp)<br />**unique classic mesh**<br /><br />Where graphics programming always begins. - *An example that illustrates how little it actually takes to render a triangle.* |\n|||\n\n<br />\n\n# Projects\n\n|||\n|:-|:-|\n| <img src=""https://raw.githubusercontent.com/pezcode/lava-rt/main/demo/res/cubes/screenshot.png""> | [![rt cubes](https://img.shields.io/badge/lava-rt_cubes-brightgreen.svg)](https://github.com/pezcode/lava-rt)<br /> **raytraced reflecting cubes**<br /><br />*Vulkan raytracing with liblava*<br />Support for the Vulkan KHR ray tracing extensions with idiomatic wrappers. ➜ [pezcode](https://github.com/pezcode) |\n| | *Do you have a project? Submit it with a [pull request](#collaborate)* |\n|||\n\n<br />\n\n# Modules\n\n## lava [engine](liblava/engine) \n\n[![engine](https://img.shields.io/badge/lava-engine-brightgreen.svg)](liblava/engine/engine.hpp) [![producer](https://img.shields.io/badge/lava-producer-brightgreen.svg)](liblava/engine/producer.hpp) [![props](https://img.shields.io/badge/lava-props-brightgreen.svg)](liblava/engine/props.hpp)\n\n&nbsp; ➜ &nbsp; *depends on [app](#lava-app)*\n\n## lava [app](liblava/app)\n\n[![app](https://img.shields.io/badge/lava-app-brightgreen.svg)](liblava/app/app.hpp) [![camera](https://img.shields.io/badge/lava-camera-brightgreen.svg)](liblava/app/camera.hpp) [![forward_shading](https://img.shields.io/badge/lava-forward_shading-brightgreen.svg)](liblava/app/forward_shading.hpp)\n\n[![benchmark](https://img.shields.io/badge/lava-benchmark-brightgreen.svg)](liblava/app/benchmark.hpp) [![config](https://img.shields.io/badge/lava-config-brightgreen.svg)](liblava/app/config.hpp) [![imgui](https://img.shields.io/badge/lava-imgui-brightgreen.svg)](liblava/app/imgui.hpp)\n\n&nbsp; ➜ &nbsp; *depends on [frame](#lava-frame) + [block](#lava-block) + [asset](#lava-asset)*\n\n## lava [frame](liblava/frame)\n\n[![argh](https://img.shields.io/badge/lava-argh-brightgreen.svg)](liblava/frame/argh.hpp) [![driver](https://img.shields.io/badge/lava-driver-brightgreen.svg)](liblava/frame/driver.hpp) [![frame](https://img.shields.io/badge/lava-frame-brightgreen.svg)](liblava/frame/frame.hpp) [![gamepad](https://img.shields.io/badge/lava-gamepad-brightgreen.svg)](liblava/frame/gamepad.hpp) [![input](https://img.shields.io/badge/lava-input-brightgreen.svg)](liblava/frame/input.hpp)\n\n[![render_target](https://img.shields.io/badge/lava-render_target-brightgreen.svg)](liblava/frame/render_target.hpp) [![renderer](https://img.shields.io/badge/lava-renderer-brightgreen.svg)](liblava/frame/renderer.hpp) [![swapchain](https://img.shields.io/badge/lava-swapchain-brightgreen.svg)](liblava/frame/swapchain.hpp) [![window](https://img.shields.io/badge/lava-window-brightgreen.svg)](liblava/frame/window.hpp)\n\n&nbsp; ➜ &nbsp; *depends on [resource](#lava-resource)*\n\n<br />\n\n## lava [block](liblava/block)\n\n[![attachment](https://img.shields.io/badge/lava-attachment-red.svg)](liblava/block/attachment.hpp) [![block](https://img.shields.io/badge/lava-block-red.svg)](liblava/block/block.hpp) [![descriptor](https://img.shields.io/badge/lava-descriptor-red.svg)](liblava/block/descriptor.hpp) [![render_pass](https://img.shields.io/badge/lava-render_pass-red.svg)](liblava/block/render_pass.hpp) [![subpass](https://img.shields.io/badge/lava-subpass-red.svg)](liblava/block/subpass.hpp)\n\n[![compute_pipeline](https://img.shields.io/badge/lava-compute_pipeline-red.svg)](liblava/block/compute_pipeline.hpp) [![render_pipeline](https://img.shields.io/badge/lava-render_pipeline-red.svg)](liblava/block/render_pipeline.hpp) [![pipeline](https://img.shields.io/badge/lava-pipeline-red.svg)](liblava/block/pipeline.hpp) [![pipeline_layout](https://img.shields.io/badge/lava-pipeline_layout-red.svg)](liblava/block/pipeline_layout.hpp)\n\n&nbsp; ➜ &nbsp; *depends on [base](#lava-base)*\n\n## lava [asset](liblava/asset)\n\n[![load_image](https://img.shields.io/badge/lava-load_image-red.svg)](liblava/asset/load_image.hpp) [![load_mesh](https://img.shields.io/badge/lava-load_mesh-red.svg)](liblava/asset/load_mesh.hpp) [![load_texture](https://img.shields.io/badge/lava-load_texture-red.svg)](liblava/asset/load_texture.hpp) [![write_image](https://img.shields.io/badge/lava-write_image-red.svg)](liblava/asset/write_image.hpp)\n\n&nbsp; ➜ &nbsp; *depends on [resource](#lava-resource) + [file](#lava-file)*\n\n## lava [resource](liblava/resource)\n\n[![buffer](https://img.shields.io/badge/lava-buffer-red.svg)](liblava/resource/buffer.hpp) [![mesh](https://img.shields.io/badge/lava-mesh-red.svg)](liblava/resource/mesh.hpp) [![primitive](https://img.shields.io/badge/lava-primitive-red.svg)](liblava/resource/primitive.hpp)\n\n[![format](https://img.shields.io/badge/lava-format-red.svg)](liblava/resource/format.hpp) [![image](https://img.shields.io/badge/lava-image-red.svg)](liblava/resource/image.hpp) [![texture](https://img.shields.io/badge/lava-texture-red.svg)](liblava/resource/texture.hpp)\n\n&nbsp; ➜ &nbsp; *depends on [base](#lava-base)*\n\n## lava [base](liblava/base)\n\n[![base](https://img.shields.io/badge/lava-base-red.svg)](liblava/base/base.hpp) [![instance](https://img.shields.io/badge/lava-instance-red.svg)](liblava/base/instance.hpp) [![memory](https://img.shields.io/badge/lava-memory-red.svg)](liblava/base/memory.hpp) [![queue](https://img.shields.io/badge/lava-queue-red.svg)](liblava/base/queue.hpp)\n\n[![platform](https://img.shields.io/badge/lava-platform-red.svg)](liblava/base/platform.hpp) [![device](https://img.shields.io/badge/lava-device-red.svg)](liblava/base/device.hpp) [![physical_device](https://img.shields.io/badge/lava-physical_device-red.svg)](liblava/base/physical_device.hpp)\n\n&nbsp; ➜ &nbsp; *depends on [util](#lava-util)*\n\n<br />\n\n## lava [file](liblava/file)\n\n[![file](https://img.shields.io/badge/lava-file-blue.svg)](liblava/file/file.hpp) [![file_system](https://img.shields.io/badge/lava-file_system-blue.svg)](liblava/file/file_system.hpp) [![file_utils](https://img.shields.io/badge/lava-file_utils-blue.svg)](liblava/file/file_utils.hpp) [![json_file](https://img.shields.io/badge/lava-json_file-blue.svg)](liblava/file/json_file.hpp) [![json](https://img.shields.io/badge/lava-json-blue.svg)](liblava/file/json.hpp)\n\n&nbsp; ➜ &nbsp; *depends on [core](#lava-core)*\n\n## lava [util](liblava/util)\n\n[![log](https://img.shields.io/badge/lava-log-blue.svg)](liblava/util/log.hpp) [![math](https://img.shields.io/badge/lava-math-blue.svg)](liblava/util/math.hpp) [![random](https://img.shields.io/badge/lava-random-blue.svg)](liblava/util/random.hpp) [![thread](https://img.shields.io/badge/lava-thread-blue.svg)](liblava/util/thread.hpp)\n\n[![hex](https://img.shields.io/badge/lava-hex-blue.svg)](liblava/util/hex.hpp) [![layer](https://img.shields.io/badge/lava-layer-blue.svg)](liblava/util/layer.hpp) [![telegram](https://img.shields.io/badge/lava-telegram-blue.svg)](liblava/util/telegram.hpp)\n\n&nbsp; ➜ &nbsp; *depends on [core](#lava-core)*\n\n## lava [core](liblava/core)\n\n[![data](https://img.shields.io/badge/lava-data-blue.svg)](liblava/core/data.hpp) [![id](https://img.shields.io/badge/lava-id-blue.svg)](liblava/core/id.hpp) [![misc](https://img.shields.io/badge/lava-misc-blue.svg)](liblava/core/misc.hpp) [![time](https://img.shields.io/badge/lava-time-blue.svg)](liblava/core/time.hpp) [![types](https://img.shields.io/badge/lava-types-blue.svg)](liblava/core/types.hpp) [![version](https://img.shields.io/badge/lava-version-blue.svg)](liblava/core/version.hpp)\n\n<br />\n\n# Collaborate\n\nUse the [issue tracker](https://github.com/liblava/liblava/issues) to report any bug or compatibility issue.\n\n:heart: &nbsp; Thanks to all [contributors](https://github.com/liblava/liblava/graphs/contributors) making **liblava** flow...\n\n<br />\n<br />\n\nIf you want to **contribute** - we suggest the following:\n\n1. Fork the [official repository](https://github.com/liblava/liblava/fork)\n2. Apply your changes to **your fork**\n3. Submit a [pull request](https://github.com/liblava/liblava/pulls) describing the changes you have made\n\n<br />\n\n## Support\n\n<br />\n\n**Need help?** &nbsp; Please feel free to ask us on ➜ [Discord](https://discord.liblava.dev)\n\n<br />\n\n| Help maintenance and development | Every star and follow motivates |\n|---------:|:---------|\n| [![paypal](https://www.paypalobjects.com/en_US/i/btn//btn_donate_SM.gif)](https://donate.liblava.dev) | [![GitHub Stars](https://img.shields.io/github/stars/liblava/liblava?style=social)](https://github.com/liblava/liblava/stargazers) &nbsp; [![Twitter URL](https://img.shields.io/twitter/follow/liblava?style=social)](https://twitter.com/liblava) |\n\n<br />\n\n# License\n\n**liblava** is licensed under [MIT License](LICENSE) which allows you to use the software for any purpose you might like - including commercial and for-profit use. However - this library includes several [Third-Party](docs/README.md#Third-Party) libraries which are licensed under their own respective [Open Source](https://opensource.org) licenses ➜ They all allow **static linking** with closed source software.\n\n> **All copies of liblava must include a copy of the MIT License terms and the copyright notice.**\n\n<br />\n\n**Vulkan** and the Vulkan logo are trademarks of the <a href=""http://www.khronos.org"" target=""_blank"">Khronos Group Inc.</a>\n\nCopyright (c) 2018-present - <a href=""https://lava-block.com"">Lava Block OÜ</a> and [contributors](https://github.com/liblava/liblava/graphs/contributors)\n\n<br />\n<br />\n\n<a href=""https://liblava.dev""><img src=""docs/assets/liblava_200px.png"" width=""50""></a>\n",766,graphics,C++,6,CMake,C++,Shell,GLSL,Batchfile,Python,,,,,,,,,,,,,,,,,,,,,,,64,4,60,0,1,9,0,10839,46,38,32,6,686915ecef46ee6054b7a795d1dc64e7ff9051ec,update,2024-07-08T20:20:14Z,Lava Block,the@lava-block.com,TheLavaBlock,liblava 2022 - v0.7.3,"<a href=""https://github.com/liblava/liblava/releases/download/0.7.3/liblava_2022.zip"">![demo](res/demo/screenshot.png)</a>\r\n\r\n**Full Changelog**: https://github.com/liblava/liblava/compare/0.5.5...0.7.3\r\n<br />\r\n\r\n<a href=""https://liblava.dev""><img src=""https://github.com/liblava.png"" width=""50""></a>",0.7.3,Lava Block,,TheLavaBlock,MIT License,liblava,liblava,8,vulkan-library,modern-cpp,modular,cross-platform,vulkan,graphics,rendering,vulkan-graphics,framework,liblava,vulkan-api,3d-graphics,cpp,graphics-engine,renderer,cpp23,,,,,/liblava/liblava,18,20,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/libgeos/geos,https://github.com/libgeos/geos,0,,,0,0,0,0,0,0,1,0,0,0,0,"Geometry Engine, Open Source","GEOS -- Geometry Engine, Open Source\n====================================\n\nGEOS is a C++ library for performing operations on two-dimensional vector\ngeometries. It is primarily a port of the [JTS Topology\nSuite](https://github.com/locationtech/jts) Java library.  It provides many of\nthe algorithms used by [PostGIS](http://www.postgis.net/), the\n[Shapely](https://pypi.org/project/Shapely/) package for Python, the\n[sf](https://github.com/r-spatial/sf) package for R, and others.\n\nMore information is available the [project homepage](https://libgeos.org).\n\nThe official Git repository is at [GitHub](https://github.com/libgeos/geos).\n\n## Build Status\n\n| CI    | Status | CI    | Status | CI    | Status |\n| :---: | :----- | :---: | :----- | :---: | :----- |\n| GitHub | [![github](https://github.com/libgeos/geos/workflows/CI/badge.svg?branch/main)](https://github.com/libgeos/geos/actions?query=workflow:CI+branch:main) | Bessie | [![bessie](https://debbie.postgis.net/buildStatus/icon?job=GEOS_Worker_Run/label=bessie&build=last:${params.reference=refs/heads/main})](https://debbie.postgis.net/view/GEOS/job/GEOS_Worker_Run/label=bessie) | Debbie | [![debbie](https://debbie.postgis.net/buildStatus/icon?job=GEOS_Master)](https://debbie.postgis.net/view/GEOS/job/GEOS_Master/) |\n| GitLab CI | [![gitlab-ci](https://gitlab.com/geos/libgeos/badges/main/pipeline.svg)](https://gitlab.com/geos/libgeos/commits/main) | Bessie32  | [![bessie32](https://debbie.postgis.net/buildStatus/icon?job=GEOS_Worker_Run/label=bessie32&build=last:${params.reference=refs/heads/main})](https://debbie.postgis.net/view/GEOS/job/GEOS_Worker_Run/label=bessie32) | Winnie | [![winnie](https://winnie.postgis.net/view/GEOS/job/GEOS_Master/badge/icon)](https://winnie.postgis.net/view/GEOS/job/GEOS_Master/) |\n| | | Berrie | [![berrie](https://debbie.postgis.net/buildStatus/icon?job=GEOS_Worker_Run/label=berrie&build=last:${params.reference=refs/heads/main})](https://debbie.postgis.net/view/GEOS/job/GEOS_Worker_Run/label=berrie) | Dronie | [![dronie](https://dronie.osgeo.org/api/badges/geos/geos/status.svg?branch=main)](https://dronie.osgeo.org/geos/geos?branch=master) |\n| | | Berrie64 | [![berrie64](https://debbie.postgis.net/buildStatus/icon?job=GEOS_Worker_Run/label=berrie64&build=last:${params.reference=refs/heads/main})](https://debbie.postgis.net/view/GEOS/job/GEOS_Worker_Run/label=berrie64) |\n\n## Community Resources\n\n* Website: https://libgeos.org\n* **git** repository: https://github.com/libgeos/geos\n* [**geos-devel** mailing list](https://lists.osgeo.org/mailman/listinfo/geos-devel) and [archive](https://lists.osgeo.org/pipermail/geos-devel/)\n* **#geos** chat channel (all bridged):\n  * Matrix: https://matrix.to/#/#geos:osgeo.org\n  * IRC: irc://irc.libera.chat/#osgeo-geos (https://kiwiirc.com/nextclient/irc.libera.chat/#osgeo-geos)\n  * Slack: https://osgeo.slack.com/messages/C07RKJ06B/\n\n## Build/Install\n\nSee the [INSTALL](INSTALL.md) file.\n\n## Reference Docs\n\n* [C API](https://libgeos.org/doxygen/geos__c_8h.html)\n* [C++ API](https://libgeos.org/doxygen/cpp_iface.html)\n\nSee also the [C API tutorial](https://libgeos.org/usage/c_api/)\nand the [C++ API tutorial](https://libgeos.org/usage/cpp_api/).\nThere are code [examples](https://github.com/libgeos/geos/tree/main/examples) in the code repository.\n\n## Client Applications\n\n### Using the C interface\n\nGEOS promises long-term stability of the C API. In general, successive releases\nof the C API may add new functions but will not remove or change existing types\nor function signatures. The C library uses the C++ interface, but the C library\nfollows normal ABI-change-sensitive versioning, so programs that link only\nagainst the C library should work without relinking when GEOS is upgraded. For\nthis reason, it is recommended to use the C API for software that is intended\nto be dynamically linked to a system install of GEOS.\n\nThe `geos-config` program can be used to determine appropriate compiler and\nlinker flags for building against the C library:\n\n    CFLAGS += `geos-config --cflags`\n    LDFLAGS += `geos-config --ldflags` -lgeos_c\n\nAll functionality of the C API is available through the `geos_c.h` header file.\n\nDocumentation for the C API is provided via comments in the `geos_c.h` header\nfile. C API usage examples can be found in the GEOS unit tests and in the\nsource code of software that uses GEOS, such as PostGIS and the sf package\nfor R.\n\n### Using the C++ interface\n\nThe C++ interface to GEOS provides a more natural API for C++ programs, as well\nas additional functionality that has not been exposed in the C API.  However,\ndevelopers who decide to use the C++ interface should be aware that GEOS does\nnot promise API or ABI stability of the C++ API between releases.  Breaking\nchanges in the C++ API/ABI are not typically announced or included in the NEWS\nfile.\n\nThe C++ library name will change on every minor release.\n\nThe `geos-config` program can be used to determine appropriate compiler and\nlinker flags for building against the C++ library:\n\n    CFLAGS += `geos-config --cflags`\n    LDFLAGS += `geos-config --ldflags` -lgeos\n\nA compiler warning may be issued when building against the C++ library. To\nremove the compiler warning, define `USE_UNSTABLE_GEOS_CPP_API` somewhere\nin the program.\n\nCommonly-used functionality of GEOS is available in the `geos.h` header file.\nLess-common functionality can be accessed by including headers for individual\nclasses, e.g. `#include <geos/algorithm/distance/DiscreteHausdorffDistance.h>`.\n\n    #include <geos.h>\n\nC++ usage examples can be found in [examples](examples/).\n\n### Using other languages\n\nGEOS has bindings in many languages, see the [bindings\npage](https://libgeos.org/usage/bindings/).\n\n## Documentation\n\nAPI documentation can be generated using Doxygen. Documentation is not included\nin the default build. To build the documentation, run:\n\n    cmake -DBUILD_DOCUMENTATION=YES\n    cmake --build . --target docs\n\n## Style\n\nTo format your code into the desired style, use the astyle\nversion included in source tree:\n\n    tools/astyle.sh <yourfile.cpp>\n\n## Testing\n\nSee documentation in [tests/README.md](tests/README.md).\n\n## Tools\n\n* `geosop` - a CLI for GEOS.  Documentation is in [util/geosop/README.md](util/geosop/README.md).\n",1127,geometry,C++,9,CMake,Shell,C++,C,Batchfile,HTML,CSS,Dockerfile,Perl,,,,,,,,,,,,,,,,,,,,745,284,451,10,19,79,495,70345,345,382,300,82,42546119c35e65aad72dea1477eb4a057ead631e,Update committers/core-contributors section in psc page (add EvenR),2024-07-18T07:35:56Z,Sandro Santilli,strk@kbt.io,strk,Release 3.12.2,"2024-06-05\n\n- Fixes:\n  - Intersection: change to using DoubleDouble computation to improve robustness (GH-937, Martin Davis)\n  - Fix build on Illumus (GH-971)\n  - Buffer mitre join error (GH-995, Paul Ramsey)\n  - Return 2D empty linestring on GEOSGeom_createLineString(NULL) (GH-998, Paul Ramsey)\n  - Fix DiscreteHausdorffDistance for LinearRing (GH-1000, Martin Davis)\n  - PointOnSurface crashes with a collection containing a empty linestring (GH-1002, Paul Ramsey)\n  - Fix IsSimpleOp for MultiPoint with empty element (GH-1005, Martin Davis)\n  - Fix PreparedPolygonContains for GC with MultiPoint (GH-1008, Martin Davis)\n  - Fix TopologyPreservingSimplifier to prevent jumping components (GH-1012, Martin Davis)\n  - Fix reading WKT with EMPTY token with white space (GH-1025, Mike Taves)\n  - Segfault in CoverageSimplify for non-polygonal inputs (GH-1039, Paul Ramsey)\n  - Fix buffer Inverted Ring Removal check (GH-1056, Martin Davis)\n  - CoveragePolygonValidator: add section performance optimization (GH-1099, Martin Davis)\n\n",03.12.2002,,,github-actions[bot],GNU Lesser General Public License v2.1,geos,libgeos,13,c,c-plus-plus,cpp,geometry,,,,,,,,,,,,,,,,,/libgeos/geos,128,62,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/libgd/libgd,https://github.com/libgd/libgd,0,,,0,0,0,0,0,0,1,1,0,0,0,GD Graphics Library,"# GD Graphics (Draw) Library\n[![Build Status](https://scan.coverity.com/projects/3810/badge.svg)](https://scan.coverity.com/projects/libgd)\n[![Chat](https://badges.gitter.im/libgd/libgd.svg)](https://gitter.im/libgd/libgd)\n[![codecov.io](https://codecov.io/github/libgd/libgd/coverage.svg?branch=master)](https://codecov.io/github/libgd/libgd/)\n\nGD is an open source code library for the dynamic creation of images by\nprogrammers.\n\nGD is written in C, and ""wrappers"" are available for Perl, PHP and other\nlanguages. GD can read and write many different image formats. GD is commonly\nused to generate charts, graphics, thumbnails, and most anything else, on the\nfly.\n\nThe most common applications of GD involve website development, although it\ncan be used with any standalone application!\n\nThe library was originally developed by Thomas Boutell and is now maintained\nby many contributors (see the [CONTRIBUTORS](CONTRIBUTORS) file) under the\numbrella of PHP.net.\n\nIf you like to contribute, report bugs, see [how to contribute document](CONTRIBUTING.md)\n\nFor security related issues, please contact us at pierre@php.net\n\nSupport available in [![Chat](https://badges.gitter.im/libgd/libgd.svg)](https://gitter.im/libgd/libgd) or using issues.\n\nWe also have a mailing list. To subscribe to any mailing list, send an email to gd-devel-subscribe@lists.php.net. Then emails can be sent to gd-devel@lists.php.net.\n\n## Downloads/etc...\n\nPlease visit our [homepage](https://www.libgd.org/) for more details.\n\n## Supported Image Formats\n\nGD has support for:\n\n* [WebP](https://en.wikipedia.org/wiki/WebP) via [libwebp](https://developers.google.com/speed/webp/)\n* [JPEG](https://en.wikipedia.org/wiki/JPEG) via [IJG/libjpeg](http://www.ijg.org/) or [libjpeg-turbo](http://libjpeg-turbo.virtualgl.org/)\n  * Does not include [JPEG 2000](https://en.wikipedia.org/wiki/JPEG_2000)\n* [PNG](https://en.wikipedia.org/wiki/Portable_Network_Graphics) via [libpng](http://www.libpng.org/)\n* [AVIF](https://en.wikipedia.org/wiki/AV1#AV1_Image_File_Format_(AVIF)) via [libavif](https://github.com/AOMediaCodec/libavif)\n  * This includes [AVIF](https://en.wikipedia.org/wiki/AV1#AV1_Image_File_Format_%28AVIF%29) read support if your system's `libheif` has AV1 decoding.\n* [HEIF](https://en.wikipedia.org/wiki/High_Efficiency_Image_File_Format) via [libheif](https://github.com/strukturag/libheif/)\n* [TIFF](https://en.wikipedia.org/wiki/Tagged_Image_File_Format) via [libtiff](http://www.libtiff.org/)\n* [BMP](https://en.wikipedia.org/wiki/BMP_file_format) (builtin)\n* [GIF](https://en.wikipedia.org/wiki/GIF) (builtin)\n* [TGA](https://en.wikipedia.org/wiki/Truevision_TGA) (builtin)\n* [WBMP](https://en.wikipedia.org/wiki/Wireless_Application_Protocol_Bitmap_Format) (builtin)\n* [XPM](https://en.wikipedia.org/wiki/X_PixMap) via [libXpm](http://xorg.freedesktop.org/)\n\nBesides that, GD depends on some external libraries, which are all optional\nand disabled by default:\n\n* [FreeType](https://freetype.org) for rendering fonts\n* [Fontconfig](https://fontconfig.org) for configuring and customizing font access\n* [libraqm](https://github.com/HOST-Oman/libraqm) for complex text layout\n* [libimagequant](https://pngquant.org/lib) for conversion of RGBA images to 8-bit indexed-color images\n  * **NOTE** libimagequant is dual-licensed: GPLv3 and commercial license\n\n\n## Platforms supported\n\nCI means whether we have an automatic CI for this platform. If someone has CI for these platforms or any other platforms not listed here and would like to add them to our automatic CI, please get in touch with us, it will much appreciated!\n\n| Platform  | Support | CI |\n| ------------- | ------------- |----|\n| Linux x64  | &#10003;  | &#10003; |\n| Linux x86  | &#10003;  | &#10003; |\n| Linux ARM64  | &#10003; | &#10003; |\n| Windows x86  | &#10003;  | &#10003; |\n| Windows x64  | &#10003;  | &#10003; |\n| Windows arm64  | &#10003;  | x |\n| macOS x64  | &#10003;  | &#10003; |\n| macOS M1  | &#10003;  | x |\n| S390  | &#10003;  | x |\n\nIt is also known to work on almost all variations of *BSD, Solaris, etc. We don't have CI nor environment to test them. However many progamming languages binding do test libgd on these platforms.\n\n## Compilers\n\nIt should compile with all C99 and C++ compliant compilers, either using CMake or the configure script.\n\nWe do have CI using:\n- GCC\n- CLang\n- Visual Studio\n- Xcode\n- MingW\n\n## Supported Versions\n\n- GD 2.3 (Branch GD-2.3) serie is in active support for bug fixes. No new additions will be added.\n- GD 2.4 (master) is the active development branch. No release date yet.\n- GD 3.0 has been started, focusing on high quality 2D Vector drawing APIs and full support of actual ARGB 32 bits/8bits or float. It is not production ready yet\n",887,graphics,C,11,Shell,CMake,C,Makefile,Perl,HTML,Tcl,C++,Batchfile,M4,Python,,,,,,,,,,,,,,,,,,300,81,203,16,10,68,0,26470,264,570,471,99,83498bbe1781bda6d122e9e3a1705116728224fb,gdImageClone: fix typo during members copy.,2024-06-07T04:57:10Z,David Carlier,devnexen@gmail.com,devnexen,"Bugs fixes releases, Windows (and vcpkg support) , MacOs and Mingw Builds fixes","The LibGD team is proud to announce the 2.3.3 release of libgd. This release brings a few fixes as well as improved compilations and builds on all platforms. On Windows, vcpkg to install libGd dependencies is now well supported.\r\n\r\n### Fixed\r\n\r\n- [#759](https://github.com/libgd/libgd/issues/759) update cmake to generate config.h in the build dir\r\n- [#756](https://github.com/libgd/libgd/issues/756) 2.3.3 release\r\n- [#750](https://github.com/libgd/libgd/issues/750) gdPutBuf return value check\r\n- [#729](https://github.com/libgd/libgd/issues/729) HEIF builds fail with latest distros\r\n- [#678](https://github.com/libgd/libgd/issues/678) segfault in heif tests due to missing label.heic\r\n- [#677](https://github.com/libgd/libgd/issues/677) Test failure avif/compare_avif_to_png with libavif-0.8.2\r\n- [#661](https://github.com/libgd/libgd/issues/661) imagecopyresampled() produce artifacts on transparent PNG\r\n- [#611](https://github.com/libgd/libgd/issues/611) Fixes to build v2.3.0 on Windows with MinGW-w64\r\n- [#415](https://github.com/libgd/libgd/issues/415) optimize option in gif animation causes segfault\r\n- [#331](https://github.com/libgd/libgd/issues/331) _gdContributionsCalc() always uses DEFAULT_BOX_RADIUS\r\n- [#320](https://github.com/libgd/libgd/issues/320) gdImageRotateInterpolated() converts the source image to truecolor\r\n- [#249](https://github.com/libgd/libgd/issues/249) CMake and Makefiles build broken on Windows\r\n- [#93 ](https://github.com/libgd/libgd/issues/93) gdImageScaleTwoPass() looses top row and left column",gd-2.3.3,Pierre Joye,,pierrejoye,Other,libgd,libgd,15,graphics,2d,vector,processing,image,cross-platform,windows,linux,macos,c,png,jpeg,image-processing,webp,heic,avif,bmp,tiff,,,/libgd/libgd,64,59,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/layabox/LayaAir,https://github.com/layabox/LayaAir,0,,,0,0,0,0,0,0,1,1,0,0,0,LayaAir is a fully platform rendering engine with rich 2D/3D rendering capabilities and a mature integrated development platform,"<p align=""center"">\n    <a href=""https://layaair.com"">\n        <img src=""https://github.com/layabox/LayaAir/assets/38777031/5519a795-c050-4612-8ee0-0907a946260b""\n    </a>\n</p>\n\n# LayaAir Engine\n\n**[LayaAir](https://layaair.com/) engine, under the [Layabox](https://www.layabox.com/) brand, is a 3D engine that supports full-platform publishing. It can be applied in various fields such as games, education, advertising, marketing, digital twins, metaverse, AR guides, VR scenes, architectural design, industrial design, etc.**\n\n[中文](README.zh-CN.md)\n\n![Screenshot of LayaAirIDE](https://github.com/layabox/LayaAir/assets/38777031/f520c762-98e4-41f0-8145-df6a6cb422d6)\n\nLayaAir engine has adapted to many mainstream graphics APIs, such as WebGL/WebGPU/OpenGL/Vulkan, and supports programmable rendering pipelines, next-generation PBR rendering streams, ClusterLighting multi light technology, Forward+rendering pipelines, etc.\n\nLayaAir engine can be released to multiple game platforms with one click. In addition to HTML5 WEB, it also supports the release of Native APP (Android、iOS、Mac、Windows、Linux), mini games (such as WeChat mini-games, ByteDance mini-games, Alipay mini-games, OPPO mini-games,vivo mini-games, and Xiaomi Quick Games).\n\nLayaAir engine provides a powerful IDE, including a 3D scene editor, material editor, particle editor, blueprint editor, animation editor, physics editor, and UI editor. The IDE provides rich extension capabilities for developers to customize workflows, and developers can upload plugins to the resource store for sharing and sales.\n\nLayaAir engine actively embraces AI and has built-in AIGC framework, providing AI creation generation, AI control IDE, AI customer service and other products.\n\n## Getting the engine\n\n### Binary downloads\n\nDownload LayaAir IDE directly from the official website of LayaAir, which includes the corresponding version of the engine.\n\n[LayaAir Engine Download](https://layaair.com/#/engineDownload).\n\n### Compiling from source\n\n#### Install\n\nRun the following command from the command line in the engine root directory, as shown below:\n\n```bash\nnpm install\n```\n\n#### Run examples\n\nRun the following command from the command line in the engine root directory, as shown below:\n\n```bash\nnpm run start\n```\n\n#### Build\n\nIn the root directory of the engine, rrun the followingcommand from the command line. After that, a build folder will be generated in the root directory, where the compiled engine is located. As shown below:\n\n```bash\nnpm run build\n```\n\n#### How to use compiled engine in LayaAirIDE\n\nIf you need to use your own compiled engine instead of the IDE's built-in engine, you can copy the JavaScript files from the ""build/libs"" folder of the engine to the ""root directory of your IDE project/engine/libs"". You don't need to copy all the JavaScript files, you can only copy the files you need to overwrite.\n\n#### How to choose a branch\n\n- LayaAir_3.x:  e.g. LayaAir_3.1. These are stable versions of the LayaAir3 engine, with each sub version number corresponding to a branch.\n\n- LayaAir_2.x:  e.g. LayaAir_2.13.3. These are stable versions of the LayaAir2 engine, with each revision version number corresponding to a branch.\n\n- Master3.0: The active development version of LayaAir3, please do not use it for production environments.\n\n## How to contribute\n\nFork an official open source project from github to your own repository, clone it to your local computer, make changes to the code, and submit a PR to the official repository. We will respond as soon as possible and provide Approve and merge for you. Welcome everyone to join us in building together.\n\n## Related links\n\n- Official website https://LayaAir.com/\n\n- Forum https://ask.LayaAir.com/\n\n- Demo https://LayaAir.com/3.x/demo/\n\n- Documents https://LayaAir.com/3.x/doc/\n\n- API References https://LayaAir.com/3.x/api/\n\n- Showcases https://LayaAir.com/#/enginedemo\n\n## License\n\nMIT\n\n",1645,physics,JavaScript,7,JavaScript,HTML,TypeScript,GLSL,Shell,RenderScript,Batchfile,,,,,,,,,,,,,,,,,,,,,,1417,133,1267,17,32,42,0,508316,459,122,103,19,8d75dbde60ca882c174baa9c9dcb87fcf6660177,Merge branch 'LayaAir_3.2' of https://github.com/layabox/LayaAir into…,2024-07-19T08:47:26Z,LayaCharley,399050@qq.com,LayaCharley,LayaAir 3.1.4 Engine Library,"> 更多版本日志以及IDE的下载，请前往LayaAir官网：https://layaair.com/#/engineDownload\r\n\r\n# Version 3.1.4 Release Changelog\r\n\r\n### Bug Fixes\r\n\r\n#### Engine\r\n\r\n1. Fixed a bug causing errors due to repeated addition of the 3D physics character controller.\r\n2. Fixed a bug where 3D rigid bodies were still affected by gravity after destroying the 3D physics collider shape.\r\n3. Fixed a bug in the 2D physics engine where applying a force or impulse to an object always directed the force to the right.\r\n4. Fixed a bug in the 2D physics engine where objects set to allow sleep (AllowSleep) did not respond to continuous impulses.\r\n5. Fixed a bug where adding a rigid body to a 2D object invalidated cacheAs.\r\n6. Fixed a memory leak in the 2D physics physics2D.\r\n7. Fixed a bug where setting a dynamic rigid body with velocity to a static rigid body in 2D physics would transfer the velocity to the static body.\r\n8. Fixed a bug in 3D UI components where child nodes with 3D UI did not refresh their positions after removing and re-adding the parent node and changing its position.\r\n9. Fixed a bug where setting blur filter parameters would change transparency.\r\n10. Fixed a bug where textures were not released after releasing Spine.\r\n11. Fixed a bug where video preloading would automatically play sound.\r\n12. Fixed a bug where clicking on a video would pause it when Native default auto-play was enabled.\r\n13. Fixed a bug where the position of Native text was offset and had jagged edges compared to the webpage.\r\n14. Fixed a bug where bold text was ineffective in Native.\r\n15. Fixed a bug where custom fonts were ineffective in Alipay mini games.\r\n\r\n#### IDE\r\n\r\n1. Fixed an issue with uncertain file order in fileconfig during multiple releases.\r\n2. Fixed an issue where the scroll bar sometimes did not appear when the hierarchy panel had a large number of nodes.\r\n3. Fixed an issue where components could not be batch deleted in the IDE.\r\n4. Fixed an issue where an error occurred when clicking the close button while the project was still loading, preventing the IDE from closing.\r\n5. Fixed a bug in the hierarchy panel where right-clicking on search results caused an error.\r\n6. Fixed a bug where the resource panel in the IDE could not search for folders.\r\n7. Fixed a bug where BC mode texture compression resulted in darker textures with green edges.\r\n8. Fixed a bug where renaming a prefab in the IDE made it unsearchable in the search box.\r\n9. Fixed a bug where 2D nodes could not be resized correctly in the IDE scene editor after rotation.\r\n10. Fixed a bug where setting multiple nodes as static only affected the first node's child nodes.\r\n11. Fixed an issue where the wireframe was obscured by the model when editing a physical bounding box in orthographic view.\r\n\r\n### Important New Features\r\n\r\n1. Added support for the Taobao mini-game engine plugin.\r\n2. Added thumbnails for materials and models in the IDE.\r\n3. Added HTTPS support for IDE previews.\r\n\r\n### Optimization Updates\r\n\r\n1. Optimized the display of QR codes in IDE previews.\r\n2. Added support for expanding and collapsing tree nodes using the left and right keyboard keys in the IDE.\r\n3. Added the ability to input specific values for particle curves in the IDE.\r\n4. Added a control interface for ScrollBar.easeFunction in the Panel component, allowing developers to take over the scrolling effect.\r\n5. Added getLinearVelocity and getAngularVelocity interfaces to Rigidbody3D to obtain the current linear velocity of objects in the physical world.\r\n6. Optimized the default texture compression format options to match the default display values on the PC platform, avoiding developer confusion.\r\n7. Removed the deprecated overrideGravity parameter for 3D rigid bodies in the IDE to avoid developer confusion.\r\n8. Enabled animation compression by default when importing FBX animations into the IDE.\r\n9. Optimized the item recycling and destruction of excess items in the List component.\r\n10. Optimized the distance and normal accessors of Plane.\r\n11. Optimized the efficiency of SingletonList.add in clipping and rendering queues.\r\n12. Optimized the efficiency of power-of-2 calculations for Camera.Layer.\r\n13. Optimized handling and error messages for circular nesting of prefabs.\r\n14. Optimized the efficiency of distance calculations during object sorting.\r\n15. Prohibited the use of ""js"" as a name for subpackage directories when building and releasing subpackages.",v3.1.4,Charley,,LayaCharley,MIT License,LayaAir,layabox,24,webgl,3d-engine,2d-game-engine,webgl2,html,javascript,typescript,physics,minigames,webgpu,html5,,,,,,,,,,/layabox/LayaAir,24,87,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/lambdaclass/lambdaworks,https://github.com/lambdaclass/lambdaworks,0,,,0,1,0,0,0,0,1,0,0,0,0,"lambdaworks offers implementations for both SNARKs and STARKs provers, along with the flexibility to leverage their individual components for constructing customized SNARKs.","# lambdaworks\n> From the heights of these towers of fields, forty centuries of mathematics look down on us. \n\nThis library provides efficient implementation of cryptographic primitives used to build proving systems. Along with it, many backends for proving systems are shipped, and compatibility with different frontends is supported.\n\n- [Transforming the Future with Zero-Knowledge Proofs, Fully Homomorphic Encryption and new Distributed Systems algorithms](https://blog.lambdaclass.com/transforming-the-future-with-zero-knowledge-proofs-fully-homomorphic-encryption-and-new-distributed-systems-algorithms/)\n- [Lambda Crypto Doctrine](https://blog.lambdaclass.com/lambda-crypto-doctrine/)\n\n[![Telegram Chat][tg-badge]][tg-url]\n[![codecov](https://img.shields.io/codecov/c/github/lambdaclass/lambdaworks)](https://codecov.io/gh/lambdaclass/lambdaworks)\n\n[tg-badge]: https://img.shields.io/endpoint?url=https%3A%2F%2Ftg.sumanjay.workers.dev%2Flambdaworks%2F&logo=telegram&label=chat&color=neon\n[tg-url]: https://t.me/lambdaworks\n\n</div>\n\n## Why we built lambdaworks\n\nZero-Knowledge and Validity Proofs have gained a lot of attention over the last few years. We strongly believe in this potential and that is why we decided to start working in this challenging ecosystem, where math, cryptography and distributed systems meet. The main barrier in the beginning was not the cryptography or math but the lack of good libraries which are performant and developer friendly. There are some exceptions, though, like gnark or halo2. Some have nice APIs and are easy to work with, but they are not written in Rust, and some are written in Rust but have poor programming and engineering practices. Most of them don't have support for CUDA, Metal and WebGPU or distributed FFT calculation using schedulers like Dask.\n\nSo, we decided to build our library, focusing on performance, with clear documentation and developer-focused. Our core team is a group of passionate people from different backgrounds and different strengths; we think that the whole is greater than just the addition of the parts. We don't want to be a compilation of every research result in the ZK space. We want this to be a library that can be used in production, not just in academic research. We want to offer developers the main building blocks and proof systems so that they can build their applications on top of this library.\n\n## [Documentation](https://lambdaclass.github.io/lambdaworks)\n\n## Main crates\n\n- [Math](https://github.com/lambdaclass/lambdaworks/tree/main/math)\n- [Crypto primitives](https://github.com/lambdaclass/lambdaworks/tree/main/crypto)\n- [STARK Prover](https://github.com/lambdaclass/lambdaworks/tree/main/provers/stark)\n- [Plonk Prover](https://github.com/lambdaclass/lambdaworks/tree/main/provers/plonk)\n- [Cairo Prover](https://github.com/lambdaclass/lambdaworks/tree/main/provers/cairo)\n- [Groth 16](https://github.com/lambdaclass/lambdaworks/tree/main/provers/groth16)\n\n### Crypto\n- [Elliptic curves](https://github.com/lambdaclass/lambdaworks/tree/main/math/src/elliptic_curve)\n- [Multiscalar multiplication](https://github.com/lambdaclass/lambdaworks/tree/main/math/src/msm)\n- [Hashes](https://github.com/lambdaclass/lambdaworks/tree/main/crypto/src/hash)\n\nMost of math and crypto crates supports no-std without allocation with `no-default-features`. A few functions and modules require the `alloc` feature.\n\nBoth Math and Crypto support wasm with target `wasm32-unknown-unknown`. To see an example of how to use this to deploy a verifier in a browser, check the Cairo Prover wasm-pack verifier.\n\n## Examples - mini apps\n- [Merkle Tree CLI](https://github.com/lambdaclass/lambdaworks/tree/main/examples/merkle-tree-cli)\n- [Proving Miden](https://github.com/lambdaclass/lambdaworks/tree/main/examples/prove-miden)\n- [Shamir's secret sharing](https://github.com/lambdaclass/lambdaworks/tree/main/examples/shamir_secret_sharing)\n- [BabySNARK](https://github.com/lambdaclass/lambdaworks/tree/main/examples/baby-snark)\n\n## Exercises and Challenges\n- [lambdaworks exercises and challenges](https://github.com/lambdaclass/lambdaworks_exercises/tree/main)\n- [Roadmap for Sparkling Water Bootcamp](https://github.com/lambdaclass/sparkling_water_bootcamp/blob/main/README.md)\n\n## Citing lambdaworks\n\nIf you use ```lambdaworks``` libraries in your research projects, please cite them using the following template:\n\n``` bibtex\n@software{lambdaworks,\n  author={lambdaworks contributors},\n  title={lambdaworks},\n  url={https://github.com/lambdaclass/lambdaworks},\n  year={2023}\n}\n```\n\n## List of features\n\nDisclaimer: This list contains cryptographic primitives and mathematical structures that we want to support in lambdaworks. It can be expanded later to include new primitives. If you find there is a mistake or there has been an update in another library, please let us know.\n\nList of symbols:\n- :heavy_check_mark: means the feature is currently supported.\n- 🏗️ means that the feature is partially implemented or is under active construction.\n- :x: means that the feature is not currently supported.\n\n| Finite Fields  | Lambdaworks        | Arkworks           | Halo2    | gnark              | Constantine |\n| -------------- | ------------------ | ------------------ | -------- | ------------------ | ----------- |\n| StarkField 252 | :heavy_check_mark: | :heavy_check_mark: | :x:      | :heavy_check_mark: | :x:         |\n| Mersenne 31    | :heavy_check_mark: | :x:                | :x:      | :x:                | :x:         |\n| Baby Bear      | :heavy_check_mark: | :x:                | :x:      | :x:                | :x:         |\n| MiniGoldilocks | :heavy_check_mark: | :x:                | :x:      | :heavy_check_mark: | :x:         |\n| **ZK friendly Hash function** | **Lambdaworks** | **Arkworks**       | **Halo2**          | **gnark** | **Constantine** |\n| Poseidon                      | 🏗️              | :heavy_check_mark: | :heavy_check_mark: | :x:       | :x:             |\n| Pedersen                      | 🏗️              | :heavy_check_mark: | :heavy_check_mark: | :x:       | :x:             |\n| Rescue Prime XLIX             | :x:             | :x:                | :x:                | :x:       | :x:             |\n| **Elliptic Curves** | **Lambdaworks** | **Arkworks**          | **Halo2**          | **gnark**          | **Constantine**    |\n| BLS12-381           | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |\n| BLS12-377           | 🏗️                 | :heavy_check_mark: | :x:                | :heavy_check_mark: | :heavy_check_mark: |\n| BN-254              | 🏗️                 | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |\n| Pallas              | :heavy_check_mark: | :heavy_check_mark: | :x:                | :x:                | :heavy_check_mark: |\n| Vesta               | :heavy_check_mark: | :heavy_check_mark: | :x:                | :x:                | :heavy_check_mark: |\n| Bandersnatch        | 🏗️                 | :heavy_check_mark: | :x:                | :heavy_check_mark:  | :heavy_check_mark: |\n| **STARKs**       | **Lambdaworks**     | **Arkworks** | **Halo2** | **gnark** | **Constantine** |\n| STARK Prover     | :heavy_check_mark:  | :x:          | :x:       | :x:       | :x:             |\n| CAIRO Prover     | 🏗️                  | :x:          | :x:       | :x:       | :x:             |\n| **SNARKs** | **Lambdaworks**    | **Arkworks**       | **Halo2** | **gnark**          | **Constantine** |\n| Groth16    | :heavy_check_mark: | :heavy_check_mark: | :x:       | :heavy_check_mark: | :x:             |\n| Plonk      | 🏗️                 | :heavy_check_mark: | ✔️         | :heavy_check_mark: | :x:             |\n| Spartan    | :x:                | :heavy_check_mark: | :x:       | :x:                | :x:             |\n| Marlin     | :x:                | :heavy_check_mark: | :x:       | :x:                | :x:             |\n| GKR        | :x:                | :heavy_check_mark: | :x:       | :heavy_check_mark: | :x:             |\n| **Polynomial Commitment Schemes** | **Lambdaworks**    | **Arkworks**       | **Halo2**          | **gnark**          | **Constantine** |\n| KZG10                             | :heavy_check_mark: | ✔️                  | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark:             |\n| FRI                               | 🏗️                 | :x:                | :x:                | :heavy_check_mark: | :x:             |\n| IPA                               | 🏗️                 | ✔️                  | :heavy_check_mark: | :x:                | :x:             |\n| Brakedown                         | :x:                | :x: | :x:                | :x:                | :x:             |\n| Basefold                          | :x:                | :x: | :x:                | :x:                | :x:             |\n| **Folding Schemes** | **Lambdaworks** | **Arkworks**       | **Halo2** | **gnark** | **Constantine** |\n| Nova                | :x:             | :heavy_check_mark: | :x:       | :x:       | :x:             |\n| Supernova           | :x:             | :x:                | :x:       | :x:       | :x:             |\n| Protostar           | :x:             | :x:                | :x:       | :x:       | :x:             |\n| Protogalaxy         | :x:             | :heavy_check_mark: | :x:       | :x:       | :x:             |\n\nAdditionally, provers are compatible with the following frontends and VMs:\n\n| Backend | Frontend | Status |\n|---------|----------|--------|\n| Groth16 | Arkworks | :heavy_check_mark: |\n| Groth16 | Gnark    | :x: |\n| Groth16 | Circom   | 🏗️  |\n| Plonk   | Gnark    | 🏗️  |\n| Plonk   | Noir    | :x: |\n| Stark   | Winterfell | :heavy_check_mark: | \n| Stark   | Miden | :heavy_check_mark: |\n| Stark   | Cairo | :heavy_check_mark: |\n\nThis can be used in a multi prover setting for extra security, or as a standalone to be used with Rust. \n\n## Additional tooling usage\n\n### Fuzzers\n\nFuzzers are divided between the ones that use only the CPU, the ones that use Metal, and the ones that use CUDA.\n\nCPU Fuzzers can be run with the command ```bash make run-fuzzer FUZZER=fuzzer_name```\n\nFor example:\n\n```bash\nmake run-fuzzer FUZZER=field_from_hex\n```\n\nThe list of fuzzers can be found in `fuzz/no_gpu_fuzz`\n\nFuzzers for FTT in Metal and Cuda can be run with `make run-metal-fuzzer` and `make run-cuda-fuzzer`\n\n\nRun a specific fuzzer from the ones contained in **fuzz/fuzz_targets/** folder with`cargo`, for example to run the one for the target `field_from_hex`:\n\n```bash\nmake run-fuzzer FUZZER=field_from_hex\n```\n\n### Documentation building\n\nTo serve the documentation locally, first install both [mdbook](https://rust-lang.github.io/mdBook/guide/installation.html) and the [Katex preprocessor](https://github.com/lzanini/mdbook-katex#getting-started) to render LaTeX, then run\n\n``` shell\nmake docs\n```\n\n## 📚 References\n\nThe following links, repos and projects have been important in the development of this library and we want to thank and acknowledge them. \n\n- [Starkware](https://starkware.co/)\n- [Winterfell](https://github.com/facebook/winterfell)\n- [Anatomy of a Stark](https://aszepieniec.github.io/stark-anatomy/overview)\n- [Giza](https://github.com/maxgillett/giza)\n- [Ministark](https://github.com/andrewmilson/ministark)\n- [Sandstorm](https://github.com/andrewmilson/sandstorm)\n- [STARK-101](https://starkware.co/stark-101/)\n- [starknet-rs](https://github.com/xJonathanLEI/starknet-rs/)\n- [Risc0](https://github.com/risc0/risc0)\n- [Neptune](https://github.com/Neptune-Crypto)\n- [Summary on FRI low degree test](https://eprint.iacr.org/2022/1216)\n- [STARKs paper](https://eprint.iacr.org/2018/046)\n- [DEEP FRI](https://eprint.iacr.org/2019/336)\n- [BrainSTARK](https://aszepieniec.github.io/stark-brainfuck/)\n- [Plonky2](https://github.com/mir-protocol/plonky2)\n- [Aztec](https://github.com/AztecProtocol)\n- [Arkworks](https://github.com/arkworks-rs)\n- [Thank goodness it's FRIday](https://vitalik.ca/general/2017/11/22/starks_part_2.html)\n- [Diving DEEP FRI](https://blog.lambdaclass.com/diving-deep-fri/)\n- [Periodic constraints](https://blog.lambdaclass.com/periodic-constraints-and-recursion-in-zk-starks/)\n- [Chiplets Miden VM](https://wiki.polygon.technology/docs/miden/design/chiplets/main/)\n- [Valida](https://github.com/valida-xyz/valida/tree/main)\n- [Solidity Verifier](https://github.com/starkware-libs/starkex-contracts/tree/master/evm-verifier/solidity/contracts/cpu)\n- [CAIRO verifier](https://github.com/starkware-libs/cairo-lang/tree/master/src/starkware/cairo/stark_verifier)\n- [EthSTARK](https://github.com/starkware-libs/ethSTARK/tree/master)\n- [CAIRO whitepaper](https://eprint.iacr.org/2021/1063.pdf)\n- [Gnark](https://github.com/Consensys/gnark)\n- [Constantine](https://github.com/mratsim/constantine)\n",583,mathematics,Rust,8,Dockerfile,Makefile,Nix,Rust,Metal,Cuda,C,Cairo,,,,,,,,,,,,,,,,,,,,,652,154,464,34,113,74,663,24897,125,230,165,65,7ffcf7c07530c53ce9b24270b0f062656b8f16c8,Release v0.8.0: Lecker Lokum (#881),2024-07-19T14:21:00Z,Mario Rugiero,mrugiero@gmail.com,Oppen,v0.8.0: Lecker Lokum,## What's Changed\r\n* Handle single element cases in MerkleTree and update tests by @jotabulacios in https://github.com/lambdaclass/lambdaworks/pull/871\r\n* feat(merkle): Pad single tree leaves to next power of two by @PatStiles in https://github.com/lambdaclass/lambdaworks/pull/876\r\n* Add error to UnsInt from_hex when hex too big by @MauroToscano in https://github.com/lambdaclass/lambdaworks/pull/880\r\n* chore(dep): Pin starknet-rs dependencies by @PatStiles in https://github.com/lambdaclass/lambdaworks/pull/879\r\n* fix: unaligned read for Metal buffers by @Oppen in https://github.com/lambdaclass/lambdaworks/pull/882\r\n\r\n## New Contributors\r\n* @jotabulacios made their first contribution in https://github.com/lambdaclass/lambdaworks/pull/871\r\n\r\n**Full Changelog**: https://github.com/lambdaclass/lambdaworks/compare/v0.7.0...v0.8.0,v0.8.0,Mario Rugiero,,Oppen,Apache License 2.0,lambdaworks,lambdaclass,11,cryptography,mathematics,zero-knowledge-proofs,,,,,,,,,,,,,,,,,,/lambdaclass/lambdaworks,12,11,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/labgem/PPanGGOLiN,https://github.com/labgem/PPanGGOLiN,1,,,1,1,1,1,0,0,0,0,0,0,1,Build a partitioned pangenome graph from microbial genomes,"# PPanGGOLiN: Depicting microbial species diversity via a Partitioned PanGenome Graph Of Linked Neighbors\n\n[![Actions](https://img.shields.io/github/actions/workflow/status/althonos/pyrodigal/test.yml?branch=main&logo=github&style=flat-square&maxAge=300)](https://github.com/labgem/ppanggolin/actions)\n[![License](https://anaconda.org/bioconda/ppanggolin/badges/license.svg)](http://www.cecill.info/licences.fr.html)\n[![Bioconda](https://img.shields.io/conda/vn/bioconda/ppanggolin?style=flat-square&maxAge=3600&logo=anaconda)](https://anaconda.org/bioconda/ppanggolin)\n[![Source](https://img.shields.io/badge/source-GitHub-303030.svg?maxAge=2678400&style=flat-square)](https://github.com/labgem/ppanggolin/)\n[![GitHub issues](https://img.shields.io/github/issues/labgem/ppanggolin.svg?style=flat-square&maxAge=600)](https://github.com/labgem/ppanggolin/issues)\n[![Docs](https://img.shields.io/readthedocs/ppanggolin/latest?style=flat-square&maxAge=600)](https://ppanggolin.readthedocs.io)\n[![Downloads](https://anaconda.org/bioconda/ppanggolin/badges/downloads.svg)](https://bioconda.github.io/recipes/ppanggolin/README.html#download-stats)\n\n**PPanGGOLiN**\n([Gautreau et al. 2020](https://doi.org/10.1371/journal.pcbi.1007732)) is a software suite used to create and manipulate prokaryotic pangenomes from a set of either genomic DNA sequences or provided genome annotations.\nIt is designed to scale up to tens of thousands of genomes.\nIt has the specificity to partition the pangenome using a statistical approach rather than using fixed thresholds which gives it the ability to work with low-quality data such as *Metagenomic Assembled Genomes (MAGs)* or *Single-cell Amplified Genomes (SAGs)* thus taking advantage of large scale environmental studies and letting users study the pangenome of uncultivable species.\n\n**PPanGGOLiN** builds pangenomes through a graphical model and a statistical method to partition gene families in persistent, shell and cloud genomes.\nIt integrates both information on the presence/absence of protein-coding genes and their genomic neighborhood to build a graph of gene families where each node is a gene family, and each edge is a relation of genetic contiguity.\nThe partitioning method promotes that two gene families that are consistent neighbors in the graph are more likely to belong to the same partition.\nIt results in a Partitioned Pangenome Graph (PPG) made of persistent, shell and cloud nodes drawing genomes on rails like a subway map to help biologists navigate the great diversity of microbial life.\n\n\nMoreover, the panRGP method ([Bazin et al. 2020](https://doi.org/10.1093/bioinformatics/btaa792)) included in **PPanGGOLiN** predicts, for each genome, Regions of Genome Plasticity (RGPs) that are clusters of genes made of shell and cloud genomes in the pangenome graph.\nMost of them arise from Horizontal gene transfer (HGT) and correspond to Genomic Islands (GIs). \nRGPs from different genomes are next grouped in spots of insertion based on their conserved flanking persistent genes.\n\n\nThose RGPs can be further divided in conserved modules by panModule ([Bazin et al. 2021](https://doi.org/10.1101/2021.12.06.471380)). Those conserved modules correspond to groups of cooccurring and colocalized genes that are gained or lost together in the variable regions of the pangenome.\n\nA complete documentation is available [here](https://ppanggolin.readthedocs.io).\n\n<!-- ![PPanGGOLiN logo](docs/_static/logo.png) -->\n\n<!-- center the image with html syntax -->\n<p align=""center"">\n  <img src=""docs/_static/logo.png"" alt=""logo"">\n</p>\n\n# Installation\n\n**PPanGGOLiN** can be is easily installed via conda, accessible through the bioconda channel.\n\nTo ensure a smoother installation and avoid conflicting dependencies, it's highly recommended to create a dedicated environment for PPanGGOLiN:\n\n```bash\n# Install PPanGGOLiN into a new conda environment\nconda create -n ppanggolin -c defaults -c conda-forge -c bioconda ppanggolin\n\n# Check PPanGGOLiN install\nconda activate ppanggolin\nppanggolin --version\n```\n\n# Quick usage\n\n## Run a complete pangenome analysis\n\nA complete pangenomic analysis with PPanGGOLiN can be performed using the [`all`](https://ppanggolin.readthedocs.io/en/latest/user/QuickUsage/quickAnalyses.html#ppanggolin-complete-workflow-analyses) subcommand. This workflow runs a series of PPanGGOLiN commands to generate a **partitioned pangenome graph** with predicted **RGPs** (Regions of Genomic Plasticity), **spots** of insertion and **modules**.\n\n\nExecute the following command to run the `all` workflow:\n\n```bash\nppanggolin all --fasta GENOMES_FASTA_LIST\n```\n\nBy default, it uses parameters that we have found to be generally the best for working with species pangenomes. For further customization, you can adjust some parameters directly on the command line. Alternatively, you can use a configuration file to fine-tune the parameters of each subcommand used by the workflow (see [here](https://ppanggolin.readthedocs.io/en/latest/user/practicalInformation.html#configuration-file) for more details).\n\n### Input files\n\nThe file `GENOMES_FASTA_LIST` is a tsv-separated file with the following organization :\n\n1. The first column contains a unique genome name **(without space)**\n2. The second column contains the path to the associated FASTA file\n3. Circular contig identifiers are indicated in the following columns\n4. Each line represents a genome\n\nAn [example](testingDataset/genomes.fasta.list) with 50 *Chlamydia trachomatis* genomes can be found in the [testingDataset/](testingDataset/) directory.\n\n\nYou can also give **PPanGGOLiN** your own annotations using *.gff* or *.gbff/.gbk* files instead of *.fasta* files,\nsuch as the ones provided by [Bakta](https://github.com/oschwengers/bakta) with the following command :\n\n```bash\nppanggolin all --anno GENOMES_ANNOTATION_LIST\n```\n\nAnother [example](testingDataset/genomes.gbff.list) of such a file can be found in the [testingDataset/](testingDataset/) directory.\n\n\nA minimum of 5 genomes is generally required to perform a pangenomic analysis using the traditional *core genome*/*accessory genome* paradigm.\nIt is recommended to use at least 15 genomes with genomic variation (and not only SNPs) to obtain robust results with the **PPanGGOLiN** statistical approach.\n\n### Results files\n\nUpon executing the `all` command, multiple output files and graphics are generated  (more information [here](https://ppanggolin.readthedocs.io/en/latest/user/QuickUsage/quickAnalyses.html#usual-pangenome-outputs)). Most notably, it writes an HDF-5 file (`pangenome.h5`).\nThis file can be used as input to any of the subcommands to rerun parts of the analysis with different parameters,\nwrite and draw different representations of the pangenome, or perform additional analyses with **PPanGGOLiN**.\n\n\n## Other Workflow Commands\n\nPPanGGOLiN offers additional workflow commands that perform more specialized functions:\n\n- [**`workflow`**](https://ppanggolin.readthedocs.io/en/latest/user/PangenomeAnalyses/pangenomeAnalyses.html#workflow): Generates a partitioned pangenome graph.\n- [**`panrgp`**](https://ppanggolin.readthedocs.io/en/latest/user/RGP/rgpAnalyses.html#panrgp): Combine the `workflow` command and the prediction of RGPs (Regions of Genomic Plasticity) and insertion spots on top of the partitioned pangenome graph.\n- [**`panmodule`**](https://ppanggolin.readthedocs.io/en/latest/user/Modules/moduleAnalyses.html#the-panmodule-workflow): Combine the `workflow` command and the prediction of Modules on top of the partitioned pangenome graph.\n\nThese commands utilize the same type of file input as the `all` command.\n\n\n# Issues, Questions, Remarks\nIf you have any questions or issues with installing,\nusing or understanding **PPanGGOLiN**, please do not hesitate to post an issue!\nWe cannot correct bugs if we do not know about them, and will try to help you the best we can.\n\n# Citation\nIf you use this tool for your research, please cite:\n\nGautreau G et al. (2020) **PPanGGOLiN**: Depicting microbial diversity via a partitioned pangenome graph.\nPLOS Computational Biology 16(3): e1007732. <https://doi.org/10.1371/journal.pcbi.1007732>\n\nIf you use this tool to study genomic islands, please cite:\n\nBazin et al., panRGP: a pangenome-based method to predict genomic islands and explore their diversity, Bioinformatics, Volume 36, Issue Supplement_2, December 2020, Pages i651–i658, <https://doi.org/10.1093/bioinformatics/btaa792>\n\nIf you use this tool to study modules, please cite:\n\nBazin et al., panModule: detecting conserved modules in the variable regions of a pangenome graph. biorxiv. <https://doi.org/10.1101/2021.12.06.471380>\n",229,bioinformatics,Python,3,C,Python,Cython,,,,,,,,,,,,,,,,,,,,,,,,,,124,9,115,0,10,13,0,139523,25,123,116,7,d49dd5d5b808f822d54047928097d842423f0b72,Merge pull request #250 from labgem/dev,2024-07-10T14:23:04Z,Jean Mainguy,jean.mainguy@outlook.fr,JeanMainguy,PPanGGOLiN  2.1.0,"## New Features\r\n\r\n- Write the translated sequence of genes using MMSeqs2 with the `--proteins` option ([documentation](https://ppanggolin.readthedocs.io/en/latest/user/writeFasta.html#proteins)), which works like the other options in the ppanggolin fasta command (added in PR #205).\r\n- Some information about contigs and genomes, such as organism name, strain, and dbx_ref information, is now extracted from annotation files (GBFF & GFF) and added to the pangenome as metadata (added in PR #227).\r\n- The command `write_metadata` has been added to allow exporting metadata to TSV files. Check out the [documentation](https://ppanggolin.readthedocs.io/en/latest/user/metadata.html#exporting-metadata-to-tsv-files) for more details (added in PR #227).\r\n- Add `infer_singleton` option in the workflow (added in PR #239).\r\n- When clustering is given, it’s now possible to specify the representative gene of the cluster (added in PR #242).\r\n\r\n## Major Change\r\n\r\n- Handling genes with joined coordinates (for example, frameshift) in input annotation files (GFF or GBFF). Such annotations were disregarded when encountered in GBFF files and improperly managed in GFF files. This change implies a change in writing gene sequences and, consequently, in clustering and, thus, in all pangenome results: graph, partition, RGP, spots, and modules. This change was measured and reported in PR #206. It is not huge on pangenomes, but needs to be known for future version comparisons. See also PR #240 and #249.\r\n\r\n## Minor Changes\r\n\r\n- Ordering gene in the whole genome MSA file (added in PR #200).\r\n- Replace the return in the try block with an else statement to return the value found in try (added in PR #204).\r\n- When writing MSA, the partial gene is handled by removing the last one or two nucleotides to translate (added in PR #205).\r\n- Change how method `get_genes` handles end position (added in PR #212).\r\n- Improve GitHub CI workflow (added in PR #216, #220, #224, #225).\r\n- PPanGGOLiN now supports using the soft-link option when building the MMSeqs2 database via subprocess, reducing temporary directory size (added in PR #214 and #229).\r\n- Report subprocess (MMSeqs2, MAFFT, etc.) error message if it crashes (reported in issue #210, added in PR #229).\r\n- When parsing annotation files, CDS are translated using the translation table code specified by the `transl_table` tag. If this tag is missing, the `translation_table` argument is now used, with a default value of 11 (reported in #226 and added in PR #230).\r\n- Added an identifier to metadata in object and HDF5. This helps to identify the right metadata in a cross-reference (added in PR #235).\r\n- Make the subprocess more detailed with info and error messages (added in PR #237).\r\n- Add the protein sequence to the gene family when reading clustering (added in PR #238).\r\n- Add gene information in RGP output (added in PR #239).\r\n- Improve metadata management in commands `projection` and `rgp_cluster` (added in PR #244).\r\n- Some developments for the PANORAMA project 🤫 (added in PR #248).\r\n\r\n## Bug Fixes\r\n\r\n- Fix the last genome missing in the whole genome MSA file (fixed in PR #200).\r\n- Write only genes associated with the RGP when writing FASTA sequences for RGP (reported in issue #122, fixed in PR #202).\r\n- Ensure proper handling of circular RGPs, addressing issues observed in the spot plot (reported in issue #124, fixed in PR #206).\r\n- Fix gene ID mismatch in projection command with GBFF files as input genome (reported in issue #207, fixed in PR #208).\r\n- Fix spot prediction in projection command (fixed in PR #209).\r\n- Fix multiple spots per RGP handling in projection command (fixed in PR #211).\r\n- Handle trailing whitespace at the end of GBFF file (reported in issue #203, fixed in PR #213).\r\n- Correctly read ""is_circular"" from GFF files (fixed in PR #215).\r\n- Fix RGP ""looping"" around circular contigs (fixed in PR #215).\r\n- Write the gene name instead of the coordinates in RGP output files (reported in issue #218, fixed in PR #219).\r\n- Write only the genes of the input genome in `gene_to_gene_family.tsv` file from projection (reported in issue #221, fixed in PR #228).\r\n- Fix `dup_margin` default value (reported in issue #223 and fixed in PR #234).\r\n- Fix missing `translation_table` handling (reported in issue #226 and fixed in PR #230).\r\n- Fix spots to modules output file always empty (fixed in PR #236).\r\n- Manage chevron in GFF start and stop (fixed in PR #241).\r\n- Ignore weird tRNA from Aragorn (fixed in PR #245).\r\n- Fix display module on Proksee with gene overlapping contig (fixed in PR #246).\r\n- Fix metadata-related issues (fixed in PR #247).\r\n\r\n## New Contributor\r\n\r\nWe thank @ktmeaton, who made their first contribution in #200. 🎉\r\n\r\n## Other Contributors\r\n\r\n- @JeanMainguy \r\n- @jpjarnoux \r\n- @axbazin \r\n- @ggautreau\r\n- @dvallenet \r\n- @acalteau ",02.01.2000,Jérôme Arnoux,,jpjarnoux,Other,PPanGGOLiN,labgem,25,comparative-genomics,bioinformatics,microbial-genomics,microbiology,bacteria,pangenome,,,,,,,,,,,,,,,/labgem/PPanGGOLiN,28,13,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/kotlin-graphics/glm,https://github.com/kotlin-graphics/glm,0,,,0,1,0,0,0,0,1,0,0,0,0,jvm glm,"![glm](src/main/resources/logo-mini.png)\n\n[![Build Status](https://github.com/kotlin-graphics/glm/workflows/build/badge.svg)](https://github.com/kotlin-graphics/glm/actions?workflow=build)\n[![license](https://img.shields.io/badge/License-MIT-orange.svg)](https://github.com/kotlin-graphics/glm/blob/master/LICENSE) \n[![Release](https://jitpack.io/v/kotlin-graphics/glm.svg)](https://jitpack.io/#kotlin-graphics/glm) \n![Size](https://github-size-badge.herokuapp.com/kotlin-graphics/glm.svg)\n[![Github All Releases](https://img.shields.io/github/downloads/kotlin-graphics/glm/total.svg)]()\n[![Awesome Kotlin Badge](https://kotlin.link/awesome-kotlin.svg)](https://github.com/KotlinBy/awesome-kotlin)\n\nThis is the Kotlin port of [OpenGL Mathematics](http://glm.g-truc.net/) (*GLM*), written by [g-truc](https://github.com/Groovounet) ([repository](https://github.com/g-truc/glm)), a header only C++ mathematics library for graphics software based on the [OpenGL Shading Language (GLSL) specifications](https://www.opengl.org/registry/doc/GLSLangSpec.4.50.diff.pdf).\n\n*GLM* provides classes and functions designed and implemented with the same naming conventions and functionality than *GLSL* so that anyone who knows *GLSL*, can use *GLM* as well in Kotlin and Java.\n\nThis project isn't limited to *GLSL* features. An extension system, based on the *GLSL* extension conventions, provides extended capabilities: matrix transformations, quaternions, data packing, random numbers, noise, etc...\n\nThis library works perfectly with *[OpenGL](https://www.opengl.org)* but it also ensures interoperability with other third party libraries and SDK. It is a good candidate for software rendering (raytracing / rasterisation), image processing, physic simulations and any development context that requires a simple and convenient mathematics library.\n\n*GLM* is written entirely in Kotlin, but can be also used from Java. It is a platform independent library with no dependences other than [kotlin-unsigned](https://github.com/elect86/kotlin-unsigned) for unsigned support and [kotlin-test](https://github.com/kotlintest/kotlintest) for testing.\n\nFor more information about *GLM*, please have a look at the [manual](https://github.com/kotlin-graphics/glm/wiki) and the original [API reference documentation](http://glm.g-truc.net/0.9.8/api/index.html).\nThe source code and the documentation are licensed under both the [Happy Bunny License (Modified MIT) or the MIT License](https://github.com/kotlin-graphics/glm/wiki/Manual#section0).\n\nDon't hesitate to contribute to the project by submitting [issues](https://github.com/kotlin-graphics/glm/issues) or [pull requests](https://github.com/kotlin-graphics/glm/pulls) for bugs and features. Any feedback is welcome at [elect86@gmail.com](mailto://elect86@gmail.com).\n\n```kotlin\nimport glm_.vec2.Vec2\nimport glm_.vec3.Vec3\nimport glm_.mat4x4.Mat4\nimport glm_.glm\n\nfun camera(translate: Float, rotate: Vec2): Mat4 {\n    val projection = glm.perspective(glm.PIf * 0.25f, 4.0f / 3.0f, 0.1f, 100.0f)\n    var view = glm.translate(Mat4(1.0f), Vec3(0.0f, 0.0f, -translate))\n    view = glm.rotate(view, rotate.y, Vec3(-1.0f, 0.0f, 0.0f))\n    view = glm.rotate(view, rotate.x, Vec3(0.0f, 1.0f, 0.0f))\n    val model = glm.scale(Mat4(1.0f), Vec3(0.5f))\n    return projection * view * model\n}\n```\n\n### How to retrieve it:\n\n#### mary\n```kotlin\nrepositories {\n    maven(""https://raw.githubusercontent.com/kotlin-graphics/mary/master"")\n    // or with magik plugin\n    //github(""kotlin-graphics/mary"")\n}\ndependencies {\n    implementation(""kotlin.graphics:glm:0.9.9.1-11"")\n}\n```\n\n#### maven central\n```kotlin\ndependencies {\n    implementation(""io.github.kotlin-graphics:glm:0.9.9.1-12"")\n}\n```\n\nYou can find more info by [mary](https://github.com/kotlin-graphics/mary)\n",122,graphics,Kotlin,2,Kotlin,Java,,,,,,,,,,,,,,,,,,,,,,,,,,,17,1,16,0,10,12,0,3283,20,23,18,5,5ac4fbaff9ad253ba918e22d65d7190802869271,Merge pull request #44 from kotlin-graphics/vec2i-times-float,2024-05-17T12:54:12Z,Giuseppe Barbieri,elect86@gmail.com,elect86,0.9.9.1 build 11,missing `operator` modifier on vars interface,v0.9.9.1-build-11,Giuseppe Barbieri,,elect86,,glm,kotlin-graphics,38,kotlin,java,graphics,mathematics,glm,jogl,lwjgl,geometry-library,matrices,vectors,android,math-library,,,,,,,,,/kotlin-graphics/glm,42,12,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/konvajs/konva,https://github.com/konvajs/konva,0,,,0,0,0,0,0,0,1,1,0,0,0,Konva.js is an HTML5 Canvas JavaScript framework that extends the 2d context by enabling canvas interactivity for desktop and mobile applications.,"<p align=""center"">\n  <img src=""https://konvajs.org/android-chrome-192x192.png"" alt=""Konva logo"" height=""180"" />\n</p>\n\n<h1 align=""center"">Konva</h1>\n\n[![Financial Contributors on Open Collective](https://opencollective.com/konva/all/badge.svg?label=financial+contributors)](https://opencollective.com/konva)\n[![npm version](https://badge.fury.io/js/konva.svg)](http://badge.fury.io/js/konva)\n[![Build Status](https://github.com/konvajs/konva/actions/workflows/test-browser.yml/badge.svg)](https://github.com/konvajs/konva/actions/workflows/test-browser.ym)\n[![Build Status](https://github.com/konvajs/konva/actions/workflows/test-node.yml/badge.svg)](https://github.com/konvajs/konva/actions/workflows/test-node.ym)[![CDNJS version](https://img.shields.io/cdnjs/v/konva.svg)](https://cdnjs.com/libraries/konva)\n\nKonva is an HTML5 Canvas JavaScript framework that enables high performance animations, transitions, node nesting, layering, filtering, caching, event handling for desktop and mobile applications, and much more.\n\nYou can draw things onto the stage, add event listeners to them, move them, scale them, and rotate them independently from other shapes to support high performance animations, even if your application uses thousands of shapes. Served hot with a side of awesomeness.\n\nThis repository began as a GitHub fork of [ericdrowell/KineticJS](https://github.com/ericdrowell/KineticJS).\n\n- **Visit:** The [Home Page](http://konvajs.org/) and follow on [Twitter](https://twitter.com/lavrton)\n- **Discover:** [Tutorials](http://konvajs.org/docs), [API Documentation](http://konvajs.org/api)\n- **Help:** [StackOverflow](http://stackoverflow.com/questions/tagged/konvajs), [Discord Chat](https://discord.gg/8FqZwVT)\n\n# Quick Look\n\n```html\n<script src=""https://unpkg.com/konva@9/konva.min.js""></script>\n<div id=""container""></div>\n<script>\n  var stage = new Konva.Stage({\n    container: 'container',\n    width: window.innerWidth,\n    height: window.innerHeight,\n  });\n\n  // add canvas element\n  var layer = new Konva.Layer();\n  stage.add(layer);\n\n  // create shape\n  var box = new Konva.Rect({\n    x: 50,\n    y: 50,\n    width: 100,\n    height: 50,\n    fill: '#00D2FF',\n    stroke: 'black',\n    strokeWidth: 4,\n    draggable: true,\n  });\n  layer.add(box);\n\n  // add cursor styling\n  box.on('mouseover', function () {\n    document.body.style.cursor = 'pointer';\n  });\n  box.on('mouseout', function () {\n    document.body.style.cursor = 'default';\n  });\n</script>\n```\n\n# Browsers support\n\nKonva works in all modern mobile and desktop browsers. A browser need to be capable to run javascript code from ES2015 spec. For older browsers you may need polyfills for missing functions.\n\nAt the current moment `Konva` doesn't work in IE11 directly. To make it work you just need to provide some polyfills such as `Array.prototype.find`, `String.prototype.trimLeft`, `String.prototype.trimRight`, `Array.from`.\n\n# Debugging\n\nThe Chrome inspector simply shows the canvas element.  To see the Konva objects and their details, install the konva-dev extension at https://github.com/konvajs/konva-devtool.\n\n# Loading and installing Konva\n\nKonva supports UMD loading. So you can use all possible variants to load the framework into your project:\n\n### Load Konva via classical `<script>` tag from CDN:\n\n```html\n<script src=""https://unpkg.com/konva@9/konva.min.js""></script>\n```\n\n### Install with npm:\n\n```bash\nnpm install konva --save\n```\n\n```javascript\n// The modern way (e.g. an ES6-style import for webpack, parcel)\nimport Konva from 'konva';\n```\n\n#### Typescript usage\n\nAdd DOM definitions into your `tsconfig.json`:\n\n```\n{\n  ""compilerOptions"": {\n    ""lib"": [\n        ""es6"",\n        ""dom""\n    ]\n  }\n}\n```\n\n### 3 Minimal bundle\n\n```javascript\nimport Konva from 'konva/lib/Core';\n// Now you have a Konva object with Stage, Layer, FastLayer, Group, Shape and some additional utils function.\n// Also core currently already have support for drag&drop and animations.\n// BUT there are no shapes (rect, circle, etc), no filters.\n\n// but you can simply add anything you need:\nimport { Rect } from 'konva/lib/shapes/Rect';\n// importing a shape will automatically inject it into Konva object\n\nvar rect1 = new Rect();\n// or:\nvar shape = new Konva.Rect();\n\n// for filters you can use this:\nimport { Blur } from 'konva/lib/filters/Blur';\n```\n\n### 4 NodeJS env\n\nIn order to run `konva` in nodejs environment you also need to install `canvas` package manually. Konva will use it for 2d canvas API.\n\n```bash\nnpm install konva canvas\n```\n\nThen you can use the same Konva API and all Konva demos will work just fine. You just don't need to use `container` attribute in your stage.\n\n```js\nimport Konva from 'konva';\n\nconst stage = new Konva.Stage({\n  width: 500,\n  height: 500,\n});\n// then all regular Konva code will work\n```\n\n# Backers\n\n![https://simpleshow.com](https://avatars.githubusercontent.com/u/99720652?s=200&v=4 'https://simpleshow.com')\n![https://www.notably.ai/](https://avatars.githubusercontent.com/u/80046841?s=200&v=4 'https://www.notably.ai/')\n\n- [myposter GmbH](https://www.myposter.de/)\n- [queue.gg](https://queue.gg/)\n\n# Change log\n\nSee [CHANGELOG.md](https://github.com/konvajs/konva/blob/master/CHANGELOG.md).\n\n## Building the Konva Framework\n\nTo make a full build run `npm run build`. The command will compile all typescript files, combine then into one bundle and run minifier.\n\n## Testing\n\nKonva uses Mocha for testing.\n\n- If you need run test only one time run `npm run test`.\n- While developing it is easy to use `npm start`. Just run it and go to [http://localhost:1234/unit-tests.html](http://localhost:1234/unit-tests.html). The watcher will rebuild the bundle on any change.\n\nKonva is covered with hundreds of tests and well over a thousand assertions.\nKonva uses TDD (test driven development) which means that every new feature or bug fix is accompanied with at least one new test.\n\n## Generate documentation\n\nRun `npx gulp api` which will build the documentation files and place them in the `api` folder.\n\n# Pull Requests\n\nI'd be happy to review any pull requests that may better the Konva project,\nin particular if you have a bug fix, enhancement, or a new shape (see `src/shapes` for examples). Before doing so, please first make sure that all of the tests pass (`npm run test`).\n\n## Contributors\n\n### Financial Contributors\n\nBecome a financial contributor and help us sustain our community. [[Contribute](https://opencollective.com/konva/contribute)]\n\n#### Individuals\n\n<a href=""https://opencollective.com/konva""><img src=""https://opencollective.com/konva/individuals.svg?width=890""></a>\n\n#### Organizations\n\nSupport this project with your organization. Your logo will show up here with a link to your website. [[Contribute](https://opencollective.com/konva/contribute)]\n\n<a href=""https://opencollective.com/konva/organization/0/website""><img src=""https://opencollective.com/konva/organization/0/avatar.svg""></a>\n<a href=""https://opencollective.com/konva/organization/1/website""><img src=""https://opencollective.com/konva/organization/1/avatar.svg""></a>\n<a href=""https://opencollective.com/konva/organization/2/website""><img src=""https://opencollective.com/konva/organization/2/avatar.svg""></a>\n<a href=""https://opencollective.com/konva/organization/3/website""><img src=""https://opencollective.com/konva/organization/3/avatar.svg""></a>\n<a href=""https://opencollective.com/konva/organization/4/website""><img src=""https://opencollective.com/konva/organization/4/avatar.svg""></a>\n<a href=""https://opencollective.com/konva/organization/5/website""><img src=""https://opencollective.com/konva/organization/5/avatar.svg""></a>\n<a href=""https://opencollective.com/konva/organization/6/website""><img src=""https://opencollective.com/konva/organization/6/avatar.svg""></a>\n<a href=""https://opencollective.com/konva/organization/7/website""><img src=""https://opencollective.com/konva/organization/7/avatar.svg""></a>\n<a href=""https://opencollective.com/konva/organization/8/website""><img src=""https://opencollective.com/konva/organization/8/avatar.svg""></a>\n<a href=""https://opencollective.com/konva/organization/9/website""><img src=""https://opencollective.com/konva/organization/9/avatar.svg""></a>\n",11108,graphics,TypeScript,4,JavaScript,HTML,Shell,TypeScript,,,,,,,,,,,,,,,,,,,,,,,,,290,74,211,5,16,180,0,22122,902,1421,1388,33,9830a6c37c828b3a423f14f3b622481480386105,Merge pull request #1795 from kyechan99/master,2024-07-19T15:35:33Z,Anton Lavrenov,lavrton@gmail.com,lavrton,09.03.2014,"## Commits\n- ac1587f: update docs, fix fullRule for hit graph, close #1787 (Anton Lavrenov)\n- 3f6f9f9: fix shadow + corner radius for images, close #1734 (Anton Lavrenov)\n- 459606b: changes (Anton Lavrenov)\n- 72d92d8: update CHANGELOG with new version (Anton Lavrenov)\n- 331bd9b: build for 9.3.14 (Anton Lavrenov)\n- 7b99aa9: update cdn link (Anton Lavrenov)",09.03.2014,,,github-actions[bot],Other,konva,konvajs,18,konva,javascript,shape,node-canvas,canvas,animation,drag-and-drop,graphics,,,,,,,,,,,,,/konvajs/konva,205,170,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/KarchinLab/open-cravat,https://github.com/KarchinLab/open-cravat,0.5,Annotation tool?,0,0,1,0,0,0,0,1,0,0,0,0,A modular annotation tool for genomic variants,"Open-CRAVAT is a python package that performs genomic variant interpretation including variant impact, annotation, and scoring.  It has a modular architecture with a wide variety of analysis modules that are developed both by the CRAVAT team and the broader variant analysis community. Open-CRAVAT is a product of the [Karchin Lab](http://karchinlab.org/) at Johns Hopkins University with funding provided by the National Cancer Institute's [ITCR](https://itcr.cancer.gov/) program.\n\n# Links\n\nPlease see: [Open-CRAVAT Docs](https://open-cravat.readthedocs.io/en/latest/index.html) for installation instructions and software documentation.\n\nPlease report issues using the github issue tracker above.\n\nCheck out our landing page at [opencravat.org](https://opencravat.org)\n",110,bioinformatics,JavaScript,10,Python,CSS,JavaScript,HTML,Jupyter Notebook,Shell,Batchfile,Inno Setup,PowerShell,Dockerfile,,,,,,,,,,,,,,,,,,,34,3,30,1,55,16,11,9313,27,206,176,30,c9c9d858ce30aab6e5c5467b5d2981622d6106ca,version bump to 2.7.3,2024-06-18T15:10:46Z,Kyle Moad,kyle.moad@gmail.com,kmoad,02.07.2003,"- Import jobs run in the command line into the gui by clicking the ""Import Job"" button below the jobs table to add a sqlite file\r\n- Create a github discussion post to suggest new annotators directly from the store tab",02.07.2003,,,github-actions[bot],MIT License,open-cravat,KarchinLab,16,variant-annotations,variant-annotation,variant-analysis,genomics,genomic-data-analysis,bioinformatics,bioinformatics-pipeline,bioinformatics-tool,annotation-tool,python,python3,javascript,,,,,,,,,/KarchinLab/open-cravat,35,21,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/JunoLab/Weave.jl,https://github.com/JunoLab/Weave.jl,0,,,0,0,0,0,0,0,1,0,0,0,0,Scientific reports/literate programming for Julia,"# Weave\n\n![CI](https://github.com/JunoLab/Weave.jl/workflows/CI/badge.svg)\n[![codecov](https://codecov.io/gh/JunoLab/Weave.jl/branch/master/graph/badge.svg)](https://codecov.io/gh/JunoLab/Weave.jl)\n[![](https://img.shields.io/badge/docs-stable-blue.svg)](http://weavejl.mpastell.com/stable/)\n[![](https://img.shields.io/badge/docs-dev-blue.svg)](http://weavejl.mpastell.com/dev/)\n[![](http://joss.theoj.org/papers/10.21105/joss.00204/status.svg)](http://dx.doi.org/10.21105/joss.00204)\n\nWeave is a scientific report generator/literate programming tool for the [Julia programming language](https://julialang.org/).\nIt resembles\n[Pweave](http://mpastell.com/pweave),\n[knitr](https://yihui.org/knitr/),\n[R Markdown](https://rmarkdown.rstudio.com/),\nand [Sweave](https://stat.ethz.ch/R-manual/R-patched/library/utils/doc/Sweave.pdf).\n\nYou can write your documentation and code in input document using Markdown, Noweb or ordinal Julia script syntax,\nand then use `weave` function to execute code and generate an output document while capturing results and figures.\n\n**Current features**\n\n- Publish markdown directly to HTML and PDF using Julia or [Pandoc](https://pandoc.org/MANUAL.html)\n- Execute code as in terminal or in a unit of code chunk\n- Capture [Plots.jl](https://github.com/JuliaPlots/Plots.jl) or [Gadfly.jl](https://github.com/GiovineItalia/Gadfly.jl) figures\n- Supports various input format: Markdown, [Noweb](https://www.cs.tufts.edu/~nr/noweb/), [Jupyter Notebook](https://jupyter.org/), and ordinal Julia script\n- Conversions between those input formats\n- Supports various output document formats: HTML, PDF, GitHub markdown, Jupyter Notebook, MultiMarkdown, Asciidoc and reStructuredText\n- Simple caching of results\n\n**Citing Weave:** *Pastell, Matti. 2017. Weave.jl: Scientific Reports Using Julia. The Journal of Open Source Software. http://dx.doi.org/10.21105/joss.00204*\n\n![Weave in Juno demo](https://user-images.githubusercontent.com/40514306/76081328-32f41900-5fec-11ea-958a-375f77f642a2.png)\n\n\n## Installation\n\nYou can install the latest release using Julia package manager:\n\n```julia\nusing Pkg\nPkg.add(""Weave"")\n```\n\n\n## Usage\n\n```julia\nusing Weave\n\n# add depencies for the example\nusing Pkg; Pkg.add([""Plots"", ""DSP""])\n\nfilename = normpath(Weave.EXAMPLE_FOLDER, ""FIR_design.jmd"")\nweave(filename, out_path = :pwd)\n```\n\nIf you have LaTeX installed you can also weave directly to pdf.\n\n```julia\nfilename = normpath(Weave.EXAMPLE_FOLDER, ""FIR_design.jmd"")\nweave(filename, out_path = :pwd, doctype = ""md2pdf"")\n```\n\nNOTE: `Weave.EXAMPLE_FOLDER` just points to [`examples` directory](./examples).\n\n\n## Documentation\n\nDocumenter.jl with MKDocs generated documentation:\n\n[![](https://img.shields.io/badge/docs-stable-blue.svg)](http://weavejl.mpastell.com/stable/)\n[![](https://img.shields.io/badge/docs-dev-blue.svg)](http://weavejl.mpastell.com/dev/)\n\n\n## Editor support\n\nInstall [language-weave](https://atom.io/packages/language-weave) to add Weave support to Juno.\nIt allows running code from Weave documents with usual keybindings and allows preview of\nhtml and pdf output.\n\nThe [Julia extension for Visual Studio Code](https://www.julia-vscode.org/)\nadds Weave support to [Visual Studio Code](https://code.visualstudio.com/).\n\n\n## Contributing\n\nYou can contribute to this package by opening issues on GitHub or implementing things yourself and making a pull request.\nWe'd also appreciate more example documents written using Weave.\n\n\n## Contributors\n\nYou can see the list of contributors on GitHub: https://github.com/JunoLab/Weave.jl/graphs/contributors .\nThanks for the important additions, fixes and comments.\n\n\n## Example projects using Weave\n\n- [DiffEqTutorials.jl](https://github.com/JuliaDiffEq/DiffEqTutorials.jl) uses Weave to output tutorials (`.jmd` documents) to html, pdf and Jupyter notebooks.\n- [TuringTutorials](https://github.com/TuringLang/TuringTutorials) uses Weave to convert notebooks to html.\n\n## Related packages\n\n- [Literate.jl](https://github.com/fredrikekre/Literate.jl) can be used to generate Markdown and Jupyter notebooks directly from Julia source files with markdown in comments.\n- [Quarto](https://quarto.org) can generate Jupyter notebooks, HTML, or PDF directly from a Markdown format containing Julia code blocks, and also works with R and Python.\n",820,scientific-workflows,Julia,6,Julia,TeX,CSS,HTML,Smarty,Jupyter Notebook,,,,,,,,,,,,,,,,,,,,,,,197,28,150,19,14,50,273,10788,94,283,150,133,77793c775c481213bc2e65b10350486aed391d00,Merge pull request #481 from dlfivefifty/patch-2,2024-04-16T10:50:53Z,Sebastian Pfitzner,pfitzseb@gmail.com,pfitzseb,v0.10.12,## Weave v0.10.12\n\n[Diff since v0.10.11](https://github.com/JunoLab/Weave.jl/compare/v0.10.11...v0.10.12)\n\n\n\n**Merged pull requests:**\n- Mention Literate.jl in README.md (#468) (@dlfivefifty),v0.10.12,,,github-actions[bot],MIT License,Weave.jl,JunoLab,40,julia,literate-programming,reproducible-research,scientific-reports,scientific-workflows,,,,,,,,,,,,,,,,/JunoLab/Weave.jl,45,17,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/JuliaSymbolics/Symbolics.jl,https://github.com/JuliaSymbolics/Symbolics.jl,0,not a project,,0,1,1,0,0,0,1,0,0,0,0,Symbolic programming for the next generation of numerical software,"# Symbolics.jl\n\n[![Github Action CI](https://github.com/JuliaSymbolics/Symbolics.jl/workflows/CI/badge.svg)](https://github.com/JuliaSymbolics/Symbolics.jl/actions)\n[![codecov](https://codecov.io/gh/JuliaSymbolics/Symbolics.jl/branch/master/graph/badge.svg)](https://codecov.io/gh/JuliaSymbolics/Symbolics.jl)\n[![Build Status](https://github.com/JuliaSymbolics/Symbolics.jl/workflows/CI/badge.svg)](https://github.com/JuliaSymbolics/Symbolics.jl/actions?query=workflow%3ACI)\n[![Stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://symbolics.juliasymbolics.org/stable/)\n[![Dev](https://img.shields.io/badge/docs-dev-blue.svg)](https://symbolics.juliasymbolics.org/dev/)\n\nSymbolics.jl is a fast and modern Computer Algebra System (CAS) for a fast and modern\nprogramming language (Julia). The goal is to have a high-performance and parallelized\nsymbolic algebra system that is directly extendable in the same language as that of the\nusers.\n\n## Installation\n\nTo install Symbolics.jl, use the Julia package manager:\n\n```julia\njulia> using Pkg\njulia> Pkg.add(""Symbolics"")\n```\n\n## Documentation\n\nFor information on using the package, see the [stable documentation](https://juliasymbolics.github.io/Symbolics.jl/stable/).\nUse the [in-development documentation](https://juliasymbolics.github.io/Symbolics.jl/dev/)\nfor the version of the documentation which contains the unreleased features.\n\n## Relationship to Other Packages\n\n- [SymbolicUtils.jl](https://github.com/JuliaSymbolics/SymbolicUtils.jl): This is a\n  rule-rewriting system that is the core of Symbolics.jl. Symbolics.jl builds off of\n  SymbolicUtils.jl to extend it to a whole symbolic algebra system, complete with\n  support for differentiation, solving symbolic systems of equations, etc. If you're\n  looking for the barebones to build a new CAS for specific algebras, SymbolicUtils.jl\n  is that foundation. Otherwise, Symbolics.jl is for you.\n- [ModelingToolkit.jl](https://github.com/SciML/ModelingToolkit.jl): This is a\n  symbolic-numeric modeling system for the SciML ecosystem. It heavily uses Symbolics.jl\n  for its representation of symbolic equations along with tools like differentiation,\n  and adds the representation of common modeling systems like ODEs, SDEs, and more.\n\n## Example\n\n```julia\njulia> using Symbolics\n\njulia> @variables t x y\njulia> D = Differential(t)\n\njulia> z = t + t^2\njulia> D(z) # symbolic representation of derivative(t + t^2, t)\nDifferential(t)(t + t^2)\n\njulia> expand_derivatives(D(z))\n1 + 2t\n\njulia> Symbolics.jacobian([x + x*y, x^2 + y],[x, y])\n2×2 Matrix{Num}:\n 1 + y  x\n    2x  1\n\njulia> B = simplify.([t^2 + t + t^2  2t + 4t\n                  x + y + y + 2t  x^2 - x^2 + y^2])\n2×2 Matrix{Num}:\n  t + 2(t^2)   6t\n x + 2t + 2y  y^2\n\njulia> simplify.(substitute.(B, (Dict(x => y^2),)))\n2×2 Matrix{Num}:\n    t + 2(t^2)   6t\n 2t + y^2 + 2y  y^2\n\njulia> substitute.(B, (Dict(x => 2.0, y => 3.0, t => 4.0),))\n2×2 Matrix{Num}:\n 36.0  24.0\n 16.0   9.0\n```\n\n## Citation\n\nIf you use Symbolics.jl, please [cite this paper](https://dl.acm.org/doi/10.1145/3511528.3511535) \n(or see the free [arxiv version](https://arxiv.org/abs/2105.03949))\n\n```bib\n@article{10.1145/3511528.3511535,\nauthor = {Gowda, Shashi and Ma, Yingbo and Cheli, Alessandro and Gw\'{o}\'{z}zd\'{z}, Maja and Shah, Viral B. and Edelman, Alan and Rackauckas, Christopher},\ntitle = {High-Performance Symbolic-Numerics via Multiple Dispatch},\nyear = {2022},\nissue_date = {September 2021},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nvolume = {55},\nnumber = {3},\nissn = {1932-2240},\nurl = {https://doi.org/10.1145/3511528.3511535},\ndoi = {10.1145/3511528.3511535},\nabstract = {As mathematical computing becomes more democratized in high-level languages, high-performance symbolic-numeric systems are necessary for domain scientists and engineers to get the best performance out of their machine without deep knowledge of code optimization. Naturally, users need different term types either to have different algebraic properties for them, or to use efficient data structures. To this end, we developed Symbolics.jl, an extendable symbolic system which uses dynamic multiple dispatch to change behavior depending on the domain needs. In this work we detail an underlying abstract term interface which allows for speed without sacrificing generality. We show that by formalizing a generic API on actions independent of implementation, we can retroactively add optimized data structures to our system without changing the pre-existing term rewriters. We showcase how this can be used to optimize term construction and give a 113x acceleration on general symbolic transformations. Further, we show that such a generic API allows for complementary term-rewriting implementations. Exploiting this feature, we demonstrate the ability to swap between classical term-rewriting simplifiers and e-graph-based term-rewriting simplifiers. We illustrate how this symbolic system improves numerical computing tasks by showcasing an e-graph ruleset which minimizes the number of CPU cycles during expression evaluation, and demonstrate how it simplifies a real-world reaction-network simulation to halve the runtime. Additionally, we show a reaction-diffusion partial differential equation solver which is able to be automatically converted into symbolic expressions via multiple dispatch tracing, which is subsequently accelerated and parallelized to give a 157x simulation speedup. Together, this presents Symbolics.jl as a next-generation symbolic-numeric computing environment geared towards modeling and simulation.},\njournal = {ACM Commun. Comput. Algebra},\nmonth = {jan},\npages = {92–96},\nnumpages = {5}\n}\n```\n",1326,mathematics,Julia,5,Julia,C,Objective-C,Stan,MATLAB,,,,,,,,,,,,,,,,,,,,,,,,520,63,412,45,53,96,1058,23122,147,667,273,394,46a6d066610cafb1d992e20bebe5e9b85018f1c8,Merge pull request #1190 from JuliaSymbolics/ChrisRackauckas-patch-1,2024-07-14T11:09:22Z,Christopher Rackauckas,accounts@chrisrackauckas.com,ChrisRackauckas,v5.33.0,## Symbolics v5.33.0\n\n[Diff since v5.32.0](https://github.com/JuliaSymbolics/Symbolics.jl/compare/v5.32.0...v5.33.0)\n\n\n**Merged pull requests:**\n- Add CommonWorldInvalidations world (#1183) (@ChrisRackauckas),v5.33.0,,,github-actions[bot],Other,Symbolics.jl,JuliaSymbolics,150,symbolic-computing,high-performance,parallel-computing,computer-algebra-system,cas,mathematics,,,,,,,,,,,,,,,/JuliaSymbolics/Symbolics.jl,150,23,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/JuliaReach/LazySets.jl,https://github.com/JuliaReach/LazySets.jl,0,"just a library, but pretty big and scietific",0,0,1,0,0,0,0,0,0,0,0,0,Scalable symbolic-numeric set computations in Julia,"# LazySets.jl\n## *Scalable Symbolic-Numeric Set Computations*\n\n| **Introduction & Documentation** | **Status** | **Community** | **Version-Specific Citation** | **License** |\n|:--------------------------------:|:----------:|:-------------:|:-----------------------------:|:-----------:|\n| [![DOI][paper-img]][paper-url] [![docs-dev][dev-img]][dev-url] | [![CI][ci-img]][ci-url] [![codecov][cov-img]][cov-url] [![PkgEval][pkgeval-img]][pkgeval-url] [![aqua][aqua-img]][aqua-url] | [![zulip][chat-img]][chat-url] [![JuliaHub][juliahub-img]][juliahub-url] | [![DOI][doi-img]][doi-url] | [![license][lic-img]][lic-url] |\n\n[paper-img]: https://proceedings.juliacon.org/papers/10.21105/jcon.00097/status.svg\n[paper-url]: https://doi.org/10.21105/jcon.00097\n[dev-img]: https://img.shields.io/badge/docs-latest-blue.svg\n[dev-url]: https://juliareach.github.io/LazySets.jl/dev/\n[ci-img]: https://github.com/JuliaReach/LazySets.jl/workflows/CI/badge.svg\n[ci-url]: https://github.com/JuliaReach/LazySets.jl/actions/workflows/test-master.yml\n[cov-img]: https://codecov.io/github/JuliaReach/LazySets.jl/coverage.svg\n[cov-url]: https://app.codecov.io/github/JuliaReach/LazySets.jl\n[pkgeval-img]: https://juliaci.github.io/NanosoldierReports/pkgeval_badges/L/LazySets.svg\n[pkgeval-url]: https://juliaci.github.io/NanosoldierReports/pkgeval_badges/L/LazySets.html\n[aqua-img]: https://raw.githubusercontent.com/JuliaTesting/Aqua.jl/master/badge.svg\n[aqua-url]: https://github.com/JuliaTesting/Aqua.jl\n[chat-img]: https://img.shields.io/badge/zulip-join_chat-brightgreen.svg\n[chat-url]: https://julialang.zulipchat.com/#narrow/stream/278609-juliareach\n[juliahub-img]: https://juliahub.com/docs/General/LazySets/stable/version.svg\n[juliahub-url]: https://juliahub.com/ui/Packages/General/LazySets\n[doi-img]: https://zenodo.org/badge/105701832.svg\n[doi-url]: https://zenodo.org/badge/latestdoi/105701832\n[lic-img]: https://img.shields.io/github/license/mashape/apistatus.svg\n[lic-url]: https://github.com/JuliaReach/LazySets.jl/blob/master/LICENSE\n\n## ❓ Introduction\n\nThe following article showcases the basic functionality, highlighting some of the key design choices:\n\n> Forets, Marcelo, and Christian Schilling. *LazySets.jl: Scalable Symbolic-Numeric Set Computations.* [Proceedings of the JuliaCon Conferences](https://doi.org/10.21105/jcon.00097) (2021).\n\nSee [below](#-how-to-cite) for how to cite it.\n\n## 🎯 Resources\n\n- [Manual](https://juliareach.github.io/LazySets.jl/dev/)\n- [Release notes of the development version](https://github.com/JuliaReach/LazySets.jl/wiki/Release-log-tracker)\n- [Release notes of previous versions](https://github.com/JuliaReach/LazySets.jl/releases)\n- [How to contribute](https://juliareach.github.io/LazySets.jl/dev/about/#Contributing-1)\n- [Developers team](https://juliareach.github.io/LazySets.jl/dev/about/#Credits-1)\n\n## 💾 Installing\n\n`LazySets.jl` is a registered Julia package and [as such you can install it](https://julialang.github.io/Pkg.jl/v1/managing-packages/) by activating the `pkg` mode (type `]`, and to leave it, type `<backspace>`),\nfollowed by\n\n```julia\npkg> add LazySets\n```\n\nSee the [Getting started](https://juliareach.github.io/LazySets.jl/dev/man/getting_started/) section of the manual for other options.\n\n## 📘 Publications\n\nThis library has been applied in a number of scientific works.\n\n<details>\n<summary>Click to see the full list of publications that use LazySets.</summary>\n\nThe articles appear in reverse chronological order.\n\n[33] **Verified propagation of imprecise probabilities in non-linear ODEs.** Ander Gray, Marcelo Forets, Christian Schilling, Scott Ferson, and Luis Benet (2024). [International Journal of Approximate Reasoning](https://www.sciencedirect.com/journal/international-journal-of-approximate-reasoning), vol. 164. [doi: 10.1016/j.ijar.2023.109044](https://doi.org/10.1016/j.ijar.2023.109044).\n\n[32] **Safety verification of decision-tree policies in continuous time.** Christian Schilling, Anna Lukina, Emir Demirović, and Kim G. Larsen (2023). [37th Conference on Neural Information Processing Systems (NeurIPS)](https://neurips.cc/Conferences/2023). [pdf](https://openreview.net/pdf?id=tEKBU5XOTw).\n\n[31] **Shielded reinforcement learning for hybrid systems.** Asger H. Brorholt, Peter G. Jensen, Kim G. Larsen, Florian Lorber, and Christian Schilling (2023). [1st International Conference on Bridging the Gap between AI and Reality (AISoLA)](https://aisola.org/), LNCS, vol. 14380, pp. 33-54. [doi: 10.1007/978-3-031-46002-9_3](https://doi.org/10.1007/978-3-031-46002-9_3), [arXiv: 2308.14424](https://arxiv.org/abs/2308.14424).\n\n[30] **The inverse problem for neural networks.** Marcelo Forets and Christian Schilling (2023). [1st International Conference on Bridging the Gap between AI and Reality (AISoLA)](https://aisola.org/), LNCS, vol. 14380, pp. 241-255. [doi: 10.1007/978-3-031-46002-9_14](https://doi.org/10.1007/978-3-031-46002-9_14), [arXiv: 2308.14093](https://arxiv.org/abs/2308.14093).\n\n[29] **ARCH-COMP23 category report: Continuous and hybrid systems with linear continuous dynamics.** Matthias Althoff, Marcelo Forets, Yangge Li, Sayan Mitra, Christian Schilling, Mark Wetzlinger, and Daniel Zhuang (2023). 10th [International Workshop on Applied Verification of Continuous and Hybrid Systems](https://cps-vo.org/group/ARCH/) (ARCH23), vol 96, pp. 34-60. [doi: 10.29007/nl86](https://doi.org/10.29007/nl86).\n\n[28] **ARCH-COMP23 category report: Continuous and hybrid systems with nonlinear dynamics.** Luca Geretti, Julien Alexandre Dit Sandretto, Matthias Althoff, Luis Benet, Pieter Collins, Marcelo Forets, Elena Ivanova, Yangge Li, Sayan Mitra, Stefan Mitsch, Christian Schilling, Mark Wetzlinger, and Daniel Zhuang (2023). 10th [International Workshop on Applied Verification of Continuous and Hybrid Systems](https://cps-vo.org/group/ARCH/) (ARCH23), vol 96, pp. 61-88. [doi: 10.29007/93f2](https://doi.org/10.29007/93f2).\n\n[27] **ARCH-COMP23 category report: Artificial intelligence and neural network control systems for continuous and hybrid systems plants.** Diego Manzanas Lopez, Matthias Althoff, Marcelo Forets, Taylor T. Johnson, Tobias Ladner, and Christian Schilling (2023). 10th [International Workshop on Applied Verification of Continuous and Hybrid Systems](https://cps-vo.org/group/ARCH/) (ARCH23), vol 96, pp. 89-125. [doi: 10.29007/x38n](https://doi.org/10.29007/x38n).\n\n[26] **ARCH-COMP22 category report: Continuous and hybrid systems with linear continuous dynamics.** Matthias Althoff, Marcelo Forets, Christian Schilling, and Mark Wetzlinger (2022). 9th [International Workshop on Applied Verification of Continuous and Hybrid Systems](https://cps-vo.org/group/ARCH/) (ARCH22), vol 90, pp. 58-85. [doi: 10.29007/mmzc](https://doi.org/10.29007/mmzc).\n\n[25] **ARCH-COMP22 category report: Continuous and hybrid systems with nonlinear dynamics.** Luca Geretti, Julien Alexandre Dit Sandretto, Matthias Althoff, Luis Benet, Pieter Collins, Parasara Sridhar Duggirala, Marcelo Forets, Edward Kim, Stefan Mitsch, Christian Schilling, and Mark Wetzlinger (2022). 9th [International Workshop on Applied Verification of Continuous and Hybrid Systems](https://cps-vo.org/group/ARCH/) (ARCH22), vol 90, pp. 86-112. [doi: 10.29007/fnzc](https://doi.org/10.29007/fnzc).\n\n[24] **ARCH-COMP22 category report: Artificial intelligence and neural network control systems for continuous and hybrid systems plants.** Diego Manzanas Lopez, Matthias Althoff, Luis Benet, Xin Chen, Jiameng Fan, Marcelo Forets, Chao Huang, Taylor T. Johnson, Tobias Ladner, Wenchao Li, Christian Schilling, and Qi Zhu (2022). 9th [International Workshop on Applied Verification of Continuous and Hybrid Systems](https://cps-vo.org/group/ARCH/) (ARCH22), vol 90, pp. 142-184. [doi: 10.29007/wfgr](https://doi.org/10.29007/wfgr).\n\n[23] **Synthesis of parametric hybrid automata from time series.** Miriam García Soto, Thomas A. Henzinger, and Christian Schilling (2022). Proceedings of the [20th International Symposium on Automated Technology for Verification and Analysis](https://atva-conference.org/2022/), LNCS, vol. 13505, pp. 337-353. [doi: 10.1007/978-3-031-19992-9_22](https://doi.org/10.1007/978-3-031-19992-9_22), [arXiv: 2208.06383](https://arxiv.org/abs/2208.06383).\n\n[22] **Decomposing reach set computations with low-dimensional sets and high-dimensional matrices (extended version).** Sergiy Bogomolov, Marcelo Forets, Goran Frehse, Andreas Podelski, and Christian Schilling (2022). [Information and Computation](https://www.sciencedirect.com/journal/information-and-computation), vol. 289. [doi: 10.1016/j.ic.2022.104937](https://doi.org/10.1016/j.ic.2022.104937).\n\n[21] **Conservative Time Discretization: A Comparative Study.** Marcelo Forets and Christian Schilling (2022). Proceedings of the [17th International Conference on integrated Formal Methods (iFM)](https://ifm22.si.usi.ch/), LNCS, vol. 13274, pp. 149-167. [doi: 10.1007/978-3-031-07727-2_9](https://doi.org/10.1007/978-3-031-07727-2_9), [arXiv: 2111.01454](https://arxiv.org/abs/2111.01454).\n\n[20] **Verification of Neural-Network Control Systems by Integrating Taylor Models and Zonotopes.** Christian Schilling, Marcelo Forets, and Sebastián Guadalupe (2022). Proceedings of the [36th Conference on Artificial Intelligence (AAAI)](https://aaai.org/Conferences/AAAI-22/). [doi: 10.1609/aaai.v36i7.20790](https://doi.org/10.1609/aaai.v36i7.20790).\n\n[19] **Combining Set Propagation with Finite Element Methods for Time Integration in Transient Solid Mechanics Problems.** Marcelo Forets, Daniel Freire Caporale, and Jorge M. Pérez Zerpa (2022). [Computers & Structures](https://www.sciencedirect.com/journal/computers-and-structures), vol 259. [doi: 10.1016/j.compstruc.2021.106699](https://doi.org/10.1016/j.compstruc.2021.106699), [arXiv: 2105.05841](https://arxiv.org/abs/2105.05841).\n\n[18] **LazySets.jl: Scalable Symbolic-Numeric Set Computations.** Marcelo Forets and Christian Schilling (2021). [Proceedings of the JuliaCon Conferences](https://proceedings.juliacon.org/). [doi: 10.21105/jcon.00097](https://doi.org/10.21105/jcon.00097).\n\n[17] **Reachability of weakly nonlinear systems using Carleman linearization.** Marcelo Forets and Christian Schilling (2021). Proceedings of the [15th International Conference on Reachability Problems (RP)](https://rp2021.csc.liv.ac.uk/), LNCS, vol. 13035, pp. 85-99. [doi: 10.1007/978-3-030-89716-1_6](https://doi.org/10.1007/978-3-030-89716-1_6), [arXiv: 2108.10390](https://arxiv.org/abs/2108.10390).\n\n[16] **Combined Exact and Heuristics Based Approach to Hamiltonian Path Problem Optimization for Route Planning.** Fernando Hernandez, Rafael Sotelo, and Marcelo Forets (2021). Technical Proceedings of the [2021 Amazon Last Mile Routing Research Challenge](https://hdl.handle.net/1721.1/131235).\n\n[15] **ARCH-COMP21 Category Report: Continuous and Hybrid Systems with Linear Continuous Dynamics.** Matthias Althoff, Erika Abraham, Marcelo Forets, Goran Frehse, Daniel Freire, Christian Schilling, Stefan Schupp, and Mark Wetzlinger (2021). 8th [International Workshop on Applied Verification of Continuous and Hybrid Systems](https://cps-vo.org/group/ARCH/) (ARCH21), vol 80, pp. 1-31. [doi: 10.29007/lhbw](https://doi.org/10.29007/lhbw).\n\n[14] **ARCH-COMP21 Category Report: Continuous and Hybrid Systems with Nonlinear Dynamics.** Luca Geretti, Julien Alexandre dit Sandretto, Matthias Althoff, Luis Benet, Alexandre Chapoutot, Pieter Collins, Parasara Sridhar Duggirala, Marcelo Forets, Edward Kim, Uziel Linares, David P. Sanders, Christian Schilling, and Mark Wetzlinger (2021). 8th [International Workshop on Applied Verification of Continuous and Hybrid Systems](https://cps-vo.org/group/ARCH/) (ARCH21), vol 80, pp. 32-54. [doi: 10.29007/2jw8](https://doi.org/10.29007/2jw8).\n\n[13] **ARCH-COMP21 Category Report: Artificial Intelligence and Neural Network Controlled Systems for Continuous and Hybrid Systems Plants.** Taylor T. Johnson, Diego Manzanas Lopez, Luis Benet, Marcelo Forets, Christian Schilling, Radoslav Ivanov, Taylor Carpenter, James Weimer, and Insup Lee (2021). 8th [International Workshop on Applied Verification of Continuous and Hybrid Systems](https://cps-vo.org/group/ARCH/) (ARCH21), vol 80, pp. 90-119. [doi: 10.29007/kfk9](https://doi.org/10.29007/kfk9).\n\n[12] **Synthesis of hybrid automata with affine dynamics from time-series data.** Miriam García Soto, Thomas A. Henzinger, and Christian Schilling (2021). [24th International Conference on Hybrid Systems: Computation and Control (HSCC)](https://hscc.acm.org/2021/). [doi: 10.1145/3447928.3456704](https://doi.org/10.1145/3447928.3456704), [arXiv: 2102.12734](https://arxiv.org/abs/2102.12734).\n\n[11] **Algorithms for verifying deep neural networks.** Changliu Liu, Tomer Arnon, Christopher Lazarus, Christopher A. Strong, Clark W. Barrett, and Mykel J. Kochenderfer (2021). [Foundations and Trends in Optimization](https://www.nowpublishers.com/OPT), vol 4, pp. 244-404. [doi: 10.1561/2400000035](https://doi.org/10.1561/2400000035), [arXiv: 1903.06758](https://arxiv.org/abs/1903.06758).\n\n[10] **Efficient reachability analysis of parametric linear hybrid systems with time-triggered transitions.** Marcelo Forets, Daniel Freire, and Christian Schilling (2020). Proceedings of the [18th International Conference on Formal Methods and Models for System Design (MEMOCODE)](https://iitjammu.ac.in/conferences/memocode2020/), pp. 137-142. [doi: 10.1109/MEMOCODE51338.2020.9314994](https://doi.org/10.1109/MEMOCODE51338.2020.9314994), [arXiv: 2006.12325](https://arxiv.org/abs/2006.12325).\n\n[9] **ARCH-COMP20 Category Report: Continuous and Hybrid Systems with Linear Continuous Dynamics.** Matthias Althoff, Stanley Bak, Zongnan Bao, Marcelo Forets, Daniel Freire, Goran Frehse, Niklas Kochdumper, Yangge Li, Sayan Mitra, Rajarshi Ray, Christian Schilling, Stefan Schupp, and Mark Wetzlinger (2020). 7th [International Workshop on Applied Verification of Continuous and Hybrid Systems](https://cps-vo.org/group/ARCH/) (ARCH20), vol 74, pp. 16-48. [doi: 10.29007/7dt2](https://doi.org/10.29007/7dt2).\n\n[8] **ARCH-COMP20 Category Report: Continuous and Hybrid Systems with Nonlinear Dynamics.** Luca Geretti, Julien Alexandre dit Sandretto, Matthias Althoff, Luis Benet, Alexandre Chapoutot, Xin Chen, Pieter Collins, Marcelo Forets, Daniel Freire, Fabian Immler, Niklas Kochdumper, David P. Sanders, and Christian Schilling (2020). 7th [International Workshop on Applied Verification of Continuous and Hybrid Systems](https://cps-vo.org/group/ARCH/) (ARCH20), vol 74, pp. 49-75. [doi: 10.29007/zkf6](https://doi.org/10.29007/zkf6).\n\n[7] **Case Study: Reachability Analysis of a unified Combat-Command-and-Control Model.** Sergiy Bogomolov, Marcelo Forets, and Kostiantyn Potomkin (2020). Proceedings of the [14th International Conference on Reachability Problems (RP)](https://www.irif.fr/~rp2020/), LNCS, vol 12448, pp. 52-66. [doi: 10.1007/978-3-030-61739-4_4](https://doi.org/10.1007/978-3-030-61739-4_4).\n\n[6] **Reachability analysis of linear hybrid systems via block decomposition.** Sergiy Bogomolov, Marcelo Forets, Goran Frehse, Kostiantyn Potomkin, and Christian Schilling (2020). IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 39, pp. 4018-4029. Presented at [Embedded Systems Week](https://esweek.org/) 2020. [doi: 10.1109/TCAD.2020.3012859](https://doi.org/10.1109/TCAD.2020.3012859), [arXiv: 1905.02458](https://arxiv.org/abs/1905.02458).\n\n[5] **ARCH-COMP19 Category Report: Continuous and Hybrid Systems with Linear Continuous Dynamics.** Matthias Althoff, Stanley Bak, Marcelo Forets, Goran Frehse, Niklas Kochdumper, Rajarshi Ray, Christian Schilling, and Stefan Schupp (2019). 6th [International Workshop on Applied Verification of Continuous and Hybrid Systems](https://cps-vo.org/group/ARCH/) (ARCH19), vol 61, pp. 14-40. [doi: 10.29007/bj1w](https://doi.org/10.29007/bj1w).\n\n[4] **ARCH-COMP19 Category Report: Continuous and Hybrid Systems with Nonlinear Dynamics.** Fabian Immler, Matthias Althoff, Luis Benet, Alexandre Chapoutot, Xin Chen, Marcelo Forets, Luca Geretti, Niklas Kochdumper, David P. Sanders, and Christian Schilling (2019). 6th [International Workshop on Applied Verification of Continuous and Hybrid Systems](https://cps-vo.org/group/ARCH/) (ARCH19), vol 61, pp. 41-61. [doi: 10.29007/m75b](https://doi.org/10.29007/m75b).\n\n[3] **JuliaReach: a Toolbox for Set-Based Reachability.** Sergiy Bogomolov, Marcelo Forets, Goran Frehse, Kostiantyn Potomkin, Christian Schilling (2019). Proceedings of the 22nd International Conference on Hybrid Systems: Computation and Control (HSCC), pp. 39-44. [doi: 10.1145/3302504.3311804](https://doi.org/10.1145/3302504.3311804), [arXiv: 1901.10736](https://arxiv.org/abs/1901.10736).\n\n[2] **ARCH-COMP18 Category Report: Continuous and Hybrid Systems with Linear Continuous Dynamics.** Matthias Althoff, Stanley Bak, Xin Chen, Chuchu Fan, Marcelo Forets, Goran Frehse, Niklas Kochdumper, Yangge Li, Sayan Mitra, Rajarshi Ray, Christian Schilling, and Stefan Schupp (2018). 5th [International Workshop on Applied Verification of Continuous and Hybrid Systems](https://cps-vo.org/group/ARCH/) (ARCH18), vol 54, pp. 23–52. [doi: 10.29007/73mb](https://doi.org/10.29007/73mb).\n\n[1] **Reach Set Approximation through Decomposition with Low-dimensional Sets and High-dimensional Matrices.** Sergiy Bogomolov, Marcelo Forets, Goran Frehse, Frédéric Viry, Andreas Podelski, and Christian Schilling (2018). Proceedings of the [21st International Conference on Hybrid Systems: Computation and Control (HSCC)](https://www.hscc2018.deib.polimi.it/), pp. 41–50. [doi: 10.1145/3178126.3178128](https://doi.org/10.1145/3178126.3178128), [arXiv: 1801.09526](https://arxiv.org/abs/1801.09526).\n\n</details>\n\n## 🗺 Ecosystem\n\nSeveral projects in the Julia technical computing stack use this library.\n\n<details>\n<summary>Click to see the full list of Julia packages that use LazySets.</summary>\n\n- [ConvexBodyProximityQueries.jl](https://github.com/arlk/ConvexBodyProximityQueries.jl) -- Proximity computation between convex bodies in 2D/3D.\n- [ClosedLoopReachability.jl](https://github.com/JuliaReach/ClosedLoopReachability.jl) -- Reachability analysis for closed-loop control systems.\n- [HySynthParametric](https://github.com/HySynth/HySynthParametric) -- Synthesis of parametric linear hybrid automata.\n- [IntervalLinearAlgebra.jl](https://github.com/JuliaIntervals/IntervalLinearAlgebra.jl) -- Routines to perform numerical linear algebra using interval arithmetic.\n- [InvariantSets.jl](https://github.com/ueliwechsler/InvariantSets.jl) -- Compute, approximate and display invariant sets.\n- [InvariantSetApproximation.jl](https://github.com/psace-uofa/InvariantSetApproximation.jl) -- Invariant sets of general discrete-time dynamical systems with controls and uncertainties using graph-based algorithms.\n- [NeuralVerification.jl](https://github.com/sisl/NeuralVerification.jl) -- Methods to verify deep neural networks.\n- [OpticSim.jl](https://github.com/microsoft/OpticSim.jl) -- Ray tracing for procedurally generated systems.\n- [Photometry.jl](https://github.com/JuliaAstro/Photometry.jl) -- Utilities for characterizing sources in astronomical images.\n- [ReachabilityAnalysis.jl](https://github.com/JuliaReach/ReachabilityAnalysis.jl) -- Methods to compute the sets of states reachable in dynamical systems.\n- [Swalbe.jl](https://github.com/Zitzeronion/Swalbe.jl) -- Simple Julia Lattice Boltzmann Solver for Thin Liquid Films and Droplets.\n- [TrajectoryGamesBase.jl](https://github.com/lassepe/TrajectoryGamesBase.jl) -- Interface to define trajectory games.\n\n</details>\n\n## 👨‍🏫 Workshop at JuliaCon 2021\n\n<details>\n<summary>Abstract</summary>\n\nWe present [JuliaReach](https://github.com/JuliaReach), a Julia ecosystem to perform reachability analysis of dynamical systems. JuliaReach builds on sound scientific approaches and was, in two occasions (2018 and 2020) the winner of the annual friendly competition on Applied Verification for Continuous and Hybrid Systems ([ARCH-COMP](https://cps-vo.org/group/ARCH)).\n\nThe workshop consists of three parts (respectively packages) in [JuliaReach](https://github.com/JuliaReach): our core package for set representations, our main package for reachability analysis, and a new package applying reachability analysis with potential use in domain of control, robotics and autonomous systems.\n\nIn the first part we present [LazySets.jl](https://github.com/JuliaReach/LazySets.jl), which provides ways to symbolically represent sets of points as geometric shapes, with a special focus on convex sets and polyhedral approximations. [LazySets.jl](https://github.com/JuliaReach/LazySets.jl) provides methods to apply common set operations, convert between different set representations, and efficiently compute with sets in high dimensions.\n\nIn the second part we present [ReachabilityAnalysis.jl](https://github.com/JuliaReach/ReachabilityAnalysis.jl), which provides tools to approximate the set of reachable states of systems with both continuous and mixed discrete-continuous dynamics, also known as hybrid systems. It implements conservative discretization and set-propagation techniques at the state-of-the-art.\n\nIn the third part we present [NeuralNetworkAnalysis.jl](https://github.com/JuliaReach/NeuralNetworkAnalysis.jl), which is an application of [ReachabilityAnalysis.jl](https://github.com/JuliaReach/ReachabilityAnalysis.jl) to analyze dynamical systems that are controlled by neural networks. This package can be used to validate or invalidate specifications, for instance about the safety of such systems.\n\nWorkshop materials are available here: https://github.com/JuliaReach/JuliaCon-2021-Workshop-Its-All-Set\n</details>\n\n[![JuliaCon 2021 video](https://img.youtube.com/vi/P4I7pTvQ4nk/0.jpg)](https://youtu.be/P4I7pTvQ4nk)\n\n\n## 📜 How to cite\n\nIf you use this package in your work, please cite it using the metadata [here](CITATION.bib) or below.\n\n<details>\n<summary>Click to see BibTeX entry. </summary>\n\n```\n@article{lazysets21,\n  title     = {{LazySets.jl: Scalable Symbolic-Numeric Set Computations}},\n  author    = {Forets, Marcelo and Schilling, Christian},\n  journal   = {Proceedings of the JuliaCon Conferences},\n  year      = {2021},\n  publisher = {The Open Journal},\n  volume    = {1},\n  number    = {1},\n  pages     = {11},\n  doi       = {10.21105/jcon.00097}\n}\n```\n\n</details>\n",226,geometry,Julia,1,Julia,,,,,,,,,,,,,,,,,,,,,,,,,,,,2117,139,1972,6,48,22,2760,37148,32,1478,1111,367,ca653088ddc0ff9eb280a06454226b41d1e2bf8e,Merge pull request #3598 from JuliaReach/schillic/exports,2024-07-19T12:03:12Z,Christian Schilling,git@christianschilling.net,schillic,v2.14.1,"## LazySets v2.14.1\r\n\r\n[Diff since v2.14.0](https://github.com/JuliaReach/LazySets.jl/compare/v2.14.0...v2.14.1)\r\n\r\n#### Announcements\r\n\r\nContributors for this release:\r\n\r\n- Marcelo Forets ([@mforets](https://github.com/mforets))\r\n- Christian Schilling ([@schillic](https://github.com/schillic))\r\n\r\n#### Features\r\n\r\n- Add more and better operations for `Interval` ([#3533](https://github.com/JuliaReach/LazySets.jl/pull/3533))\r\n\r\n#### Enhancements\r\n\r\n- Allow SymEngine v0.12 ([#3541](https://github.com/JuliaReach/LazySets.jl/pull/3541))\r\n- Make all `API` functions available in `IntervalModule` & `EmptySetModule` ([#3545](https://github.com/JuliaReach/LazySets.jl/pull/3545))\r\n- Outsource `Ball1` to its own module ([#3547](https://github.com/JuliaReach/LazySets.jl/pull/3547))\r\n- Outsource `Ball2` to its own module ([#3549](https://github.com/JuliaReach/LazySets.jl/pull/3549))\r\n- Outsource `BallInf` to its own module ([#3550](https://github.com/JuliaReach/LazySets.jl/pull/3550))\r\n- Outsource `Ballp` to its own module ([#3548](https://github.com/JuliaReach/LazySets.jl/pull/3548))\r\n- Outsource `Ellipsoid` to its own module ([#3555](https://github.com/JuliaReach/LazySets.jl/pull/3555))\r\n- Outsource `LineSegment` to its own module ([#3556](https://github.com/JuliaReach/LazySets.jl/pull/3556))\r\n- Outsource `ZeroSet` to its own module ([#3557](https://github.com/JuliaReach/LazySets.jl/pull/3557))\r\n- Outsource `Singleton` to its own module ([#3558](https://github.com/JuliaReach/LazySets.jl/pull/3558))\r\n- Outsource `Universe` to its own module ([#3564](https://github.com/JuliaReach/LazySets.jl/pull/3564))\r\n\r\n#### Internal changes\r\n\r\n- Temporary fix for Symbolics doctest ([#3542](https://github.com/JuliaReach/LazySets.jl/pull/3542))\r\n- Add space in error message ([#3544](https://github.com/JuliaReach/LazySets.jl/pull/3544))\r\n- Define `AbstractBallp` interface functions ([#3546](https://github.com/JuliaReach/LazySets.jl/pull/3546))\r\n- Define `AbstractSingleton` interface function ([#3560](https://github.com/JuliaReach/LazySets.jl/pull/3560))\r\n- Update docs ([#3559](https://github.com/JuliaReach/LazySets.jl/pull/3559))\r\n- Add option for shorter tests, used in PkgEval ([#3563](https://github.com/JuliaReach/LazySets.jl/pull/3563))\r\n- Revise documentation of interface requirements ([#3565](https://github.com/JuliaReach/LazySets.jl/pull/3565))\r\n\r\n---\r\n\r\n**Merged pull requests:**\r\n- Add more and better operations for `Interval` (#3533) (@schillic)\r\n- Allow SymEngine v0.12 (#3541) (@schillic)\r\n- Temporary fix for Symbolics doctest (#3542) (@schillic)\r\n- Automatic JuliaFormatter.jl run (#3543) (@github-actions[bot])\r\n- Add space in error message (#3544) (@schillic)\r\n- Make all API functions available in IntervalModule & EmptySetModule (#3545) (@schillic)\r\n- Define `AbstractBallp` interface functions (#3546) (@schillic)\r\n- Outsource `Ball1` to its own module (#3547) (@schillic)\r\n- Outsource `Ballp` to its own module (#3548) (@schillic)\r\n- Outsource `Ball2` to its own module (#3549) (@schillic)\r\n- Outsource `BallInf` to its own module (#3550) (@schillic)\r\n- Automatic JuliaFormatter.jl run (#3552) (@github-actions[bot])\r\n- Fix documentation (#3554) (@schillic)\r\n- Outsource `Ellipsoid` to its own module (#3555) (@schillic)\r\n- Outsource `LineSegment` to its own module (#3556) (@schillic)\r\n- Outsource `ZeroSet` to its own module (#3557) (@schillic)\r\n- Outsource `Singleton` to its own module (#3558) (@schillic)\r\n- Update docs (#3559) (@schillic)\r\n- Define `AbstractSingleton` interface function (#3560) (@schillic)\r\n- Add option for shorter tests, used in PkgEval (#3563) (@schillic)\r\n- Outsource `Universe` to its own module (#3564) (@schillic)\r\n- Revise documentation of interface requirements (#3565) (@schillic)\r\n\r\n**Closed issues:**\r\n- Special sets in concrete projection (#1990)\r\n- Segfault when calling affine_map (#2278)",v2.14.1,,,github-actions[bot],Other,LazySets.jl,JuliaReach,149,julia,convex-sets,formal-verification,reachability-analysis,lazy-evaluation,polyhedra,convex-hull,calculus,sets,geometry,geometry-algorithms,computational-geometry,zonotope,polygons,minkowski-sum,projections,set-propagation,,,,/JuliaReach/LazySets.jl,149,10,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/JuliaPlots/PlotlyJS.jl,https://github.com/JuliaPlots/PlotlyJS.jl,0,,0,0,0,0,0,0,0,1,1,0,0,0,Julia library for plotting with plotly.js,"# PlotlyJS\n\n[_plotlyjs]: https://plot.ly/javascript\n\n[docs-img]: https://img.shields.io/badge/docs-stable-blue.svg\n[docs-url]: http://juliaplots.org/PlotlyJS.jl/stable\n\n[![][docs-img]][docs-url]\n[![Build Status](https://github.com/JuliaPlots/PlotlyJS.jl/actions/workflows/ci-master-workflow.yml/badge.svg)](https://github.com/JuliaPlots/PlotlyJS.jl/actions/workflows/ci-master-workflow.yml)\n[![project chat](https://img.shields.io/badge/zulip-join_chat-brightgreen.svg)](https://julialang.zulipchat.com/#narrow/stream/227176-plotting)\n\nJulia interface to [plotly.js][_plotlyjs] visualization library.\n\nThis package constructs plotly graphics using all local resources. To interact or save graphics to the Plotly cloud, use the  [`Plotly`](https://github.com/plotly/Plotly.jl) package.\n\n## Installation\n\nIf you have issues building this package because of installation of the MbedTLS  package please see [this issue](https://github.com/sglyon/PlotlyJS.jl/issues/83).\n\n### Jupyterlab\n\nIf you will be using this package from within Jupyterlab, please also install the plotly jupyterlab extension by running:\n\n\n```sh\njupyter labextension install jupyterlab-plotly\n```\n\nSee the [jupyterlab extension documentation](https://jupyterlab.readthedocs.io/en/stable/user/extensions.html) for more details.\n\n\n",413,graphics,Julia,2,Julia,JavaScript,,,,,,,,,,,,,,,,,,,,,,,,,,,130,29,96,5,24,71,79,33612,77,355,259,96,75eacf4542dc12d7b05242b550f250e6039929bf,substitute old savefig code with PlotlyKaleido (#481),2024-02-23T13:50:53Z,Simon Christ,christ@cell.uni-hannover.de,BeastyBlacksmith,v0.18.13,"## PlotlyJS v0.18.13\n\n[Diff since v0.18.12](https://github.com/JuliaPlots/PlotlyJS.jl/compare/v0.18.12...v0.18.13)\n\n\n**Merged pull requests:**\n- migrate to weakdeps (#463) (@sjkelly)\n- CompatHelper: add new compat entry for DelimitedFiles at version 1, (keep existing compat) (#465) (@github-actions[bot])\n- substitute old savefig code with PlotlyKaleido (#481) (@BeastyBlacksmith)\n\n**Closed issues:**\n- Setting PlotlyJS as default backend for Plots (#478)",v0.18.13,,,github-actions[bot],Other,PlotlyJS.jl,JuliaPlots,54,julia,plotlyjs,graphics,plotting,javascript,,,,,,,,,,,,,,,,/JuliaPlots/PlotlyJS.jl,72,15,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/JuliaPhysics/Measurements.jl,https://github.com/JuliaPhysics/Measurements.jl,0,,,0,1,0,0,0,0,1,0,0,0,0,"Error propagation calculator and library for physical measurements.  It supports real and complex numbers with uncertainty, arbitrary precision calculations, operations with arrays, and numerical integration.","# Measurements.jl\n\n| **Documentation**                       | **Build Status**                    | **Code Coverage**               | **Quality** |\n|:---------------------------------------:|:-----------------------------------:|:-------------------------------:|:-----------:|\n| [![][docs-stable-img]][docs-stable-url] | [![Build Status][gha-img]][gha-url] | [![][coveral-img]][coveral-url] |[![Aqua QA][aqua-img]][aqua-url]|\n| [![][docs-latest-img]][docs-latest-url] |                                     | [![][codecov-img]][codecov-url] |\n\n## Introduction\n\n### What Is This Package Useful For?\n\n![image](docs/src/error_bars_2x.png)\n\n*Image credit: ""[xkcd: Error Bars](https://xkcd.com/2110/)"" ([CC-BY-NC\n2.5](https://creativecommons.org/licenses/by-nc/2.5/))*\n\nPhysical measures are typically reported with an error, a quantification of the\n[uncertainty](https://en.wikipedia.org/wiki/Measurement_uncertainty) of the\naccuracy of the measurement.  Whenever you perform mathematical operations\ninvolving these quantities you have also to [propagate the\nuncertainty](https://en.wikipedia.org/wiki/Propagation_of_uncertainty), so that\nthe resulting number will also have an attached error to quantify the confidence\nabout its accuracy.\n[Measurements.jl](https://github.com/JuliaPhysics/Measurements.jl) relieves you\nfrom the hassle of propagating uncertainties coming from physical measurements,\nwhen performing mathematical operations involving them.  The [linear error\npropagation\ntheory](https://en.wikipedia.org/wiki/Propagation_of_uncertainty#Linear_combinations)\nis employed to propagate the errors.\n\nThis library is written in [Julia](http://julialang.org/), a modern high-level,\nhigh-performance dynamic programming language designed for technical computing.\n\nWhen used in the [Julia interactive\nsession](https://docs.julialang.org/en/v1/stdlib/REPL/), it can serve also as an\neasy-to-use calculator.\n\n### Features List\n\n* Support for most mathematical operations available in Julia standard library\n  and special functions\n  from [`SpecialFunctions.jl`](https://github.com/JuliaMath/SpecialFunctions.jl)\n  package, involving real and complex numbers.  All existing functions that\n  accept `AbstractFloat` (and `Complex{AbstractFloat}` as well) arguments and\n  internally use already supported functions can in turn perform calculations\n  involving numbers with uncertainties without being redefined.  This greatly\n  enhances the power of `Measurements.jl` without effort for the users\n* Functional correlation between variables is correctly handled, so `x-x ≈\n  zero(x)`, `x/x ≈ one(x)`, `tan(x) ≈ sin(x)/cos(x)`, `cis(x) ≈ exp(im*x)`,\n  etc...\n* Support for\n  [arbitrary precision](https://docs.julialang.org/en/v1/manual/integers-and-floating-point-numbers/#Arbitrary-Precision-Arithmetic-1)\n  (also called multiple precision) numbers with uncertainties.  This is useful\n  for measurements with very low relative error\n* Define arrays of measurements and perform calculations with them.  Some linear\n  algebra functions work out-of-the-box\n* Propagate uncertainty for any function of real arguments (including functions\n  based on\n  [C/Fortran calls](https://docs.julialang.org/en/v1/manual/calling-c-and-fortran-code/)),\n  using `@uncertain`\n  [macro](https://docs.julialang.org/en/v1/manual/metaprogramming/)\n* Function to get the derivative and the gradient of an expression with respect\n  to one or more independent measurements\n* Functions to calculate\n  [standard score](https://en.wikipedia.org/wiki/Standard_score) and\n  [weighted mean](https://en.wikipedia.org/wiki/Weighted_arithmetic_mean)\n* Parse strings to create measurement objects\n* Easy way to attach the uncertainty to a number using the `±` sign as infix\n  operator.  This syntactic sugar makes the code more readable and visually\n  appealing\n* Extensible in combination with external packages: you can propagate errors of\n  measurements with their physical units, perform numerical integration\n  with [`QuadGK.jl`](https://github.com/JuliaMath/QuadGK.jl), numerical and\n  automatic differentiation, and much more.\n* Integration with [`Plots.jl`](https://github.com/JuliaPlots/Plots.jl).\n\nThe method used to handle functional correlation is described in this paper:\n\n* M. Giordano, 2016, ""Uncertainty propagation with functionally correlated\n  quantities"", [arXiv:1610.08716](http://arxiv.org/abs/1610.08716)\n  (Bibcode:\n  [`2016arXiv161008716G`](http://adsabs.harvard.edu/abs/2016arXiv161008716G))\n\nA current limitation of the package is that it is not yet possible to define\nquantities related by a correlation matrix.\n\nIf you use use this package for your research, please cite it.\n\n### Documentation\n\nThe complete manual of `Measurements.jl` is available at\nhttps://juliaphysics.github.io/Measurements.jl/stable/.  There, people\ninterested in the details of the package, in order integrate the package in\ntheir workflow, can find a technical appendix explaining how the package\ninternally works.\n\n## Installation\n\nThe latest version of `Measurements.jl` is available for Julia v1.0 and later\nreleases, and can be installed with [Julia built-in package\nmanager](https://julialang.github.io/Pkg.jl/stable/).  In a Julia session, after\nentering the package manager mode with `]`, run the command\n\n```julia\npkg> update\npkg> add Measurements\n```\n\nOlder versions of this package are also available for Julia 0.4-0.7.\n\n## Usage\n\nAfter installing the package, you can start using it with\n\n```julia\nusing Measurements\n```\n\nThe module defines a new `Measurement` data type.  `Measurement` objects can be\ncreated with the two following constructors:\n\n``` julia\nmeasurement(value, uncertainty)\nvalue ± uncertainty\n```\n\nwhere\n\n* `value` is the nominal value of the measurement\n* `uncertainty` is its uncertainty, assumed to be a\n  [standard deviation](https://en.wikipedia.org/wiki/Standard_deviation).\n\nHere is a quick taster of the functionalities of the package:\n\n``` julia\njulia> using Measurements\n\njulia> a = measurement(4.5, 0.1)\n4.5 ± 0.1\n\njulia> b = 3.8 ± 0.4\n3.8 ± 0.4\n\njulia> 2a + b\n12.8 ± 0.4472135954999579\n\njulia> x = 8.4 ± 0.7\n\njulia> x - x\n0.0 ± 0.0\n\njulia> x/x\n1.0 ± 0.0\n\njulia> x*x*x - x^3\n0.0 ± 0.0\n\njulia> sin(x)/cos(x) - tan(x)\n-2.220446049250313e-16 ± 0.0 # They are equal within numerical accuracy\n```\n\nFor more details about the use of the package read the\n[documentation](https://juliaphysics.github.io/Measurements.jl/stable/), in\nparticular the\n[Usage](https://juliaphysics.github.io/Measurements.jl/stable/usage/) and\n[Examples](https://juliaphysics.github.io/Measurements.jl/stable/examples/)\nsections.\n\n## License\n\nThe `Measurements.jl` package is licensed under the MIT ""Expat"" License.  The\noriginal author is Mosè Giordano.\n\nPlease, cite the paper Giordano 2016 (http://arxiv.org/abs/1610.08716) if you\nemploy this package in your research work.  For your convenience, a BibTeX entry\nis provided in the [`CITATION.bib`](CITATION.bib) file.\n\n\n[docs-latest-img]: https://img.shields.io/badge/docs-latest-blue.svg\n[docs-latest-url]: https://juliaphysics.github.io/Measurements.jl/dev/\n\n[docs-stable-img]: https://img.shields.io/badge/docs-stable-blue.svg\n[docs-stable-url]: https://juliaphysics.github.io/Measurements.jl/stable/\n\n[gha-img]: https://github.com/JuliaPhysics/Measurements.jl/workflows/CI/badge.svg\n[gha-url]: https://github.com/JuliaPhysics/Measurements.jl/actions?query=workflow%3ACI\n\n[coveral-img]: https://coveralls.io/repos/github/JuliaPhysics/Measurements.jl/badge.svg?branch=master\n[coveral-url]: https://coveralls.io/github/JuliaPhysics/Measurements.jl?branch=master\n\n[codecov-img]: https://codecov.io/gh/JuliaPhysics/Measurements.jl/branch/master/graph/badge.svg\n[codecov-url]: https://codecov.io/gh/JuliaPhysics/Measurements.jl\n\n[aqua-img]: https://raw.githubusercontent.com/JuliaTesting/Aqua.jl/master/badge.svg\n[aqua-url]: https://github.com/JuliaTesting/Aqua.jl\n",480,physics,Julia,1,Julia,,,,,,,,,,,,,,,,,,,,,,,,,,,,93,6,82,5,11,23,143,21854,37,73,54,19,833064e4a4dca58be82ceaaa257d96a6b3708cfe,Bump julia-actions/cache from 1 to 2 (#169),2024-05-06T06:19:08Z,dependabot[bot],49699333+dependabot[bot]@users.noreply.github.com,dependabot[bot],v2.11.0,## Measurements v2.11.0\n\n[Diff since v2.10.0](https://github.com/JuliaPhysics/Measurements.jl/compare/v2.10.0...v2.11.0)\n\n\n**Merged pull requests:**\n- Attempt 2: Implement BaseType extension (#156) (@MilesCranmer)\n- Minor fixes (#157) (@pitmonticone)\n- Update MeasurementsJunoExt.jl (#159) (@longemen3000)\n- Release v2.11.0 (#160) (@giordano)\n- Add compat bounds for stdlibs (#161) (@giordano)\n\n**Closed issues:**\n- Trying to use Measurements to differentiate respect to a unitful quantity. (#151)\n- Broken `MeasurementsJunoExt.jl` (#158),v2.11.0,,,github-actions[bot],MIT License,Measurements.jl,JuliaPhysics,20,uncertainty-propagation,julia,error-propagation,physics,complex-numbers,uncertainties,multiprecision,arbitrary-precision,linear-algebra,numerical-integration,physical-quantities,,,,,,,,,,/JuliaPhysics/Measurements.jl,31,14,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/JuliaGraphics/Luxor.jl,https://github.com/JuliaGraphics/Luxor.jl,0,,,0,0,0,0,0,0,1,1,0,0,0,"Simple drawings using vector graphics; Cairo ""for tourists!""","![luxor splash image](docs/src/assets/figures/luxor-social-media-preview.png)\n\n| **Documentation**                       | **Build Status**                          | **Code Coverage**               |\n|:---------------------------------------:|:-----------------------------------------:|:-------------------------------:|\n| [![][docs-stable-img]][docs-stable-url] | [![Build Status][ci-img]][ci-url]         | [![][codecov-img]][codecov-url] |\n| [![][docs-development-img]][docs-development-url] | [![Build Status][appvey-img]][appvey-url] |                                 |\n\nPkgEval: [![PkgEval][pkgeval-img]][pkgeval-url]\n\n## Luxor\n\nLuxor is a Julia package for drawing simple static 2D vector graphics. It provides basic drawing functions and utilities for working with shapes, polygons, clipping masks, PNG and SVG images, turtle graphics, and simple animations.\n\n![""luxor gallery""](docs/src/assets/figures/luxorgallery.svg)\n\nThe focus of Luxor is on simplicity and ease of use: it should be easier to use than plain [Cairo.jl](https://github.com/JuliaLang/Cairo.jl), with shorter names, fewer underscores, default contexts, and simplified functions. \n\nFor more complex and sophisticated graphics in 2D and 3D, [Makie.jl](https://docs.makie.org/stable/) is the best choice.\n\nLuxor is thoroughly procedural and static: your code issues a sequence of simple graphics ‘commands’ until you’ve completed a drawing, then the results are saved into a PDF, PNG, SVG, or EPS file.\n\nTutorials can be found in the documentation, which you find by clicking on the badges above:\n![where is the documentation?](docs/src/assets/figures/where-is-the-documentation.png)\n\n“stable” describes the current release; “development” contains changes that are still in the master branch and may change before the next release.\n\nThere are some Luxor-related videos on [YouTube](https://www.youtube.com/channel/UCfd52kTA5JpzOEItSqXLQxg), and some Luxor-related blog posts at [cormullion.github.io/](https://cormullion.github.io/).\n\nLuxor is designed primarily for drawing static pictures and simple animations. If you want to build complex or elaborate animations, use [Javis.jl](https://github.com/JuliaAnimators/Javis.jl) and [Makie](https://docs.makie.org/stable/). \n\nLuxor isn't interactive: for building interactivity, look at [Pluto.jl](https://github.com/fonsp/Pluto.jl) and [Makie](https://docs.makie.org/stable/).\n\n[docs-development-img]: https://img.shields.io/badge/docs-development-blue\n[docs-development-url]: https://juliagraphics.github.io/LuxorManual/dev/\n\n[docs-stable-img]: https://img.shields.io/badge/docs-stable-blue.svg\n[docs-stable-url]: https://juliagraphics.github.io/LuxorManual/stable/\n\n[travis-img]: https://travis-ci.org/JuliaGraphics/Luxor.jl.svg?branch=master\n[travis-url]: https://travis-ci.org/JuliaGraphics/Luxor.jl\n\n[appvey-img]: https://ci.appveyor.com/api/projects/status/6pq9v30famcoe3dd?svg=true\n[appvey-url]: https://ci.appveyor.com/project/cormullion/luxor-jl/branch/master\n\n[codecov-img]: https://codecov.io/gh/JuliaGraphics/Luxor.jl/branch/master/graph/badge.svg\n[codecov-url]: https://codecov.io/gh/JuliaGraphics/Luxor.jl\n\n[ci-img]: https://github.com/cormullion/Luxor.jl/workflows/CI/badge.svg\n[ci-url]: https://github.com/cormullion/Luxor.jl/actions?query=workflow%3ACI\n\n[pkgeval-img]: https://juliaci.github.io/NanosoldierReports/pkgeval_badges/L/Luxor.svg\n[pkgeval-url]: https://juliaci.github.io/NanosoldierReports/pkgeval_badges/L/Luxor.html",576,graphics,Julia,1,Julia,,,,,,,,,,,,,,,,,,,,,,,,,,,,116,7,105,4,8,64,766,391519,72,202,192,10,3fdfebfcaa4693e76b8b970ac4f0794d318a5085,compat themes,2024-07-07T19:19:08Z,cormullion,cormullion@mac.com,cormullion,Release v4.0,"## This is a breaking release compared with v3.8:\r\n\r\n- Some 'invalid' Point methods have been removed:\r\n\r\n    - Broadcasting on xy-elements like `Point(x, y) .+ n` are no longer valid. Use `Point(x, y) + Point(n, n)`.\r\n\r\n    - `Point - Real` arithmetic operations such as `Point(x, y) + n` are also no longer valid.\r\n \r\n## Added\r\n\r\n- `textformat()`\r\n- `polysmooth()` has close option\r\n- `markcells()` and `getcells()`\r\n- use package extension for LaTeX support\r\n- add CompatHelper git workflow\r\n- add Aqua.jl testing\r\n- `createmovie` option for animate to make MKV and MP4 videos\r\n- `polybspline` draws bspline polygons\r\n\r\n## Changed\r\n\r\n- minimum Julia version 1.9\r\n- fixes for drawpath(p, f) to do the Bezier curve truncation better\r\n- added dependency PolygonAlgorithms.jl and replace poly intersection routines with new ones\r\n- Aqua says TOML deps must be in alphabetical order :)\r\n- remove @assert statements\r\n- documents now built to https://github.com/JuliaGraphics/LuxorManual\r\n- fixed bug in box(pt, w, h, cr, :path) (don't create new path)\r\n- removed some invalid Point methods (#294)\r\n- between has more methods for ranges and arrays\r\n\r\n## Removed\r\n\r\n- invalid Point methods such as `Point(1, 3) + 6` or `Point(1, 3) .+ 4`\r\n",v4.0.0,cormullion,,cormullion,Other,Luxor.jl,JuliaGraphics,76,graphics,vector-graphics,drawing,diagrams,julia,luxor,cairo,simple,2d-graphics,visualization,turtle-graphics,,,,,,,,,,/JuliaGraphics/Luxor.jl,82,14,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/JuliaGeometry/CoordinateTransformations.jl,https://github.com/JuliaGeometry/CoordinateTransformations.jl,0,,,0,0,0,0,0,0,1,1,0,0,0,A fresh approach to coordinate transformations...,"# CoordinateTransformations\n\n[![Build Status](https://github.com/JuliaGeometry/CoordinateTransformations.jl/workflows/CI/badge.svg)](https://github.com/JuliaGeometry/CoordinateTransformations.jl/actions?query=workflow%3ACI)\n\n**CoordinateTransformations** is a Julia package to manage simple or complex\nnetworks of coordinate system transformations. Transformations can be easily\napplied, inverted, composed, and differentiated (both with respect to the\ninput coordinates and with respect to transformation parameters such as rotation\nangle). Transformations are designed to be light-weight and efficient enough\nfor, e.g., real-time graphical applications, while support for both explicit\nand automatic differentiation makes it easy to perform optimization and\ntherefore ideal for computer vision applications such as SLAM (simultaneous\nlocalization and mapping).\n\nThe package provide two main pieces of functionality\n\n1. Primarily, an interface for defining `Transformation`s and applying\n   (by calling), inverting (`inv()`), composing (`∘` or `compose()`) and\n   differentiating (`transform_deriv()` and `transform_deriv_params()`) them.\n\n2. A small set of built-in, composable, primitive transformations for\n   transforming 2D and 3D points (optionally leveraging the *StaticArrays*\n   and *Rotations* packages).\n\n### Quick start\n\nLet's translate a 3D point:\n```julia\nusing CoordinateTransformations, Rotations, StaticArrays\n\nx = SVector(1.0, 2.0, 3.0)  # SVector is provided by StaticArrays.jl\ntrans = Translation(3.5, 1.5, 0.0)\n\ny = trans(x)\n```\n\nWe can either apply different transformations in turn,\n```julia\nrot = LinearMap(RotX(0.3))  # Rotate 0.3 radians about X-axis, from Rotations.jl\n\nz = trans(rot(x))\n```\nor build a composed transformation using the `∘` operator (accessible at the\nREPL by typing `\circ` then tab):\n```julia\ncomposed = trans ∘ rot  # alternatively, use compose(trans, rot)\n\ncomposed(x) == z\n```\nA composition of a `Translation` and a `LinearMap` results in an `AffineMap`.\n\nWe can invert the transformation:\n```julia\ncomposed_inv = inv(composed)\n\ncomposed_inv(z) == x\n```\n\nFor any transformation, we can shift the origin to a new point using `recenter`:\n```julia\nrot_around_x = recenter(rot, x)\n```\nNow `rot_around_x` is a rotation around the point `x = SVector(1.0, 2.0, 3.0)`.\n\n\nFinally, we can construct a matrix describing how the components of `z`\ndifferentiates with respect to components of `x`:\n```julia\n∂z_∂x = transform_deriv(composed, x) # In general, the transform may be non-linear, and thus we require the value of x to compute the derivative\n```\n\nOr perhaps we want to know how `y` will change with respect to changes of\nto the translation parameters:\n```julia\n∂y_∂θ = transform_deriv_params(trans, x)\n```\n\n### The interface\n\nTransformations are derived from `Transformation`. As an example, we have\n`Translation{T} <: Transformation`. A `Translation` will accept and translate\npoints in a variety of formats, such as `Vector` or `SVector`, but in general\nyour custom-defined `Transformation`s could transform any Julia object.\n\nTransformations can be reversed using `inv(trans)`. They can be chained\ntogether using the `∘` operator (`trans1 ∘ trans2`) or `compose` function (`compose(trans1, trans2)`).\nIn this case, `trans2` is applied first to the data, before `trans1`.\nComposition may be intelligent, for instance by precomputing a new `Translation`\nby summing the elements of two existing `Translation`s, and yet other\ntransformations may compose to the `IdentityTransformation`. But by default,\ncomposition will result in a `ComposedTransformation` object which simply\ndispatches to apply the transformations in the correct order.\n\nFinally, the matrix describing how differentials propagate through a transform\ncan be calculated with the `transform_deriv(trans, x)` method. The derivatives\nof how the output depends on the transformation parameters is accessed via\n`transform_deriv_params(trans, x)`. Users currently have to overload these methods,\nas no fall-back automatic differentiation is currently included. Alternatively,\nall the built-in types and transformations are compatible with automatic differentiation\ntechniques, and can be parameterized by *DualNumbers*' `DualNumber` or *ForwardDiff*'s `Dual`.\n\n### Built-in transformations\n\nA small number of 2D and 3D coordinate systems and transformations are included.\nWe also have `IdentityTransformation` and `ComposedTransformation`, which allows us\nto nest together arbitrary transformations to create a complex yet efficient\ntransformation chain.\n\n#### Coordinate types\n\nThe package accepts any `AbstractVector` type for Cartesian coordinates. For speed, we recommend\nusing a statically-sized container such as `SVector{N}` from *StaticArrays*.\n\nWe do provide a few specialist coordinate types. The `Polar(r, θ)` type is a 2D\npolar representation of a point, and similarly in 3D we have defined\n`Spherical(r, θ, ϕ)` and `Cylindrical(r, θ, z)`.\n\n#### Coordinate system transformations\n\nTwo-dimensional coordinates may be converted using these parameterless (singleton)\ntransformations:\n\n1. `PolarFromCartesian()`\n2. `CartesianFromPolar()`\n\nThree-dimensional coordinates may be converted using these parameterless\ntransformations:\n\n1. `SphericalFromCartesian()`\n2. `CartesianFromSpherical()`\n3. `SphericalFromCylindrical()`\n4. `CylindricalFromSpherical()`\n5. `CartesianFromCylindrical()`\n6. `CylindricalFromCartesian()`\n\nHowever, you may find it simpler to use the convenience constructors like\n`Polar(SVector(1.0, 2.0))`.\n\n#### Translations\n\nTranslations can be be applied to Cartesian coordinates in arbitrary dimensions,\nby e.g. `Translation(Δx, Δy)` or `Translation(Δx, Δy, Δz)` in 2D/3D, or by\n`Translation(Δv)` in general (with `Δv` an `AbstractVector`). Compositions of\ntwo `Translation`s will intelligently create a new `Translation` by adding the\ntranslation vectors.\n\n#### Linear transformations\n\nLinear transformations (a.k.a. linear maps), including rotations, can be\nencapsulated in the `LinearMap` type, which is a simple wrapper of an\n`AbstractMatrix`.\n\nYou are able to provide any matrix of your choosing, but your choice of type\nwill have a large effect on speed. For instance, if you know the dimensionality\nof your points (e.g. 2D or 3D) you might consider a statically sized matrix\nlike `SMatrix` from *StaticArrays.jl*. We recommend performing 3D rotations\nusing those from *Rotations.jl* for their speed and flexibility. Scaling will\nbe efficient with Julia's built-in `UniformScaling`. Also note that compositions\nof two `LinearMap`s will intelligently create a new `LinearMap` by multiplying\nthe transformation matrices.\n\n#### Affine maps\n\nAn Affine map encapsulates a more general set of transformation which are\ndefined by a composition of a translation and a linear transformation. An\n`AffineMap` is constructed from an `AbstractVector` translation `v` and an\n`AbstractMatrix` linear transformation `M`. It will perform the mapping\n`x -> M*x + v`, but the order of addition and multiplication will be more obvious\n(and controllable) if you construct it from a composition of a linear map\nand a translation, e.g. `Translation(v) ∘ LinearMap(v)` (or any combination of\n`LinearMap`, `Translation` and `AffineMap`).\n\n`AffineMap`s can be constructed to fit point pairs `from_points => to_points`:\n\n```julia\njulia> from_points = [[0, 0], [1, 0], [0, 1]];\n\njulia> to_points   = [[1, 1], [3, 1], [1.5, 3]];\n\njulia> AffineMap(from_points => to_points)\nAffineMap([1.9999999999999996 0.4999999999999999; -5.551115123125783e-16 2.0], [0.9999999999999999, 1.0000000000000002])\n```\n\nThe points can be supplied as a collection of vectors or as a matrix with points as columns.\n\n#### Perspective transformations\n\nThe perspective transformation maps real-space coordinates to those on a virtual\n""screen"" of one lesser dimension. For instance, this process is used to render\n3D scenes to 2D images in computer generated graphics and games. It is an ideal\nmodel of how a pinhole camera operates and is a good approximation of the modern\nphotography process.\n\nThe `PerspectiveMap()` command creates a `Transformation` to perform the\nprojective mapping. It can be applied individually, but is particularly\npowerful when composed with an `AffineMap` containing the position and\norientation of the camera in your scene. For example, to transfer `points` in 3D\nspace to 2D `screen_points` giving their projected locations on a virtual camera\nimage, you might use the following code:\n\n```julia\ncam_transform = PerspectiveMap() ∘ inv(AffineMap(cam_rotation, cam_position))\nscreen_points = map(cam_transform, points)\n```\n\nThere is also a `cameramap()` convenience function that can create a composed\ntransformation that includes the intrinsic scaling (e.g. focal length and pixel\nsize) and offset (defining which pixel is labeled `(0,0)`) of an imaging system.\n\n## Acknowledgements\n\n[![FugroRoames](https://avatars.githubusercontent.com/FugroRoames?s=150)](https://github.com/FugroRoames)\n",179,geometry,Julia,1,Julia,,,,,,,,,,,,,,,,,,,,,,,,,,,,52,5,47,0,12,31,0,208,25,43,27,16,692120de016194c5f4fb43eb2eb801cc57933b40,Merge pull request #94 from abhro/petite-output,2024-06-20T15:38:33Z,Steve Kelly,kd2cca@gmail.com,sjkelly,v0.6.3,## CoordinateTransformations v0.6.3\n\n[Diff since v0.6.2](https://github.com/JuliaGeometry/CoordinateTransformations.jl/compare/v0.6.2...v0.6.3)\n\n\n**Closed issues:**\n- Set of measured points to known coordinates (#30)\n- Make perspective transformations composable and invertable (#66)\n\n**Merged pull requests:**\n- Fix typo (#79) (@t-bltg)\n- Fix testing (#80) (@t-bltg)\n- Added convenience Polar() constructor (#84) (@mofii)\n- Fix equality/isapprox comparisons between different affine subtypes (#86) (@gpeairs)\n- Construct affine transforms from point-pairs (#87) (@timholy)\n- Fix test failure from Polar on Julia < 1.7 (#88) (@timholy)\n- Version 0.6.3 (#89) (@timholy),v0.6.3,,,github-actions[bot],Other,CoordinateTransformations.jl,JuliaGeometry,8,coordinate-transformations,geometry,julia,,,,,,,,,,,,,,,,,,/JuliaGeometry/CoordinateTransformations.jl,13,11,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/JuliaDynamics/DynamicalSystems.jl,https://github.com/JuliaDynamics/DynamicalSystems.jl,0,"Small, collection of software, general purpose tool",,0,1,0,0,0,0,1,0,0,0,0,Award winning software library for nonlinear dynamics and nonlinear timeseries analysis,"![DynamicalSystems.jl logo: The Double Pendulum](https://i.imgur.com/nFQFdB0.gif)\n\n[![](https://img.shields.io/badge/docs-online-blue.svg)](https://juliadynamics.github.io/DynamicalSystemsDocs.jl/dynamicalsystems/dev/)\n[![DocBuild](https://github.com/juliadynamics/DynamicalSystems.jl/workflows/CI/badge.svg)](https://github.com/JuliaDynamics/DynamicalSystems.jl/actions)\n[![DOI](http://joss.theoj.org/papers/10.21105/joss.00598/status.svg)](https://doi.org/10.21105/joss.00598)\n[![Textbook](https://img.shields.io/badge/Textbook-10.1007%2F978--3--030--91032--7-purple)](https://link.springer.com/book/10.1007/978-3-030-91032-7)\n[![Package Downloads](https://img.shields.io/badge/dynamic/json?url=http%3A%2F%2Fjuliapkgstats.com%2Fapi%2Fv1%2Ftotal_downloads%2FDynamicalSystems&query=total_requests&label=Downloads)](http://juliapkgstats.com/pkg/DynamicalSystems)\n\n\n**DynamicalSystems.jl** is an [award-winning](https://dsweb.siam.org/The-Magazine/Article/winners-of-the-dsweb-2018-software-contest) Julia software library for nonlinear dynamics and nonlinear timeseries analysis.\n\nTo install **DynamicalSystems.jl**, run `import Pkg; Pkg.add(""DynamicalSystems"")` as a Julia language command.\nTo learn how to use it and see its contents visit the documentation, which you can either find [online](https://juliadynamics.github.io/DynamicalSystems.jl/dev/) or build locally by running the `docs/make.jl` file.\n\n**DynamicalSystems.jl** is part of [JuliaDynamics](https://juliadynamics.github.io/JuliaDynamics/), an organization dedicated to creating high quality scientific software.\n\n## Highlights\n\nAspects of **DynamicalSystems.jl** that make it stand out among other codebases for nonlinear dynamics or nonlinear timeseries analysis are:\n\n- **Exceptional documentation**. All implemented algorithms provide a high-level scientific description of their functionality in their documentation string as well as references to scientific papers. The documentation features hundreds of tutorials and examples ranging from introductory to expert usage.\n- **Accessible source code**. One of the main priorities of the library is that the source code of (almost) all implementations is small, simple, easy to understand and modify. This increases confidence, reduces bugs, and allows users to become developers without unnecessary effort.\n- **Open source community project**. Built from the ground up entirely on GitHub, **DynamicalSystems.jl** is 100% open source and based on community contributions. Anyone can be a developer of the library. Everyone is welcomed.\n- **Extensive content**. It aims to cover the entire field of nonlinear dynamics and nonlinear timeseries analysis. It has functionality for complexity measures, delay embeddings, periodic orbits, nonlocal stability analysis, continuation, chaos, fractal dimensions, surrogate testing, recurrence quantification analysis, and much more. Furthermore, all algorithms are ""general"" and work for any dynamical system applicable. Missing functionality that falls under this wide category of content is welcomed to be part of the library!\n- **Well tested**. All implemented functionality is extensively tested. Each time any change in the code base is done, the extensive test suite is run and checked before merging the change in.\n- **Extendable**. **DynamicalSystems.jl** is a living, evolving project. New contributions can become part of the library and be accessed by all users in the next release. Most importantly, all parts of the library follow professional standards in software design and implement extendable interfaces so that it is easy to contribute new functionality.\n- **Active development**. Since the start of the project (May 2017) there has been  activity every month: new features, bugfixes, and the developer team answers users questions on official Julia language forums.\n- **Performant**. Written entirely in Julia, heavily optimized and parallelized, and taking advantage of some of the best packages within the language, **DynamicalSystems.jl** is _really fast_.\n\n## Goals\n\nThe **DynamicalSystems.jl** library started as a vision with three main goals;\nThese same goals now are the core pillars guiding development, and are largely the source of where the aforementioned unique highlights stem from.\n\n### Goal 1: Accessible and reproducible nonlinear dynamics\n\nThe first goal of the library is to make this beautiful field **accessible and reproducible**.\n\n**Accessible** means that if you read on some sorts of fancy algorithm online in a scientific article, you should be able to use it instantly. You shouldn't have to put in the work to code it yourself. The authors of the paper already did that.\n_So why should you do it again?!_ To resolve this problem we developed, and continue to develop, a library that has an incredibly low threshold of entry: contributing to **DynamicalSystems.jl** and making your code available to all is truly _easier_ than coding your own algorithms from scratch, due to the well thought out and generic interfaces it provides for dynamical systems.\n\n**Reproducible** means that given some sorts of dynamical systems analysis in a scientific article, you should be able to do _exactly the same analysis_ and get _exactly the same results_ (within some numeric precision) as the article.\nAfter all, computers are deterministic constructs.\n**DynamicalSystems.jl** allows this by (1) being written in a modern programming language with incredible environment and reproducibility support, (2) being well tested, and (3) by providing thousands of algorithms out of the box, allowing most dynamical systems analysis to be done instantly while implementing only as little new stuff as necessary.\n\n### Goal 2: Library in the literal sense\n\n**DynamicalSystems.jl** is not just a software library. It is also a library in the literal sense: _where people go to learn something new_ (here in particular for nonlinear dynamics).\nThat is why the documentation is of exceptionally high quality: detailed descriptions and explanations of algorithms, with references to the scientific articles articles. It is also partly a reason for the source code to be written as clearly as possible, so that it is examinable by any user.\n\n### Goal 3: A general purpose software\n\nThe third goal is to fill the missing gap of a high quality _general purpose software_ for nonlinear dynamics which can be easily extended with new functionality. This can be particularly impactful in teaching.\nYou see, it is unfortunately rarely the case that real, _runnable_ code is shown in the classroom, because it is often long and messy. This is especially hurtful for nonlinear dynamics, a field where computer-assisted exploration is critical.\n\n**DynamicalSystems.jl** provides teachers with a framework capable of demonstrating actual, real-world nonlinear dynamics code and its output, without having to invest the weeks to code the internal infrastructure themselves.\nIts high level syntax requires writing little code to get lots of meaningful analysis done, while its extensive functionality covers most typical classroom applications.\n",828,mathematics,Julia,2,Julia,TeX,,,,,,,,,,,,,,,,,,,,,,,,,,,124,24,100,0,2,43,663,350037,90,110,100,10,8a29a50384022c9a1c93156a3b3c324d6dd4a84e,Update links to SciMLSensitivity,2024-07-10T12:49:31Z,George Datseris,datseris.george@gmail.com,Datseris,v3.3.17,## DynamicalSystems v3.3.17\n\n[Diff since v3.3.16](https://github.com/JuliaDynamics/DynamicalSystems.jl/compare/v3.3.16...v3.3.17),v3.3.17,,,github-actions[bot],Other,DynamicalSystems.jl,JuliaDynamics,78,physics,mathematics,dynamical-systems,julia,lyapunov,dimension,nonlinear,chaos,julialang,entropy,attractor,nonlinear-dynamics,delay-coordinates,hacktoberfest,,,,,,,/JuliaDynamics/DynamicalSystems.jl,79,25,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/JuliaDynamics/ChaosTools.jl,https://github.com/JuliaDynamics/ChaosTools.jl,0.5,tool set,0,0,1,0,0,0,0,1,0,0,0,0,Tools for the exploration of chaos and nonlinear dynamics,"# ChaosTools.jl\n\n[![docsdev](https://img.shields.io/badge/docs-dev-lightblue.svg)](https://juliadynamics.github.io/DynamicalSystemsDocs.jl/chaostools/dev/)\n[![docsstable](https://img.shields.io/badge/docs-stable-blue.svg)](https://juliadynamics.github.io/DynamicalSystemsDocs.jl/chaostools/stable/)\n[![](https://img.shields.io/badge/DOI-10.1007%2F978--3--030--91032--7-purple)](https://link.springer.com/book/10.1007/978-3-030-91032-7)\n[![CI](https://github.com/JuliaDynamics/ChaosTools.jl/workflows/CI/badge.svg)](https://github.com/JuliaDynamics/ChaosTools.jl/actions?query=workflow%3ACI)\n[![codecov](https://codecov.io/gh/JuliaDynamics/ChaosTools.jl/branch/main/graph/badge.svg)](https://codecov.io/gh/JuliaDynamics/ChaosTools.jl)\n[![Package Downloads](https://shields.io/endpoint?url=https://pkgs.genieframework.com/api/v1/badge/ChaosTools)](https://pkgs.genieframework.com?packages=ChaosTools)\n\nA Julia module that offers various tools for analysing nonlinear dynamics and chaotic behaviour.\nIt can be used as a standalone package, or as part of [DynamicalSystems.jl](https://juliadynamics.github.io/DynamicalSystemsDocs.jl/dynamicalsystems/dev/).\n\nTo install it, run `import Pkg; Pkg.add(""ChaosTools"")`.\n\nAll further information is provided in the documentation, which you can either find online or build locally by running the `docs/make.jl` file.\n\n_ChaosTools.jl is the jack-of-all-trades package of the DynamicalSystems.jl library: methods that are not extensive enough to be a standalone package are added here. You should see the full DynamicalSystems.jl library for other packages that may contain functionality you are looking for but did not find in ChaosTools.jl._",186,physics,Julia,1,Julia,,,,,,,,,,,,,,,,,,,,,,,,,,,,190,38,152,0,3,33,41,9607,35,137,113,24,0ecb479b4f4034e2c6db45c3e9bae4a319d41eee,Store UPOs in a set & test possibly wrong (#334),2024-06-11T20:30:49Z,Jonáš Koziorek,73384756+JonasKoziorek@users.noreply.github.com,JonasKoziorek,v3.1.2,## ChaosTools v3.1.2\n\n[Diff since v3.1.1](https://github.com/JuliaDynamics/ChaosTools.jl/compare/v3.1.1...v3.1.2)\n\n\n**Merged pull requests:**\n- Bump actions/checkout from 3 to 4 (#313) (@dependabot[bot])\n- Bump styfle/cancel-workflow-action from 0.11.0 to 0.12.0 (#314) (@dependabot[bot]),v3.1.2,,,github-actions[bot],MIT License,ChaosTools.jl,JuliaDynamics,118,physics,nonlinear,chaos,entropy,dynamical-systems,lyapunov,julia,dimension,attractor,dynamicalsystems,hacktoberfest,,,,,,,,,,/JuliaDynamics/ChaosTools.jl,119,11,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/jsxgraph/jsxgraph,https://github.com/jsxgraph/jsxgraph,0,,,0,0,0,0,0,0,1,1,0,0,0,"JSXGraph is a cross-browser library for interactive geometry, function plotting, charting, and data visualization in a web browser.","JSXGraph\n========\n\nJavaScript library for interactive math visualizations in the web browser.\n\nAbout\n-----\n\n*JSXGraph* is a cross-browser library for interactive geometry, function plotting,\ncharting, and data visualization in a web browser. It is implemented completely\nin JavaScript, does not rely on any other library, and uses SVG, canvas, or even the venerable VML.\n*JSXGraph* is easy to embed and has a small footprint: approx. 160 KByte if\nembedded in a web page. No plug-ins are required! Special care has been taken\nto optimize the performance.\n\n*JSXGraph* supports multi-touch events and runs on all major browsers, even on very old IEs.\n\n*JSXGraph* is developed at the\nLehrstuhl für Mathematik und ihre Didaktik\nUniversity of Bayreuth, Germany\n\nWebsite\n-------\n\n- Project web site: https://jsxgraph.org/\n- Project wiki with hundreds of examples: https://jsxgraph.org/wiki/\n- GitHub project site: https://github.com/jsxgraph/jsxgraph\n- Mailing List/Google Group: https://groups.google.com/group/jsxgraph\n- JSXGraph questions at https://stackoverflow.com/search?tab=newest&q=jsxgraph\n- jsFiddle template: https://jsfiddle.net/8kep9syd/\n- YouTube channel: https://www.youtube.com/channel/UCANBFoVoOyW2eNyTvx-VZdQ\n- Moodle filter: https://github.com/jsxgraph/moodle-filter_jsxgraph\n- (outdated: SourceForge project site: https://sf.net/projects/jsxgraph)\n- CDNs: Embed JSXGraph via\n\n```html\n<script type=""text/javascript"" charset=""UTF-8""\n src=""https://cdn.jsdelivr.net/npm/jsxgraph/distrib/jsxgraphcore.js""></script>\n<link rel=""stylesheet""\n type=""text/css"" href=""https://cdn.jsdelivr.net/npm/jsxgraph/distrib/jsxgraph.css"" />\n ```\n\nor\n\n```html\n<script type=""text/javascript"" charset=""UTF-8""\n src=""//cdnjs.cloudflare.com/ajax/libs/jsxgraph/1.4.6/jsxgraphcore.js""></script>\n<link rel=""stylesheet""\n type=""text/css"" href=""//cdnjs.cloudflare.com/ajax/libs/jsxgraph/1.4.6/jsxgraph.css"" />\n```\n\nPlease report bugs to our issue tracking system found at\nhttps://github.com/jsxgraph/jsxgraph/issues\n\nUsage\n-----\n\nInclude\n\n- `jsxgraphcore.js` and\n- `jsxgraph.css` and,\n- if required, one or more file readers\n\nfrom a CDN or a local version in your HTML file.\n\n**HTML template:**\n\n```html\n<!doctype html>\n<html lang=""en"">\n  <head>\n    <meta charset=""UTF-8"">\n    <title>JSXGraph template</title>\n    <meta content=""text/html; charset=utf-8"" http-equiv=""Content-Type"">\n    <link href=""https://cdn.jsdelivr.net/npm/jsxgraph/distrib/jsxgraph.css"" rel=""stylesheet"" type=""text/css"" />\n    <script src=""https://cdn.jsdelivr.net/npm/jsxgraph/distrib/jsxgraphcore.js"" charset=""UTF-8""></script>\n\n    <!-- The next line is optional: MathJax -->\n    <script src=""https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"" id=""MathJax-script"" async></script>\n  </head>\n  <body>\n\n  <div id=""jxgbox"" class=""jxgbox"" style=""max-width:800px; aspect-ratio: 1/1;""></div>\n\n  <script>\n    var board = JXG.JSXGraph.initBoard('jxgbox', {boundingbox: [-8, 8, 8, -8]});\n    var p = board.create('point', [1, 3], {name: 'point'});\n  </script>\n\n  </body>\n</html>\n```\n\nFor developing content, it is recommended to include `jsxgraphsrc.js` (`jsxgraphcore.js` is the minified version of `jsxgraphsrc.js`).\nFor further usage instructions please consult our [wiki](https://jsxgraph.org/wiki/)\nespecially our [tutorials](https://jsxgraph.org/wiki/index.php/Documentation)\nor [the API reference docs](https://jsxgraph.org/docs/).\n\nBuild and develop JSXGraph\n--------------\n\n1) Clone this repository or download the zip file.\n2) To build and develop *JSXGraph* you need [node.js](https://nodejs.org/) v0.6+. First, install all\ndependencies required to build JSXGraph using npm in the JSXGraph root directory: `$ npm install`.\nThis will create a new subdirectory ```node_modules``` in the JSXGraph root directory which holds\nall tools and libraries required to build ```jsxgraphcore.js```.\n3) To build JSXGraph run `$ npm run buildCore`. This will output an non-minified version `jsxgraphsrc.js`\nand the minified version `jsxgraphcore.js` into the folder `distrib`.\n4) Develop JSXGraph:\n  - Edit the source files in the folder `src`\n  - Write unit tests in folder `test`\n  - Run `$ npm run eslint` and `$ npm run test` to check for errors\n  - Run `$ npm run check-format` to check the formatting of the source code\n  - Submit a pull request\n\n\nLicense\n-------\n\nJSXGraph is free software dual licensed under the GNU LGPL or MIT License.\n\nYou can redistribute it and/or modify it under the terms of the\n\n- GNU Lesser General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version\n  \nor\n\n- MIT License: https://github.com/jsxgraph/jsxgraph/blob/master/LICENSE.MIT\n\nJSXGraph is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Lesser General Public License for more details.\n\nYou should have received a copy of the GNU Lesser General Public License and\nthe MIT License along with JSXGraph. If not, see <https://www.gnu.org/licenses/>\nand <https://opensource.org/licenses/MIT/>.\n\n[![ITEMS](img/items_logo_blue.png)](https://itemspro.eu)\n[![Cofunded by the Erasmus+ programme of the European union](img/eu_flag_co_funded_pos_rgb_left_small.jpg)](https://ec.europa.eu/programmes/erasmus-plus/)\n",1068,mathematics,JavaScript,6,Python,JavaScript,CSS,PHP,Makefile,Shell,,,,,,,,,,,,,,,,,,,,,,,214,27,187,0,11,51,0,117080,177,475,387,88,9a7b4a3b8344f964e555a8559e530c0581c88c8b,Merge pull request #689 from StephanvandenBerkmortel/robust_getLabelA…,2024-07-18T14:06:10Z,alfredwassermann,alfred.wassermann@uni-bayreuth.de,alfredwassermann,Release of v1.9.2,"JSXGraph v1.9.2 is the next patch release, fixing a couple of errors, but also introducing a few new features.\r\n\r\nThis release contains incremental improvements of 3D handling. The trackball navigation has been much improved, there is now a third slider `bank` to control the view port, and first attempts to order elements according to their z-value have been implemented. Thanks go to @Vectornaut for doing this incredibly valuable work. \r\n\r\nBeside 3D support, there is a new attribute `nonnegativeOnly` which controls how a negative circle radius or negative segment length should be treated. Previously, the absolute value was taken. Now, also the maximum of 0 and this value can be taken.\r\n\r\nThe most notable fixes are addressing a regression in construction of parallelograms,  displaying of axis labels as fractions, automatic label placement, infinite growing of a JSXGraph container, and tangents on arcs and sectors. Moreover, printing support has been improved, as well as resizing of the JSXGraph container in general.\r\n\r\nIf there are new regressions, please, do not hesitate to submit a report on [github](https://github.com/jsxgraph/jsxgraph/).\r\n\r\nEnjoy, Alfred",v1.9.2,,,alfredwassermann,Other,jsxgraph,jsxgraph,30,geometry,mathematics,jsxgraph,javascript,data-visualization,charting,,,,,,,,,,,,,,,/jsxgraph/jsxgraph,116,55,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/jgraph/drawio-desktop,https://github.com/jgraph/drawio-desktop,0,,,0,0,0,0,0,0,1,1,0,0,0,Official electron build of draw.io,"About\n----- \n\n**drawio-desktop** is a diagramming and whiteboarding desktop app based on [Electron](https://electronjs.org/) that wraps the [core draw.io editor](https://github.com/jgraph/drawio).\n\nDownload built binaries from the [releases section](https://github.com/jgraph/drawio-desktop/releases).\n\n**Can I use this app for free?** Yes, under the apache 2.0 license. If you don't change the code and accept it is provided ""as-is"", you can use it for any purpose.\n\nSecurity\n--------\n\ndraw.io Desktop is designed to be completely isolated from the Internet, apart from the update process. This checks github.com at startup for a newer version and downloads it from an AWS S3 bucket owned by Github. All JavaScript files are self-contained, the Content Security Policy forbids running remotely loaded JavaScript.\n\nNo diagram data is ever sent externally, nor do we send any analytics about app usage externally. This means certain functionality for which we do not have a JavaScript implementation do not work in the Desktop build, namely .vsd and Gliffy import.\n\nSecurity and isolating the app are the primarily objectives of draw.io desktop. If you ask for anything that involves external connections enabled in the app by default, the answer will be no.\n\nSupport\n-------\n\nSupport is provided on a reasonable business constraints basis, but without anything contractually binding. All support is provided via this repo. There is no private ticketing support.\n\nPurchasing draw.io for Confluence or Jira does not entitle you to commercial support for draw.io desktop. The draw.io integrations for Atlassian are sold by Seibert Media, they have no involvement with this project.\n\nDeveloping\n----------\n\n**draw.io** is a git submodule of **drawio-desktop**. To get both you need to clone recursively:\n\n`git clone --recursive https://github.com/jgraph/drawio-desktop.git`\n\nTo run this:\n1. `npm install` (in the root directory of this repo)\n2. export DRAWIO_ENV=dev if you want to develop/debug in dev mode.\n3. `npm start` _in the root directory of this repo_ runs the app. For debugging, use `npm start --enable-logging`.\n\nNote: If a symlink is used to refer to drawio repo (instead of the submodule), then symlink the `node_modules` directory inside `drawio/src/main/webapp` also.\n\nTo release:\n1. Update the draw.io sub-module and push the change. Add version tag before pushing to origin.\n2. Wait for the builds to complete (https://travis-ci.org/jgraph/drawio-desktop and https://ci.appveyor.com/project/davidjgraph/drawio-desktop)\n3. Go to https://github.com/jgraph/drawio-desktop/releases, edit the preview release.\n4. Download the windows exe and windows portable, sign them using `signtool sign /a /tr http://rfc3161timestamp.globalsign.com/advanced /td SHA256 c:/path/to/your/file.exe`\n5. Re-upload signed file as `draw.io-windows-installer-x.y.z.exe` and `draw.io-windows-no-installer-x.y.z.exe`\n6. Add release notes\n7. Publish release\n\n*Note*: In Windows release, when using both x64 and is32 as arch, the result is one big file with both archs. This is why we split them.\n\nLocal Storage and Session Storage is stored in the AppData folder:\n\n- macOS: `~/Library/Application Support/draw.io`\n- Windows: `C:\Users\<USER-NAME>\AppData\Roaming\draw.io\`\n\nNot open-contribution\n---------------------\n\ndraw.io is closed to contributions.\n\nThe level of complexity of this project means that even simple changes \ncan break a _lot_ of other moving parts. The amount of testing required \nis far more than it first seems. If we were to receive a PR, we'd have \nto basically throw it away and write it how we want it to be implemented.\n\nWe are grateful for community involvement, bug reports, & feature requests. We do\nnot wish to come off as anything but welcoming, however, we've\nmade the decision to keep this project closed to contributions for \nthe long term viability of the project.\n",48178,graphics,JavaScript,2,JavaScript,Shell,,,,,,,,,,,,,,,,,,,,,,,,,,,35,15,19,1,8,14,0,2611,4833,1430,1321,109,7e6d2b0688c833c070752ba609374282ebe681e6,Merge commit '050b0590067aaa6389ffd282593546768e59372f' into dev,2024-06-27T17:34:37Z,David Benson,david@draw.io,davidjgraph,24.06.2004,"## Releases Notes for 24.6.4\r\n[Windows Installer](https://github.com/jgraph/drawio-desktop/releases/download/v24.6.4/draw.io-24.6.4-windows-installer.exe)\r\n[Windows No Installer](https://github.com/jgraph/drawio-desktop/releases/download/v24.6.4/draw.io-24.6.4-windows-no-installer.exe)\r\n[macOS - Universal](https://github.com/jgraph/drawio-desktop/releases/download/v24.6.4/draw.io-universal-24.6.4.dmg)\r\nLinux - [deb](https://github.com/jgraph/drawio-desktop/releases/download/v24.6.4/drawio-amd64-24.6.4.deb), [AppImage](https://github.com/jgraph/drawio-desktop/releases/download/v24.6.4/drawio-x86_64-24.6.4.AppImage) or [rpm](https://github.com/jgraph/drawio-desktop/releases/download/v24.6.4/drawio-x86_64-24.6.4.rpm)\r\n\r\nWindows intel x32 releases are marked -ia32-\r\n\r\n**ChangeLog:**\r\n\r\n- #1783 , #1784 , #1792\r\n- Uses electron 30.0.6.\r\n- Updates to [draw.io core 24.6.4](https://github.com/jgraph/drawio/blob/v24.6.4/ChangeLog). All changes from 24.6.2 to 24.6.4 are added in this build.",v24.6.4,,,github-actions[bot],Apache License 2.0,drawio-desktop,jgraph,140,javascript-applications,electron-app,graphics,diagram-editor,,,,,,,,,,,,,,,,,/jgraph/drawio-desktop,192,523,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/JetBrains/lets-plot-kotlin,https://github.com/JetBrains/lets-plot-kotlin,0,,,0,0,0,0,0,0,1,1,0,0,0,Grammar of Graphics for Kotlin,"# Lets-Plot Kotlin API\nA **Grammar of Graphics** for Kotlin.\n\n[![official JetBrains project](http://jb.gg/badges/official-flat-square.svg)](https://confluence.jetbrains.com/display/ALL/JetBrains+on+GitHub)\n[![License MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://raw.githubusercontent.com/JetBrains/lets-plot-kotlin/master/LICENSE)\n[![Latest Release](https://img.shields.io/github/v/release/JetBrains/lets-plot-kotlin)](https://github.com/JetBrains/lets-plot-kotlin/releases/latest)\n\n**Lets-Plot Kotlin API** is a <a href=""https://lets-plot.org/kotlin"">Kotlin API</a> for [Lets-Plot Multiplatform](https://github.com/JetBrains/lets-plot) \n plotting library, \\nwhich is built on the principles of layered graphics first described in the \\nLeland Wilkinson work [The Grammar of Graphics](https://www.goodreads.com/book/show/2549408.The_Grammar_of_Graphics).\n\n<table>\n    <tr>\n        <td>\n            <a href=""https://ggplot2-book.org/index.html"" target=""_blank""> \n               <img src=""https://raw.githubusercontent.com/JetBrains/lets-plot-kotlin/master/docs/images/ggplot2-elegant-graphics-for-data-analysis.jpeg"" \n                    width=""150"" height=""228"" alt=""book cover"">\n            </a>\n        </td>\n        <td>\n            <p>Lets-Plot <a href=""https://lets-plot.org/kotlin"">Kotlin API</a> is largely based on the API<br>provided by \n            <a href=""https://ggplot2.tidyverse.org/"">ggplot2</a> package well-known to data scientists who use R.</p>\n            <p>To learn more about the <i>Grammar of Graphics</i>,<br>we recommend an excellent book called<br> \n            <a href=""https://ggplot2-book.org/index.html"" target=""_blank"">“ggplot2: Elegant Graphics for Data Analysis”</a>.</p> \n            <p>This will be a good prerequisite for further exploration of the Lets-Plot library.</p>\n        </td>  \n    </tr>\n</table>\n\n\n<a id=""quickstart""></a>\n### Quickstart\n\nInside [Kotlin Notebook](https://plugins.jetbrains.com/plugin/16340-kotlin-notebook),\n[Datalore](https://datalore.jetbrains.com/) or\n[Jupyter with Kotlin Kernel](https://github.com/Kotlin/kotlin-jupyter#readme):\n\n```\n%use lets-plot\n```     \n\n```kotlin\nval rand = java.util.Random()\nval data = mapOf(\n    ""rating"" to List(200) { rand.nextGaussian() } + List(200) { rand.nextGaussian() * 1.5 + 1.5 },\n    ""cond"" to List(200) { ""A"" } + List(200) { ""B"" }\n)\n\nvar p = letsPlot(data)\np += geomDensity(color = ""dark_green"", alpha = .3) { x = ""rating""; fill = ""cond"" }\np + ggsize(700, 350)\n```\n\n<img src=""https://raw.githubusercontent.com/JetBrains/lets-plot-kotlin/master/docs/images/quickstart_notebook.png"" alt=""Couldn't load quickstart_notebook.png"" width=""523"" height=""261""/>\n<br/>\n\nSee the ""Quickstart"" notebook in [Datalore](https://datalore.jetbrains.com/view/notebook/aTA9lQnPkRwdCzT6uy95GZ) or\n[Jupyter nbviewer](https://nbviewer.jupyter.org/github/JetBrains/lets-plot-kotlin/blob/master/docs/examples/jupyter-notebooks/quickstart.ipynb).\n\n\n<a name=""toc"" id=""toc""></a>\n## Table of Contents\n             \n- [Usage](#usage)\n  - [Notebooks](#in-notebook)\n  - [Compose Multiplatform](#in-compose-multiplatform)\n  - [JVM and Kotlin/JS](#in-jvm-js)\n- [Documentation](#documentation)\n- [What is new in 4.7.0](#new)\n- [Recent Updates in the Gallery](#recent_gallery_updates)\n- [Change Log](#change_log)\n- [Code of Conduct](#CoC)\n- [License](#license)\n                          \n\n\n<a id=""usage""></a>\n## Usage\n\n<a id=""in-notebook""></a>\n### Notebooks\n\nWith the help of Lets-Plot Kotlin API you can easily create plots in [Kotlin Notebook](https://plugins.jetbrains.com/plugin/16340-kotlin-notebook),\n[Datalore](https://datalore.jetbrains.com/), [Jupyter with Kotlin Kernel](https://github.com/Kotlin/kotlin-jupyter#readme) \\nor any other notebook that supports `Kotlin Kernel`.\n\n\n#### ""Line Magics""\n\n```\n%use lets-plot\n```  \nThis ""line magic"" will apply **Lets-Plot library descriptor** which adds to your notebook all the boilerplate code necessary to create plots.\n\nBy default, `library descriptor` is bundled with the Kotlin Jupyter Kernel installed in your environment. \\nHowever, you can override the default settings using:\n```\n%useLatestDescriptors\n```\nIn this case the latest `library descriptor` will be pulled from the [Kotlin Jupyter Libraries](https://github.com/Kotlin/kotlin-jupyter-libraries) repository.\n\n#### Library Descriptor Parameters\n\n```\n%use lets-plot(api=4.7.3, lib=4.3.3, js=4.3.3, isolatedFrame=false)\n```                                                                 \n- `api` - version of the Lets-Plot Kotlin API.\n- `lib` - version of the Lets-Plot Multiplatform (JARs).\n- `js`  - version of the Lets-PLot Multiplatform JavaScript bundle.\n- `isolatedFrame` - If `false`: load JS just once per notebook (default in Jupyter).\n  If `true`: include Lets-Plot JS in each output (default in [Datalore](https://datalore.jetbrains.com/) notebooks).\n\n\n<a id=""in-compose-multiplatform""></a>\n### Compose Multiplatform\nTo learn how to embed Lets-Plot charts in [Compose Multiplatform](https://github.com/JetBrains/compose-multiplatform) applications, please check out the [Lets-Plot Skia Frontend](https://github.com/JetBrains/lets-plot-skia) project.\n   \n\n<a id=""in-jvm-js""></a>\n### JVM and Kotlin/JS\n\nTo learn more about creating plots in JVM or Kotlin/JS environment please read [USAGE_SWING_JFX_JS.md](https://github.com/JetBrains/lets-plot-kotlin/blob/master/USAGE_BATIK_JFX_JS.md). \n        \n#### Examples\nExamples of using of the Lets-Plot Kotlin API in JVM and Kotlin/JS applications are available in the [Lets-Plot Kotlin Mini Apps (Demos)](https://github.com/alshan/lets-plot-mini-apps) GitHub repository.\n\n<a id=""documentation""></a>\n## Documentation\n\n* _Lets-Plot Kotlin API_ documentation and API reference: [**Lets-Plot for Kotlin**](https://lets-plot.org/kotlin)\n\n* A quick introduction to the _Grammar of Graphics_ and _Lets-Plot Kotlin API_: [Lets-Plot Usage Guide](https://nbviewer.jupyter.org/github/JetBrains/lets-plot-kotlin/blob/master/docs/guide/user_guide.ipynb) \n\n\n<a id=""new""></a>\n## What is new in 4.7.0\n\n- #### `coordPolar()`\n\n  The polar coordinate system is most commonly used for pie charts, but</br>\n  it can also be used for constructing **Spider or Radar charts** using the `flat` option.\n\n    <br>\n    <img src=""https://raw.githubusercontent.com/JetBrains/lets-plot/master/docs/f-24a/images/polar_coord_pie.png"" alt=""f-24a/images/polar_coord_pie.png"" width=""256"" height=""214"">\n    <img src=""https://raw.githubusercontent.com/JetBrains/lets-plot/master/docs/f-24a/images/radar_chart.png"" alt=""f-24a/images/radar_chart.png"" width=""256"" height=""196"">\n\n  See: [example notebook](https://nbviewer.org/github/JetBrains/lets-plot-kotlin/blob/master/docs/examples/jupyter-notebooks/f-4.7.0/coord_polar.ipynb).\n\n- #### In the `theme()`:\n\n  - `panelInset`  parameter - primarily used for plots with polar coordinates.\n    See: [example notebook](https://nbviewer.org/github/JetBrains/lets-plot-kotlin/blob/master/docs/examples/jupyter-notebooks/f-4.7.0/theme_panel_inset.ipynb).\n\n  - `panelBorderOntop` parameter - enables the drawing of panel border on top of the plot geoms.\n  - `panelGridOntop, panelGridOntopX, panelGridOntopY` parameters - enable the drawing of grid lines on top of the plot geoms.\n\n- #### `geomCurve()`\n\n    <br>\n    <img src=""https://raw.githubusercontent.com/JetBrains/lets-plot/master/docs/f-24a/images/curve_annotation.png"" alt=""f-24a/images/curve_annotation.png"" width=""338"" height=""296"">\n\n  See: [example notebook](https://nbviewer.org/github/JetBrains/lets-plot-kotlin/blob/master/docs/examples/jupyter-notebooks/f-4.7.0/geom_curve.ipynb).\n\n- #### [**UNIQUE**] Visualizing Graph-like Data with `geomSegment()` and `geomCurve()`\n\n  - Aesthetics `sizeStart, sizeEnd, strokeStart` and `strokeEnd` enable better alignment of</br>\n    segments/curves with nodes of the graph by considering the size of the nodes.\n\n  - The `spacer` parameter allows for additional manual fine-tuning.\n\n    <br>\n    <img src=""https://raw.githubusercontent.com/JetBrains/lets-plot/master/docs/f-24a/images/graph_simple.png"" alt=""f-24a/images/graph_simple.png"" width=""384"" height=""252"">\n\n  See: [example notebook](https://nbviewer.org/github/JetBrains/lets-plot-kotlin/blob/master/docs/examples/jupyter-notebooks/f-4.7.0/graph_edges.ipynb).\n\n- #### The `alphaStroke` Parameter in `geomLabel()`\n\n  Use the `alphaStroke` parameter to apply `alpha` to entire `label`. By default, `alpha` is only applied to the label background.\n\n  See: [example notebook](https://nbviewer.org/github/JetBrains/lets-plot-kotlin/blob/master/docs/examples/jupyter-notebooks/f-4.7.0/geom_label_alpha_stroke.ipynb).\n\n<a id=""recent_gallery_updates""></a>\n## Recent Updates in the [Gallery](https://lets-plot.org/kotlin/gallery.html)\n\n  <a href=""https://nbviewer.org/github/JetBrains/lets-plot-docs/blob/master/source/kotlin_examples/demo/geotools_naturalearth.ipynb"">\n    <img src=""https://raw.githubusercontent.com/JetBrains/lets-plot-kotlin/master/docs/images/gal_nat_earth.png"" alt=""images/gal_nat_earth.png"" width=""128"" height=""128"">\n  </a>\n  <a href=""https://nbviewer.org/github/JetBrains/lets-plot-docs/blob/master/source/kotlin_examples/demo/mpg_daisy.ipynb"">\n    <img src=""https://raw.githubusercontent.com/JetBrains/lets-plot-kotlin/master/docs/images/gal_mpg_daisy.png"" alt=""images/gal_mpg_daisy.png"" width=""128"" height=""128"">\n  </a>\n  <a href=""https://nbviewer.org/github/JetBrains/lets-plot-docs/blob/master/source/kotlin_examples/demo/palmer_penguins.ipynb"">\n    <img src=""https://raw.githubusercontent.com/JetBrains/lets-plot-kotlin/master/docs/images/gal_penguins.png"" alt=""images/gal_penguins.png"" width=""128"" height=""128"">\n  </a>\n  <a href=""https://nbviewer.org/github/JetBrains/lets-plot-docs/blob/master/source/kotlin_examples/demo/pushkin.ipynb"">\n    <img src=""https://raw.githubusercontent.com/JetBrains/lets-plot-kotlin/master/docs/images/gal_pushkin_trips.png"" alt=""images/gal_pushkin_trips.png"" width=""128"" height=""128"">\n  </a>\n  <a href=""https://nbviewer.org/github/JetBrains/lets-plot-docs/blob/master/source/kotlin_examples/demo/spatialdataset_kotlin_isl.ipynb"">\n    <img src=""https://raw.githubusercontent.com/JetBrains/lets-plot-kotlin/master/docs/images/gal_kotlin_isl.png"" alt=""images/gal_kotlin_isl.png"" width=""128"" height=""128"">\n  </a>\n  <a href=""https://nbviewer.org/github/JetBrains/lets-plot-docs/blob/master/source/kotlin_examples/demo/wind_rose.ipynb"">\n    <img src=""https://raw.githubusercontent.com/JetBrains/lets-plot-kotlin/master/docs/images/gal_wind_rose.png"" alt=""images/gal_wind_rose.png"" width=""128"" height=""128"">\n  </a>\n\n<a id=""change_log""></a>\n## Change Log\n\nSee [CHANGELOG.md](https://github.com/JetBrains/lets-plot-kotlin/blob/master/CHANGELOG.md).\n\n\n<a id=""CoC""></a>\n## Code of Conduct\n\nThis project and the corresponding community are governed by the \n[JetBrains Open Source and Community Code of Conduct](https://confluence.jetbrains.com/display/ALL/JetBrains+Open+Source+and+Community+Code+of+Conduct). \nPlease make sure you read it.\n\n<a id=""license""></a>\n## License\n\nCode and documentation released under\nthe [MIT license](https://github.com/JetBrains/lets-plot-kotlin/blob/master/LICENSE).\nCopyright © 2019-2024, JetBrains s.r.o.\n",420,graphics,Kotlin,3,Kotlin,Shell,HTML,,,,,,,,,,,,,,,,,,,,,,,,,,135,1,133,1,6,794,0,60279,36,96,83,13,681728b5886c3093fe8c627d71327ef97887713b,Fix JVM artifacts usage link.,2024-07-13T23:52:11Z,Igor Alshannikov,Igor.Alshannikov@jetbrains.com,alshan,v4.7.3,## [4.7.3] - 2024-05-30\r\n\r\nThis release is 100% compatible with [Lets-Plot v 4.3.3](https://github.com/JetBrains/lets-plot/releases/tag/v4.3.3).\r\n\r\n### Added\r\n\r\n- Support for `angle` aesthetic in `geomPoint()` [[#736](https://github.com/JetBrains/lets-plot/issues/736)].  \r\n  See [example notebook](https://nbviewer.org/github/JetBrains/lets-plot-kotlin/blob/master/docs/examples/jupyter-notebooks/f-4.7.3/geom_point_angle.ipynb).\r\n\r\n### Fixed\r\n\r\n- Undesired vertical scroller when displaying `gggrid` in Jupyter notebook.\r\n- Memory leak in SVG observable model (thanks to [contribution](https://github.com/JetBrains/lets-plot/commit/9c85d1ef39e3d6022880f068ce68e97b1b7ba2f1) by Ilya Muradyan).\r\n- GeoJson structure breaks if the ring start label occurs several times [[#1086](https://github.com/JetBrains/lets-plot/issues/1086)].\r\n- `theme`: left margin doesn't work for the `plot_title` parameter [[#1101](https://github.com/JetBrains/lets-plot/issues/1101)].\r\n- Improve border line type experience [[LPK-220](https://github.com/JetBrains/lets-plot-kotlin/issues/220)].\r\n,v4.7.3,Igor Alshannikov,,alshan,MIT License,lets-plot-kotlin,JetBrains,30,kotlin-api,jupyter,plot,graphics,jvm,kotlin,plot-library,jupyter-notebooks,ggplot,geotools,charts,ggplot2,plots,plots-in-kotlin,,,,,,,/JetBrains/lets-plot-kotlin,54,178,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/ITensor/ITensors.jl,https://github.com/ITensor/ITensors.jl,0,,,0,1,1,0,0,0,1,0,0,0,0,A Julia library for efficient tensor computations and tensor network calculations,docs/src/index.md,509,physics,Julia,2,Julia,Dockerfile,,,,,,,,,,,,,,,,,,,,,,,,,,,1040,138,892,10,56,61,2298,49278,119,469,346,123,d734e640a385ffa9157d5edd4786aea98033fc0b,[BlockSparseArrays] Permute and merge blocks (#1514),2024-07-01T22:21:57Z,Matt Fishman,mtfishman@users.noreply.github.com,mtfishman,v0.6.16,## ITensors v0.6.16\n\n[Diff since v0.6.15](https://github.com/ITensor/ITensors.jl/compare/v0.6.15...v0.6.16)\n\n\n**Merged pull requests:**\n- [NDTensorsGPUArraysCoreExt] Fix nonuniform Diag-Dense contractions on GPU (#1511) (@mtfishman),v0.6.16,,,github-actions[bot],Apache License 2.0,ITensors.jl,ITensor,145,tensors,tensor-decomposition,tensor-networks,dmrg,matrix-product-states,physics,tensor-train,peps,,,,,,,,,,,,,/ITensor/ITensors.jl,154,24,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/InteractiveComputerGraphics/SPlisHSPlasH,https://github.com/InteractiveComputerGraphics/SPlisHSPlasH,1,,,1,1,1,0,0,0,0,0,0,0,0,SPlisHSPlasH is an open-source library for the physically-based simulation of fluids.,"<img src=""https://raw.githubusercontent.com/InteractiveComputerGraphics/SPlisHSPlasH/master/doc/images/logo.jpg"" width=""250"">\n<br>\n\n<p align=center><img src=""https://github.com/InteractiveComputerGraphics/SPlisHSPlasH/workflows/build-linux/badge.svg"">&nbsp;&nbsp; <img src=""https://github.com/InteractiveComputerGraphics/SPlisHSPlasH/workflows/build-windows/badge.svg"">&nbsp;&nbsp; <a href='https://splishsplash.readthedocs.io/en/latest/?badge=latest'><img src='https://readthedocs.org/projects/splishsplash/badge/?version=latest' alt='Documentation Status' /></a></p>\n<p align=center>\n <img src=""https://raw.githubusercontent.com/InteractiveComputerGraphics/SPlisHSPlasH/master/doc/images/teaser.gif"">\n</p>\n\nSPlisHSPlasH is an open-source library for the physically-based simulation of fluids. The simulation in this library is based on the Smoothed Particle Hydrodynamics (SPH) method which is a popular meshless Lagrangian approach to simulate complex fluid effects. The SPH formalism allows an efficient computation of a certain quantity of a fluid particle by considering only a finite set of neighboring particles. One of the most important research topics in the field of SPH methods is the simulation of incompressible fluids. SPlisHSPlasH implements current state-of-the-art pressure solvers (WCSPH, PCISPH, PBF, IISPH, DFSPH, PF) to simulate incompressibility. Moreover, the library provides different methods to simulate viscosity, surface tension and vorticity. \n\nThe library uses the following external libraries: [Eigen](http://eigen.tuxfamily.org/), [json](https://github.com/nlohmann/json/), [partio](https://github.com/wdas/partio/), [zlib](https://github.com/madler/zlib), [cxxopts](https://github.com/jarro2783/cxxopts), [tinyexpr](https://github.com/codeplea/tinyexpr), [toojpeg](https://github.com/stbrumme/toojpeg), [pybind](https://github.com/pybind/pybind11), [glfw](https://www.glfw.org/), [hapPLY](https://github.com/nmwsharp/happly), [nfd](https://github.com/btzy/nativefiledialog-extended), and [imgui](https://github.com/ocornut/imgui). All external dependencies are included. \n\nFurthermore we use our own libraries:\n- [PositionBasedDynamics](https://github.com/InteractiveComputerGraphics/PositionBasedDynamics/) to simulate dynamic rigid bodies\n- [Discregrid](https://github.com/InteractiveComputerGraphics/Discregrid) to detect collisions between rigid bodies\n- [CompactNSearch](https://github.com/InteractiveComputerGraphics/CompactNSearch) to perform the neighborhood search \n- [cuNSearch](https://github.com/InteractiveComputerGraphics/cuNSearch) to perform the neighborhood search on the GPU\n- [GenericParameters](https://github.com/InteractiveComputerGraphics/GenericParameters) to handle generic parameters\n\nSPlisHSPlasH can export the particle data in the partio and vtk format. If you want to import partio files in Maya or Blender, try out our plugins: \n- [Blender Sequence Loader](https://github.com/InteractiveComputerGraphics/blender-sequence-loader)\n- [MayaPartioTools](https://github.com/InteractiveComputerGraphics/MayaPartioTools)\n\n**Author**: [Jan Bender](https://animation.rwth-aachen.de/person/1/)\n\n## License\n\nThe SPlisHSPlasH library code is licensed under the MIT license. See [LICENSE](https://github.com/InteractiveComputerGraphics/SPlisHSPlasH/blob/master/LICENSE) for details.\n\nExternal dependencies are covered by separate licensing terms.\nSee the [extern](https://github.com/InteractiveComputerGraphics/SPlisHSPlasH/tree/master/extern) folder for the code and respective licensing terms of each dependency.\n\n\n## Documentation\n\n* [Documentation](https://splishsplash.readthedocs.io)\n* [SPH tutorial](https://interactivecomputergraphics.github.io/SPH-Tutorial)\n\n## Forum\n\nOn our [GitHub discussions](https://github.com/InteractiveComputerGraphics/SPlisHSPlasH/discussions) page you can ask questions, discuss about simulation topics, and share ideas.\n\n\n## Build Instructions\n\nThis project is based on [CMake](https://cmake.org/). Simply generate project, Makefiles, etc. using [CMake](https://cmake.org/) and compile the project with a compiler of your choice that supports C++11. The code was tested with the following configurations:\n- Windows 10 64-bit, CMake 3.18.3, Visual Studio 2019\n- Debian 11.5 64-bit, CMake 3.18.4, GCC 10.2.1.\n\nNote: Please use a 64-bit target on a 64-bit operating system. 32-bit builds on a 64-bit OS are not supported.\n\n## Python Installation Instruction\n\nFor Windows and Linux targets there exists prebuilt python wheel files which can be installed using\n```\npip install pysplishsplash\n```\nThese are available for Python versions 3.6-3.10. See also here: [pySPlisHSPlasH](https://pypi.org/project/pySPlisHSPlasH/).\nIf you do not meet these conditions please refer to the build instructions and to the python binding \n[Getting started guide](https://github.com/InteractiveComputerGraphics/SPlisHSPlasH/blob/master/doc/py_getting_started.md).\n\nThe command line simulator is available by running one of the following\n```\nsplash\nsplash --help\n```\n\n## Features\n\nSPlisHSPlasH implements:\n* an open-source SPH fluid simulation (2D & 3D)\n* neighborhood search on CPU or GPU\n* supports vectorization using AVX\n* Python binding (thanks to Stefan Jeske)\n* supports embedded Python scripts\n* several implicit pressure solvers (WCSPH, PCISPH, PBF, IISPH, DFSPH, PF)\n* explicit and implicit viscosity methods\n* current surface tension approaches\n* different vorticity methods\n* computation of drag forces\n* support for multi-phase simulations\n* simulation of deformable solids \n* rigid-fluid coupling with static and dynamic bodies\n* two-way coupling with deformable solids\n* XSPH velocity filter\n* fluid emitters\n* scripted animation fields\n* a json-based scene file importer\n* automatic surface sampling\n* a tool for volume sampling of closed geometries\n* a tool to generate spray, foam and bubble particles in a postprocessing step \n* a tool to skin a visual mesh to the moving particles of an elastic solid in a postprocessing step\n* partio file export of all particle data\n* VTK file export of all particle data (enables the data import in ParaView)\n* rigid body export\n* a Maya plugin to model and generate scene files \n* a ParaView plugin to import particle data\n\nA list of all implemented simulation methods can be found here: \n[https://splishsplash.physics-simulation.org/features](https://splishsplash.physics-simulation.org/features/)\n\n\n## Screenshots & Videos\n\n[https://splishsplash.physics-simulation.org/gallery](https://splishsplash.physics-simulation.org/gallery/)\n\n\n\n## Citation \n\nTo cite SPlisHSPlasH you can use this BibTeX entry:\n\n```bibtex\n@software{SPlisHSPlasH_Library,\n  author = {Bender, Jan and others},\n  license = {MIT},\n  title = {{SPlisHSPlasH Library}},\n  url = {https://github.com/InteractiveComputerGraphics/SPlisHSPlasH},\n}\n```\n",1532,fluid-dynamics,C++,5,CMake,C++,GLSL,C,Python,,,,,,,,,,,,,,,,,,,,,,,,26,2,19,5,3,13,31,18147,282,170,166,4,dc2f63f3188cfe5a6fac8f6837de1335782958be,fix bug in adhesion (#308),2024-03-06T06:34:27Z,Stefan Jeske,stefan.jeske@rwth-aachen.de,digitalillusions,2.13.0,"- added implementation of surface tension method: Jeske et al. ""Implicit Surface Tension for SPH Fluid Simulation"", ACM Transactions on Graphics, 2023 (thanks to Stefan Rhys Jeske)\r\n- bugfix: in emitter scenes now all particles are reset correctly\r\n- updated PositionBasedDynamics\r\n- updated GenericParameters\r\n- added citation file\r\n- fixed rigid body colors in visualization\r\n- fixed export of static rigid bodies\r\n- added missing include\r\n- fixed addKeyFunc in Python interface\r\n- added a CMake option for each tool so that the build can be disabled/enabled\r\n- all scene file parameters are now GenericParameters\r\n- updated Python examples\r\n- added ParameterParser tool which can generate a scene file json schema or an example scene file with all parameters\r\n- added json schema for scene files \r\n- fixed OBJ meshes\r\n- updated glfw\r\n- updated pybind11 ",2.13.0,Jan Bender,,janbender,MIT License,SPlisHSPlasH,InteractiveComputerGraphics,21,sph,fluids,sph-fluids,smoothed-particle-hydrodynamics,fluid-simulation,fluid-dynamics,multiphase-flow,viscous-fluids,deformable-solids,simulation,,,,,,,,,,,/InteractiveComputerGraphics/SPlisHSPlasH,21,69,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/immunomind/immunarch,https://github.com/immunomind/immunarch,0.5,"Scientific, but package",0,0,1,1,0,0,0,1,0,0,0,0,🧬 Immunarch: an R Package for Fast and Painless Exploration of Single-cell and Bulk T-cell/Antibody Immune Repertoires,"[![Follow](https://img.shields.io/twitter/follow/immunomind.svg?style=social)](https://twitter.com/intent/follow?screen_name=immunomind)\n[![CRAN](http://www.r-pkg.org/badges/version-ago/immunarch?style=flat-square)](https://cran.r-project.org/package=immunarch)\n[![Downloads_all](http://cranlogs.r-pkg.org/badges/grand-total/immunarch)](https://www.r-pkg.org/pkg/immunarch)\n[![Downloads_week](http://cranlogs.r-pkg.org/badges/last-week/immunarch)](https://www.r-pkg.org/pkg/immunarch)\n[![Issues](https://img.shields.io/github/issues/immunomind/immunarch?style=flat-square)](https://github.com/immunomind/immunarch/issues)\n[![CI](https://gitlab.com/immunomind/immunarch/badges/master/pipeline.svg?style=flat-square)](https://gitlab.com/immunomind/immunarch/-/jobs)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3367200.svg)](https://doi.org/10.5281/zenodo.3367200)\n![Visitors](https://visitor-badge.glitch.me/badge?page_id=immunomind.immunarch)\n[![Downloads_all](http://cranlogs.r-pkg.org/badges/grand-total/tcR)](https://www.r-pkg.org/pkg/tcR)\n[![Downloads_week](http://cranlogs.r-pkg.org/badges/last-week/tcR)](https://www.r-pkg.org/pkg/tcR)\n\n\n# `immunarch` --- Fast and Seamless Exploration of Single-cell and Bulk T-cell/Antibody Immune Repertoires in R\n\n## Why `immunarch`?\n- **Work with any type of data:** single-cell, bulk, data tables, databases --- you name it.\n- **Community at the heart:** ask questions, share knowledge and thrive in the community of almost 30,000 researchers and medical scientists worldwide. **Pfizer, Novartis, Regeneron, Stanford, UCSF** and **MIT** trust us.\n- **One plot --- one line:** write a [whole PhD thesis in 8 lines of code](https://twitter.com/Nusob88/status/1127601201112129536) or reproduce almost any publication in 5-10 lines of `immunarch` code.\n- **Be on the bleeding edge of science:** we regularly update `immunarch` with the latest methods. [Let us know what you need!](#help-the-community)\n- **Automatic format detection and parsing** for all popular immunosequencing formats: from **MiXCR** and **ImmunoSEQ** to **10XGenomics** and **ArcherDX**.\n\n\n### Lightning-fast Start\n```r\ninstall.packages(""immunarch"")           # Install the package\nlibrary(immunarch); data(immdata)       # Load the package and the test dataset\nrepOverlap(immdata$data) %>% vis()      # Compute and visualise the most important statistics:\ngeneUsage(immdata$data[[1]]) %>% vis()  #     public clonotypes, gene usage, sample diversity\nrepDiversity(immdata$data) %>% vis(.by = ""Status"", .meta = immdata$meta)      # Group samples\n```\n\n\n### From Berkeley with devotion\n\n`immunarch` is brought to you by [ImmunoMind](https://immunomind.com) --- a [UC Berkeley SkyDeck](https://www.forbes.com/sites/avivalegatt/2019/01/07/launch-your-startup-at-these-five-college-incubators/) startup. ImmunoMind improves the design of adoptive T-cell therapies such as CAR-T by precisely identifying T-cell subpopulations and their immune profile. ImmunoMind's tools are trusted by researchers from top pharma companies and universities, including 10X Genomics, Pfizer, Regeneron, UCSF, MIT, Stanford, John Hopkins School of Medicine and Vanderbilt University.\n\n[![Follow](https://img.shields.io/twitter/follow/immunomind.svg?style=social)](https://twitter.com/intent/follow?screen_name=immunomind)\n\n---\n\n## Table of Contents\n\n- [Introduction](#introduction)\n- [Contact](#contact)\n- [Installation](#installation)\n- [Features](#features)\n- [Quick Start](#quick-start)\n- [Bugs and Issues](#bugs-and-issues)\n- [Contribution](#help-the-community)\n- [Citation](#citation)\n\n## Introduction\n\n`immunarch` is an R package designed to analyse T-cell receptor (TCR) and B-cell receptor (BCR) repertoires, mainly tailored to medical scientists and bioinformaticians. The mission of `immunarch` is to make immune sequencing data analysis as effortless as possible and help you focus on research instead of coding.\n\n\n## Contact\nCreate a ticket with a bug or question on [GitHub Issues](https://github.com/immunomind/immunarch/issues) to get help from the community and enrich it with your experience. If you need to send us sensitive data, feel free to contact us via [support@immunomind.io](mailto:support@immunomind.io).\n\n\n## Installation\n\n### Latest release on CRAN\nIn order to install `immunarch` execute the following command:\n\n```r\ninstall.packages(""immunarch"")\n```\n\nThat's it, you can start using `immunarch` now! See the [Quick Start](#quick-start) section below to dive into immune repertoire data analysis. If you run in any trouble during installation, take a look at the [Installation Troubleshooting](https://immunarch.com/articles/v1_introduction.html#installation-troubleshooting) section.\n\nNote: there are quite a lot of dependencies to install with the package because it installs all the widely-used packages for data analysis and visualisation. You got both the AIRR data analysis framework and the full Data Science package ecosystem with only one command, making `immunarch` the entry-point for single-cell & immune repertoire Data Science.\n\n\n### Latest release on GitHub\nIf the above command doesn't work for any reason, try installing `immunarch` directly from its repository:\n\n```r\ninstall.packages(c(""devtools"", ""pkgload"")) # skip this if you already installed these packages\ndevtools::install_github(""immunomind/immunarch"")\ndevtools::reload(pkgload::inst(""immunarch""))\n```\n\n\n### Latest pre-release on GitHub\nSince releasing on CRAN is limited to one release per one or two months, you can install the latest pre-release version with all the bleeding edge and optimised features directly from the code repository. In order to install the latest pre-release version, you need to execute the following commands:\n\n```r\ninstall.packages(c(""devtools"", ""pkgload"")) # skip this if you already installed these packages\ndevtools::install_github(""immunomind/immunarch"", ref=""dev"")\ndevtools::reload(pkgload::inst(""immunarch""))\n```\n\nYou can find the list of releases of `immunarch` here: https://github.com/immunomind/immunarch/releases\n\n\n## Key Features\n\n1. Data agnostic. Fast and easy manipulation of immune repertoire data:\n\n    + The package automatically detects the format of your files---no more guessing what format is *that* file, just pass them to the package;\n  \n    + Supports all popular TCR and BCR analysis and post-analysis formats, including single-cell data: [ImmunoSEQ](https://www.adaptivebiotech.com/adaptive-immunosequencing/), [IMGT](https://www.imgt.org/IMGTindex/IMGTHighV-QUEST.php), [MiTCR](https://github.com/milaboratory/mitcr), [MiXCR](https://github.com/milaboratory/mixcr), [MiGEC](https://github.com/mikessh/migec), [MigMap](https://github.com/mikessh/migmap), [VDJtools](https://github.com/mikessh/vdjtools), [tcR](https://github.com/imminfo/tcr), [AIRR](http://docs.airr-community.org/en/latest/), [10XGenomics](https://www.10xgenomics.com/resources/datasets?menu%5Bproducts.name%5D=Single+Cell+Immune+Profiling), ArcherDX. More coming in the future;\n\n    + Works on any data source you are comfortable with: R data frames, data tables from [data.table](https://rdatatable.gitlab.io/data.table/), databases like [MonetDB](https://github.com/MonetDB), Apache Spark data frames via [sparklyr](https://spark.posit.co/);\n    \n    + Tutorial is available [here](https://immunarch.com/articles/v2_data.html).\n\n2. Beginner-friendly. Immune repertoire analysis made simple:\n\n    + Most methods are incorporated in a couple of main functions with clear naming---no more remembering dozens and dozens of functions with obscure names. For details see [link](https://immunarch.com/articles/web_only/v3_basic_analysis.html);\n\n    + Repertoire overlap analysis *(common indices including overlap coefficient, Jaccard index and Morisita's overlap index)*. Tutorial is available [here](https://immunarch.com/articles/web_only/v4_overlap.html);\n  \n    + Gene usage estimation *(correlation, Jensen-Shannon Divergence, clustering)*. Tutorial is available [here](https://immunarch.com/articles/web_only/v5_gene_usage.html);\n\n    + Diversity evaluation *(ecological diversity index, Gini index, inverse Simpson index, rarefaction analysis)*. Tutorial is available [here](https://immunarch.com/articles/web_only/v6_diversity.html);\n\n    + Tracking of clonotypes across time points, widely used in vaccination and cancer immunology domains. Tutorial is available [here](https://immunarch.com/articles/web_only/v8_tracking.html);\n    \n    + K-mer distribution measures and statistics. Tutorial is available [here](https://immunarch.com/articles/web_only/v9_kmers.html);\n    \n    + Coming in the next releases: CDR3 amino acid physical and chemical properties assessment, mutation networks.\n\n3. Seamless publication-ready plots with a built-in tool for visualisation manipulation: \n\n    + Rich visualisation procedures with [ggplot2](https://ggplot2.tidyverse.org/);\n  \n    + Built-in tool `FixVis` makes your plots publication-ready: easily change font sizes, text angles, titles, legends and many more with clear-cut GUI;\n    \n    + Tutorial is available [here](https://immunarch.com/articles/web_only/v7_fixvis.html).\n    \n    \n## Quick start\nThe gist of the typical TCR or BCR data analysis workflow can be reduced to the next few lines of code.\n\n### Use `immunarch` data\n\n**1) Load the package and the data**\n\n```r\nlibrary(immunarch)  # Load the package into R\ndata(immdata)  # Load the test dataset\n```\n\n**2) Calculate and visualise basic statistics**\n\n```r\nrepExplore(immdata$data, ""lens"") %>% vis()  # Visualise the length distribution of CDR3\nrepClonality(immdata$data, ""homeo"") %>% vis()  # Visualise the relative abundance of clonotypes\n```\n\n**3) Explore and compare T-cell and B-cell repertoires**\n```r\nrepOverlap(immdata$data) %>% vis()  # Build the heatmap of public clonotypes shared between repertoires\ngeneUsage(immdata$data[[1]]) %>% vis()  # Visualise the V-gene distribution for the first repertoire\nrepDiversity(immdata$data) %>% vis(.by = ""Status"", .meta = immdata$meta)  # Visualise the Chao1 diversity of repertoires, grouped by the patient status\n```\n\n### Use your own data\n\n```r\nlibrary(immunarch)  # Load the package into R\nimmdata <- repLoad(""path/to/your/data"")  # Replace it with the path to your data. Immunarch automatically detects the file format.\n```\n\n### Advanced methods\n\nFor advanced methods such as clonotype annotation, clonotype tracking, k-mer analysis and public repertoire analysis see ""Tutorials"".\n\n\n## Bugs and Issues\n\nThe mission of `immunarch` is to make bulk and single-cell immune repertoires analysis painless. All bug reports, documentation improvements, enhancements and ideas are appreciated. Just let us know via [GitHub](https://github.com/immunomind/immunarch/issues) (preferably) or [support@immunomind.io](mailto:support@immunomind.io) (in case of private data).\n\nBug reports must: \n\n1. Include a short, self-contained R snippet reproducing the problem. \n2. Add a minimal data sample for us to reproduce the problem. In case of sensitive data you can send it to [support@immunomind.io](mailto:support@immunomind.io) instead of GitHub issues.\n3. Explain why the current behavior is wrong/not desired and what you expect instead.\n4. If the issue is about visualisations, please attach a picture to the issue. In other case we wouldn't be able to reproduce the bug and fix it.\n\n\n## Help the community\n\nAspiring to help the community build the ecosystem of scRNAseq & AIRR analysis tools? Found a bug? A typo? Would like to improve documentation, add a method or optimise an algorithm?\n\nWe are always open to contributions. There are two ways to contribute:\n\n1. Create an issue [here](https://github.com/immunomind/immunarch/issues) and describe what would you like to improve or discuss.\n\n2. Create an issue or find one [here](https://github.com/immunomind/immunarch/issues), fork the repository and make a pull request with the bugfix or improvement.\n\n\n## Citation\n\nImmunoMind Team. (2019). immunarch: An R Package for Painless Bioinformatics Analysis of T-Cell and B-Cell Immune Repertoires. Zenodo. http://doi.org/10.5281/zenodo.3367200\n\nBibTex:\n```\n@misc{immunomind_team_2019_3367200,\n  author       = {{ImmunoMind Team}},\n  title        = {{immunarch: An R Package for Painless Bioinformatics Analysis \n                    of T-Cell and B-Cell Immune Repertoires}},\n  month        = aug,\n  year         = 2019,\n  doi          = {10.5281/zenodo.3367200},\n  url          = {https://doi.org/10.5281/zenodo.3367200}\n}\n```\n\nFor EndNote citation import the [`immunarch-citation.xml`](https://gitlab.com/immunomind/immunarch/raw/master/immunarch-citation.xml?inline=false) file.\n\nPreprint on BioArxiv is coming soon.\n\n\n## License\n\nThe package is freely distributed under the Apache-2.0 license. You can read more about it [here](https://www.tldrlegal.com/license/apache-license-2-0-apache-2-0).\n\nFor commercial or server use, please contact ImmunoMind via [support@immunomind.io](mailto:support@immunomind.io) about solutions for biomarker data science of single-cell immune repertoires.\n\n\n## Commercial Support\n",297,bioinformatics,R,4,R,CSS,C++,Dockerfile,,,,,,,,,,,,,,,,,,,,,,,,,106,7,93,6,10,10,95,445754,65,297,150,147,0b6544a101e9f42ee1bf6a5f14cbd234147e18ad,Merge pull request #403 from immunomind/dev,2024-03-19T20:27:38Z,Vadim I. Nazarov,2979713+vadimnazarov@users.noreply.github.com,vadimnazarov,Immunarch 0.9.1,## What's Changed\r\n* Dev by @vadimnazarov in https://github.com/immunomind/immunarch/pull/403\r\n\r\n\r\n**Full Changelog**: https://github.com/immunomind/immunarch/compare/0.9.0...0.9.1,0.9.1,Vadim I. Nazarov,,vadimnazarov,Apache License 2.0,immunarch,immunomind,19,immunology,tcr,tcr-repertoire,immunoinformatics,immune-repertoire,rep-seq,bcr-repertoire,bcr,ig,ig-repertoire,bioinformatics,immune-repertoire-data,immune-repertoire-analysis,t-cell-receptor,b-cell-receptor,immunoglobulin,repertoire-analysis,airr-analysis,single-cell,single-cell-analysis,/immunomind/immunarch,21,13,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/ImGuiNET/ImGui.NET,https://github.com/ImGuiNET/ImGui.NET,0,,,0,0,0,0,0,0,1,1,0,0,0,An ImGui wrapper for .NET.,"# ImGui.NET\n\nThis is a .NET wrapper for the immediate mode GUI library, Dear ImGui (https://github.com/ocornut/imgui). ImGui.NET lets you build graphical interfaces using a simple immediate-mode style. ImGui.NET is a .NET Standard library, and can be used on all major .NET runtimes and operating systems.\n\nIncluded is a basic sample program that shows how to use the library, and renders the UI using [Veldrid](https://github.com/veldrid/veldrid), a portable graphics library for .NET. By itself, Dear ImGui does not care what technology you use for rendering; it simply outputs textured triangles. Example renderers also exist for MonoGame and OpenTK (OpenGL).\n\nThis wrapper is built on top of [cimgui](https://github.com/cimgui/cimgui), which exposes a plain C API for Dear ImGui. If you are using Windows, OSX, or a mainline Linux distribution, then the ImGui.NET NuGet package comes bundled with a pre-built native library. If you are using another operating system, then you may need to build the native library yourself; see the cimgui repo for build instructions.\n\n[![NuGet](https://img.shields.io/nuget/v/ImGui.NET.svg)](https://www.nuget.org/packages/ImGui.NET)\n\n___As of February 2023, I (@mellinoe) am no longer able to publicly share updates to ImGui.NET and related libraries. A big thanks to @zaafar who continues to actively maintain the library and keep it up to date with new versions of native Dear ImGui. Feel free to join the [Discord server](https://discord.gg/s5EvvWJ) for more information about the current status of development.___\n\n# Building\n\nImGui.NET can be built in Visual Studio or on the command line. The .NET Core SDK is needed to build on the command line, and it can be downloaded [here](https://www.microsoft.com/net/core). Visual Studio 2017 is the minimum VS version supported for building.\n\n# Usage\n\nImGui.NET currently provides a raw wrapper around the ImGui native API, and also provides a very thin safe, managed API for convenience. It is currently very much like using the native library, which is very simple, flexible, and robust. The easiest way to figure out how to use the library is to read the documentation of imgui itself, mostly in the imgui.cpp, and imgui.h files, as well as the exported functions in cimgui.h. Looking at the [sample program code](https://github.com/ImGuiNET/ImGui.NET/tree/master/src) will also give some indication about basic usage.\n\n# Debugging native code\n\nImGui.NET is a wrapper over native code. By default, this native code is packaged and released in an optimized form, making debugging difficult. To obtain a debuggable version of the native code, follow these steps:\n\n1. Clone the [ImGui.NET-nativebuild](https://github.com/ImGuiNET/ImGui.NET-nativebuild) repo, at the tag matching the version of ImGui.NET you are using.\n2. In the ImGui.NET-nativebuild repo, run `build.cmd debug` or `build.sh debug` (depending on your platform).\n3. Copy the produced binaries (cimgui.dll, libcimgui.so, or libcimgui.dylib) into your application.\n4. Run the program under a native debugger, or enable mixed-mode debugging in Visual Studio.\n\n# See Also\n\nhttps://github.com/ocornut/imgui\n> Dear ImGui is a bloat-free graphical user interface library for C++. It outputs optimized vertex buffers that you can render anytime in your 3D-pipeline enabled application. It is fast, portable, renderer agnostic and self-contained (no external dependencies).\n\n> Dear ImGui is designed to enable fast iterations and to empower programmers to create content creation tools and visualization / debug tools (as opposed to UI for the average end-user). It favors simplicity and productivity toward this goal, and lacks certain features normally found in more high-level libraries.\n\n> Dear ImGui is particularly suited to integration in games engine (for tooling), real-time 3D applications, fullscreen applications, embedded applications, or any applications on consoles platforms where operating system features are non-standard.\n\nSee the [official screenshot thread](https://github.com/ocornut/imgui/issues/123) for examples of many different kinds of interfaces created with Dear ImGui.\n\nhttps://github.com/cimgui/cimgui\n> This is a thin c-api wrapper for the excellent C++ intermediate gui imgui. This library is intended as a intermediate layer to be able to use imgui from other languages that can interface with C .\n",1807,graphics,C#,7,C#,GLSL,HLSL,Metal,Batchfile,PowerShell,Shell,,,,,,,,,,,,,,,,,,,,,,108,26,74,8,16,35,0,65743,298,375,234,141,42803e790833dfddc2755e9d0c6111ef65a4215c,Update ImGuiRenderer.cs (#484),2024-07-08T20:10:09Z,Soroush,67832386+soroushJuly@users.noreply.github.com,soroushJuly,v1.90.9.1,## What's Changed\r\n* upgrade to 1.90.9 by @zaafar in https://github.com/ImGuiNET/ImGui.NET/pull/483\r\n* added dependabot to autoupgrade github actions by @zaafar in https://github.com/ImGuiNET/ImGui.NET/pull/483\r\n\r\n**Full Changelog**: https://github.com/ImGuiNET/ImGui.NET/compare/v1.90.8.1...v1.90.9.1,v1.90.9.1,Zaafar.A,,zaafar,MIT License,ImGui.NET,ImGuiNET,14,imgui,netcore,games,graphics,,,,,,,,,,,,,,,,,/ImGuiNET/ImGui.NET,27,60,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/ImagingDataCommons/slim,https://github.com/ImagingDataCommons/slim,0.5,"Image viewer with extra features, but used in scintific purpose. ",0,0,1,0,0,0,0,1,0,0,0,0,Interoperable web-based slide microscopy viewer and annotation tool,"[![DOI](https://zenodo.org/badge/335130719.svg)](https://zenodo.org/badge/latestdoi/335130719)\n[![Build Status](https://github.com/imagingdatacommons/slim/actions/workflows/unit-tests.yml/badge.svg)](https://github.com/imagingdatacommons/slim/actions)\n\n# Slim: Interoperable slide microscopy viewer and annotation tool for imaging data science and computational pathology\n\n*Slim* is a single-page application for interactive visualization and annotation of digital whole slide microscopy images and derived image analysis results in standard DICOM format.\nThe application is based on the [dicom-microscopy-viewer](https://github.com/MGHComputationalPathology/dicom-microscopy-viewer) JavaScript library and runs fully client side without any custom server components.\nIt relies on [DICOMweb](https://www.dicomstandard.org/dicomweb/) RESTful services to search for, retrieve, and store imaging data and can thereby simply be placed in front of any DICOMweb-conformant Image Management System (IMS), Picture Archiving and Communication (PACS), or Vendor Neutral Archive (VNA).\n\n## Explore\n\n### National Cancer Institute's Imaging Data Commons\n\n*Slim* is used as the slide microscopy viewer by the [National Cancer Institute's Imaging Data Commons (IDC)](https://imaging.datacommons.cancer.gov).\n\n<img src=""docs/screenshots/IDC_CPTAC_C3L-00965-26.png"" alt=""IDC CPTAC C3L-00965-26"" width=""100%"">\n\nExplore public IDC cancer imaging data collections by visiting the IDC web portal: [portal.imaging.datacommons.cancer.gov](https://portal.imaging.datacommons.cancer.gov/).\n\nThe IDC viewer uses the [Google Cloud Healthcare API](https://cloud.google.com/healthcare-api/) as DICOMweb server.\n\n### Demo\n\nBelow you will find links to the representative DICOM SM images opened in Slim viewer:\n\n* H&E: https://viewer.imaging.datacommons.cancer.gov/slim/studies/2.25.211094631316408413440371843585977094852/series/1.3.6.1.4.1.5962.99.1.208792987.352384958.1640886332827.2.0\n* multichannel fluorescence: https://viewer.imaging.datacommons.cancer.gov/slim/studies/2.25.93749216439228361118017742627453453196/series/1.3.6.1.4.1.5962.99.1.2344794501.795090168.1655907236229.4.0?state=1.2.826.0.1.3680043.10.511.3.79630386778396943986328353882008803\n\n## Features\n\n### Display of images\n\n*Slim* enables interactive visualization of [DICOM VL Whole Slide Microscopy Image](https://dicom.nema.org/medical/dicom/current/output/chtml/part03/sect_A.32.8.html) instances in a vendor-neutral and device-independent manner.\n\nInteroperability with various image acquisition and management systems was successfully demonstrated at the [DICOM WG-26 Connectathon at Path Visions 2020](https://digitalpathologyassociation.org/past-presentations#PV20) and the [DICOM WG-26 Hackathon at Path Visions 2021](https://digitalpathologyassociation.org/past-presentations#PV21).\nShown below are screenshots of examples images that are publicly available on the NEMA FTP server at [medical.nema.org](ftp://medical.nema.org).\n\n|     | Vendor | Illumination | Stain |\n| :-: |:------ |:------------ | :---  |\n| <img src=""docs/screenshots/NEMA_Roche_TriChrome.png"" alt=""NEMA Roche Brightfield"" width=""350""> | Roche Tissue Diagnostics | Brightfield | Trichrome |\n| <img src=""docs/screenshots/NEMA_3DHISTECH_HE.png"" alt=""NEMA 3DHISTECH Brightfield"" width=""350""> | 3DHISTECH | Brightfield | H&E |\n| <img src=""docs/screenshots/NEMA_3DHISTECH_DAPI-FITC-Rhodamine.png"" alt=""NEMA 3DHISTECH Flourescence"" width=""350""> | 3DHISTECH | Fluorescence | DAPI, FITC, Rhodamine |\n| <img src=""docs/screenshots/NEMA_SamanTree_Histolog.png"" alt=""NEMA SamanTree Flourescence"" width=""350""> | SamanTree Medical | Fluorescence | Histolog |\n\n### Display of image annotations and analysis results\n\n*Slim* further allows for interative visualization of image annotations and analysis results.\nThe viewer currently supports the following types of DICOM instances:\n\nVector graphics:\n\n- [DICOM Comprehensive 3D SR](https://dicom.nema.org/medical/dicom/current/output/chtml/part03/sect_A.35.13.html) instances that are structured according to template [TID 1500 ""Measurements Report""](https://dicom.nema.org/medical/dicom/current/output/chtml/part16/chapter_A.html#sect_TID_1500) and contain planar image region of interest (ROI) annotations structured according to template [TID 1410 ""Planar ROI Measurements and Qualitative Evaluations""](http://dicom.nema.org/medical/dicom/current/output/chtml/part16/chapter_A.html#sect_TID_1410)\n- [DICOM Microscopy Bulk Simple Annotations](https://dicom.nema.org/medical/dicom/current/output/chtml/part03/sect_A.87.html) instances that contain groups of many ROI annotations (e.g., single cells)\n\nRaster graphics:\n\n- [DICOM Segmentation](https://dicom.nema.org/medical/dicom/current/output/chtml/part03/sect_A.51.html) instances that contain binary or fractional segmentation masks\n- [DICOM Parametric Map](https://dicom.nema.org/medical/dicom/current/output/chtml/part03/sect_A.75.html) instances that contain saliency maps, attention maps, class activation maps, etc.\n\n\n|     | DICOM IOD |\n| :-: |:--------- |\n| <img src=""docs/screenshots/IDC_CPTAC_C3N-01016-22_segmentation.png"" alt=""IDC CPTAC Segmentation"" width=""350""> | Segmentation |\n| <img src=""docs/screenshots/IDC_CPTAC_C3N-01016-22_parametric_map.png"" alt=""IDC CPTAC Parametric Map"" width=""350""> | Parametric Map |\n| <img src=""docs/screenshots/IDC_CPTAC_C3N-01016-22_annotation.png"" alt=""IDC CPTAC Comprehensive 3D SR"" width=""350""> | Comprehensive 3D SR |\n| <img src=""docs/screenshots/IDC_TCGA_TCGA-05-4244-01Z-00-DX1_segmentation.png"" alt=""IDC TCGA Segmentation"" width=""350""> | Segmentation |\n| <img src=""docs/screenshots/IDC_TCGA_TCGA-05-4244-01Z-00-DX1_bulk_annotations.png"" alt=""IDC TCGA Segmentation"" width=""350""> | Microscopy Bulk Simple Annotations |\n\n\n### Annotation of images\n\nIn addition to display, *Slim* provides annotation tools that allow users to create graphical image region of interest (ROI) annotations and store them as [DICOM Comprehensive 3D SR](https://dicom.nema.org/medical/dicom/current/output/chtml/part03/sect_A.35.13.html) instances using SR template [TID 1500 ""Measurement Report""](http://dicom.nema.org/medical/dicom/current/output/chtml/part16/chapter_A.html#sect_TID_1500).\nROIs are stored as 3D spatial coordinates (SCOORD3D) in millimeter unit according to SR template [TID 1410 ""Planar ROI Measurements and Qualitative Evaluations""](http://dicom.nema.org/medical/dicom/current/output/chtml/part16/chapter_A.html#sect_TID_1410) together with measurements and qualitative evaluations (labels).\nSpecifically, [Image Region](http://dicom.nema.org/medical/dicom/current/output/chtml/part16/chapter_A.html#para_b68aa0a9-d0b1-475c-9630-fbbd48dc581d) is used to store the vector graphic data and [Finding](http://dicom.nema.org/medical/dicom/current/output/chtml/part16/chapter_A.html#para_c4ac1cac-ee86-4a86-865a-8137ebe1bd95) is used to describe what has been annotated using a standard medical terminology such as [SNOMED CT](https://www.snomed.org/).\nThe terms that can be chosen by a user can be configured (see [AppConfig.d.ts](src/AppConfig.d.ts)).\n\n\n## Autentication and authorization\n\nUsers can authenticate and authorize the application to access data via [OpenID Connect (OIDC)](https://openid.net/connect/) based on the [OAuth 2.0](https://oauth.net/2/) protocol using either the [authorization code grant type](https://oauth.net/2/grant-types/authorization-code/) (with [Proof Key for Code Exchange (PKCE)](https://oauth.net/2/pkce/) extension) or the legacy [implicit grant type](https://oauth.net/2/grant-types/implicit/).\n\n## Configuration\n\nThe app can be configured via a `public/config/{name}.js` JavaScript configuration file (see for example the default `public/config/local.js`).\nPlease refer to the [AppConfig.d.ts](src/AppConfig.d.ts) file for configuration options.\n\nThe configuration can be changed at build-time using the `REACT_APP_CONFIG` environment variable.\n\n## Deployment\n\nDownload the latest release from [github.com/imagingdatacommons/slim/releases](https://github.com/imagingdatacommons/slim/releases) and then run the following commands to install build dependencies and build the app:\n\n```none\nyarn install\nPUBLIC_URL=/ yarn build\n```\n\nOnce the app has been built, the content of the `build` folder can be directly served by a static web server at the location specified by `PUBLIC_URL` (in this case at `/`).\nThe `PUBLIC_URL` must be either a full URL or a relative path to the location at which the viewer application will get deployed (e.g., `PUBLIC_URL=https://imagingdatacommons.github.io/slim` or `PUBLIC_URL='/slim'`).\n\nTo learn how to deploy Slim as a Google Firebase webapp, consider [this tutorial](https://tinyurl.com/idc-slim-gcp).\n\n### Local\n\nThe repository provides a [Docker compose file](https://docs.docker.com/compose/compose-file/) to deploy a static web server and a [dcm4chee-arc-light](https://github.com/dcm4che/dcm4chee-arc-light) DICOMweb server on localhost for local app development and testing:\n\n```none\ndocker-compose up -d\n```\n\nThe local deployment serves the app via an NGINX web server at `http://localhost:8008` and exposes the DICOMweb services at `http://localhost:8008/dcm4chee-arc/aets/DCM4CHEE/rs`.\nOnce the serives are up, one can store DICOM objects in the archive using the [Store transaction of the DICOMweb Studies Service](http://dicom.nema.org/medical/dicom/current/output/chtml/part18/sect_10.5.html).\n\nThe command line interface of the [dicomweb-client Python package](https://dicomweb-client.readthedocs.io/en/latest/usage.html#command-line-interface-cli) makes storing DICOM files in the archive straight forward:\n\n```none\ndicomweb_client -vv --url http://localhost:8008/dcm4chee-arc/aets/DCM4CHEE/rs store instances -h\n```\n\nThe local deployment uses the default configuration file `public/config/local.js`:\n\n```js\nwindow.config = {\n  path: ""/"",\n  servers: [\n    {\n      id: ""local"",\n      url: ""http://localhost:8008/dcm4chee-arc/aets/DCM4CHEE/rs"",\n      write: true\n    }\n  ],\n  annotations: [\n    {\n      finding: {\n        value: '85756007',\n        schemeDesignator: 'SCT',\n        meaning: 'Tissue'\n      },\n      style: {\n        stroke: {\n          color: [251, 134, 4, 1],\n          width: 2\n        },\n        fill: {\n          color: [255, 255, 255, 0.2]\n        }\n      }\n    }\n  ]\n};\n```\n\nCustomize the configuration according to your needs at either build-time or run-time.\n\n### Google Cloud Platform\n\n*Slim* can be readily configured to connect to a secured DICOMweb endpoint of the [Google Cloud Healthcare API](https://cloud.google.com/healthcare) with OIDC authentication:\n\n```js\nconst gcpProject = """"\nconst gcpLocation = """"\nconst gcpDataset = """"\nconst gcpStore = """"\nconst gcpClientID = """"\n\nwindow.config = {\n  path: ""/"",\n  servers: [\n    {\n      id: ""gcp"",\n      url: `https://healthcare.googleapis.com/v1/projects/${gcpProject}/locations/${gcpLocation}/datasets/${gcpDataset}/dicomStores/${gcpStore}/dicomWeb`,\n      write: true\n    }\n  ],\n  oidc: {\n    authority: ""https://accounts.google.com"",\n    clientId: gcpClientID,\n    scope: ""email profile openid https://www.googleapis.com/auth/cloud-healthcare"",\n    grantType: ""implicit"",\n    endSessionEndpoint: ""https://www.google.com/accounts/Logout""\n  },\n  annotations: [\n    {\n      finding: {\n        value: '108369006',\n        schemeDesignator: 'SCT',\n        meaning: 'Neoplasm'\n      },\n      style: {\n        stroke: {\n          color: [251, 134, 4, 1],\n          width: 2\n        },\n        fill: {\n          color: [255, 255, 255, 0.2]\n        }\n      }\n    },\n    {\n      finding: {\n        value: '85756007',\n        schemeDesignator: 'SCT',\n        meaning: 'Tissue'\n      },\n      style: {\n        stroke: {\n          color: [255, 255, 0, 1],\n          width: 2\n        },\n        fill: {\n          color: [255, 255, 255, 0.2]\n        }\n      }\n    }\n  ]\n};\n```\n\n#### OAuth 2.0 configuration\n\nCreate an [OIDC client ID for web application](https://developers.google.com/identity/sign-in/web/sign-in).\n\nNote that Google's OIDC implementation does currently not yet support the authorization code grant type with PKCE challenge for private clients.\nFor the time being, the legacy implicit grand type has to be used.\n\n\n## Development\n\nTo install requirements and run the app for local development, run the following commands:\n\n```none\nyarn install\nyarn start\n```\n\nThis will serve the app via a development server at [http://localhost:3000](http://localhost:3000) using the default `local` configuration.\n\nThe configuration can be specified using the `REACT_APP_CONFIG` environment variable, which can be set either in the `.env` file or directly in the command line:\n\n```none\nREACT_APP_CONFIG=local yarn start\n```\n\n## Citation\n\nFor more information about the motivation, design, and capabilities of Slim, please see the following article:\n\n> [Interoperable slide microscopy viewer and annotation tool for imaging data science and computational pathology]([https://arxiv.org/abs/2205.09122](https://www.nature.com/articles/s41467-023-37224-2))\n> C. Gorman, D. Punzo, I. Octaviano, S. Pieper, W.J.R. Longabaugh, D.A. Clunie, R. Kikinis, A.Y. Fedorov, M.D. Herrmann\n> Nature Communications 4:1572 (2023)\n\nIf you use Slim in your research, please cite the above article.\n\n## DICOM Conformance Statement\n\nThe DICOM conformance statement for Slim is available in this repository [here](/DICOM-Conformance-Statement.md)\n",111,digital-pathology,TypeScript,6,Dockerfile,JavaScript,HTML,Less,TypeScript,CSS,,,,,,,,,,,,,,,,,,,,,,,84,20,62,2,24,18,120,69947,36,136,73,63,b0f3991d109c18cdec6bd79f388870b159b6c5b8,Bump ws from 7.5.9 to 7.5.10 (#219),2024-07-16T12:20:00Z,dependabot[bot],49699333+dependabot[bot]@users.noreply.github.com,dependabot[bot],v0.31.3,## [0.31.3](https://github.com/ImagingDataCommons/slim/compare/v0.31.2...v0.31.3) (2024-05-09)\r\n\r\n\r\n### Bug Fixes\r\n\r\n* **README.md:** Update README.md to fix build status ([#210](https://github.com/ImagingDataCommons/slim/issues/210)) ([7574a93](https://github.com/ImagingDataCommons/slim/commit/7574a93b0c9a303202f135566328c69eb605cd69))\r\n\r\n## What's Changed\r\n* feat(public/preview.js): Update preview config to allow changing dicom store by @igoroctaviano in https://github.com/ImagingDataCommons/slim/pull/209\r\n* fix(README.md): Update README.md to fix build status by @igoroctaviano in https://github.com/ImagingDataCommons/slim/pull/210\r\n\r\n\r\n**Full Changelog**: https://github.com/ImagingDataCommons/slim/compare/v0.31.2...v0.31.3,v0.31.3,,,,Apache License 2.0,slim,ImagingDataCommons,39,dicomweb,oidc,react,digital-pathology,idc-viewers,,,,,,,,,,,,,,,,/ImagingDataCommons/slim,39,10,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/igraph/rigraph,https://github.com/igraph/rigraph,0,,,0,1,0,0,0,0,1,0,0,0,0,igraph R package,"\n<!-- badges: start -->\n![R-CMD-check](https://github.com/igraph/rigraph/workflows/R-CMD-check/badge.svg)\n![CRAN Downloads](https://cranlogs.r-pkg.org/badges/igraph)\n[![Codecov test coverage](https://codecov.io/gh/igraph/rigraph/branch/main/graph/badge.svg)](https://app.codecov.io/gh/igraph/rigraph?branch=main)\n<!-- badges: end -->\n\n# R/igraph <a href=""https://r.igraph.org/""><img src=""man/figures/logo.png"" align=""right"" height=""120"" alt=""igraph website"" /></a>\n\nR/igraph is an R package of the igraph network analysis library.\n\n## Installation\n\nYou can install the stable version of R/igraph from CRAN:\n\n```r\ninstall.packages(""igraph"")\n```\n\nFor the development version, you can use R-universe\n\n```r\noptions(\n  repos = c(\n    igraph = 'https://igraph.r-universe.dev',\n    CRAN = 'https://cloud.r-project.org'\n  )\n)\ninstall.packages('igraph')\n```\n\nor Github, with the [pak package](https://pak.r-lib.org/):\n\n```r\npak::pak(""igraph/rigraph"")\n```\n\nWhen compiling from sources, make sure that you have C, C++ and Fortran\ncompilers, as well as development packages for `glpk` and `libxml2`.\nOn Debian/Ubuntu, use `apt install libglpk-dev libxml2-dev`.\nOn Fedora, use `yum install glpk-devel libxml2-devel`.\n\nFor installation from source on Windows, you need to have\n[RTools](https://cran.r-project.org/bin/windows/Rtools/) installed.\nFor versions R >= 4.0 you can install the dependencies using:\n\n```\npacman -Sy mingw-w64-{i686,x86_64}-glpk mingw-w64-{i686,x86_64}-libxml2\n```\n\n## Installation troubleshooting\n\nSee the [Installation FAQ](https://r.igraph.org/articles/installation-troubleshooting).\n\n## Documentation\n\nSee the [igraph package's website](https://r.igraph.org/) for the complete manual.\n\n## Contributions\n\nPlease read our\n[contribution guide](https://github.com/igraph/rigraph/blob/dev/CONTRIBUTING.md).\n\n## License\n\nGNU GPL version 2 or later\n\n## Contributors\n\n\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n\nAll contributions to this project are gratefully acknowledged using the [`allcontributors` package](https://github.com/ropenscilabs/allcontributors) following the [all-contributors](https://allcontributors.org) specification. Contributions of any kind are welcome!\n\n<table>\n\n<tr>\n<td align=""center"">\n<a href=""https://github.com/gaborcsardi"">\n<img src=""https://avatars.githubusercontent.com/u/660288?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=gaborcsardi"">gaborcsardi</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/ntamas"">\n<img src=""https://avatars.githubusercontent.com/u/195637?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=ntamas"">ntamas</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/vtraag"">\n<img src=""https://avatars.githubusercontent.com/u/6057804?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=vtraag"">vtraag</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/szhorvat"">\n<img src=""https://avatars.githubusercontent.com/u/1212871?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=szhorvat"">szhorvat</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/krlmlr"">\n<img src=""https://avatars.githubusercontent.com/u/1741643?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=krlmlr"">krlmlr</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/adalisan"">\n<img src=""https://avatars.githubusercontent.com/u/1790714?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=adalisan"">adalisan</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/igraph"">\n<img src=""https://avatars.githubusercontent.com/u/8360597?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=igraph"">igraph</a>\n</td>\n</tr>\n\n\n<tr>\n<td align=""center"">\n<a href=""https://github.com/pupamanyu"">\n<img src=""https://avatars.githubusercontent.com/u/1937416?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=pupamanyu"">pupamanyu</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/Antonov548"">\n<img src=""https://avatars.githubusercontent.com/u/22891541?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=Antonov548"">Antonov548</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/dmurdoch"">\n<img src=""https://avatars.githubusercontent.com/u/1935680?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=dmurdoch"">dmurdoch</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/clpippel"">\n<img src=""https://avatars.githubusercontent.com/u/9609214?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=clpippel"">clpippel</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/dougmet"">\n<img src=""https://avatars.githubusercontent.com/u/5878305?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=dougmet"">dougmet</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/wael-sadek"">\n<img src=""https://avatars.githubusercontent.com/u/42678896?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=wael-sadek"">wael-sadek</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/kasterma"">\n<img src=""https://avatars.githubusercontent.com/u/421437?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=kasterma"">kasterma</a>\n</td>\n</tr>\n\n\n<tr>\n<td align=""center"">\n<a href=""https://github.com/cfhammill"">\n<img src=""https://avatars.githubusercontent.com/u/7467038?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=cfhammill"">cfhammill</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/clhunsen"">\n<img src=""https://avatars.githubusercontent.com/u/2649820?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=clhunsen"">clhunsen</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/rundel"">\n<img src=""https://avatars.githubusercontent.com/u/273926?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=rundel"">rundel</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/qsz13"">\n<img src=""https://avatars.githubusercontent.com/u/4075761?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=qsz13"">qsz13</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/DexGroves"">\n<img src=""https://avatars.githubusercontent.com/u/10374782?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=DexGroves"">DexGroves</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/QuLogic"">\n<img src=""https://avatars.githubusercontent.com/u/302469?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=QuLogic"">QuLogic</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/dalloliogm"">\n<img src=""https://avatars.githubusercontent.com/u/14500?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=dalloliogm"">dalloliogm</a>\n</td>\n</tr>\n\n\n<tr>\n<td align=""center"">\n<a href=""https://github.com/Hosseinazari"">\n<img src=""https://avatars.githubusercontent.com/u/971459?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=Hosseinazari"">Hosseinazari</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/hclimente"">\n<img src=""https://avatars.githubusercontent.com/u/5196281?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=hclimente"">hclimente</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/jooolia"">\n<img src=""https://avatars.githubusercontent.com/u/1899722?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=jooolia"">jooolia</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/MajoroMask"">\n<img src=""https://avatars.githubusercontent.com/u/19700954?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=MajoroMask"">MajoroMask</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/MatthieuStigler"">\n<img src=""https://avatars.githubusercontent.com/u/108840?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=MatthieuStigler"">MatthieuStigler</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/mhils"">\n<img src=""https://avatars.githubusercontent.com/u/1019198?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=mhils"">mhils</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/das-intensity"">\n<img src=""https://avatars.githubusercontent.com/u/12521554?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=das-intensity"">das-intensity</a>\n</td>\n</tr>\n\n\n<tr>\n<td align=""center"">\n<a href=""https://github.com/peranti"">\n<img src=""https://avatars.githubusercontent.com/u/9472741?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=peranti"">peranti</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/RahulHP"">\n<img src=""https://avatars.githubusercontent.com/u/4871132?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=RahulHP"">RahulHP</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/raulzr"">\n<img src=""https://avatars.githubusercontent.com/u/13007941?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=raulzr"">raulzr</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/richardfergie"">\n<img src=""https://avatars.githubusercontent.com/u/2488905?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=richardfergie"">richardfergie</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/zeehio"">\n<img src=""https://avatars.githubusercontent.com/u/75441?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=zeehio"">zeehio</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/simoncarrignon"">\n<img src=""https://avatars.githubusercontent.com/u/4749455?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=simoncarrignon"">simoncarrignon</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/smoe"">\n<img src=""https://avatars.githubusercontent.com/u/207407?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=smoe"">smoe</a>\n</td>\n</tr>\n\n\n<tr>\n<td align=""center"">\n<a href=""https://github.com/covoes"">\n<img src=""https://avatars.githubusercontent.com/u/4109501?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=covoes"">covoes</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/mirca"">\n<img src=""https://avatars.githubusercontent.com/u/13077051?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=mirca"">mirca</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/adriandiazlab"">\n<img src=""https://avatars.githubusercontent.com/u/50892826?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=adriandiazlab"">adriandiazlab</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/aleszib"">\n<img src=""https://avatars.githubusercontent.com/u/9567948?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=aleszib"">aleszib</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/cynthiahqy"">\n<img src=""https://avatars.githubusercontent.com/u/29718979?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=cynthiahqy"">cynthiahqy</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/etheleon"">\n<img src=""https://avatars.githubusercontent.com/u/2868858?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=etheleon"">etheleon</a>\n</td>\n<td align=""center"">\n<a href=""https://github.com/stnava"">\n<img src=""https://avatars.githubusercontent.com/u/324222?v=4"" width=""100px;"" alt=""""/>\n</a><br>\n<a href=""https://github.com/igraph/rigraph/commits?author=stnava"">stnava</a>\n</td>\n</tr>\n\n</table>\n\n<!-- markdownlint-enable -->\n<!-- prettier-ignore-end -->\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n\n",532,mathematics,R,10,Makefile,R,Tcl,HTML,CSS,Shell,C,C++,Python,CMake,,,,,,,,,,,,,,,,,,,563,100,451,12,118,60,536,515749,200,858,648,210,706de5529bd67acd8d03e04ef3b0b73750afcad4,Bump version to 2.0.3.9047,2024-07-19T01:48:27Z,krlmlr,krlmlr@users.noreply.github.com,krlmlr,igraph 2.0.2,"See <https://github.com/igraph/rigraph/blob/f3fa58b/src/vendor/cigraph/CHANGELOG.md> for a complete changelog of the bundled C core, and <https://github.com/igraph/rigraph/compare/1bd2bf79..f3fa58b#diff-aeb78e0159780a9b26daabaf6f95f450b0cfec7161fc735f27ad69145a57dc84> for the changes since the igraph 2.0.1.\n(A permanent link to the most recent changelog of the C core used in the R package is\n<https://github.com/igraph/rigraph/blob/main/src/vendor/cigraph/CHANGELOG.md>.)\n\n## Bug fixes\n\n- `g + vertices(1, 2, foo = 3)` works again, regression introduced in igraph 2.0.0 (#1247).\n- `sample_pa()` respects the `out.seq` and `out.dist` arguments again, regression introduced in igraph 2.0.0 (#1226).\n- `isomorphisms()` and `subgraph_isomorphisims(method = ""vf2"")` work again, regression introduced in 2.0.0 (#1219).\n- `biconnected_components()` now returns edge and vertex sequences again, regression introduced in 2.0.0 (#1213).\n- Remove zeros from the `order` and `order.out` components returned by `dfs()`, regression introduced in 2.0.0 (#1179).\n- Memory leaks when converting data to C (#1196).\n\n## Features\n\n- `realize_bipartite_degseq()` creates a bipartite graph from two degree sequences (#1212).\n- `is_biconnected()` checks if a graph is biconnected (#1204).\n- `distances()` now supports the Floyd-Warshall algorithm (#1186).\n\n## Documentation\n\n- Use more culturally diverse names in intro vignettes (#1246).\n- Formatting tweaks in introductory vignettes (#1243).\n- Recommend {pak} instead of {remotes} (#1228).\n- Fix typo in `mean_distance()` docs.\n- Update troubleshooting document, emphasize issues with Anaconda environments (#1209).\n- Improved docs for shortest path functions (#1201).\n- Document `""dsatur""` heuristic for `greedy_vertex_coloring()` (#1206).\n- Remove scg related docs (#1167).\n- Fix typo in `?articulation_points` (#1191).\n- Improve installation and troubleshooting instructions (#1184).\n- Improve docs of assortativity (#1151).\n\n## Testing\n\n- Add tests for `isomorphisms()` and `subgraph_isomorphisms()` (#1225).\n\n## Packaging\n\n- Always use bundled mini-gmp (#1233).\n- `config.h` defines `HAVE___UINT128_T` (#1216).\n- Do not rely on `which` program during configuration (#1232).\n- `configure` manage libxml multiple include paths (#1197).\n- Remove empty string in `configure` (#1235).\n- Link Fortran runtime on Windows as needed by arpack. (#1215).\n- Workaround for deprecated enum values not being supported with old GCC (#1205).\n- `HAVE_GFORTRAN` flag for `win` and `ucrt` (#1171).\n- `make_empty_graph()` is now fully auto-generated (#1068).\n- Eliminate manual edits from autogenerated files (#1207).\n- Add read-only comments for RStudio IDE (#1152).\n\n## Internal\n\n- Remove unused patch files (#1234).\n- Update stimulus to 0.21.4 (#1210).\n- Avoid duplicate objects (#1223).\n- Eliminate a compiler warning from simpleraytracer (#1185).\n\n",v2.0.2,,,github-actions[bot],,rigraph,igraph,17,complex-networks,graph-algorithms,graph-theory,mathematics,network-analysis,network-graph,r,,,,,,,,,,,,,,/igraph/rigraph,271,35,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/igraph/python-igraph,https://github.com/igraph/python-igraph,0,,0,0,0,0,0,0,0,1,1,0,0,0,Python interface for igraph,"\n[![Build and test with tox](https://github.com/igraph/python-igraph/actions/workflows/build.yml/badge.svg)](https://github.com/igraph/python-igraph/actions/workflows/build.yml)\n[![PyPI pyversions](https://img.shields.io/pypi/pyversions/igraph)](https://pypi.python.org/pypi/igraph)\n[![PyPI wheels](https://img.shields.io/pypi/wheel/igraph.svg)](https://pypi.python.org/pypi/igraph)\n[![Documentation Status](https://readthedocs.org/projects/igraph/badge/?version=latest)](https://igraph.readthedocs.io/)\n\n# Python interface for the igraph library\n\nigraph is a library for creating and manipulating graphs.\nIt is intended to be as powerful (ie. fast) as possible to enable the\nanalysis of large graphs.\n\nThis repository contains the source code to the Python interface of\nigraph.\n\nSince version 0.10.2, the documentation is hosted on\n[readthedocs](https://igraph.readthedocs.io). Earlier versions are documented\non [our old website](https://igraph.org/python/versions/0.10.1/).\n\nigraph is a collaborative work of many people from all around the world —\nsee the [list of contributors here](./CONTRIBUTORS.md).\n\n## Citation\n\nIf you use igraph in your research, please cite\n\n> Csardi, G., & Nepusz, T. (2006). The igraph software package for complex network research. InterJournal, Complex Systems, 1695.\n\n# Installation\n\nWe aim to provide wheels on PyPI for most of the stock Python versions;\ntypically at least the three most recent minor releases from Python 3.x.\nTherefore, running the following command should work without having to compile\nanything during installation:\n\n```\npip install igraph\n```\n\nSee details in [Installing Python Modules](https://docs.python.org/3/installing/).\n\n## Installation from source with pip on Debian / Ubuntu and derivatives\n\nIf you need to compile igraph from source for some reason, you need to\ninstall some dependencies first:\n\n```\nsudo apt install build-essential python-dev libxml2 libxml2-dev zlib1g-dev\n```\n\nand then run\n\n```\npip install igraph\n```\n\nThis should compile the C core of igraph as well as the Python extension\nautomatically.\n\n## Installation from source on Windows\n\nIt is now also possible to compile `igraph` from source under Windows for\nPython 3.7 and later. Make sure that you have Microsoft Visual Studio 2015 or\nlater installed, and of course Python 3.7 or later. First extract the source to\na suitable directory. If you launch the Developer command prompt and navigate to\nthe directory where you extracted the source code, you should be able to build\nand install igraph using `pip install .`, assuming that you have `pip`\ninstalled in your Python environment.\n\nYou may need to set the architecture that you are building on explicitly by setting the environment variable\n\n```\nset IGRAPH_CMAKE_EXTRA_ARGS=-A [arch]\n```\n\nwhere `[arch]` is either `Win32` for 32-bit builds or `x64` for 64-bit builds.\nAlso, when building in MSYS2, you need to set the `SETUPTOOLS_USE_DISTUTILS`\nenvironment variable to `stdlib`; this is because MSYS2 uses a patched version\nof `distutils` that conflicts with `setuptools >= 60.0`.\n\n> [!TIP]\n> You need the following packages:\n> `$MINGW_PACKAGE_PREFIX-python-pip $MINGW_PACKAGE_PREFIX-python-setuptools $MINGW_PACKAGE_PREFIX-cc $MINGW_PACKAGE_PREFIX-cmake`\n\n### Enabling GraphML\n\nBy default, GraphML is disabled, because `libxml2` is not available on Windows in\nthe standard installation. You can install `libxml2` on Windows using\n[`vcpkg`](https://github.com/Microsoft/vcpkg). After installation of `vcpkg` you\ncan install `libxml2` as follows\n\n```\nvcpkg.exe install libxml2:x64-windows-static-md\n```\n\nfor 64-bit version (for 32-bit versions you can use the `x86-windows-static-md`\ntriplet). You need to integrate `vcpkg` in the build environment using\n\n```\nvcpkg.exe integrate install\n```\n\nThis mentions that\n\n> CMake projects should use: `-DCMAKE_TOOLCHAIN_FILE=[vcpkg build script]`\n\nwhich we will do next. In order to build `igraph` correctly, you also\nneed to set some other environment variables before building `igraph`:\n\n```\nset IGRAPH_CMAKE_EXTRA_ARGS=-DVCPKG_TARGET_TRIPLET=x64-windows-static-md -DCMAKE_TOOLCHAIN_FILE=[vcpkg build script]\nset IGRAPH_EXTRA_LIBRARY_PATH=[vcpkg directory]/installed/x64-windows-static-md/lib/\nset IGRAPH_STATIC_EXTENSION=True\nset IGRAPH_EXTRA_LIBRARIES=libxml2,lzma,zlib,iconv,charset\nset IGRAPH_EXTRA_DYNAMIC_LIBRARIES: wsock32,ws2_32\n```\n\nYou can now build and install `igraph` again by simply running `pip install .`.\nPlease make sure to use a clean source tree, if you built previously without\nGraphML, it will not update the build.\n\n## Linking to an existing igraph installation\n\nThe source code of the Python package includes the source code of the matching\nigraph version that the Python interface should compile against. However, if\nyou want to link the Python interface to a custom installation of the C core\nthat has already been compiled and installed on your system, you can ask our\nbuild system to use the pre-compiled version. This option requires that your\ncustom installation of igraph is discoverable with `pkg-config`. First, check\nwhether `pkg-config` can tell you the required compiler and linker flags for\nigraph:\n\n```bash\npkg-config --cflags --libs igraph\n```\n\nIf `pkg-config` responds with a set of compiler and linker flags and not an\nerror message, you are probably okay. You can then proceed with the\ninstallation using pip after setting the environment variable named\n`IGRAPH_USE_PKG_CONFIG` to `1` to indicate that you want to use an\nigraph instance discoverable with `pkg-config`:\n\n```bash\nIGRAPH_USE_PKG_CONFIG=1 pip install igraph\n```\n\nAlternatively, if you have already downloaded and extracted the source code\nof igraph, you can run `pip install` on the source tree directly:\n\n```bash\nIGRAPH_USE_PKG_CONFIG=1 pip install .\n```\n\n(Note that you need the `IGRAPH_USE_PKG_CONFIG=1` environment variable\nfor both invocations, otherwise the call to `pip install` would still\nbuild the vendored C core instead of linking to an existing installation).\n\nThis option is primarily intended for package maintainers in Linux\ndistributions so they can ensure that the packaged Python interface links to\nthe packaged igraph library instead of bringing its own copy.\n\nIt is also useful on macOS if you want to link to the igraph library installed\nfrom Homebrew.\n\nDue to the lack of support of `pkg-config` on MSVC, it is currently not\npossible to build against an external library on MSVC.\n\nIn case you are already using a MSYS2/[MinGW](https://www.mingw-w64.org/) and already have\n[mingw-w64-igraph](https://packages.msys2.org/base/mingw-w64-igraph) installed,\nsimply type:\n```\nIGRAPH_USE_PKG_CONFIG=1 SETUPTOOLS_USE_DISTUTILS=stdlib pip install igraph\n```\nto build.\n\n**Warning:** the Python interface is guaranteed to work only with the same\nversion of the C core that is vendored inside the `vendor/source/igraph`\nfolder. While we try hard not to break API or ABI in the C core of igraph\nbetween minor versions in the 0.x branch and we will keep on doing so for major\nversions once 1.0 is released, there are certain functions in the C API that\nare marked as _experimental_ (see the documentation of the C core for details),\nand we reserve the right to break the APIs of those functions, even if they are\nalready exposed in a higher-level interface. This is because the easiest way to\ntest these functions in real-life research scenarios is to expose them in one\nof the higher level interfaces. Therefore, if you unbundle the vendored source\ncode of igraph and link to an external version instead, we can make no\nguarantees about stability unless you link to the exact same version as the\none we have vendored in this source tree.\n\nIf you are curious about which version of the Python interface is compatible\nwith which version of the C core, you can look up the corresponding tag in\nGithub and check which revision of the C core the repository points to in\nthe `vendor/source/igraph` submodule.\n\n## Compiling the development version\n\nIf you want to install the development version, the easiest way to do so is to\ninstall it using\n\n```bash\npip install git+https://github.com/igraph/python-igraph\n```\n\nThis automatically fetches the development version from the repository, builds\nthe package and installs it. By default, this will install the Python interface\nfrom the `main` branch, which is used as the basis for the development of the\ncurrent release series. Unstable and breaking changes are being made in the\n`develop` branch. You can install this similarly by doing\n\n```bash\npip install git+https://github.com/igraph/python-igraph@develop\n```\n\nIn addition to `git`, the installation of the development version requires some\nadditional dependencies, read further below for details.\n\nFor more information about installing directly from `git` using `pip` see\nhttps://pip.pypa.io/en/stable/topics/vcs-support/#git.\n\nAlternatively, you can clone this repository locally. This repository contains a\nmatching version of the C core of `igraph` as a git submodule. In order to\ninstall the development version from source, you need to instruct git to check\nout the submodules first:\n\n```bash\ngit submodule update --init\n```\n\nCompiling the development version additionally requires `flex` and `bison`. You\ncan install those on Ubuntu using\n\n```bash\nsudo apt install bison flex\n```\n\nOn macOS you can install these from Homebrew or MacPorts. On Windows you can\ninstall `winflexbison3` from Chocolatey.\n\nThen you can install the package directly with `pip` (see also the previous section):\n\n```bash\npip install .\n```\n\nIf you would like to create a source distribution or a Python wheel instead of\ninstalling the module directly in your Python environment, use a standard build\nfrontend like [build](https://pypa-build.readthedocs.io/en/stable/). If you\nuse [pipx](https://pypa.github.io/pipx/) to isolate command-line Python tools\nin their own separate virtualenvs, you can simply run:\n\n```bash\npipx run build\n```\n\n### Running unit tests\n\nUnit tests can be executed from within the repository directory with `tox` or\nwith the built-in `unittest` module:\n\n```bash\npython -m unittest\n```\n\nNote that unit tests have additional dependencies like NumPy, PIL or\n`matplotlib`. The unit test suite will try to do its best to skip tests\nrequiring external dependencies, but if you want to make sure that all the unit\ntests are executed, either use `tox` (which will take care of installing the\ntest dependencies in a virtualenv), or install the module with the `test`\nextras:\n\n```bash\npip install '.[test]'\n```\n\n# Contributing\n\nContributions to `igraph` are welcome!\n\nIf you want to add a feature, fix a bug, or suggest an improvement, open an\nissue on this repository and we'll try to answer. If you have a piece of code\nthat you would like to see included in the main tree, open a PR on this repo.\n\nTo start developing `igraph`, follow the steps above about installing the development version. Make sure that you do so by cloning the repository locally so that you are able to make changes.\n\nFor easier development, you can install `igraph` in ""editable"" (i.e.\ndevelopment) mode so your changes in the Python source code are picked up\nautomatically by Python:\n\n```bash\npip install -e .\n```\n\nChanges that you make to the Python code do not need any extra action. However,\nif you adjust the source code of the C extension, you need to rebuild it by running\n`pip install -e .` again. Compilation of the C core of `igraph` is\ncached in ``vendor/build`` and ``vendor/install`` so subsequent builds are much\nfaster than the first one as the C core does not need to be recompiled.\n\n# Notes\n\n## Supported Python versions\n\nWe aim to keep up with the development cycle of Python and support all official\nPython versions that have not reached their end of life yet. Currently this\nmeans that we support Python 3.8 to 3.12, inclusive. Please refer to [this\npage](https://devguide.python.org/versions/) for the status of Python\nbranches and let us know if you encounter problems with `igraph` on any\nof the non-EOL Python versions.\n\nContinuous integration tests are regularly executed on all non-EOL Python\nbranches.\n\n## PyPy\n\nThis version of igraph is compatible with [PyPy](http://pypy.org/) and\nis regularly tested on [PyPy](http://pypy.org/) with ``tox``. However, the\nPyPy version falls behind the CPython version in terms of performance; for\ninstance, running all the tests takes ~5 seconds on my machine with CPython and\n~15 seconds with PyPy. This can probably be attributed to the need for\nemulating CPython reference counting, and does not seem to be alleviated by the\nJIT.\n\nThere are also some subtle differences between the CPython and PyPy versions:\n\n- Docstrings defined in the C source code are not visible from PyPy.\n\n- ``GraphBase`` is hashable and iterable in PyPy but not in CPython. Since\n  ``GraphBase`` is internal anyway, this is likely to stay this way.\n",1272,mathematics,Python,6,Python,Shell,C,Makefile,C++,Dockerfile,,,,,,,,,,,,,,,,,,,,,,,222,15,203,4,7,72,0,35103,248,550,508,42,f04cd570d6fcc64f8845c52d25ad6d3406566fd8,chore: add some modifications to the Sphinx config file based on a no…,2024-07-17T09:39:00Z,Tamas Nepusz,ntamas@gmail.com,ntamas,igraph 0.11.6,### Added\r\n\r\n - Added `Graph.Hypercube()` for creating n-dimensional hypercube graphs.\r\n - Added `Graph.Chung_Lu()` for sampling from the Chung-Lu model as well as several related models.\r\n - Added `Graph.is_complete()` to test if there is a connection between all distinct pairs of vertices.\r\n - Added `Graph.is_clique()` to test if a set of vertices forms a clique.\r\n - Added `Graph.is_independent_vertex_set()` to test if some vertices form an independent set.\r\n - Added `Graph.mean_degree()` for a convenient way to compute the average degree of a graph.\r\n\r\n### Changed\r\n\r\n - The C core of igraph was updated to version 0.10.13.\r\n - `Graph.rewire()` now attempts to perform edge swaps 10 times the number of edges by default.\r\n - Error messages issued when an attribute is not found now mention the name and type of that attribute.\r\n\r\n**Full Changelog**: https://github.com/igraph/python-igraph/compare/0.11.5...0.11.6,0.11.6,Tamás Nepusz,,ntamas,GNU General Public License v2.0,python-igraph,igraph,29,complex-networks,graph-algorithms,graph-theory,mathematics,network-analysis,network-graph,python,,,,,,,,,,,,,,/igraph/python-igraph,47,35,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/igraph/igraph,https://github.com/igraph/igraph,0,,,0,1,0,0,0,0,1,0,0,0,0,Library for the analysis of networks,"[![Build Status on Azure Pipelines](https://dev.azure.com/igraph-team/igraph/_apis/build/status/igraph.igraph?branchName=master)](https://dev.azure.com/igraph-team/igraph/_build/latest?definitionId=1&branchName=master)\n![Build Status on Github Actions](https://github.com/igraph/igraph/workflows/MINGW/badge.svg?branch=master)\n[![codecov](https://codecov.io/gh/igraph/igraph/branch/master/graph/badge.svg?token=xGFabHJE2I)](https://codecov.io/gh/igraph/igraph)\n[![DOI](https://zenodo.org/badge/8546198.svg)](https://zenodo.org/badge/latestdoi/8546198)\n\nThe igraph library\n------------------\n\nigraph is a C library for complex network analysis and graph theory, with\nemphasis on efficiency, portability and ease of use.\n\nSee https://igraph.org for installation instructions and documentation.\n\nigraph can also be used from:\n\n - R — https://github.com/igraph/rigraph\n - Python — https://github.com/igraph/python-igraph\n - Mathematica — https://github.com/szhorvat/IGraphM\n\nigraph is a collaborative work of many people from all around the world —\nsee the [list of contributors here](./CONTRIBUTORS.md). If you would like\nto contribute yourself, [click here to see how you can\nhelp](./CONTRIBUTING.md).\n\nCitation\n--------\n\nIf you use igraph in your research, please cite\n\n> Csardi, G., & Nepusz, T. (2006). The igraph software package for complex network research. InterJournal, Complex Systems, 1695.\n",1698,mathematics,C,13,Shell,C,C++,Makefile,Python,Fortran,Lex,Yacc,sed,CMake,Game Maker Language,Mathematica,Emacs Lisp,,,,,,,,,,,,,,,,725,81,630,14,41,65,0,134432,399,1893,1657,236,1503c175708209d93a6f5edbd9a7899008830eb7,refactor: rename vector_contains() parameter in header as well,2024-07-18T14:19:31Z,Szabolcs Horvát,szhorvat@gmail.com,szhorvat,igraph 0.10.13,"### Added\r\n\r\n - `igraph_bitset_fill()` sets all elements of a bitset to the same value (experimental function).\r\n - `igraph_bitset_null()` clears all elements of a bitset (experimental function).\r\n - `igraph_bitset_is_all_zero()`, `igraph_bitset_is_all_one()`, `igraph_bitset_is_any_zero()`, `igraph_bitset_is_any_one()` check if any/all elements of a bitset are zeros/ones (experimental functions).\r\n - `igraph_chung_lu_game()` implements the classic Chung-Lu model, as well as a number of its variants (experimental function).\r\n - `igraph_mean_degree()` computes the average of vertex degrees (experimental function).\r\n - `igraph_count_loops()` counts self-loops in the graph (experimental function).\r\n - `igraph_is_clique()` checks if all pairs within a set of vertices are connected (experimental function).\r\n - `igraph_is_independent_vertex_set()` checks if no pairs within a set of vertices are connected (experimental function).\r\n - `igraph_hypercube()` creates a hypercube graph (experimental function).\r\n - `igraph_vector_intersection_size_sorted()` counts elements common to two sorted vectors (experimental function).\r\n - `igraph_stack_capacity()` returns the allocated capacity of a stack.\r\n - `igraph_vector_is_all_finite()` checks if all elements in a vector are finite (i.e. neither NaN nor Inf).\r\n\r\n### Fixed\r\n\r\n - Fixed a bug that incorrectly cached that a graph has no multiple edges when `igraph_init_adjlist()` was called with `IGRAPH_NO_LOOPS` and `IGRAPH_NO_MULTIPLE` and all the multi-edges were loop edges.\r\n - `igraph_is_forest()` would fail to set the result variable when testing for a directed forest, and it was already cached that the graph was not an undirected forest.\r\n - `igraph_hub_and_authority_scores()` no longer clips negative results to zeros when negative weights are present.\r\n - Fixed an assertion failure in `igraph_realize_bipartite_degree_sequence()` with some non-graphical degree sequences when requesting simple bipartite graphs.\r\n - `igraph_static_fitness_game()` checks the input more carefully, and avoids an infinite loop in rare edge cases, such as when (almost) all fitness scores are zero.\r\n - `igraph_arpack_rnsolve()` used the incorrect error message text for some errors. This is now corrected.\r\n - Corrected the detection of some MSVC-specific bitset intrinsics during configuration.\r\n - Corrected a bug in the fallback implementation of `igraph_bitset_countl_zero()` when `IGRAPH_INTEGER_SIZE` was set to 32. This fallback implementation was _not_ used with GCC, Clang, or MSVC.\r\n\r\n### Changed\r\n\r\n - `igraph_is_graphical()` and `igraph_is_bigraphical()` are now linear-time in all cases, and generally several times faster than before (thanks to @gendelpiekel, contributed in #2605).\r\n - `igraph_erdos_renyi_game_gnp()` can now generate graphs with more than a hundred million vertices.\r\n - `igraph_hub_and_authority_scores()` now warns when negative edge weights are present.\r\n - `igraph_layout_lgl()` now uses a BFS tree rooted in the vertex specified as `proot` to guide the layout. Previously it used an unspecified (arbitrary) spanning tree.\r\n - Updated the internal heuristics used by igraph's ARPACK interface, `igraph_arpack_rssolve()` and `igraph_arpack_rnsolve()`, to improve the robustness of calculations.\r\n - Updated the initial vector construction in `igraph_hub_and_authority_scores()`, `igraph_eigenvector_centrality()` and `igraph_(personalized_)pagerank()` with `IGRAPH_PAGERANK_ALGO_ARPACK`. This improves the robustness and convergence of calculations.\r\n\r\n### Other\r\n\r\n - Documentation improvements.\r\n - Reduced the memory usage of several functions by using bitsets instead of Boolean vectors.\r\n - `igraph_vector_intersect_sorted()` has better performance when the input vector sizes are similar.\r\n",0.10.13,Tamás Nepusz,,ntamas,GNU General Public License v2.0,igraph,igraph,34,graph-algorithms,network-graph,c,mathematics,complex-networks,graph-theory,network-analysis,,,,,,,,,,,,,,/igraph/igraph,51,83,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/idaholab/raven,https://github.com/idaholab/raven,0,Not directly science related,,0,1,1,1,0,0,0,0,0,0,1,"RAVEN is a flexible and multi-purpose probabilistic risk analysis, validation and uncertainty quantification, parameter optimization, model reduction and data knowledge-discovering framework.","# Raven <img src=""https://www.rdworldonline.com/wp-content/uploads/2023/08/RD100_2023_Winner_Logo.png"" align=""right"" height=""150"" width=""125""/>\r\n\r\nRisk Analysis Virtual Environment\r\n\r\nRAVEN is designed to perform parametric and probabilistic analysis based on the response of complex system codes. RAVEN is capable of investigating the system response as well as the input space using Monte Carlo, Grid, or Latin Hyper Cube sampling schemes, but its strength is focused toward system feature discovery, such as limit surfaces, separating regions of the input space leading to system failure, using dynamic supervised learning techniques. RAVEN includes the following major capabilities:\r\n\r\n- Sampling of codes for uncertainty quantification and reliability analyses\r\n- Generation and use of reduced-order models (also known as surrogate)\r\n- Data post-processing (time dependent and steady state)\r\n- Time dependent and steady state, statistical estimation and sensitivity analysis (mean, variance, sensitivity coefficients, etc.).\r\n\r\nThe RAVEN statistical analysis framework can be employed for several types of applications:\r\n\r\n- Uncertainty Quantification\r\n- Sensitivity Analysis / Regression Analysis\r\n- Probabilistic Risk and Reliability Analysis (PRA)\r\n- Data Mining Analysis\r\n- Model Optimization\r\n\r\nRAVEN provides a set of basic and advanced capabilities that ranges from data generation, data processing and data visualization.\r\n\r\n## Computing environment\r\n\r\n- Parallel computation capabilities (multi-thread and multi-core)\r\n- Supported operating systems: MAC, Linux and Windows\r\n- Workstation and high performance computing (HPC) systems\r\n\r\n## Forward propagation of uncertainties\r\n\r\n- MonteCarlo sampling\r\n- Grid sampling\r\n- Stratified Sampling\r\n- Factorial design\r\n- Response surface design\r\n- Generalized Polynomial Chaos (gPC) with sparse grid collocation (SGC)\r\n- Generalized Polynomial Chaos (gPC) with sparse grid collocation (SGC) using the High Dimensional Model Representation expansion (HDMR)\r\n\r\n- General combination of the above sampling strategies\r\n\r\n## Advance sampling methods\r\n\r\n- Moment driven adaptive gPC using SGC\r\n- Sobol index driven HDMR integrated using SGC over gPC basis\r\n- Adaptive sampling for limit surface finding (surrogate and multi grid based accelerations)\r\n- Dynamic event tree-based sampling (Dynamic Event Trees, Hybrid Dynamic Event Trees, Adaptive Dynamic Event Trees, Adaptive Hybrid Dynamic Event Trees)\r\n\r\n## Creation and use of reduced order models\r\n\r\n- Support Vector Machine-based surrogates\r\n- Gaussian process models\r\n- Linear models\r\n- Multi-class classifiers\r\n- Decision trees\r\n- Naive Bayes\r\n- Neighbors classifiers and regressors\r\n- Multi-dimensional interpolators\r\n- High dimension model reduction (HDMR)\r\n- Morse-Smale complex\r\n\r\n## Model capabilities\r\n\r\n- Generic interface with external codes\r\n- Custom code interfaces (third-party software(s) currently available:\r\n    - [RELAP5-3D](https://relap53d.inl.gov/SitePages/Home.aspx)\r\n    - [MELCOR](https://melcor.sandia.gov/about.html)\r\n    - [MAAP5](https://www.fauske.com/nuclear/maap-modular-accident-analysis-program)\r\n    - [MOOSE-BASED Apps](https://mooseframework.inl.gov/)\r\n    - [SCALE](https://www.ornl.gov/onramp/scale-code-system)\r\n    - [SERPENT](http://montecarlo.vtt.fi/)\r\n    - [CTF - COBRA TF](https://www.ne.ncsu.edu/rdfmg/cobra-tf/)\r\n    - [SAPHIRE](https://saphire.inl.gov/)\r\n    - [MODELICA](https://www.modelica.org/modelicalanguage)\r\n    - [DYMOLA](https://www.3ds.com/products-services/catia/products/dymola/)\r\n    - [BISON](https://bison.inl.gov/SitePages/Home.aspx)\r\n    - [RATTLESNAKE](https://rattlesnake.inl.gov/SitePages/Home.aspx)\r\n    - [MAMMOTH](https://moose.inl.gov/mammoth/SitePages/Home.aspx)\r\n    - [GOTHIC](http://www.numerical.com/products/gothic/gothic_all.php)\r\n    - [PHISICS](https://modsimcode.inl.gov/SitePages/Home.aspx)\r\n    - [NEUTRINO](http://www.neutrinodynamics.com/)\r\n    - [RAVEN running itself](https://raven.inl.gov/SitePages/Overview.aspx)\r\n\r\n- Custom ad-hoc external models (build in python internally to RAVEN)\r\n\r\n## Data Post-Processing capabilities\r\n\r\n- Data clustering\r\n- Data regression\r\n- Data dimensionality Reduction\r\n- Custom generic post-processors\r\n- Time-dependent data analysis (statistics, clustering and time warping metrics)\r\n- Data plotting\r\n\r\n## Model parameter optimization\r\n\r\n- Simultaneous perturbation stochastic approximation method\r\n\r\n## Data management\r\n\r\n- Data importing and exporting\r\n- Databases creation\r\n\r\nMore information on this project is available at the [RAVEN website](https://raven.inl.gov/SitePages/Overview.aspx).\r\n\r\nThis project is supported by [Idaho National Laboratory](https://www.inl.gov/).\r\n\r\n### Other Software\r\n[Idaho National Laboratory](https://www.inl.gov/) is a cutting edge research facility which is a constantly producing high quality research and software. Feel free to take a look at our other software and scientific offerings at:\r\n\r\n[Primary Technology Offerings Page](https://www.inl.gov/inl-initiatives/technology-deployment)\r\n\r\n[Supported Open Source Software](https://github.com/idaholab)\r\n\r\n[Raw Experiment Open Source Software](https://github.com/IdahoLabResearch)\r\n\r\n[Unsupported Open Source Software](https://github.com/IdahoLabCuttingBoard)\r\n\r\n\r\n### License\r\n\r\nFiles in crow/contrib, src/contrib and framework/contrib are third party libraries that are not part of Raven and are provided here for covenience. These are under their own, seperate licensing which is described in those directories.\r\n\r\nRaven itself is licensed as follows:\r\n\r\nCopyright 2016 Battelle Energy Alliance, LLC\r\n\r\nLicensed under the Apache License, Version 2.0 (the ""License"");\r\nyou may not use this file except in compliance with the License.\r\nYou may obtain a copy of the License at\r\n\r\n  http://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software\r\ndistributed under the License is distributed on an ""AS IS"" BASIS,\r\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\nSee the License for the specific language governing permissions and\r\nlimitations under the License.\r\n",213,uncertainty-quantification,C++,14,Makefile,Python,TeX,Shell,C++,Batchfile,SWIG,Assembly,R,CMake,C,Perl,MATLAB,Jupyter Notebook,,,,,,,,,,,,,,,1203,226,956,21,18,58,0,848280,131,1132,1041,91,469e9a2efd9eb0181ba355dee43d094c6cccce5d,Fixed multi-word arguments bug in run_tests (#2340),2024-07-17T16:40:34Z,caleb-sitton-inl,caleb.sitton@inl.gov,caleb-sitton-inl,RAVENv3.1,"## Official Release of the RAVEN code.\r\nVersion: **3.1**\r\n\r\n- [RAVEN USER MANUAL](https://github.com/user-attachments/files/16181551/raven_user_manual.pdf)\r\n- [RAVEN USER GUIDE](https://github.com/user-attachments/files/16181367/raven_user_guide.pdf)\r\n- [RAVEN THEORY MANUAL](https://github.com/user-attachments/files/16181369/raven_theory_manual.pdf)\r\n- [ANALYTICAL TEST DOCUMENTATION](https://github.com/user-attachments/files/16181371/analytic_tests.pdf)\r\n- [RAVEN PLUGIN MANUAL](https://github.com/user-attachments/files/16181372/raven_plugins_manual.pdf)\r\n\r\n## Important Features:\r\n\r\n- Updated workshop materials located at raven/doc/workshop for 2024 raven workshop\r\n- Add capability to handle interdependent functions in samplers/optimizers\r\n- Enable SimulatedAnnealing Optimizer to handle discrete variables\r\n- Enable Genetic Optimization with Ensemble Models of multiple codes\r\n- Added AutoARMA algorithm for synthetic data generation \r\n- Updated Serpent code interface\r\n- Updated Relap5 code interface \r\n- Added BayCal plugin for bayesian calibration \r\n- Enable Bayesian Optimization to use pre-trained Gaussian Process ROM \r\n\r\n## What's Changed\r\n* Adding non-conda pip package building script. by @joshua-cogliati-inl in https://github.com/idaholab/raven/pull/2259\r\n* HERON update by @PaulTalbot-INL in https://github.com/idaholab/raven/pull/2261\r\n* Only do lapack override on x86 machine. by @joshua-cogliati-inl in https://github.com/idaholab/raven/pull/2260\r\n* Removing MAMBA_SKIP by @joshua-cogliati-inl in https://github.com/idaholab/raven/pull/2270\r\n* Use the WORKING_PYTHON_COMMAND during create_libraries by @joshua-cogliati-inl in https://github.com/idaholab/raven/pull/2266\r\n* submod update by @PaulTalbot-INL in https://github.com/idaholab/raven/pull/2271\r\n* Addition of datatypes for R5 interface by @alfoa in https://github.com/idaholab/raven/pull/2212\r\n* Wangc/bison interface by @wangcj05 in https://github.com/idaholab/raven/pull/2274\r\n* Support of pip installation of repo via pip. Closes #2276 by @alfoa in https://github.com/idaholab/raven/pull/2277\r\n* Allow Bayesian Optimization to use Pre-trained GP ROM by @wangcj05 in https://github.com/idaholab/raven/pull/2280\r\n* Method to Convert InputData.ParameterInput back to ElementTree XML node(s) by @GabrielSoto-INL in https://github.com/idaholab/raven/pull/2264\r\n* Removing numexpr dependency. by @joshua-cogliati-inl in https://github.com/idaholab/raven/pull/2287\r\n* Add BayCal plugin by @wangcj05 in https://github.com/idaholab/raven/pull/2285\r\n* LaTeX _ escape improvement by @joshua-cogliati-inl in https://github.com/idaholab/raven/pull/2292\r\n* Serpent Interface Upgrade  by @alfoa in https://github.com/idaholab/raven/pull/2290\r\n* update manual by @wangcj05 in https://github.com/idaholab/raven/pull/2293\r\n* updating HERON by @GabrielSoto-INL in https://github.com/idaholab/raven/pull/2297\r\n* Defect fix of SimulatedAnnealing with 1 variable optimization by @alfoa in https://github.com/idaholab/raven/pull/2296\r\n* remove pyDOE package from contrib and install it instead by @joshua-cogliati-inl in https://github.com/idaholab/raven/pull/2298\r\n* RavenFramework tests using pip package or pre-built executable by @j-bryan in https://github.com/idaholab/raven/pull/2273\r\n* [TASK] Allow functions in samplers/optimizers to be implemented in the same python module by @alfoa in https://github.com/idaholab/raven/pull/2301\r\n* Remove unneccesary print statement by @dylanjm in https://github.com/idaholab/raven/pull/2313\r\n* Allow no precommand by @joshua-cogliati-inl in https://github.com/idaholab/raven/pull/2311\r\n* Specialized Plot sub-directory ignoring (Fix) by @alfoa in https://github.com/idaholab/raven/pull/2314\r\n* AutoARMA Algorithm for SyntheticHistory ROM by @GabrielSoto-INL in https://github.com/idaholab/raven/pull/2309\r\n* Fix defect ensemble model (with Code) and genetic algorithm by @alfoa in https://github.com/idaholab/raven/pull/2317\r\n* submodule update for HERON and TEAL by @GabrielSoto-INL in https://github.com/idaholab/raven/pull/2320\r\n* SimulatedAnnealing Discrete Variables by @alfoa in https://github.com/idaholab/raven/pull/2312\r\n* Removing specifying liblapack. by @joshua-cogliati-inl in https://github.com/idaholab/raven/pull/2327\r\n* Add capability to handle interdependent functions in samplers/optimizers by @alfoa in https://github.com/idaholab/raven/pull/2319\r\n* Arbitrary Custom Input in InputSpec by @PaulTalbot-INL in https://github.com/idaholab/raven/pull/2332\r\n* Optimization Path Plot Fix for long strings in ylabel by @GabrielSoto-INL in https://github.com/idaholab/raven/pull/2330\r\n* Update FARM user manual and citation by @wanghy-anl in https://github.com/idaholab/raven/pull/2333\r\n* Workshop2024 merge by @PaulTalbot-INL in https://github.com/idaholab/raven/pull/2328\r\n* Updating version to 3.1 by @joshua-cogliati-inl in https://github.com/idaholab/raven/pull/2335\r\n\r\n\r\n**Full Changelog**: https://github.com/idaholab/raven/compare/RAVENv3.0...RAVENv3.1\r\n\r\n## Submodule Updates:\r\nThe updates for the submodules are tracked by issue #1114. In this release, there are significant updates in following Plugins. We recommend the users to check the following links for more details.\r\nTEAL: https://github.com/idaholab/TEAL \r\nHERON: https://github.com/idaholab/HERON \r\nSR2ML: https://github.com/idaholab/SR2ML \r\nLOGOS: https://github.com/idaholab/LOGOS \r\nFARM: https://github.com/Argonne-National-Laboratory/FARM\r\n\r\n## Developers:\r\nWe would like to thank all RAVEN internal and external developers for their significant contributions.\r\n",RAVENv3.1,Congjian Wang - INL,,wangcj05,Apache License 2.0,raven,idaholab,8,risk-analysis,uncertainty-quantification,probabilistic-analysis,data-mining,optimization-algorithms,parametric-analysis,validation,model-reduction,model-calibration,,,,,,,,,,,,/idaholab/raven,12,28,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/icons8/lunacy-docs,https://github.com/icons8/lunacy-docs,0,,,0,0,0,0,0,0,1,0,0,0,0,"Documentation for Lunacy, Graphic Design Software with built-in assets","# Lunacy Documentation\n\nThis project is a collaboration to create the documentation for [Lunacy](https://icons8.com/lunacy), Graphic Design Software with built-in assets.\n\n## Contribute\n\nWe're looking for your help editing and expanding it. In particular, we'll gladly accept:\n* Spellchecks \n* Better illustrations\n* New sections\n* Video tutorials\n\n## Related\n\n* [Lunacy home page](https://icons8.com/lunacy)\n* [Release notes](https://lunacy.docs.icons8.com/release-notes/)\n* [Suggest feature](http://lunatics.icons8.com)\n",306,graphics,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30,1,24,5,3,32,1709,545529,52,52,31,21,82569c0782fe584bb395149b92f5736029265729,rn fix,2024-06-11T16:51:03Z,uksla,alsku2016@yandex.ru,uksla,Lunacy 3.9.1 Hotfix,### Bug Fixes\r\n\r\n- Space key is not working in text edit mode\r\n- Scaling issue of resizing thumbs,v3.9.1,Igor Gritsenko,,gritsenko,,lunacy-docs,icons8,4,lunacy,sketch-files,graphics,ux,ux-design,sketch,sketch-app,,,,,,,,,,,,,,/icons8/lunacy-docs,4,28,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/iced-rs/iced,https://github.com/iced-rs/iced,0,,,0,0,0,0,0,0,1,1,0,0,0,"A cross-platform GUI library for Rust, inspired by Elm","<div align=""center"">\n\n<img src=""docs/logo.svg"" width=""140px"" />\n\n# Iced\n\n[![Documentation](https://docs.rs/iced/badge.svg)][documentation]\n[![Crates.io](https://img.shields.io/crates/v/iced.svg)](https://crates.io/crates/iced)\n[![License](https://img.shields.io/crates/l/iced.svg)](https://github.com/iced-rs/iced/blob/master/LICENSE)\n[![Downloads](https://img.shields.io/crates/d/iced.svg)](https://crates.io/crates/iced)\n[![Test Status](https://img.shields.io/github/actions/workflow/status/iced-rs/iced/test.yml?branch=master&event=push&label=test)](https://github.com/iced-rs/iced/actions)\n[![Discourse](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscourse.iced.rs%2Fsite%2Fstatistics.json&query=%24.users_count&suffix=%20users&label=discourse&color=5e7ce2)](https://discourse.iced.rs/)\n[![Discord Server](https://img.shields.io/discord/628993209984614400?label=&labelColor=6A7EC2&logo=discord&logoColor=ffffff&color=7389D8)](https://discord.gg/3xZJ65GAhd)\n\nA cross-platform GUI library for Rust focused on simplicity and type-safety.\nInspired by [Elm].\n\n<a href=""https://iced.rs/examples/todos.mp4"">\n  <img src=""https://iced.rs/examples/todos.gif"" width=""275px"">\n</a>\n<a href=""https://iced.rs/examples/tour.mp4"">\n  <img src=""https://iced.rs/examples/tour.gif"" width=""273px"">\n</a>\n\n</div>\n\n## Features\n\n* Simple, easy-to-use, batteries-included API\n* Type-safe, reactive programming model\n* [Cross-platform support] (Windows, macOS, Linux, and the Web)\n* Responsive layout\n* Built-in widgets (including [text inputs], [scrollables], and more!)\n* Custom widget support (create your own!)\n* [Debug overlay with performance metrics]\n* First-class support for async actions (use futures!)\n* [Modular ecosystem] split into reusable parts:\n  * A [renderer-agnostic native runtime] enabling integration with existing systems\n  * Two [built-in renderers] leveraging [`wgpu`] and [`tiny-skia`]\n    * [`iced_wgpu`] supporting Vulkan, Metal and DX12\n    * [`iced_tiny_skia`] offering a software alternative as a fallback\n  * A [windowing shell]\n  * A [web runtime] leveraging the DOM\n\n__Iced is currently experimental software.__ [Take a look at the roadmap],\n[check out the issues], and [feel free to contribute!]\n\n[Cross-platform support]: https://raw.githubusercontent.com/iced-rs/iced/master/docs/images/todos_desktop.jpg\n[text inputs]: https://iced.rs/examples/text_input.mp4\n[scrollables]: https://iced.rs/examples/scrollable.mp4\n[Debug overlay with performance metrics]: https://iced.rs/examples/debug.mp4\n[Modular ecosystem]: ECOSYSTEM.md\n[renderer-agnostic native runtime]: runtime/\n[`wgpu`]: https://github.com/gfx-rs/wgpu\n[`tiny-skia`]: https://github.com/RazrFalcon/tiny-skia\n[`iced_wgpu`]: wgpu/\n[`iced_tiny_skia`]: tiny_skia/\n[built-in renderers]: ECOSYSTEM.md#Renderers\n[windowing shell]: winit/\n[`dodrio`]: https://github.com/fitzgen/dodrio\n[web runtime]: https://github.com/iced-rs/iced_web\n[Take a look at the roadmap]: ROADMAP.md\n[check out the issues]: https://github.com/iced-rs/iced/issues\n[feel free to contribute!]: #contributing--feedback\n\n## Overview\n\nInspired by [The Elm Architecture], Iced expects you to split user interfaces\ninto four different concepts:\n\n* __State__ — the state of your application\n* __Messages__ — user interactions or meaningful events that you care\n  about\n* __View logic__ — a way to display your __state__ as widgets that\n  may produce __messages__ on user interaction\n* __Update logic__ — a way to react to __messages__ and update your\n  __state__\n\nWe can build something to see how this works! Let's say we want a simple counter\nthat can be incremented and decremented using two buttons.\n\nWe start by modelling the __state__ of our application:\n\n```rust\n#[derive(Default)]\nstruct Counter {\n    value: i32,\n}\n```\n\nNext, we need to define the possible user interactions of our counter:\nthe button presses. These interactions are our __messages__:\n\n```rust\n#[derive(Debug, Clone, Copy)]\npub enum Message {\n    Increment,\n    Decrement,\n}\n```\n\nNow, let's show the actual counter by putting it all together in our\n__view logic__:\n\n```rust\nuse iced::widget::{button, column, text, Column};\n\nimpl Counter {\n    pub fn view(&self) -> Column<Message> {\n        // We use a column: a simple vertical layout\n        column![\n            // The increment button. We tell it to produce an\n            // `Increment` message when pressed\n            button(""+"").on_press(Message::Increment),\n\n            // We show the value of the counter here\n            text(self.value).size(50),\n\n            // The decrement button. We tell it to produce a\n            // `Decrement` message when pressed\n            button(""-"").on_press(Message::Decrement),\n        ]\n    }\n}\n```\n\nFinally, we need to be able to react to any produced __messages__ and change our\n__state__ accordingly in our __update logic__:\n\n```rust\nimpl Counter {\n    // ...\n\n    pub fn update(&mut self, message: Message) {\n        match message {\n            Message::Increment => {\n                self.value += 1;\n            }\n            Message::Decrement => {\n                self.value -= 1;\n            }\n        }\n    }\n}\n```\n\nAnd that's everything! We just wrote a whole user interface. Let's run it:\n\n```rust\nfn main() -> iced::Result {\n    iced::run(""A cool counter"", Counter::update, Counter::view)\n}\n```\n\nIced will automatically:\n\n  1. Take the result of our __view logic__ and layout its widgets.\n  1. Process events from our system and produce __messages__ for our\n     __update logic__.\n  1. Draw the resulting user interface.\n\nRead the [book], the [documentation], and the [examples] to learn more!\n\n## Implementation details\n\nIced was originally born as an attempt at bringing the simplicity of [Elm] and\n[The Elm Architecture] into [Coffee], a 2D game engine I am working on.\n\nThe core of the library was implemented during May 2019 in [this pull request].\n[The first alpha version] was eventually released as\n[a renderer-agnostic GUI library]. The library did not provide a renderer and\nimplemented the current [tour example] on top of [`ggez`], a game library.\n\nSince then, the focus has shifted towards providing a batteries-included,\nend-user-oriented GUI library, while keeping [the ecosystem] modular:\n\n<p align=""center"">\n  <a href=""ECOSYSTEM.md"">\n    <img alt=""The Iced Ecosystem"" src=""docs/graphs/ecosystem.png"" width=""80%"">\n  </a>\n</p>\n\n[this pull request]: https://github.com/hecrj/coffee/pull/35\n[The first alpha version]: https://github.com/iced-rs/iced/tree/0.1.0-alpha\n[a renderer-agnostic GUI library]: https://www.reddit.com/r/rust/comments/czzjnv/iced_a_rendereragnostic_gui_library_focused_on/\n[tour example]: examples/README.md#tour\n[`ggez`]: https://github.com/ggez/ggez\n[the ecosystem]: ECOSYSTEM.md\n\n## Contributing / Feedback\n\nContributions are greatly appreciated! If you want to contribute, please\nread our [contributing guidelines] for more details.\n\nFeedback is also welcome! You can create a new topic in [our Discourse forum] or\ncome chat to [our Discord server].\n\n## Sponsors\n\nThe development of Iced is sponsored by the [Cryptowatch] team at [Kraken.com]\n\n[book]: https://book.iced.rs/\n[documentation]: https://docs.rs/iced/\n[examples]: https://github.com/iced-rs/iced/tree/master/examples#examples\n[Coffee]: https://github.com/hecrj/coffee\n[Elm]: https://elm-lang.org/\n[The Elm Architecture]: https://guide.elm-lang.org/architecture/\n[the current issues]: https://github.com/iced-rs/iced/issues\n[contributing guidelines]: https://github.com/iced-rs/iced/blob/master/CONTRIBUTING.md\n[our Discourse forum]: https://discourse.iced.rs/\n[our Discord server]: https://discord.gg/3xZJ65GAhd\n[Cryptowatch]: https://cryptowat.ch/charts\n[Kraken.com]: https://kraken.com/\n",23490,graphics,Rust,2,Rust,WGSL,,,,,,,,,,,,,,,,,,,,,,,,,,,1079,197,836,46,21,220,0,17224,1078,1044,794,250,c851e67734ec0c761adfd7881c576856ea38734b,Fix `text::State` downcast in some widgets,2024-07-18T22:59:54Z,Héctor Ramón Jiménez,hector0193@gmail.com,hecrj,0.12.1,"### Added\r\n- `extend` and `from_vec` methods for `Column` and `Row`. [#2264](https://github.com/iced-rs/iced/pull/2264)\r\n- `PartialOrd`, `Ord`, and `Hash` implementations for `keyboard::Modifiers`. [#2270](https://github.com/iced-rs/iced/pull/2270)\r\n- `clipboard` module in `advanced` module. [#2272](https://github.com/iced-rs/iced/pull/2272)\r\n- Default `disabled` style for `checkbox` and `hovered` style for `Svg`. [#2273](https://github.com/iced-rs/iced/pull/2273)\r\n- `From<u16>` and `From<i32>` implementations for `border::Radius`. [#2274](https://github.com/iced-rs/iced/pull/2274)\r\n- `size_hint` method for `Component` trait. [#2275](https://github.com/iced-rs/iced/pull/2275)\r\n\r\n### Fixed\r\n- Black images when using OpenGL backend in `iced_wgpu`. [#2259](https://github.com/iced-rs/iced/pull/2259)\r\n- Documentation for `horizontal_space` and `vertical_space` helpers. [#2265](https://github.com/iced-rs/iced/pull/2265)\r\n- WebAssembly platform. [#2271](https://github.com/iced-rs/iced/pull/2271)\r\n- Decouple `Key` from `keyboard::Modifiers` and apply them to `text` in `KeyboardInput`. [#2238](https://github.com/iced-rs/iced/pull/2238)\r\n- Text insertion not being prioritized in `TextInput` and `TextEditor`. [#2278](https://github.com/iced-rs/iced/pull/2278)\r\n- `iced_tiny_skia` clipping line strokes. [#2282](https://github.com/iced-rs/iced/pull/2282)\r\n\r\nMany thanks to...\r\n\r\n- @bungoboingo\r\n- @PolyMeilex\r\n- @rizzen-yazston\r\n- @wash2",0.12.1,Héctor Ramón,,hecrj,MIT License,iced,iced-rs,15,gui,graphics,rust,elm,interface,widget,widgets,toolkit,user-interface,renderer-agnostic,,,,,,,,,,,/iced-rs/iced,55,196,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/helix-toolkit/helix-toolkit,https://github.com/helix-toolkit/helix-toolkit,0,,,0,0,0,0,0,0,1,1,0,0,0,Helix Toolkit is a collection of 3D components for .NET.,"[![HelixToolkit](https://img.shields.io/badge/-Helix%20Toolkit-blue)](https://github.com/helix-toolkit/helix-toolkit) \r\n\r\n<img src='https://avatars3.githubusercontent.com/u/8432523?s=200&v=4' width='64' />\r\n\r\n# Helix Toolkit\r\n\r\n**Helix Toolkit is a collection of 3D components for .NET Framework.**\r\n\r\n[**HelixToolkit.WPF:**](/Source/HelixToolkit.Wpf) \r\nAdds variety of functionalities/models on the top of internal WPF 3D models (Media3D namespace). \r\n\r\n[**HelixToolkit.Core.WPF:**](/Source/HelixToolkit.Core.Wpf) \r\nAdds variety of functionalities/models on the top of internal .NET Core WPF 3D models (Media3D namespace).\r\n\r\n[**HelixToolkit.SharpDX.WPF:**](/Source/HelixToolkit.Wpf.SharpDX) \r\nCustom 3D Engine and XAML/MVVM compatible Scene Graphs based on [SharpDX](https://github.com/sharpdx/SharpDX)(DirectX 11) for high performance usage.\r\n\r\n[**HelixToolkit.UWP:**](/Source/HelixToolkit.UWP) \r\nCustom 3D Engine and XAML/MVVM compatible Scene Graphs based on [SharpDX](https://github.com/sharpdx/SharpDX)(DirectX 11) for Universal Windows App.\r\n\r\n[**HelixToolkit.SharpDX.Core:**](/Source/HelixToolkit.SharpDX.Core) \r\nCustom 3D Engine and Scene Graphs based on [SharpDX](https://github.com/sharpdx/SharpDX)(DirectX 11) for netstandard and .NET Core.\r\n\r\n[**HelixToolkit.SharpDX.Core.Wpf:**](/Source/HelixToolkit.SharpDX.Core.Wpf) \r\nWpf Wrapper Components based on `HelixToolkit.SharpDX.Core` for .NET Core Wpf.\r\n\r\n[**HelixToolkit.WinUI:**](/Source/HelixToolkit.WinUI) \r\nCustom 3D Engine and XAML/MVVM compatible Scene Graphs based on [SharpDX](https://github.com/sharpdx/SharpDX)(DirectX 11) for WinUI.\r\n\r\n\r\n[**HelixToolkit.SharpDX.Assimp:**](/Source/HelixToolkit.Wpf.SharpDX.Assimp) \r\n[Assimp.Net](https://bitbucket.org/Starnick/assimpnet/src/master/) 3D model importer/expoter support for HelixToolkit.SharpDX Components.\r\n\r\n[**Examples:**](/Source/Examples)\r\nPlease download full source code to run examples. Or download [compiled version](https://ci.appveyor.com/project/objorke/helix-toolkit/branch/master/artifacts)\r\n\r\n[![License: MIT](https://img.shields.io/github/license/helix-toolkit/helix-toolkit)](https://github.com/helix-toolkit/helix-toolkit/blob/develop/LICENSE)\r\n[![Build status](https://ci.appveyor.com/api/projects/status/vbrornad55ln8tp4?svg=true)](https://ci.appveyor.com/project/holance/helix-toolkit-qqf1e)\r\n[![Release](https://img.shields.io/github/release/helix-toolkit/helix-toolkit.svg?style=popout)](https://www.nuget.org/packages?q=Helix-Toolkit)\r\n[![Chat](https://img.shields.io/gitter/room/helix-toolkit/helix-toolkit.svg)](https://gitter.im/helix-toolkit/helix-toolkit)\r\n\r\nDescription         | Value\r\n--------------------|-----------------------\r\nWeb page            | http://helix-toolkit.github.io/\r\nWiki                | https://github.com/helix-toolkit/helix-toolkit/wiki\r\nDocumentation       | http://helix-toolkit.readthedocs.io/\r\nChat                | https://gitter.im/helix-toolkit/helix-toolkit\r\nSource repository   | http://github.com/helix-toolkit/helix-toolkit\r\nLatest build        | http://ci.appveyor.com/project/holance/helix-toolkit\r\nIssue tracker       | http://github.com/helix-toolkit/helix-toolkit/issues\r\nNuGet packages      | http://www.nuget.org/packages?q=HelixToolkit\r\nNightly build       | https://www.myget.org/F/helixtoolkit-nightly\r\nStackOverflow       | http://stackoverflow.com/questions/tagged/helix-3d-toolkit\r\nTwitter             | https://twitter.com/hashtag/Helix3DToolkit\r\n\r\n## Project Build\r\n\r\n**Visual Studio 2019. Windows 10 SDK (Min Ver.10.0.18362.0).**\r\n\r\n## Notes\r\n\r\n#### 1. Right-handed Cartesian coordinate system and row major matrix by default\r\nHelixToolkit default is using right-handed Cartesian coordinate system, including Meshbuilder etc. To use left-handed Cartesian coordinate system (Camera.CreateLeftHandedSystem = true), user must manually correct the triangle winding order or IsFrontCounterClockwise in raster state description if using SharpDX. Matrices are row major by default.\r\n\r\n#### 2. Performance [Topics](https://github.com/helix-toolkit/helix-toolkit/wiki/Tips-on-performance-optimization-(WPF.SharpDX-and-UWP)) for WPF.SharpDX and UWP.\r\n\r\n#### 3. Following features are not supported currently on FeatureLevel 10 graphics card:\r\nFXAA, Order Independant Transparent Rendering, Particle system, Tessellation.\r\n\r\n#### 4. [Wiki](https://github.com/helix-toolkit/helix-toolkit/wiki) and useful [External Resources](https://github.com/helix-toolkit/helix-toolkit/wiki/External-References) on Computer Graphics.\r\n\r\n## HelixToolkit Library Structure\r\n\r\n### WPF Internal 3D Engine (DirectX9)\r\n\r\n```mermaid\r\ngraph TD\r\n    wpf[WPF Framework] --> hxWpf[HelixToolkit.Wpf]\r\n    wpf --> hxCoreWpf[HelixToolkit.Core.Wpf]\r\n```\r\n### HelixToolkit DirectX11 Engine\r\n\r\n```mermaid\r\ngraph TD\r\n    hx[HelixToolkit] --> dx11[DirectX11 Engine]    \r\n    dx11 --> hxSharpDX[HelixToolkit.Wpf.SharpDX]\r\n    dx11 --> hxUWP[HelixToolkit.UWP]\r\n    dx11 --> hxCore[HelixToolkit.SharpDX.Core]\r\n    hxCore --> hxWinUI[HelixToolkit.SharpDX.Core.Wpf]\r\n    hxCore --> hxSharpDXCoreWpf[HelixToolkit.WinUI]\r\n    hxSharpDX --> hxAssimp[HelixToolkit.SharpDX.Assimp]\r\n    hxUWP --> hxAssimp\r\n    hxCore --> hxAssimp\r\n```\r\n\r\n## Bug Report\r\nPlease use the following template to report bugs.\r\n\r\n- Version: [Example: 2.20]\r\n- Package: [Example: Helixtoolkit.Wpf]\r\n- Issue: \r\n- Reproduce Steps:\r\n- Sample Code:\r\n\r\n## News\r\n#### 2024-02-27\r\n[v2.25.0](https://github.com/helix-toolkit/helix-toolkit/releases/tag/v2.25.0) releases are available on nuget. [Release Note](/CHANGELOG.md)\r\n- [WPF](https://www.nuget.org/packages/HelixToolkit.Wpf/2.25.0)\r\n- [Core.WPF](https://www.nuget.org/packages/HelixToolkit.Core.Wpf/2.25.0)\r\n- [WPF.Input](https://www.nuget.org/packages/HelixToolkit.Wpf.Input/2.25.0)\r\n- [WPF.SharpDX](https://www.nuget.org/packages/HelixToolkit.Wpf.SharpDX/2.25.0)\r\n- [UWP](https://www.nuget.org/packages/HelixToolkit.UWP/2.25.0)\r\n- [SharpDX.Core](https://www.nuget.org/packages/HelixToolkit.SharpDX.Core/2.25.0)\r\n- [SharpDX.Core.Wpf](https://www.nuget.org/packages/HelixToolkit.SharpDX.Core.Wpf/2.25.0)\r\n- [WinUI](https://www.nuget.org/packages/HelixToolkit.WinUI/2.25.0)\r\n- [SharpDX.Assimp](https://www.nuget.org/packages/HelixToolkit.SharpDX.Assimp/2.25.0)\r\n\r\n#### Changes (Please refer to [Release Note](https://github.com/helix-toolkit/helix-toolkit/blob/master/CHANGELOG.md) for details)\r\n\r\n#### 2023-03-17\r\nNightly build myget feed link has been updated to: https://www.myget.org/F/helixtoolkit-nightly\r\n",1818,graphics,C#,3,C#,Batchfile,HLSL,,,,,,,,,,,,,,,,,,,,,,,,,,1059,85,974,0,6,75,0,358060,657,1190,908,282,5115d4d98c336173c1b58008ac28a830b801d931,Fix zoom extend. #2180 (#2245),2024-06-26T22:24:09Z,Lunci,lunci.app@gmail.com,holance,v2.25.0,"## [2.25.0] Release - 2024-02-27\r\n\r\n### Added\r\n1. Add CombinedSelectionCommand. (WPF)\r\n1. Allow importer to load obj,off,ply,stl from application resources. (WPF)\r\n1. Add AngleProperty for BillboardTextVisual3D (WPF)\r\n1. Add Angle Property for TextVisual3D. (WPF)\r\n1. Added ability to create tiled image materials with controlled vertical and horizontal scale factor. (WPF)\r\n1. Support color inner rectangle of RectangleAdorner. (WPF)\r\n1. Add FillRectangleBrush property for RectangleSelectionCommand, CombinedSelectionCommand. (WPF)\r\n\r\n### Improvement\r\n1. Re-implmenet zoom extents algorithm.(WPF.SharpDX/UWP/WinUI)\r\n1. Get rid of the sync context in RenderHostBase. Get rid of unnecessary parallel frustum tests. Change SceneNode ItemsInternal from ObservableCollection to own implementation.(WPF.SharpDX/UWP/WinUI)\r\n1. Improve dual depth peeling implementation. (WPF.SharpDX/UWP/WinUI)\r\n   \r\n### Fixed\r\n1. Fixed NullRefrenceException in CuttingPlaneGroup.CuttingPlanes (WPF)\r\n1. Fixed null exception XamlExporter (WPF)\r\n1. Fix outline highlight not showing up while rending backface only. (WPF.SharpDX/UWP/WinUI)\r\n1. Fix particle system not able to render 2x2 particle texture. (WPF.SharpDX/UWP/WinUI)\r\n1. Fix rendering doesn't update after removing item from Viewport3DX.Items.(WPF.SharpDX/UWP/WinUI)\r\n1. Fix OIT mode switch not working in UWP demo.(WPF.SharpDX/UWP/WinUI)\r\n2. Fix WinUI crash. (WinUI)\r\n\r\n\r\n**Full Changelog**: https://github.com/helix-toolkit/helix-toolkit/compare/v2.24.0...v2.25.0",v2.25.0,Lunci,,holance,MIT License,helix-toolkit,helix-toolkit,34,sharpdx,c-sharp,wpf,helix-toolkit,uwp,3d,3d-engine,data-visualization,dotnet,directx,graphics,,,,,,,,,,/helix-toolkit/helix-toolkit,34,114,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/HandmadeMath/HandmadeMath,https://github.com/HandmadeMath/HandmadeMath,0,,,0,0,0,0,0,0,1,0,0,0,0,A simple math library for games and computer graphics. Compatible with both C and C++. Public domain and easy to modify.,"# Handmade Math\n\nA single-file, cross-platform, public domain graphics math library for both C and C++. Supports vectors, matrices, quaternions, and all the utilities you'd expect.\n\nTo get started, go download [the latest release](https://github.com/HandmadeMath/HandmadeMath/releases).\n\n> If you are upgrading to Handmade Math 2.0, save yourself some time and use our [automatic update tool](./update).\n\nHere's what sets Handmade Math apart:\n\n- **A simple single-header library.** Just `#include ""HandmadeMath.h""`.\n- **Supports both C and C++.** While libraries like GLM only support C++, Handmade Math supports both C and C++, with convenient overloads wherever possible. For example, C++ codebases get operator overloading, and C11 codebases get `_Generic` versions of common operations.\n- **Supports all graphics APIs.** Handmade Math has left- and right-handed versions of each operation, as well as support for zero-to-one and negative-one-to-one NDC conventions.\n- **Swizzling, sort of.** Handmade Math's vector types use unions to provide several ways of accessing the same underlying data. For example, the components of an `HMM_Vec3` can be accessed as `XYZ`, `RGB`, or `UVW` - or subsets can be accessed like `.XY` and `.YZ`.\n- **Your choice of angle unit.** While Handmade Math uses radians by default, you can configure it to use degrees or [turns](https://www.computerenhance.com/p/turns-are-better-than-radians) instead.\n\n\n## Usage\n\nSimply `#include ""HandmadeMath.h""`. All functions are `static inline`, so there is no need for an ""implementation"" file as with some other single-header libraries.\n\nA few config options are available. See the header comment in [the source](./HandmadeMath.h) for details.\n\n\n## FAQ\n\n**What conventions does HMM use, e.g. row vs. column major, handedness, etc.?**\n\nHandmade Math's matrices are column-major, i.e. data is stored by columns, then rows. It also assumes column vectors, i.e. vectors are written vertically and matrix-vector multiplication is `M * V` instead of `V * M`. For more information, see [this issue](https://github.com/HandmadeMath/HandmadeMath/issues/124#issuecomment-775737253).\n\nFor other properties, we provide variants for each common convention. Functions that care about handedness have left-handed (`LH`) and right-handed (`RH`) variants. Projection functions have zero-to-one (`ZO`) and negative-one-to-one (`NO`) variants for different NDC conventions.\n\n**What if I don't want the `HMM_` prefix?**\n\nDo a find and replace in the library source.\n\n**What's the license?**\n\nThis library is in the public domain. You can do whatever you want with it.\n\n**Where can I contact you to ask questions?**\n\nFeel free to make GitHub issues for any questions, concerns, or problems you encounter.\n",1179,graphics,C,5,C++,Makefile,C,Batchfile,Python,,,,,,,,,,,,,,,,,,,,,,,,101,15,85,1,18,17,0,3502,92,71,66,5,bdc7dd2a516b08715a56f8b8eecefe44c9d68f40,Merge pull request #172 from jonasgf/whitespace,2024-05-28T14:46:45Z,Ben Visness,bvisness@users.noreply.github.com,bvisness,v2.0.0,"After far too long, we are happy to release Handmade Math 2.0. Handmade Math 2.0 introduces the following new features and changes:\r\n\r\n- **A concise naming scheme.** Handmade Math 1.0 has extremely verbose names. For version 2, we've reworked them to use a terse naming scheme that will make expressions much more readable. Here are some examples:\r\n\r\n    - `HMM_MultiplyMat4ByVec4` -> `HMM_MulM4V4`\r\n    - `HMM_MultiplyVec3f` -> `HMM_MulV3F`\r\n    - `HMM_EqualsVec4` -> `HMM_EqV4`\r\n\r\n    This is obviously an extremely breaking change, but to ease the transition, we provide a tool to automatically rename Handmade Math functions and types in your codebase. See the `update` folder of the repo.\r\n\r\n- **Matrix inverses.** We are happy to finally provide our most long-requested feature. We now have a variety of functions for matrix inverses.\r\n\r\n- **2x2 and 3x3 matrices.** We now provide types `HMM_Mat2` and `HMM_Mat3`.\r\n\r\n- **C11 generics.** C programmers can now use ""overloaded"" functions like `HMM_Add` instead of `HMM_AddV2`, `HMM_AddV3`, etc. Your expressions can now be a little more readable!\r\n\r\n- **Consistent (and configurable) angle units.** Handmade Math 1.0 used radians for some operations and degrees for others. Handmade Math 2.0 makes all functions use the same angle units - but also makes those units configurable, so you can choose to use radians, degrees, or turns across your codebase. See the documentation in `HandmadeMath.h`.\r\n\r\n- **Left-handed and right-handed operations.** Handmade Math 2.0 has both left-handed and right-handed versions of all operations where handedness applies.\r\n\r\nThis release would not be possible without the help of @dev-dwarf. Many thanks for his contributions!\r\n\r\nOther breaking changes:\r\n\r\n- If you use Handmade Math without the standard library, the way you provide custom math functions has changed. You must now `#define HANDMADE_MATH_PROVIDE_MATH_FUNCTIONS` in addition to `#define HMM_SINF` etc.\r\n- `HMM_ExpF`, `HMM_LogF`, `HMM_Power`, and `HMM_PowerF` have been removed. They were not used internally and provided no benefit.\r\n- `HMM_PREFIX` has been removed. If you wish to use a different prefix, find and replace within `HandmadeMath.h`.\r\n- Semi-breaking: `HMM_InvSqrtF` (formerly `HMM_RSquareRootF`) no longer does a fast inverse square root under any circumstances. It now always does a full-precision square root. This is to ensure consistent results on all platforms.",v2.0.0,Ben Visness,,bvisness,Creative Commons Zero v1.0 Universal,HandmadeMath,HandmadeMath,17,math,game-development,graphics,matrix,single-header,vector,,,,,,,,,,,,,,,/HandmadeMath/HandmadeMath,23,36,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/hail-is/hail,https://github.com/hail-is/hail,1,,,1,1,1,1,0,0,0,0,0,0,1,Cloud-native genomic dataframes and batch computing,"# Hail\n\n[![Zulip](https://img.shields.io/badge/zulip-join_chat-brightgreen.svg)](https://hail.zulipchat.com?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge) [![DOI](https://zenodo.org/badge/45069467.svg)](https://zenodo.org/badge/latestdoi/45069467) [![PyPI version](https://badge.fury.io/py/hail.svg)](https://badge.fury.io/py/hail)\n\n[Hail](https://hail.is) is an open-source, general-purpose, Python-based data analysis tool with additional data types and methods for working with genomic data.\n\nHail is built to scale and has first-class support for multi-dimensional structured data, like the genomic data in a genome-wide association study (GWAS).\n\nHail is exposed as a Python library, using primitives for distributed queries and linear algebra implemented in Scala, [Spark](https://spark.apache.org/docs/latest/index.html), and increasingly C++.\n\nSee the [documentation](https://hail.is/docs/0.2/) for more info on using Hail.\n\n### Community\n\nHail has been widely adopted in academia and industry, including as the analysis platform for the [genome aggregation database](https://gnomad.broadinstitute.org) and [UK Biobank rapid GWAS](https://www.nealelab.is/uk-biobank). Learn more about [Hail-powered science](https://hail.is/references.html).\n\n### Contribute\n\nIf you'd like to discuss or contribute to the development of methods or infrastructure, please:\n\n- see the [For Software Developers](https://hail.is/docs/0.2/getting_started_developing.html) section of the installation guide for info on compiling Hail\n- chat with us about development in our [Zulip chatroom](https://hail.zulipchat.com)\n- visit the [Development Forum](https://dev.hail.is) for longer-form discussions\n<!--- - read [this post]() (coming soon!) for tips on submitting a successful Pull Request to our repository --->\n\nHail uses a continuous deployment approach to software development, which means we frequently add new features. We update users about changes to Hail via the [Discussion Forum](https://discuss.hail.is). We recommend creating an account on the Discussion Forum so that you can subscribe to these updates as well.\n\n### Maintainer\n\nHail is maintained by a team in the [Neale lab](https://nealelab.is/) at the [Stanley Center for Psychiatric Research](https://www.broadinstitute.org/stanley) of the [Broad Institute of MIT and Harvard](https://www.broadinstitute.org) and the [Analytic and Translational Genetics Unit](https://www.atgu.mgh.harvard.edu/) of [Massachusetts General Hospital](https://www.massgeneral.org/).\n\nContact the Hail team at <code><a href=""mailto:hail@broadinstitute.org"">hail@broadinstitute.org</a></code>.\n\n### Citing Hail\n\nIf you use Hail for published work, please cite the software. You can get a\ncitation for the version of Hail you installed by executing:\n\n```python\nimport hail as hl\nprint(hl.citation())\n```\n\nWhich will look like:\n\n```\nHail Team. Hail 0.2.13-81ab564db2b4. https://github.com/hail-is/hail/releases/tag/0.2.13.\n```\n\n##### Acknowledgements\n\nThe Hail team has several sources of funding at the Broad Institute:\n- The Stanley Center for Psychiatric Research, which together with Neale Lab has provided an incredibly supportive and stimulating home.\n- Principal Investigators Benjamin Neale and Daniel MacArthur, whose scientific leadership has been essential for solving the right problems.\n- Jeremy Wertheimer, whose strategic advice and generous philanthropy have been essential for growing the impact of Hail.\n\nWe are grateful for generous support from:\n- The National Institute of Diabetes and Digestive and Kidney Diseases\n- The National Institute of Mental Health\n- The National Human Genome Research Institute\n- The Chan Zuckerberg Initiative\n\nWe would like to thank <a href=""https://zulipchat.com/"">Zulip</a> for supporting\nopen-source by providing free hosting, and YourKit, LLC for generously providing\nfree licenses for <a href=""https://www.yourkit.com/java/profiler/"">YourKit Java\nProfiler</a> for open-source development.\n\n<img src=""https://www.yourkit.com/images/yklogo.png"" align=""right"" />\n",955,bioinformatics,Python,18,R,Scala,Python,Java,Shell,HTML,CSS,Makefile,Batchfile,C++,Jupyter Notebook,XSLT,Dockerfile,C,JavaScript,SCSS,Emacs Lisp,HCL,,,,,,,,,,,12228,1778,10412,38,13,79,0,124613,238,2394,2197,197,bc665db6993bb46d76e90e1a4ef7a15f661fa22d,[gateway] Fix top level domain for website (#14617),2024-07-16T15:24:09Z,Daniel Goldstein,danielgold95@gmail.com,daniel-goldstein,0.2.132,Hail version 0.2.132\n\n[Change log](https://hail.is/docs/0.2/change_log.html#version-0-2-132),0.2.132,,,hail-ci-robot,MIT License,hail,hail-is,120,genetics,vcf,genomics,gwas,bioinformatics,python,software,hail,,,,,,,,,,,,,/hail-is/hail,132,57,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/h2oai/nitro,https://github.com/h2oai/nitro,0,,,0,0,0,0,0,0,1,1,0,0,0,"Create apps 10x quicker, without Javascript/HTML/CSS.","# Nitro\n\nCreate apps 10x quicker, without Javascript, HTML, or CSS.\n\n![Nitro](docs/assets/banner.png)\n\n**Currently a Work in Progress.**\nFollow [the development blog](https://wrong.technology/tags/nitro/) or\n[@CrunchingData](https://twitter.com/CrunchingData) for updates, or use\n[Discord](https://discord.gg/6RUdk2CPgw) or\n[Discussions](https://github.com/h2oai/nitro/discussions) for help / ideas / suggestions.\n\n## Get started\n\n- **[Install](https://nitro.h2o.ai/install/)**\n- **Learn**:\n  [Introduction](https://nitro.h2o.ai/intro/)\n  | [Guide](https://nitro.h2o.ai/guide/basics/)\n  | [Gallery](https://nitro.h2o.ai/gallery/)\n  | [Plugins](https://nitro.h2o.ai/plugins/)\n  | [Cheatsheet](https://nitro.h2o.ai/cheatsheet/)\n- **More**:\n  [Roadmap](https://github.com/h2oai/nitro/issues/4)\n  | [Milestones](https://github.com/h2oai/nitro/milestones)\n  | [Backlog](https://github.com/h2oai/nitro/issues/15)\n  | [Change Log](https://nitro.h2o.ai/change-log/)\n\n\n## Philosophy\n\nRecall how simple it is to author interactive command line applications using Python's built-in `input()` and `print()`:\n\n```py\ndef main():\n    name = input('What is your name?')\n    feel = input(f'How do you feel today, {name}?')\n    print(f'What a coincidence, {name}, I feel {feel}, too!')\n```\n\n```\n> What is your name?\n> Boaty McBoatface\n> How do you feel today, Boaty McBoatface?\n> intrigued\n> What a coincidence, Boaty McBoatface, I feel intrigued, too!\n```\n\nNitro brings that same level of simplicity to authoring web applications:\n\n```py\nfrom h2o_nitro import View, box\n\ndef main(view: View):\n    name = view(box('What is your name?', value='Boaty McBoatface'))\n    feel = view(box(f'How do you feel today, {name}?', value='intrigued'))\n    view(f'What a coincidence, {name}, I feel {feel}, too!')\n```\n\n![Hello World app](help/docs/assets/images/app-basic.gif)\n\nAnd here's a more elaborate example with seven pages in [seven Python statements](https://github.com/h2oai/nitro/blob/main/py/examples/space_flight.py):\n\n![Recruitment app](help/docs/assets/images/app-recruitment.gif)\n",200,graphics,TypeScript,8,HTML,CSS,TypeScript,JavaScript,Python,Makefile,Go,CoffeeScript,,,,,,,,,,,,,,,,,,,,,10,3,5,2,3,230,0,7623,14,113,67,46,d839e725e6afe0fee6e97f11eb0d514c919445bf,chore: Bump version,2024-07-09T22:38:20Z,lo5,prithvi.prabhu@gmail.com,lo5,v0.21.1,## Changelog\n\n,v0.21.1,Prithvi,,lo5,Apache License 2.0,nitro,h2oai,24,low-code,python,webapp,app,apps,data-science,graphics,h2o-nitro,ui,user-interface,web-application,ui-components,widget-library,widgets,data-analysis,developer-tools,devtools,,,,/h2oai/nitro,39,12,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/gwastro/pycbc,https://github.com/gwastro/pycbc,0.5,"Scientific, but package",1,1,1,1,1,0,0,0,0,0,0,1,"Core package to analyze gravitational-wave data, find signals, and study their parameters. This package was used in the first direct detection of gravitational waves (GW150914), and is used in the ongoing analysis of LIGO/Virgo data.  ","![GW150914](https://raw.githubusercontent.com/gwastro/pycbc-logo/master/pycbc_logo_name.png)\n\n[PyCBC](http://pycbc.org) is a software package used to explore astrophysical sources of gravitational waves.\nIt contains algorithms to analyze gravitational-wave data,\ndetect coalescing compact binaries, and make bayesian inferences from gravitational-wave data.\nPyCBC was used in the [first direct detection of gravitational waves](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.116.061102) and\nis used in flagship analyses of LIGO and Virgo data.\n\nPyCBC is collaboratively developed by the community and is lead by a team of GW astronomers with the\naim to build accessible tools for gravitational-wave data analysis.\n\nThe PyCBC home page is located on github at\n\n * https://pycbc.org/\n\nDocumentation is automatically built from the latest master version\n\n * https://pycbc.org/pycbc/latest/html/\n\nFor the detailed installation instructions of PyCBC\n\n * https://pycbc.org/pycbc/latest/html/install.html\n\nWant to get going using PyCBC?\n\n * [Try out our tutorials](https://github.com/gwastro/PyCBC-Tutorials). No software installation required and these can run entirely from the browser.\n\nQuick Installation\n```\npip install pycbc\n```\n\nTo test the code on your machine\n```\npip install pytest ""tox<4.0.0""\ntox\n```\n\nIf you use any code from PyCBC in a scientific publication, then please see our [citation guidelines](http://pycbc.org/pycbc/latest/html/credit.html) for more details on how to cite pycbc algorithms and\nprograms.\n\nFor the citation of the ``pycbc library``,  please use a bibtex entry and DOI for the\nappropriate release of the PyCBC software (or the latest available release).\nA bibtex key and DOI for each release is avaliable from [Zenodo](http://zenodo.org/).\n\n[![DOI](https://zenodo.org/badge/31596861.svg)](https://zenodo.org/badge/latestdoi/31596861) [![Build Status](https://travis-ci.org/gwastro/pycbc.svg?branch=master)](https://travis-ci.org/gwastro/pycbc)\n[![PyPI version](https://badge.fury.io/py/PyCBC.svg)](https://badge.fury.io/py/PyCBC) ![PyPI - Downloads](https://img.shields.io/pypi/dm/pycbc) [![Anaconda-Server Badge](https://anaconda.org/conda-forge/pycbc/badges/version.svg)](https://anaconda.org/conda-forge/pycbc) [![Anaconda-Server Badge](https://anaconda.org/conda-forge/pycbc/badges/downloads.svg)](https://anaconda.org/conda-forge/pycbc)\n[![astropy](http://img.shields.io/badge/powered%20by-AstroPy-orange.svg?style=flat)](http://www.astropy.org/)\n",307,physics,Python,8,Python,Shell,HTML,CSS,JavaScript,Dockerfile,C++,Cython,,,,,,,,,,,,,,,,,,,,,3980,386,3554,40,10,147,2777,69179,345,825,658,167,e1d273485105e229cf1ad51c3a9cc3b99b81c822,Update pycbc_pygrb_efficiency (#4812),2024-07-18T12:15:17Z,Marco Cusinato,marco.cusinato@uv.es,MarcoCusinato,Release of 2.4.2,"This is the v2.4.2 release of PyCBC.\r\n\r\nThis is a bugfix release which addresses scipy incompatibilities which prevented the 2.4.1 release.\r\n\r\nA [Docker](https://www.docker.com/community-edition) container for this release is available from the [pycbc/pycbc-el8](https://hub.docker.com/r/pycbc/pycbc-el8/) repository on Docker Hub and can be downloaded using the command:\r\n```sh\r\ndocker pull pycbc/pycbc-el8:v2.4.2\r\n```\r\nOn a machine with [CVMFS installed](https://twiki.grid.iu.edu/bin/view/Documentation/Release3/InstallCvmfs), a pre-built virtual environment is available for Red Hat 8 compatible operating systems by running the command:\r\n```sh\r\nsource /cvmfs/software.igwn.org//pycbc/x86_64_rhel_8/virtualenv/pycbc-v2.4.2/bin/activate\r\n```\r\n\r\nA singularity container is available at `/cvmfs/singularity.opensciencegrid.org/pycbc/pycbc-el8:v2.4.2` which can be started with the command:\r\n```sh\r\nsingularity shell --home  ${HOME}:/srv --pwd /srv --bind /cvmfs --contain --ipc --pid /cvmfs/singularity.opensciencegrid.org/pycbc/pycbc-el8:v2.4.2\r\n```\r\n\r\n\r\n## What's Changed\r\n* Support for Nessai sampler\r\n* Optimizations to sky marginalization\r\n\r\n\r\n**Full Changelog**: https://github.com/gwastro/pycbc/compare/v2.3.7..v2.4.2",v2.4.2,Alex Nitz,,ahnitz,GNU General Public License v3.0,pycbc,gwastro,155,astronomy,physics,gravity,ligo,gravitational-waves,pycbc,analysis,python,black-hole,neutron-star,gwastro,virgo,signal-processing,open-science,cosmic-explorer,einstein-telescope,lisa,,,,/gwastro/pycbc,168,41,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/gpac/gpac,https://github.com/gpac/gpac,0,,,0,0,0,0,0,0,1,1,0,0,0,"GPAC Ultramedia OSS for Video Streaming & Next-Gen Multimedia Transcoding, Packaging & Delivery","[![Build Status](https://tests.gpac.io/testres/badge/build/ubuntu64)](https://buildbot.gpac.io/#/grid?branch=master)\n[![Tests](https://tests.gpac.io/testres/badge/tests/linux64)](https://tests.gpac.io/)\n\n[![Build Status](https://tests.gpac.io/testres/badge/build/debian32)](https://buildbot.gpac.io/#/grid?branch=master)\n[![Tests](https://tests.gpac.io/testres/badge/tests/linux32)](https://tests.gpac.io/)\n\n[![Build Status](https://tests.gpac.io/testres/badge/build/windows64)](https://buildbot.gpac.io/#/grid?branch=master)\n[![Tests](https://tests.gpac.io/testres/badge/tests/win64)](https://tests.gpac.io/)\n\n[![Build Status](https://tests.gpac.io/testres/badge/build/windows32)](https://buildbot.gpac.io/#/grid?branch=master)\n[![Tests](https://tests.gpac.io/testres/badge/tests/win32)](https://tests.gpac.io/)\n\n[![Build Status](https://tests.gpac.io/testres/badge/build/macos)](https://buildbot.gpac.io/#/grid?branch=master)\n[![Tests](https://tests.gpac.io/testres/badge/tests/macos)](https://tests.gpac.io/)\n\n[![Build Status](https://tests.gpac.io/testres/badge/build/ios)](https://buildbot.gpac.io/#/grid?branch=master)\n[![Build Status](https://tests.gpac.io/testres/badge/build/android)](https://buildbot.gpac.io/#/grid?branch=master)\n\n[![Coverage](https://tests.gpac.io/testres/badge/cov/linux64?branch=master)](https://tests.gpac.io/testres/)\n[![Coverage](https://tests.gpac.io/testres/badge/covfn/linux64?branch=master)](https://tests.gpac.io/testres/)\n\n![License](https://img.shields.io/badge/license-LGPL-blue.svg)\n[![OpenHub](https://www.openhub.net/p/gpac/widgets/project_thin_badge.gif)](https://www.openhub.net/p/gpac)\n\n\n# GPAC Introduction\n\nCurrent version: 2.5-DEV\n\nLatest Release: 2.4\n\nGPAC is an open-source multimedia framework focused on modularity and standards compliance.\nGPAC provides tools to process, inspect, package, stream, playback and interact with media content. Such content can be any combination of audio, video, subtitles, metadata, scalable graphics, encrypted media, 2D/3D graphics and ECMAScript.\nGPAC is best-known for its wide MP4/ISOBMFF capabilities and is popular among video enthusiasts, academic researchers, standardization bodies, and professional broadcasters.\n\nFor more information, visit [https://gpac.io](https://gpac.io)\n\nGPAC is distributed under the LGPL v2.1 or later, and is also available, for most of it, under a [commercial license](https://www.motionspell.com/gpac-licensing).\n\nPlease ! _cite_ ! our work in your research:\n- ""GPAC Filters"" (https://doi.org/10.1145/3339825.3394929) for recent versions (0.9 or above) \n- ""GPAC: open source multimedia framework"" (https://doi.org/10.1145/1291233.1291452) for older versions.\n\n\n# Features\n\nGPAC can process, analyse, package, stream, encode, decode and playback a wide variety of contents. Selected feature list:\n- Audio: MPEG audio (mp1/2/3, aac), AC3, E-AC3, Opus, FLAC, …\n- Video: MPEG 1 / 2 / 4 (H264/AVC) / H (HEVC), VVC, AV1, VP9, Theora, ...\n- Subtitles: WebVTT, TTML (full, EBU-TTD, …), 3GPP/Apple Timed Text, …\n- Encryption: CENC, PIFF, ISMA, OMA, ...\n- Containers: MP4/fMP4/CMAF/Quicktime MOV/ProRes MOV, AVI, MPG, OGG, MKV, ...\n- Streaming: MPEG-2 Transport Stream, RTP, RTSP, HTTP, Apple HLS, MPEG-DASH, ATSC 3.0 ROUTE, ...\n- Supported IOs: local files, pipes, UDP/TCP, HTTP(S), custom IO\n- Presentation formats: MPEG-4 BIFS, SVG Tiny 1.2, VRML/X3D\n- JS scripting through QuickJS for both SVG/BIFS/VRML and extending GPAC framework tools\n- 3D support (360 videos, WebGL JS filters…)\n- Inputs: microphone, camera, desktop grabbing\n- Highly configurable media processing pipeline\n- Python and NodeJS bindings\n\nFeatures are encapsulated in processing modules called filters:\n- to get the full list of available features, you can run the command line `gpac -h filters` or check [filters' wiki](https://wiki.gpac.io/Filters/Filters).\n- to get the full list of playback features, check [the dedicated wiki page](https://wiki.gpac.io/Player/Player).\n\n\n# Tools\n\n## MP4Box\nMP4Box is a multi-purpose MP4 file manipulation for the prompt, featuring media importing and extracting, file inspection, DASH segmentation, RTP hinting, ... See `MP4Box -h`, `man MP4Box` or [our wiki](https://wiki.gpac.io/MP4Box/MP4Box).\n\n\n## gpac \nGPAC includes a filter engine in charge of stream management and used by most applications in GPAC - [read this post](https://wiki.gpac.io/Filters/Rearchitecture) for more discussion on how this impacts MP4Box.\nThe gpac application is a direct interface to the filter engine of GPAC, allowing any combination of filters not enabled by other applications. See `gpac -h`, `man gpac`, `man gpac-filters` or [our wiki](https://wiki.gpac.io/Filters/Filters) for more details.\n\n# Getting started\n## Download\nStable and nightly builds installers for Windows, Linux, OSX, Android, iOS are available on [gpac.io](https://gpac.io/downloads/).\n\nIf you want to compile GPAC yourself, please follow the instructions in the [build section](https://wiki.gpac.io/Build/Build-Introduction) of our wiki.\n\n## Documentation\nThe general GPAC framework documentation is available on [wiki.gpac.io](https://wiki.gpac.io), including [HowTos](https://wiki.gpac.io/Howtos/howtos/).\n\nGPAC tools are mostly wrappers around an underlying library called libgpac which can easily be embedded in your projects. The libgpac developer documentation is available at [doxygen.gpac.io](https://doxygen.gpac.io), including documentation of [JS APIs](https://doxygen.gpac.io/group__jsapi__grp.html), [Python APIs](https://doxygen.gpac.io/group__pyapi__grp.html) and [NodeJS APIs](https://doxygen.gpac.io/group__nodejs__grp.html).\n\n\n## Testing\nGPAC has a test suite exercising most features of the framework. The test suite is in a separate repository [https://github.com/gpac/testsuite/](https://github.com/gpac/testsuite/), but is available as a submodule of the GPAC main repository. To initialize the testsuite submodule, do `git submodule update --init`.\n\nFor more details on the test suite, read [this page](https://wiki.gpac.io/Build/tests/GPAC_tests/) and check the [testsuite readme](https://github.com/gpac/testsuite).\n\nPer-commit [build](https://buildbot.gpac.io/) and [tests results](https://tests.gpac.io) are available.\n\n\n## Support, ongoing tasks and bugs\n\nPlease use [github](https://github.com/gpac/gpac/issues) for feature requests and bug reports. When filing a request there, please tag it as _feature-request_.	\n\n## Contributing\nA complex project like GPAC wouldn’t exist and persist without the support of its community. Please contribute: a nice message, supporting us in our communication, reporting issues when you see them… any gesture, even the smallest ones, counts. \n\nIf you want to contribute to GPAC, you can find ideas at [GSoC page](https://gpac.io/jobs/google-summer-of-code-ideas/) or look for a [good first issue](https://github.com/gpac/gpac/labels/good%20first%20issue). In any doubt please feel free to [contact us](mailto:contact@gpac.io).\n\n# Team\nGPAC is brought to you by an experienced team of developers with a wide track-record on media processing. \n\nThe project is mainly developed in the MultiMedia group of [Telecom Paris](https://www.telecom-paris.fr/) with the help of many [great contributors](https://github.com/gpac/gpac/graphs/contributors).\n\nGPAC has a peculiar story: started as a startup in NYC, GPAC gained traction from research and a nascent multimedia community as it was open-sourced in 2003. Since then we have never stopped transforming GPAC into a useful and up-to-date project, with many industrial R&D collaborations and a community of tens of thousands of users. This makes GPAC one of the few open-source multimedia projects that gathers so much diversity.\n\n\n# Roadmap\nUsers are encouraged to use the latest tag or the master branch.\n\n## V2.X\nTargets:\n- [ ] DASH event support\n- [ ] Web GUI\n- [ ] QUIC support\n- [ ] ROUTE file repair support\n- [ ] FLUTE support\n- [ ] Rust Bindings\n",2643,graphics,C,17,Shell,C++,C,Makefile,Java,Objective-C,JavaScript,HTML,CSS,NSIS,Batchfile,GLSL,Roff,Python,Nix,Rez,Dockerfile,,,,,,,,,,,,508,111,392,5,39,77,0,151637,521,2406,2365,41,e2250d68fd67cfa770b270b0ac882a39e4e93f32,fix previous commit,2024-07-18T00:44:13Z,Romain Bouqueau,romain.bouqueau.pro@gmail.com,rbouqueau,GPAC 2.4,"We are happy to announce the release of GPAC 2.4\r\n\r\nThis release marks the beginning of GPAC in your browser with emscripten support, with a live demo at https://wasm.gpac.io !\r\n\r\nThis release also brings many new features including pcap support, async net IOs, JIT packaging for on-demand content and better subtitle/CC support.\r\n\r\nAs usual, installers are available on [gpac.io](http://gpac.io/downloads) for most common platforms.\r\n\r\nEnjoy, give us feedback and spread the news!\r\n\r\nDetailed changes:\r\n\r\n## Emscripten|WebAssembly(WASM) support\r\n- Session can run in worker or in main browser loop\r\n- Automatic disable of ffdec and ffenc threads when not running in worker\r\n- Use fetch() api for downloader\r\n- Support for WebCodecs\r\n- Support for getUserMedia and canvas readable streams\r\n- gpac.html demo page\r\n\r\n## Media Formats\r\n- Fixes in text subtitle converters\r\n- Improved DolbyVision muxing from mkv\r\n- Dasher support for inputs with multiple stsd entries\r\n- Allow multiplexed representations for LL-HLS\r\n- Improved eac3 support\r\n- QT cmov support (reading and writing)\r\n- QT lpcm support\r\n- Improved chaptering support\r\n- Improved timecode inspection\r\n- Support for big-endian PCM formats\r\n- Support for uncv (raw video in mp4 and heif)\r\n- Support for forced subtitles\r\n- Support for HEVC bitstreams merging in dash (for multi-res tile adaptation)\r\n \r\n## Filters\r\n- Async HTTP request in all filters\r\n- JS/Python/NodeJS bindings for httpout server\r\n- Fixed CORS and mime types in httpout for wasm and sharedArrayuffer support\r\n- Allow compilation without threads\r\n- Allow compilation without network\r\n- Fixed GF_FileIO for async read and write modes\r\n- Added seeking in mp4dmx in mem mode\r\n- vout updated to use VBO (required for gles2)\r\n- libcaca video output support\r\n- Allow ffdmx to work as demux filter on gpac input file pids\r\n- GHI format for pre-indexing DASH/HLS session for JIT packaging/encryption/transcode\r\n- Zero-copy for mp4dmx and fragmented mp4mx\r\n- Various speed optimizations (filter session, isomedia lib, disk io, xml)\r\n- Allow running the session without mutexes\r\n- Closed Caption decoding filter (CEA708)\r\n- MPEG-H decoder using IIS mpeghdec\r\n- Templating for property assignment \r\n- Conditional filter replacement to identify based on codecID \r\n- Per-filter logging\r\n\r\n## MP4Box\r\n- Add track reordering option\r\n\r\n## Misc\r\n- Migrated doc from github's wiki to wiki.gpac.io\r\n- FFmpeg 7 support\r\n- Added features for configure (vout, aout, fonts, doc, evg)\r\n- Allow specifying network interface by name or IP (instead of IP only)\r\n- UDP/TCP filtering and recording to / playback from pcap, pcapng and GPAC gpc files\r\n- Added Nix and Docker build files\r\n- Many bug fixes, improvements and security patches",v2.4.0,Jean Le Feuvre,,jeanlf,GNU Lesser General Public License v2.1,gpac,gpac,13,streaming,graphics,gpac,mp4box,mpeg-dash,hls,mp4,cenc,mpeg-ts,broadcast,vr,mov,prores,atsc3,tiling,,,,,,/gpac/gpac,15,105,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/google-deepmind/mujoco,https://github.com/google-deepmind/mujoco,0.5,accoring to description does not fit under scientific projects: general purpose physics engine that aims to facilitate research and development in robotics,0,0,0,1,1,0,0,0,0,0,0,0,Multi-Joint dynamics with Contact. A general purpose physics simulator.,"<h1>\n  <a href=""#""><img alt=""MuJoCo"" src=""banner.png"" width=""100%""/></a>\n</h1>\n\n<p>\n  <a href=""https://github.com/google-deepmind/mujoco/actions/workflows/build.yml?query=branch%3Amain"" alt=""GitHub Actions"">\n    <img src=""https://img.shields.io/github/actions/workflow/status/google-deepmind/mujoco/build.yml?branch=main"">\n  </a>\n  <a href=""https://mujoco.readthedocs.io/"" alt=""Documentation"">\n    <img src=""https://readthedocs.org/projects/mujoco/badge/?version=latest"">\n  </a>\n  <a href=""https://github.com/google-deepmind/mujoco/blob/main/LICENSE"" alt=""License"">\n    <img src=""https://img.shields.io/github/license/google-deepmind/mujoco"">\n  </a>\n</p>\n\n**MuJoCo** stands for **Mu**lti-**Jo**int dynamics with **Co**ntact. It is a\ngeneral purpose physics engine that aims to facilitate research and development\nin robotics, biomechanics, graphics and animation, machine learning, and other\nareas which demand fast and accurate simulation of articulated structures\ninteracting with their environment.\n\nThis repository is maintained by [Google DeepMind](https://www.deepmind.com/).\n\nMuJoCo has a C API and is intended for researchers and developers. The runtime\nsimulation module is tuned to maximize performance and operates on low-level\ndata structures that are preallocated by the built-in XML compiler. The library\nincludes interactive visualization with a native GUI, rendered in OpenGL. MuJoCo\nfurther exposes a large number of utility functions for computing\nphysics-related quantities.\n\nWe also provide [Python bindings] and a plug-in for the [Unity] game engine.\n\n## Documentation\n\nMuJoCo's documentation can be found at [mujoco.readthedocs.io]. Upcoming features due for the next\nrelease can be found in the [changelog] in the latest branch.\n\n## Getting Started\n\nThere are two easy ways to get started with MuJoCo:\n\n1. **Run `simulate` on your machine.**\n[This video](https://www.youtube.com/watch?v=P83tKA1iz2Y) shows a screen capture\nof `simulate`, MuJoCo's native interactive viewer. Follow the steps described in\nthe [Getting Started] section of the documentation to get `simulate` running on\nyour machine.\n\n2. **Explore our online IPython notebooks.**\nIf you are a Python user, you might want to start with our tutorial notebooks\nrunning on Google Colab:\n\n - The **introductory** tutorial teaches MuJoCo basics:\n   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-deepmind/mujoco/blob/main/python/tutorial.ipynb)\n - The **LQR** tutorial synthesizes a linear-quadratic controller, balancing a humanoid on one leg:\n   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-deepmind/mujoco/blob/main/python/LQR.ipynb)\n - The **least-squares** tutorial explains how to use the Python-based nonlinear least-squares solver:\n   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-deepmind/mujoco/blob/main/python/least_squares.ipynb)\n - The **MJX** tutorial provides usage examples of\n   [MuJoCo XLA](https://mujoco.readthedocs.io/en/stable/mjx.html), a branch of MuJoCo written in JAX:\n   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-deepmind/mujoco/blob/main/mjx/tutorial.ipynb)\n - The **differentiable physics** tutorial trains locomotion policies with analytical gradients automatically derived from MuJoCo's physics step:\n   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-deepmind/mujoco/blob/main/mjx/training_apg.ipynb)\n\n## Installation\n\n### Prebuilt binaries\n\nVersioned releases are available as precompiled binaries from the GitHub\n[releases page], built for Linux (x86-64 and AArch64), Windows (x86-64 only),\nand macOS (universal). This is the recommended way to use the software.\n\n### Building from source\n\nUsers who wish to build MuJoCo from source should consult the [build from\nsource] section of the documentation. However, please note that the commit at\nthe tip of the `main` branch may be unstable.\n\n### Python (>= 3.8)\n\nThe native Python bindings, which come pre-packaged with a copy of MuJoCo, can\nbe installed from [PyPI] via:\n\n```bash\npip install mujoco\n```\n\nNote that Pre-built Linux wheels target `manylinux2014`, see\n[here](https://github.com/pypa/manylinux) for compatible distributions. For more\ninformation such as building the bindings from source, see the [Python bindings]\nsection of the documentation.\n\n## Contributing\n\nWe welcome community engagement: questions, requests for help, bug reports and\nfeature requests. To read more about bug reports, feature requests and more\nambitious contributions, please see our [contributors guide](CONTRIBUTING.md)\nand [style guide](STYLEGUIDE.md).\n\n## Asking Questions\n\nQuestions and requests for help are welcome on the GitHub\n[Issues](https://github.com/google-deepmind/mujoco/issues) page and should focus\non a specific problem or question.\n\n[Discussions](https://github.com/google-deepmind/mujoco/discussions) should\naddress wider concerns that might require input from multiple participants.\n\nHere are some guidelines for asking good questions:\n\n1. Search for existing questions or issues that touch on the same subject.\n\n   You can add comments to existing threads or start new ones. If you start a\n   new thread and there are existing relevant threads, please link to them.\n\n2. Use a clear and specific title. Try to include keywords that will make your\n   question easy for other to find in the future.\n\n3. Introduce yourself and your project more generally.\n\n   If your level of expertise is exceptional (either high or low), and it might\n   be relevant to what we can assume you know, please state that as well.\n\n4. Take a step back and tell us what you're trying to accomplish, if we\n   understand you goal we might suggest a different type of solution than the\n   one you are having problems with\n\n5. Make it easy for others to reproduce the problem or understand your question.\n\n   If this requires a model, please include it. Try to make the model minimal:\n   remove elements that are unrelated to your question. Pure XML models should\n   be inlined. Models requiring binary assets (meshes, textures), should be\n   attached as a `.zip` file. Please make sure the included model is loadable\n   before you attach it.\n\n6. Include an illustrative screenshot or video, if relevant.\n\n7. Tell us how you are accessing MuJoCo (C API, Python bindings, etc.) and which\n   MuJoCo version and operating system you are using.\n\n## Related software\nMuJoCo forms the backbone of many environment packages, but these are too many\nto list here individually. Below we focus on bindings and converters.\n\n### Bindings\n\nThese packages give users of various languages access to MuJoCo functionality:\n\n#### First-party bindings:\n\n- [Python bindings](https://mujoco.readthedocs.io/en/stable/python.html)\n  - [dm_control](https://github.com/google-deepmind/dm_control), Google\n    DeepMind's related environment stack, includes\n    [PyMJCF](https://github.com/google-deepmind/dm_control/blob/main/dm_control/mjcf/README.md),\n    a module for procedural manipulation of MuJoCo models.\n- [C# bindings and Unity plug-in](https://mujoco.readthedocs.io/en/stable/unity.html)\n\n#### Third-party bindings:\n\n- **WebAssembly**: [mujoco_wasm](https://github.com/zalo/mujoco_wasm) by [@zalo](https://github.com/zalo) with contributions by\n  [@kevinzakka](https://github.com/kevinzakka), based on the [emscripten build](https://github.com/stillonearth/MuJoCo-WASM) by\n  [@stillonearth](https://github.com/stillonearth).\n\n  :arrow_right: [Click here](https://zalo.github.io/mujoco_wasm/) for a live demo of MuJoCo running in your browser.\n- **MATLAB Simulink**: [Simulink Blockset for MuJoCo Simulator](https://github.com/mathworks-robotics/mujoco-simulink-blockset)\n  by [Manoj Velmurugan](https://github.com/vmanoj1996).\n- **Swift**: [swift-mujoco](https://github.com/liuliu/swift-mujoco)\n- **Java**: [mujoco-java](https://github.com/CommonWealthRobotics/mujoco-java)\n- **Julia**: [MuJoCo.jl](https://github.com/JamieMair/MuJoCo.jl)\n\n\n### Converters\n\n- **OpenSim**: [MyoConverter](https://github.com/MyoHub/myoconverter) converts\n  OpenSim models to MJCF.\n- **SDFormat**: [gz-mujoco](https://github.com/gazebosim/gz-mujoco/) is a\n  two-way SDFormat <-> MJCF conversion tool.\n- **OBJ**: [obj2mjcf](https://github.com/kevinzakka/obj2mjcf)\n  a script for converting composite OBJ files into a loadable MJCF model.\n\n## Citation\n\nIf you use MuJoCo for published research, please cite:\n\n```\n@inproceedings{todorov2012mujoco,\n  title={MuJoCo: A physics engine for model-based control},\n  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},\n  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},\n  pages={5026--5033},\n  year={2012},\n  organization={IEEE},\n  doi={10.1109/IROS.2012.6386109}\n}\n```\n\n## License and Disclaimer\n\nCopyright 2021 DeepMind Technologies Limited.\n\nBox collision code ([`engine_collision_box.c`](https://github.com/google-deepmind/mujoco/blob/main/src/engine/engine_collision_box.c))\nis Copyright 2016 Svetoslav Kolev.\n\nReStructuredText documents, images, and videos in the `doc` directory are made\navailable under the terms of the Creative Commons Attribution 4.0 (CC BY 4.0)\nlicense. You may obtain a copy of the License at\nhttps://creativecommons.org/licenses/by/4.0/legalcode.\n\nSource code is licensed under the Apache License, Version 2.0. You may obtain a\ncopy of the License at https://www.apache.org/licenses/LICENSE-2.0.\n\nThis is not an officially supported Google product.\n\n[build from source]: https://mujoco.readthedocs.io/en/latest/programming#building-mujoco-from-source\n[Getting Started]: https://mujoco.readthedocs.io/en/latest/programming#getting-started\n[Unity]: https://unity.com/\n[releases page]: https://github.com/google-deepmind/mujoco/releases\n[GitHub Issues]: https://github.com/google-deepmind/mujoco/issues\n[mujoco.readthedocs.io]: https://mujoco.readthedocs.io\n[changelog]: https://mujoco.readthedocs.io/en/latest/changelog.html\n[Python bindings]: https://mujoco.readthedocs.io/en/stable/python.html#python-bindings\n[PyPI]: https://pypi.org/project/mujoco/\n",7573,physics,Jupyter Notebook,9,C,C#,Python,Shell,CMake,C++,Objective-C++,Jupyter Notebook,Objective-C,,,,,,,,,,,,,,,,,,,,196,72,96,28,3,64,0,104272,749,1360,1179,181,d49d4bf64cd83b9d0c8e5cddde4b0d3b614f25de,Make unsupported feature checking optional in put_model.,2024-07-19T22:23:32Z,Saran Tunyasuvunakool,stunya@google.com,saran-t,03.02.2000,"See the [changelog](https://mujoco.readthedocs.io/en/latest/changelog.html#version-3-2-0-jul-15-2024).\r\n\r\nIntroduced a major new feature: **procedural model creation and editing**, using a new top-level data-structure [mjSpec](https://mujoco.readthedocs.io/en/latest/APIreference/APItypes.html#mjspec). See the [Model Editing](https://mujoco.readthedocs.io/en/latest/programming/modeledit.html) chapter for details. Note that as of this release this feature is still in testing and subject to future breaking changes. Fixes #364.",03.02.2000,Erik Frey,,erikfrey,Apache License 2.0,mujoco,google-deepmind,27,robotics,physics,mujoco,,,,,,,,,,,,,,,,,,/google-deepmind/mujoco,29,104,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/google/s2geometry,https://github.com/google/s2geometry,0,,,0,0,0,0,0,0,1,1,0,0,0,Computational geometry and spatial indexing on the sphere,"# S2 Geometry Library\n\n## Overview\n\nThis is a package for manipulating geometric shapes. Unlike many geometry\nlibraries, S2 is primarily designed to work with _spherical geometry_, i.e.,\nshapes drawn on a sphere rather than on a planar 2D map. This makes it\nespecially suitable for working with geographic data.\n\nIf you want to learn more about the library, start by reading the\n[overview](http://s2geometry.io/about/overview) and [quick start\ndocument](http://s2geometry.io/devguide/cpp/quickstart), then read the\nintroduction to the [basic types](http://s2geometry.io/devguide/basic_types).\n\nS2 documentation can be found on [s2geometry.io](http://s2geometry.io).\n\n## API/ABI Stability\n\nNote that all [releases](https://github.com/google/s2geometry/releases) are\nversion 0.x, so there are\n[no API or ABI stability guarantees](https://semver.org/#spec-item-4).\nStarting with 1.0 we will adhere to [SemVer](https://semver.org/).\n\nThe Python API is particularly unstable, and it is planned that the SWIGged\nAPI will be replaced by a pybind11 version with more Pythonic names and more\ncomplete functionality.\n\n## Requirements for End Users\n\n* [CMake](http://www.cmake.org/)\n* A C++ compiler with C++14 support, such as [g++ >= 5](https://gcc.gnu.org/)\n* [Abseil](https://github.com/abseil/abseil-cpp) >= LTS\n  [`20240116`](https://github.com/abseil/abseil-cpp/releases/tag/20240116.1)\n  (standard library extensions)\n* [OpenSSL](https://github.com/openssl/openssl) (for its bignum library)\n* [googletest testing framework >= 1.10](https://github.com/google/googletest)\n  (to build tests and example programs, optional)\n\nOn Ubuntu, all of these other than abseil can be installed via apt-get:\n\n```\nsudo apt-get install cmake googletest libssl-dev\n```\n\nOtherwise, you may need to install some from source.\n\nCurrently, Abseil must always be installed from source.  See the use of\n`-DCMAKE_PREFIX_PATH` in the [build instructions below](#building).\nThis is likely to change.\n\nOn macOS, use [MacPorts](http://www.macports.org/) or\n[Homebrew](http://brew.sh/).  For MacPorts:\n\n```\nsudo port install cmake openssl\n```\n\nDo not install `gtest` from MacPorts; instead download [release\n1.10.0](https://github.com/google/googletest/releases/tag/release-1.10.0), unpack,\nand substitute\n\n```\ncmake -DGOOGLETEST_ROOT=/...absolute path to.../googletest-release-1.10.0 ..\n```\n\nin the build instructions below.\n\nThorough testing has only been done on Ubuntu 14.04.3 and macOS 10.12.\n\n## Build and Install\n\nYou may either download the source as a ZIP archive, or [clone the git\nrepository](https://help.github.com/articles/cloning-a-repository/).\n\n### Via ZIP archive\n\nDownload [ZIP file](https://github.com/google/s2geometry/archive/master.zip)\n\n```\ncd [parent of directory where you want to put S2]\nunzip [path to ZIP file]/s2geometry-master.zip\ncd s2geometry-master\n```\n\n### Via `git clone`\n\n```\ncd [parent of directory where you want to put S2]\ngit clone https://github.com/google/s2geometry.git\ncd s2geometry\n```\n\n### Building\n\nFirst, [install Abseil](https://github.com/abseil/abseil-cpp/blob/master/CMake/README.md#traditional-cmake-set-up).\nIt must be configured with `-DCMAKE_POSITION_INDEPENDENT_CODE=ON`.\ns2geometry must be configured to use the same C++ version that\nabseil uses.  The easiest way to achieve this is to pass\n`-DCMAKE_CXX_STANDARD=14` (or `-DCMAKE_CXX_STANDARD=17`) to `cmake`\nwhen compiling both abseil and s2geometry.\n\nFrom the appropriate directory depending on how you got the source:\n\n```\nmkdir build\ncd build\n# You can omit -DGOOGLETEST_ROOT to skip tests; see above for macOS.\n# Use the same CMAKE_CXX_STANDARD value that was used with absl.\ncmake -DGOOGLETEST_ROOT=/usr/src/googletest -DCMAKE_PREFIX_PATH=/path/to/absl/install -DCMAKE_CXX_STANDARD=14 ..\nmake -j $(nproc)\nmake test ARGS=""-j$(nproc)""  # If GOOGLETEST_ROOT specified above.\nsudo make install\n```\n\nOn macOS, `sysctl -n hw.logicalcpu` is the equivalent of `nproc`.\n\nDisable building of shared libraries with `-DBUILD_SHARED_LIBS=OFF`.\n\nEnable the python interface with `-DWITH_PYTHON=ON`.\n\nIf OpenSSL is installed in a non-standard location set `OPENSSL_ROOT_DIR`\nbefore running configure, for example on macOS:\n```\nOPENSSL_ROOT_DIR=/opt/homebrew/Cellar/openssl@3/3.1.0 cmake -DCMAKE_PREFIX_PATH=/opt/homebrew -DCMAKE_CXX_STANDARD=17\n```\n\n## Installing\n\nFrom `build` subdirectory:\n\n```\nmake install\n```\n\nPrefix it with `sudo` if needed:\n\n```\nsudo make install\n```\n\n_NOTE_: There is not `uninstall` target but `install_manifest.txt` may be helpfull.\n\nAll files will be installed at location specified in `CMAKE_INSTALL_PREFIX` variable.\n\nSeveral suffix variables used for some file groups:\n\nVariable | Default | Description\n-------- | ------- | -----------\n`CMAKE_INSTALL_INCLUDEDIR` | `include` | For header files\n`CMAKE_INSTALL_BINDIR`     | `bin`     | For executables and `*.dll` files on `DLL`-based platforms\n`CMAKE_INSTALL_LIBDIR`     | `lib`     | For library files (`*.so`, `*.a`, `*.lib` etc)\n\nIf needed set this variables on command line as `cmake` arguments with `-D` prefix or edit from `build` subdirectory:\n\n```\nmake edit_cache\n```\n\nFor more info read: [The CMake Cache](https://cmake.org/cmake/help/latest/guide/user-interaction/index.html#the-cmake-cache).\n\n## Python\n\nIf you want the Python interface, you need to run cmake using\n`-DWITH_PYTHON=ON`. You will also need to install the following dependencies:\n\n* [SWIG 4](https://github.com/swig/swig) (for Python support, optional)\n* python3-dev (for Python support, optional)\n\nwhich can be installed via\n\n```\nsudo apt-get install swig python3-dev\n```\n\nor on macOS:\n\n```\nsudo port install swig\n```\nVersion 4.0 is required, but it should be easy to make it work 3.0 or probably\neven 2.0.\n\nPython 3 is required.\n\n### Creating wheels\nFirst, make a virtual environment and install `cmake_build_extension` and `wheel`\ninto it:\n```\npython3 -m venv venv\nsource venv/bin/activate\npip install cmake_build_extension wheel\n```\n\nThen build the wheel:\n```\npython setup.py bdist_wheel\n```\n\nThe resulting wheel will be in the `dist` directory.\n\n> If OpenSSL is in a non-standard location make sure to set `OPENSSL_ROOT_DIR` \n> when calling `setup.py`, see above for more information.\n\n## Other S2 implementations\n\n* [Go](https://github.com/golang/geo) (Approximately 40% complete.)\n* [Java](https://github.com/google/s2-geometry-library-java)\n* [Kotlin](https://github.com/Enovea/s2-geometry-kotlin) (Complete except binary serialization)\n\n## Disclaimer\n\nThis is not an official Google product.\n",2236,geometry,C++,5,CMake,Python,C,C++,SWIG,,,,,,,,,,,,,,,,,,,,,,,,144,30,99,15,2,32,0,11589,295,198,155,43,ec260c3ad4c2bf812b54a4bc984b76747ace9d5d,Fix build with libcxx16 (#373),2024-07-05T13:24:27Z,Robert Schulze,robert@clickhouse.com,rschu1ze,v0.11.1,## What's Changed\r\n* Fix vs2019 & namespace for install targets by @jherico in https://github.com/google/s2geometry/pull/349\r\n\r\n\r\n**Full Changelog**: https://github.com/google/s2geometry/compare/v0.11.0...v0.11.1,v0.11.1,Jesse Rosenstock,,jmr,Apache License 2.0,s2geometry,google,8,geometry,spherical-geometry,s2,spatial-indexing,,,,,,,,,,,,,,,,,/google/s2geometry,8,71,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/google/filament,https://github.com/google/filament,0,,,0,0,0,0,0,0,1,1,0,0,0,"Filament is a real-time physically based rendering engine for Android, iOS, Windows, Linux, macOS, and WebGL2","# Filament\n\n[![Android Build Status](https://github.com/google/filament/workflows/Android/badge.svg)](https://github.com/google/filament/actions?query=workflow%3AAndroid)\n[![iOS Build Status](https://github.com/google/filament/workflows/iOS/badge.svg)](https://github.com/google/filament/actions?query=workflow%3AiOS)\n[![Linux Build Status](https://github.com/google/filament/workflows/Linux/badge.svg)](https://github.com/google/filament/actions?query=workflow%3ALinux)\n[![macOS Build Status](https://github.com/google/filament/workflows/macOS/badge.svg)](https://github.com/google/filament/actions?query=workflow%3AmacOS)\n[![Windows Build Status](https://github.com/google/filament/workflows/Windows/badge.svg)](https://github.com/google/filament/actions?query=workflow%3AWindows)\n[![Web Build Status](https://github.com/google/filament/workflows/Web/badge.svg)](https://github.com/google/filament/actions?query=workflow%3AWeb)\n\nFilament is a real-time physically based rendering engine for Android, iOS, Linux, macOS, Windows,\nand WebGL. It is designed to be as small as possible and as efficient as possible on Android.\n\n## Download\n\n[Download Filament releases](https://github.com/google/filament/releases) to access stable builds.\nFilament release archives contains host-side tools that are required to generate assets.\n\nMake sure you always use tools from the same release as the runtime library. This is particularly\nimportant for `matc` (material compiler).\n\nIf you'd rather build Filament yourself, please refer to our [build manual](BUILDING.md).\n\n### Android\n\nAndroid projects can simply declare Filament libraries as Maven dependencies:\n\n```gradle\nrepositories {\n    // ...\n    mavenCentral()\n}\n\ndependencies {\n    implementation 'com.google.android.filament:filament-android:1.53.1'\n}\n```\n\nHere are all the libraries available in the group `com.google.android.filament`:\n\n| Artifact      | Description   |\n| ------------- | ------------- |\n| [![filament-android](https://maven-badges.herokuapp.com/maven-central/com.google.android.filament/filament-android/badge.svg?subject=filament-android)](https://maven-badges.herokuapp.com/maven-central/com.google.android.filament/filament-android)  | The Filament rendering engine itself. |\n| [![filament-android-debug](https://maven-badges.herokuapp.com/maven-central/com.google.android.filament/filament-android-debug/badge.svg?subject=filament-android-debug)](https://maven-badges.herokuapp.com/maven-central/com.google.android.filament/filament-android-debug)  | Debug version of `filament-android`. |\n| [![gltfio-android](https://maven-badges.herokuapp.com/maven-central/com.google.android.filament/gltfio-android/badge.svg?subject=gltfio-android)](https://maven-badges.herokuapp.com/maven-central/com.google.android.filament/gltfio-android) | A glTF 2.0 loader for Filament, depends on `filament-android`. |\n| [![filament-utils-android](https://maven-badges.herokuapp.com/maven-central/com.google.android.filament/filament-utils-android/badge.svg?subject=filament-utils-android)](https://maven-badges.herokuapp.com/maven-central/com.google.android.filament/filament-utils-android) | KTX loading, Kotlin math, and camera utilities, depends on `gltfio-android`. |\n| [![filamat-android](https://maven-badges.herokuapp.com/maven-central/com.google.android.filament/filamat-android/badge.svg?subject=filamat-android)](https://maven-badges.herokuapp.com/maven-central/com.google.android.filament/filamat-android) | A runtime material builder/compiler. This library is large but contains a full shader compiler/validator/optimizer and supports both OpenGL and Vulkan. |\n| [![filamat-android-lite](https://maven-badges.herokuapp.com/maven-central/com.google.android.filament/filamat-android-lite/badge.svg?subject=filamat-android-lite)](https://maven-badges.herokuapp.com/maven-central/com.google.android.filament/filamat-android-lite) | A much smaller alternative to `filamat-android` that can only generate OpenGL shaders. It does not provide validation or optimizations. |\n\n### iOS\n\niOS projects can use CocoaPods to install the latest release:\n\n```shell\npod 'Filament', '~> 1.53.1'\n```\n\n### Snapshots\n\nIf you prefer to live on the edge, you can download a continuous build by following the following\nsteps:\n\n1. Find the [commit](https://github.com/google/filament/commits/main) you're interested in.\n2. Click the green check mark under the commit message.\n3. Click on the _Details_ link for the platform you're interested in.\n4. On the top left click _Summary_, then in the _Artifacts_ section choose the desired artifact.\n\n## Documentation\n\n- [Filament](https://google.github.io/filament/Filament.html), an in-depth explanation of\n  real-time physically based rendering, the graphics capabilities and implementation of Filament.\n  This document explains the math and reasoning behind most of our decisions. This document is a\n  good introduction to PBR for graphics programmers.\n- [Materials](https://google.github.io/filament/Materials.html), the full reference\n  documentation for our material system. This document explains our different material models, how\n  to use the material compiler `matc` and how to write custom materials.\n- [Material Properties](https://google.github.io/filament/Material%20Properties.pdf), a reference\n  sheet for the standard material model.\n\n## Examples\n\n![Night scene](docs/images/samples/example_bistro1.jpg)\n![Night scene](docs/images/samples/example_bistro2.jpg)\n![Materials](docs/images/samples/example_materials1.jpg)\n![Materials](docs/images/samples/example_materials2.jpg)\n![Helmet](docs/images/samples/example_helmet.jpg)\n![Screen-space refraction](docs/images/samples/example_ssr.jpg)\n\n## Features\n\n### APIs\n\n- Native C++ API for Android, iOS, Linux, macOS and Windows\n- Java/JNI API for Android\n- JavaScript API\n\n### Backends\n\n- OpenGL 4.1+ for Linux, macOS and Windows\n- OpenGL ES 3.0+ for Android and iOS\n- Metal for macOS and iOS\n- Vulkan 1.0 for Android, Linux, macOS, and Windows\n- WebGL 2.0 for all platforms\n\n### Rendering\n\n- Clustered forward renderer\n- Cook-Torrance microfacet specular BRDF\n- Lambertian diffuse BRDF\n- Custom lighting/surface shading\n- HDR/linear lighting\n- Metallic workflow\n- Clear coat\n- Anisotropic lighting\n- Approximated translucent (subsurface) materials\n- Cloth/fabric/sheen shading\n- Normal mapping & ambient occlusion mapping\n- Image-based lighting\n- Physically-based camera (shutter speed, sensitivity and aperture)\n- Physical light units\n- Point lights, spot lights, and directional light\n- Specular anti-aliasing\n- Point, spot, and directional light shadows\n- Cascaded shadows\n- EVSM, PCSS, DPCF, or PCF shadows\n- Transparent shadows\n- Contact shadows\n- Screen-space ambient occlusion\n- Screen-space reflections\n- Screen-space refraction\n- Global fog\n- Dynamic resolution (with support for AMD FidelityFX FSR)\n\n### Post processing\n\n- HDR bloom\n- Depth of field bokeh\n- Multiple tone mappers: generic (customizable), ACES, filmic, etc.\n- Color and tone management: luminance scaling, gamut mapping\n- Color grading: exposure, night adaptation, white balance, channel mixer,\n  shadows/mid-tones/highlights, ASC CDL, contrast, saturation, etc.\n- TAA, FXAA, MSAA\n- Screen-space lens flares\n\n### glTF 2.0\n\n- Encodings\n  - [x] Embeded\n  - [x] Binary\n\n- Primitive Types\n  - [x] Points\n  - [x] Lines\n  - [ ] Line Loop\n  - [x] Line Strip\n  - [x] Triangles\n  - [x] Triangle Strip\n  - [ ] Triangle Fan\n\n- Animation\n  - [x] Transform animation\n  - [x] Linear interpolation\n  - [x] Morph animation\n    - [x] Sparse accessor\n  - [x] Skin animation\n  - [x] Joint animation\n\n- Extensions\n  - [x] KHR_draco_mesh_compression\n  - [x] KHR_lights_punctual\n  - [x] KHR_materials_clearcoat\n  - [x] KHR_materials_emissive_strength\n  - [x] KHR_materials_ior\n  - [x] KHR_materials_pbrSpecularGlossiness\n  - [x] KHR_materials_sheen\n  - [x] KHR_materials_transmission\n  - [x] KHR_materials_unlit\n  - [x] KHR_materials_variants\n  - [x] KHR_materials_volume\n  - [x] KHR_materials_specular\n  - [x] KHR_mesh_quantization\n  - [x] KHR_texture_basisu\n  - [x] KHR_texture_transform\n  - [x] EXT_meshopt_compression\n\n\n## Rendering with Filament\n\n### Native Linux, macOS and Windows\n\nYou must create an `Engine`, a `Renderer` and a `SwapChain`. The `SwapChain` is created from a\nnative window pointer (an `NSView` on macOS or a `HWND` on Windows for instance):\n\n```c++\nEngine* engine = Engine::create();\nSwapChain* swapChain = engine->createSwapChain(nativeWindow);\nRenderer* renderer = engine->createRenderer();\n```\n\nTo render a frame you must then create a `View`, a `Scene` and a `Camera`:\n\n```c++\nCamera* camera = engine->createCamera(EntityManager::get().create());\nView* view = engine->createView();\nScene* scene = engine->createScene();\n\nview->setCamera(camera);\nview->setScene(scene);\n```\n\nRenderables are added to the scene:\n\n```c++\nEntity renderable = EntityManager::get().create();\n// build a quad\nRenderableManager::Builder(1)\n        .boundingBox({{ -1, -1, -1 }, { 1, 1, 1 }})\n        .material(0, materialInstance)\n        .geometry(0, RenderableManager::PrimitiveType::TRIANGLES, vertexBuffer, indexBuffer, 0, 6)\n        .culling(false)\n        .build(*engine, renderable);\nscene->addEntity(renderable);\n```\n\nThe material instance is obtained from a material, itself loaded from a binary blob generated\nby `matc`:\n\n```c++\nMaterial* material = Material::Builder()\n        .package((void*) BAKED_MATERIAL_PACKAGE, sizeof(BAKED_MATERIAL_PACKAGE))\n        .build(*engine);\nMaterialInstance* materialInstance = material->createInstance();\n```\n\nTo learn more about materials and `matc`, please refer to the\n[materials documentation](./docs/Materials.md.html).\n\nTo render, simply pass the `View` to the `Renderer`:\n\n```c++\n// beginFrame() returns false if we need to skip a frame\nif (renderer->beginFrame(swapChain)) {\n    // for each View\n    renderer->render(view);\n    renderer->endFrame();\n}\n```\n\nFor complete examples of Linux, macOS and Windows Filament applications, look at the source files\nin the `samples/` directory. These samples are all based on `libs/filamentapp/` which contains the\ncode that creates a native window with SDL2 and initializes the Filament engine, renderer and views.\n\nFor more information on how to prepare environment maps for image-based lighting please refer to\n[BUILDING.md](https://github.com/google/filament/blob/main/BUILDING.md#running-the-native-samples).\n\n### Android\n\nSee `android/samples` for examples of how to use Filament on Android.\n\nYou must always first initialize Filament by calling `Filament.init()`.\n\nRendering with Filament on Android is similar to rendering from native code (the APIs are largely\nthe same across languages). You can render into a `Surface` by passing a `Surface` to the\n`createSwapChain` method. This allows you to render to a `SurfaceTexture`, a `TextureView` or\na `SurfaceView`. To make things easier we provide an Android specific API called `UiHelper` in the\npackage `com.google.android.filament.android`. All you need to do is set a render callback on the\nhelper and attach your `SurfaceView` or `TextureView` to it. You are still responsible for\ncreating the swap chain in the `onNativeWindowChanged()` callback.\n\n### iOS\n\nFilament is supported on iOS 11.0 and above. See `ios/samples` for examples of using Filament on\niOS.\n\nFilament on iOS is largely the same as native rendering with C++. A `CAEAGLLayer` or `CAMetalLayer`\nis passed to the `createSwapChain` method. Filament for iOS supports both Metal (preferred) and\nOpenGL ES.\n\n## Assets\n\nTo get started you can use the textures and environment maps found respectively in\n`third_party/textures` and `third_party/environments`. These assets are under CC0 license. Please\nrefer to their respective `URL.txt` files to know more about the original authors.\n\nEnvironments must be pre-processed using\n[`cmgen`](https://github.com/google/filament/blob/main/BUILDING.md#running-the-native-samples) or\nusing the `libiblprefilter` library.\n\n## How to make contributions\n\nPlease read and follow the steps in [CONTRIBUTING.md](/CONTRIBUTING.md). Make sure you are\nfamiliar with the [code style](/CODE_STYLE.md).\n\n## Directory structure\n\nThis repository not only contains the core Filament engine, but also its supporting libraries\nand tools.\n\n- `android`:                  Android libraries and projects\n  - `filamat-android`:        Filament material generation library (AAR) for Android\n  - `filament-android`:       Filament library (AAR) for Android\n  - `filament-utils-android`: Extra utilities (KTX loader, math types, etc.)\n  - `gltfio-android`:         Filament glTF loading library (AAR) for Android\n  - `samples`:                Android-specific Filament samples\n- `art`:                      Source for various artworks (logos, PDF manuals, etc.)\n- `assets`:                   3D assets to use with sample applications\n- `build`:                    CMake build scripts\n- `docs`:                     Documentation\n  - `math`:                   Mathematica notebooks used to explore BRDFs, equations, etc.\n- `filament`:                 Filament rendering engine (minimal dependencies)\n  - `backend`:                Rendering backends/drivers (Vulkan, Metal, OpenGL/ES)\n- `ide`:                      Configuration files for IDEs (CLion, etc.)\n- `ios`:                      Sample projects for iOS\n- `libs`:                     Libraries\n  - `bluegl`:                 OpenGL bindings for macOS, Linux and Windows\n  - `bluevk`:                 Vulkan bindings for macOS, Linux, Windows and Android\n  - `camutils`:               Camera manipulation utilities\n  - `filabridge`:             Library shared by the Filament engine and host tools\n  - `filaflat`:               Serialization/deserialization library used for materials\n  - `filagui`:                Helper library for [Dear ImGui](https://github.com/ocornut/imgui)\n  - `filamat`:                Material generation library\n  - `filamentapp`:            SDL2 skeleton to build sample apps\n  - `filameshio`:             Tiny filamesh parsing library (see also `tools/filamesh`)\n  - `geometry`:               Mesh-related utilities\n  - `gltfio`:                 Loader for glTF 2.0\n  - `ibl`:                    IBL generation tools\n  - `image`:                  Image filtering and simple transforms\n  - `imageio`:                Image file reading / writing, only intended for internal use\n  - `matdbg`:                 DebugServer for inspecting shaders at run-time (debug builds only)\n  - `math`:                   Math library\n  - `mathio`:                 Math types support for output streams\n  - `utils`:                  Utility library (threads, memory, data structures, etc.)\n  - `viewer`:                 glTF viewer library (requires gltfio)\n- `samples`:                  Sample desktop applications\n- `shaders`:                  Shaders used by `filamat` and `matc`\n- `third_party`:              External libraries and assets\n  - `environments`:           Environment maps under CC0 license that can be used with `cmgen`\n  - `models`:                 Models under permissive licenses\n  - `textures`:               Textures under CC0 license\n- `tools`:                    Host tools\n  - `cmgen`:                  Image-based lighting asset generator\n  - `filamesh`:               Mesh converter\n  - `glslminifier`:           Minifies GLSL source code\n  - `matc`:                   Material compiler\n  - `matinfo`                 Displays information about materials compiled with `matc`\n  - `mipgen`                  Generates a series of miplevels from a source image\n  - `normal-blending`:        Tool to blend normal maps\n  - `resgen`                  Aggregates binary blobs into embeddable resources\n  - `roughness-prefilter`:    Pre-filters a roughness map from a normal map to reduce aliasing\n  - `specular-color`:         Computes the specular color of conductors based on spectral data\n- `web`:                      JavaScript bindings, documentation, and samples\n\n## License\n\nPlease see [LICENSE](/LICENSE).\n\n## Disclaimer\n\nThis is not an officially supported Google product.\n",17400,graphics,C++,24,CMake,C++,Objective-C++,C,Java,Shell,HTML,Python,Assembly,GLSL,Kotlin,Batchfile,JavaScript,CSS,Objective-C,TypeScript,Groovy,Ruby,Dockerfile,RenderScript,Go,C#,Tcl,Emacs Lisp,,,,,4746,268,4463,15,71,157,5049,698538,1840,2226,2088,138,a8ace2891d2d83e8b9dd7d06a01591d3ff53c764,improve frame timing info,2024-07-19T04:14:44Z,Mathias Agopian,mathias@google.com,pixelflinger,v1.53.1,,v1.53.1,Ben Doherty,,bejado,Apache License 2.0,filament,google,191,pbr,graphics,3d-graphics,real-time,android,opengl,opengl-es,vulkan,webgl,wasm,metal,gltf,gltf-viewer,,,,,,,,/google/filament,196,382,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/google/deepvariant,https://github.com/google/deepvariant,1,deep learning-based,,1,1,1,1,0,0,0,0,0,0,1,DeepVariant is an analysis pipeline that uses a deep neural network to call genetic variants from next-generation DNA sequencing data.,"<img src=""docs/images/dv_logo.png"" width=50% height=50%>\n\n[![release](https://img.shields.io/badge/release-v1.6.1-green?logo=github)](https://github.com/google/deepvariant/releases)\n[![announcements](https://img.shields.io/badge/announcements-blue)](https://groups.google.com/d/forum/deepvariant-announcements)\n[![blog](https://img.shields.io/badge/blog-orange)](https://goo.gl/deepvariant)\n\nDeepVariant is a deep learning-based variant caller that takes aligned reads (in\nBAM or CRAM format), produces pileup image tensors from them, classifies each\ntensor using a convolutional neural network, and finally reports the results in\na standard VCF or gVCF file.\n\nDeepVariant supports germline variant-calling in diploid organisms.\n\n*   NGS (Illumina or Element) data for either a\n    [whole genome](docs/deepvariant-case-study.md) or\n    [whole exome](docs/deepvariant-exome-case-study.md).\n*   [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina\n    RNA-seq.\n*   PacBio HiFi data, see the\n    [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).\n*   Oxford Nanopore R10.4.1 Simplex or Duplex data, see the\n    [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md)\n    and\n    [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).\n*   Hybrid PacBio HiFi + Illumina WGS, see the\n    [hybrid case study](docs/deepvariant-hybrid-case-study.md).\n*   Oxford Nanopore R9.4.1 data by using\n    [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper).\n*   To map using a pangenome to improve accuracy, use this\n    [vg case study](docs/deepvariant-vg-case-study.md).\n*   Complete Genomics data:\n    [T7 case study](docs/deepvariant-complete-t7-case-study.md);\n    [G400 case study](docs/deepvariant-complete-g400-case-study.md)\n\nPlease also note:\n\n*   For somatic data or any other samples where the genotypes go beyond two\n    copies of DNA, DeepVariant will not work out of the box because the only\n    genotypes supported are hom-alt, het, and hom-ref.\n*   The models included with DeepVariant are only trained on human data. For\n    other organisms, see the\n    [blog post on non-human variant-calling](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/)\n    for some possible pitfalls and how to handle them.\n\n## DeepTrio\n\nDeepTrio is a deep learning-based trio variant caller built on top of\nDeepVariant. DeepTrio extends DeepVariant's functionality, allowing it to\nutilize the power of neural networks to predict genomic variants in trios or\nduos. See [this page](docs/deeptrio-details.md) for more details and\ninstructions on how to run DeepTrio.\n\nDeepTrio supports germline variant-calling in diploid organisms for the\nfollowing types of input data:\n\n*   NGS (Illumina) data for either\n    [whole genome](docs/deeptrio-wgs-case-study.md) or whole exome.\n*   PacBio HiFi data, see the\n    [PacBio case study](docs/deeptrio-pacbio-case-study.md).\n\nPlease also note:\n\n*   All DeepTrio models were trained on human data.\n*   It is possible to use DeepTrio with only 2 samples (child, and one parent).\n*   External tool [GLnexus](https://github.com/dnanexus-rnd/GLnexus) is used to\n    merge output VCFs.\n\n## How to run DeepVariant\n\nWe recommend using our Docker solution. The command will look like this:\n\n```\nBIN_VERSION=""1.6.1""\ndocker run \\n  -v ""YOUR_INPUT_DIR"":""/input"" \\n  -v ""YOUR_OUTPUT_DIR:/output"" \\n  google/deepvariant:""${BIN_VERSION}"" \\n  /opt/deepvariant/bin/run_deepvariant \\n  --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**\n  --ref=/input/YOUR_REF \\n  --reads=/input/YOUR_BAM \\n  --output_vcf=/output/YOUR_OUTPUT_VCF \\n  --output_gvcf=/output/YOUR_OUTPUT_GVCF \\n  --num_shards=$(nproc) \ **This will use all your cores to run make_examples. Feel free to change.**\n  --logging_dir=/output/logs \ **Optional. This saves the log output for each stage separately.\n  --haploid_contigs=""chrX,chrY"" \ **Optional. Heterozygous variants in these contigs will be re-genotyped as the most likely of reference or homozygous alternates. For a sample with karyotype XY, it should be set to ""chrX,chrY"" for GRCh38 and ""X,Y"" for GRCh37. For a sample with karyotype XX, this should not be used.\n  --par_regions_bed=""/input/GRCh3X_par.bed"" \ **Optional. If --haploid_contigs is set, then this can be used to provide PAR regions to be excluded from genotype adjustment. Download links to this files are available in this page.\n  --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.\n```\n\nFor details on X,Y support, please see\n[DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case\nstudy in\n[DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You\ncan download the PAR bed files from here:\n[GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),\n[GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed).\n\nTo see all flags you can use, run: `docker run\ngoogle/deepvariant:""${BIN_VERSION}""`\n\nIf you're using GPUs, or want to use Singularity instead, see\n[Quick Start](docs/deepvariant-quick-start.md) for more details or see all the\n[setup options](#deepvariant_setup) available.\n\nFor more information, also see:\n\n*   [Full documentation list](docs/README.md)\n*   [Detailed usage guide](docs/deepvariant-details.md) with more information on\n    the input and output file formats and how to work with them.\n*   [Best practices for multi-sample variant calling with DeepVariant](docs/trio-merge-case-study.md)\n*   [(Advanced) Training tutorial](docs/deepvariant-training-case-study.md)\n*   [DeepVariant's Frequently Asked Questions, FAQ](docs/FAQ.md)\n\n## How to cite\n\nIf you're using DeepVariant in your work, please cite:\n\n[A universal SNP and small-indel variant caller using deep neural networks. *Nature Biotechnology* 36, 983–987 (2018).](https://rdcu.be/7Dhl) <br/>\nRyan Poplin, Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas Colthurst, Alexander Ku, Dan Newburger, Jojo Dijamco, Nam Nguyen, Pegah T. Afshar, Sam S. Gross, Lizzie Dorfman, Cory Y. McLean, and Mark A. DePristo.<br/>\ndoi: https://doi.org/10.1038/nbt.4235\n\nAdditionally, if you are generating multi-sample calls using our\n[DeepVariant and GLnexus Best Practices](docs/trio-merge-case-study.md), please\ncite:\n\n[Accurate, scalable cohort variant calls using DeepVariant and GLnexus.\n_Bioinformatics_ (2021).](https://doi.org/10.1093/bioinformatics/btaa1081)<br/>\nTaedong Yun, Helen Li, Pi-Chuan Chang, Michael F. Lin, Andrew Carroll, and Cory\nY. McLean.<br/>\ndoi: https://doi.org/10.1093/bioinformatics/btaa1081\n\n## Why Use DeepVariant?\n\n*   **High accuracy** - DeepVariant won 2020\n    [PrecisionFDA Truth Challenge V2](https://precision.fda.gov/challenges/10/results)\n    for All Benchmark Regions for ONT, PacBio, and Multiple Technologies\n    categories, and 2016\n    [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results)\n    for best SNP Performance. DeepVariant maintains high accuracy across data\n    from different sequencing technologies, prep methods, and species. For\n    [lower coverage](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/),\n    using DeepVariant makes an especially great difference. See\n    [metrics](docs/metrics.md) for the latest accuracy numbers on each of the\n    sequencing types.\n*   **Flexibility** - Out-of-the-box use for\n    [PCR-positive](https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html)\n    samples and\n    [low quality sequencing runs](https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/),\n    and easy adjustments for\n    [different sequencing technologies](https://google.github.io/deepvariant/posts/2019-01-14-highly-accurate-snp-and-indel-calling-on-pacbio-ccs-with-deepvariant/)\n    and\n    [non-human species](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).\n*   **Ease of use** - No filtering is needed beyond setting your preferred\n    minimum quality threshold.\n*   **Cost effectiveness** - With a single non-preemptible n1-standard-16\n    machine on Google Cloud, it costs ~$11.8 to call a 30x whole genome and\n    ~$0.89 to call an exome. With preemptible pricing, the cost is $2.84 for a\n    30x whole genome and $0.21 for whole exome (not considering preemption).\n*   **Speed** - See [metrics](docs/metrics.md) for the runtime of all supported\n    datatypes on a 64-core CPU-only machine</sup>. Multiple options for\n    acceleration exist.\n*   **Usage options** - DeepVariant can be run via Docker or binaries, using\n    both on-premise hardware or in the cloud, with support for hardware\n    accelerators like GPUs and TPUs.\n\n<a name=""myfootnote1"">(1)</a>: Time estimates do not include mapping.\n\n## How DeepVariant works\n\n![Stages in DeepVariant](docs/images/inference_flow_diagram.svg)\n\nFor more information on the pileup images and how to read them, please see the\n[""Looking through DeepVariant's Eyes"" blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/).\n\nDeepVariant relies on [Nucleus](https://github.com/google/nucleus), a library of\nPython and C++ code for reading and writing data in common genomics file formats\n(like SAM and VCF) designed for painless integration with the\n[TensorFlow](https://www.tensorflow.org/) machine learning framework. Nucleus\nwas built with DeepVariant in mind and open-sourced separately so it can be used\nby anyone in the genomics research community for other projects. See this blog\npost on\n[Using Nucleus and TensorFlow for DNA Sequencing Error Correction](https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/).\n\n## DeepVariant Setup\n\n### Prerequisites\n\n*   Unix-like operating system (cannot run on Windows)\n*   Python 3.8\n\n### Official Solutions\n\nBelow are the official solutions provided by the\n[Genomics team in Google Health](https://health.google/health-research/).\n\nName                                                                                                | Description\n:-------------------------------------------------------------------------------------------------: | -----------\n[Docker](docs/deepvariant-quick-start.md)           | This is the recommended method.\n[Build from source](docs/deepvariant-build-test.md) | DeepVariant comes with scripts to build it on Ubuntu 20.04. To build and run on other Unix-based systems, you will need to modify these scripts.\nPrebuilt Binaries                                                                                   | Available at [`gs://deepvariant/`](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you will need a CPU (such as Intel Sandy Bridge) that supports them. You can check the `/proc/cpuinfo` file on your computer, which lists these features under ""flags"".\n\n## Contribution Guidelines\n\nPlease [open a pull request](https://github.com/google/deepvariant/compare) if\nyou wish to contribute to DeepVariant. Note, we have not set up the\ninfrastructure to merge pull requests externally. If you agree, we will test and\nsubmit the changes internally and mention your contributions in our\n[release notes](https://github.com/google/deepvariant/releases). We apologize\nfor any inconvenience.\n\nIf you have any difficulty using DeepVariant, feel free to\n[open an issue](https://github.com/google/deepvariant/issues/new). If you have\ngeneral questions not specific to DeepVariant, we recommend that you post on a\ncommunity discussion forum such as [BioStars](https://www.biostars.org/).\n\n## License\n\n[BSD-3-Clause license](LICENSE)\n\n## Acknowledgements\n\nDeepVariant happily makes use of many open source packages. We would like to\nspecifically call out a few key ones:\n\n*   [Boost Graph Library](http://www.boost.org/doc/libs/1_65_1/libs/graph/doc/index.html)\n*   [abseil-cpp](https://github.com/abseil/abseil-cpp) and\n    [abseil-py](https://github.com/abseil/abseil-py)\n*   [CLIF](https://github.com/google/clif)\n*   [GNU Parallel](https://www.gnu.org/software/parallel/)\n*   [htslib & samtools](http://www.htslib.org/)\n*   [Nucleus](https://github.com/google/nucleus)\n*   [numpy](http://www.numpy.org/)\n*   [SSW Library](https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library)\n*   [TensorFlow and Slim](https://www.tensorflow.org/)\n\nWe thank all of the developers and contributors to these packages for their\nwork.\n\n## Disclaimer\n\nThis is not an official Google product.\n\nNOTE: the content of this research code repository (i) is not intended to be a\nmedical device; and (ii) is not intended for clinical use of any kind, including\nbut not limited to diagnosis or prognosis.\n",3128,bioinformatics,Python,5,Python,Shell,C++,Dockerfile,Starlark,,,,,,,,,,,,,,,,,,,,,,,,61,21,40,0,22,26,41,664220,703,790,783,7,bf9ed7e6de97cf6c8381694cb996317a740625ad,Resolve conflicts in test files,2024-03-18T19:51:35Z,shafin,shafin@google.com,kishwarshafin,DeepVariant 1.6.1,"In this release:\r\n\r\n* We fixed a bug in `call_variants` that caused the step to freeze in cases where there were no examples. This bug was observed and reported in https://github.com/google/deepvariant/issues/764, https://github.com/google/deepvariant/issues/769, https://github.com/google/deepsomatic/issues/8.\r\n* Updated `libssw` library from 1.2.4 to 1.2.5. \r\n* The same model files are used for v1.6.0 and v1.6.1 for all technologies.\r\n",v1.6.1,Kishwar Shafin,,kishwarshafin,"BSD 3-Clause ""New"" or ""Revised"" License",deepvariant,google,21,tensorflow,deep-neural-network,genomics,science,dna,sequencing,genome,bioinformatics,deep-learning,ngs,deepvariant,machine-learning,,,,,,,,,/google/deepvariant,21,160,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/google/chicago-brick,https://github.com/google/chicago-brick,0,,0,0,0,1,0,0,0,0,1,0,0,0,Software that displays content on a multi-node video wall.,"# Chicago Brick: the Google Chicago Video Wall Software\n\n## Quick Start\n\nTo use this software, first download and install [deno](http://deno.land).\n\nNext, clone the repo to your machine:\n\n```bash\n$ git clone https://github.com/google/chicago-brick.git\n```\n\nThen, to run the server in 1x1 mode with the gears module:\n\n```\nchicago-brick$ ./bin/run_1x1.sh -m gears\n```\n\nThis should open a Chrome window to http://localhost:3000/?config=0,0,1920,1080.\nYou should be able to see some gears rotating. If you don't see that, try\nrunning the `npm install` command again.\n\nOr, to run the server in 2x2 mode with the gears module:\n\n```\nchicago-brick$ ./bin/run_2x2.sh -m gears\n```\n\nAnd open the client windows like so:\n\n```\nchicago-brick$ ./bin/start_2x2_clients.sh\n```\n\nYou should be able to see the same thing with four browser windows instead.\n\nYou can play with different modules by substituting the `gears` argument with\nother names from `chicago-brick/config/demo-playlist.json` or from the various\n`brick.json` files within the `demo_modules` folder.\n\n## Geometry\n\nThe wall server needs to know the shape of the screens that make up the wall. At\nthe moment, this shape must be a single polygon, though it can be concave. By\ndefault, the wall assumes that it's going to display on a 1920x1080 screen. To\nchange this, you can use the `--use_geometry` flag to specify the shape in a\nturtle-like langauge. You can also specify the points in a JSON-formatted file\nand use the `--geometry_file` flag to pass the path to the file.\n\n## Modules\n\nA chicago brick module is responsible for showing control across the wall.\nModules are stored in a directory and contain a `brick.json` file with metadata\nabout the module. The directory also contains the client and server parts to the\nmodule, which are executed on the clients or server respectively. These parts\ncan be written in TypeScript or JavaScript, though TypeScript is strongly\npreferred. See examples in demo_modules.\n\nThe server learns about modules that can be shown via the `--module_dir` flag,\nwhich is scanned for any `brick.json` files. Playlists are only allowed to\nreference modules that the wall knows about.\n\n## Playlist\n\nA playlist defines the order in which the wall should play modules. The playlist\nconsists of layouts, each of which refers to either a specific list of modules\nor a pre-defined collection of modules. The layout will randomly select among\nits set of modules and play each for the specified module duration. After the\nlayout duration expires, the next layout is shown. For example:\n\n```json\n{\n  ""playlist"": [\n    {\n      ""modules"": [""gears"", ""slither""],\n      ""duration"": 600,\n      ""moduleDuration"": 60\n    },\n    {\n      ""modules"": [""matrix""],\n      ""duration"": 600,\n      ""moduleDuration"": 600\n    }\n  ]\n}\n```\n\n## Contributing\n\nWe welcome contributions of new modules and of improvements to the wall software\nitself! See the CONTRIBUTING file for some stuff you need to complete before you\ncontribute.\n\nHopefully, this gets you developing!\n\n– Chicago Brick Team\n",152,graphics,TypeScript,7,Shell,JavaScript,HTML,CSS,SCSS,TypeScript,GLSL,,,,,,,,,,,,,,,,,,,,,,337,34,300,3,9,16,0,15587,34,82,58,24,c9d0e48fcef304540378d173c309d92bae99457d,Merge pull request #423 from aleksrozman/master,2024-06-03T21:25:23Z,Gabriel Harel,gharel@google.com,g-harel,0.6.2,## What's Changed\r\n* Add standard collision detection routines by @applmak in https://github.com/google/chicago-brick/pull/368\r\n* Open source win31 module. by @applmak in https://github.com/google/chicago-brick/pull/369\r\n* More fixes by @applmak in https://github.com/google/chicago-brick/pull/370\r\n* Even more fixes by @applmak in https://github.com/google/chicago-brick/pull/371\r\n* Optimize the Mandelbrot fractal shader by @nputikhin in https://github.com/google/chicago-brick/pull/372\r\n* Fix global content refreshes by @applmak in https://github.com/google/chicago-brick/pull/373\r\n* Change behavior of module prep timeouts by @applmak in https://github.com/google/chicago-brick/pull/374\r\n\r\n## New Contributors\r\n* @nputikhin made their first contribution in https://github.com/google/chicago-brick/pull/372\r\n\r\n**Full Changelog**: https://github.com/google/chicago-brick/compare/0.6.1...0.6.2,0.6.2,Matt Handley,,applmak,Apache License 2.0,chicago-brick,google,7,video-wall,nodejs,graphics,chrome,visualization,,,,,,,,,,,,,,,,/google/chicago-brick,45,18,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/GooFit/GooFit,https://github.com/GooFit/GooFit,0,,,0,0,0,0,0,0,1,0,0,0,0,"Code repository for the massively-parallel framework for maximum-likelihood fits, implemented in CUDA/OpenMP","[![Actions Status][actions-badge]][actions-link]\n[![Travis Status][travis-badge]][travis-link]\n[![Join the chat at https://gitter.im/GooFit/Lobby][gitter-badge]][gitter-link]\n[![License: BSD][license-badge]](./LICENSE)\n[![Latest release][releases-badge]][releases-link]\n[![PyPI Status][pypi-status]][pypi-link]\n[![Conda-Forge Status][cf-status]][cf-link]\n[![DOI][DOI-badge]][DOI-link]\n[![Scikit-HEP][sk-badge]](https://scikit-hep.org/)\n\n<p align=""center"">\n  <img width=""495"" height=""220"" src=""https://raw.githubusercontent.com/GooFit/GooFit/master/docs/GooFitLogo.png""/>\n</p>\n\nGooFit is a massively-parallel framework, written using Thrust for CUDA and OpenMP, for\ndoing maximum-likelihood fits with a familiar syntax.\n\n[What's new](./docs/CHANGELOG.md)\n• [Tutorials]\n• [API documentation]\n• [2.0 upgrade](./docs/CONVERTING20.md)\n• [2.1 upgrade](./docs/CONVERTING21.md)\n• [2.2 upgrade](./docs/CONVERTING22.md)\n• [Build recipes](./docs/SYSTEM_INSTALL.md)\n• [Python](https://pypi.python.org/pypi/goofit/)\n\n## Known issues\nhttps://github.com/GooFit/GooFit/labels/critical https://github.com/GooFit/GooFit/labels/amplitude%20analysis https://github.com/GooFit/GooFit/labels/cuda\n\n## Requirements\n\n* A recent version of CMake is required. The minimum is 3.9. CMake is incredibly easy to install, you can even use `pip` (see [the system install page](./docs/SYSTEM_INSTALL.md)). GooFit developers have supplied patches to CMake 3.12, so at least that is highly recommended. CMake 3.16 does not currently work with the Python bindings.\n* A ROOT 6 build highly recommended -- GooFit will use the included Minuit2 submodule if ROOT is not found, and the Minuit1 based fitter will not be available. Supports 6.04-6.24 (6.10+ recommended).\n\n<details><summary>If using CUDA: (click to expand)</summary><p>\n\n* CMake 3.9+\n* CUDA 8.0+ (with caveats below)\n    * CUDA 8: Supported\n    * CUDA 9.2, 10.0: Some warnings from Eigen, supported\n    * CUDA 9.0, 9.1: Buggy, see [known issues](https://github.com/GooFit/GooFit/issues/173)\n    * CUDA 10.1, 10.2: Not yet supported due to Thrust 1.8 incompatibility\n* An nVidia GPU supporting compute capability at least 3.0 (3.5+ recommended)\n\n</p></details>\n\n<details><summary>If using OpenMP: (click to expand)</summary><p>\n\n* A compiler supporting OpenMP and C++11 (GCC 4.8+, Clang, and Intel 17 tested, GCC 4.7 not supported)\n* Note that TBB is also available as a backend, but it still requires OpenMP to be present.\n* On macOS, this backend requires `brew install libomp` or a custom compiler.\n\n</p></details>\n\n<details><summary>If using CPP: (click to expand)</summary><p>\n\n* Single threaded builds are available for debugging and development (such as on the default Clang on macOS)\n\n</p></details>\n\n<br/>\n\nA list of exact commands required for several platforms is [available here](./docs/SYSTEM_INSTALL.md).\n\n\n<details><summary>Python Bindings: (click to expand)</summary><p>\n\nThere are also Python Bindings. This requires Python (2 or 3), [NumPy](http://www.numpy.org), [SciKit-Build](http://scikit-build.readthedocs.io), and CMake. CUDA 8+ is required if using CUDA. If you want the most recent stable release, use `pip install -v goofit` (If you have pip 9 or less, you'll need scikit-build and cmake installed beforehand). Python 2 will be removed soon.\n\nRepository method:\n\nYou can uses `pip install -v .` inside the repository. You can also directly force the bindings from a normal build with `-DGOOFIT_PYTHON=ON`. You can check your install with `python -m goofit`. You can debug a goofit file named `python_script.py` with gcc using `gdb -ex r --args python python_script.py`.\n\nOther python requirements for the examples:\n\n* numpy-1.11.1+\n* pandas-0.15.1+\n* uncertainties-3.0.2\n* matplotlib\n* plumbum\n\nOptional:\n\n* numba\n\n</p></details>\n\n<br/>\n\n## Getting the files\n\n* Clone with git:\n\n```bash\ngit clone git://github.com/GooFit/GooFit.git --recursive\ncd GooFit\n```\n\nYou can either checkout a tagged version, or stay on the master for the latest and greatest. There are often development branches available, too. You can use `--jobs=N` or set git's `submodule.fetchJobs` configuration parameter to download the submodules in parallel with `N` threads.\n\n## Building\n\nIf you just want to get started as fast as possible, running `make`, `make omp`, or `make cuda` in the main directory will make a build directory for you, and will run CMake and make. It is recommended that you instead directly use the CMake powered build system as described below, so that you will have a better understanding of what you are doing and more flexibility.\n\nThe build system uses CMake. The procedure is standard for CMake builds:\n\n```bash\n# Classic method\nmkdir build\ncd build\ncmake ..\nmake -j4 # 4 threads, adjust as needed\n\n# Newer method (CMake 3.13+)\ncmake -S . -B build\ncmake --build build -j4 # 4 threads, adjust as needed\n```\n\nIf you don't have a modern CMake, Kitware provides installers for every OS. You can even get a copy using python: `pip install cmake` or locally with `pip install --user cmake`.\nOn a Mac, you can also use any package manager, such as Homebrew: `brew install cmake`.\n\nIf you want to change compiler, set `CC` and `CXX` to appropriate defaults *before* you run CMake either inline or in your environment. You can also set `CMAKE_C_COMPILER` and `CMAKE_CXX_COMPILER` directly on the command line with `-D`. If you want to set the host and device backends, you can set those options. The defaults are:\n\n```bash\ncmake .. -DGOOFIT_DEVICE=CUDA -DGOOFIT_HOST=CPP\n```\n\nValid options are `CUDA` (device only), `OMP`, `TBB`, and `CPP`. The Thrust `TBB` backend requires the Intel compiler.  The default device is `Auto`, and will select `CUDA` if CUDA is found, `OMP` or `CPP` otherwise.\n\nOther custom options supported along with the defaults:\n\n* `-DGOOFIT_DEVICE=Auto`: The device to use for computation (`CUDA`, `OMP`, `TBB`, or `CPP`). Default setting of `Auto` looks for CUDA first, then OpenMP, then CPP.\n* `-DGOOFIT_ARCH=Auto`: (`Auto`, `Common`, `All`, valid number(s) or name(s)): sets the compute architecture. See [CUDA_SELECT_NVCC_ARCH_FLAGS][]. Can be set to `OFF` to avoid adding any flags.\n* `-DGOOFIT_EXAMPLES=ON`: Build the examples\n* `-DGOOFIT_PACKAGES=ON`: Build any packages found with the name `goofit_*`\n* `-DGOOFIT_DEBUG=ON` and `-DGOOFIT_TRACE=ON` will enable the matching printout macros\n* `-DGOOFIT_PYTHON=ON`: Include the python bindings using [pybind11] if Python found (use `-DPYTHON_EXECUTABLE=$(which python3)` to use a specific interpreter).\n\n<details><summary>Advanced Options: (click to expand)</summary><p>\n\n* `-DGOOFIT_HOST=Auto`: This is CPP unless device is `OMP`, in which case it is also `OMP`. This changes `thrust::host_vector` calculations, and is not fully supported when set to a non-default setting.\n* `-DGOOFIT_TESTS=ON`: Build the GooFit tests\n* `-DGOOFIT_MPI=ON`: (OFF/ON.  With this feature on, GPU devices are selected automatically).  Tested with MVAPICH2/2.2 and OpenMPI.\n* You can enable sanitizers on non-CUDA builds with `-DSANITIZE_ADDRESS=ON`, `-DSANITIZE_MEMORY=ON`, `-DSANITIZE_THREAD=ON` or `-DSANITIZE_UNDEFINED=ON`.\n* If `clang-tidy` is available, it will automatically be used to check the source. If you set `-DGOOFIT_TIDY_FIX=ON`, fixes will be applied to the GooFit source.\n* `-DGOOFIT_SPLASH=ON`: Controls the unicode splash at the beginning.\n* `-DGOOFIT_CERNROOT=ON`: Allows you to disable the automatic search for ROOT (used by the PIP Python build)\n* `-DCMAKE_UNITY_BUILD=OFF`: Turn on Unity builds in CMake 3.16+. Should be a bit faster (does not speed up CUDA portions of builds).\n\n</p></details>\n\n<details><summary>A few standard CMake tricks: (click to expand)</summary><p>\n\n* Use `make VERBOSE=1` to see the commands used to build the files.\n* Use `cmake .. -LH` to list the CMake options with help.\n* Use `ccmake` if available to see a curses (terminal) gui, or `cmake-gui` for a completely graphical interface.\n* Use `-G` and the name of a generator to use something other than `make`, like `Xcode` or `Ninja`.\n* Open the `CMakeLists.txt` with QtCreator to generate for that IDE.\n* Set the release type with `-DCMAKE_BUILD_TYPE=Release`, `RelWithDebInfo`, `Debug`, etc.\n* Set up multiple build directories, like `build-omp` and `build-cuda`.\n* CMake caches your `-D` option selections in your build directory so you don't have to specify them again.\n* CMake reruns when needed when you `make` unless you add a file that it globs for (like new `goofit_projects`).\n* Use `make -j12` to build with 12 cores (for example). You can set this as the `MAKEFLAGS` environment variable, too.\n* Use `CMake --build .` to build without referring to your specific build tool, like `make` or `ninja`.\n* If you are using the `llvm` tool-suite, you can use `-DCMAKE_EXPORT_COMPILE_COMMANDS=ON` to generate the .json file that the `clang-*` commands expect.\n\n</p></details>\n\n\n\n## Running the examples and tests\n\n* To run all the examples, with timing information, use:\n\n```bash\n./examples/RunAll.py\n./pyexamples/RunAll.sh # Python\n```\n\n(This requires the [Plumbum] library, install with `pip install plumbum`, `pip install --user plumbum`, or `conda -c conda-forge plumbum`.)\n\nIf you want to run an individual example, those are in subdirectories in examples (built products are in your build directory, the source is in `/examples`).\n\nThe tests can be run with `make test` or `ctest`. The python bindings, if built, can be tested with `pytest`, run from the main build directory. The python examples and tests folders are linked to the build directory with a `py` prefix.\n\n## Other topics\n\n<details><summary>Adding a new example: (click to expand)</summary><p>\n\nThe examples are designed to be easy to add to. Make a new directory, then add a new CMakeLists.txt in your directory with one or more of the following two lines:\n\n```cmake\ngoofit_add_directory()\ngoofit_add_executible(MyNewExample MyNewExample.cu)\n```\n\nThe first line adds your `.cu` file with GooFit code as an executable, and the second one sets up a symbolic links to the source and `dataFiles` in the build directory to the source directory. If you prefer to only have some files symbolically linked, use `goofit_add_link(filename.ext)` explicitly for each file. This happens at configure time. To get the example to build when you build GooFit, add the name of your directory to `examples/CMakeLists.txt`.\n\nIf you are building with separable compilation, you can also use `goofit_add_pdf(mypdf.cu)` to add a PDF. This will also require that you include any directory that you need with `include_directory`, as usual.\n\nTo add packages, use standard CMake tools. For example (CMake 3.5+), to add [Boost][FindBoost] 1.49+ filesystem and `TTreeReader` from ROOT:\n\n```cmake\nset(Boost_USE_STATIC_LIBS OFF)\nset(Boost_USE_MULTITHREADED ON)\nset(Boost_USE_STATIC_RUNTIME OFF)\nfind_package(Boost 1.49 REQUIRED COMPONENTS filesystem)\n\ngoofit_add_executable(K3Pi K3Pi.cu)\ntarget_link_libraries(MyNewExample Boost::filesystem ROOT::TreePlayer)\n```\n\n</p></details>\n\n\n<details><summary>Adding a new project: (click to expand)</summary><p>\n\n### External package (BETA)\n\nGooFit now requires separable compilation, so it also now supports ""external"" packages, much like most other libraries. You can design your package with GooFit included as a subdirectory, and\nit should just work. You'll also save time by not building examples, python bindings, and tests. The recommended procedure:\n\n```bash\ngit add submodule <url to goofit> goofit\ngit submodule update --init --recursive\n```\n\nThen, you'll need a CMakeLists that looks something like this:\n\n```bash\ncmake_minimum_required(VERSION 3.9...3.16)\n\nproject(my_external_package LANGUAGES CXX)\n\nadd_subdirectory(goofit)\ngoofit_external_package()\n\ngoofit_add_executable(myapp myapp.cpp)\n```\n\nThat's it! Just make a build directory and build. The `goofit_external_package()` command sets up optional CUDA, as well as links all reasonable files into your build directory. You can run `goofit_setup_std()`, `goofit_optional_cuda()` and `goofit_add_directory()` instead if you want.\n\n### Classic method\n\nIf you'd like to make a separate GooFit project, you can do so. Simply checkout your project inside GooFit, with the name `work` or `goofit_`+something. CMake will automatically pick up those directories and build them, and GooFit's git will ignore them. Otherwise, they act just like the example directory. If you add a new directory, you will need to explicitly rerun CMake, as that cannot be picked up by the makefile. The automatic search can be turned off with the `GOOFIT_PROJECTS` option, or by using `GOOFIT_PROJECT_<name>` for a specific package.\nGooFit packages should contain:\n\n```cmake\ngoofit_add_package(MyPackageName)\n```\n\nAfter the package name, you can list `ROOT` to require that ROOT. The package will be disabled if ROOT is not found.\n\n</p></details>\n\n\n\n\n<details><summary style=""font-size: 1.5em; margin-top: 24px; font-weight: 600; border-bottom: 1px solid #eaecef; line-height: 1.25"">Using an IDE: (click to expand)</summary><p>\n\nThe following IDEs have been tested. Here `$SRC` refers to the source directory, and usually is `..` or `../GooFit`. You may want `-DCMAKE_BUILD_TYPE=Debug` and/or `-DGOOFIT_DEBUG=ON`.\n\n| Name | Platform | Setup | Notes |\n|------|----------|:------|:------|\n| Xcode | macOS | `cmake $SRC -GXcode` | Only CPP version, works well though |\n| Nsight-Eclipse | Linux | `cmake $SRC -G ""Eclipse CDT4 - Unix Makefiles""` | Must be out-of-source, supports CUDA backend |\n| QtCreator | All | Open from QtCreator dialog | Requires CMake extension (usually present). Might be able to use CMake 3.7+ Server |\n| CLion | All | Open from CLion menu | Young but promising |\n\n</p></details>\n\n\n<details><summary>Converting from older GooFit Code: (click to expand)</summary><p>\n\nThe build system underwent a major upgrade in the move to CMake. The folders that were introduced to keep the includes structured require modifications of source code, converting lines like `#include ""Variable.hh""` to `#include ""GooFit/Variable.h""`. This modification can be done for you by running the provided script, `scripts/ModernizeGooFit.py` on your source files (requires Python and [Plumbum](https://github.com/tomerfiliba/plumbum)). You should remove your old Makefiles and use the new `CMakeFiles.txt` files provided in examples - this should require\nwriting two lines of code instead of the 50 or so previously needed. You should also add a GooFit Application to your code. (2 lines of CMake)\n\nThe new `GooFit::Application`, which is not required but provides GooFit options, like GPU selection and status, as well as MPI support and configurable command line options, is available by adding:\n\n```cpp\n#include ""GooFit/Application.h""\nusing namespace GooFit;\n\n// Place this at the beginning of main\nApplication app{""Optional description"", argc, argv};\n\n// Command line options can be added here.\n\nGOOFIT_PARSE(app);\n```\n\nSee [CLI11] for more details. The [pipipi0](./examples/pipipi0DPFit) example has an example of a complex set of options.\n\nThe other key differences in code are the addition of the `GooFit` namespace (`using namespace GooFit` allows fast conversion), and the removal of direct access to members of `Variable` (using getters/setters, or directly treat the variable like its value).\n\nSee Converting to [GooFit 2.0](./docs/CONVERTING20.md), [GooFit 2.1](./docs/CONVERTING21.md), and the [Changelog](./docs/CHANGELOG.md).\n\n</p></details>\n\n\n<details><summary>Improving performance with MPI: (click to expand)</summary><p>\n\nUsing the MPI version with an appropriate environment setup will allow for multiple GPU's to be used, and/or allow for multiple nodes.  To use this feature simply turn the flag on with CMake `-DGOOFIT_MPI=ON`.  This will divide the dataset by the number of processes involved.  For instance, if you have two nodes that will be involved in the calculation, the data will be split in half.  Currently, each node will load the entire buffer from disk, then load partitioned data it will work on.  It is highly recommended not to use more than one process per node for MPI+OpenMP versions.\n\nA few notes about using the MPI version:\n\n* You will need to use the `CountingVariable` for any event numbers used or referenced within the code, or anything that counts with the events.\n* Please call `setDataSize` after `setData`.  If you do not, `setDataSize` doesn't have `m_iEventsPerTask`, which will need to be recalculated.\n\n</p></details>\n\n\n<details><summary>Configuring group size and grain size: (click to expand)</summary><p>\n\nThis advanced option is for GPU devices only. The script `scripts/find_optimal.py` will search a programmable group and grain space in order to find the optimal configuration for the particular PDFs.  This should be run after an example has been developed and tested.  Please look at `scripts/find_optimal.py` to see how to formulate a particular script.  Depending on the searchable space, this can take hours to days to compute.\nThe script will loop over the space and configure each parameter, then recompile and run the example a number of times.  A spreadsheet is calculated to help notice patterns, and the fastest version is printed to the user.\n\n</p></details>\n\n\n## Acknowledgement\n\nGooFit's development is supported by the National Science Foundation under grant number [1414736]\nand was developed under grant number [1005530].\nAny opinions, findings, and conclusions or recommendations expressed in this material are those of the developers\nand do not necessarily reflect the views of the National Science Foundation.\nIn addition, we thank the nVidia GPU Grant Program for donating hardware used in developing this framework.\n\nGooFit is available under the BSD license, except for the Landau distribution & MINUIT code. You must remove these\nto get a permissive version of GooFit.\n\n[actions-badge]:     https://github.com/GooFit/GooFit/workflows/CI/badge.svg\n[actions-link]:      https://github.com/GooFit/GooFit/actions\n[DOI-badge]:         https://zenodo.org/badge/9017446.svg\n[DOI-link]:          https://zenodo.org/badge/latestdoi/9017446\n[API documentation]: https://GooFit.github.io/GooFit\n[travis-badge]:      https://travis-ci.org/GooFit/GooFit.svg?branch=master\n[travis-link]:       https://travis-ci.org/GooFit/GooFit\n[gitter-badge]:      https://badges.gitter.im/GooFit/GooFit.svg\n[gitter-link]:       https://gitter.im/GooFit/Lobby\n[license-badge]:     https://img.shields.io/badge/License-BSD-blue.svg\n[1005530]:           https://nsf.gov/awardsearch/showAward?AWD_ID=1005530\n[1414736]:           https://nsf.gov/awardsearch/showAward?AWD_ID=1414736\n[CUDA_SELECT_NVCC_ARCH_FLAGS]: https://cmake.org/cmake/help/v3.7/module/FindCUDA.html\n[Plumbum]:           https://plumbum.readthedocs.io/en/latest/\n[FindBoost]:         https://cmake.org/cmake/help/v3.7/module/FindBoost.html\n[CLI11]:             https://github.com/CLIUtils/CLI11\n[pybind11]:          http://pybind11.readthedocs.io/en/master\n[ROOT]:              https://root.cern.ch\n[Tutorials]:         https://goofit.gitlab.io/Goo2Torial\n[pypi-status]:       https://img.shields.io/pypi/v/goofit.svg?logo=PyPI&logoColor=white\n[pypi-link]:         https://pypi.python.org/pypi/goofit/\n[cf-status]:         https://img.shields.io/conda/vn/conda-forge/goofit.svg?logo=Conda-Forge&logoColor=white\n[cf-link]:           https://github.com/conda-forge/goofit-split-feedstock\n[releases-badge]:    https://img.shields.io/github/release/GooFit/GooFit.svg\n[releases-link]:     https://github.com/GooFit/GooFit/releases\n[sk-badge]:          https://scikit-hep.org/assets/images/Scikit--HEP-Affiliated-blue.svg\n",126,physics,Cuda,8,C++,Cuda,CMake,Python,Shell,Makefile,Jupyter Notebook,C,,,,,,,,,,,,,,,,,,,,,280,22,243,15,71,26,109,102794,41,96,73,23,21748d7fa5bfc878c110a6c845d07bddc68d0320,chore: ruff moved to astral-sh (#373),2024-05-22T06:53:43Z,Henry Schreiner,hschrein@cern.ch,henryiii,Version 2.3.0,"## What's Changed\r\n* fix: Add sdist to .github by @henryiii in https://github.com/GooFit/GooFit/pull/233\r\n* fix: Mhilton three gauss resolution by @marthaisabelhilton in https://github.com/GooFit/GooFit/pull/218\r\n* chore: CMake 3.9+ now required by @danielsibemol in https://github.com/GooFit/GooFit/pull/240\r\n* feat: K Matrix Resonance by @marthaisabelhilton in https://github.com/GooFit/GooFit/pull/209\r\n* chore: Formatting with pre-commit by @henryiii in https://github.com/GooFit/GooFit/pull/241\r\n* fix: Set FE by @henryiii in https://github.com/GooFit/GooFit/pull/243\r\n* chore: updates by @henryiii in https://github.com/GooFit/GooFit/pull/252\r\n* chore: update pybind11 to 2.6.0 by @henryiii in https://github.com/GooFit/GooFit/pull/255\r\n* feat: Daniel amp3 body generation by @danielsibemol in https://github.com/GooFit/GooFit/pull/254\r\n* fixing bug in Resonance.cu and SpecialResonanceCalculator.cu by @danielsibemol in https://github.com/GooFit/GooFit/pull/257\r\n* fix: support latest versions of some deps by @henryiii in https://github.com/GooFit/GooFit/pull/266\r\n* fix: rbw by @marthaisabelhilton in https://github.com/GooFit/GooFit/pull/260\r\n* fix: incrementing in Mapped PDF | fix parameter index in LASS function | fix Amp3Body normalisation by @FlorianReiss in https://github.com/GooFit/GooFit/pull/259\r\n* ci: display test output by @henryiii in https://github.com/GooFit/GooFit/pull/269\r\n* ci(fix): ROOT is now built in 14 mode by @henryiii in https://github.com/GooFit/GooFit/pull/275\r\n* chore: bump pybind11 to 2.7.1 by @henryiii in https://github.com/GooFit/GooFit/pull/273\r\n* chore: bump to scikit-build 0.12 by @henryiii in https://github.com/GooFit/GooFit/pull/271\r\n* chore: use pip clang format by @henryiii in https://github.com/GooFit/GooFit/pull/276\r\n* fix: drop CompilerFeatures (unused, removed from CMake 3.20+) by @henryiii in https://github.com/GooFit/GooFit/pull/277\r\n* chore: bump to cli11 2.1.1 by @henryiii in https://github.com/GooFit/GooFit/pull/274\r\n* feat: adding rho-omega mixing lineshape. by @marthaisabelhilton in https://github.com/GooFit/GooFit/pull/278\r\n* fix: correct offsets used in Amp3Body for cached values by @FlorianReiss in https://github.com/GooFit/GooFit/pull/264\r\n* chore: use pre-commit/mirrors-clang-format instead by @henryiii in https://github.com/GooFit/GooFit/pull/284\r\n* style: add more pre-commit checks by @henryiii in https://github.com/GooFit/GooFit/pull/292\r\n* refactor: nicer PC impl by @henryiii in https://github.com/GooFit/GooFit/pull/291\r\n* chore: bump dependencies by @henryiii in https://github.com/GooFit/GooFit/pull/293\r\n* fix: convolutions fix by @henryiii in https://github.com/GooFit/GooFit/pull/294\r\n* feat: add a few missing methods by @henryiii in https://github.com/GooFit/GooFit/pull/296\r\n* chore: use scikit-build 0.13 by @henryiii in https://github.com/GooFit/GooFit/pull/299\r\n* feat: PdfBase::status method - tool for debugging by @danielsibemol in https://github.com/GooFit/GooFit/pull/298\r\n* fix: Martha fix cpv by @marthaisabelhilton in https://github.com/GooFit/GooFit/pull/279\r\n* fix: Pappenheimer: Amp4Body_TD normalisation by @cpappenheimer in https://github.com/GooFit/GooFit/pull/245\r\n* fix: latest git, avoid defaulting to submodule checkout by @henryiii in https://github.com/GooFit/GooFit/pull/314\r\n* feat(python): nicer particle shape by @henryiii in https://github.com/GooFit/GooFit/pull/307\r\n* feat: cherry-pick FitManager improvements by @FlorianReiss in https://github.com/GooFit/GooFit/pull/313\r\n* refactor: Add templated determinant function and LU decomposition inverse to kMatrixUtils by @thboettc in https://github.com/GooFit/GooFit/pull/302\r\n* refactor: converting license by @henryiii in https://github.com/GooFit/GooFit/pull/315\r\n* chore: prepare for 2.3.0 by @henryiii in https://github.com/GooFit/GooFit/pull/316\r\n\r\n## New Contributors\r\n* @FlorianReiss made their first contribution in https://github.com/GooFit/GooFit/pull/259\r\n* @pre-commit-ci made their first contribution in https://github.com/GooFit/GooFit/pull/280\r\n* @cpappenheimer made their first contribution in https://github.com/GooFit/GooFit/pull/245\r\n* @thboettc made their first contribution in https://github.com/GooFit/GooFit/pull/302\r\n\r\n**Full Changelog**: https://github.com/GooFit/GooFit/compare/v2.2.3...v2.3.0",v2.3.0,Henry Schreiner,,henryiii,Other,GooFit,GooFit,14,cuda,omp,thrust,fitting,root-cern,gpu,gpu-computing,physics,,,,,,,,,,,,,/GooFit/GooFit,17,19,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/godot-jolt/godot-jolt,https://github.com/godot-jolt/godot-jolt,0,,,0,0,0,0,0,0,1,1,0,0,0,Godot Jolt is a Godot extension that integrates the Jolt physics engine,"<p align=""center"">\n  <a href=""https://github.com/godot-jolt/godot-jolt"">\n    <img alt=""Godot Jolt"" src=""docs/logo.svg"">\n  </a>\n</p>\n\nGodot Jolt is a native extension for the [Godot game engine][god] that allows you to use the [Jolt\nphysics engine][jlt] to power Godot's 3D physics.\n\nIt functions as a drop-in replacement for Godot Physics, by implementing the same nodes that you\nwould use normally, like `RigidBody3D` or `CharacterBody3D`.\n\n## Table of Contents\n\n- [What features are there?](#what-features-are-there)\n- [What about determinism?](#what-about-determinism)\n- [What's not supported?](#whats-not-supported)\n- [What else is different?](#what-else-is-different)\n- [What versions of Godot are supported?](#what-versions-of-godot-are-supported)\n- [What platforms are supported?](#what-platforms-are-supported)\n- [How do I get started?](#how-do-i-get-started)\n- [What settings are there?](#what-settings-are-there)\n- [How do I build from source?](#how-do-i-build-from-source)\n- [What's the license?](#whats-the-license)\n\n## What features are there?\n\nBetter performance, mainly, but also generally having a more stable simulation compared to Godot\nPhysics.\n\nThere are also (completely optional) substitute nodes available for all the joints, which line up\nbetter with the interface that Jolt offers than what the default joints do. This allows for things\nlike breakable joints, soft limits and the ability to override solver iterations per-joint.\n\n## What about determinism?\n\nWhile Jolt itself offers deterministic simulations, Godot Jolt is not able to make such guarantees.\nSimulations in Godot Jolt may look deterministic, and may even happen to be deterministic, but this\nshould not be relied upon if determinism is a hard requirement.\n\n## What's not supported?\n\n- `WorldBoundaryShape3D` is not supported\n- The physics server is not thread-safe (yet)\n- Memory usage is not reflected in Godot's performance monitors (yet)\n- Ray-casts do not support `face_index`\n- `SoftBody3D` does not support any interactions with `Area3D`\n\n## What else is different?\n\n- `Area3D` detecting static bodies is opt-in, at a potentially [heavy performance/memory cost][jst]\n- Joints only support soft limits through their substitutes (`JoltHingeJoint3D`, etc.)\n- Springs and linear motors are actually implemented in `Generic6DOFJoint3D`\n- Single-body joints will make `node_a` be the ""world node"" rather than `node_b`\n- Ray-casts using `hit_back_faces` will hit the back/inside of all shapes, not only concave ones\n- Ray-casts are not affected by the `backface_collision` property of `ConcavePolygonShape3D`\n- Shape-casts should be more accurate, but their cost also scale with the cast distance\n- Shape margins are used, but are treated as an upper bound and scale with the shape's extents\n- Manipulating a body's shape(s) after it has entered a scene tree can be costly\n- Contact impulses are estimations and won't be accurate when colliding with multiple bodies\n- Contact reporting for kinematic bodies is partially opt-in, at a potentially [heavy\n  performance/memory cost][jst]\n\nAlso consider this note from Jolt's [documentation][jdc]:\n\n> In order for the simulation to be accurate, dynamic objects should be in the order of 0.1 to 10 m\n> long, have speeds in the order of 0 to 500 m/s and have gravity in the order of 0 to 10 m/s^2.\n> Static object should be in the order of 0.1 to 2000 m long.\n\n## What versions of Godot are supported?\n\nCurrently the **only** supported version is **Godot 4.3** (including 4.3.x).\n\n## What platforms are supported?\n\n- Windows (x86-64, x86)\n- Linux (x86-64, x86)\n- macOS (x86-64 + Apple Silicon)\n- iOS\n- Android (ARM64, ARM32, x86-64, x86)\n\nNote that Linux support is limited to glibc 2.31 or newer, which for Ubuntu means 20.04 (Focal\nFossa) or newer.\n\n## How do I get started?\n\n1. Download it from [GitHub][rls] or from [Godot Asset Library][ast]\n2. Extract the files to your project directory\n3. Start (or restart) Godot\n4. Open your project settings\n5. Make sure ""Advanced Settings"" is enabled\n6. Go to ""Physics"" and then ""3D""\n7. Change ""Physics Engine"" to ""JoltPhysics3D""\n8. Restart Godot\n\n## What settings are there?\n\nSee [`docs/settings.md`][set] for information about the project settings available in Godot Jolt.\n\n## How do I build from source?\n\nSee [`docs/building.md`][bld] for information about how to build Godot Jolt from source.\n\n## What's the license?\n\nGodot Jolt is distributed under the MIT license. See [`LICENSE.txt`][lic] for more details and\n[`THIRDPARTY.txt`][trd] for third-party licenses.\n\n[god]: https://godotengine.org/\n[jlt]: https://github.com/jrouwe/JoltPhysics\n[jst]: docs/settings.md#jolt-3d\n[jdc]: https://jrouwe.github.io/JoltPhysics/\n[rls]: https://github.com/godot-jolt/godot-jolt/releases/latest\n[ast]: https://godotengine.org/asset-library/asset/1918\n[set]: docs/settings.md\n[bld]: docs/building.md\n[lic]: LICENSE.txt\n[trd]: THIRDPARTY.txt\n",1902,physics,C++,3,CMake,PowerShell,C++,,,,,,,,,,,,,,,,,,,,,,,,,,588,16,571,1,2,3,0,3219,65,232,213,19,611e76d8addcf9f5a3a186431918c99ef04d403a,Added removal of pending overlaps in `JoltAreaImpl3D::area_exited`,2024-07-18T10:06:49Z,Mikael Hermansson,mikael@hermansson.io,mihe,0.12.0-stable,"This release only supports **Godot 4.2** (including 4.2.x).\r\n\r\n## Changelog\r\n\r\nThese are the notable changes that have been made since [0.11.0-stable](https://github.com/godot-jolt/godot-jolt/releases/tag/v0.11.0-stable) was released. You can also find a list of all the commits [here](https://github.com/godot-jolt/godot-jolt/compare/v0.11.0-stable...v0.12.0-stable).\r\n\r\nAny breaking changes are denoted with ⚠️.\r\n\r\n### Changed\r\n\r\n- ⚠️ Changed so that single-body joints now implicitly sets `node_a` to be the ""world node"" rather than `node_b`. This diverges from how Godot Physics behaves, but matches how Bullet behaves in Godot 3, and yields more intuitive outcomes for the 6DOF joints.\r\n- ⚠️ Changed `Generic6DOFJoint3D` and `ConeTwistJointImpl3D`, as well as their substitute joints, to use pyramid-shaped angular limits instead of cone-shaped limits, to better match Godot Physics.\r\n- ⚠️ Reversed the direction of the `equilibrium_point` properties for `Generic6DOFJoint3D` and `JoltGeneric6DOFJoint3D`, to match the direction of the angular limits.\r\n- ⚠️ Changed the rotation order of the `equilibrium_point` properties for `Generic6DOFJoint3D` and `JoltGeneric6DOFJoint3D`, from ZXY to XYZ, to match the rotation order of the angular limits.\r\n- Mirrored the way in which linear limits are visualized for `JoltSliderJoint3D` and `JoltGeneric6DOFJoint3D`.\r\n\r\n### Added\r\n\r\n- Added new project setting, ""World Node"", for controlling which of the two nodes in a single-body joint becomes the ""world node"" when omitting one of the nodes. This allows for reverting back to the behavior of Godot Physics if needed, effectively undoing the breaking change mentioned above.\r\n- Added new project setting, ""Report All Kinematic Contacts"", for allowing `RigidBody3D` frozen with `FREEZE_MODE_KINEMATIC` to report contacts/collisions with other kinematic/static bodies, at a potentially heavy performance/memory cost.\r\n- Added support for using NaN to indicate holes in `HeightMapShape3D`.\r\n- Added support for holes in a non-square `HeightMapShape3D`.\r\n\r\n### Fixed\r\n\r\n- ⚠️ Fixed issue with non-square `HeightMapShape3D` not using back-face collision.\r\n- Fixed issue where contact shape indices would sometimes always be the same index across all contacts with a particular body.\r\n- Fixed runtime crash when setting the `max_contacts_reported` property to a lower value.\r\n- Fixed issue where `Generic6DOFJoint3D` and `JoltGeneric6DOFJoint3D` would yield odd limit shapes when using both linear and angular asymmetrical limits.\r\n- Fixed issue where the equilibrium point for `Generic6DOFJoint3D` and `JoltGeneric6DOFJoint3D` would be moved when using asymmetrical limits.\r\n- Fixed crash that could occur under rare circumstances when shutting down the editor after having added/removed collision shapes.\r\n- Fixed issue where a `RigidBody3D` with locked axes colliding with a `StaticBody3D` (or another frozen `RigidBody3D` using `FREEZE_MODE_STATIC`) would result in NaNs.\r\n- Fixed issue where `HingeJoint3D` and `JoltHingeJoint3D` would sometimes dull forces applied to either of its bodies when at either of its limits.\r\n- Fixed issue with iOS `Info.plist` missing the `MinimumOSVersion` key.",v0.12.0-stable,Mikael Hermansson,,mihe,MIT License,godot-jolt,godot-jolt,29,game-development,gdextension,godot,godot-engine,godotengine,physics,physics-simulation,gamedev,,,,,,,,,,,,,/godot-jolt/godot-jolt,29,27,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/gnustep/libs-gui,https://github.com/gnustep/libs-gui,0,,,0,0,0,0,0,0,1,0,0,0,0,The GNUstep gui library is a library of graphical user interface classes written completely in the Objective-C language; the classes are based upon Apple's Cocoa framework (which came from the OpenStep specification). *** Larger patches require copyright assignment to FSF.  please file bugs here. ***,"GNUstep GUI Library\n====================\n\n[![CI](https://github.com/gnustep/libs-gui/actions/workflows/main.yml/badge.svg)](https://github.com/gnustep/libs-gui/actions/workflows/main.yml?query=branch%3Amaster)\n\nThe GNUstep gui library is a library of graphical user interface classes\nwritten completely in the Objective-C language; the classes are based\nupon Apple's Cocoa framwork (which came from the OpenStep\nspecification).  These classes include graphical objects such as\nbuttons, text fields, popup lists, browser lists, and windows; there are\nalso many associated classes for handling events, colors, fonts,\npasteboards and images.\n\nInitial reading\n---------------\nThe file [ANNOUNCE](ANNOUNCE) contains a very brief overview of the library.\nIt also tells you where to get the most recent version.\n     \nThe file [NEWS](NEWS) has the library's feature history.\n\nThe files [INSTALL](INSTALL) or [GNUstep-HOWTO][1] (from the web site)\ngives instructions for installing the library.\n\n[1]: http://www.gnustep.org/resources/documentation/User/GNUstep/gnustep-howto.pdf\n\nLicense\n-------\n\nThe GNUstep libraries and library resources are covered under the GNU\nLesser Public License.  This means you can use these libraries in any\nprogram (even non-free programs).  If you distribute the libraries along\nwith your program, you must make the improvements you have made to the\nlibraries freely available.  You should read the COPYING.LIB file for\nmore information.  All files in the 'Source', 'Headers', directories and\nsubdirectories under this are covered under the LGPL.\n\nGNUstep tools, test programs, and other files are covered under the\nGNU Public License.  This means if you make changes to these programs,\nyou cannot charge a fee, other than distribution fees, for others to use\nthe program.  All files in this package EXCEPT files in the 'Tools'\ndirectories and subdirectories under this are covered under the LGPL.\n\nHow can you help?\n-----------------\n\nGive us feedback! Tell us what you like; tell us what you think could be better.\n\nPlease log bug reports on the [GitHub issues page][2].\n\n[2]: https://github.com/gnustep/libs-gui/issues\n\nHappy hacking!\n\nCopyright (C) 2005, 2024 Free Software Foundation\n\nCopying and distribution of this file, with or without modification,\nare permitted in any medium without royalty provided the copyright\nnotice and this notice are preserved.\n",273,graphics,Objective-C,8,Makefile,Objective-C,C,PostScript,Shell,Yacc,M4,Roff,,,,,,,,,,,,,,,,,,,,,196,29,157,10,88,43,0,22443,99,84,50,34,b32f21e54952f09b0beb4d1cdac49ce042b64d70,Merge branch 'master' of https://github.com/gnustep/libs-gui,2024-07-18T11:17:14Z,Riccardo Mottola,rm@gnu.org,rmottola,Release 0.31.1,"1 Announcement\r\n**************\r\n\r\nThis is version 0.31.1 of the GNUstep GUI library ('gnustep-gui').\r\n\r\n1.1 What is the GNUstep GUI Library?\r\n====================================\r\n\r\nIt is a library of graphical user interface classes written completely\r\nin the Objective-C language; the classes are based upon Apple's Cocoa\r\nframework.  The library has been enhanced in a number of ways to take\r\nadvantage of the GNU system.  These classes include graphical objects\r\nsuch as buttons, text fields, popup lists, browser lists, and windows;\r\nthere are also many associated classes for handling events, colors,\r\nfonts, pasteboards and images.\r\n\r\n   The GNUstep GUI Library is designed in two parts.  The first part is\r\nthe front-end component which is independent of platform and display\r\nsystem.  This front-end is combined with a back-end component which\r\nhandles all of the display system dependent such as specific calls to\r\nX/Windows.  This design allows the GNUstep applications to have the\r\n""look and feel"" of the underlying display system without any changes to\r\nthe application, and the library can be easily ported to other display\r\nsystems.\r\n\r\n   The GNUstep GUI Library requires the GNU Objective-C compiler, the\r\nGNUstep Base Library, the TIFF Graphics library, Independent JPEG\r\nGroup's libjpeg library, and a back-end component from the GNUstep\r\n'Back' library.\r\n\r\n   Additional functionality may be enabled by installing additional\r\nlibraries.  For example, to build the Cairo backend in the GNUstep Back\r\nlibrary, you will need to install Cairo.\r\n\r\n1.2 Noteworthy changes in version '0.31.1'\r\n==========================================\r\n\r\nThis is a bugfix release\r\n\r\n   * Fix bug decoding menu items (breaking archive)\r\n   * Remove use of deprecated lock from base library\r\n\r\n1.3 Where can you get it? How can you compile it?\r\n=================================================\r\n\r\nThe gnustep-gui-0.31.1.tar.gz distribution file has been placed at\r\n<ftp://ftp.gnustep.org/pub/gnustep/core>.\r\n\r\n   It is accompanied by gnustep-gui-0.31.1.tar.gz.sig, a PGP signature\r\nwhich you can validate by putting both files in the same directory and\r\nusing:\r\n\r\n     gpg --verify gnustep-gui-0.31.1.tar.gz.sig\r\n\r\n   Signature has been created using the key with the following\r\nfingerprint:\r\n\r\n     83AA E47C E829 A414 6EF8  3420 CA86 8D4C 9914 9679\r\n\r\n   Read the INSTALL file or the GNUstep-HOWTO for installation\r\ninstructions.\r\n\r\n1.4 Where do I send bug reports?\r\n================================\r\n\r\nPlease log bug reports on the GNUstep project page\r\n<http://savannah.gnu.org/bugs/?group=gnustep> or send bug reports to\r\n<bug-gnustep@gnu.org>.\r\n\r\n1.5 Obtaining GNU Software\r\n==========================\r\n\r\nCheck out the GNUstep web site.  (<http://www.gnustep.org/>), and the\r\nGNU web site.  (<http://www.gnu.org/>)\r\n\r\n",gui-0_31_1,rfm,,rfm,GNU General Public License v3.0,libs-gui,gnustep,9,objective-c,gui,gnu,graphics,widgets,,,,,,,,,,,,,,,,/gnustep/libs-gui,111,29,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/gnu-octave/symbolic,https://github.com/gnu-octave/symbolic,0,,,0,0,0,0,0,0,1,0,0,0,0,A Symbolic Package for Octave using SymPy,"Symbolic Package for GNU Octave\n===============================\n\nAn implementation of a symbolic toolbox using SymPy.\n\nhttps://octave.sourceforge.io/symbolic\n\nhttps://github.com/gnu-octave/symbolic\n\n\n\nGoals\n-----\n\nFeature parity with the other symbolic toolboxes.\n\n![Screenshot 1](/screenshot.png)\n\n![Screenshot 2](/screenshot-install.png)\n\n\n\nHow to Install\n--------------\n\n1.  The dependencies are Octave, Python, and SymPy.  Consult the SymPy\n    website for details on how to install SymPy.\n\n2.  Start Octave.\n\n3.  At Octave prompt type `pkg install -forge symbolic`.\n\n4.  At Octave prompt, type `pkg load symbolic`.\n\n5.  At Octave prompt, type `syms x`, then `f = (sin(x/2))^3`,\n    `diff(f, x)`, etc.\n\n\nHow to install on Ubuntu\n-------------------------\n\n1.  Install the dependencies with\n    `sudo apt install octave python3-sympy`.\n2.  Follow steps 2--5 above.\n\n\nHow to Install on Windows\n-------------------------\n\n1.  Get [Octave](http://www.octave.org) for Windows.\n\n2.  At the Octave prompt, type `pkg install -forge symbolic`.\n\n3.  At the Octave prompt, type `pkg load symbolic`.\n\n4.  At the Octave prompt, type `syms x`, then `f = (sin(x/2))^3`,\n    `diff(f, x)`, etc.\n\nIf you encounter any difficulties (even minor ones) please read and\nif possible help us improve the\n[wiki page on Windows Installation](https://github.com/gnu-octave/symbolic/wiki/Notes-on-Windows-installation).\n\n\n\nHow to Install on Matlab\n------------------------\n\nAlthough this package is designed for GNU Octave, it will work with\nMatlab.  Currently only the slower system()-based communication is\navailable.\n\n1.  Download the latest release, e.g., `octsympy-matlab-2.7.0.tar.gz`.\n\n2.  Unzip it somewhere and add it to your Matlab Path.\n\nThe .m files for Matlab have been reformatted for Matlab comment\nconventions, but are otherwise the same as the Octave source.\n\n\nHow to Help\n-----------\n\nWe have a list of things to work on tagged [help\nwanted](https://github.com/gnu-octave/symbolic/issues?q=is:open+is:issue+label:""help+wanted"").\nSome of these should be quite easy to fix and would be a great way to\nget involved.  Come join us!\n\nHow to hack on the code:\n\n1.  Clone the repo with git (preferred, but you can use the ""Download\n    ZIP"" instead if you want).\n\n2.  Run Octave in the `inst/` directory.  It should be safe\n    to do this even if you have the released version of the package\n    installed (but not loaded).\n\n\n\nImplementation\n--------------\n\nPython code is generated to do the actual work.  Each sym object keeps\na text field for display purposes and a string (a SymPy `srepr`).  The\nobjects are communicated between Python and Octave by passing the\nsrepr string back-and-forth.  Currently pure m-file (and Python)\nimplementation, no code to be compiled.\n\n\n\nRelated Projects\n----------------\n\n  * There was a previous ""symbolic"" package in Octave Forge based on\n    GiNaC.  Its history has now been merged into this project.\n\n  * [""SymPy CAS"" by Jonathan Lister](http://www.mathworks.com/matlabcentral/fileexchange/42787-sympy-cas-in-matlab).\n    Calls SymPy commands using system().\n",149,mathematics,MATLAB,5,MATLAB,Scilab,Python,Shell,Makefile,,,,,,,,,,,,,,,,,,,,,,,,570,55,498,17,13,39,0,4792,36,735,631,104,931a70ce97348e4d5c478fbdc5209eb154fb39b8,Merge pull request #1302 from mmuetzel/CI,2024-07-01T22:39:45Z,Colin B. Macdonald,cbm@m.fsf.org,cbm755,,"See the [NEWS file](https://github.com/gnu-octave/symbolic/blob/v3.2.1/NEWS) for a list of changes.\r\n\r\nOctave users can usually install with `pkg install -forge symbolic` (from within Octave, no need to download anything from this page).  You will need Python and SymPy installed.\r\n\r\n## File hashes\r\n\r\n### Octave package: symbolic-3.2.1.tar.gz\r\n\r\nmd5sum: 02e0cb39d309a3a2eaf78cd85f178cf6\r\nsha256sum: d1a6ef4d12c48fc4412ceec380f398a6cd5180e518c131ba12683e9eb8f75460\r\n\r\n### Matlab packages\r\n\r\nMay return in a future release.",v3.2.1,Colin B. Macdonald,,cbm755,GNU General Public License v3.0,symbolic,gnu-octave,26,octave,symbolic,computer-algebra,mathematics,,,,,,,,,,,,,,,,,/gnu-octave/symbolic,28,16,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/gnudatalanguage/gdl,https://github.com/gnudatalanguage/gdl,0,,,0,0,0,0,0,0,1,0,0,0,0,GDL - GNU Data Language,"[![Build status](https://github.com/gnudatalanguage/gdl/workflows/build/badge.svg)](https://github.com/gnudatalanguage/gdl/actions)\n[![Coverage Status](https://img.shields.io/codecov/c/github/gnudatalanguage/gdl/master.svg)](https://codecov.io/github/gnudatalanguage/gdl?branch=master)\n[![License: GPL v2](https://img.shields.io/badge/License-GPL%20v2-blue.svg)](https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html)\n[![DOI](https://joss.theoj.org/papers/10.21105/joss.04633/status.svg)](https://doi.org/10.21105/joss.04633)\n\nGDL - GNU Data Language\n=======================\n\nGDL is a free/libre/open source incremental compiler compatible with IDL (Interactive Data Language) and to some extent with PV-WAVE. \nTogether with its library routines it serves as a tool for data analysis and visualization in such disciplines \nas astronomy, geosciences and medical imaging. \nGDL development had been started by **Marc Schellens** back in early noughties and has since continued \nwith help of a team of maintainers, developers, packagers and thanks to feedback from users.\n\nIDL is a registered trademark of [Harris Geospatial Solutions](https://www.harrisgeospatial.com).\nPV-WAVE is a product of [Rogue Wave Software](https://www.roguewave.com).\n\nOverview\n--------\n\nGDL is a domain-specific programming language and a data analysis environment.\nAs a language, it is dynamically-typed, array-oriented, vectorised and has \nobject-oriented programming capabilities. \nGDL library routines handle numerical calculations, data visualisation, signal/image processing, \ninteraction with host OS and data input/output. \nGDL supports several data formats such as netCDF, HDF4, HDF5, GRIB, PNG, TIFF, DICOM, etc. \nGraphical output is handled by X11, PostScript, SVG or z-buffer terminals, the last one allowing \noutput graphics (plots) to be saved in a variety of raster graphics formats. \nGDL features integrated debugging facilities. \nThe built-in widget functionality enables development of GUI-based software.\nGDL has also a Python bridge (Python code can be called from GDL; GDL can be compiled as a Python module). \nDevelopment and maintenance of GDL is carried out targeting Linux, BSD, OSX and Windows (MinGW, Cygwin).\n\nGDL is invoked just by typing `gdl` but see `gdl -h` as it has a number of commandline options.\nGDL may be known as `gnudl` or `gnudatalanguage` on some operating systems.\n\nOther open-source numerical data analysis tools similar to GDL include\n[SciPy](http://www.scipy.org/),\n[GNU Octave](http://www.gnu.org/software/octave/),\n[Scilab](http://www.scilab.org/),\n[PDL](http://pdl.perl.org/),\n[NCL](http://www.ncl.ucar.edu/),\n[R](http://www.r-project.org/),\n[Yorick](http://yorick.sourceforge.net/).\n\nGetting GDL\n-------------------------------------\n\nSee:\n- [Cloning GDL](https://github.com/gnudatalanguage/gdl/wiki/Cloning-gnudatalanguage-gdl) (new!)\n- [GDL on Linux](https://github.com/gnudatalanguage/gdl/wiki/GDL-on-Linux)\n- [GDL on OSX](https://github.com/gnudatalanguage/gdl/wiki/GDL-on-OSX)\n- [GDL on BSD](https://github.com/gnudatalanguage/gdl/wiki/GDL-on-BSD)\n- [GDL on Windows](https://github.com/gnudatalanguage/gdl/wiki/GDL-on-Windows)\n\nFind specific information on GDL\n-------------------------------------\n- Browse the [WIKI](https://github.com/gnudatalanguage/gdl/wiki)\n- Be aware of current problems/limitations: Check the [issues](https://github.com/gnudatalanguage/gdl/wiki/Known-issues).\n\nDependencies \n-------------------------------------\n\nPackaged versions of GDL are available for several Linux distributions, BSD and Mac OS X. \nPlease note that several features of GDL depend on compile-time configuration, and might not \nbe available in pre-built or pre-configured packages. \n\nGDL has numerous dependencies, most of the optional but highly recommended if you want it to be areally useful tool.\n- [readline](https://tiswww.cwru.edu/php/chet/readline/rltop.html) mandatory. For easy command line editing, recalling, history. \n- [\[n\]curses](https://www.gnu.org/software/ncurses/) mandatory. Terminal management.\n- [zlib](https://zlib.net/) mandatory. compressed file access.\n- [GSL](https://www.gnu.org/software/gsl/) mandatory, for many math functions.\n- [OpenMP](http://www.openmp.org/) optional, but speed will suffer if not present\n- [Magick++](https://imagemagick.org/) / [GraphicsMagick](http://graphicsmagick.org/) optional, but don't you want to read/write many image formats?\n- [wxWidgets](https://www.wxwidgets.org/) mandatory unless you do not want graphic outputs and widgets?\n- [Xlib/X11](https://sourceforge.net/projects/libx11/) not used unless you explictly ask for it (replaced by wxWidgets for sake of compatibility on Windows, linux and MacOSX. \n- [netCDF](https://www.unidata.ucar.edu/software/netcdf/) optional, but useful for reading this kind of data.\n- [HDF4](https://support.hdfgroup.org/products/hdf4/)  optional, but useful for reading this kind of data.\n- [HDF5](https://support.hdfgroup.org/HDF5/)   optional, but useful for reading this kind of data.\n- [FFTW](http://www.fftw.org/) optional, but don't you need a fast fft at times?\n- [PROJ](http://proj.org/) optional but forget about mapping capabilities if absent.\n- [Shapelib](http://shapelib.maptools.org/) optional but forget about mapping capabilities if absent.\n- [Expat](https://libexpat.github.io/) optional but helps implement IDLffXMLSAX parser objects. \n- [MPI](https://en.wikipedia.org/wiki/Message_Passing_Interface) optional but provides clustering facilities.\n- [Python](https://www.python.org/)/[NumPy](http://www.numpy.org/) optional but add python bridge and jupyter notebook.\n- [udunits](https://www.unidata.ucar.edu/software/udunits/) optional, units conversion\n- [Eigen](https://eigen.tuxfamily.org/) optional but provides inordinate speed enhancements...\n- [ecCodes](https://confluence.ecmwf.int/display/ECC/ecCodes+Home) optional, for GRIB support.\n- [GLPK](https://www.gnu.org/software/glpk/) optional, provides the SIMPLEX command.\n\nBesides, for optimal use (speed mainly), GDL incorporates slightly edited code of\n- [dSFMT](http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/SFMT) as our parallel random Generator.\n- [delaunator](https://github.com/mapbox/delaunator) as our new hyperfast triangulation.\n- [ANTLR3](https://www.antlr3.org/) as interpretor.\n- [Median Filtering (S. Perreault)](http://nomis80.org/ctmf.html )\n- [Median Filtering (J. Suomela)](http://users.ics.aalto.fi/suomela)\n- [Radix Sorting](https://github.com/Pierre-Terdiman/RadixRedux) (we have written all variants up to doubles).\n- [whereami](https://github.com/gpakosz/whereami) \n\nBuild-time dependencies\n-----------------------\n\nBuild and test automation is carried out using [CMake](http://cmake.org/).\n\nGDL interpreter has been developed using [ANTLR v2](http://www.antlr2.org) but unless you want \nto change the grammar (\*.g files) you don't need ANTLR. \nAll relevant ANTLR files are included in the source tree.\n\nSupport, feedback and contributions\n-----------------------------------\n\nYour comments are welcome! Let us know what you use GDL for. Or if you don't, why not. \nWhich functionality are you missing/would appreciate most for coming versions. \nPlease use the github issue-tracking system to report \nbugs, complaints, suggestions and comments.\n\nCode enhancements in the form of pull requests are very welcome!\nNote that contributions can be made in C++, IDL/GDL or Python, as well as\nby providing enhancements and extensions of the README files, diagnostic messages, etc.\n\nAmong the major challenges GDL development is facing currently, there are:\n- [enhancing test coverage](https://codecov.io/github/gnudatalanguage/gdl?branch=master) by writing test programs in GDL\n- streamlining development and maintenance of GDL reference docs and examples (using the [Jupyter kernel](https://github.com/gnudatalanguage/idl_kernel)?)\n- bringing in into the team the needed know-how to address the [backlog of ANTLR-related issues](https://github.com/gnudatalanguage/gdl/labels/antlr)\n- increasing presence within and interoperability with the Python ecosystem, including adding support for Python 3 (calling GDL from Python 2 and calling Python 2 from GDL is already implemented!)\n\nHelp welcome!\n\nInformation resources\n---------------------\nGDL does not maintain a proper documentation: as GDL is aimed as a drop-in replacement for IDL,\nresources for IDL constitute the valuable sources of information for GDL users as well. GDL MUST behave (at least) as IDL, and any discrepancy should be reported by opening an issue.\nConversely, the GDL issues and discussion forum on GitHub are not the good place for beginners to ask for advice on how to use IDL (or GDL). Use the forum below.\nIDL freely available resources include:\n- the [official IDL documentation](https://www.harrisgeospatial.com/docs/)\n- the [idl-pvwave Google Group](https://groups.google.com/forum/#!forum/idl-pvwave)\n- the [comp.lang.idl-pvwave usenet group archives](http://www.idlcoyote.com/comp.lang.idl-pvwave/) (dating back to 1991!)\n- Wikipedia article on [IDL](https://en.wikipedia.org/wiki/IDL_\(programming_language\)) and references therein\n- websites of IDL gurus including [David Fanning](http://www.idlcoyote.com/) and [Michael Galloy](http://michaelgalloy.com/)\n- numerous [tutorials and lecture notes](https://www.google.com/search?q=interactive+data+language) introducing IDL\n- old, used, but still very valid IDL booklets can be found in various libraries, second-hand bookstores etc.\n\nThere are several open source packages compatible or interoperable with GDL, including:\n- the [MPFIT](https://pages.physics.wisc.edu/~craigm/idl/cmpfit.html) curve fitting library written in IDL (also available as a [Debian package](https://packages.debian.org/gdl-mpfit))\n- the [IDL Astronomy User's Library](https://github.com/wlandsman/IDLAstro) written in IDL (also available as a [Debian package](https://packages.debian.org/gdl-idlastro))\n- the [Coyote](https://www.idlcoyote.com) library of IDL-written utilities (also available as a [Debian package](https://packages.debian.org/gdl-coyote))\n- the [TeXtoIDL](http://physics.mnstate.edu/craig/textoidl/) package \n- the [gdlde](https://github.com/gnudatalanguage/gdlde) IDE\n- the [IDL/GDL Jupyter kernel](https://github.com/gnudatalanguage/idl_kernel)\n- the [IDLWAVE Emacs mode](https://www.gnu.org/software/emacs/manual/html_mono/idlwave.html)\n- IDL [syntax highlighting module for Vim](https://github.com/vim/vim/blob/master/runtime/syntax/idlang.vim)\n- the [SingleCompile extension for Vim](https://github.com/vim-scripts/SingleCompile)\n\nAlain Coulais maintains the [GDL-announces mailing list](https://sympa.obspm.fr/wws/info/gdl-announces).\n\nThere have been quite some [mentions of GDL in scientific literature](https://scholar.google.com/scholar?q=""gnu+data+language"") \nwhich also provide example use cases.\nThe Coulais et al. papers from the ADASS conferences are the best way to cite GDL as of now.\n\nAcknowledgements\n----------------\n\nGDL development had been carried out at [SourceForge](http://sourceforge.net/) in years 2003-2018 - thank you!\n",270,astronomy,C++,13,CMake,Makefile,C++,Shell,GAP,C,IDL,Prolog,Python,Fortran,Roff,NSIS,TeX,,,,,,,,,,,,,,,,759,115,642,2,48,95,0,68894,61,1005,733,272,c154de16a72701a5ad2dd8026479525fee92ccbd,changes in shmmap (#1865),2024-07-18T16:22:45Z,Giloo,33936193+GillesDuvert@users.noreply.github.com,GillesDuvert,v1.0.6,GDL v1.0.6,v1.0.6,,,github-actions[bot],GNU General Public License v2.0,gdl,gnudatalanguage,12,astronomy,data-analysis,antlr,python,pv-wave,programming-language,geophysics,mapping,scientific-computing,scientific-visualization,plplot,gsl-library,plotting,netcdf,hdf5,hdf,dicom,fits-files,grib,eigen3,/gnudatalanguage/gdl,30,18,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/GMOD/jbrowse-components,https://github.com/GMOD/jbrowse-components,0,,,0,0,0,0,0,0,1,1,0,0,0,"Source code for JBrowse 2, a modern React-based genome browser","[![Build Status](https://img.shields.io/github/actions/workflow/status/GMOD/jbrowse-components/push.yml?branch=main&logo=github&style=for-the-badge)](https://github.com/GMOD/jbrowse-components/actions)\n[![Coverage Status](https://img.shields.io/codecov/c/github/GMOD/jbrowse-components/main.svg?logo=codecov&style=for-the-badge)](https://codecov.io/gh/GMOD/jbrowse-components/branch/main)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg?style=for-the-badge&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAyNTYgMjU2IiB2ZXJzaW9uPSIxLjEiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgeG1sbnM6eGxpbms9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkveGxpbmsiPjx0aXRsZT5Db250cmlidXRvciBDb3ZlbmFudCBMb2dvPC90aXRsZT48ZyBpZD0iQ2FudmFzIj48ZyBpZD0iR3JvdXAiPjxnIGlkPSJTdWJ0cmFjdCI+PHVzZSB4bGluazpocmVmPSIjcGF0aDBfZmlsbCIgZmlsbD0iIzVFMEQ3MyIvPjwvZz48ZyBpZD0iU3VidHJhY3QiPjx1c2UgeGxpbms6aHJlZj0iI3BhdGgxX2ZpbGwiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDU4IDI0KSIgZmlsbD0iIzVFMEQ3MyIvPjwvZz48L2c+PC9nPjxkZWZzPjxwYXRoIGlkPSJwYXRoMF9maWxsIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik0gMTgyLjc4NyAxMi4yODQ2QyAxNzMuMDA1IDkuNDk0MDggMTYyLjY3NyA4IDE1MiA4QyA5MC4xNDQxIDggNDAgNTguMTQ0MSA0MCAxMjBDIDQwIDE4MS44NTYgOTAuMTQ0MSAyMzIgMTUyIDIzMkMgMTg4LjQ2NCAyMzIgMjIwLjg1NyAyMTQuNTc1IDI0MS4zMDggMTg3LjU5OEMgMjE5Ljg3IDIyOC4yNzIgMTc3LjE3MyAyNTYgMTI4IDI1NkMgNTcuMzA3NSAyNTYgMCAxOTguNjkyIDAgMTI4QyAwIDU3LjMwNzUgNTcuMzA3NSAwIDEyOCAwQyAxNDcuNjA0IDAgMTY2LjE3OSA0LjQwNzA5IDE4Mi43ODcgMTIuMjg0NloiLz48cGF0aCBpZD0icGF0aDFfZmlsbCIgZmlsbC1ydWxlPSJldmVub2RkIiBkPSJNIDEzNy4wOSA5LjIxMzQyQyAxMjkuNzU0IDcuMTIwNTYgMTIyLjAwOCA2IDExNCA2QyA2Ny42MDgxIDYgMzAgNDMuNjA4MSAzMCA5MEMgMzAgMTM2LjM5MiA2Ny42MDgxIDE3NCAxMTQgMTc0QyAxNDEuMzQ4IDE3NCAxNjUuNjQzIDE2MC45MzEgMTgwLjk4MSAxNDAuNjk4QyAxNjQuOTAzIDE3MS4yMDQgMTMyLjg4IDE5MiA5NiAxOTJDIDQyLjk4MDcgMTkyIDAgMTQ5LjAxOSAwIDk2QyAwIDQyLjk4MDcgNDIuOTgwNyAwIDk2IDBDIDExMC43MDMgMCAxMjQuNjM0IDMuMzA1MzEgMTM3LjA5IDkuMjEzNDJaIi8+PC9kZWZzPjwvc3ZnPg==)](CODE_OF_CONDUCT.md)\n\n# jbrowse-components\n\nMonorepo using Lerna and Yarn workspaces containing many related packages for\nnext-generation JBrowse development.\n\nHomepage https://jbrowse.org/jb2\n\nDocs http://jbrowse.org/jb2/docs/\n\nFall 2023: New outreach! We created an ""office hours"" Google Calendar for anyone\nto schedule 1-on-1 meetings with the development team. Details below:\n\n- [Schedule 1-on-1 appointment](https://calendar.app.google/1AYZkNCQNmwdY2R26)\n\n## Pre-requisites\n\n- [git](https://git-scm.com/downloads)\n- [nodejs](https://nodejs.org/en/download/) (node 18 or greater)\n- [yarn](https://yarnpkg.com/en/docs/install)\n\nYou may need additional pre-requisites on certain versions of nodejs.\n\nOn macOS with homebrew:\n\n    brew install pkg-config cairo pango libpng jpeg giflib librsvg\n\nOn Ubuntu, with apt:\n\n    sudo apt install -y python make gcc libcairo2-dev libpango1.0-dev libjpeg-dev libgif-dev librsvg2-dev\n\n## Install (Linux/Mac)\n\nSimply clone the git repo and run yarn in the root repository\n\n```sh\ngit clone https://github.com/GMOD/jbrowse-components.git\ncd jbrowse-components\nyarn\n```\n\n## Install (Windows)\n\n```pwsh\n# Make sure you check out line-endings as-is by running\n# `git config --global core.autocrlf false`\n# Also, make sure symlinks are enabled by running\n# `git config --global core.symlinks true`.\n# You may also need to clone as an administrator for symlinks to work.\ngit clone -c core.symlinks=true https://github.com/GMOD/jbrowse-components.git\ncd .\jbrowse-components\\nyarn\n```\n\n## Quick start for developers\n\nYou can use these commands to help get started with your development environment\n\nFor running jbrowse-web\n\n```sh\ncd products/jbrowse-web\nyarn start\n```\n\nFor jbrowse-desktop, launch two tabs\n\n```sh\n# starts webpack dev server\ncd products/jbrowse-desktop\nyarn start\n\n# starts electron window\ncd products/jbrowse-desktop\nyarn electron\n```\n\nFor running e.g. jbrowse-react-linear-genome-view you can use storybook\n\n```sh\ncd products/jbrowse-react-linear-genome-view\nyarn storybook\n```\n\nSee CONTRIBUTING.md for more info\n\nIf you are installing JBrowse on your server, check out our quick start guides\nhere https://jbrowse.org/jb2/docs/\n",199,bioinformatics,TypeScript,8,JavaScript,HTML,CSS,TypeScript,Batchfile,Shell,C++,MDX,,,,,,,,,,,,,,,,,,,,,2188,242,1926,20,177,44,0,200915,61,1886,1567,319,e5af16b7653889d4cf41ede6939a6b2e11b670c1,Bump deps,2024-07-11T18:07:39Z,Colin,colin.diesh@gmail.com,cmdcolin,Release v2.12.3,This is a hotfix release for a bug in desktop that affected v2.12.2 that prevented sessions from being started\r\n\r\n\r\n#### :rocket: Enhancement\r\n\r\n- [#4465](https://github.com/GMOD/jbrowse-components/pull/4465) Add `contig` to the default dontRedispatch list for Gff3TabixAdapter ([@cmdcolin](https://github.com/cmdcolin))\r\n- [#4464](https://github.com/GMOD/jbrowse-components/pull/4464) Bump generic-filehandle to put URL in error messages ([@cmdcolin](https://github.com/cmdcolin))\r\n\r\n#### :bug: Bug Fix\r\n\r\n- [#4469](https://github.com/GMOD/jbrowse-components/pull/4469) Fix error launching session on desktop in v2.12.2 ([@cmdcolin](https://github.com/cmdcolin))\r\n\r\n#### :memo: Documentation\r\n\r\n- [#4466](https://github.com/GMOD/jbrowse-components/pull/4466) Add demo of using farm-fe bundler for embedded components ([@cmdcolin](https://github.com/cmdcolin))\r\n\r\n#### Committers: 1\r\n\r\n- Colin Diesh ([@cmdcolin](https://github.com/cmdcolin)) Done in 1.30s.,v2.12.3,,,github-actions[bot],Apache License 2.0,jbrowse-components,GMOD,94,jbrowse,genomics,bioinformatics,visualization,,,,,,,,,,,,,,,,,/GMOD/jbrowse-components,336,22,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/globalbioticinteractions/globalbioticinteractions,https://github.com/globalbioticinteractions/globalbioticinteractions,0,,,0,0,0,0,0,0,1,0,0,0,0,Global Biotic Interactions provides access to existing species interaction datasets,"Welcome to Global Biotic Interactions (GloBI)\n======================================\n\nThe mission of this project is to find efficient ways to normalize and integrate species interaction data to enable researchers and enthusiasts to answer questions like: Which species does an Angel Shark ( _Squatina squatina_ ) eat in the Gulf of Mexico? \n\nPlease see https://globalbioticinteractions.org or https://github.com/globalbioticinteractions/globalbioticinteractions/wiki for more information.\n\n [![Java CI](https://github.com/globalbioticinteractions/globalbioticinteractions/workflows/Java%20CI/badge.svg)](https://github.com/globalbioticinteractions/globalbioticinteractions/actions?query=workflow%3A%22Java+CI%22) [![DOI](https://zenodo.org/badge/2478263.svg)](https://zenodo.org/badge/latestdoi/2478263) \n\n## Citing GloBI\n\nPoelen, J. H., Simons, J. D., & Mungall, C. J. (2014). **Global Biotic Interactions: An open infrastructure to share and analyze species-interaction datasets**. *Ecological Informatics*, 24, 148–159. [doi:10.1016/j.ecoinf.2014.08.005](https://doi.org/10.1016/j.ecoinf.2014.08.005)\n\n## Licenses\n[![gplv3](https://www.gnu.org/graphics/gplv3-88x31.png)](https://www.gnu.org/licenses/gpl.html)[![cc-by-nc](https://i.creativecommons.org/l/by/4.0/88x31.png)](https://creativecommons.org/licenses/by/4.0/)\n\nUnless otherwise noted, source code is released under [GLPv3](https://www.gnu.org/licenses/gpl.html) and data is available under [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/). We are trying to do the best we can to ensure that the references to the original data sources are preserved and attributed. If you feel that there are better ways to do this, please [let us know](https://github.com/globalbioticinteractions/globalbioticinteractions/issues/new).\n",120,bioinformatics,Java,4,Java,Shell,HTML,JavaScript,,,,,,,,,,,,,,,,,,,,,,,,,20,5,15,0,1,17,0,157224,17,973,646,327,47cc24de161123f787b4b73ea58ee09c5bdd52f7,"Revert ""downgrade to taxon graph 0.5.1 ; related to https://github.co…",2024-07-17T18:31:04Z,Jorrit Poelen,jhpoelen+git@jhpoelen.nl,,,"## Features \r\nn/a\r\n\r\n## Improvements\r\n * [update globi taxon graph 0.4.5-> 0.4.6](https://github.com/globalbioticinteractions/globalbioticinteractions/commit/f1ff6e4b6ade18e2c2f98f9a19a070b7348b3e8b)\r\n *  add support for separated values in CSV meta table definition. For example see https://github.com/globalbioticinteractions/globalbioticinteractions/blob/b5259582794568ca78077412cf694b9ba51ed3a9/eol-globi-data-sources/src/test/resources/org/eol/globi/data/test-meta-globi-separator.json\r\n\r\nExample below, with special focus on `separator` field in the Sequences column.\r\n\r\n```json\r\n{\r\n  ""@context"": [\r\n    ""http://www.w3.org/ns/csvw"",\r\n    {\r\n      ""@language"": ""en""\r\n    }\r\n  ],\r\n  ""rdfs:comment"": [\r\n    ""inspired by https://www.w3.org/TR/2015/REC-tabular-data-model-20151217/""\r\n  ],\r\n  ""tables"": [\r\n    {\r\n      ""@context"": [\r\n        ""http://www.w3.org/ns/csvw"",\r\n        {\r\n          ""@language"": ""en""\r\n        }\r\n      ],\r\n      ""rdfs:comment"": [\r\n        ""inspired by https://www.w3.org/TR/2015/REC-tabular-data-model-20151217/""\r\n      ],\r\n      ""url"": ""https://figshare.com/ndownloader/files/2196534"",\r\n      ""dcterms:bibliographicCitation"": ""Wardeh, M., Risley, C., McIntyre, M. et al. Database of host-pathogen and related species interactions, and their global distribution. Sci Data 2, 150049 (2015). https://doi.org/10.1038/sdata.2015.49"",\r\n      ""delimiter"": "","",\r\n      ""headerRowCount"": 1,\r\n      ""interactionTypeName"": ""hasHost"",\r\n      ""interactionTypeId"": ""http://purl.obolibrary.org/obo/RO_0002454"",\r\n      ""null"": [\r\n        """"\r\n      ],\r\n      ""tableSchema"": {\r\n        ""columns"": [\r\n          {\r\n            ""name"": ""sourceTaxonName"",\r\n            ""titles"": ""Cargo"",\r\n            ""datatype"": ""string""\r\n          },\r\n          {\r\n            ""name"": ""Cargo classification"",\r\n            ""titles"": ""Cargo classification"",\r\n            ""datatype"": ""string""\r\n          },\r\n          {\r\n            ""name"": ""targetTaxonName"",\r\n            ""titles"": ""Carrier"",\r\n            ""datatype"": ""string""\r\n          },\r\n          {\r\n            ""name"": ""Carrier classification"",\r\n            ""titles"": ""Carrier classification"",\r\n            ""datatype"": ""string""\r\n          },\r\n          {\r\n            ""name"": ""Sequences count"",\r\n            ""titles"": ""Sequences count"",\r\n            ""datatype"": ""string""\r\n          },\r\n          {\r\n            ""name"": ""Publications count"",\r\n            ""titles"": ""Publications count"",\r\n            ""datatype"": ""string""\r\n          },\r\n          {\r\n            ""name"": ""referenceUrl"",\r\n            ""titles"": ""Sequences"",\r\n            ""separator"": "";"",\r\n            ""datatype"": {\r\n              ""base"": ""string"",\r\n              ""valueUrl"": ""https://www.ncbi.nlm.nih.gov/nuccore/{referenceUrl}""\r\n            }\r\n          },\r\n          {\r\n            ""name"": ""Publications"",\r\n            ""titles"": ""Publications"",\r\n            ""datatype"": ""string""\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  ]\r\n}\r\n\r\n\r\n```\r\n\r\n## Bug fixes\r\n n/a",v0.26.1,Jorrit Poelen,,jhpoelen,GNU General Public License v3.0,globalbioticinteractions,globalbioticinteractions,150,eol,ecology,biology,ecoinformatics,globi,species-interactions,bioinformatics,etl-framework,food-webs,pollinators,diseases,parasites,biodiversity,diet,,,,,,,/globalbioticinteractions/globalbioticinteractions,158,22,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/gismo/gismo,https://github.com/gismo/gismo,0.5,not sure if this is relevant. Do not explicitly mention science,0,0,1,1,0,0,0,0,0,0,0,0,G+Smo (pronounced gismo or gizmo) is a C++ library for isogeometric analysis (IGA). Geometry plus simulation modules aims at the seamless integration of Computer-aided Design (CAD) and Finite Element Analysis (FEA).,"```\n     GGGGGGGGG      GGGG      GGGGGGGGG  GGGGGG   GGGGGG  GGGGGGGGGG\n    GGGG            GGGG     GGGG        GGGGGG  GGGGGG  GGGG   GGGG\n   GGGG         GGGGGGGGGGGG GGGGGGGGG   G GGGG  G GGGG GGGG    GGGG\n   GGGG GGGGGG GGGGGGGGGGGGG GGGGGGGGGG GG GGGG GG GGGG GGGG   GGGGG\n  GGGGG  GGGGG GGGGGGGGGGGG  GGGGGGGGG  GG GGGGGG GGGG  GGGG   GGGG\n  GGGG   GGGG      GGGG           GGGG  GG  GGGG  GGGG  GGGG   GGGG\n   GGGGGGGGGG      GGGG     GGGGGGGGG  GG   GGG   GGGG  GGGGGGGGGG\n\n======================================================================\n=====             Geometry plus Simulation modules               =====\n=====                   https://github.com/gismo                 =====\n======================================================================\n```\n\n# Continuous Integration status\n| **System** | **Status** | **More information** |\n|------------|------------|----------------------|\n| [CDash](https://cdash-ci.irisa.fr/index.php?project=Gismo) | [![cdash](https://img.shields.io/website?down_color=lightgrey&down_message=offline&label=CDash&up_color=green&up_message=up&url=https%3A%2F%2Fcdash-ci.irisa.fr%2Findex.php%3Fproject%3DGismo)](https://cdash-ci.irisa.fr/index.php?project=Gismo) | Report results from all builds |\n| [Appveyor](https://ci.appveyor.com/project/gismo/gismo)  | [![Appveyor status](https://ci.appveyor.com/api/projects/status/abps59xbt1gjwci1/branch/stable?svg=true)](https://cdash-ci.irisa.fr/index.php?project=Gismo&filtercount=1&field1=site&compare1=63&value1=[appVeyor]) | Windows MSVC 14.0 |\n| [Circle CI](https://circleci.com/gh/gismo/gismo) | [![Circle CI](https://circleci.com/gh/gismo/gismo.svg?style=svg)](https://cdash-ci.irisa.fr/index.php?project=Gismo&filtercount=1&field1=site&compare1=63&value1=[cci]) | MacOS XCode 14.3 (x86_64/arm64) |\n| [Codeship](https://app.codeship.com/projects/123289)  | [![Codeship Status](https://app.codeship.com/projects/2aa19360-8998-0133-39fd-66416d65b267/status?branch=stable)](https://cdash-ci.irisa.fr/index.php?project=Gismo&filtercount=1&field1=site&compare1=63&value1=[codeship]) | |\n| [GitLab](https://gitlab.com/gismo-ci/gismo/-/pipelines)    | [![pipeline status](https://gitlab.com/gismo-ci/gismo/badges/gitlab_ci/pipeline.svg)](https://cdash-ci.irisa.fr/index.php?project=Gismo&filtercount=1&field1=site&compare1=63&value1=[gitlab-ci]) | Linux non-default configurations |\n| [GitHub Actions](https://github.com/gismo/gismo/actions) | [![Build Status](https://github.com/gismo/gismo/workflows/gismo/badge.svg?branch=stable)](https://cdash-ci.irisa.fr/index.php?project=Gismo&filtercount=1&field1=site&compare1=63&value1=[actions]) | Latest Linux/MacOS/Windows |\n| [GitLab-Inria]() | [![Build Status](https://gitlab.inria.fr/gismo/gismo/badges/stable/pipeline.svg)](https://gitlab.inria.fr/gismo/gismo/-/pipelines) | CI at Inria |\n| GCC Farm | [Status](https://cdash-ci.irisa.fr/index.php?project=Gismo&filtercount=1&field1=site&compare1=63&value1=[gccfarm]) | Builders from the GCC Farm   |\n| [OBS](https://build.opensuse.org/package/show/home:filiatra/gismo) | [binaries](https://software.opensuse.org/download/package?project=home:filiatra&package=gismo)  | Upstream package builds for many Linux distributions |\n| [Launchpad](https://code.launchpad.net/~g+smo/+recipe/g+smo-daily) |[binaries](https://launchpad.net/~g+smo/+archive/ubuntu/upstream/+packages)  | Upstream package builds for Ubuntu distributions |\n\nThis README file contains brief information. More details are found in\nthe [Wiki pages](https://github.com/gismo/gismo/wiki).\n\nThe latest revision of the code can be obtained using git (via https):\n\n```git clone https://github.com/gismo/gismo.git```\n\nor using subversion:\n\n```svn co https://github.com/gismo/gismo/trunk gismo```\n\nor as a tar.gz or zip file:\n\n* https://github.com/gismo/gismo/archive/stable.tar.gz\n* https://github.com/gismo/gismo/archive/stable.zip\n\n# Prerequisites\n\n* Operating systems:\n  - MS Windows\n  - Linux\n  - macOS\n  - FreeBSD\n\n* Configuration: [CMake 2.8.12](https://cmake.org) or newer.\n\n* Compilers tested include recent versions of\n  - [AMD Optimizing C/C++ Compiler](https://developer.amd.com/amd-aocc/)\n  - [AppleClang](https://developer.apple.com/documentation/xcode/) see [here](https://mac.r-project.org/openmp/) for OpenMP support\n  - [Clang](https://clang.llvm.org)\n  - [GNU GCC](https://gcc.gnu.org)\n  - [Intel C++ compiler](https://software.intel.com/content/www/us/en/develop/tools/compilers/c-compilers.html)\n  - [Mingw64](http://mingw-w64.org/)\n  - [MS Visual Studio C++](https://visualstudio.microsoft.com)\n  - [PGI C/C++](https://www.pgroup.com/index.htm) only with `GISMO_WITH_OPENMP=OFF`\n  \n* Compilers known to not work\n  - [Oracle Developer Studio](https://www.oracle.com/application-development/technologies/developerstudio.html) fails to compile Eigen\n  - [IBM XLC C/C++](https://www.ibm.com/products/xl-cpp-linux-compiler-power) fails to compile Eigen\n\n* Recommended:\n   - [Doxygen](https://www.doxygen.org) for generating documentation.\n   - [Paraview](https://www.paraview.org) for visualization.\n\n# Compilation\n\nThe compilation requires configuration using [CMake](https://cmake.org)\nat a new, empty folder (in-source builds are disabled).\n\n* On **Linux/macOS**: A Unix makefile exists in the root source\n  folder. Running `make` creates a sub folder named `build` and\n  executes CMake and compilation inside that folder. Alternatively,\n  choose your own build folder and execute CMake pointing to the\n  sources.\n\n* On **MS Windows**: \n     * To compile G+Smo natively, you can use MS Visual Studio which has [built-in CMake\n       support](https://docs.microsoft.com/en-us/cpp/build/cmake-projects-in-visual-studio)\n       since version 2015. Alternatively, you can run the `cmake-gui` tool\n       (from an environment that is configured with your compiler) to\n       generate makefiles (or Visual Studio project files). Then execute\n       the make tool to launch compilation. Alternatively, use the\n       QtCreator GUI and open the CMakeLists.txt file on the root folder to\n       create a QtCreator project.\n     * Another option is to install [Windows Subsystem for Linux](https://learn.microsoft.com/en-us/windows/wsl/install)\n       which:\n       > lets developers install a Linux distribution [...] and use Linux applications, utilities, \n       > and Bash command-line tools directly on Windows, unmodified, without the overhead of \n       > a traditional virtual machine or dualboot setup.\n     \n       Then you can download, compile and use G+Smo as if your were using a native Linux machine.\n\nAfter successful compilation a dynamic library is created in `./lib` and\nexecutable example programs are output at the `./bin` subdirectory of\nthe build folder.\n\nAdditionally, if [Doxygen](https://www.doxygen.org) is available on\nthe system one can execute (eg. on Linux):\n\n```make doc```\n\nto obtain the Doxygen documentation in HTML format. The main doxygen\npage is at `./doc/html/index.html`.\n\nMore information at https://github.com/gismo/gismo/wiki\n\n# Optional modules\n\nThere is a number of optional modules that may be enabled.\n\n| **Name** | **Description** |\n|----------|-----------------|\n|[gsOpenCascade](https://github.com/gismo/gismo/tree/stable/extensions/gsOpenCascade#readme)| Extends functionality using OpenCascade|\n|[gsElasticity](https://github.com/gismo/gsElasticity#readme)|  |\n|[gsKLShell](https://github.com/gismo/gsKLShell#readme)|  |\n|[gsStructuralAnalysis](https://github.com/gismo/gsStructuralAnalysis#readme)|  |\n\nTo enable e.g. gsSpectra and gsOpenCascade set the following option in CMake:\n\n ``` -D GISMO_OPTIONAL=""gsSpectra;gsOpenCascade"" ```\n\n# Configuration Options\n\nThe available options are displayed at CMake configuration.  Short\ndescription and default setting follows:\n\n* CMAKE_BUILD_TYPE        *Release*\n\n  Available values are the standard CMake build configurations: Debug,\nRelease, RelWithDebInfo, MinSizeRel.\n\n* GISMO_COEFF_TYPE        *double*\n\n  The arithmetic type to be used for all computations. Available options\ninclude double, long double, float.\n\n* GISMO_EXTRA_INSTANCE    *not set*\n\n  If set to one or more of the options available for GISMO_COEFF_TYPE\n  the G+Smo library is compiled with extra arithmetic types enabled.\n\n* GISMO_WITH_XDEBUG       *OFF*\n\n  If set to ON additional debugging tools are enabled during\ncompilation. These include checked iterators for GCC and MSVC\ncompilers and call stack back-trace printout when a runtime exception\noccurs.\n\n* GISMO_BUILD_LIB         *ON*\n\n  If enabled a dynamic library is created using GISMO_COEFF_TYPE\narithmetic. A target for a static library named gismo_static is also\ncreated but not compiled by default.\n\n* GISMO_BUILD_EXAMPLES    *ON*\n\n  If enabled the programs in the examples folder are compiled, and\nexecutables are created in build-folder/bin.\n\n* GISMO_BUILD_UNITTESTS   *OFF*\n\n  If enabled the tests in the unittests folder are compiled, and an\nexecutable is created in build-folder/bin.\n\n* GISMO_PLUGIN_AXL         *OFF*\n\n  If enabled the plugin for Axel modeler is compiled (requires Axel).\n\n* GISMO_WITH_PSOLID       *OFF*\n\n  If enabled the extensions using functionalities of Parasolid geometric\nkernel are compiled (requires Parasolid).\n\n* gsOpennurbs\n\n  Extension for reading and writing of Rhinoceros' 3DM.\n\n* CMAKE_INSTALL_PREFIX   (system dependent)\n\n  The location for installation of the library, e.g. /usr/local on some\nLinux systems.\n\n\n# Directory structure\n\n\nThe source tree consists of the following sub-folders:\n\n* **src**\n\nContains all source files. Code is partitioned into modules. Currently\neleven modules are present as sub-folders:\n\n   - **gsCore**\n   - **gsMatrix**\n   - **gsNurbs**\n   - **gsHSplines**\n   - **gsModeling**\n   - **gsAssembler**\n   - **gsSolver**\n   - **gsPde**\n   - **gsTensor**\n   - **gsIO**\n   - **gsUtils**\n\n* **examples**\n\n  Examples of usage, small programs and tutorials.\n\n* **unittests**\n\n  Unittests for some parts of the codebase.\n\n* **filedata**\n\n  Data files in the XML format the G+Smo can read and write.\n\n* **extensions**\n\n  Optional additional features that can be compiled along G+Smo.\n\n* **plugins**\n\n   The plugins for:\n\n   - Axel modeler\n   - Rhinoceros' 3DM\n\n* **cmake**\n\n  Cmake configuration files.\n\n* **doc**\n\n  Files related to doxygen documentation.\n\n# Third-party repository distribution\n\n- openSUSE Science Project: https://en.opensuse.org/openSUSE:Science_Math\n- FreeBSD port: https://www.freshports.org/math/gismo/\n- Ubuntu upstream packages: https://launchpad.net/~g+smo/+archive/ubuntu/upstream\n\n# Contact and support\n\n* Wiki pages:\n\n  https://github.com/gismo/gismo/wiki\n\n* Bug reports:\n\n  https://github.com/gismo/gismo/issues\n\n* Questions (Q&A):\n\n  https://github.com/gismo/gismo/discussions/categories/q-a\n\n# People\n\nCoordinator and maintainer: Angelos Mantzaflaris\n\nSee full list in [our wiki pages](https://github.com/gismo/gismo/wiki/About--G-Smo)\n\n# OS-license\n\nThe G+Smo library is distributed under the Mozilla Public License v2.0.  (see [LICENSE.txt](https://github.com/gismo/gismo/blob/stable/LICENSE.txt)).\n\n",328,geometry,C++,9,CMake,Makefile,C++,C,MATLAB,Dockerfile,Shell,Python,Jupyter Notebook,,,,,,,,,,,,,,,,,,,,579,52,516,11,109,82,0,242681,82,115,94,21,248641eefa0c4441a17fbf8b7efe5e1077da8a27,add new thb-evaluation function (improves speed),2024-07-18T18:37:45Z,Angelos Mantzaflaris,angelos.mantzaflaris@inria.fr,filiatra,G+Smo v23.12,"v23.12.0 (https://github.com/gismo/gismo/releases/tag/v23.12.0)\r\n------\r\n* NEW\r\n  - Floating point precision showcase, quadrature lookup guard\r\n  - Introduce gsFunction::recoverPoints, moved includes of third parties\r\n  - Biharmonic equation assembler\r\n  - Unittest gsGeometry_test\r\n  - Example file: precision_example\r\n  - Labels and file includes in XML file format\r\n\r\n* IMPROVED\r\n  - Python bindings (pygismo)\r\n  - XML file indenting\r\n  - Add unittests for spectra\r\n  - ACC3 scheme for boudaries\r\n  - gsFuncCoordinate is now more general\r\n  - Option to keep Sparsity Pattern when reseting the sparse matrix in gsExpAssembler\r\n  - Parallelization structs (gsParallel)\r\n  - Optional gsSpectra module\r\n  - Several fixes in expressions\r\n* FIXED\r\n  - CI scripts\r\n  - Header only and mpfr compilation\r\n  - Linking of MPFR with gcc\r\n  - Replace 'sprintf' by 'snprintf' as the former is marked deprecated\r\n  - Checking if the Eigen symlink already exists before creating a new one\r\n  - Static linking for pygismo\r\n  - Make G+Smo compile on Ookami (Fujitsu A64FX)\r\n\r\n* API\r\n  - Several changes in expressions\r\n",v23.12.0,Angelos Mantzaflaris (Άγγελος Μαντζαφλάρης),,filiatra,Mozilla Public License 2.0,gismo,gismo,7,isogeometric,cagd,b-splines,hierarchical,simulation,geometry,cad,bezier,,,,,,,,,,,,,/gismo/gismo,7,40,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/GiovineItalia/Gadfly.jl,https://github.com/GiovineItalia/Gadfly.jl,0,,,0,0,0,0,0,0,1,1,0,0,0,Crafty statistical graphics for Julia.,"<div align=""center""> <img\nsrc=""https://cdn.rawgit.com/GiovineItalia/Gadfly.jl/master/docs/src/assets/logo.svg""\nalt=""Gadfly Logo"" width=""210""></img> </div>\n\n| **Documentation** | **Build Status** | **Help** |\n|:---:|:---:|:---:|\n| [![][docs-dev-img]][docs-dev-url] [![][docs-stable-img]][docs-stable-url] | [![][ci-img]][ci-url] [![][codecov-img]][codecov-url] | [![][slack-img]][slack-url] [![][gitter-img]][gitter-url] |\n\n**Gadfly** is a plotting and data visualization system written in\n[Julia](http://julialang.org/).\n\nIt's influenced heavily by Leland Wilkinson's book [The Grammar of Graphics][gog-book]\nand Hadley Wickham's refinement of that grammar in [ggplot2](http://ggplot2.org/).\n\nIf you use **Gadfly** in a publication please consider citing it: [![DOI][citation-img]][citation-url]\n\n## Package features\n\n- Renders publication quality graphics to SVG, PNG, Postscript, and PDF\n- Intuitive and consistent plotting interface\n- Works with [IJulia](https://github.com/JuliaLang/IJulia.jl) and [Pluto.jl](https://github.com/fonsp/Pluto.jl) out of the box\n- Tight integration with [DataFrames.jl](https://github.com/JuliaStats/DataFrames.jl)\n- Interactivity like panning, zooming, toggling powered by [Snap.svg](http://snapsvg.io/)\n- Supports a large number of common plot types\n\n## Installation & Quickstart\n\n**Gadfly** is registered in [Julia's `General` registry](https://github.com/JuliaRegistries/General) and so can be installed using `Pkg.add`.\n\n```julia\nPkg.add(""Gadfly"")\n```\n\nTo create a plot it's as simple as:\n\n```julia\nusing Gadfly\nplot(y=[1,2,3])\n```\n\n## Gallery\n\n<div align=""center""> <img\nsrc=""https://cdn.rawgit.com/GiovineItalia/Gadfly.jl/master/docs/src/assets/gallery.png""\nalt=""Gadfly Gallery"" width=""1024""></img> </div>\n\n## Documentation\n\n- [**STABLE**][docs-stable-url] &mdash; **most recently tagged version of the documentation.**\n- [**DEVEL**][docs-dev-url] &mdash; *in-development version of the documentation.*\n\n## Contributing and Questions\n\nThis is a new and fairly complex piece of software. [Filing an\nissue](https://github.com/GiovineItalia/Gadfly.jl/issues/new) to report a\nbug, counterintuitive behavior, or even requesting a feature is extremely\nvaluable in helping us prioritize what to work on, so don't hesitate.\n\nIf you have a question then you can ask for help in the plotting team of the\n[Julia Slack channel][slack-url] or the [Gitter chat room][gitter-url].\n\n[docs-dev-img]: https://img.shields.io/badge/docs-dev-blue.svg\n[docs-dev-url]: http://gadflyjl.org/dev\n\n[docs-stable-img]: https://img.shields.io/badge/docs-stable-blue.svg\n[docs-stable-url]: http://gadflyjl.org/stable\n\n[ci-img]: https://github.com/GiovineItalia/Gadfly.jl/workflows/CI/badge.svg\n[ci-url]: https://github.com/GiovineItalia/Gadfly.jl/actions\n\n[codecov-img]: https://codecov.io/gh/GiovineItalia/Gadfly.jl/branch/master/graph/badge.svg\n[codecov-url]: https://codecov.io/gh/GiovineItalia/Gadfly.jl\n\n[citation-img]: https://zenodo.org/badge/DOI/10.5281/zenodo.593105.svg\n[citation-url]: https://doi.org/10.5281/zenodo.593105\n\n[gog-book]: http://www.cs.uic.edu/~wilkinson/TheGrammarOfGraphics/GOG.html\n\n[slack-img]: https://img.shields.io/badge/chat-on%20slack-yellow.svg\n[slack-url]: https://julialang.slack.com\n\n[gitter-img]: https://badges.gitter.im/dcjnones/Gadfly.jl.svg\n[gitter-url]: https://gitter.im/dcjones/Gadfly.jl\n",1899,graphics,Julia,1,Julia,,,,,,,,,,,,,,,,,,,,,,,,,,,,542,61,465,16,25,107,286,268698,252,1091,833,258,92b3f0a8d7982c7b3d5d499093ac5e5e6baef278,update TagBot,2024-03-13T12:16:54Z,Ben Arthur,bjarthur70@gmail.com,bjarthur,v1.3.4,"## Gadfly v1.3.4\n\n[Diff since v1.3.3](https://github.com/GiovineItalia/Gadfly.jl/compare/v1.3.3...v1.3.4)\n\n\n**Closed issues:**\n- Infer more defaults for univariate distributions (#1489)\n- Geom.label positioning with stacked bar plot (#1547)\n- Gadfly no interactivity in VSCode plot pane (and how to fix it) (#1548)\n- Error regarding Wine - Artix Linux (#1550)\n\n**Merged pull requests:**\n- Add note to tutorial.md explaining pain point (#1534) (@JarredAllen)\n- CompatHelper: bump compat for ""Distributions"" to ""0.25"" (#1540) (@github-actions[bot])\n- Fix interactivity in VSCode plot pane. (#1549) (@MrVPlusOne)\n- upgrade to IndirectArrays 1.0; drop support for 0.5 (#1554) (@bjarthur)",v1.3.4,Ben Arthur,,bjarthur,Other,Gadfly.jl,GiovineItalia,13,julia,graphics,plotting,ggplot2,gadfly,hacktoberfest,,,,,,,,,,,,,,,/GiovineItalia/Gadfly.jl,75,53,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/GiovineItalia/Compose.jl,https://github.com/GiovineItalia/Compose.jl,0,,,0,0,0,0,0,0,1,1,0,0,0,Declarative vector graphics,"# Compose!\n\n[![][docs-latest-img]][docs-latest-url] [![][travis-img]][travis-url] [![][codecov-img]][codecov-url]\n\nCompose is a vector graphics library for Julia.\nIt forms the basis for the statistical graphics system\n[Gadfly](https://github.com/GiovineItalia/Gadfly.jl).\n\n## Synopsis\n\nUnlike most vector graphics libraries, Compose is thoroughly declarative. Rather\nthan issue a sequence of drawing commands, graphics are formed by sticking\nvarious things together and then letting the library figure out how to draw it.\nThe ""things"" in this case fall one of three types: Property, Form, and Canvas.\n""Sticking together"" is primary achieved with the `compose` function.\n\nThe semantics of composition are fairly simple, and once grasped provide a\nconsistent and powerful means of building vector graphics.\n\n## Documentation\n\n- [**LATEST**][docs-latest-url] &mdash; *in-development version of the documentation.*\n\n[docs-latest-img]: https://img.shields.io/badge/docs-latest-blue.svg\n[docs-latest-url]: https://giovineitalia.github.io/Compose.jl/latest\n\n[travis-img]: http://img.shields.io/travis/GiovineItalia/Compose.jl.svg\n[travis-url]: https://travis-ci.org/GiovineItalia/Compose.jl\n[codecov-img]: https://codecov.io/gh/GiovineItalia/Compose.jl/branch/master/graph/badge.svg\n[codecov-url]: https://codecov.io/gh/GiovineItalia/Compose.jl\n",248,graphics,Julia,1,Julia,,,,,,,,,,,,,,,,,,,,,,,,,,,,267,39,226,2,13,74,94,3813,83,176,119,57,3c87c1bd85aead7bbca0d0877362677253e92622,avoid using internal `.data` field (#441),2024-03-06T16:41:24Z,Kristoffer Carlsson,kcarlsson89@gmail.com,KristofferC,v0.9.2,"## Compose v0.9.2\n\n[Diff since v0.9.1](https://github.com/GiovineItalia/Compose.jl/compare/v0.9.1...v0.9.2)\n\n\n**Closed issues:**\n- ngon and xgon are transformed incorrectly in UnitBox? (#406)\n- tests fail on 1.6 (#411)\n\n**Merged pull requests:**\n- Update typo in docs for `text` function in form.jl (#405) (@senritsu)\n- Switch to GitHub Actions (#408) (@rikhuijzer)\n- Size units (sx, sy) (#409) (@Mattriks)\n- fix tests on julia 1.6 (#412) (@bjarthur)\n- tag v0.9.2 (#413) (@bjarthur)",v0.9.2,Ben Arthur,,bjarthur,Other,Compose.jl,GiovineItalia,11,julia,vector-graphics,graphics,gadfly,hacktoberfest,,,,,,,,,,,,,,,,/GiovineItalia/Compose.jl,68,16,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/GimelStudio/GimelStudio,https://github.com/GimelStudio/GimelStudio,0,,,0,0,0,0,0,0,1,1,0,0,0,"Non-destructive, node based 2D image editor with an API for custom nodes","<h1 align=""center"">Gimel Studio</h1>\n\n<p align=""center"">\n  <img href=""https://github.com/GimelStudio/GimelStudio/blob/master/LICENSE"" src=""https://img.shields.io/badge/License-Apache2.0-green.svg"" />\n  <br/>\n  Non-destructive, node-based 2D graphics editor, focused on simplicity, speed, elegance, and usability<br/>\n  <a href=""https://gimelstudio.github.io"">Official Website</a> | <a href=""https://gimelstudio.zulipchat.com/join/sif32f3gjpnikveonzgc7zhw/"">Community Chat</a> | <a href=""https://gimelstudio.readthedocs.io/en/latest/"">Official Manual</a>\n</p>\n\n![""Gimel Studio Banner""](/assets/banner/gimel-studio-06-pre-3.png ""Gimel Studio"")\n\n\n# About Gimel Studio\n\n> [!NOTE]\n> 10/30/2023 The Gimel Studio maintainers have decided to place the project on a **temporary hiatus**. Our dedication to Gimel Studio remains and we have some ideas we want to bring to the open-source community. We plan to pick back up on planning, design, and development sometime in 2024. Thanks for your support and understanding. -Noah, on behalf of the Gimel Studio maintainers\n\nThis repository tracks the next step of Gimel Studio to become a truly usable and serious non-destructive graphics editor. It is currently in ``initial planning/development`` stage, working towards a usable MVP application. Things will probably change **a lot** from what is currently here.\n\nYou’re always welcome to help out, ask questions, give feedback, and suggest ideas/improvements.\n\n**Join us in the Zulip chat organization [here](https://gimelstudio.zulipchat.com/join/sif32f3gjpnikveonzgc7zhw/) to give your feedback and stay in the loop!**\n\n\n# Project Vision\n\nThe main goal is to expand on and greatly improve upon the concepts from the [previous version](https://github.com/Correct-Syntax/Gimel-Studio) of Gimel Studio to create a serious (yet fun!) 2D graphics editor.\n\nThis includes:\n\n- Re-designed UI (inspired by Blender and Sketch)\n- Improved file-type support (.tiff, .exr files, etc)\n- 16-bit workflow support\n- CPU and GPU based processing\n- Highly improved node-graph and overall workflow for image editing\n- Greater emphasis on re-usabilty of node graph setups via templates, etc\n- User preferences for customizabilty\n- UI translations\n- Gizmos for the viewport to allow for WYSIWYG-like interaction for transforms, etc. (e.g. crop, rotate, etc)\n- Continued improvement and additions to the Python API for scripting custom nodes\n- And more...\n\nNodes can be used to composite, edit and create new effects and/or composite raster and vector graphics on-demand and visually with node thumbnails showing each step of the process (where applicable). Helpful gizmos in the interactive viewport can be used to do various editing tasks and speed up the workflow. Preset node graph templates can be created, used and re-used to save time setting up common node-setups.\n\nCustom nodes can be scripted with the built-in Python API for maximum flexibility. Integrations with other software are planned.\n\nWith a fully non-destructive workflow that uses both GPU and CPU processing (via GLSL + OpenGL) while being seamlessly cross-platform on Windows, Linux and macOS (for 64-bit systems), Gimel Studio aims to be a simple, yet powerful 2D graphics editing tool for anyone with an image to edit.\n\n\n# Current Status\n\nThere are [alpha pre-releases](https://github.com/GimelStudio/GimelStudio/releases) available for those wanting to test the current functionality.\n\n![gs-wip-demo](https://user-images.githubusercontent.com/60711001/148820733-358faad6-ee80-4d27-b9c2-2503c6b0abf8.gif)\n*The status of the next generation of Gimel Studio as of 1/10/2022*\n\n\n# Contributing\n\nTake a look at the [GitHub Issues](https://github.com/GimelStudio/GimelStudio/issues) for details on immediate and future tasks to be done. Issues labeled “Good first issue” will be the best for new contributors. We are willing to mentor any contributors as needed.\n\n**Pull requests are always welcome! :)**\n\nPlease see the [CONTRIBUTING.md](CONTRIBUTING.md) for some details on contributing.\n\n\n# Running the code\n\n*Please note: At this stage of development, the code is highly WIP and likely to change a lot. Many things are not implemented and not stable. Please don't expect too much at this point...*\n\n## Windows\n\n1. Make sure you have Python 3.9 or 3.10 installed on your system.\n2. Run ``pip install -r requirements.txt``\n3. You can get an older release of OIIO (OpenImageIO) [here](https://www.lfd.uci.edu/~gohlke/pythonlibs/#openimageio) and install it with ``pip install <the_path_to_the_whl_here>``. Or, to use the latest version of OIIO, follow the build instructions [here](https://github.com/Correct-Syntax/py-oiio#windows-build-steps).\n4. Run ``cd src`` then ``python main.py`` to navigate to the src directory and run Gimel Studio.\n5. To build an executable, make sure you are in the root directory and run ``python build.py``. The executable will be generated in the ``dist`` folder.\n\n## Linux\n\n*This has been tested on Debian-based systems (Ubuntu, Linux Mint). Other distros may require extra configuration.*\n\n1. Make sure you have Python 3.9 installed on your system.\n2. Navigate to the root folder and in your terminal, run ``python3 build.py``. This will begin a process to install all of the neccesary libraries and will give you the option to create a standalone executable or just run the code with Python.\n\n## macOS\n\n1. Make sure you have Python 3.8 or 3.9 installed on your system.\n2. Navigate to the root folder and in your terminal, run ``python3 build.py``. This will install all of the neccesary libraries and will give you the option to create a standalone executable or just run the code with Python.\n",657,graphics,Python,2,Python,GLSL,,,,,,,,,,,,,,,,,,,,,,,,,,,53,4,47,2,3,15,0,9917,37,139,68,71,5bac429417164bfc210f77c2c49c0294bb51ff1f,Update requirements.txt,2024-06-02T18:14:48Z,Noah Rahm,correctsyntax@yahoo.com,Correct-Syntax,,,,,,,Apache License 2.0,GimelStudio,GimelStudio,2,image-editor,compositor,non-destructive,graphics,gimel-studio,design-tools,photo-editor,2d-graphics,photography,node-editor,image-manipulation,python,wxpython,graphics-editor,glsl,node-based,,,,,/GimelStudio/GimelStudio,3,16,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/gibbonCode/GIBBON,https://github.com/gibbonCode/GIBBON,0.5,"MATLAB toolbox, but pretty powerful one. Not sure if it suits us though",0,0,1,0,0,0,0,0,0,0,0,0,The Geometry and Image-Based Bioengineering add-On for MATLAB,"\n<img src=""docs/img/logos/gibbonLogoBanner_1018x300.png"" href=""https://gibboncode.org"" alt=""GIBBON"" width=""100%"">   \n\n**Cite GIBBON:** [![DOI](http://joss.theoj.org/papers/10.21105/joss.00506/status.svg)](https://doi.org/10.21105/joss.00506)\n\n**License:** [![License](https://img.shields.io/badge/License-GNU_AGPLv3-orange.svg)](https://github.com/gibbonCode/GIBBON/blob/master/LICENSE)\n\n**Join chat:** [![Join the chat at https://gitter.im/GIBBONchat/Lobby](https://badges.gitter.im/GIBBONchat/Lobby.svg)](https://gitter.im/GIBBONchat/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n# Table of contents\n- [Project Summary](#Summary)  \n- [Application highlights](#Application)  \n- [Installation](#Installation)  \n- [Getting started](#Start)\n- [Testing](#Test)\n- [License](#License)  \n- [Citing GIBBON](#Cite)\n- [Contributing](#Contributing)  \n- [Code of conduct](#CodeOfConduct)  \n- [Road Map](#RoadMap)  \n\n# Project summary <a name=""Summary""></a>\nGIBBON (The Geometry and Image-Based Bioengineering add-ON) is an open-source MATLAB toolbox, which includes an array of image and geometry visualization and processing tools and is interfaced with free open source software such as [TetGen](http://wias-berlin.de/software/tetgen/), for robust tetrahedral meshing, and [FEBio](http://febio.org/) for finite element analysis. The combination provides a highly flexible image-based modelling environment and enables advanced inverse finite element analysis.\n\n[![GIBBON overview](docs/html/GIBBON_overview.jpg)](https://gibboncode.org)\n\n## Application highlights <a name=""Application""></a>  \n- [Segmentation](#Segmentation)  \n- [Computer Aided Design (CAD) tools](#CAD)  \n- [Surface meshing tools](#SurfaceMeshing)  \n- [Volumetric meshing](#Meshing)  \n- [Lattice structures](#Lattice)\n- [Finite element analysis](#FEA)\n- [Visualization](#Visualization)    \n\n### Segmentation  <a name=""Segmentation""></a>    \nGIBBON offers image filtering and smoothing methods, and has a graphical user interface for 3D image segmentation (`HELP_imx.m`). The segmented image data can be converted to 3D surface models (`DEMO_imx_levelset_surface_compare`) which can be meshed for FEA (`HELP_runTetGen`).   \n\n<div>\n<img src=""docs/img/imx_demo.gif"" width=""50%"">   \n<img src=""docs/img/footrevolve.gif"" width=""25%"">   \n<img src=""docs/img/elefootSurfs.png"" width=""15%"">   \n</div>\n\n### Computer Aided Design (CAD) tools <a name=""CAD""></a>  \nUsing GIBBON, geometry can be imported from common mesh based CAD files (such as STL, `HELP_import_STL`). For generating geometries within MATLAB®, GIBBON also provides several CAD-style commands such as polygon rounding (`HELP_filletCurve`), revolution (`HELP_polyRevolve`), extrusion (`HELP_polyExtrude`), and sweeping and lofting (`HELP_polyLoftLinear` and `HELP_sweepLoft`). Simple geometries such as spheres (`HELP_geoSphere`), boxes (`HELP_quadBox`), platonic solids (`HELP_platonic_solid`), and rhombic dodecahedra (`HELP_rhombicDodecahedron`) can also be directly created using GIBBON.  \n\n### Surface meshing tools<a name=""SurfaceMeshing""></a>   \n2D multi-region triangular meshing (e.g. `HELP_regionTriMesh2D` and `HELP_multiRegionTriMeshUneven2D`), resampling meshes geodesically (`DEMO_geodesic_remeshing`), smoothing (`DEMO_surface_smooth_methods`), and surface mesh refinement (e.g. `HELP_subtri`, `HELP_subTriDual` and `HELP_subQuad`), mesh type conversions (e.g. `HELP_tri2quad`, `HELP_quad2tri`), and mesh dual computation (`HELP_patch_dual`). Geometries can also be exported to the STL format e.g. for computer aided manufacture and 3D printing.\n\n### Volumetric meshing <a name=""Meshing""></a>   \nTetrahedral meshing (and constrained Delaunay tessellation) of multi-region domains is enabled through an interface with the [TetGen](http://wias-berlin.de/software/tetgen/) package (`HELP_runTetGen` and `HELP_constrainedDelaunayTetGen`). Hexahedral meshes for some geometry types can be directly coded (e.g. spheres `HELP_hexMeshSphere`, boxes `HELP_hexMeshBox` and lattices `HELP_element2HexLattice`). For general input surfaces multi-region mixed tetrahedral-hexahedral meshing is also available (e.g. `DEMO_MixedTetHexMeshing`).\n\n<div>\n<img src=""docs/img/bunnyMesh.gif"" width=""40%"">\n<img src=""docs/img/mixedMesh.png"" width=""40%"">\n</div>\n\n### Lattice structures <a name=""Lattice""></a>\nOne method to generate surface geometry for lattices is the use of triply-periodic functions (`HELP_triplyPeriodicMinimal`). Functions to convert element descriptions, such as tetrahedral and hexahedral elements, to lattice structures have also been implemented (`HELP_element2lattice` and `HELP_element2HexLattice`). These allow for the creation of 3D boundary conforming lattice structures on arbitrary input geometry. Exporting of hexahedral elements is also supported allowing for FEA on the created lattice structures (`DEMO_febio_0026_hexlattice_compression`).\n\n<div>\n<img src=""docs/img/latticeCompress.gif"" width=""40%"">   \n<img src=""docs/img/dualClad.gif"" width=""40%"">   \n<img src=""docs/img/octet.gif"" width=""40%"">   \n</div>\n\n### Finite Element Analysis <a name=""FEA""></a>\nFor finite element analysis GIBBON currently links with either the free and open source software [FEBio](http://febio.org/) or with Simulia ABAQUS. Both the FEBio and ABAQUS interface is based on MATLAB® structures. The image below shows the coding of a material section in a MATLAB® structure (top row) and how these components are represented in the input files for FEBio or ABAQUS (bottom row). Through this structure to input file conversion process **any FEBio or ABAQUS functionality can be directly coded in MATLAB®**.\n<div>\n<img src=""docs/img/FEA_interface_syntax.jpg"" width=""100%"">   \n</div>\n\n##### FEBio\nGIBBON can be used as a pre- and post- processor for FEBio as it enables code-based development of meshes, boundary conditions, and input files. FEBio files can be directly exported based on dedicated MATLAB® structures (`HELP_febioStruct2xml`). Furthermore, GIBBON can be used to start and control FEBio simulations. As such, iterative and inverse FEA (e.g. based on MATLAB® optimization routines) is also enabled. All `DEMO_febio_...` files are FEBio demos, e.g. `DEMO_febio_0001_cube_uniaxial` is a simple uniaxial loading example, and `DEMO_febio_0042_inverse_FEA_cube_uniaxial` is an example of inverse FEA.    \nThe image below is for large strain analysis of a twisting bar and stems from the demo `DEMO_febio_0004_beam_twist`. Other demos cover tension, compression, shear, applied forces, applied pressures, applied displacements, bending, poroelasticity, dynamic and viscoelastic analysis, contact and indentation problems, multi-generational materials for pre-load analysis.     \n\n<div>\n<img src=""docs/img/barTwist.gif"" width=""40%"">\n<img src=""docs/img/slidingBall.gif"" width=""40%"">  \n<img src=""docs/img/squasher.gif"" width=""40%"">     \n<img src=""docs/img/propFlap.gif"" width=""40%"">     \n</div>\n\n#### Abaqus\nThe interface for ABAQUS is a recent development. Users can look at `HELP_abaqusStruct2inp` to study how input files are coded. The demo `DEMO_abaqus_0001_cube_uniaxial` is for uniaxial loading of a cube and steps through geometry creation, setting up the ABAQUS structure, saving the .inp file, running the job, and importing the results for visualization. Data is imported into MATLAB® using `importAbaqusDat` which parses ABAQUS `.DAT` files.\n\n### Visualization <a name=""Visualization""></a>    \nGIBBON expands the standard MATLAB® visualization capabilities by adding 3D image and voxel visualization (`HELP_im2patch` and `HELP_sliceViewer`), meshed geometries (`HELP_gpatch` and `HELP_meshView`), finite element models (`HELP_element2patch`), and colormapped vector data (`HELP_quiverVec`), and all visualization methods enable multiple colormaps to be used in each figure or axis window. Furthermore GIBBON offers a custom figure window `cFigure` containing 3D rotation options (`HELP_vcw`) that mimic CAD behavior of 3D scene rendering, and high quality figure exporting options (`HELP_efw`). Advanced graphics animation creation and exporting capabilities through a figure window based GUI are also enabled (`HELP_anim8`).\n\n# Installation <a name=""Installation""></a>  \n\n## Summary\n1. Download GIBBON\n2. Just run: `installGibbon.m` (found in GIBBON's main folder) and provide a link to where FEBio is installed during installation\n\n## More detailed installation instructions\nThe steps below guide you through a streamlined installation procedure using the `installGibbon.m` function<sup>\*</sup>.   \n\n\*<sub>If you prefer manual installation do the following: 1) Add the GIBBON folder (with subfolders) to the path and save the path definitions, 2) Run `createHelpDemoDocumentation.m` to integrate the help and documentation, 3) For the 3rd party packages: Go to the config folder in _../GIBBON/config_ and edit the _FEBioPath.txt_ file to contain the full path to the FEBio executable </sub>\n\n### 1. Get a copy of GIBBON\nTo use GIBBON you need to create a copy on a local directory on your machine. Obtain a copy by downloading and unzipping the latest [zip file](https://github.com/gibbonCode/GIBBON/archive/master.zip) or clone GIBBON instead e.g. using: `git clone https://github.com/gibbonCode/GIBBON.git`. You can place the GIBBON folder anywhere on your machine, however, MATLAB (and the 3rd party packages listed below) may have file permission limitations for some locations which you may need to address<sup>\*</sup>.   \n\n<sup>\* For instance, some of GIBBON's features, such as those associated with TetGen and FEBio, regularly create and delete temporary files. As such MATLAB (and the 3rd party packes) should have full permissions for these folders. First of all users should make sure MATLAB has full (e.g. read/write/delete) permissions for the entire GIBBON folder (and its sub-folders). On some platforms the 3rd party packages require special treatment. For instance some OSX users have found it necessary to give tetGen or FEBio particular file permissions. For instance in the case of TetGen related features using `git update-index --chmod=+x path/to/tetgen`. </sup>\n\n### 2. Setting up 3rd party packages\nBelow is a list of 3rd party packages that are required for full functionality. All all included with GIBBON **except for FEBio** which users need to download and install. You may skip FEBio installation if finite element analysis (with FEBio) is not desired.\n\n| Package | Description | Included? | Download |\n|:--|:--|:--:|--:|\n|[__FEBio__](https://febio.org) <br/> <img src=""docs/img/logos/febioLogo.png"" href=""https://febio.org"" alt=""FEBIO"" width=""100%"">|FEBio is a finite element solver and is used in GIBBON for all finite element analysis. Use of FEBio is featured in the many `DEMO_FEBio...` files. FEBio version 2.5.0 or newer is recommended. |__No__|[__FEBio website__](https://febio.org) |\n|[__export_fig__](https://github.com/altmany/export_fig) <br/> <img src=""docs/img/logos/export_fig_logo.jpg"" href=""https://github.com/altmany/export_fig"" alt=""export_fig"" width=""100%"">| `export_fig` helps to export publication quality images (e.g. .png, .jpg, .pdf, .eps), in GIBBON it is integrated in the export figure widget `efw` to export such images from the `cFigure` window directly. `export_fig` is also used for exporting images for creation of .gif animations with the GIBBON `anim8` function. |__Yes__|[__export_fig page on GitHub__](https://github.com/altmany/export_fig) |\n|[__TetGen__](http://wias-berlin.de/software/tetgen/) <br/> <img src=""docs/img/logos/tetgenLogo.gif"" href=""http://wias-berlin.de/software/tetgen/"" alt=""TetGen"" width=""100px"">| Is used for tetrahedral meshing (and possibly constrained 3D Delaunay tessellation). See for instance `HELP_runTetGen.m`. |__Yes__| For other versions: [__TetGen website__](http://wias-berlin.de/software/tetgen/)|\n\n### 2. Install GIBBON\nBy running `installGibbon.m` the GIBBON, FEBio, and export_fig path definitions will be added and saved to MATLAB. The help and documentation will also be integrated. Once finished you will be asked to restart MATLAB. `installGibbon.m` can be found in the main GIBBON folder.  \n\n# Getting started <a name=""Start""></a>\n\n### Access the integrated help\n* To access the help documentation from MATLAB simply type: `gdoc` which will open the GIBBON documentation page in MATLAB's help brower. The documentation which is searchable and integrated just like the rest of MATLAB's help and documentation. You can also find this page manually under `Supplemental Software` as shown below.\n* To open GIBBON documentation for a particular function can use `gdoc <functionName>` (similar to MATLAB's `doc` function). For instance for help on `im2patch` use `gdoc im2patch`. This will search the integrated help for im2patch and display the results.\n\n<img src=""docs/img/gif_helpSearch.gif"" alt=""Help integration"" width=""100%"">\n\n### Where to find functions and the executable help and demo files\n* The `lib` folder contains all GIBBON's functions.\n* The `lib_ext` folder contains external functions and libraries developed by others.\n* The `docs` folder contains the help & documentation files, and demo files which when ""published"" (using MATLAB's publish functionality) create the .html documentation files (found in `docs/html`) which are integrated in MATLAB.  \n\n* The source for the help information for any function `functionName` is named `HELP_functionName`, and  the source for demos have `DEMO_` as part of the name. Therefore if one is interested in reproducing or starting off from codes in the help and documentation simply start typing code names starting in `HELP_` or `DEMO_` in the MATLAB command window, e.g. `HELP_ind2patch` can be used to generate the help information for the `ind2patch` function. Users can start editing the file by typing `open HELP_ind2patch` in the command window. By publishing (use `gpublish <HELP_functionName>`) the HELP_ or DEMO_ files .html files are created in the `docs\html` folder. As such if users alter or contribute code in the `lib` folder and generate associated `HELP_` or `DEMO_` files, new help and documentation is then added and rendered on the website. For new help and documentation to become integrated in MATLAB run the `createHelpDemoDocumentation` function and restart MATLAB.  \n\n* Many of the `DEMO_` files focus on the use of FEBio. The demo `DEMO_febio_0001_cube_uniaxial` for instance features a simple cube subjected to a uniaxial load. Other demos focus on different load types, single versus multi-step analysis, different materials and inverse analysis (e.g. `DEMO_febio_0042_inverse_FEA_cube_uniaxial`).\n\n# Testing <a name=""Test""></a>\nGIBBON's core functionality can be tested by running `testGibbon('all','test');`. Use `testGibbon('demo','test');` or `testGibbon('help','test');` for running the demo or help files only.     \nGIBBON is currently developed and tested using the most recent version of MATLAB (or the latest pre-release) and has been tested on Windows 10, Ubuntu 20.04, and Mac OS. Most of GIBBON's functionality is compatible with older MATLAB versions, especially MATLAB R2014a and newer (Delaunay tessellation and toolbox help integration are among things that have undergone large changes). Please inform the developers (or open an issue) if a particular function does not work for your MATLAB environment. It is likely that codes can be easily altered to work for your version.    \nA large portion of GIBBON's functionality does not rely on special MATLAB toolboxes. However some functions do. Here is a list of toolboxes which appear to be used in GIBBON:\n* Image Processing Toolbox\n* Statistics and Machine Learning Toolbox\n* Symbolic Math Toolbox\n* Curve Fitting Toolbox\n* Parallel Computing Toolbox\n\n# License <a name=""License""></a>\nGIBBON is provided under:\n[![License](https://img.shields.io/badge/License-GNU_AGPLv3-orange.svg)](https://opensource.org/licenses/AGPL-3.0). The [license file](https://github.com/gibbonCode/GIBBON/blob/master/LICENSE) is found on the GitHub repository.\n\n# Citing GIBBON <a name=""Cite""></a>\nIf you use GIBBON in your work you should cite the following paper:  \n[![DOI](http://joss.theoj.org/papers/10.21105/joss.00506/status.svg)](https://doi.org/10.21105/joss.00506)    \n\n\n> Moerman, (2018). GIBBON: _The Geometry and Image-Based Bioengineering add-On_. Journal of Open Source Software, 3(22), 506, [https://doi.org/10.21105/joss.00506](https://doi.org/10.21105/joss.00506)\n\n\n# Contributing <a name=""Contributing""></a>\nWe'd love for you to contribute to GIBBON or even to join the development team! We welcome all sorts of contributions including technical content, documentation developments, or website improvements. Please read our [contributing guidelines](CONTRIBUTING.md).\n\nYou can also ask questions and learn more about contributing via Gitter chat: [![Join the chat at https://gitter.im/GIBBONchat/Lobby](https://badges.gitter.im/GIBBONchat/Lobby.svg)](https://gitter.im/GIBBONchat/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n# Code of conduct <a name=""CodeOfConduct""></a>\nSee [CODE_OF_CONDUCT](CODE_OF_CONDUCT.md)\n\n# Roadmap <a name=""RoadMap""></a>\nSee [ROADMAP](ROADMAP.md)\n",184,geometry,MATLAB,3,MATLAB,TeX,GLSL,,,,,,,,,,,,,,,,,,,,,,,,,,37,18,19,0,3,13,532,2690588,59,130,116,14,9b3caca3a4d60440f7bd8c9e7680c8f006337417,Added proper curvature analysis based on polynomials,2024-07-15T13:20:16Z,Kevin-Mattheus-Moerman,kevin.moerman@gmail.com,Kevin-Mattheus-Moerman,GIBBON: The Geometry and Image-Based Bioengineering add-On (Release: Hylobates Klossii),"The main new features of this release (v3.5.0, Hylobates Klossii) are: new FEBio demos for the new febio_spec format, support for coding and running ABAQUS models from GIBBON, updated help and documentation. ",v3.5.0,Kevin Mattheus Moerman,,Kevin-Mattheus-Moerman,GNU Affero General Public License v3.0,GIBBON,gibbonCode,3,finite-element-analysis,tetgen,segmentation,geometry,matlab,visualization,,,,,,,,,,,,,,,/gibbonCode/GIBBON,8,21,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/getzlab/rnaseqc,https://github.com/getzlab/rnaseqc,0,,,0,1,0,0,0,0,1,0,0,0,0,"Fast, efficient RNA-Seq metrics for quality control and process optimization","# RNA-SeQC\n\n[![Version](https://img.shields.io/github/release/getzlab/rnaseqc.svg?label=Version)](https://github.com/getzlab/rnaseqc/releases)\n[![CI](https://github.com/getzlab/rnaseqc/workflows/CI/badge.svg)](https://github.com/getzlab/rnaseqc/actions)\n\nRNA-SeQC 2 is described in [A. Graubert*, F. Aguet*, A. Ravi, K.G. Ardlie, Gad Getz, ""RNA-SeQC 2: efficient RNA-seq quality control and quantification for large cohorts,"" *Bioinformatics*, 2021](https://doi.org/10.1093/bioinformatics/btab135).\n\n## Installing\n\nThe latest stable build of RNA-SeQC is available on the [GitHub Releases](https://github.com/getzlab/rnaseqc/releases) page, and contains static binaries for Linux and OSX.\n\nRNA-SeQC is also available as a docker image: `gcr.io/broad-cga-aarong-gtex/rnaseqc:latest` which is automatically updated with any code change.\nOlder versions of the docker image are tagged using the full commit SHA of any commit which introduced a code change.\n\nTo checkout the source of RNA-SeQC run `git clone --recursive https://github.com/getzlab/rnaseqc.git`.\nIf you do not use the `--recursive` flag, you'll need to run `git submodule update --init --recursive` or you will be missing [SeqLib](https://github.com/walaj/SeqLib).\n\n#### Unit Tests\n\nInput data for RNA-SeQC's testing suite is not stored in the repository due to\nsize constraints. The current test data is available [here](https://storage.googleapis.com/agraubert/broadinstitute/rnaseqc/test_inputs.tar.gz), and must be unpacked within the `test_data/` directory.\nPlease note that the location of the test data is subject to change.\nThe test resources use **~1.2 GB** of space.\n\nYou can download and unpack test data with:\n\n```\ncd test_data\nwget https://storage.googleapis.com/agraubert/broadinstitute/rnaseqc/test_inputs.tar.gz\ntar xzf test_inputs.tar.gz\n```\n\nYou can run the unit tests with `make test`\n\n## Usage\n\n**NOTE**: This tool requires that the provided GTF be collapsed in such a way that there are no overlapping transcripts **on the same strand** and that each gene have a single transcript whose id matches the parent gene id. This is **not** a transcript-quantification method. Readcounts and coverage are made towards exons and genes only if *all* aligned segments of a read fully align to exons of a gene, but keep in mind that coverage may be counted towards multiple transcripts (and its exons) if these criteria are met. Beyond this, no attempt will be made to disambiguate which transcript a read belongs to.\nYou can collapse an existing GTF using the [GTEx collapse annotation script](https://github.com/broadinstitute/gtex-pipeline/tree/master/gene_model)\n\n### Command Line Usage:\n\n`rnaseqc [OPTIONS] gtf bam output`\n\nExample: `./rnaseqc test_data/downsampled.gtf test_data/downsampled.bam --bed test_data/downsampled.bed --coverage .`\n\n###### OPTIONS:\n      -h, --help                        Display this message and quit\n\n      --version                         Display the version and quit\n\n      gtf                               The input GTF file containing features\n                                        to check the bam against\n\n      bam                               The input SAM/BAM file containing reads\n                                        to process\n\n      output                            Output directory\n\n      -s[sample], --sample=[sample]     The name of the current sample. Default:\n                                        The bam's filename\n\n      --bed=[BEDFILE]                   Optional input BED file containing\n                                        non-overlapping exons used for fragment\n                                        size calculations\n\n      --fasta=[fasta]                   Optional input FASTA/FASTQ file\n                                        containing the reference sequence used\n                                        for parsing CRAM files\n\n      --chimeric-distance=[DISTANCE]    Set the maximum accepted distance\n                                        between read mates. Mates beyond this\n                                        distance will be counted as chimeric\n                                        pairs. Default: 2000000 [bp]\n\n      --fragment-samples=[SAMPLES]      Set the number of samples to take when\n                                        computing fragment sizes. Requires the\n                                        --bed argument. Default: 1000000\n\n      -q[QUALITY],\n      --mapping-quality=[QUALITY]       Set the lower bound on read quality for\n                                        exon coverage counting. Reads below this\n                                        number are excluded from coverage\n                                        metrics. Default: 255\n\n      --base-mismatch=[MISMATCHES]      Set the maximum number of allowed\n                                        mismatches between a read and the\n                                        reference sequence. Reads with more than\n                                        this number of mismatches are excluded\n                                        from coverage metrics. Default: 6\n\n      --offset=[OFFSET]                 Set the offset into the gene for the 3'\n                                        and 5' windows in bias calculation. A\n                                        positive value shifts the 3' and 5'\n                                        windows towards eachother, while a\n                                        negative value shifts them apart.\n                                        Default: 150 [bp]\n\n      --window-size=[SIZE]              Set the size of the 3' and 5' windows in\n                                        bias calculation. Default: 100 [bp]\n\n      --gene-length=[LENGTH]            Set the minimum size of a gene for bias\n                                        calculation. Genes below this size are\n                                        ignored in the calculation. Default: 600\n                                        [bp]\n\n      --legacy                          Use legacy counting rules. Gene and exon\n                                        counts match output of RNA-SeQC 1.1.9\n\n      --stranded=[stranded]             Use strand-specific metrics. Only\n                                        features on the same strand of a read\n                                        will be considered. Allowed values are\n                                        'RF', 'rf', 'FR', and 'fr'\n\n      -v, --verbose                     Give some feedback about what's going\n                                        on. Supply this argument twice for\n                                        progress updates while parsing the bam\n\n      -t[TAG...], --tag=[TAG...]        Filter out reads with the specified tag.\n\n      --chimeric-tag=[TAG]              Reads maked with the specified tag will\n                                        be labeled as Chimeric. Defaults to 'ch'\n                                        for STAR\n\n      --exclude-chimeric                Exclude chimeric reads from the read\n                                        counts\n\n      -u, --unpaired                    Allow unpaired reads to be quantified.\n                                        Required for single-end libraries\n\n      --rpkm                            Output gene RPKM values instead of TPMs\n\n      --coverage                        If this flag is provided, coverage\n                                        statistics for each transcript will be\n                                        written to a table. Otherwise, only\n                                        summary coverage statistics are\n                                        generated and added to the metrics table\n\n      --coverage-mask=[SIZE]            Sets how many bases at both ends of a\n                                        transcript are masked out when computing\n                                        per-base exon coverage. Default: 500bp\n\n      -d[threshold],\n      --detection-threshold=[threshold] Number of counts on a gene to consider\n                                        the gene 'detected'. Additionally, genes\n                                        below this limit are excluded from 3'\n                                        bias computation. Default: 5 reads\n\n      ""--"" can be used to terminate flag options and force all following\n      arguments to be treated as positional options\n\n### Output files:\nThe following output files are generated in the output directory you provide:\n* {sample}.metrics.tsv : A tab-delimited list of (Statistic, Value) pairs of all statistics and metrics recorded.\n* {sample}.exon_reads.gct : A tab-delimited GCT file with (Exon ID, Gene Name, coverage) tuples for all exons which had at least part of one read mapped.\n* {sample}.gene_reads.gct : A tab-delimited GCT file with (Gene ID, Gene Name, coverage) tuples for all genes which had at least one read map to at least one of its exons. This file contains the gene-level read counts used, e.g., for differential expression analyses.\n* {sample}.gene_tpm.gct : A tab-delimited GCT file with (Gene ID, Gene Name, TPM) tuples for all genes reported in the gene_reads.gct file, with expression values in transcript per million (TPM) units. Note: this file is renamed to .gene_rpkm.gct if the **--rpkm** flag is present.\n* {sample}.fragmentSizes.txt : A list of fragment sizes recorded, if a BED file was provided\n* {sample}.coverage.tsv : A tab-delimited list of (Gene ID, Transcript ID, Mean Coverage, Coverage Std, Coverage CV) tuples for all transcripts encountered in the GTF.\n\n#### Metrics reported:\n\nSee [Metrics.md](Metrics.md) for a description of all metrics reported in the `metrics.tsv`, `coverage.tsv`, and `fragmentSizes.txt` files.\n\n### Legacy mode differences\n\nThe **--legacy** flag enables compatibility with RNASeQC 1.1.9. This ensures that exon and gene readcounts match exactly the counts which would have been produced by running that version. This also adds an extra condition to classify reads as chimeric (see ""Chimeric Reads"", above). Any metrics which existed in 1.1.9 will also match within Java's floating point precision.\n",144,bioinformatics,C++,4,C++,Makefile,Dockerfile,Python,,,,,,,,,,,,,,,,,,,,,,,,,16,2,13,1,13,4,0,13344,19,71,53,18,3d11a67d149bbe07fa3389f174c94be887b82758,Numpy 2 compatibility fix,2024-06-16T19:56:22Z,francois-a,francois@broadinstitute.org,francois-a,RNA-SeQC 2.4.2,"Bug Fixes:\r\n* Fixed an issue preventing GC content statistics from being generated\r\n* Fixed a long standing issue with `--fasta` references and crams (#50)\r\n\r\nOther Changes:\r\n* Bumped the `rnaseqc` python module to version `0.0.3`\r\n* Improved unit tests\r\n* Added tests for, and warnings regarding cram/reference mismatches. htslib is a nightmare",v2.4.2,Aaron Graubert,,agraubert,Other,rnaseqc,getzlab,17,rnaseq,rna-seq,bioinformatics,,,,,,,,,,,,,,,,,,/getzlab/rnaseqc,19,11,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/GeomScale/volesti,https://github.com/GeomScale/volesti,0,,,0,0,0,0,0,0,1,0,0,0,0,Practical volume computation and sampling in high dimensions,"<p align=""center""><img src=""docs/logo/volesti_logo.jpg""></p>\n\n**VolEsti** is a `C++` library for volume approximation and sampling of convex bodies (*e.g.* polytopes) with an `R`  interface. For a limited `Python` interface we refer to package [dingo](https://github.com/GeomScale/dingo). **VolEsti** is part of the [GeomScale](https://geomscale.github.io) project.\n\n[![CRAN status](https://www.r-pkg.org/badges/version/volesti)](https://cran.r-project.org/package=volesti)\n[![CRAN downloads](https://cranlogs.r-pkg.org/badges/volesti)](https://cran.r-project.org/package=volesti)\n![CRAN/METACRAN](https://img.shields.io/cran/l/volesti)\n[![Documentation Status](https://readthedocs.org/projects/volesti/badge/?version=latest)](https://volesti.readthedocs.io/en/latest/?badge=latest)\n[![Chat](https://badges.gitter.im/boostorg/geometry.png)](https://gitter.im/GeomScale/community?utm_source=share-link&utm_medium=link&utm_campaign=share-link)\n\n### 🧪 Test results\n\n[![CRAN check](https://badges.cranchecks.info/worst/volesti.svg)](https://cran.r-project.org/web/checks/check_results_volesti.html)\n[![CircleCI master](https://circleci.com/gh/GeomScale/volesti/tree/master.svg?style=shield)](https://circleci.com/gh/GeomScale/volesti/tree/master)\n[![CircleCI develop](https://circleci.com/gh/GeomScale/volesti/tree/develop.svg?style=shield)](https://circleci.com/gh/GeomScale/volesti/tree/develop)\n[![gcc-test](https://github.com/GeomScale/volesti/actions/workflows/cmake-gcc.yml/badge.svg)](https://github.com/GeomScale/volesti/actions/workflows/cmake-gcc.yml?query=branch%3Adevelop)\n[![clang-test](https://github.com/GeomScale/volesti/actions/workflows/cmake-clang.yml/badge.svg)](https://github.com/GeomScale/volesti/actions/workflows/cmake-clang.yml?query=branch%3Adevelop)\n[![codecov](https://codecov.io/gh/GeomScale/volesti/branch/develop/graph/badge.svg)](https://codecov.io/gh/GeomScale/volesti)\n[![doc-actions](https://github.com/GeomScale/volesti/actions/workflows/docs.yml/badge.svg)](https://github.com/GeomScale/volesti/actions/workflows/docs.yml?query=branch%3Adevelop)\n\n[![R-CMD-ubuntu](https://github.com/GeomScale/volesti/workflows/R-CMD-check-ubuntu/badge.svg)](https://github.com/GeomScale/volesti/actions?query=workflow%3AR-CMD-ubuntu)\n[![R-CMD-macOS](https://github.com/GeomScale/volesti/workflows/R-CMD-check-macOS/badge.svg)](https://github.com/GeomScale/volesti/actions?query=workflow%3AR-CMD-macOS)\n[![R-CMD-windows](https://github.com/GeomScale/volesti/workflows/R-CMD-check-windows/badge.svg)](https://github.com/GeomScale/volesti/actions?query=workflow%3AR-CMD-windows)\n\n### 📄 Documentation\n\n* [Package documentation](https://volesti.readthedocs.io)\n* [Wikipage with Tutorials and Demos](https://github.com/GeomScale/volesti/wiki)\n* [Tutorial given to PyData meetup](https://vissarion.github.io/tutorials/volesti_tutorial_pydata.html)\n* [Tutorial on (truncated) logconcave sampling (R and C++)](https://papachristoumarios.github.io/2020/07/21/Sampling-from-high-dimensional-truncated-log-concave-densities-with-volesti)\n* [Contributing](CONTRIBUTING.md)\n\n### ⭐ Credits\n\n* [Contributors and Package History](doc/credits.md)\n* [List of Publications](doc/publications.md)\n\n### © Copyright and Licensing\n\nCopyright (c) 2012-2022 Vissarion Fisikopoulos\\nCopyright (c) 2018-2022 Apostolos Chalkis\\nCopyright (c) 2020-2022 Elias Tsigaridas\n\nYou may redistribute or modify the software under the [GNU Lesser General Public License](/LICENSE) as published by Free Software Foundation, either version 3 of the License, or (at your option) any later version. It is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY.\n",142,geometry,C++,3,C++,CMake,Python,,,,,,,,,,,,,,,,,,,,,,,,,,242,75,161,6,12,29,0,94808,114,69,53,16,3325d6a7749c3a626ab44e2f6339840921d99fd6,CRHMC Integration within Volume Cooling Gaussians (#314),2024-07-19T16:10:00Z,Vladimir Necula,151810681+vgnecula@users.noreply.github.com,vgnecula,v1.1.2-6: CRAN package,,v1.1.2-6,Vissarion Fisikopoulos,,vissarion,GNU Lesser General Public License v3.0,volesti,GeomScale,13,geometry,statistics,polyhedral-computations,random-walk,sampling,volume,hit-and-run,polytope,cran,finance,monte-carlo,scientific-computing,,,,,,,,,/GeomScale/volesti,15,12,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/GeoCoq/GeoCoq,https://github.com/GeoCoq/GeoCoq,0,,,0,1,1,0,0,0,1,0,0,0,0,A formalization of geometry in Coq based on Tarski's axiom system,"# GeoCoq\nA formalization of geometry in Coq.\n\nThis library contains a formalization of geometry using the Coq proof assistant. It contains both proofs about the foundations of geometry and high-level proofs in the same style as in high-school.\n\nDetails and installation instructions can be found [here](http://geocoq.github.io/GeoCoq/).\n\nBug reports are to be submitted [here](https://github.com/GeoCoq/GeoCoq/issues).\n\nIt is possible to contact the authors of the GeoCoq library using our [mailing list](https://groups.google.com/forum/?hl=fr#!forum/geocoq).\n\nGeoCoq is available as [releases packages](https://github.com/coq/opam/tree/master/released).\n\n## Building and installation\n\n- To get the required dependencies, you can use [opam](https://opam.ocaml.org).\n\n  - To pin [pin](https://opam.ocaml.org/doc/Usage.html#opam-pin) the development packages.\n    - `opam pin -n .`\n\n  - GeoCoq relies on other released packages that need to be [added](https://opam.ocaml.org/doc/Usage.html#opam-repo).\n    - `opam repo add coq-released https://coq.inria.fr/opam/released`\n\n  - The required dependencies can now be installed:\n    - `opam install ./coq-geocoq-coinc.opam --deps-only` to get the _GeoCoq Coinc_ dependencies;\n    - `opam install ./coq-geocoq-axioms.opam --deps-only` to get the _GeoCoq Axioms_ dependencies;\n    - `opam install ./coq-geocoq-elements.opam --deps-only` to get the _GeoCoq Elements_ dependencies;\n    - `opam install ./coq-geocoq-main.opam --deps-only` to get the _GeoCoq Main_ dependencies;\n    - `opam install ./coq-geocoq-algebraic.opam --deps-only` to get the _GeoCoq Algebraic_ dependencies;\n    - `opam install ./coq-geocoq.opam --deps-only` to get the _GeoCoq_ dependencies.\n\n- The general Makefile is in the top directory.\n  - `make` : compilation of the Coq scripts (after using `./configure.sh`).\n\n- [Dune](https://dune.readthedocs.io/en/stable/coq.html) can used for compilation.\n  - `dune build`\n\n- You may also rely on `dune` to install just one part. Run:\n  - `dune build coq-geocoq-coinc.install` to build only the _GeoCoq Coinc_ part (and its dependencies);\n  - `dune build coq-geocoq-axioms.install` to build only the _GeoCoq Axioms_ part (and its dependencies);\n  - `dune build coq-geocoq-elements.install` to build only the _GeoCoq Elements_ part (and its dependencies);\n  - `dune build coq-geocoq-main.install` to build only the _GeoCoq Main_ part (and its dependencies);\n  - `dune build coq-geocoq-algebraic.install` to build only the _GeoCoq Algebraic_ part (and its dependencies);\n  - `dune build coq-geocoq.install` to build only the _GeoCoq_ part (and its dependencies).\n",179,geometry,Coq,2,Coq,Shell,,,,,,,,,,,,,,,,,,,,,,,,,,,35,2,32,1,2,16,20,8033,26,13,10,3,155d567a23dc02df52c11324f44d75c4d4a88953,dimensional axioms handled in all the formalized models,2024-05-31T14:01:41Z,Pierre Boutry,contact@pierre-boutry.fr,Boutry,Spring 2024,"- This release is compatible, at the moment of the release, with Coq 8.10.0-8.19.1. Some files depends on mathcomp field library (1.11.0 <= version <= 1.19.0).\r\n- Reorganization of files into directories that match the various subpackages of GeoCoq.\r\n- Add the possibility to build with dune. This requires dune >= 3.8 so it is only compatible with Coq > 8.13.\r\n- While the compilation time of the library decreased thanks to some performance improvements made by Coq, we also made GeoCoq compile faster.\r\n- We formalized ten out of the eleven counter-models for planar geometry present in Gupta's thesis. For a few of them we did not yet mechanize the proof that the continuity axiom holds.\r\n- We defined an alternative axiom system that, we believe, allows to obtain the arithmetization of n-dimensional geometry without relying on decidability properties.\r\n- We formalized counter-models for this system, either by generalizing Gupta's ones or by inventing new ones. Up to the mechanization of the proofs that the proposed dimensional axioms hold in some counter-models, we have the same results as for planar geometry.",v2.5.0,Pierre Boutry,,Boutry,GNU Lesser General Public License v3.0,GeoCoq,GeoCoq,12,formalization,coq,geometry,tarski-axiom,hilbert-axioms,desargues,pappus,parallel-postulate,euclid,elements,continuity,archimedes,,,,,,,,,/GeoCoq/GeoCoq,12,19,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/gazebosim/gz-sim,https://github.com/gazebosim/gz-sim,1,,,1,0,1,0,0,0,0,0,0,0,0,Open source robotics simulator. The latest version of Gazebo.,"# Gazebo Sim : A Robotic Simulator\n\n**Maintainer:** michael AT openrobotics DOT org\n\n[![GitHub open issues](https://img.shields.io/github/issues-raw/gazebosim/gz-sim.svg)](https://github.com/gazebosim/gz-sim/issues)\n[![GitHub open pull requests](https://img.shields.io/github/issues-pr-raw/gazebosim/gz-sim.svg)](https://github.com/gazebosim/gz-sim/pulls)\n[![Discourse topics](https://img.shields.io/discourse/https/community.gazebosim.org/topics.svg)](https://community.gazebosim.org)\n[![Hex.pm](https://img.shields.io/hexpm/l/plug.svg)](https://www.apache.org/licenses/LICENSE-2.0)\n\nBuild | Status\n-- | --\nTest coverage | [![codecov](https://codecov.io/gh/gazebosim/gz-sim/tree/gz-sim8/graph/badge.svg)](https://codecov.io/gh/gazebosim/gz-sim/tree/gz-sim8)\nUbuntu Jammy  | [![Build Status](https://build.osrfoundation.org/buildStatus/icon?job=gz_sim-ci-gz-sim8-jammy-amd64)](https://build.osrfoundation.org/job/gz_sim-ci-gz-sim8-jammy-amd64)\nHomebrew      | [![Build Status](https://build.osrfoundation.org/buildStatus/icon?job=gz_sim-ci-gz-sim8-homebrew-amd64)](https://build.osrfoundation.org/job/gz_sim-ci-gz-sim8-homebrew-amd64)\nWindows       | [![Build Status](https://build.osrfoundation.org/job/gz_sim-8-win/badge/icon)](https://build.osrfoundation.org/job/gz_sim-8-win/)\n\nGazebo Sim is an open source robotics simulator. Through Gazebo Sim, users have access to high fidelity physics, rendering, and sensor models. Additionally, users and developers have multiple points of entry to simulation including a graphical user interface, plugins, and asynchronous message passing and services.\n\nGazebo Sim is derived from [Gazebo Classic](http://classic.gazebosim.org) and represents over 16 years of development and experience in robotics and simulation. This library is part of the [Gazebo](https://gazebosim.org) project.\n\n# Table of Contents\n\n[Features](#features)\n\n[Install](#install)\n\n[Usage](#usage)\n\n[Documentation](#documentation)\n\n[Testing](#testing)\n\n[Folder Structure](#folder-structure)\n\n[Code of Conduct](#code-of-conduct)\n\n[Contributing](#contributing)\n\n[Versioning](#versioning)\n\n[License](#license)\n\n# Features\n\n* **Dynamics simulation**: Access multiple high-performance physics engines\nthrough\n[Gazebo Physics](https://github.com/gazebosim/gz-physics).\n\n* **Advanced 3D graphics**: Through\n[Gazebo Rendering](https://github.com/gazebosim/gz-rendering),\nit's possible to use rendering engines such as OGRE v2 for realistic rendering\nof environments with high-quality lighting, shadows, and textures.\n\n* **Sensors and noise models**: Generate sensor data, optionally with noise,\nfrom laser range finders, 2D/3D cameras, Kinect style sensors, contact sensors,\nforce-torque, IMU, GPS, and more, all powered by\n[Gazebo Sensors](https://github.com/gazebosim/gz-sensors)\n\n* **Plugins**: Develop custom plugins for robot, sensor, and\nenvironment control.\n\n* **Graphical interface**: Create, introspect and interact with your simulations\nthrough plugin-based graphical interfaces powered by\n[Gazebo GUI](https://github.com/gazebosim/gz-gui).\n\n* **Simulation models**: Access numerous robots including PR2, Pioneer2 DX,\niRobot Create, and TurtleBot, and construct environments using other physically\naccurate models available through\n[Gazebo Fuel](https://app.gazebosim.org/fuel). You can also build a\nnew model using [SDF](http://sdformat.org).\n\n* **TCP/IP Transport**: Run simulation on remote servers and interface to\nGazebo Sim through socket-based message passing using\n[Gazebo Transport](https://github.com/gazebosim/gz-transport).\n\n* **Command line tools**: Extensive command line tools for increased simulation\nintrospection and control.\n\n# Install\n\nSee the [installation tutorial](https://gazebosim.org/api/sim/8/install.html).\n\n# Usage\n\nGazebo Sim can be run from the command line, once [installed](#install), using:\n\n```\ngz sim\n```\n\nFor help, and command line options use:\n\n```\ngz sim -h\n```\n\n## Known issue of command line tools\n\nIn the event that the installation is a mix of Debian and from source, command\nline tools from `gz-tools` may not work correctly.\n\nA workaround for a single package is to define the environment variable\n`GZ_CONFIG_PATH` to point to the location of the Gazebo library installation,\nwhere the YAML file for the package is found, such as\n```\nexport GZ_CONFIG_PATH=/usr/local/share/gz\n```\n\nHowever, that environment variable only takes a single path, which means if the\ninstallations from source are in different locations, only one can be specified.\n\nAnother workaround for working with multiple Gazebo libraries on the command\nline is using symbolic links to each library's YAML file.\n```\nmkdir ~/.gz/tools/configs -p\ncd ~/.gz/tools/configs/\nln -s /usr/local/share/gz/fuel8.yaml .\nln -s /usr/local/share/gz/transport13.yaml .\nln -s /usr/local/share/gz/transportlog13.yaml .\n...\nexport GZ_CONFIG_PATH=$HOME/.gz/tools/configs\n```\n\nThis issue is tracked [here](https://github.com/gazebosim/gz-tools/issues/8).\n\n# Documentation\n\nSee the [installation tutorial](https://gazebosim.org/api/sim/8/install.html).\n\n# Testing\n\nSee the [installation tutorial](https://gazebosim.org/api/sim/8/install.html).\n\nSee the [Writing Tests section of the contributor guide](https://github.com/gazebosim/gz-sim/blob/main/CONTRIBUTING.md#writing-tests) for help creating or modifying tests.\n\n# Folder Structure\n\nRefer to the following table for information about important directories and files in this repository.\n\n```\ngz-sim\n├── examples                     Various examples that can be run against binary or source installs of gz-sim.\n│   ├── plugin                   Example plugins.\n│   ├── standalone               Example standalone programs that use gz-sim as a library.\n│   └── worlds                   Example SDF world files.\n├── include/gz/sim               Header files that downstream users are expected to use.\n│   └── detail                   Header files that are not intended for downstream use, mainly template implementations.\n├── python                       Python wrappers\n├── src                          Source files and unit tests.\n│   ├── gui                      Graphical interface source code.\n│   └── systems                  System source code.\n├── test\n│   ├── integration              Integration tests.\n│   ├── performance              Performance tests.\n│   ├── plugins                  Plugins used in tests.\n│   ├── regression               Regression tests.\n│   └── tutorials                Tutorials, written in markdown.\n├── Changelog.md                 Changelog.\n├── CMakeLists.txt               CMake build script.\n├── Migration.md                 Migration guide.\n└── README.md                    This readme.\n```\n\n# Contributing\n\nPlease see\n[CONTRIBUTING.md](https://github.com/gazebosim/gz-sim/blob/main/CONTRIBUTING.md).\n\n# Code of Conduct\n\nPlease see\n[CODE_OF_CONDUCT.md](https://github.com/gazebosim/gz-sim/blob/main/CODE_OF_CONDUCT.md).\n\n# Versioning\n\nThis library uses [Semantic Versioning](https://semver.org/). Additionally, this library is part of the [Gazebo project](https://gazebosim.org) which periodically releases a versioned set of compatible and complimentary libraries. See the [Gazebo website](https://gazebosim.org) for version and release information.\n\n# License\n\nThis library is licensed under [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). See also the [LICENSE](https://github.com/gazebosim/gz-sim/blob/main/LICENSE) file.\n",631,physics,C++,8,CMake,Shell,C++,QML,Ruby,Python,HTML,C,,,,,,,,,,,,,,,,,,,,,1715,164,1524,27,177,119,0,125326,251,771,468,303,dd8c4d78253a4fdce3b698e67d19e4474651ba4b,Fix lidar visualization when gz_frame_id is specified (#2481),2024-07-17T20:12:27Z,Ian Chen,ichen@openrobotics.org,iche033,Gazebo Sim 8: Harmonic,Read all about it on [this blog post](https://www.openrobotics.org/blog/2023/9/26/gazebo-harmonic-released).\r\n\r\nGet started with [these instructions.](https://gazebosim.org/docs/harmonic/getstarted)\r\n\r\nAnd get your [Harmonic swag](https://ignition-gazebo.creator-spring.com/)!\r\n\r\n*The Gazebo Dev Team*\r\n![harmonic](https://github.com/gazebosim/gz-sim/assets/206116/4a993616-7ea3-4080-bcde-5cdcc57e869e)\r\n\r\n,gz-sim8_8.0.0,Addisu Z. Taddese,,azeey,Apache License 2.0,gz-sim,gazebosim,5,ignition-gazebo,ignition-robotics,graphical-interface,ignition-libraries,robotics,simulation,robotics-simulation,cpp,gazebo,physics,rendering,hacktoberfest,robot-simulator,robot-simulation,simulated-robots,ros,ros2,gazebosim,,,/gazebosim/gz-sim,128,17,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/gap-system/gap,https://github.com/gap-system/gap,0,"dedicated language, collection",,0,0,1,0,0,0,1,0,0,0,0,"Main development repository for GAP - Groups, Algorithms, Programming, a System for Computational Discrete Algebra","[![Build Status](https://github.com/gap-system/gap/workflows/CI/badge.svg?branch=master)](https://github.com/gap-system/gap/actions?query=workflow%3ACI+branch%3Amaster)\n[![Code Coverage](https://codecov.io/gh/gap-system/gap/branch/master/graphs/badge.svg)](https://codecov.io/gh/gap-system/gap/branch/master)\n[![ref](https://img.shields.io/badge/docs-ref-blue)](https://gap-system.github.io/gap/doc/ref/chap0_mj.html)\n[![dev](https://img.shields.io/badge/docs-dev-blue)](https://gap-system.github.io/gap/doc/dev/chap0_mj.html)\n[![hpc](https://img.shields.io/badge/docs-hpc-blue)](https://gap-system.github.io/gap/doc/hpc/chap0_mj.html)\n[![tut](https://img.shields.io/badge/docs-tut-blue)](https://gap-system.github.io/gap/doc/tut/chap0_mj.html)\n\n# What is GAP?\n\nGAP is a system for computational discrete algebra, with particular emphasis\non computational group theory. GAP provides a programming language, a library\nof thousands of functions implementing algebraic algorithms written in the GAP\nlanguage as well as large data libraries of algebraic objects. For a more\ndetailed overview, see\n  <https://www.gap-system.org/Overview/overview.html>.\nFor a description of the mathematical capabilities, see\n  <https://www.gap-system.org/Overview/Capabilities/capabilities.html>.\n\nGAP is used in research and teaching for studying groups and their\nrepresentations, rings, vector spaces, algebras, combinatorial structures, and\nmore. The system, including source, is distributed freely. You can study and\neasily modify or extend it for your special use.\n\n\n# How to obtain GAP?\n\n## Download a stable release version\n\nThe latest stable release of the GAP system, including all currently\nredistributed GAP packages, can be obtained from\n  <https://www.gap-system.org/Releases/index.html>.\nAfterwards, follow the instructions in the file `INSTALL.md` in the GAP root\ndirectory.\n\n\n# Using a GAP development version\n\nAlternatively, you can compile the latest development version of GAP. However,\nmost users should instead use the latest official release instead as described\nin the previous section.\n\nIf you really want to use a development version of GAP, start by cloning the\nGAP source repository using git:\n\n    git clone https://github.com/gap-system/gap\n\n\n## Installing required dependencies\n\nIn this case, you need to have some more software dependencies installed than\nwith a stable release in order to compile GAP. In particular, you need at\nleast these:\n\n* a C compiler, e.g. GCC or Clang\n* a C++ compiler\n* GNU Make\n* GNU Autoconf\n\nIn addition, we recommend that you install at least the following optional\ndependencies (if you do not, GAP will either build its own copies of these,\nslowing down the compilation process, or omit certain features):\n* Development headers for GMP, the GNU Multiple Precision Arithmetic Library\n* Development headers for zlib\n* Development headers for GNU Readline\n\nOn Ubuntu or Debian, you can install these with the following command:\n\n    sudo apt-get install build-essential autoconf libgmp-dev libreadline-dev zlib1g-dev\n\nOn Fedora:\n\n    sudo dnf install gcc gcc-c++ make autoconf gmp gmp-devel readline readline-devel zlib zlib-devel\n\nOn Alpine:\n\n    sudo apk add build-base autoconf gmp-dev readline-dev zlib-dev\n\nOn macOS, you can install the dependencies in several ways:\n\n * using Homebrew: `brew install autoconf gmp readline`\n * using Fink: `fink install autoconf2.6 gmp5 readline7`\n * using MacPorts: `port install autoconf gmp readline`\n\nOn other operating systems, you will need to figure out equivalent commands\nto install the required dependencies.\n\n\n## Building GAP\n\nThen to build GAP, first run this command to generate the `configure` script:\n\n    ./autogen.sh\n\nAfterwards you can proceed as described in `INSTALL.md`. If you are on macOS,\nwe recommend that you take a look at section ""GAP for macOS"" of `INSTALL.md`\nfor a few additional hints.\n\n\n## Obtaining the GAP package distribution\n\nIn contrast to the GAP stable releases, the development version does not come\nbundled with all the GAP packages. Therefore, if you do not have a GAP package\narchive yet, we recommend that you bootstrap the stable versions of packages\nby executing one of the following commands. Whether you choose to\n`bootstrap-pkg-minimal` or `bootstrap-pkg-full` depends on your needs for\ndevelopment.\n\n    make bootstrap-pkg-minimal\n\nor\n\n    make bootstrap-pkg-full\n\nIn the latter case please note that `make bootstrap-pkg-full` only unpacks packages\nbut does not build those of them that require compilation. You can change to the\n`pkg` directory and then call `../bin/BuildPackages.sh` from there to build as many\npackages as possible.\n\nIf everything goes well, you should be able to start GAP by executing\n\n    ./gap\n\nYou can also find development versions of some of the GAP packages on\n<https://github.com/gap-packages> resp. on <https://gap-packages.github.io>.\n\n\n# We welcome contributions\n\nThe GAP Project welcomes contributions from everyone, in the shape of code,\ndocumentation, blog posts, or other. For contributions to this repository,\nplease read the [contributor guidelines](CONTRIBUTING.md). Additional information:\n- [Developer guidelines](DEVELOPING.md)\n- [Notes on the build system](README.buildsys.md)\n\nTo keep up to date on GAP news (discussion of problems, release announcements,\nbug fixes), you can subscribe to the\n[GAP forum](https://www.gap-system.org/Contacts/Forum/forum.html) and\n[GAP development](https://lists.uni-kl.de/gap/info/gap) mailing lists,\nnotifications on GitHub, and chat with us on [Slack](https://gap-system.org/slack).\n\nIf you have any questions about working with GAP, you can ask them on\n[GAP forum](https://www.gap-system.org/Contacts/Forum/forum.html) (requires subscription)\nor [GAP support](https://www.gap-system.org/Contacts/People/supportgroup.html) mailing lists.\n\nPlease tell us about your use of GAP in research or teaching. We maintain a\n[bibliography of publications citing GAP](https://www.gap-system.org/Doc/Bib/bib.html).\nPlease [help us](https://www.gap-system.org/Contacts/publicationfeedback.html)\nkeeping it up to date.\n\n\n# License\n\nGAP is free software; you can redistribute it and/or modify it under the terms\nof the GNU General Public License as published by the Free Software\nFoundation; either version 2 of the License, or (at your option) any later\nversion. For details, please refer to the GAP reference manual, as well as the\nfile `LICENSE` in the root directory of the GAP distribution or see\n<https://www.gnu.org/licenses/gpl.html>.\n",777,mathematics,GAP,15,GAP,Shell,Makefile,HTML,CSS,TeX,Perl,C,C++,M4,Scilab,Python,Vim Script,Ruby,JavaScript,,,,,,,,,,,,,,3981,368,3547,66,12,61,411,438097,161,1759,1332,427,81659b10d21d0d3dee49e3c24637b2eaf05a2e5c,Add test for `GAPInfo.Date` format (#5757),2024-07-01T08:02:51Z,Lars Göttgens,lars.goettgens@rwth-aachen.de,lgoettgens,v4.13.1,For an overview of changes in GAP 4.13.1 see the [CHANGES.md](https://github.com/gap-system/gap/blob/v4.13.1/CHANGES.md) file.,v4.13.1,,,github-actions[bot],GNU General Public License v2.0,gap,gap-system,12,group-theory,computer-algebra,discrete-mathematics,computer-algebra-system,math,mathematics,algebra,representation-theory,,,,,,,,,,,,,/gap-system/gap,116,37,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/galacean/engine,https://github.com/galacean/engine,0,,,0,0,1,0,0,0,0,1,0,0,0,"A typescript interactive engine, support 2D, 3D, animation, physics, built on WebGL and glTF.","# Galacean Engine\n\n<a href=""https://www.npmjs.com/package/@galacean/engine""><img src=""https://img.shields.io/npm/v/@galacean/engine""/></a>\n![npm-size](https://img.shields.io/bundlephobia/minzip/@galacean/engine)\n![npm-download](https://img.shields.io/npm/dm/@galacean/engine)\n[![codecov](https://codecov.io/gh/galacean/engine/branch/main/graph/badge.svg?token=KR2UBKE3OX)](https://codecov.io/gh/galacean/engine)\n\n[Galacean](https://galacean.antgroup.com/editor) is a **web-first** and **mobile-first** high-performance real-time interactive engine. Use **component system design** and pursue ease of use and light weight. Developers can independently use and write Typescript scripts to develop projects using pure code.\n\n## Features\n\n- 🖥 &nbsp;**Platform** - Support HTML5 and Alipay miniprogram\n- 🔮 &nbsp;**Graphics** - Advanced 2D + 3D graphics engine\n- 🏃 &nbsp;**Animation** - Powerful animation system\n- 🧱 &nbsp;**Physics** - Powerful and easy-to-use physical features\n- 👆 &nbsp;**Input** - Easy-to-use interactive capabilities\n- 📑 &nbsp;**Scripts** - Use TypeScript to write logic efficiently\n\n## npm\n\nThe engine is published on npm with full typing support. To install, use:\n\n```sh\nnpm install @galacean/engine\n```\n\nThis will allow you to import engine entirely using:\n\n```javascript\nimport * as GALACEAN from ""@galacean/engine"";\n```\n\nor individual classes using:\n\n```javascript\nimport { Engine, Scene, Entity } from ""@galacean/engine"";\n```\n\n## Usage\n\n```typescript\n// Create engine by passing in the HTMLCanvasElement id and adjust canvas size\nconst engine = await WebGLEngine.create({ canvas: ""canvas-id"" });\nengine.canvas.resizeByClientSize();\n\n// Get scene and create root entity\nconst scene = engine.sceneManager.activeScene;\nconst rootEntity = scene.createRootEntity(""Root"");\n\n// Create light\nconst lightEntity = rootEntity.createChild(""Light"");\nconst directLight = lightEntity.addComponent(DirectLight);\nlightEntity.transform.setRotation(-45, -45, 0);\ndirectLight.intensity = 0.4;\n\n// Create camera\nconst cameraEntity = rootEntity.createChild(""Camera"");\ncameraEntity.addComponent(Camera);\ncameraEntity.transform.setPosition(0, 0, 12);\n\n// Create sphere\nconst meshEntity = rootEntity.createChild(""Sphere"");\nconst meshRenderer = meshEntity.addComponent(MeshRenderer);\nconst material = new BlinnPhongMaterial(engine);\nmeshRenderer.setMaterial(material);\nmeshRenderer.mesh = PrimitiveMesh.createSphere(engine, 1);\n\n// Run engine\nengine.run();\n```\n\n## Contributing\n\nEveryone is welcome to join us! Whether you find a bug, have a great feature request or you fancy owning a task from the road map feel free to get in touch.\n\nMake sure to read the [Contributing Guide](.github/HOW_TO_CONTRIBUTE.md) / [贡献指南](https://github.com/galacean/engine/wiki/%E5%A6%82%E4%BD%95%E4%B8%8E%E6%88%91%E4%BB%AC%E5%85%B1%E5%BB%BA-Oasis-%E5%BC%80%E6%BA%90%E4%BA%92%E5%8A%A8%E5%BC%95%E6%93%8E) before submitting changes.\n\n## Clone\nPrerequisites:\n- [git-lfs](https://git-lfs.com/) (Install by official website)\n  \nClone this repository:\n\n```sh\ngit clone git@github.com:galacean/runtime.git\n```\n\n## Build\n\nPrerequisites:\n\n- [Node.js v15.0.0+](https://nodejs.org/en/) and NPM (Install by official website)\n- [PNPM](https://pnpm.io/) (Install globally by `npm install -g pnpm`)\n\nIn the folder where you have cloned the repository, install the build dependencies using pnpm:\n\n```sh\npnpm install\n```\n\nThen, to build the source, using npm:\n\n```sh\nnpm run b:all\n```\n\n## Links\n\n- [Official Site](https://galacean.antgroup.com/)\n- [Examples](https://galacean.antgroup.com/#/examples/latest)\n- [Documentation](https://galacean.antgroup.com/#/docs/latest/en/install)\n- [API References](https://galacean.antgroup.com/#/api/latest)\n\n## License\n\nThe engine is released under the [MIT](https://opensource.org/licenses/MIT) license. See LICENSE file.\n",4045,graphics,TypeScript,7,JavaScript,TypeScript,GLSL,Shell,HTML,Sass,EJS,,,,,,,,,,,,,,,,,,,,,,1440,166,1243,31,15,32,0,22476,289,708,399,309,4fbf848cc81a7f7072aae05edecf3f6dd035394a,Update release.yml,2024-07-19T05:21:30Z,ChenMo,gl3336563@163.com,GuoLei1990,v1.2.0-beta.6,<!-- Release notes generated using configuration in .github/release.yml at v1.2.0-beta.6 -->\r\n\r\n## What's Changed\r\n### Features 🎉\r\n* Optimize Shader default texture color by @GuoLei1990 in https://github.com/galacean/engine/pull/2114\r\n### Fixed 🐞\r\n* Add `Script` in `onStart` may cause some functions to not be executed correctly. by @cptbtptpbcptdtptp in https://github.com/galacean/engine/pull/2102\r\n* Fix use default material to prevent material[] by @zhuxudong in https://github.com/galacean/engine/pull/2104\r\n* Fix array index loss after compilation by @Sway007 in https://github.com/galacean/engine/pull/2111\r\n\r\n**Full Changelog**: https://github.com/galacean/engine/compare/v1.2.0-beta.5...v1.2.0-beta.6,v1.2.0-beta.6,AZhan,,cptbtptpbcptdtptp,MIT License,engine,galacean,253,webgl,typescript,graphics,javascript,2d,3d,animation,input,physics,web3d,html5,webgl2,,,,,,,,,/galacean/engine,439,66,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/fulcrumgenomics/fgbio,https://github.com/fulcrumgenomics/fgbio,0.5,"Scientific, but package",1,1,1,1,1,0,0,0,0,0,0,1,Tools for working with genomic and high throughput sequencing data.,"[![Build Status](https://github.com/fulcrumgenomics/fgbio/workflows/unit%20tests/badge.svg)](https://github.com/fulcrumgenomics/fgbio/actions?query=workflow%3A%22unit+tests%22)\n[![codecov](https://codecov.io/gh/fulcrumgenomics/fgbio/branch/main/graph/badge.svg)](https://codecov.io/gh/fulcrumgenomics/fgbio)\n[![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.fulcrumgenomics/fgbio_2.13/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.fulcrumgenomics/fgbio_2.13)\n[![Bioconda](https://img.shields.io/conda/dn/bioconda/fgbio.svg?label=Bioconda)](http://bioconda.github.io/recipes/fgbio/README.html)\n[![Javadocs](http://javadoc.io/badge/com.fulcrumgenomics/fgbio_2.13.svg)](http://javadoc.io/doc/com.fulcrumgenomics/fgbio_2.13)\n[![License](http://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/fulcrumgenomics/fgbio/blob/main/LICENSE)\n[![Language](http://img.shields.io/badge/language-scala-brightgreen.svg)](http://www.scala-lang.org/)\n[![DOI](https://zenodo.org/badge/53011104.svg)](https://zenodo.org/doi/10.5281/zenodo.10456900)\n\nfgbio\n====\n\nA set of tools to analyze genomic data with a focus on Next Generation Sequencing.\n\nThis readme document is mostly for developers/contributors and those attempting to build the project from source.\nDetailed user documentation is available on the [project website](http://fulcrumgenomics.github.io/fgbio/) including [tool usage](http://fulcrumgenomics.github.io/fgbio/tools/latest) and [documentation of metrics produced](http://fulcrumgenomics.github.io/fgbio/metrics/latest).  Detailed developer documentation can be found [here](http://javadoc.io/doc/com.fulcrumgenomics/fgbio_2.13).\n\n<!---toc start-->\n  * [Quick Installation](#quick-installation)\n  * [Goals](#goals)\n  * [Overview](#overview)\n  * [List of tools](#list-of-tools)\n  * [Building](#building)\n  * [Command line](#command-line)\n  * [Include fgbio in your project](#include-fgbio-in-your-project)\n  * [Contributing](#contributing)\n  * [Authors](#authors)\n  * [License](#license)\n  * [Sponsorship](#sponsorship)\n\n<!---toc end-->\n\n# Quick Installation\n\nThe [conda](https://conda.io/) package manager (configured with [bioconda channels](https://bioconda.github.io/)) can be used to quickly install fgbio:\n\n```\nconda install fgbio\n```\n\nTo install fgbio without extra dependencies (e.g. [R](https://www.r-project.org/)), use the command:\n\n```\nconda install fgbio-minimal\n```\n\n# Goals\n\nThere are many toolkits available for analyzing genomic data; fgbio does not aim to be all things to all people but is specifically focused on providing:\n\n* Robust, well-tested tools.\n* An easy to use command-line.\n* Clear and thorough documentation for each tool.\n* Open source development for the benefit of the community and our clients.\n\n## Overview\n\nFgbio is a set of command line tools to perform bioinformatic/genomic data analysis. \nThe collection of tools within `fgbio` are used by our customers and others both for ad-hoc data analysis and within production pipelines.\nThese tools typically operate on read-level data (ex. FASTQ, SAM, or BAM) or variant-level data (ex. VCF or BCF).\nThey range from simple tools to filter reads in a BAM file, to tools to compute consensus reads from reads with the same molecular index/tag.\nSee the [list of tools](#list-of-tools) for more detail on the tools\n\n## List of tools\n\nFor a full list of available tools please see the [tools section](http://fulcrumgenomics.github.io/fgbio/tools/latest) of the project website.\n\nBelow we highlight a few tools that you may find useful.\n\n-   Tools for working with Unique Molecular Indexes (UMIs, aka Molecular IDs or Molecular Barcodes):\n    -   Annotate/Extract Umis from read-level data: [`FastqToBam`][fgbio-fastqtobam-link], [`AnnotateBamWithUmis`][fgbio-annotatebamwithumis-link], [`ExtractUmisFromBam`][fgbio-extractumisfrombam-link], and [`CopyUmiFromReadName`][fgbio-copyumifromreadname-link].\n    -   Manipulate read-level data containing Umis: [`CorrectUmis`][fgbio-correctumis-link], [`GroupReadsByUmi`][fgbio-groupreadsbyumi-link], [`CallMolecularConsensusReads`][fgbio-callmolecularconsensusreads-link], [`CallDuplexConsensusReads`][fgbio-callduplexconsensusreads-link], and [`FilterConsensusReads`][fgbio-filterconsensusreads-link].\n    -   Collect metrics and review consensus reads: [`CollectDuplexSeqMetrics`][fgbio-collectduplexseqmetrics-link] and [`ReviewConsensusVariants`][fgbio-reviewconsensusvariants-link].\n-   Tools to manipulate read-level data:\n    -   Fastq Manipulation: [`FastqToBam`][fgbio-fastqtobam-link], [`ZipperBams`][fgbio-zipperbams-link], and [`DemuxFastqs`][fgbio-demuxfastqs-link] (see `[fqtk`][fqtk-link], our rust re-implementation for sample demultiplexing).\n    -   Filter, clip, randomize, sort, and update metadata for read-level data: [`FilterBam`][fgbio-filterbam-link], [`ClipBam`][fgbio-clipbam-link], [`RandomizeBam`][fgbio-randomizebam-link], [`SortBam`][fgbio-sortbam-link], [`SetMateInformation`][fgbio-setmateinformation-link] and [`UpdateReadGroups`][fgbio-updatereadgroups-link].\n-   Tools for quality control assessment:\n    -   Detailed substitution error rate evaluation: [`ErrorRateByReadPosition`][fgbio-errorratebyreadposition-link].\n    -   Sample pooling QC: [`EstimatePoolingFractions`]: [fgbio-estimatepoolingfractions-link].\n    -   Splice-aware insert size QC for RNA-seq libraries: [`EstimateRnaSeqInsertSize`][fgbio-estimaternaseqinsertsize-link].\n-   Tools for adding or manipulating alternate contig names:\n    -   Extract contig names from an NCBI Assembly Report: [`CollectAlternateContigNames`][fgbio-collectalternatecontignames-link].\n    -   Update contig names in common file formats: [`UpdateFastaContigNames`][fgbio-updatefastacontignames-link], [`UpdateVcfContigNames`][fgbio-updatevcfcontignames-link], [`UpdateGffContigNames`][fgbio-updategffcontignames-link], [`UpdateIntervalListContigNames`][fgbio-updateintervallistcontignames-link], [`UpdateDelimitedFileContigNames`][fgbio-updatedelimitedfilecontignames-link].\n-   Miscellaneous tools:\n    -   Pick molecular indices (ex. sample barcodes, or molecular indexes): [`PickIlluminaIndices`][fgbio-pickilluminaindices-link] and [`PickLongIndices`][fgbio-picklongindices-link].\n    -   Find technical/synthetic, or switch-back sequences in read-level data: [`FindTechnicalReads`][fgbio-findtechnicalreads-link] and [`FindSwitchbackReads`][fgbio-findswitchbackreads-link].\n    -   Make synthetic mixture VCFs: [`MakeMixtureVcf`][fgbio-makemixturevcf-link] and [`MakeTwoSampleMixtureVcf`][fgbio-maketwosamplemixturevcf-link].\n\n[fgbio-fastqtobam-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/FastqToBam.html\n[fgbio-annotatebamwithumis-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/AnnotateBamWithUmis.html\n[fgbio-extractumisfrombam-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/ExtractUmisFromBam.html\n[fgbio-copyumifromreadname-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/CopyUmiFromReadName.html\n[fgbio-correctumis-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/CorrectUmis.html\n[fgbio-groupreadsbyumi-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/GroupReadsByUmi.html\n[fgbio-callmolecularconsensusreads-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/CallMolecularConsensusReads.html\n[fgbio-callduplexconsensusreads-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/CallDuplexConsensusReads.html\n[fgbio-filterconsensusreads-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/FilterConsensusReads.html\n[fgbio-collectduplexseqmetrics-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/CollectDuplexSeqMetrics.html\n[fgbio-reviewconsensusvariants-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/ReviewConsensusVariants.html\n[fgbio-fastqtobam-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/FastqToBam.html\n[fgbio-zipperbams-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/ZipperBams.html\n[fgbio-demuxfastqs-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/DemuxFastqs.html\n[fgbio-filterbam-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/FilterBam.html\n[fgbio-clipbam-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/ClipBam.html\n[fgbio-randomizebam-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/RandomizeBam.html\n[fgbio-setmateinformation-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/SetMateInformation.html\n[fgbio-updatereadgroups-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/UpdateReadGroups.html\n[fgbio-collectalternatecontignames-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/CollectAlternateContigNames.html\n[fgbio-updatefastacontignames-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/UpdateFastaContigNames.html\n[fgbio-updatevcfcontignames-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/UpdateVcfContigNames.html\n[fgbio-updategffcontignames-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/UpdateGffContigNames.html\n[fgbio-updateintervallistcontignames-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/UpdateIntervalListContigNames.html\n[fgbio-updatedelimitedfilecontignames-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/UpdateDelimitedFileContigNames.html\n[fgbio-errorratebyreadposition-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/ErrorRateByReadPosition.html\n[fgbio-estimatepoolingfractions-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/EstimatePoolingFractions.html\n[fgbio-estimaternaseqinsertsize-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/EstimateRnaSeqInsertSize.html\n[fgbio-pickilluminaindices-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/PickIlluminaIndices.html\n[fgbio-picklongindices-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/PickLongIndices.html\n[fgbio-findtechnicalreads-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/FastqToBam.html\n[fgbio-sortbam-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/SortBam.html\n[fgbio-makemixturevcf-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/MakeMixtureVcf.html\n[fgbio-maketwosamplemixturevcf-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/MakeTwoSampleMixtureVcf.html\n[fgbio-findswitchbackreads-link]: https://fulcrumgenomics.github.io/fgbio/tools/latest/FindSwitchbackReads.html\n\n## Building \n### Cloning the Repository\n\n[Git LFS](https://git-lfs.github.com/) is used to store large files used in testing fgbio.  In order to compile and run tests it is necessary to [install git lfs](https://git-lfs.github.com/).  To retrieve the large files either:\n\n1. Clone the repository _after_ installing git lfs, or\n2. In a previously cloned repository run `git lfs pull` once\n\nAfter initial setup regular git commands (e.g. `pull`, `fetch`, `push`) will also operate on large files and no special handling is needed.\n\nTo clone the repository: `git clone https://github.com/fulcrumgenomics/fgbio.git`\n\n### Running the build\nfgbio is built using [sbt](http://www.scala-sbt.org/).\n\nUse ```sbt assembly``` to build an executable jar in ```target/scala-2.13/```.\n\nTests may be run with ```sbt test```.\n\nJava SE 8 is required.\n\n\n## Command line\n\n`java -jar target/scala-2.13/fgbio-<version>.jar` to see the commands supported.  Use `java -jar target/scala-2.13/fgbio-<version>.jar <command>` to see the help message for a particular command.\n\n## Include fgbio in your project\n\nYou can include `fgbio` in your project using:\n\n```\n""com.fulcrumgenomics"" %% ""fgbio"" % ""1.0.0""\n```\n\nfor the latest released version or (buyer beware):\n\n```\n""com.fulcrumgenomics"" %% ""fgbio"" % ""0.9.0-<commit-hash>-SNAPSHOT""\n```\n\nfor the latest development snapshot.\n\n## Contributing\n\nContributions are welcome and encouraged.\nWe will do our best to provide an initial response to any pull request or issue within one-week.\nFor urgent matters, please contact us directly.\n\n## Authors\n\n* [Tim Fennell](https://github.com/tfenne) (maintainer)\n* [Nils Homer](https://github.com/nh13) (maintainer)\n\n## License\n\n`fgbio` is open source software released under the [MIT License](https://github.com/fulcrumgenomics/fgbio/blob/main/LICENSE).\n\n## Sponsorship\n\n### Become a sponsor\n\nAs a free and open source project, `fgbio` relies on the support of the community of users for its development. If you work for an organization that uses and benefits from `fgbio`, please consider supporting `fgbio`. There are different ways, such as employing people to work on `fgbio`, funding the project, or becoming a [sponsor](https://github.com/sponsors/fulcrumgenomics) to support the broader ecosystem. Please [contact@fulcrumgenomics.com](https://www.fulcrumgenomics.com/contact/) to discuss.\n\n### Sponsors\n\nSponsors provide support for `fgbio` through direct funding or employing contributors.\nPublic sponsors include:\n\n<p>\n<a href float=""left""=""https://fulcrumgenomics.com""><img src="".github/logos/fulcrumgenomics.svg"" alt=""Fulcrum Genomics"" height=""35""/></a>\n&nbsp;\n<a href float=""left""=""https://twinstrandbio.com/""><img src="".github/logos/twinstrandbio.svg"" alt=""TwinStrand Biosciences"" height=""45""/></a>\n&nbsp;\n<a href float=""left""=""https://www.jumpcodegenomics.com//""><img src="".github/logos/jumpcodegenomics.png"" alt=""Jumpcode Genomics"" height=""30""/></a>\n&nbsp;\n<a href float=""left""=""https://www.igenomx.com//""><img src="".github/logos/igenomx.png"" alt=""iGenomX"" height=""30""/></a>\n&nbsp;\n<a href float=""left""=""https://myriad.com""><img src="".github/logos/myriad.png"" alt=""Myriad Genetics"" height=""35""/></a>\n&nbsp;\n<a href float=""left""=""https://missionbio.com""><img src="".github/logos/missionbio.svg"" alt=""Mission Bio"" height=""30""/></a>\n&nbsp;\n<a href float=""left""=""https://singulargenomics.com""><img src="".github/logos/singulargenomics.svg"" alt=""Singular Genomics"" height=""30""/></a>\n&nbsp;\n<a href float=""left""=""https://verogen.com""><img src="".github/logos/verogen.jpg"" alt=""Verogen"" height=""30""/></a>\n&nbsp;\n<a href float=""left""=""https://idtdna.com""><img src="".github/logos/idtdna.png"" alt=""Integrated DNA Technologies"" height=""30""/></a>\n&nbsp;\n<a href float=""left""=""https://strataoncology.com""><img src="".github/logos/strataoncology.png"" alt=""Strata Oncology"" height=""30""/></a>\n</p>\n\nThe full list of sponsors supporting `fgbio` is available in the [sponsor](https://github.com/sponsors/fulcrumgenomics) page.\n\n",305,bioinformatics,Scala,4,Java,Scala,Shell,R,,,,,,,,,,,,,,,,,,,,,,,,,596,55,509,32,65,28,116,9598,67,405,314,91,9a4b01d45a38c4990bff2b011cba7abdde134190,feat: SequenceMetadata can have name and length looked up by key (#1002),2024-07-16T18:26:15Z,Nils Homer,nh13@users.noreply.github.com,nh13,Release 2.3.0,"## What's Changed\r\n* Add a Zenodo DOI to the README by @nh13 in https://github.com/fulcrumgenomics/fgbio/pull/955\r\n* PileupBuilder should not report insertions when checking the final mapped base before soft-clipping by @jrm5100 in https://github.com/fulcrumgenomics/fgbio/pull/956\r\n* fix: ensure that the GroupReadsByUmiTests for marking duplicates are by @nh13 in https://github.com/fulcrumgenomics/fgbio/pull/962\r\n* fix: mapped header records should overwrite unmapped in ZipperBams by @nh13 in https://github.com/fulcrumgenomics/fgbio/pull/963\r\n* Fix typo in ExtractUmisFromBam.scala by @nh13 in https://github.com/fulcrumgenomics/fgbio/pull/966\r\n* ZipperBams to produce mate score (""ms"") for samtools markdup by @nh13 in https://github.com/fulcrumgenomics/fgbio/pull/952\r\n* Make `PileupBuilder.includeMapPositionsOutsideFrInsert` intuitively correct by @clintval in https://github.com/fulcrumgenomics/fgbio/pull/981\r\n* doc: update description of consenus tags for duplex by @nh13 in https://github.com/fulcrumgenomics/fgbio/pull/983\r\n* Update UpdateGffContigNames.scala (typo in docs) by @yfarjoun in https://github.com/fulcrumgenomics/fgbio/pull/985\r\n* Add conda install instructions to README by @clintval in https://github.com/fulcrumgenomics/fgbio/pull/988\r\n* Add TemplateCoordinate sort order to the usage of SortBam by @nh13 in https://github.com/fulcrumgenomics/fgbio/pull/993\r\n* Create CODEOWNERS by @nh13 in https://github.com/fulcrumgenomics/fgbio/pull/999\r\n* feat: add --umi-prefix to CopyUmiFromReadName by @msto in https://github.com/fulcrumgenomics/fgbio/pull/958\r\n* Validate IO in SortBam to provide nicer exceptions by @nh13 in https://github.com/fulcrumgenomics/fgbio/pull/994\r\n* Improve the list of tools in the README.md by @nh13 in https://github.com/fulcrumgenomics/fgbio/pull/991\r\n* doc: fix duplicate ""the"" in sequence dictionary docstrings by @nh13 in https://github.com/fulcrumgenomics/fgbio/pull/1000\r\n\r\n## New Contributors\r\n* @yfarjoun made their first contribution in https://github.com/fulcrumgenomics/fgbio/pull/985\r\n* @msto made their first contribution in https://github.com/fulcrumgenomics/fgbio/pull/958\r\n\r\n**Full Changelog**: https://github.com/fulcrumgenomics/fgbio/compare/2.2.1...2.3.0",02.03.2000,Nils Homer,,nh13,MIT License,fgbio,fulcrumgenomics,30,analyzing-genomic-data,umi,bioinformatics,ngs,scala,fgbio,molecular-indexes,genomics,,,,,,,,,,,,,/fulcrumgenomics/fgbio,32,21,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/fslaborg/FSharp.Stats,https://github.com/fslaborg/FSharp.Stats,0,Like a collection of algorithms instead of a project,,0,0,1,0,0,0,1,0,0,0,0,"statistical testing, linear algebra, machine learning, fitting and signal processing in F#","![](docs/img/logo_title.svg)\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6337056.svg)](https://doi.org/10.5281/zenodo.6337056)\n[![Discord](https://img.shields.io/discord/836161044501889064?color=purple&label=Join%20our%20Discord%21&logo=discord&logoColor=white)](https://discord.gg/y95XRJg23e)\n[![Generic badge](https://img.shields.io/badge/Made%20with-FSharp-rgb(1,143,204).svg)](https://shields.io/)\n![GitHub contributors](https://img.shields.io/github/contributors/CSBiology/FSharp.Stats)\n[![Build status](https://ci.appveyor.com/api/projects/status/gjsjlqmrljtty780/branch/developer?svg=true)](https://ci.appveyor.com/project/kMutagene/fsharp-stats/branch/developer)\n[![codecov](https://codecov.io/gh/fslaborg/FSharp.Stats/branch/developer/graph/badge.svg?token=LRBZPV6MH8)](https://codecov.io/gh/fslaborg/FSharp.Stats)\n\nFSharp.Stats is a multipurpose project for statistical testing, linear algebra, machine learning, fitting and signal processing.\n\n### Amongst others, following functionalities are covered:\n\n#### Descriptive statistics\n  - <a href=""https://fslab.org/FSharp.Stats/BasicStats.html"">Measures of central tendency</a>\n  - <a href=""https://fslab.org/FSharp.Stats/BasicStats.html"">Measures of dispersion</a>\n  - <a href=""https://fslab.org/FSharp.Stats/Correlation.html"">Correlation</a>\n  - <a href=""https://fslab.org/FSharp.Stats/Quantiles.html"">Quantile/Rank</a>\n  - <a href=""https://fslab.org/FSharp.Stats/Distributions.html"">Distribution</a>\n\n#### Fitting\n  - <a href=""https://fslab.org/FSharp.Stats/Fitting.html#Linear-Regression"">Linear regression</a>\n    - <a href=""https://fslab.org/FSharp.Stats/Fitting.html#Simple-Linear-Regression"">Simple linear regression (weighted and constrained)</a>\n    - <a href=""https://fslab.org/FSharp.Stats/Fitting.html#Polynomial-Regression"">Polynomial regression (weighted and constrained)</a>\n  - <a href=""https://fslab.org/FSharp.Stats/Fitting.html#Nonlinear-Regression"">Nonlinear regression</a>\n  - <a href=""https://fslab.org/FSharp.Stats/Fitting.html#Smoothing-spline"">Spline regression</a>\n  - <a href=""https://fslab.org/FSharp.Stats/GoodnessOfFit.html"">Goodness of fit</a>\n\n#### Interpolation\n  - <a href=""https://fslab.org/FSharp.Stats/Interpolation.html#Polynomial-Interpolation"">Linear spline interpolation</a>\n  - <a href=""https://fslab.org/FSharp.Stats/Interpolation.html#Polynomial-Interpolation"">Polynomial interpolation</a>\n  - <a href=""https://fslab.org/FSharp.Stats/Interpolation.html#Cubic-interpolating-Spline"">Cubic spline interpolation</a>\n  - <a href=""https://fslab.org/FSharp.Stats/Interpolation.html"">Akima subspline interpolation</a>\n  - <a href=""https://fslab.org/FSharp.Stats/Interpolation.html"">Hermite subspline interpolation</a>\n\n\n#### Signal processing\n  - <a href=""https://fslab.org/FSharp.Stats/Signal.html#Continuous-Wavelet"">Continuous wavelet transform</a>\n  - <a href=""https://fslab.org/FSharp.Stats/Signal.html"">Smoothing filters</a>\n  - Peak detection\n\n  \n#### Linear Algebra\n  - Singular value decomposition\n  \n#### Machine learning\n  - <a href=""https://fslab.org/FSharp.Stats/ML.html"">PCA</a>\n  - <a href=""https://fslab.org/FSharp.Stats/Clustering.html"">Clustering</a>\n  - Surprisal analysis\n  \n#### Optimization\n  - Brent minimization\n  - Bisection\n  - [Nelder Mead](https://fslab.org/FSharp.Stats/Optimization.html#Nelder-Mead)\n  \n#### Statistical testing\n  - <a href=""https://fslab.org/FSharp.Stats/Testing.html#T-Test"">t test</a>, <a href=""https://fslab.org/FSharp.Stats/Testing.html#H-Test"">H test</a>, etc.<br>\n  - <a href=""https://fslab.org/FSharp.Stats/Testing.html#Anova"">ANOVA</a><br>\n  - <a href=""https://fslab.org/FSharp.Stats/Testing.html#PostHoc"">Post hoc tests</a><br>\n  - <a href=""https://fslab.org/FSharp.Stats/Testing.html#Q-Value"">q values</a><br>\n  - <a href=""https://fslab.org/FSharp.Stats/Testing.html#SAM"">SAM</a><br>\n  - RMT\n\n\n## Documentation\n\nIndepth explanations, tutorials and general information about the project can be found [here](https://fslab.org/FSharp.Stats) or at [fslab](https://fslab.org/).\nThe documentation and tutorials for this library are automatically generated (using the F# Formatting) from *.fsx and *.md files in the docs folder. If you find a typo, please submit a pull request!\n\n\n## Contributing\n\nPlease refer to the [Contribution guidelines](.github/CONTRIBUTING.md).\n\n## Development\n\nto build the project, run either `build.cmd` or `build.sh` depending on your OS.\n\nbuild targets are defined in the modules of /build/build.fsproj. \n\nSome interesting targets may be:\n\n  - `./build.cmd runtests` will build the project and run tests\n  - `./build.cmd watchdocs` will build the project, run tests, and build and host a local version of the documentation.\n  - `./build.cmd release` will start the full release pipeline.\n\n\n## Library license\n\nThe library is available under Apache 2.0. For more information see the License file in the GitHub repository.\n\n## Citation\n\nFSharp.Stats can be cited using its [zenodo record](https://zenodo.org/record/7568568). \n\nGlobal FSharp.Stats reference:\n> Benedikt Venn, Lukas Weil, Kevin Schneider, David Zimmer & Timo Mühlhaus. (2022). fslaborg/FSharp.Stats. Zenodo. https://doi.org/10.5281/zenodo.6337056 \n\nLatest release reference (0.5.0):\n> Benedikt Venn, Lukas Weil, Kevin Schneider, David Zimmer & Timo Mühlhaus. (2023). fslaborg/FSharp.Stats: Release 0.5.0 (0.5.0). Zenodo. https://doi.org/10.5281/zenodo.8215188",206,mathematics,F#,5,Batchfile,F#,Shell,PowerShell,Jupyter Notebook,,,,,,,,,,,,,,,,,,,,,,,,143,11,126,6,8,36,71,175464,54,184,131,53,c5444cc52c2e58b92d34f10c0c6de90fc4625433,Merge pull request #333 from kevmal/shadowedops,2024-07-03T09:51:38Z,Benedikt Venn,b.venn@gmx.de,bvenn,Release 0.5.0,## Nuget\r\nThe nuget package is available at [www.nuget.org/packages/FSharp.Stats/0.5.0](http://www.nuget.org/packages/FSharp.Stats/0.5.0)\r\n\r\n### 0.4.11+0c6567d4 (Released 2023-2-6)\r\n\r\n* add various XML comments in Interpolation and Fitting modules\r\n* BREAKING: major refactor of Fitting and Interpolation module\r\n  * separate CubicSpline.Simple and CubicSpline.Akima\r\n  * rename coefficient to fit and fit to predict\r\n* addition of LinearRegression and Interpolation type\r\n* [[#8ab0975](https://github.com/fslaborg/FSharp.Stats/commit/8ab0975e53933d27d894fc67651d0f01d924b6eb)] addition of clamped cubic spline\r\n* [[#e884a75e](https://github.com/fslaborg/FSharp.Stats/commit/e884a75e92c84372f0e727192e6419c9c591a3cd)] update FSharpAux version\r\n* update interval type,0.5.0,Benedikt Venn,,bvenn,Other,FSharp.Stats,fslaborg,15,fsharp,statistics,numerics,algebra,linear-algebra,mathematics,,,,,,,,,,,,,,,/fslaborg/FSharp.Stats,16,21,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/fplll/fplll,https://github.com/fplll/fplll,0,,,0,1,0,0,0,0,1,0,0,0,0,Lattice algorithms using floating-point arithmetic,"# fplll #\n\n[![Build Status](https://github.com/fplll/fplll/workflows/Tests/badge.svg)](https://github.com/fplll/fplll/actions?query=workflow%3ATests) [![codecov](https://codecov.io/gh/fplll/fplll/branch/master/graph/badge.svg)](https://codecov.io/gh/fplll/fplll)\n\n\nfplll contains implementations of several lattice algorithms. The implementation relies on floating-point orthogonalization, and LLL [[LLL82](#LLL82)] is central to the code, hence the name.\n\nIt includes implementations of floating-point LLL reduction algorithms [[NS09](#NS09),[MSV09](#MSV09)], offering different speed/guarantees ratios. It contains a 'wrapper' choosing the estimated best sequence of variants in order to provide a guaranteed output as fast as possible [[S09](#S09)]. In the case of the wrapper, the succession of variants is oblivious to the user. \n\nIt includes an implementation of the BKZ reduction algorithm [[SE94](#SE94)], including the BKZ-2.0 [[CN11](#CN11)] improvements (extreme enumeration pruning, pre-processing of blocks, early termination). Additionally, Slide reduction [[GN08](#GN08)] and self dual BKZ [[MW16](#MW16)] are supported. \n\nIt also includes a floating-point implementation of the Kannan-Fincke-Pohst algorithm [[K83](#K83),[FP85](#FP85)] that finds a shortest non-zero lattice vector. Finally, it contains a variant of the enumeration algorithm that computes a lattice vector closest to a given vector belonging to the real span of the lattice.\n\nfplll is distributed under the [GNU Lesser General Public License](COPYING) (either version 2.1 of the License, or, at your option, any later version) as published by the Free Software Foundation.\n\n## How to cite ##\n\n	@unpublished{fplll,\n	    author = {The {FPLLL} development team},\n	    title = {{fplll}, a lattice reduction library, {Version}: 5.4.5},\n	    year = 2023,\n	    note = {Available at \url{https://github.com/fplll/fplll}},\n	    url = {https://github.com/fplll/fplll}\n	}\n\n\n# Table of contents #\n\n  * [fplll](#fplll)\n    * [How to cite](#How-to-cite)\n  * [Table of contents](#table-of-contents)\n  * [Compilation](#compilation)\n    * [Installation from packages](#Installation-from-packages)\n    	* [Ubuntu and Debian](#Ubuntu-and-Debian)\n    	* [Conda](#Conda)\n    	* [MacOS](#MacOS)\n    	* [Docker and AWS](#Docker-and-AWS)\n    * [Installation from source](#Installation-from-source)\n    	* [Dependencies](#dependencies)\n      		* [Required](#required), [Optional](#optional).\n      	* [Linux and MacOS](#Linux-and-MacOS)\n      	* [Windows 10](#windows-10)\n    * [Check](#check)\n    * [Optimization](#optimization)\n  * [How to use](#how-to-use)\n    * Programs [latticegen](#latticegen), [fplll](#fplll-1), [llldiff](#llldiff).\n    * [How to use as a library](#how-to-use-as-a-library)\n    * [Multicore support](#multicore-support)\n  * [Examples](#examples)\n  * [Alternative interfaces](#alternative-interfaces)\n  * [Credit](#credit)\n    * [Maintainers](#maintainers), [Contributors](#contributors), [Acknowledgments](#acknowledgments).\n  * [Contributing](#contributing)\n  * [New releases and bug reports](#new-releases-and-bug-reports)\n  * [Bibliography](#bibliography)\n\n\n# Compilation #\n\n## Installation from packages ##\n\nfplll is available as a pre-built package for a variety of operating systems; these pre-built packages typically include all mandatory dependencies, and so these packages can be used to start running fplll quickly.\n\nBelow, we give some instructions on how to install these packaged variants of fplll.\n\nNote that these packages will be up-to-date for the most recent version of fplll. However, if you want a feature that has recently been added to master (that is not yet in a release) then it is necessary to build from source. If this is the case, please see the [Installation from Source](#installation-from-source) section.\n\n### Ubuntu and Debian ###\nfplll can be installed directly via Aptitude or Synaptic. Both of these package managers package fplll in the package `fplll-tools`. Therefore, to install this package using Aptitude, run the following command\n\n``` \nsudo aptitude install fplll-tools\n```\n\nIf you want to use Synaptic, then you will need to search for the `fplll-tools` package using the search bar.\n\n\n### Conda ###\nfplll can be installed natively as a conda package using the following command\n\n```\nconda install fplll\n```\n\n### MacOS ###\n\nMacOS has a package for fplll inside HomeBrew. Assuming that you have HomeBrew installed, you may install fplll using the following command\n\n```\nbrew install fplll\n```\n\n### Docker and AWS ###\nWe now have Docker/AWS images for fplll too. They aren't on this repository, though; you can find them [here](https://hub.docker.com/u/fplll)\n\n## Installation from source ##\n\nfplll can also be built from source. Below, we explicate some of the dependencies for building from source, as well as operating systems specific instructions.\n\n### Dependencies ###\n\n#### Required ####\n\n- GNU MP 4.2.0 or higher [http://gmplib.org/](http://gmplib.org/) or MPIR 1.0.0 or higher [http://mpir.org](http://mpir.org)\n- MPFR 2.3.0 or higher, COMPLETE INSTALLATION [http://www.mpfr.org/](http://www.mpfr.org/)\n- autotools 2.61 or higher\n- g++ 4.9.3 or higher\n\n#### Optional ####\n- QD 2.3.15 or higher (a C++/Fortran-90 double-double and quad-double package), compile and install\n  the shared library (e.g. `./configure --enable-shared=yes`).\n  [http://crd-legacy.lbl.gov/~dhbailey/mpdist/](http://crd-legacy.lbl.gov/~dhbailey/mpdist/)\n  \nNOTE: If you are intending to use fplll on Windows 10, then these packages should be installed after the `Windows Subsystem for Linux` has been installed and activated. Please go to the [Windows 10](#windows-10) instructions for more information.\n\n### Linux and MacOS ###\n\nYou should download the source code from Github and then run\n\n    ./autogen.sh\n\nwhich generates the `./configure` script used to configure fplll by calling the appropriate\nautotools command.\n\nThen, to compile and install type\n\n	./configure\n	make\n	make install			# (as root)\n\nIf GMP, MPFR and/or MPIR are not in the `$LD_LIBRARY_PATH`, you have to point to the directories where the libraries are, with\n\n    ./configure --with-gmp=path/to/gmp\n\nor\n\n    ./configure --with-mpfr=path/to/mpfr\n\nThe same philosophy applies to the (optional) QD library. If you want to use\nmpir instead of gmp, use `--enable-mpir` and `--with-mpir=path/to/mpir`.\n\nYou can remove the program binaries and object files from the source code directory by typing `make\nclean`. To also remove the files that `./configure` created (so you can compile the package for a\ndifferent kind of computer), type `make distclean`.  By default, `make install` installs the package\ncommands under `/usr/local/bin`, include files under `/usr/local/include`, etc.  You can specify an\ninstallation directory name other than `/usr/local` by giving `./configure` the option\n`--prefix=dirname`.  Run `./configure --help` for further details.\n\n### Windows 10 ###\n\nWindows 10 has the ""Windows Subsystem for Linux""(WSL), which essentially allows you to use Linux features in Windows without the need for a dual-boot system or a virtual machine. To activate this, first go to **Settings** -> **Update and security** -> **For developers** and enable developer mode. (This may take a while.) Afterwards, open Powershell as an administrator and run \n\n	Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux\n\nThis will enable the WSL. Next, open the Windows Store and navigate to your favourite available Linux distribution - this may be installed as if it were a regular application. Afterwards, this system will act as a regular program, and so it can be opened however you like e.g. by opening command prompt and typing `bash`. With this Linux-like subsystem, installing fplll is then similar to above, except that most likely the package repository is not up to date, and various additional packages need to be installed first. To make sure you only install the most recent software, run:\n	\n	sudo apt-get update\n	\nThen run `sudo apt-get install <packages>` for the (indirectly) required packages, such as `make`, `autoconf`, `libtool`, `gcc`, `g++`, `libgmp-dev`, `libmpfr-dev` and `pkg-config`. Finally, download the fplll source code, extract the contents, navigate to this folder in Bash (commonly found under `/mnt/c/<local path>` when stored somewhere on the `C:\` drive), and run:\n	\n	./autogen.sh\n	./configure\n	make \n\nThe same comments as before apply for using e.g. `make install` or `make distclean` instead of `make`.\n\nNote: to fix a potential error `libfplll.so.5: cannot open shared object file: No such file or directory` raised after trying to run `fplll` after a successful compilation, find the location of `libfplll.so.5` (probably something like `/../fplll/.libs/`; run `find -name libfplll.so.5` to find it) and run `export LD_LIBRARY_PATH=<path>`. \n\n## Check ##\n\nType\n\n	make check\n\n\n## Optimization ##\n\nThe default compilation flag is `-O3`. One may use the `-march=native -O3` flag to optimize the binaries. See ""[this issue](https://github.com/fplll/fplll/issues/169)"" for its impact on the enumeration speed.\n\n\n# How to use #\n\nExecutable files `fplll` and `latticegen` are installed in the directory\n`/usr/local/bin`. (Note that the programs generated by `make` in the `fplll/` directory are only wrappers to the programs in `fplll/.libs/`).\n\nIf you type `make check`, it will also generate the executable file `llldiff`, \nin `fplll/.libs/`.\n\n\n## latticegen ##\n\n`latticegen` is a utility for generating matrices (rows form input\nlattice basis vectors).\n\nThe options are:\n\n* `r` `d` `b` : generates a knapsack like matrix of dimension d x (d+1) and b bits (see, e.g., [[S09](#S09)]): the i-th vector starts with a random integer of bit-length <=b and the rest is the i-th canonical unit vector.\n* `s` `d` `b` `b2` : generates a d x d matrix of a form similar to that is involved when trying to find rational approximations to reals with the same small denominator (see, e.g., [[LLL82](#LLL82)]): the first vector starts with a random integer of bit-length <=b2 and continues with d-1 independent integers of bit-lengths <=b; the i-th vector for i>1 is the i-th canonical unit vector scaled by a factor 2^b.\n* `u` `d` `b` : generates a d x d matrix whose entries are independent integers of bit-lengths <=b.\n* `n` `d` `b` `c` : generates an ntru-like matrix. If char is 'b', then it first samples an integer q of bit-length b, whereas if char is 'q', then it sets q to the provided value. Then it samples a uniform h in the ring Z_q[x]/(x^n-1). It finally returns the 2 x 2 block matrix [[I, Rot(h)], [0, q*I]], where each block is d x d, the first row of Rot(h) is the coefficient vector of h, and the i-th row of Rot(h) is the shift of the (i-1)-th (with last entry put back in first position), for all i>1. Warning: this does not produce a genuine ntru lattice with h a genuine public key (see [[HPS98](#HPS98)]).\n* `N` `d` `b` `c` : as the previous option, except that the constructed matrix is [[q*I, 0], [Rot(h), I]]. \n* `q` `d` `k` `b` `c` : generates a q-ary matrix. If char is 'b', then it first samples an integer q of bit-length b; if char is 'p', it does the same and updates q to the smallest (probabilistic) prime that is greater; if char is 'q', then it sets q to the provided value. It returns a 2 x 2 block matrix [[I, H], [0, q*I]], where H is (d-k) x k and uniformly random modulo q. These bases correspond to the SIS/LWE q-ary lattices (see [[MR09](#MR09)]). Goldstein-Mayer lattices correspond to k=1 and q prime (see [[GM03](#GM03)]).\n* `t` `d` `f` : generates a d x d lower-triangular matrix B with B_ii = 2^(d-i+1)^f for all i, and B_ij is uniform between -B_jj/2 and B_jj/2 for all j<i.\n* `T` `d` : also takes as input a d-dimensional vector vec read from a file. It generates a d x d lower-triangular matrix B with B_ii = vec[i] for all i and B_ij is uniform between -B_jj/2 and B_jj/2 for all j<i.\n\nThe generated matrix is printed in stdout.\n\nNote that by default, the random bits always use the same seed, to ensure reproducibility. The seed may be changed with the option `-randseed <integer>` or by using the current time (in seconds) `-randseed time`. If you use this option, it must be the first one on the command line.\n\n## fplll ##\n\n`fplll` does LLL, BKZ, HKZ or SVP on a matrix (considered as a set of row\nvectors) given in stdin or in a file as parameter. \n\nThe options are:\n\n* `-a lll` : LLL-reduction (default).\n* `-a bkz` : BKZ-reduction.\n* `-a hkz` : HKZ-reduction.\n* `-a svp` : prints a shortest non-zero vector of the lattice.\n* `-a sdb` : self dual variant of BKZ-reduction.\n* `-a sld` : slide reduction.\n* `-a cvp` : prints the vector in the lattice closest to the input vector.\n* `-v` : verbose mode. \n* `-nolll` : does not apply to LLL-reduction. In the case of bkz, hkz and svp, by default, the input basis is LLL-reduced before anything else. This option allows to remove that initial LLL-reduction (note that other calls to LLL-reduction may occur during the execution). In the case of hlll, verify if the input basis is HLLL-reduced.\n\n* `-a hlll` : HLLL-reduction.\n\nOptions for LLL-reduction:\n\n\n* `-d delta` :     δ (default=0.99)\n* `-e eta` :       η (default=0.51). See [[NS09](#NS09)] for the definition of (δ,η)-LLL-reduced bases. \n* `-l lovasz` :    if !=0 Lovasz's condition. Otherwise, Siegel's condition (default: Lovasz). See [[A02](#A02)] for the definition of Siegel condition.\n\n* `-f mpfr` : sets the floating-point type to MPFR (default if `m=proved`).\n* `-p precision` : precision of the floating-point arithmetic, works only with `-f mpfr`.\n* `-f dd` : sets the floating-point type to double-double.\n* `-f qd` : sets the floating-point type to quad-double.\n* `-f dpe` : sets the floating-point type to DPE (default if `m=heuristic`).\n* `-f double` : sets the floating-point type to double (default if `m=fast`).\n* `-f longdouble` : sets the floating-point type to long double.\n\n* `-z mpz` : sets the integer type to mpz, the integer type of GMP (default).\n* `-z int` : sets the integer type to int.\n* `-z long` : as `-z int`.\n* `-z double` : sets the integer type to double.\n\n* `-m wrapper` : uses the wrapper. (default if `z=mpz`).\n* `-m fast` : uses the fast method, works only with `-f double`.\n* `-m heuristic` : uses the heuristic method.\n* `-m proved` : uses the proved version of the algorithm.\n* `-y` : early reduction.\n\nWith the wrapper or the proved version, it is guaranteed that the basis is LLL-reduced with δ'=2×δ-1\nand η'=2×η-1/2. For instance, with the default options, it is guaranteed that the basis is\n(0.98,0.52)-LLL-reduced.\n\n\nOptions for BKZ-reduction:\n\n* `-b block_size` :            block size, mandatory, between 2 and the number of vectors.\n\n* `-f float_type` :            same as LLL (`-p` is required if `float_type=mpfr`).\n* `-p precision` :             precision of the floating-point arithmetic with `-f mpfr`.\n\n* `-bkzmaxloops loops` :       maximum number of full loop iterations.\n* `-bkzmaxtime time` :         stops after `time` seconds (up to completion of the current loop iteration).\n* `-bkzautoabort` :            stops when the average slope of the log ||b_i*||'s does not decrease fast enough.\n\nWithout any of the last three options, BKZ runs until no block has been updated for a full loop iteration.\n\n* `-s filename.json` :         use strategies for preprocessing and pruning parameter (/strategies/default.json provided). Experimental.\n\n* `-bkzghbound factor` :       multiplies the Gaussian heuristic by `factor` (of float type) to set the enumeration radius of the SVP calls.\n* `-bkzboundedlll` :	       restricts the LLL call before considering a block to vector indices within that block.\n\n* `-bkzdumpgso file_name` :     dumps the log ||b_i*|| 's in specified file.\n\nOutput formats:\n\n* `-of  ` : prints new line (if `-a [lll|bkz]`)\n* `-of b` : prints the basis (if `-a [lll|bkz]`, this value by default)\n* `-of bk` : prints the basis (if `-a [lll|bkz]`, format compatible with sage)\n* `-of c` : prints the closest vector (if `-a cvp`, this value by default)\n* `-of s` : prints the closest vector (if `-a svp`, this value by default)\n* `-of t` : prints status (if `-a [lll|bkz|cvp|svp]`)\n* `-of u` : prints unimodular matrix (if `-a [lll|bkz]`)\n* `-of uk` : prints unimodular matrix (if `-a [lll|bkz]`, format compatible with sage)\n* `-of v` : prints inverse of u (if `-a lll`)\n* `-of vk` : prints inverse of u (if `-a lll`, format compatible with sage)\n\nA combination of these option is allowed (e.g., `-of bkut`).\n\nOnly for `-a hlll`:\n* `-t theta` : θ (default=0.001). See [[MSV09](#MSV09)] for the definition of (δ,η,θ)-HLLL-reduced bases.\n* `-c c` : constant for HLLL during the size-reduction (only used if `fplll` is compiled with `-DHOUSEHOLDER_USE_SIZE_REDUCTION_TEST`)\n\n## llldiff ##\n\n`llldiff` compares two bases (b1,...,bd) and (c1,...c_d'): they are considered\nequal iff d=d' and for any i, bi = +- ci. Concretely, if basis B is in file 'B.txt' and if basis C is in file 'C.txt' (in the fplll format), then one may run `cat B.txt C.txt | ./llldiff`.\n\n\n## How to use as a library ##\n\nSee [API documentation](https://fplll.github.io/fplll/annotated.html) and [tests](https://github.com/fplll/fplll/tree/master/tests) as a source of examples.\n\n## Multicore support ##\nThis library does not currently use multiple cores and running multiple threads working on the same object `IntegerMatrix`, `LLLReduction`, `MatGSO` etc. is not supported.  Running multiple threads working on *different* objects, however, is supported. That is, there are no global variables and it is safe to e.g. reduce several lattices in parallel in the same process.\n\nAs an exception to the above, fplll has an implementation of parallel lattice point enumeration. \nTo enable this implementation, make sure you compile with the maximum parallel enumeration dimension greater than 0. Note that increasing this value will increase the compile-time due to the use of templates.\n\nAt present fplll does not contain strategies for multi-core pruned enumeration, and so speedups for pruned enumeration may be sub-linear (see [this](https://martinralbrecht.wordpress.com/2020/10/26/multicore-bkz-in-fplll/) for more information). On the other hand, unpruned enumeration appears to scale linearly.\n\n# Examples #\n\n1. LLL reduction\n\n   ``` \n   ./latticegen r 10 1000 | ./fplll\n   ``` \n\n2. Fileinput for reduction. If the file `matrix` contains\n\n   ``` \n   [[ 10 11]\n   [11 12]]\n   ``` \n\n   then\n\n   ``` \n   ./fplll matrix\n   ```\n\n   produces\n\n   ``` \n   [[0 1 ]\n    [1 0 ]\n   ]\n   ``` \n\n3. Random generator\n\n   ``` \n   ./latticegen -randseed 1234 r 10 1000 | ./fplll\n   ./latticegen -randseed time u 10 16 | ./fplll\n   ``` \n	\n4. Solving SVP\n\n   ```\n   ./latticegen r 30 3000 | ./fplll -a svp\n   ```\n\n5. Solving CVP\n\n   ```\n   echo ""[[17 42 4][50 75 108][11 47 33]][100 101 102]"" | ./fplll -a cvp\n   ```\n\n# Alternative interfaces #\n\n- [fpylll](https://github.com/malb/fpylll) is a stand-alone Python interface for fplll.\n- fplll is included in [Sage](http://sagemath.org), see documentation for [IntegerMatrix](http://doc.sagemath.org/html/en/reference/matrices/sage/matrix/matrix_integer_dense.html) and [IntegerLattice](http://doc.sagemath.org/html/en/reference/modules/sage/modules/free_module_integer.html).\n\n\n# Credit #\n\n## Maintainers ##\n\nfplll is currently maintained by:\n\n- Martin Albrecht, <martinralbrecht@googlemail.com>\n- Shi Bai, <shih.bai@gmail.com>\n\n## Contributors ##\n\nThe following people have contributed to fplll:\n\n- Martin Albrecht\n- Shi Bai\n- Guillaume Bonnoron\n- David Cade\n- Léo Ducas\n- Joop van de Pol\n- Xavier Pujol\n- Damien Stehlé\n- Marc Stevens\n- Gilles Villard\n- Michael Walter\n\nPlease add yourself here if you make a contribution.\n\n## Acknowledgments ##\n\n- Patrick Pelissier and Paul Zimmermann for `dpe`.\n\n- David H. Bailey for `QD`.\n\n- Sylvain Chevillard, Christoph Lauter and Gilles Villard for the `configure/make/make install` packaging.\n\n- Timothy Abbott, Michael Abshoff, Bill Allombert, John Cannon, Sylvain Chevillard, Julien Clement, Andreas Enge, Jean-Pierre Flori, Laurent Fousse, Guillaume Hanrot, Jens Hermans, Jerry James, Christoph Lauter, Tancrède Lepoint, Andrew Novocin, Willem Jan Palenstijn, Patrick Pelissier, Julien Puydt, Michael Schneider, Thiemo Seufer, Allan Steel, Gilles Villard and Paul Zimmermann for their support and for many suggestions that helped debugging and improving this code.\n\n- [CONTRIBUTING.md](CONTRIBUTING.md) is taken, almost verbatim, from https://github.com/pydanny/djangopackages/blob/master/docs/contributing.rst\n\n- [json.hpp](fplll/io/json.hpp) is taken from https://github.com/nlohmann/json\n\n- This project has been supported by ERC Starting Grant ERC-2013-StG-335086-LATTAC, by the European Union PROMETHEUS project (Horizon 2020 Research and Innovation Program, grant 780701), by EPSRC grant EP/P009417/1 and by EPSRC grant EP/S020330/1.\n\n# Contributing #\n\nfplll welcomes contributions. See [CONTRIBUTING.md](CONTRIBUTING.md) for details.\n\n# New releases and bug reports #\n\nNew releases will be announced on [https://groups.google.com/forum/#!forum/fplll-devel](https://groups.google.com/forum/#!forum/fplll-devel).\n\nBug reports may be sent to [https://groups.google.com/forum/#!forum/fplll-devel](https://groups.google.com/forum/#!forum/fplll-devel) or via\n[https://github.com/fplll/fplll/issues](https://github.com/fplll/fplll/issues). \n\n# Bibliography #\n\n<a name=""A02"">[A02]<a/> A. Akhavi. Random lattices, threshold phenomena and efficient reduction algorithms. Theor. Comput. Sci. 287(2): 359-385 (2002)\n\n<a name=""Chen13"">[Chen13]</a> Y. Chen, Lattice reduction and concrete security of fully homomorphic encryption.\n\n<a name=""CN11"">[CN11]</a> Y. Chen and P. Q. Nguyen. BKZ 2.0: Better Lattice Security Estimates. ASIACRYPT 2011: 1-20\n\n<a name=""GM03"">[GM03]</a> D. Goldstein and A. Mayer. On the equidistribution of Hecke points. Forum Mathematicum, 15:165–189 (2003)\n\n<a name=""GN08"">[GN08]</a> N. Gama and P. Q. Nguyen. Finding Short Lattice Vectors within Mordell's Inequality. STOC 2008: 207-216\n\n<a name=""GNR13"">[GNR13]</a> N. Gama, P. Q. Nguyen and Oded Regev. Lattice Enumeration Using Extreme Pruning.\n\n<a name=""HPS98"">[HPS98]</a> J. Hoffstein, J. Pipher, J. H. Silverman. NTRU: A Ring-Based Public Key Cryptosystem. ANTS 1998: 267-288\n\n<a name=""K83"">[K83]</a> R. Kannan. Improved algorithms for integer programming and related lattice problems. STOC 1983, 99-108\n\n<a name=""FP85"">[FP85]</a> U. Fincke and M. Pohst. Improved methods for calculating vectors of short length in a lattice, including a complexity analysis. Math. Comp., 44(170):463–471 (1985)\n\n<a name=""LLL82"">[LLL82]</a> A. K. Lenstra, H. W. Lenstra, Jr. and L. Lovasz. Factoring polynomials with rational coefficients. Math. Ann., 261: 515–534 (1982)\n\n<a name=""MSV09"">[MSV09]</a> I. Morel, D. Stehle and G. Villard. H-LLL: using Householder inside LLL. ISSAC 2009: 271-278\n\n<a name=""MW16"">[MW16]</a> D. Micciancio and M. Walter. Practical, Predictable Lattice Basis Reduction. EUROCRYPT 2016: 820-849\n\n<a name=""MR09"">[MR09]</a> D. Micciancio and O. Regev. Post-Quantum Cryptography. Chapter of Lattice-based Cryptography, 147-191 (2009)\n\n<a name=""NS09"">[NS09]</a> P. Q. Nguyen and D. Stehle. An LLL Algorithm with Quadratic Complexity. SIAM J. Comput. 39(3): 874-903 (2009)\n\n<a name=""S09"">[S09]</a> D. Stehle. Floating-Point LLL: Theoretical and Practical Aspects. The LLL Algorithm 2009: 179-213\n\n<a name=""SE94"">[SE94]</a>: C.-P. Schnorr and M. Euchner. Lattice basis reduction: Improved practical algorithms and solving subset sum problems. Math. Program. 66: 181-199 (1994)\n\n",304,mathematics,C++,8,Shell,C++,Makefile,M4,Python,Perl,Dockerfile,C,,,,,,,,,,,,,,,,,,,,,327,40,280,7,6,37,19,14286,98,205,174,31,f8536a69708a7d9c059fa4180106882a9bc80d8d,Merge pull request #528 from alxiong/header-style,2024-06-19T08:36:23Z,Martin R. Albrecht,martinralbrecht@googlemail.com,malb,05.04.2005,FPLLL 5.4.5 was released on 17 October 2023 and is available at\r\n\r\nhttps://github.com/fplll/fplll/releases/tag/5.4.5\r\n\r\nChanges in fplll-5.4.5 compared to fplll-5.4.4:\r\n\r\nhttps://github.com/fplll/fplll/compare/5.4.4...5.4.5\r\n\r\nThe following PRs were merged:\r\n\r\n-  Fix a minor typo (cas/case) in README.md #506\r\n-  More small README.md typo fixes #507\r\n-  Remove FPLLL_SONUM from fplll/fplll_config.h.in #511\r\n - update checkout to v4 #518\r\n- Threadpool fix: dynamically switch from deprecated std::result_of to … #517 \r\n\r\nThe following people have contributed to this release (based on the link above):\r\n\r\n- Ben Beasley \r\n- Marc Stevens\r\n- Martin Albrecht\r\n\r\nPlease report issues at\r\n\r\nhttps://github.com/fplll/fplll/issues\r\n,05.04.2005,,,github-actions[bot],GNU Lesser General Public License v2.1,fplll,fplll,19,lattice-reduction,lattice-based-crypto,mathematics,,,,,,,,,,,,,,,,,,/fplll/fplll,19,32,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/FosterFramework/Foster,https://github.com/FosterFramework/Foster,0,,,0,0,0,0,0,0,1,0,0,0,0,A small C# game framework,"<p align=""center"">\n<img width=""480"" src=""Foster.png"" alt=""Foster logo"">\n</p>\n\n# Foster\nFoster is a small cross-platform 2D game framework in C#.\n\n_★ very work in progress! likely to have frequent, breaking changes! please use at your own risk! ★_\n\nTo use the framework either \n - add a refence to the [NuGet package](https://www.nuget.org/packages/FosterFramework), \n - or clone this repository and add a reference to `Foster/Framework/Foster.Framework.csproj`.\n\nThere is a [Samples](https://github.com/FosterFramework/Samples) repo which contains various demos and examples that can help you get started.\n\nCheck out [Discussons](https://github.com/FosterFramework/Foster/discussions) or [Discord](https://discord.gg/K7tdFuP3Bg) to get involved.\n\n### Dependencies\n - [dotnet 8.0](https://dotnet.microsoft.com/en-us/download/dotnet/8.0) and [C# 12](https://learn.microsoft.com/en-us/dotnet/csharp/whats-new/csharp-12)\n - [SDL2](https://github.com/libsdl-org/sdl) is the only external dependency, which is required by the [Platform library](https://github.com/FosterFramework/Foster/tree/main/Platform). By default this is statically compiled.\n\n### Platform Library\n - The [Platform library](https://github.com/FosterFramework/Foster/tree/main/Platform) is a C library that implements native methods required to run the application.\n - By default, it is currently being built for 64-bit Linux, MacOS, and Windows through [Github Actions](https://github.com/FosterFramework/Foster/actions/workflows/build-libs.yml).\n - To add support for more platforms, you need to build the [Platform library](https://github.com/FosterFramework/Foster/tree/main/Platform) and then include it in [Foster.Framework.csproj](https://github.com/FosterFramework/Foster/blob/main/Framework/Foster.Framework.csproj#L27)\n\n### Rendering\n - Implemented in OpenGL for Linux/Mac/Windows and D3D11 for Windows.\n - Separate Shaders are required depending on which rendering API you're targetting.\n - Planning to replace the rendering implementation with [SDL3 GPU when it is complete](https://github.com/FosterFramework/Foster/issues/1).\n\n### Notes\n - Taken a lot of inspiration from other Frameworks and APIs, namely [FNA](https://fna-xna.github.io/).\n - This is the second iteration of this library. The first [can be found here](https://github.com/NoelFB/fosterold).\n - Contributions are welcome! However, anything that adds external dependencies or complicates the build process will not be accepted.\n",398,graphics,C#,3,C#,CMake,C,,,,,,,,,,,,,,,,,,,,,,,,,,48,2,46,0,2,17,0,60942,35,41,36,5,75aebe0add6b49c847d6c1fbae72f032ca0439dc,StackList4,2024-07-05T19:14:23Z,Maddy Thorson,maddymakesgames@gmail.com,MaddyThorson,,,,,,,MIT License,Foster,FosterFramework,18,2d,cross-platform,csharp,dotnet,game-development,game-engine,game-framework,gamedev,graphics,,,,,,,,,,,,/FosterFramework/Foster,18,10,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/fltk-rs/fltk-rs,https://github.com/fltk-rs/fltk-rs,0,,,0,0,0,0,0,0,1,1,0,0,0,Rust bindings for the FLTK GUI library.,"# fltk-rs\n\n[![Documentation](https://docs.rs/fltk/badge.svg)](https://docs.rs/fltk)\n[![Crates.io](https://img.shields.io/crates/v/fltk.svg)](https://crates.io/crates/fltk)\n[![License](https://img.shields.io/crates/l/fltk.svg)](https://github.com/fltk-rs/fltk-rs/blob/master/LICENSE)\n[![Build](https://github.com/fltk-rs/fltk-rs/workflows/Build/badge.svg?branch=master)](https://github.com/fltk-rs/fltk-rs/actions)\n\n\nRust bindings for the FLTK Graphical User Interface library. \n\nThe fltk crate is a cross-platform lightweight gui library which can be statically linked to produce small, self-contained and fast gui applications.\n\nResources:\n- [Book](https://fltk-rs.github.io/fltk-book/)\n- [本書的中文翻譯](https://flatig.vip/fltk-book-zh)\n- [Documentation](https://docs.rs/fltk)\n- [Videos](https://github.com/fltk-rs/fltk-rs#tutorials)\n- [Discussions](https://github.com/fltk-rs/fltk-rs/discussions)\n- [Examples](https://github.com/fltk-rs/fltk-rs/tree/master/fltk/examples)\n- [Demos](https://github.com/fltk-rs/demos)\n- [7guis-fltk-rs](https://github.com/tdryer/7guis-fltk-rs)\n- [FLTK-RS-Examples](https://github.com/wyhinton/FLTK-RS-Examples)\n- Erco's FLTK cheat [page](http://seriss.com/people/erco/fltk/), which is an excellent FLTK C++ reference. \n\nWhy choose FLTK?\n- Lightweight. Small binary, around 1mb after stripping. [Small memory footprint](https://szibele.com/memory-footprint-of-gui-toolkits/).\n- Speed. Fast to install, fast to build, fast at startup and fast at runtime. \n- Single executable. No DLLs to deploy.\n- Supports old architectures. \n- FLTK's permissive license which allows static linking for closed-source applications.\n- Themeability (5 supported schemes: Base, GTK, Plastic, Gleam and Oxy), and additional theming using [fltk-theme](https://crates.io/crates/fltk-theme).\n- Provides around 80 customizable widgets. \n- Has inbuilt image support.\n\nHere is a [list](https://en.wikipedia.org/wiki/FLTK#Use) of software using FLTK. For software using fltk-rs, check [here](https://github.com/fltk-rs/fltk-rs/issues/418).\n\n- [Link](https://github.com/fltk/fltk) to the official FLTK repository.\n- [Link](https://www.fltk.org/doc-1.4/index.html) to the official documentation.\n\n## Usage\n\nJust add the following to your project's Cargo.toml file:\n\n```toml\n[dependencies]\nfltk = ""^1.4""\n```\nTo use the latest changes in the repo:\n```toml\n[dependencies]\nfltk = { version = ""^1.4"", git = ""https://github.com/fltk-rs/fltk-rs"" }\n```\nOr if you have other depenendencies which depend on fltk-rs:\n```toml\n[dependencies]\nfltk = ""^1.4""\n\n[patch.crates-io]\nfltk = { git = ""https://github.com/fltk-rs/fltk-rs"" }\n```\n\nTo use the bundled libs (available for x64 windows (msvc & gnu (msys2-mingw)), x64 & aarch64 linux & macos):\n```toml\n[dependencies]\nfltk = { version = ""^1.4"", features = [""fltk-bundled""] }\n```\n\nThe library is automatically built and statically linked to your binary.\n\nAn example hello world application:\n\n```rust,no_run\nuse fltk::{app, prelude::*, window::Window};\n\nfn main() {\n    let app = app::App::default();\n    let mut wind = Window::new(100, 100, 400, 300, ""Hello from rust"");\n    wind.end();\n    wind.show();\n    app.run().unwrap();\n}\n```\n\nAnother example showing the basic callback functionality:\n```rust,no_run\nuse fltk::{app, button::Button, frame::Frame, prelude::*, window::Window};\n\nfn main() {\n    let app = app::App::default();\n    let mut wind = Window::new(100, 100, 400, 300, ""Hello from rust"");\n    let mut frame = Frame::new(0, 0, 400, 200, """");\n    let mut but = Button::new(160, 210, 80, 40, ""Click me!"");\n    wind.end();\n    wind.show();\n    but.set_callback(move |_| frame.set_label(""Hello World!"")); // the closure capture is mutable borrow to our button\n    app.run().unwrap();\n}\n```\nPlease check the [examples](https://github.com/fltk-rs/fltk-rs/tree/master/fltk/examples) directory for more examples.\nYou will notice that all widgets are instantiated with a new() method, taking the x and y coordinates, the width and height of the widget, as well as a label which can be left blank if needed. Another way to initialize a widget is using the builder pattern: (The following buttons are equivalent)\n\n```rust,no_run\nuse fltk::{button::Button, prelude::*};\nlet but1 = Button::new(10, 10, 80, 40, ""Button 1"");\n\nlet but2 = Button::default()\n    .with_pos(10, 10)\n    .with_size(80, 40)\n    .with_label(""Button 2"");\n```\n\nAn example of a counter showing use of the builder pattern:\n```rust,no_run\nuse fltk::{app, button::Button, frame::Frame, prelude::*, window::Window};\nfn main() {\n    let app = app::App::default();\n    let mut wind = Window::default()\n        .with_size(160, 200)\n        .center_screen()\n        .with_label(""Counter"");\n    let mut frame = Frame::default()\n        .with_size(100, 40)\n        .center_of(&wind)\n        .with_label(""0"");\n    let mut but_inc = Button::default()\n        .size_of(&frame)\n        .above_of(&frame, 0)\n        .with_label(""+"");\n    let mut but_dec = Button::default()\n        .size_of(&frame)\n        .below_of(&frame, 0)\n        .with_label(""-"");\n    wind.make_resizable(true);\n    wind.end();\n    wind.show();\n    /* Event handling */\n    app.run().unwrap();\n}\n```\n\nAlternatively, you can use Flex (for flexbox layouts), Pack or [Grid](https://github.com/fltk-rs/fltk-grid):\n```rust,no_run\nuse fltk::{app, button::Button, frame::Frame, group::Flex, prelude::*, window::Window};\nfn main() {\n    let app = app::App::default();\n    let mut wind = Window::default().with_size(160, 200).with_label(""Counter"");\n    let mut flex = Flex::default().with_size(120, 140).center_of_parent().column();\n    let mut but_inc = Button::default().with_label(""+"");\n    let mut frame = Frame::default().with_label(""0"");\n    let mut but_dec = Button::default().with_label(""-"");\n    flex.end();\n    wind.end();\n    wind.show();\n    app.run().unwrap();\n}\n```\n\nAnother example:\n```rust,no_run\nuse fltk::{app, button::Button, frame::Frame, group::Flex, prelude::*, window::Window};\n\nfn main() {\n    let app = app::App::default();\n    let mut wind = Window::default().with_size(400, 300);\n    let mut col = Flex::default_fill().column();\n    col.set_margins(120, 80, 120, 80);\n    let mut frame = Frame::default();\n    let mut but = Button::default().with_label(""Click me!"");\n    col.fixed(&but, 40);\n    col.end();\n    wind.end();\n    wind.show();\n\n    but.set_callback(move |_| frame.set_label(""Hello world""));\n\n    app.run().unwrap();\n}\n```\n\n### Events\nEvents can be handled using the `set_callback` method (as above) or the available `fltk::app::set_callback()` free function, which will handle the default trigger of each widget(like clicks for buttons):\n```rust,ignore\n    /* previous hello world code */\n    but.set_callback(move |_| frame.set_label(""Hello World!""));\n    another_but.set_callback(|this_button| this_button.set_label(""Works""));\n    app.run().unwrap();\n```\nAnother way is to use message passing:\n```rust,ignore\n    /* previous counter code */\n    let (s, r) = app::channel::<Message>();\n\n    but_inc.emit(s, Message::Increment);\n    but_dec.emit(s, Message::Decrement);\n    \n    while app.wait() {\n        let label: i32 = frame.label().parse().unwrap();\n        if let Some(msg) = r.recv() {\n            match msg {\n                Message::Increment => frame.set_label(&(label + 1).to_string()),\n                Message::Decrement => frame.set_label(&(label - 1).to_string()),\n            }\n        }\n    }\n```\nFor the remainder of the code, check the full example [here](https://github.com/fltk-rs/fltk-rs/tree/master/fltk/examples/counter2.rs).\n\nFor custom event handling, the handle() method can be used:\n```rust,ignore\n    some_widget.handle(move |widget, ev: Event| {\n        match ev {\n            Event::Push => {\n                println!(""Pushed!"");\n                true\n            },\n            /* other events to be handled */\n            _ => false,\n        }\n    });\n```\nHandled or ignored events using the handle method should return true, unhandled events should return false. More examples are available in the fltk/examples directory.\n\nFor an alternative event handling mechanism using an immediate-mode approach, check the [fltk-evented crate](https://crates.io/crates/fltk-evented).\n\n### Theming\n\nFLTK offers 5 application schemes:\n- Base\n- Gtk\n- Gleam\n- Plastic\n- Oxy\n\n(Additional theming can be found in the [fltk-theme](https://crates.io/crates/fltk-theme) crate)\n\nThese can be set using the `App::with_scheme()` method.\n```rust,ignore\nlet app = app::App::default().with_scheme(app::Scheme::Gleam);\n```\nThemes of individual widgets can be optionally modified using the provided methods in the `WidgetExt` trait, such as `set_color()`, `set_label_font()`, `set_frame()` etc:\n```rust,ignore\n    some_button.set_color(Color::Light1); // You can use one of the provided colors in the fltk enums\n    some_button.set_color(Color::from_rgb(255, 0, 0)); // Or you can specify a color by rgb or hex/u32 value\n    some_button.set_color(Color::from_u32(0xffebee));\n    some_button.set_frame(FrameType::RoundUpBox);\n    some_button.set_font(Font::TimesItalic);\n```\nFor default application colors, fltk-rs provides `app::background()`, `app::background2()` and `app::foreground()`. You can also specify the default application selection/inactive colors, font, label size, frame type, scrollbar size, menu line-spacing. Additionally the [fltk-theme](https://crates.io/crates/fltk-theme) crate offers some other predefined color maps (dark theme, tan etc) and widget themes which can be loaded into your application.\n\n## Build Dependencies\n\nRust (version > 1.63), CMake (version > 3.15), Git and a C++17 compiler need to be installed and in your PATH for a cross-platform build from source. [Ninja](https://github.com/ninja-build/ninja) is recommended, but not required. This crate also offers a bundled form of fltk on selected x86_64 and aarch64 platforms (Windows (msvc and gnu), MacOS, Linux), this can be enabled using the fltk-bundled feature flag as mentioned in the usage section (this requires curl and tar to download and unpack the bundled libraries).\n\n- Windows: \n    - MSVC: Windows SDK\n    - Gnu: No dependencies\n- MacOS: No dependencies.\n- Linux/BSD: X11 and OpenGL development headers need to be installed for development. The libraries themselves are normally available on linux/bsd distros with a graphical user interface.\n\nFor Debian-based GUI distributions, that means running:\n```bash\nsudo apt-get install libx11-dev libxext-dev libxft-dev libxinerama-dev libxcursor-dev libxrender-dev libxfixes-dev libpango1.0-dev libgl1-mesa-dev libglu1-mesa-dev\n```\nFor RHEL-based GUI distributions, that means running:\n```bash\nsudo yum groupinstall ""X Software Development"" && sudo yum install pango-devel libXinerama-devel libstdc++-static\n```\nFor Arch-based GUI distributions, that means running:\n```bash\nsudo pacman -S libx11 libxext libxft libxinerama libxcursor libxrender libxfixes pango cairo libgl mesa --needed\n```\nFor Alpine linux:\n```bash\napk add pango-dev fontconfig-dev libxinerama-dev libxfixes-dev libxcursor-dev mesa-gl\n```\nFor NixOS (Linux distribution) this `nix-shell` environment can be used:\n```bash\nnix-shell --packages rustc cmake git gcc xorg.libXext xorg.libXft xorg.libXinerama xorg.libXcursor xorg.libXrender xorg.libXfixes libcerf pango cairo libGL mesa pkg-config\n```\n\n## Runtime Dependencies\n- Windows: None\n- MacOS: None\n- Linux: You need X11 libraries, as well as pango and cairo for drawing (and OpenGL if you want to enable the enable-glwindow feature):\n```bash\napt-get install -qq --no-install-recommends libx11-6 libxinerama1 libxft2 libxext6 libxcursor1 libxrender1 libxfixes3 libcairo2 libpango-1.0-0 libpangocairo-1.0-0 libpangoxft-1.0-0 libglib2.0-0 libfontconfig1 libglu1-mesa libgl1\n```\nNote that if you installed the build dependencies, it will also install the runtime dependencies automatically as well.\n\nAlso note that most graphical desktop environments already have these libs already installed. This list can be useful if you want to test your already built package in CI/docker (where there is no graphical user interface).\n\n## Features\n\nThe following are the features offered by the crate:\n- use-ninja: Uses the ninja build system if available for a faster build, especially on Windows.\n- no-pango: Build without pango support on Linux/BSD, if rtl/cjk font support is not needed.\n- fltk-bundled: Support for bundled versions of cfltk and fltk on selected platforms (requires curl and tar)\n- enable-glwindow: Support for drawing using OpenGL functions.\n- system-libpng: Uses the system libpng\n- system-libjpeg: Uses the system libjpeg\n- system-zlib: Uses the system zlib\n- use-wayland: Uses FLTK's wayland hybrid backend (runs on wayland when present, and on X11 when not present). Requires libwayland-dev, wayland-protocols, libdbus-1-dev, libxkbcommon-dev, libgtk-3-dev (optional, for the GTK-style titlebar), in addition to the X11 development packages. Sample [CI](https://github.com/MoAlyousef/test_wayland/blob/main/.github/workflows/rust.yml).\n- fltk-config: Uses an already installed FLTK's fltk-config to build this crate against. This still requires FLTK 1.4. Useful for reducing build times, testing against a locally built FLTK and doesn't need to invoke neither git nor cmake. \n\n## FAQ\n\nplease check the [FAQ](FAQ.md) page for frequently asked questions, encountered issues, guides on deployment, and contribution.\n\n## Building\n\nTo build, just run:\n```bash\ngit clone https://github.com/fltk-rs/fltk-rs --recurse-submodules\ncd fltk-rs\ncargo build\n```\n\n## Currently implemented types:\n\n### Image types:\n- SharedImage\n- BmpImage\n- JpegImage\n- GifImage\n- AnimGifImage\n- PngImage\n- SvgImage\n- Pixmap\n- RgbImage\n- XpmImage\n- XbmImage\n- PnmImage\n- TiledImage\n\n### Widgets:\n- Buttons\n    - Button\n    - RadioButton\n    - ToggleButton\n    - RoundButton\n    - CheckButton\n    - LightButton\n    - RepeatButton\n    - RadioLightButton\n    - RadioRoundButton\n    - ReturnButton\n    - ShortcutButton\n- Dialogs\n    - Native FileDialog\n    - FileChooser\n    - HelpDialog\n    - Message dialog\n    - Alert dialog\n    - Password dialog\n    - Choice dialog\n    - Input dialog\n    - ColorChooser dialog\n- Frame (Fl_Box)\n- Windows\n    - Window\n    - SingleWindow (single buffered)\n    - DoubleWindow (double buffered)\n    - MenuWindow\n    - OverlayWindow\n    - GlWindow (requires the ""enable-glwindow"" flag)\n    - Experimental GlWidgetWindow (requires the ""enable-glwindow"" flag)\n- Groups\n    - Group\n    - Pack (Horizontal and Vertical)\n    - Tabs\n    - Scroll\n    - Tile\n    - Wizard\n    - ColorChooser\n    - Flex (Column and Row)\n    - Grid (https://github.com/fltk-rs/fltk-grid)\n- Text display widgets\n    - TextDisplay\n    - TextEditor\n    - SimpleTerminal\n- Input widgets\n    - Input\n    - IntInput\n    - FloatInput\n    - MultilineInput\n    - SecretInput\n    - FileInput\n- Output widgets\n    - Output\n    - MultilineOutput\n- Menu widgets\n    - MenuBar\n    - MenuItem\n    - Choice (dropdown list)\n    - SysMenuBar (MacOS menu bar which appears at the top of the screen)\n- Valuator widgets\n    - Slider\n    - NiceSlider\n    - ValueSlider\n    - Dial\n    - LineDial\n    - Counter\n    - Scrollbar\n    - Roller\n    - Adjuster\n    - ValueInput\n    - ValueOutput\n    - FillSlider\n    - FillDial\n    - HorSlider (Horizontal slider)\n    - HorFillSlider\n    - HorNiceSlider\n    - HorValueSlider\n- Browsing widgets\n    - Browser\n    - SelectBrowser\n    - HoldBrowser\n    - MultiBrowser\n    - FileBrowser\n    - CheckBrowser\n- Miscelaneous widgets\n    - Spinner\n    - Clock (Round and Square)\n    - Chart (several chart types are available)\n    - Progress (progress bar)\n    - Tooltip\n    - InputChoice\n    - HelpView\n- Table widgets\n    - Table\n    - TableRow\n    - SmartTable (via the [fltk-table crate](https://crates.io/crates/fltk-table))\n- Trees\n    - Tree\n    - TreeItem\n\n### Drawing primitives\n(In the draw module)\n### Surface types:\n- Printer.\n- ImageSurface.\n- SvgFileSurface.\n\n### GUI designer\n\nfltk-rs supports FLUID, the RAD wysiwyg designer for FLTK. \nCheckout the [fl2rust crate](https://github.com/fltk-rs/fl2rust) and [fl2rust template](https://github.com/fltk-rs/fl2rust-template).\n\n- [FLTK Rust: Latest FLUID, fl2rust and fltk-rs](https://www.youtube.com/watch?v=33NdaW08fP8)\n\n## Examples\n\nTo run the [examples](https://github.com/fltk-rs/fltk-rs/tree/master/fltk/examples): \n```bash\ncargo run --example editor\ncargo run --example calculator\ncargo run --example calculator2\ncargo run --example counter\ncargo run --example hello_svg\ncargo run --example hello_button\ncargo run --example fb\ncargo run --example pong\ncargo run --example custom_widgets\ncargo run --example custom_dial\n...\n```\n\nUsing custom theming and also FLTK provided default schemes like Gtk:\n\n- [hello_svg](https://github.com/fltk-rs/fltk-rs/tree/master/fltk/examples/hello_svg.rs)\n\n- ![alt_test](https://github.com/fltk-rs/fltk-rs/raw/master/screenshots/hello.jpg)\n\n- [calculator2](https://github.com/fltk-rs/fltk-rs/tree/master/fltk/examples/calculator2.rs)\n\n- ![alt_test](https://github.com/fltk-rs/fltk-rs/raw/master/screenshots/calc2.jpg)\n\n- [counter3](https://github.com/fltk-rs/fltk-rs/tree/master/fltk/examples/counter3.rs)\n\n- ![alt_test](https://github.com/fltk-rs/fltk-rs/raw/master/screenshots/flutter_like.jpg)\n\n- [custom_dial](https://github.com/fltk-rs/fltk-rs/tree/master/fltk/examples/custom_dial.rs)\n\n- ![alt_test](https://github.com/fltk-rs/fltk-rs/raw/master/screenshots/dial.jpg)\n\n- [calculator](https://github.com/fltk-rs/fltk-rs/tree/master/fltk/examples/calculator.rs)\n\n- ![alt_test](https://github.com/fltk-rs/fltk-rs/raw/master/screenshots/calc.jpg)\n\n- [tabs](https://github.com/fltk-rs/fltk-rs/tree/master/fltk/examples/tabs.rs)\n\n- ![alt_test](https://github.com/fltk-rs/fltk-rs/raw/master/screenshots/tabs.jpg)\n\n- [counter](https://github.com/fltk-rs/fltk-rs/tree/master/fltk/examples/counter.rs)\n\n- ![alt_test](https://github.com/fltk-rs/fltk-rs/raw/master/screenshots/counter.jpg)\n\n- [editor](https://github.com/fltk-rs/fltk-rs/tree/master/fltk/examples/editor.rs)\n\n- ![alt_test](https://github.com/fltk-rs/fltk-rs/raw/master/screenshots/editor.jpg)\n\n- [table](https://github.com/fltk-rs/fltk-rs/tree/master/fltk/examples/table.rs)\n\n- ![alt_test](https://github.com/fltk-rs/fltk-rs/raw/master/screenshots/table.jpg)\n\n- [charts](https://github.com/fltk-rs/fltk-rs/tree/master/fltk/examples/charts.rs)\n\n- ![alt_test](https://github.com/fltk-rs/fltk-rs/raw/master/screenshots/charts.jpg)\n\n- [pong](https://github.com/fltk-rs/fltk-rs/tree/master/fltk/examples/pong.rs)\n\n- ![alt_test](https://github.com/fltk-rs/fltk-rs/raw/master/screenshots/pong.gif)\n\n- [frames](https://github.com/fltk-rs/fltk-rs/tree/master/fltk/examples/frames.rs)\n\n- ![alt_test](https://github.com/fltk-rs/fltk-rs/raw/master/screenshots/frames.jpg)\n\nDifferent frame types which can be used with many different widgets such as Frame, Button widgets, In/Output widgets...etc.\n\nMore interesting examples can be found in the fltk-rs-demos [repo](https://github.com/fltk-rs/demos).\nAlso a nice implementation of the 7guis tasks can be found [here](https://github.com/tdryer/7guis-fltk-rs).\nVarious advanced examples can also be found [here](https://github.com/wyhinton/FLTK-RS-Examples).\n\n\n### Themes\n\nAdditional themes can be found in the [fltk-theme crate](https://github.com/fltk-rs/fltk-theme).\n\n- ![screenshots/aero.jpg](https://github.com/fltk-rs/fltk-theme/blob/dfd1e97b62c94d7af9d615cab0ab809226957a3f/screenshots/aero.jpg)\n\n- ![screenshots/black.jpg](https://github.com/fltk-rs/fltk-theme/blob/dfd1e97b62c94d7af9d615cab0ab809226957a3f/screenshots/black.jpg)\n\nAnd more...\n\n### Extra widgets\n\nThis crate exposes FLTK's set of widgets, which are all customizable. Additional custom widgets can be found in the [fltk-extras crate](https://github.com/fltk-rs/fltk-theme).\n\n![image](https://user-images.githubusercontent.com/37966791/212541355-91062d78-5c5d-4b7a-aa6d-e1be49cff340.png)\n\n![image](https://user-images.githubusercontent.com/37966791/212541425-f594a7bc-d7bc-49e5-90f3-03f52d437cce.png)\n\n![ss](https://github.com/fltk-rs/fltk-extras/assets/37966791/a03e1912-7658-48be-a354-2b588b417fd8)\n\n![image](https://user-images.githubusercontent.com/37966791/212541392-2cd4fb08-4152-484a-86da-64b2bc476a0e.png)\n\n## Tutorials\n\n- [Basics](https://www.youtube.com/watch?v=ygP4egJtmzw)\n- [New basics](https://youtu.be/S1NSsHZs6hI) (Uses fltk post 1.0)\n- [User input](https://youtu.be/rIq2O4vg9fQ)\n- [Client-side web todo app](https://youtu.be/tdfFXi4-Yrw)\n- [Create a media player using the vlc crate](https://youtu.be/enxqU3bhCEs)\n- [Custom dialogs](https://youtu.be/tXeXHoKG6-I)\n- [Add drag and drop to the editor example](https://www.youtube.com/watch?v=qp5hnRvSxAg)\n- [Drawing things with fltk](https://www.youtube.com/watch?v=r9MOpvfBPWs)\n- [Working with images](https://www.youtube.com/watch?v=Rn2sjfAX4WI)\n- [Audio player with custom widgets](https://www.youtube.com/watch?v=okdFx6tv7ds)\n- [Use FLUID (RAD tool) with Rust](https://www.youtube.com/watch?v=k_P0wG3-dNk)\n- [multiple windows and embedding windows](https://www.youtube.com/watch?v=qEPYx1Lw7fY)\n- [FLTK Rust tutorial: Improve FLTK's toggle button appearance!](https://www.youtube.com/watch?v=WCTbPKHXR-o)\n- [FLTK Rust: Customizing your app and widgets](https://www.youtube.com/watch?v=uCZl0PuMVGo)\n- [FLTK Rust: fltk-table, a boilerplate-less table creating crate](https://www.youtube.com/watch?v=pVJ8Yq1kDGs)\n- [FLTK Rust: intro into the fltk-evented crate](https://www.youtube.com/watch?v=rAVHBl3W9W8)\n- [FLTK Rust: Latest FLUID, fl2rust and fltk-rs](https://www.youtube.com/watch?v=33NdaW08fP8)\n\nMore videos in the playlist [here](https://www.youtube.com/playlist?list=PLHqrrowPLkDu9U-uk60sGM-YWLOJFfLoE).\nSome of the demo projects can be found [here](https://github.com/fltk-rs/demos).\n",1571,graphics,Rust,2,Rust,Shell,,,,,,,,,,,,,,,,,,,,,,,,,,,769,14,755,0,2,42,1583,11258,107,355,348,7,a23f88621288f03eb7d4ce50c322f610b83ac7ac,simple raw handle resolution,2024-07-19T23:29:29Z,Mohammed Alyousef,may642_2000@hotmail.com,MoAlyousef,01.04.1932,- Add BrowserExt::deselect.\r\n- Update FLTK submodule.,01.04.1932,Mohammed Alyousef,,MoAlyousef,MIT License,fltk-rs,fltk-rs,272,fltk,widgets,gui,graphics,bindings,,,,,,,,,,,,,,,,/fltk-rs/fltk-rs,289,28,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/flingengine/FlingEngine,https://github.com/flingengine/FlingEngine,0,,,0,0,0,0,0,0,1,0,0,0,0,A Vulkan game engine with a focus on data oriented design,"![Fling Engine Logo](docs/Fling-Engine-logo/cover.png)\n\nThe Fling Engine aims to be a cross platform Vulkan game engine that will experiment with the following:\n\n* Low-level engine systems such as render API abstraction, file systems, and custom allocators.\n* Multithreaded engine architecture\n* The Vulkan graphics API for real time rendering\n\n[![Build Status](https://travis-ci.com/flingengine/FlingEngine.svg?branch=master)](https://travis-ci.com/flingengine/FlingEngine)\n[![Build status](https://ci.appveyor.com/api/projects/status/3791w1nlc3usf5qx?svg=true)](https://ci.appveyor.com/project/BenjaFriend/flingengine)\n[![Open Source Helpers](https://www.codetriage.com/flingengine/flingengine/badges/users.svg)](https://www.codetriage.com/flingengine/flingengine)\n[![GitHub license](https://img.shields.io/github/license/flingengine/FlingEngine)](https://github.com/flingengine/FlingEngine/blob/master/LICENSE)\n[![Work in progress badge](https://img.shields.io/badge/this%20is-a%20work%20in%20progress!-yellow)](https://img.shields.io/badge/this%20is-a%20work%20in%20progress!-yellow)\n[![Discord Chat](https://img.shields.io/static/v1?logo=discord&label=Discord&message=Join%20and%20chat!&color=blueviolet&link=https://discord.gg/HUpX6ZE)](https://discord.gg/HUpX6ZE)\n\n# Getting Started\n\nThere are a few basic steps to compiling Fling on your platform. \n\n## [CMake 3.13](https://cmake.org/download/) or higher!\nThis project requires CMake 3.13 or higher, you can install it [here](https://cmake.org/download/).\n\n## For Linux\nThis project uses GLFW, so you will need to install those libraries to your machine.\nGLFW also depends on having Doxygen, so you may want to have that as well.\n\nUbuntu:\n```\nsudo apt-get update\nsudo apt-get install doxygen\nsudo apt-get install -y libglm-dev libxcb-dri3-0 libxcb-present0\nsudo apt-get install -y libpciaccess0 libpng-dev libxcb-keysyms1-dev\nsudo apt-get install -y libxcb-dri3-dev libx11-dev libmirclient-dev\nsudo apt-get install -y libwayland-dev libxrandr-dev\nsudo apt-get install -y libglfw3-dev\nsudo apt-get install -y xorg-dev\n```\n\n## Vulkan SDK\nObviously this project is build using Vulkan, so you will need to install it before compiling \nor running the program. \n\nYou can download the SDK from the LunarG website [here](https://www.lunarg.com/vulkan-sdk/). \n\nIf you are having trouble with the Vulkan SDK then check out some of these resources: \n* [Vulkan Verify Install](https://vulkan.lunarg.com/doc/view/1.1.106.0/windows/getting_started.html#user-content-verify-the-installation)\n* [Vulkan Tutorial FAQ](https://vulkan-tutorial.com/FAQ)\n\n## `Init.bat` and `Init.sh`\nAfter installing the SDK, you can simply run one of the provided scripts. \n\nRunning either one of these scripts will simply get all submodules and external libraries\nthat the engine uses and create a folder called `build`. The `build` folder will have your\nplatform specific build files (Visual Studio, Makefiles, etc). \n\n#### Packaging a project\n\nFor ease of development and iteration the file paths to Assets (shaders, textures, models, etc) are all \nabsolute paths generated by CMake. If you want to have a copy of your executeable with asset paths relative\nto the program, then generate your project files with CMake with this flag: \n\n```\ncmake -DDEFINE_SHIPPING=ON -B build .\n```\n\nNotice the `-DDEFINE_SHIPPING` option is set to `ON`. This sets a definiton that you can use in C++: \n\n```C\n#ifdef FLING_SHIPPING\n// Do some nice stuff\n#else\n// Do non-shipping code, perhaps with a lot of log messages\n#endif\n```\n\n### Wanna contribute?\n\nIf you have any contributions or fixes that you want to contribute, then feel free to open \nan issue or a pull request! I'm happy to talk about the project, so feel free to reach out\nto me on [Twitter](https://twitter.com/BenjaFriend?lang=en) or here on GitHub. Eventually a\ngoal is to have some more specific PR templates/coding standards but for now that is not a \npriority. \n\n## Branching Strategy\nWe use a pretty basic branching strategy. Make a feature branch off of `Main` for something like ""add-support-for-x"", and then that feature is done and tested create a pull request to get it into Main.\n\nWe will create stable ""Release"" branches and tag them accordingly with stable versions of the build. \n\n### Cool Resources\n\nSome great resources are the \n[Vulkan Tutorial](https://vulkan-tutorial.com/Drawing_a_triangle/Setup/Validation_layers) and \n[SaschaWillems](https://github.com/SaschaWillems/Vulkan)'s repo with different Vulkan examples\n",391,graphics,C++,7,CMake,C++,Batchfile,Python,GLSL,Shell,C,,,,,,,,,,,,,,,,,,,,,,65,2,63,0,6,4,63,190895,15,111,93,18,56d3c4b5f1cb0c2857b8e1e56fd57d25140d9746,Define NOMINMAX before Windows.h,2024-06-14T00:39:38Z,Ben Hoffman,benjamin.hoffman.dev@gmail.com,BenjaFriend,,,,,,,MIT License,FlingEngine,flingengine,3,game,engine,vulkan,graphics,cpp,c,cmake,opensource,hacktoberfest,,,,,,,,,,,,/flingengine/FlingEngine,3,18,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/f3d-app/f3d,https://github.com/f3d-app/f3d,0,,0,0,0,1,0,0,0,0,0,0,0,0,Fast and minimalist 3D viewer.,"[![CI](https://img.shields.io/github/actions/workflow/status/f3d-app/f3d/ci.yml?label=CI&logo=github)](https://github.com/f3d-app/f3d/actions/workflows/ci.yml) [![Packaging](https://img.shields.io/github/actions/workflow/status/f3d-app/f3d-superbuild/nightly.yml?label=Packaging&logo=github)](https://github.com/f3d-app/f3d-superbuild) [![codecov](https://codecov.io/gh/f3d-app/f3d/branch/master/graph/badge.svg?token=siwG82IXK7)](https://codecov.io/gh/f3d-app/f3d) [![Downloads](https://img.shields.io/github/downloads/f3d-app/f3d/total.svg)](https://github.com/f3d-app/f3d/releases) [![Sponsors](https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&color=%23fe8e86)](https://github.com/sponsors/f3d-app) [![Discord](https://discordapp.com/api/guilds/1046005690809978911/widget.png?style=shield)](https://discord.f3d.app) [![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](CODE_OF_CONDUCT.md)\n\n# F3D - Fast and minimalist 3D viewer\nBy Michael Migliore and Mathieu Westphal.\n\n<img src=""https://raw.githubusercontent.com/f3d-app/f3d/master/resources/logo.svg"" align=""left"" width=""20px""/>\nF3D (pronounced `/fɛd/`) is a fast and minimalist 3D viewer desktop application. It supports many file formats, from digital content to scientific datasets (including glTF, USD, STL, STEP, PLY, OBJ, FBX, Alembic), can show animations and support thumbnails and many rendering and texturing options including real time physically based rendering and raytracing.\n<br clear=""left""/>\n\nIt is fully controllable from the command line and support configuration files. It can provide thumbnails, support interactive hotkeys, drag&drop and integration into file managers.\n\nF3D also contains the libf3d, a simple library to render meshes, with C++ and Python Bindings, as well as experimental Java and Javascript bindings.\n\n<img src=""https://user-images.githubusercontent.com/3129530/194735416-3f386437-456c-4145-9b5e-6bb6451d7e9a.png"" width=""640"">\n\n*A typical render by F3D*\n\n<img src=""https://user-images.githubusercontent.com/3129530/194735261-dd6f1c1c-fa57-47b0-9d27-f735d18ccd5e.gif"" width=""640"">\n\n*Animation of a glTF file within F3D*\n\n<img src=""https://user-images.githubusercontent.com/3129530/194735272-5bcd3e7c-a333-41f5-8066-9b0bec9885e8.png"" width=""640"">\n\n*A direct scalars render by F3D*\n\nSee the [gallery](doc/GALLERY.md) for more images, take a look at the [changelog](doc/CHANGELOG.md) or go to the [install guide](doc/user/INSTALLATION.md) to download and install F3D!\n\nYou can even use F3D directly in your [browser](https://f3d.app/web)!\n\nIf you need any help or want to discuss with other F3D users and developers, head over to our [discord](https://discord.f3d.app).\n\n# Quickstart\n\nOpen a file and visualize it interactively:\n\n```\nf3d /path/to/file.ext\n```\n\nOpen a file and save the rendering into an image file:\n\n```\nf3d /path/to/file.ext --output=/path/to/img.png\n```\n\nGet help:\n\n```\nf3d --help\nman f3d # Linux only\n```\n\n# Documentation\n\n- To get started, please take a look at the [user documentation](doc/user/README_USER.md).\n- If you need any help, are looking for a feature or found a bug, please open an [issue](https://github.com/f3d-app/f3d/issues).\n- If you want to use the libf3d, please take a look at its [documentation](doc/libf3d/README_LIBF3D.md).\n- If you want to build F3D, please take a look at the [developer documentation](doc/dev/README_DEV.md).\n\n# Support\n\nF3D needs your help!\n\nIf you can, please consider sponsoring F3D. Even a small donation would help us offset the recurring maintenance costs.\nWith enough sponsors we would even be able to add support for new devices (as we would need do acquire or rent them first). Read more about it on our [sponsor page](https://github.com/sponsors/f3d-app).\n\nIf not, please use F3D, star it on github and share the word about it!\n\n# Vision\n\nAs a minimalist 3D viewer F3D aims to:\n\n- Support as many 3D file formats as possible\n- Support many types of renderings (textures, edges, etc... ) and visualizations (meshes, volumic, point sprites)\n- Support any and all use-cases dealing with 3D datasets\n- Let any user easily and quickly view any model with good defaults\n- Be as configurable as possible\n- Be fully controllable from the command line and configuration file\n- Be usable non-interactively\n- Be as modular as possible to be built with a small number of dependencies\n\nbut there is no plan to:\n\n- Provide a classic mouse-based UI, with menus and buttons\n- Provide data processing tools\n- Provide export feature\n\n# Contributing\n\nF3D as a community-driven, inclusive and beginner-friendly project. We love to see how the project is growing thanks to the contributions from the community. We would love to see your face in the list below! If you want to contribute to F3D, you are very welcome to! Take a look at our [contribution documentation](CONTRIBUTING.md), [governance documentation](doc/dev/GOVERNANCE.md) and [code of conduct](CODE_OF_CONDUCT.md).\n\n<a href=""https://github.com/f3d-app/f3d/graphs/contributors"">\n  <img src=""https://contrib.rocks/image?repo=f3d-app/f3d"" />\n</a>\n\n# Acknowledgments\n\nF3D was initially created by [Kitware SAS](https://www.kitware.eu/) and is relying on many awesome open source projects, including [VTK](https://vtk.org/), [OCCT](https://dev.opencascade.org/), [Assimp](https://www.assimp.org/), [Alembic](http://www.alembic.io/), [Draco](https://google.github.io/draco/), [OpenUSD](https://openusd.org/release/index.html), [OpenVDB](https://www.openvdb.org/) and [OSPRay](https://www.ospray.org/).\n\n# License\n\nF3D can be used and distributed under the 3-Clause BSD License, see the [license](LICENSE.md).\nF3D integrate the sources of other libraries and tools, all under permissive licenses, see the [third party licenses](doc/THIRD_PARTY_LICENSES.md).\nF3D packages relies on other libraries and tools, all under permissive licenses, all listed in the respective packages.\n",2597,graphics,C++,16,CMake,Game Maker Language,Roff,Shell,C++,Objective-C++,C,Python,Java,HTML,Eiffel,RPC,SCSS,JavaScript,GLSL,PowerShell,,,,,,,,,,,,,1021,152,856,13,4,46,367,33699,184,524,345,179,6701eeb427230c3a536274a089f0c6232861e072,CI: Use VTK 9.3.1 in CI (#1542),2024-07-16T06:32:29Z,Mathieu Westphal,mathieu.westphal@gmail.com,mwestphal,v2.5.0,"![shader-rain](https://github.com/f3d-app/f3d/assets/3129530/fe65ea02-80a4-4b70-a8f3-c4836b9ffc2f)\r\n\r\n### 🗣 Join our community:\r\nF3D Community is welcoming to users and developers alike!\r\nAsk questions, gets involved and starts contributing in a beginner-friendly environment.\r\n[![Discord](https://discordapp.com/api/guilds/1046005690809978911/widget.png?style=shield)](https://discord.f3d.app)\r\n\r\n### ❤️ Sponsor F3D:\r\nF3D is looking for sponsors!\r\nWe have some expanses mainly related to github and web hosting bills, any help to cover the cost would be highly appreciated.\r\nTo thank you, we'll offer preferential support and vote on next feature and bugfix as soon as you start sponsoring.\r\n[![Sponsors](https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&color=%23fe8e86)](https://github.com/sponsors/f3d-app)\r\nMuch love to our sponsors for this release @parkerstafford @kidharb @Ramalama2 :heart:\r\n\r\n### 🌐 F3D Web\r\n\r\nF3D is now available as a 3D web viewer!\r\n\r\nThanks to webassembly, you can now use F3D in the comfort of your browser.\r\nF3D Web is a simple yet complete application based on the f3d javascript package available at https://f3d.app/web\r\n\r\n![f3dweb](https://github.com/f3d-app/f3d/assets/3129530/d43bb441-55af-4bed-855b-d28e1e0d4e4a)\r\n\r\n### 🖼️ Rendering Improvements\r\n\r\nRendering improvements have been added!\r\n\r\nF3D now uses [Khronos' ""PBR Neutral""](https://www.khronos.org/news/press/khronos-pbr-neutral-tone-mapper-released-for-true-to-life-color-rendering-of-3d-products) tone mapping for nicer renderings:\r\n\r\nBefore:\r\n\r\n![no](https://github.com/f3d-app/f3d/assets/3129530/fa501d39-badd-4596-9bdf-a97abe311a98)\r\n\r\nNow:\r\n\r\n![neutral](https://github.com/f3d-app/f3d/assets/3129530/a9672e70-d11b-4de5-a5c6-ce2770364f1f)\r\n\r\nYou can also now specify your own custom post-processing shader for even fancier effects, using `--final-shader` option, here using a ""negative"" shader.\r\n\r\n![negative](https://github.com/f3d-app/f3d/assets/3129530/a67934cf-3ca9-4175-a3da-e39599945047)\r\n\r\n### 📸 Screenshot\r\n\r\nDo you use F3D interactively and wants to take a screenshot at any point?\r\nJust press `F12` and a screenshot will be saved on disk.\r\nF3D will name and number the image so you can save many screenshots as you want but you can specify your own [filename template](https://f3d.app/doc/user/OPTIONS.html#filename-templating) using `--screenshot-filename` if you prefer.\r\n\r\n![ss](https://github.com/f3d-app/f3d/assets/3129530/10415738-9229-4fac-bce0-ae3fd1d4199b)\r\n\r\n### 📝 Complete changelog\r\n<details>\r\n\r\nFor F3D users:\r\n- Added a Webassembly version available online: https://f3d.app/web\r\n- Added a ""neutral"" tone mapping feature\r\n- Added an screenshot feature when pressing F12, with its dedicated option, `--screenshot-filename` (thanks @snoyer!)\r\n- Added a `--final-shader` option to customize post-processing.\r\n- Added a `--grid-color` option to set the color of the grid\r\n- Added a `--animation-progress` option to control if the animation progress bar should be shown (thanks @spevnev!)\r\n- Added a `--backface-type` option to control backface visibility (thanks @KeflerExe!)\r\n- Added a concept of filename [template](https://f3d.app/doc/user/OPTIONS.html#filename-templating) for saving screenshots and outputs (thanks @snoyer!)\r\n- Added native menus for macOS\r\n- Improved documentation all around (thanks @kidharb @kathleenhang @Nokse22 @vikaskok @spevnev!)\r\n- Reworked config file logic to avoid duplicating of the default config (thanks @snoyer!)\r\n- Fixed a long standing issue with FBX skinning animation\r\n- Fixed zsh completion (thanks @allemangD!)\r\n- Fixed an opacity blending issue\r\n- Fixed a crash when dropping a HDRI while playing an animation\r\n- Fixed a focus issue on macOS\r\n- Fixed a high DPI issue on Windows\r\n\r\nFor libf3d users:\r\n- Added an API to control camera pan and zoom (thanks @snoyer!)\r\n- Added a tkinter python example (thanks @JPLost!)\r\n- Exposed log level in the python API (thanks @snoyer!)\r\n\r\n\r\nFor F3D packagers:\r\n- Fixed compatibility with CMake 3.29.1\r\n- Fixed compatibility with OCCT 7_8_0\r\n- Fixed build reproducibility by removing a path from the binary\r\n\r\n</details>\r\n\r\nThanks to all our packagers that help F3D reach a wider audience: AndnoVember @kylosus @kevinsmia1939 @yurivict @bcdarwin @svenstaro @mzf-guest @papoteur-mga @berolinux @topazus @thierry-FreeBSD @xiota @alerque @chenrui333\r\nThanks to all our contributors for this release: @allemangD @snoyer @kathleenhang @shambhupatil  @JPLost  @KeflerExe @kidharb @vikaskok @spevnev @Meakk @mwestphal @jubalh\r\n \r\n Credits:\r\n - Sir Frog - Chrono Trigger by [Adrian Carter](https://sketchfab.com/Adrian.Carter3D)\r\n - Castel St. Angelo Roof by [Andreas Mischok](https://polyhaven.com/all?a=Andreas%20Mischok)\r\n - Heartfelt shader by [BigWIngs](https://www.shadertoy.com/user/BigWIngs)\r\n - DamagedHelmet by ctxwing and theblueturtle_",v2.5.0,Mathieu Westphal,,mwestphal,"BSD 3-Clause ""New"" or ""Revised"" License",f3d,f3d-app,39,stl-viewer,gltf-viewer,vtk,3d-viewer,raytracing,physically-based-rendering,volume-rendering,kiss,command-line-tool,3d,graphics,gltf,rendering,3d-graphics,step,fbx,dxf,obj,glb,usd,/f3d-app/f3d,41,29,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/exoplanet-dev/exoplanet,https://github.com/exoplanet-dev/exoplanet,0,,0,0,1,1,0,0,0,0,0,0,0,0,Fast & scalable MCMC for all your exoplanet needs!,"<p align=""center"">\n  <img width=""240"" src=""https://raw.githubusercontent.com/exoplanet-dev/exoplanet/main/docs/_static/logo.png"">\n  <br><br>\n  <a href=""https://github.com/exoplanet-dev/exoplanet/actions/workflows/tests.yml"">\n    <img src=""https://github.com/exoplanet-dev/exoplanet/actions/workflows/tests.yml/badge.svg"" alt=""Tests"">\n  </a>\n  <a href=""https://docs.exoplanet.codes"">\n    <img src=""https://readthedocs.org/projects/exoplanet/badge/?version=latest"" alt=""Docs"">\n  </a>\n  <a href=""https://coveralls.io/github/exoplanet-dev/exoplanet?branch=main"">\n    <img src=""https://coveralls.io/repos/github/exoplanet-dev/exoplanet/badge.svg?branch=main"" alt=""Coverage"">\n  </a>\n</p>\n\n# exoplanet\n\nFast & scalable MCMC for all your exoplanet needs! _exoplanet_ is a toolkit for\nprobabilistic modeling of time series data in astronomy with a focus on\nobservations of [exoplanets](https://en.wikipedia.org/wiki/Exoplanet), using\n[PyMC](https://www.pymc.io). _PyMC_ is a flexible and high-performance\nmodel-building language and inference engine that scales well to problems with a\nlarge number of parameters. _exoplanet_ extends _PyMC_'s language to support\nmany of the custom functions and distributions required when fitting exoplanet\ndatasets.\n\nRead the full documentation at [docs.exoplanet.codes](https://docs.exoplanet.codes).\n\n## Installation\n\nThe quickest way to get started is to use [pip](https://pip.pypa.io):\n\n```bash\npython -m pip install ""exoplanet[pymc]""\n```\n\nNote that you will need Python (>=3.6) installed for this to work, but then this\nwill install all the required dependencies.\n\nCheck out the [main installation documentation\npage](https://docs.exoplanet.codes/en/latest/user/install/) for more options.\n\n## Usage\n\nCheck out the tutorials and API docs on [the docs\npage](https://docs.exoplanet.codes) for example usage and much more info. You\ncan also find more in-depth examples on the [exoplanet case studies\npage](https://gallery.exoplanet.codes).\n\n## Contributing\n\n_exoplanet_ is an open-source project, and we would love it if you wanted to\ncontribute. Check out [the developer\ndocumentation](https://docs.exoplanet.codes/en/latest/user/dev/) for more info\nabout getting started.\n",202,astronomy,Python,2,Python,TeX,,,,,,,,,,,,,,,,,,,,,,,,,,,173,12,157,4,17,15,186,15496,54,146,123,23,2320ddff77dd570b8a2065e8ddf48d601558e252,[pre-commit.ci] pre-commit autoupdate (#321),2024-07-18T23:34:39Z,pre-commit-ci[bot],66853113+pre-commit-ci[bot]@users.noreply.github.com,pre-commit-ci[bot],v0.6.0,"**Note:** This release will probably break your installation scripts. Since both PyMC3 and PyMC (v5+) are supported, you'll need to explicitly use:\r\n\r\n```bash\r\npip install ""exoplanet[pymc3]""\r\n```\r\n\r\nor \r\n\r\n```bash\r\npip install ""exoplanet[pymc]""\r\n```\r\n\r\ndepending on which version you want to use. Or, you can manually install the dependencies that you want, but you might need to pin `numpy` and/or `xarray` in some cases.\r\n\r\n## What's Changed\r\n* Fixing lexer issues for docs by @dfm in https://github.com/exoplanet-dev/exoplanet/pull/276\r\n* Pinning max xarray version by @dfm in https://github.com/exoplanet-dev/exoplanet/pull/316\r\n* Update requirements.txt by @gjgilbert in https://github.com/exoplanet-dev/exoplanet/pull/311\r\n* Support PyMC v5 by @vandalt in https://github.com/exoplanet-dev/exoplanet/pull/309\r\n* Updating publishing workflow by @dfm in https://github.com/exoplanet-dev/exoplanet/pull/322\r\n* Update setup-python action to v5 by @dfm in https://github.com/exoplanet-dev/exoplanet/pull/323\r\n\r\n## New Contributors\r\n* @gjgilbert made their first contribution in https://github.com/exoplanet-dev/exoplanet/pull/311\r\n* @vandalt made their first contribution in https://github.com/exoplanet-dev/exoplanet/pull/309\r\n\r\n**Full Changelog**: https://github.com/exoplanet-dev/exoplanet/compare/v0.5.3...v0.6.0",v0.6.0,Dan Foreman-Mackey,,dfm,MIT License,exoplanet,exoplanet-dev,40,exoplanet,exoplanet-transits,exoplanet-radial-velocities,mcmc,gaussian-processes,bayesian-inference,pymc3,time-series,time-series-analysis,astronomy,astrophysics,exoplanets,,,,,,,,,/exoplanet-dev/exoplanet,40,19,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/Esri/cityengine-sdk,https://github.com/Esri/cityengine-sdk,0,,,0,0,0,0,0,0,1,1,0,0,0,"CityEngine is a 3D city modeling software for urban design, visual effects, and VR/AR production. With its C++ SDK you can create plugins and standalone apps capable to execute CityEngine CGA procedural modeling rules.","# CityEngine SDK\n\nThis is the official site for the SDK of [CityEngine](https://www.esri.com/en-us/arcgis/products/esri-cityengine/overview), a 3D city modeling software for urban design, visual effects, and VR/AR production. Using the CityEngine SDK, a number of [open source plugins and APIs](https://www.esri.com/en-us/arcgis/products/arcgis-cityengine/integrations) have been built, allowing to integrate CityEngine technology into other tools.\n\nThe CityEngine SDK can be used for the development of:\n\n1. *Custom importers and exporters for CityEngine.*\n\n   This means the SDK enables you to develop CityEngine plugins to read or write additional 3D and image formats or your own proprietary 3D data format. A simple use case example is 3D printing where the STL geometry format is often needed. STL support is not provided out-of-the-box in CityEngine, but you can develop your own STL exporter as described below.\n\n2. *3D apps which need a procedural geometry engine.*\n\n   The core of CityEngine is its unique geometry generation engine, called Procedural Runtime (PRT). PRT takes as input an initial geometry and then applies a given rule package (= CGA rules authored in CityEngine) to generate more detailed 3D geometry as output. For example, PRT can generate - based on given rules - a 3D model of a building out of a parcel polygon. With the SDK you can integrate PRT in your own 3D applications taking full advantage of the procedural geometry generation without running CityEngine. An interesting use case example is [Palladio](https://github.com/esri/palladio), a plugin for SideFX’s Houdini software. Palladio includes PRT and therefore extends Houdini with the procedural geometry engine of CityEngine. Another use case example could be a specific cultural heritage 3D application which automatically generates detailed 3D models of temples based on input attributes.\n\nThis document explains how to install the CityEngine SDK and how to work with the source-code examples contained in this repository. The CityEngine SDK is packaged into three parts:\n\n1. This git repository with the example sources.\n2. An archive per platform with the SDK binaries and offline documentation. The archives are attached to the [releases](https://github.com/Esri/cityengine-sdk/releases).\n3. An archive with example data (e.g. rule packages and initial shapes), attached to the [releases](https://github.com/Esri/cityengine-sdk/releases) as well.\n\n## Quick Start\n\n1. Clone or download this repository.\n2. Download the [example data](https://github.com/Esri/cityengine-sdk/releases/latest/download/esri_ce_sdk-example_data-v3.zip) and unpack it into a `data` directory next to `examples` inside the repo:\n\n   ```text\n   /cityengine-sdk/\n      data/...\n      examples/...\n   ```\n\n3. Continue with building and running the `prt4cmd` example for either [Windows](examples/prt4cmd/README_windows.md) or [Linux](examples/prt4cmd/README_linux.md). This will automatically download the SDK archive for your platform.\n\n## Table of Contents\n\n- [CityEngine SDK](#cityengine-sdk)\n	- [Quick Start](#quick-start)\n	- [Table of Contents](#table-of-contents)\n	- [Downloads](#downloads)\n	- [Examples](#examples)\n	- [Documentation](#documentation)\n	- [Related Software Products](#related-software-products)\n	- [General Software Requirements](#general-software-requirements)\n		- [All Platforms](#all-platforms)\n		- [Windows](#windows)\n		- [Linux](#linux)\n	- [Release History and Changelog](#release-history-and-changelog)\n	- [CityEngine Resources](#cityengine-resources)\n	- [Issues and Contributions](#issues-and-contributions)\n	- [Licensing](#licensing)\n\n## Downloads\n\nYou can manually download the SDK archive, the open-source examples in this repository and the example data from the Assets section on the [Releases](https://github.com/Esri/cityengine-sdk/releases/latest) page.\n\n## Examples\n\nThis repository contains a number of source code examples in the `examples` directory. Each example contains a platform-specific README with detailed instructions how to build and use it:\n\n* [prt4cmd](examples/prt4cmd): a simple command line utility to apply rule packages onto initial shapes and generate models.\n* [stlenc](examples/stlenc): demonstrates how to write a custom encoder (exporter), in this case for the STL geometry format.\n* [stldec](examples/stldec): demonstrates how to write a custom decoder (importer) for the STL geometry format.\n\n## Documentation\nRead the [whitepaper](https://esri.github.io/cityengine-sdk/arcgis_prt_whitepaper.pdf) and the [architecture](https://esri.github.io/cityengine-sdk/arcgis_prt_architecture.pdf) overview pdfs or browse the [online reference](https://esri.github.io/cityengine-sdk). There is also an offline version in the downloaded sdk archive (/doc subdirectory).\n\n## Related Software Products\n\n* [Overview](https://www.esri.com/en-us/arcgis/products/arcgis-cityengine/integrations)\n* [PyPRT](https://esri.github.io/cityengine/pyprt): Python language bindings for the Procedural Runtime (PRT)\n* [Palladio](https://esri.github.io/cityengine/palladio): CityEngine plugin for SideFX Houdini\n* [Puma](https://esri.github.io/cityengine/puma): CityEngine plugin for McNeel Rhino3d\n* [Serlio](https://esri.github.io/cityengine/serlio): CityEngine plugin for Autodesk Maya\n* [Vitruvio](https://esri.github.io/cityengine/vitruvio): CityEngine plugin for Epic Unreal Engine\n\n## General Software Requirements\n\nPlease note that the individual example READMEs may include further requirements.\n\n### All Platforms\n\n* To load custom encoders built with the latest SDK, CityEngine 2024.0 is required. For older versions of CityEngine an older version of the SDK might have to be used, see [Release History](#release-history-and-changelog). Some SDK versions don't have a matching CityEngine.\n* A license for the corresponding CityEngine version. For example, a license for CityEngine 2024.0 to author Rule Packages for the current SDK release.\n* CMake 3.19 or later (<https://www.cmake.org>)\n\n### Windows\n\n* Windows 10 or 11 (64bit)\n* Required C++ compiler: Visual Studio 2022 with Toolset MSVC 14.37 or later\n* Required flags for extension libraries release mode: `/std:c++17 /bigobj /GR /EHsc /MD`\n\n### Linux\n\n* RedHat Enterprise Linux 8.x or 9.x and compatible (e.g. Alma Linux or Rocky Linux)\n* Required C++ compiler: GCC 11.2 or later (RedHat Enterprise Linux DevToolSet 11)\n* Required flags for extension libraries: `-std=c++17 -march=nocona -fvisibility=hidden -fvisibility-inlines-hidden -Wl,--exclude-libs,ALL`\n\n## Release History and Changelog\n\nA detailed list of changes to the API, CGA language and built-in codecs can be found in the [Changelog](changelog.md).\n\n* [v3.2.10211 (2024-07-11, CityEngine 2024.0)](https://github.com/Esri/cityengine-sdk/releases/tag/3.2.10211)\n* [v3.2.9903 (2024-06-11, ArcGIS Pro 3.3, Enterprise 11.3)](https://github.com/Esri/cityengine-sdk/releases/tag/3.2.9903)\n* [v3.1.9666 (2023-11-20, CityEngine 2023.1)](https://github.com/Esri/cityengine-sdk/releases/tag/3.1.9666)\n* [v3.0.8961 (2023-10-05, ArcGIS Pro 3.2, Enterprise 11.2)](https://github.com/Esri/cityengine-sdk/releases/tag/3.0.8961)\n* [v3.0.8905 (2023-06-10, CityEngine 2023.0)](https://github.com/Esri/cityengine-sdk/releases/tag/3.0.8905)\n* [v2.7.8603 (2023-02-22, ArcGIS Pro 3.1, Enterprise 11.1)](https://github.com/Esri/cityengine-sdk/releases/tag/2.7.8603)\n* [v2.7.8538 (2022-10-24, CityEngine 2022.1)](https://github.com/Esri/cityengine-sdk/releases/tag/2.7.8538)\n* [v2.6.8300 (2022-06-16, CityEngine 2022.0)](https://github.com/Esri/cityengine-sdk/releases/tag/2.6.8300)\n* [v2.6.8135 (2022-01-07, no matching CityEngine version)](https://github.com/Esri/cityengine-sdk/releases/tag/2.6.8135)\n* [v2.5.7799 (2021-10-22, CityEngine 2021.1)](https://github.com/Esri/cityengine-sdk/releases/tag/2.5.7799)\n* [v2.4.7316 (2021-05-27, CityEngine 2021.0)](https://github.com/Esri/cityengine-sdk/releases/tag/2.4.7316)\n* [v2.3.6821 (2020-10-22, CityEngine 2020.1)](https://github.com/Esri/cityengine-sdk/releases/tag/2.3.6821)\n* [v2.2.6332 (2020-06-09, CityEngine 2020.0)](https://github.com/Esri/cityengine-sdk/releases/tag/2.2.6332)\n* [v2.1.5705 (2019-12-11, CityEngine 2019.1)](https://github.com/Esri/cityengine-sdk/releases/tag/2.1.5705)\n* v2.1.5704 (2019-09-25, CityEngine 2019.1, replaced by v2.1.5705 with minor documentation fix)\n* [v2.0.5403 (2019-05-08, CityEngine 2019.0)](https://github.com/Esri/cityengine-sdk/releases/tag/2.0.5403)\n* [v1.10.4198 (2018-09-17, CityEngine 2018.1)](https://github.com/Esri/cityengine-sdk/releases/tag/1.10.4198)\n* [v1.10.4051 (2018-05-11, CityEngine 2018.0)](https://github.com/Esri/cityengine-sdk/releases/tag/1.10.4051)\n* [v1.9.3786 (2017-11-06, CityEngine 2017.1)](https://github.com/Esri/cityengine-sdk/releases/tag/1.9.3786)\n* [v1.8.3501 (2017-06-29, CityEngine 2017.0)](https://github.com/Esri/cityengine-sdk/releases/tag/1.8.3501)\n* [v1.7.2915 (2016-10-03, CityEngine 2016.1)](https://github.com/Esri/cityengine-sdk/releases/tag/1.7.2915)\n* [v1.6.2663 (2016-06-21, CityEngine 2016.0)](https://github.com/Esri/cityengine-sdk/releases/tag/1.6.2663)\n* [v1.4.2074 (2015-10-06, CityEngine 2015.2)](https://github.com/Esri/cityengine-sdk/releases/tag/1.4.2074)\n* [v1.3.1969 (2015-06-17, CityEngine 2015.1)](https://github.com/Esri/cityengine-sdk/releases/tag/1.3.1969)\n* [v1.3.1888 (2015-03-31, CityEngine 2015.0)](https://github.com/Esri/cityengine-sdk/releases/tag/1.3.1888)\n* [v1.2.1591 (2014-09-01, CityEngine 2014.1)](https://github.com/Esri/cityengine-sdk/releases/tag/1.2.1591)\n* [v1.1.1471 (2014-05-29, CityEngine 2014.0)](https://github.com/Esri/cityengine-sdk/releases/tag/1.1.1471)\n* v1.0.1209 (2014-01-15, CityEngine 2013.1)\n\n## CityEngine Resources\n\n* [CityEngine Community Forum](https://community.esri.com/t5/arcgis-cityengine/ct-p/arcgis-cityengine) \n* [CityEngine Free Trial](https://www.esri.com/en-us/arcgis/products/arcgis-cityengine/trial)\n* [CityEngine Product Page](https://www.esri.com/en-us/arcgis/products/arcgis-cityengine/overview)\n* [CityEngine Resources, Tutorials, Examples, Release Notes](https://www.esri.com/en-us/arcgis/products/arcgis-cityengine/resources)\n* [CityEngine Integrations (Urban, Plugins)](https://www.esri.com/en-us/arcgis/products/arcgis-cityengine/integrations)\n* Youtube: <https://www.youtube.com/c/CityEngineTV>\n* Facebook: <https://www.facebook.com/CityEngine>\n* Twitter: <https://twitter.com/CityEngine>\n\n## Issues and Contributions\n\nDid you find a bug or do you want to request a new feature? Please let us know by submitting an issue. Anyone and everyone is welcome to contribute and to extend and improve the examples by sending us pull requests.\n\n## Licensing\n\nThe CityEngine SDK is free for personal, educational, and non-commercial use. Commercial use requires at least one commercial license of the latest CityEngine version installed in the organization. Redistribution or web service offerings are not allowed unless expressly permitted.\n\nThe CityEngine SDK is licensed under the Esri Terms of Use:\n\n* <https://www.esri.com/en-us/legal/terms/full-master-agreement>\n* <https://www.esri.com/en-us/legal/terms/product-specific-scope-of-use>\n* All content in the ""Examples"" directory/section is licensed under the APACHE 2.0 license. You may obtain a copy of this license at <https://www.apache.org/licenses/LICENSE-2.0>.\n* For questions or enquiries regarding licensing, please contact the CityEngine team at cityengine-info@esri.com.\n",203,geometry,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,33,7,26,0,3,19,24,44675,64,40,40,0,cbe0aa0826b55126a5db0b62653e31036c5f7ba9,Merge pull request #74 from Esri/develop,2024-07-16T15:42:47Z,Martin Manzer,mmanzer@esri.com,mama10,CityEngine SDK 3.2.10211,"Welcome to CityEngine SDK 3.2.10211 for CityEngine 2024.0!\r\n\r\nThis SDK contains the CityEngine Procedural Runtime (PRT), documentation and C++ example code.\r\n\r\nPlease refer to the [README](README.md) in the repository root for instructions and how to build the SDK examples.\r\n\r\nThe latest changes in PRT are listed in the [changelog.md](changelog.md) and include:\r\n\r\n- Improvements and bug fixes to the CGA language: Support for Visual CGA designs, improvements for setback and convexify.\r\n- Improvements and bug fixes for materials and triangulations among all Decoders and Encoders.\r\n- ... and much more ...\r\n\r\nThank you for your interest in the CityEngine SDK!",3.2.10211,Martin Manzer,,mama10,Apache License 2.0,cityengine-sdk,Esri,27,cityengine,3d,c-plus-plus,sdk,api,geometry,processing,procedural,modelling,encoder,decoder,collada,fbx,alembic,native-development,cityengine-sdk,cross-platform,esri,usd,arcgis,/Esri/cityengine-sdk,27,44,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/epam/Indigo,https://github.com/epam/Indigo,0,,,0,1,1,0,0,0,1,0,0,0,0,"Universal cheminformatics toolkit, utilities and database search tools","![Build Status](https://github.com/epam/indigo/workflows/CI/badge.svg)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n\n# EPAM Indigo projects #\n\nCopyright (c) 2009-2022 EPAM Systems, Inc.\n\nLicensed under the [Apache License version 2.0](LICENSE)\n\n## Introduction ##\n\nThis repository includes:\n\n* Bingo: Chemistry search engine for Oracle, Microsoft SQL Server and PostgreSQL databases\n* Bingo-Elastic: Set of APIs for efficient chemistry search in Elasticsearch\n  - Java API. Full README is available [here](/bingo/bingo-elastic/java/README.md)\n  - Python API. Full README is available [here](/bingo/bingo-elastic/python/README.md)\n* Indigo: Universal cheminformatics library with bindings to .NET, Java, Python, R and WebAssembly, and the following tools:\n  - Legio: GUI application for combinatorial chemistry\n  - ChemDiff: Visual comparison of two SDF or SMILES files\n  - indigo-depict: Molecule and reaction rendering utility\n  - indigo-cano: Canonical SMILES generator\n  - indigo-deco: R-Group deconvolution utility\n\nDetailed documentation is available at <http://lifescience.opensource.epam.com>\n\nChangelog could be found in [CHANGELOG.md](/CHANGELOG.md).\n\n\n## Download ##\n<https://lifescience.opensource.epam.com/download/indigo/index.html>\n\n### Bindings in public repositories:\n* .NET: <https://www.nuget.org/packages/Indigo.Net>\n* Java: <https://search.maven.org/search?q=g:com.epam.indigo>\n* Python: <https://pypi.org/project/epam.indigo/>\n\n## Source code organization ##\n\nMain directory structure layout:\n* `api`: Indigo API sources\n* `bingo`: Bingo sources\n* `core`: Core algorithms and data structures sources\n* `third_party`: sources for third-party libraries\n* `utils`: utilities sources\n\nEach project is placed in the corresponding directory with CMakeList.txt configuration\nfile, that does not include other projects. In order to build the whole project with the\ncorrect references you need to use CMake configurations from the build_scripts directory.\n\n## Preinstalled build tools ##\n\nTo build the project from the sources, the following tools should be installed:\n\n* GIT 1.8.2+\n* C/C++ compilers with C++14 support (GCC, Clang and MSVC are officially supported)\n* CMake 3.4+\n* Python 3.6+\n* JDK 1.8+\n* .NET Standard 2.0+\n* Emscripten SDK\n* Ninja\n\n## Build instruction ##\n\nCreate build folder and use cmake with desired options. For instance:\n\n```\nIndigo/build>cmake .. -DBUILD_INDIGO=ON -DBUILD_INDIGO_WRAPPERS=ON -DBUILD_INDIGO_UTILS=ON\n```\n\nTo build Indigo from console:\n```\nIndigo/build>cmake --build . --config Release --target all\n```\n\nor any of the following targets could be specified: --target { indigo-dotnet | indigo-java | indigo-python }\nBuild results could be collected from Indigo/dist folder.\n\n## Run tests ##\n\nBefo run any test you have to build and install indigo-python\n1) Build indigo-python using '--target all' or '--target indigo=python'.\n   Package should be in 'build' directory, it will be named like 'epam.indigo-version-arch.whl'\n3) Install package using pip `python -m pip uninstall epam.indigo -y ; python -m pip install dist/epam.indigo-version-arch.whl`\n\nRun integration test using `python api/tests/integration/test.py -t 1` for all test, or `python api/tests/integration/test.py -t 1 -p test_name` to run tests by mask `test_name`.\n\nTo run backend API test:\n1) Install epam-indigo\n2) Install waitress `python pip install waitress`\n3) Run backend service :\n  * `cd utils/indigo-service/backend/service`\n  * `cp v2/common/config.py .`\n  * `waitress-serve --listen=""127.0.0.1:5000 [::1]:5000""  app:app` you may use any port instead of 5000\n4) Run backend API test:\n  * set environment variable `export INDIGO_SERVICE_URL=http://localhost:5000/v2` (in powershell `$env:INDIGO_SERVICE_URL=""http://localhost:5000/v2""`)\n  * run test `python utils/indigo-service/backend/service/tests/api/indigo_test.py` use `-k test_name` to run test by pattern.\n\n## How to build Indigo-WASM ##\n\n### Build tools prerequisites ###\n\n* Git\n\nMake sure git is running from path:\n```\n>git --version\ngit version 2.26.2.windows.1\n```\n\n* Python (https://www.python.org/downloads/)\n\nMake sure python is running from path:\n```\n>python --version\nPython 3.9.0\n```\n\n* cmake (https://cmake.org/download/)\n\nMake sure cmake is running from path:\n```\n>cmake --version\ncmake version 3.18.4\n```\n\n* Install ninja (https://github.com/ninja-build/ninja/releases)\n\nDownload corresponding ninja-xxx.zip and unpack to folder on path.\nMake sure it's running from path:\n```\n>ninja --version\n1.10.2\n```\n\n* Install emscripten sdk (https://github.com/emscripten-core/emsdk)\n\n```\n>git clone https://github.com/emscripten-core/emsdk.git\n>cd emsdk\n>./emsdk install latest\n>./emsdk activate latest\n>source ./emsdk_env.sh\n```\n\nNote: On Windows, run `emsdk` instead of `./emsdk`, and `emsdk_env.bat` instead of source `./emsdk_env.sh`, use `cmd` instead of `powershell`.\n\n### Get Indigo sources ###\n\nClone (or checkout) Indigo repository\n\n```\n>git clone https://github.com/epam/Indigo.git\n```\n\n### Build Indigo ###\n\nFor each new session, set environment anew:\n```\n>cd emsdk\n>./emsdk activate latest\n```\n\nIf fresh build:\n```\n>mkdir build\n>cd build\n```\n\nNow build:\n```\n>emcmake cmake .. -DCMAKE_BUILD_TYPE=Debug -G Ninja\n>ninja indigo-ketcher-js-test\n```\n",292,chemistry,C++,23,CMake,C#,C,C++,Java,Python,R,Makefile,Batchfile,Shell,PLSQL,NSIS,TSQL,PowerShell,Dockerfile,JavaScript,HTML,Pawn,PLpgSQL,Jupyter Notebook,CSS,TypeScript,SCSS,,,,,,854,92,729,33,164,96,0,232602,100,1264,749,515,5cdb34c329e2109a3f29b1d720ecc20400f544c7,#2124 UT under linux cause core dump (#2125),2024-07-15T14:31:45Z,Aliaksandr Dziarkach,18146690+AliaksandrDziarkach@users.noreply.github.com,AliaksandrDziarkach,indigo-1.22.0-rc.2,## What's Changed\r\n* #2067 Separator between connection data missed\r\n* #2060 Label for unknown monomer is wrong\r\n* #2055 Impot of RNA (RNA1{RP}$$$$V2.0) without base doesn't work\r\n* #2077 Export of CHEM-Nucleotide to HELM is wrong\r\n* #2064 System establishes connection between monomers even if data is wrong\r\n\r\n**Full Changelog**: https://github.com/epam/Indigo/compare/indigo-1.22.0-rc.1...indigo-1.22.0-rc.2,indigo-1.22.0-rc.2,Aliaksandr Dziarkach,,AliaksandrDziarkach,Apache License 2.0,Indigo,epam,24,indigo,bingo,cheminformatics,chemistry,python,dotnet,postgresql,webassembly,java,molfile,smiles,inchi,smarts,drug-discovery,molecule-visualization,,,,,,/epam/Indigo,125,46,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/ECSIM/opem,https://github.com/ECSIM/opem,1,,,1,1,1,1,0,0,0,0,0,0,1,OPEM (Open Source PEM Fuel Cell Simulation Tool),,194,physics,Python,6,Python,TeX,Shell,Batchfile,MATLAB,Dockerfile,,,,,,,,,,,,,,,,,,,,,,,154,24,130,0,3,7,30,18604,58,69,69,0,27d343fb966361a4e6cc99650241f651778fc50f,Version 1.4 (#215),2024-03-16T15:51:16Z,Sepand Haghighi,sepand.haghighi@yahoo.com,sepandhaghighi,Version 1.4,- `feature_request.yml` template added\r\n- `config.yml` for issue template added\r\n- Anaconda workflow added #177 \r\n- Discord badge added #178 \r\n- `notebook_to_html.py` script added\r\n- Bug report template modified\r\n- License updated\r\n- `Python 3.10` added to `test.yml` #162 \r\n- `Python 3.11` added to `test.yml` #183 \r\n- `Python 3.12` added to `test.yml` #200 \r\n- Logo updated\r\n- `AUTHORS.md` updated\r\n- `README.md` modified\r\n- `INSTALL.md` modified\r\n- Test system modified\r\n- Test files moved to `test` folder #202 \r\n- Setup system modified\r\n- Docstrings modified\r\n- CLI mode updated #182 \r\n- `description_control` function modified\r\n- `check_update` function modified #189 \r\n- `codecov` removed from `dev-requirements.txt`\r\n- Dockerfile updated\r\n- `Folder` parameter added to `Static_Analysis` function #174 \r\n- `Folder` parameter added to `Dynamic_Analysis` function #174 \r\n- `notebook_check.py` renamed to `notebook_run.py`\r\n- Document modified\r\n,v1.4,Sepand Haghighi,,sepandhaghighi,MIT License,opem,ECSIM,14,chemistry,pem,fuel-cell,opem,script,python,electrochemistry,dynamic-analysis,simulation,static-analysis,static-analyzer,simulator,physics,physics-simulation,,,,,,,/ECSIM/opem,14,12,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/ECP-WarpX/WarpX,https://github.com/ECP-WarpX/WarpX,1,,,1,1,1,1,0,0,0,0,0,0,1,"WarpX is an advanced, time-based electromagnetic & electrostatic Particle-In-Cell code.","# WarpX\n\n[![Code Status development](https://dev.azure.com/ECP-WarpX/WarpX/_apis/build/status/ECP-WarpX.WarpX?branchName=development)](https://dev.azure.com/ECP-WarpX/WarpX/_build/latest?definitionId=1&branchName=development)\n[![Nightly Installation Tests](https://dev.azure.com/ECP-WarpX/WarpX/_apis/build/status/ECP-WarpX.Nightly?branchName=nightly&label=nightly%20packages)](https://dev.azure.com/ECP-WarpX/WarpX/_build?definitionId=2)\n[![Documentation Status](https://readthedocs.org/projects/warpx/badge/?version=latest)](https://warpx.readthedocs.io)\n[![Spack Version](https://img.shields.io/spack/v/warpx)](https://spack.readthedocs.io/en/latest/package_list.html#warpx)\n[![Conda Version](https://img.shields.io/conda/vn/conda-forge/warpx)](https://anaconda.org/conda-forge/warpx)\n[![Discussions](https://img.shields.io/badge/chat-discussions-turquoise.svg)](https://github.com/ECP-WarpX/WarpX/discussions)  \n[![Supported Platforms](https://img.shields.io/badge/platforms-linux%20|%20osx%20|%20win-blue)](https://warpx.readthedocs.io/en/latest/install/users.html)\n[![GitHub commits since last release](https://img.shields.io/github/commits-since/ECP-WarpX/WarpX/latest/development.svg)](https://github.com/ECP-WarpX/WarpX/compare/development)\n[![Exascale Computing Project](https://img.shields.io/badge/supported%20by-ECP-orange)](https://www.exascaleproject.org/research/)\n[![Language: C++17](https://img.shields.io/badge/language-C%2B%2B17-orange.svg)](https://isocpp.org/)\n[![Language: Python](https://img.shields.io/badge/language-Python-orange.svg)](https://python.org/)  \n[![License WarpX](https://img.shields.io/badge/license-BSD--3--Clause--LBNL-blue.svg)](https://spdx.org/licenses/BSD-3-Clause-LBNL.html)\n[![DOI (source)](https://img.shields.io/badge/DOI%20(source)-10.5281/zenodo.4571577-blue.svg)](https://doi.org/10.5281/zenodo.4571577)\n[![DOI (paper)](https://img.shields.io/badge/DOI%20(paper)-10.1109/SC41404.2022.00008-blue.svg)](https://doi.org/10.1109/SC41404.2022.00008)\n\n## Overview\n\nWarpX is an advanced **electromagnetic & electrostatic Particle-In-Cell** code.\nIt supports many features including Perfectly-Matched Layers (PML), mesh refinement, and the boosted-frame technique.\n\nWarpX is a *highly-parallel and highly-optimized code*, which can run on GPUs and multi-core CPUs, and includes load balancing capabilities.\nWarpX scales to the world's largest supercomputers and was awarded the [2022 ACM Gordon Bell Prize](https://www.exascaleproject.org/ecp-supported-collaborative-teams-win-the-2022-acm-gordon-bell-prize-and-special-prize/).\n\n## Documentation\n\n[![PICMI](https://img.shields.io/static/v1?label=""works%20with""&message=""PICMI""&color=""blueviolet"")](https://picmi-standard.github.io)\n[![openPMD](https://img.shields.io/static/v1?label=""works%20with""&message=""openPMD""&color=""blueviolet"")](https://www.openPMD.org)\n[![yt-project](https://img.shields.io/static/v1?label=""works%20with""&message=""yt""&color=""blueviolet"")](https://yt-project.org)\n\nIn order to learn how to install and run the code, please see the online documentation:\nhttps://warpx.readthedocs.io\n\nTo contact the developers, feel free to open an issue on this repo, or visit our discussions page at https://github.com/ECP-WarpX/WarpX/discussions\n\n## Contributing\n\n[![AMReX](https://img.shields.io/static/v1?label=""runs%20on""&message=""AMReX""&color=""blueviolet"")](https://amrex-codes.github.io/)\n[![PICSAR](https://img.shields.io/static/v1?label=""runs%20on""&message=""PICSAR""&color=""blueviolet"")](https://picsar.net)\n[![openPMD-api](https://img.shields.io/static/v1?label=""runs%20on""&message=""openPMD-api""&color=""blueviolet"")](https://openpmd-api.readthedocs.io)\n[![ADIOS](https://img.shields.io/static/v1?label=""runs%20on""&message=""ADIOS""&color=""blueviolet"")](https://csmd.ornl.gov/adios)\n[![HDF5](https://img.shields.io/static/v1?label=""runs%20on""&message=""HDF5""&color=""blueviolet"")](https://www.hdfgroup.org/)\n[![Ascent](https://img.shields.io/static/v1?label=""runs%20on""&message=""Ascent""&color=""blueviolet"")](http://www.ascent-dav.org)\n[![SENSEI](https://img.shields.io/static/v1?label=""runs%20on""&message=""SENSEI""&color=""blueviolet"")](https://sensei-insitu.org)\n\nOur workflow is described in [CONTRIBUTING.rst](CONTRIBUTING.rst).\n\n## Copyright Notice\n\nWarpX Copyright (c) 2018, The Regents of the University of California,\nthrough Lawrence Berkeley National Laboratory (subject to receipt of any\nrequired approvals from the U.S. Dept. of Energy).  All rights reserved.\n\nIf you have questions about your rights to use or distribute this software,\nplease contact Berkeley Lab's Innovation & Partnerships Office at\nIPO@lbl.gov.\n\nPlease see the full license agreement in [LICENSE.txt](LICENSE.txt).\nPlease see the notices in [NOTICE.txt](NOTICE.txt).\nThe SPDX license identifier is `BSD-3-Clause-LBNL`.\n",278,physics,C++,8,Makefile,Python,Jupyter Notebook,C++,C,Shell,Roff,CMake,,,,,,,,,,,,,,,,,,,,,4149,321,3645,183,2,110,0,81308,179,820,468,352,8bf0fb0d7a63248b6aa1084a5294996034a31e43,"Doc: Lassen Stays TOSS3, Fix SciPy (#5063)",2024-07-19T22:40:53Z,Axel Huebl,axel.huebl@plasma.ninja,ax3l,24.07,[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.12659374.svg)](https://doi.org/10.5281/zenodo.12659374)\r\n\r\n# Dependencies\r\n\r\n- [AMReX](https://github.com/AMReX-Codes/amrex/tree/24.07): `24.07`\r\n- [PICSAR](https://github.com/ECP-WarpX/picsar/tree/23.09)-QED: `23.09`\r\n- [picmistandard](https://github.com/picmi-standard/picmi): release `0.28.0`\r\n- [pyAMReX](https://github.com/AMReX-Codes/pyamrex/tree/24.07): `24.07`\r\n- [openPMD-api](https://github.com/openPMD/openPMD-api): release `0.15.1`\r\n- Python: 3.8 - 3.12\r\n\r\n## What's Changed\r\n\r\n* Simplify main file by @lucafedeli88 in https://github.com/ECP-WarpX/WarpX/pull/4964\r\n* Fix FieldProbe Check: Particle Shape by @ax3l in https://github.com/ECP-WarpX/WarpX/pull/4983\r\n* CMake: heFFTe Support by @ax3l in https://github.com/ECP-WarpX/WarpX/pull/4986\r\n* Resetting collisionXYZ Temperature by @ax3l in https://github.com/ECP-WarpX/WarpX/pull/4999\r\n* Cleanup deposition and gather routines by @dpgrote in https://github.com/ECP-WarpX/WarpX/pull/4978\r\n* Documentation: fix a typo in parameters.rst by @lucafedeli88 in https://github.com/ECP-WarpX/WarpX/pull/5001\r\n* Doc: Update WarpX Ack Text by @ax3l in https://github.com/ECP-WarpX/WarpX/pull/5004\r\n* Doc: Resonant excitation of plasma waves in a plasma channel by @ax3l in https://github.com/ECP-WarpX/WarpX/pull/4998\r\n* Warning instead of error for Background MCC by @RevathiJambunathan in https://github.com/ECP-WarpX/WarpX/pull/4991\r\n* AMReX/pyAMReX/PICSAR: Weekly Update by @ax3l in https://github.com/ECP-WarpX/WarpX/pull/4995\r\n* Fix: Perlmutter heFFTe CPU by @ax3l in https://github.com/ECP-WarpX/WarpX/pull/5006\r\n* Fix ccache for macos by @WeiqunZhang in https://github.com/ECP-WarpX/WarpX/pull/5007\r\n* BLAS++: v2024.05.31+ by @ax3l in https://github.com/ECP-WarpX/WarpX/pull/5012\r\n* Doc: Latest Commit of BLAS++/LAPACK++ by @ax3l in https://github.com/ECP-WarpX/WarpX/pull/5013\r\n* AMReX/pyAMReX/PICSAR: Weekly Update by @ax3l in https://github.com/ECP-WarpX/WarpX/pull/5014\r\n* obvious bug fix in ElasticCollisionPerez by @JustinRayAngus in https://github.com/ECP-WarpX/WarpX/pull/5021\r\n* Document how to define EB with STL file by @RemiLehe in https://github.com/ECP-WarpX/WarpX/pull/5023\r\n* Add random seed to Examples/Tests/collision/inputs_3d by @dpgrote in https://github.com/ECP-WarpX/WarpX/pull/5022\r\n* Simplify VelocityProperties.cpp by @lucafedeli88 in https://github.com/ECP-WarpX/WarpX/pull/5016\r\n* Fix gatherParticlesFromEmbeddedBoundaries  by @lucafedeli88 in https://github.com/ECP-WarpX/WarpX/pull/5011\r\n* Update default behavior for gathering with direct deposition by @RemiLehe in https://github.com/ECP-WarpX/WarpX/pull/5024\r\n* Release 24.07 by @ax3l in https://github.com/ECP-WarpX/WarpX/pull/5025\r\n\r\n\r\n**Full Changelog**: https://github.com/ECP-WarpX/WarpX/compare/24.06...24.07,24.07,Axel Huebl,,ax3l,Other,WarpX,ECP-WarpX,61,laser,plasma,physics,gpu,simulation,particle-in-cell,pic,research,hpsf,,,,,,,,,,,,/ECP-WarpX/WarpX,61,14,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/Ecogenomics/GTDBTk,https://github.com/Ecogenomics/GTDBTk,1,,,1,1,1,1,0,0,0,0,0,0,1,GTDB-Tk: a toolkit for assigning objective taxonomic classifications to bacterial and archaeal genomes.,"# GTDB-Tk\n\n[![PyPI](https://img.shields.io/pypi/v/gtdbtk.svg)](https://pypi.python.org/pypi/gtdbtk)\n[![PyPI Downloads](https://pepy.tech/badge/gtdbtk)](https://pepy.tech/project/gtdbtk)\n[![Bioconda](https://img.shields.io/conda/vn/bioconda/gtdbtk.svg?color=43b02a)](https://anaconda.org/bioconda/gtdbtk)\n[![BioConda Downloads](https://img.shields.io/conda/dn/bioconda/gtdbtk.svg?style=flag&label=downloads&color=43b02a)](https://anaconda.org/bioconda/gtdbtk)\n[![Docker Image Version (latest by date)](https://img.shields.io/docker/v/ecogenomic/gtdbtk?sort=date&color=299bec&label=docker)](https://hub.docker.com/r/ecogenomic/gtdbtk)\n[![Docker Pulls](https://img.shields.io/docker/pulls/ecogenomic/gtdbtk?color=299bec&label=pulls)](https://hub.docker.com/r/ecogenomic/gtdbtk)\n\nGTDB-Tk is a software toolkit for assigning objective taxonomic classifications to bacterial and archaeal genomes based \non the Genome Database Taxonomy ([GTDB](https://gtdb.ecogenomic.org/)). It is designed to work with recent advances that \nallow hundreds or thousands of metagenome-assembled genomes (MAGs) to be obtained directly from environmental samples. \nIt can also be applied to isolate and single-cell genomes. The GTDB-Tk is open source and released under the \n[GNU General Public License (Version 3)](https://www.gnu.org/licenses/gpl-3.0.en.html).\n\nNotifications about GTDB-Tk releases will be available through the [GTDB Twitter](https://twitter.com/ace_gtdb) \naccount and the [GTDB Announcements Forum](https://forum.gtdb.ecogenomic.org/c/announcements/10).\n\nPlease post questions and issues related to GTDB-Tk on the Issues section of the GitHub repository. Questions \nrelated to the [GTDB](https://gtdb.ecogenomic.org/) can be posted on the [GTDB Forum](https://forum.gtdb.ecogenomic.org/) \nor sent to the [GTDB team](https://gtdb.ecogenomic.org/about).\n\n\n## 🚀 Getting started\n\nBe sure to check the [hardware requirements](https://ecogenomics.github.io/GTDBTk/installing/index.html), then choose your preferred method:\n\n* [Bioconda](https://ecogenomics.github.io/GTDBTk/installing/bioconda.html)\n* [Docker](https://ecogenomics.github.io/GTDBTk/installing/docker.html)\n* [pip](https://ecogenomics.github.io/GTDBTk/installing/pip.html)\n\n\n## 📖 Documentation\n\nDocumentation for GTDB-Tk can be found [here](https://ecogenomics.github.io/GTDBTk/).\n\n\n## ✨ New Features\n\nGTDB-Tk v2.4.0+ includes the following new features:\n- `FastANI` has been replaced by `skani` as the primary tool for computing Average Nucleotide Identity (ANI).Users may notice slight variations in the results compared to those obtained using `FastANI`.\n\n\n## 📈 Performance\nUsing ANI screen ""can"" reduce computation by >50%, although it depends on the set of input genomes. A set of input genomes consisting primarily of new species will not benefit from ANI screen as much as a set of genomes that are largely assigned to GTDB species clusters. In the latter case, the ANI screen will reduce the number of genomes that need to be classified by pplacer which reduces computation time substantially (between 25% and 60% in our testing).\n\n## 📚 References\n\nGTDB-Tk is described in:\n\n* Chaumeil PA, et al. 2022. [GTDB-Tk v2: memory friendly classification with the Genome Taxonomy Database](https://academic.oup.com/bioinformatics/advance-article-abstract/doi/10.1093/bioinformatics/btac672/6758240?utm_source=advanceaccess&utm_campaign=bioinformatics&utm_medium=email). <i>Bioinformatics</i>, btac672.\n* Chaumeil PA, et al. 2019. [GTDB-Tk: A toolkit to classify genomes with the Genome Taxonomy Database](https://academic.oup.com/bioinformatics/advance-article-abstract/doi/10.1093/bioinformatics/btz848/5626182). <i>Bioinformatics</i>, btz848.\n\nThe Genome Taxonomy Database (GTDB) is described in:\n\n* Parks, D.H., et al. (2021). [GTDB: an ongoing census of bacterial and archaeal diversity through a phylogenetically consistent, rank normalized and complete genome-based taxonomy](https://academic.oup.com/nar/advance-article/doi/10.1093/nar/gkab776/6370255). <i>Nucleic Acids Research</i>, <b>50</b>: D785–D794.\n* Rinke, C, et al. (2021). [A standardized archaeal taxonomy for the Genome Taxonomy Database](https://www.nature.com/articles/s41564-021-00918-8). <i>Nature Microbiology</i>, <b>6</b>: 946–959.\n* Parks, D.H., et al. 2020. [A complete domain-to-species taxonomy for Bacteria and Archaea](https://rdcu.be/b3OI7). <i>Nature Biotechnology</i>, https://doi.org/10.1038/s41587-020-0501-8.\n* Parks DH, et al. 2018. [A standardized bacterial taxonomy based on genome phylogeny substantially revises the tree of life](https://www.nature.com/articles/nbt.4229). <i>Nature Biotechnology</i>, http://dx.doi.org/10.1038/nbt.4229.\n \n\nWe strongly encourage you to cite the following 3rd party dependencies:\n\n* Matsen FA, et al. 2010. [pplacer: linear time maximum-likelihood and Bayesian phylogenetic placement of sequences onto a fixed reference tree](https://www.ncbi.nlm.nih.gov/pubmed/21034504). <i>BMC Bioinformatics</i>, 11:538.\n* Jain C, et al. 2019. [High-throughput ANI Analysis of 90K Prokaryotic Genomes Reveals Clear Species Boundaries](https://www.nature.com/articles/s41467-018-07641-9). <i>Nat. Communications</i>, doi: 10.1038/s41467-018-07641-9.\n* Shaw J. and Yu Y.W. 2023. [Fast and robust metagenomic sequence comparison through sparse chaining with skani](https://www.nature.com/articles/s41592-023-02018-3). <i>Nature Methods</i>, 20, pages1661–1665 (2023).\n* Hyatt D, et al. 2010. [Prodigal: prokaryotic gene recognition and translation initiation site identification](https://www.ncbi.nlm.nih.gov/pubmed/20211023). <i>BMC Bioinformatics</i>, 11:119. doi: 10.1186/1471-2105-11-119.\n* Price MN, et al. 2010. [FastTree 2 - Approximately Maximum-Likelihood Trees for Large Alignments](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2835736/). <i>PLoS One</i>, 5, e9490.\n* Eddy SR. 2011. [Accelerated profile HMM searches](https://www.ncbi.nlm.nih.gov/pubmed/22039361). <i>PLOS Comp. Biol.</i>, 7:e1002195.\n* Ondov BD, et al. 2016. [Mash: fast genome and metagenome distance estimation using MinHash](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0997-x). <i>Genome Biol</i> 17, 132. doi: 10.1186/s13059-016-0997-x.\n\n\n## © Copyright\n\nCopyright 2017 Pierre-Alain Chaumeil. See LICENSE for further details.\n",451,bioinformatics,Python,3,Python,Dockerfile,Shell,,,,,,,,,,,,,,,,,,,,,,,,,,150,4,146,0,11,19,196,30246,81,446,433,13,ce3ead34182f4adef16e96aa4ade5cc2de5e05b3,Update index.rst,2024-05-17T06:26:26Z,Pierre Chaumeil,uqpchaum@uq.edu.au,pchaumeil,02.04.2000,"Bug Fixes:\r\n\r\n* (#576) When all genomes fail the prodigal step in the `classify_wf`, The\r\nbac120 summary file is still produced with the all failed genomes listed as 'Unclassified'\r\n* (#573) When running the 3 classify steps independently, a genome can be filtered out in the `align`\r\nstep but still be classified in the `identify` step. To avoid duplication of row, the genome is classified with a warning.\r\n* (#540 ) Empty files are skipped during the sketch step of `Mash`,\r\nthey are then catched in the `prodigal` step and are returned as 'Unclassified'\r\n* (#549) : `--force` has been modified to deal with #540. `Prodigal`\r\nwasn't returning the empty files as failed genomes, it was only skipping them. These genomes are now returned in the summary file and flagged as Unclassified.\r\n\r\nMajor Changes:\r\n\r\n* `FastANI` has been replaced by `skani` as the primary tool for computing Average Nucleotide Identity (ANI).Users may notice slight variations in the results compared to those obtained using `FastANI`.\r\n* In the generated `summary.tsv` files, several columns have been renamed for clarity and consistency. The following columns have been affected:\r\n\r\n    - ""`fastani_reference`"" column has been renamed to ""`closest_genome_reference`"".\r\n    - ""`fastani_reference_radius`"" column has been renamed to ""`closest_genome_reference_radius`"".\r\n    - ""`fastani_taxonomy`"" column has been renamed to ""`closest_genome_taxonomy`"".\r\n    - ""`fastani_ani`"" column has been renamed to ""`closest_genome_ani`"".\r\n    - ""`fastani_af`"" column has been renamed to ""`closest_genome_af`"".\r\n\r\n These changes have been implemented to improve the readability and understanding of the data within the `summary.tsv` files. Users should update their scripts or processes accordingly to reflect these renamed column headers.\r\n\r\n",02.04.2000,Pierre Chaumeil,,pchaumeil,GNU General Public License v3.0,GTDBTk,Ecogenomics,42,taxonomy,species-assignments,phylogenetics,archaea,bacteria,nomenclature,bioinformatics,metagenomics,,,,,,,,,,,,,/Ecogenomics/GTDBTk,46,23,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/ebi-webcomponents/nightingale,https://github.com/ebi-webcomponents/nightingale,0,,,0,0,0,0,0,0,1,1,0,0,0,Data visualisation web components for the life sciences.,"[![Test and Publish Nightingale App](https://github.com/ebi-webcomponents/nightingale/workflows/Test%20and%20Publish%20Nightingale%20App/badge.svg)](https://github.com/ebi-webcomponents/nightingale/actions)\n\n# nightingale\n\nNightingale is a monorepo containing visualisation web components to use with\nbiological data.\n\n## Cite us\n\n**Nightingale: web components for protein feature visualization**, Bioinformatics Advances, Volume 3, Issue 1, 2023, vbad064, [https://doi.org/10.1093/bioadv/vbad064](https://doi.org/10.1093/bioadv/vbad064)\n\n## Documentation/Getting started\n\nDocumentation, getting started guide and examples for each of the components are\navailable here\n[https://ebi-webcomponents.github.io/nightingale](https://ebi-webcomponents.github.io/nightingale)\n\n## Contributing\n\nRead our guide [here](/CONTRIBUTING.md) as well as our\n[code of conduct](/CODE_OF_CONDUCT.md)\n\n## Installing\n\nNightingale uses [Lerna](https://lerna.js.org/) to manage its packages.\n\nFirst run `yarn` to install root packages.\n\n## Building the components\n\nRun `yarn build`\n\n## Showcase application\n\n### Locally\n\nRun `yarn build && yarn storybook` to run the application locally.\n\n### Build\n\nRun `yarn build-storybook` to build the application ready for deployment.\n",118,bioinformatics,TypeScript,5,JavaScript,HTML,CSS,TypeScript,MDX,,,,,,,,,,,,,,,,,,,,,,,,201,53,144,4,17,11,345,69293,34,89,77,12,aa7d93e663c2d1a9aeb927dcbf74f075fa15b679,v5.0.1,2024-05-29T13:25:05Z,Gustavo A. Salazar,gsalazar@ebi.ac.uk,gustavo-salazar,Version 5.0.0,### Major Version Release\r\n\r\n#### ⚠️ Breaking features\r\n* Drop support for Node 16. Components are now built using Node 18.\r\n\r\n\r\n#### 🌟 New Components\r\n* [`nightingale-sequence-heatmap`](https://ebi-webcomponents.github.io/nightingale/?path=/story/components-tracks-sequenceheatmap-readme--page)\r\n* [`nightingale-variation`](https://ebi-webcomponents.github.io/nightingale/?path=/story/components-tracks-variation-readme--page)\r\n\r\n\r\n#### 💫 Updates\r\n* [`nightingale-msa`](https://ebi-webcomponents.github.io/nightingale/?path=/story/components-tracks-alignments-readme--page)\r\n  * worker got rewritten to be inline to avoid incompatibilities between webpack or rollup builts.\r\n  * Exposing click and double click events\r\n* `nightingale-new-core`:\r\n   * new mixin: [`withSVGHighlight`](https://github.com/ebi-webcomponents/nightingale/blob/main/packages/nightingale-new-core/src/mixins/withHighlight/SVG/index.ts) To use when the component is a track that uses SVG to render. It renders a translucent and overlaying rectangle to represent the highlight.\r\n   * Now the mixins include the types of implemented mixins.\r\n   * The `height` attribute defined in withDimension now reflects its value when changed by the component.\r\n  \r\n   \r\n#### 🏡 Housekeeping\r\n* Dependencies updates.\r\n\r\n**Full Changelog**: https://github.com/ebi-webcomponents/nightingale/compare/v4.5.0...v5.0.0,v5.0.0,Gustavo A. Salazar,,gustavo-salazar,MIT License,nightingale,ebi-webcomponents,15,biohackeu21,bioinformatics,visualization,webcomponents,biohackeu22,,,,,,,,,,,,,,,,/ebi-webcomponents/nightingale,234,10,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/dotnet/Silk.NET,https://github.com/dotnet/Silk.NET,0,,0,0,0,0,0,0,0,1,0,0,0,0,"The high-speed OpenGL, OpenCL, OpenAL, OpenXR, GLFW, SDL, Vulkan, Assimp, WebGPU, and DirectX bindings library your mother warned you about.","<!-- Begin exclude from NuGet readme. -->\n<h1 align=""center"">\n    <a href=""#""><img align=""center"" src=""documentation/readme/silkdotnet_v3_horizontal_96.svg""></a>\n    <br />\n</h1>\n<div align=""center"">\n\n[![NuGet Version](https://img.shields.io/nuget/v/Silk.NET)](https://nuget.org/packages/Silk.NET)\n[![Preview Feed](https://img.shields.io/badge/nuget-experimental%20feed-yellow)](https://gitlab.com/silkdotnet/Silk.NET/-/packages)\n[![CI Build](https://github.com/Ultz/Silk.NET/workflows/CI%20Build/badge.svg)](https://github.com/dotnet/Silk.NET/actions/workflows/build.yml)\n[![Join our Discord](https://img.shields.io/badge/chat%20on-discord-7289DA)](https://discord.gg/DTHHXRt)\n\n</div>\n\n<div>\n<!-- End exclude from NuGet readme. -->\n<!-- Begin include in NuGet readme.\n![Silk.NET Logo](https://raw.githubusercontent.com/dotnet/Silk.NET/main/documentation/readme/silkdotnet_v3_horizontal_96.svg)\nEnd include in NuGet readme. -->\n\nSilk.NET is your one-stop-shop for high-speed .NET multimedia, graphics, and compute; providing bindings to popular low-level APIs such as OpenGL, OpenCL, OpenAL, OpenXR, GLFW, SDL, Vulkan, Assimp, WebGPU, and DirectX.\n\nUse Silk.NET to spruce up applications with cross-platform 3D graphics, audio, compute and haptics!\n\nSilk.NET works on any .NET Standard 2.0 compliant platform, including .NET 6.0, Xamarin, .NET Framework 4.6.1+, and .NET Core 2.0+.\n\n</div>\n<br />\n<div>\n<!-- Begin exclude from NuGet readme. -->\n<a href=""https://dotnetfoundation.org"" align=""right""><img src=""https://github.com/dotnet-foundation/swag/blob/main/logo/dotnetfoundation_v4.svg"" alt="".NET Foundation"" class=""logo-footer"" width=""72"" align=""left"">\n<!-- End exclude from NuGet readme. -->\n<!-- Begin include in NuGet readme.\n![.NET Foundation](https://raw.githubusercontent.com/dotnet/Silk.NET/main/documentation/readme/dotnetfoundation_v4_horizontal_64.svg)\nEnd include in NuGet readme. -->\n</a>\n<br />\n\nProud to be an official project under the benevolent [.NET Foundation](https://dotnetfoundation.org) umbrella.\n\n</div>\n\n<!-- Package description inserted here automatically. -->\n\n<h1 align=""center"">Features</h1>\n\n### Performance\n\nHaving poured lots of hours into examining generated C# code and its JIT assembly, you can count on us to deliver blazing fast bindings with negligible overhead induced by Silk.NET!\n\n### Up-to-date\n\nWith an efficient bindings regeneration mechanism, we are committed to ensuring our bindings reflect the latest specifications with frequent updates generated straight from the upstream sources.\n\n### High-level utilities\n\nIn addition to providing high-speed, direct, and transparent bindings, we provide high-level utilities and wrappers to maximise productivity in common workloads such as platform-agnostic abstractions around Windowing and Input, bringing your apps to a vast number of platforms without changing a single line!\n\n### Good-to-go\n\nSilk.NET caters for anything you could need in swift development of multimedia, graphics, compute applications. Silk.NET is an all-in-one solution, complete with Graphics, Compute, Audio, Input, and Windowing.\n\n<!-- Begin exclude from NuGet readme. -->\n\n<h1 align=""center"">The team</h1>\n\nWe currently have the following maintainers:\n- [Kai Jellinghaus](https://github.com/HurricanKai) [<img src=""https://about.twitter.com/etc/designs/about2-twitter/public/img/favicon.ico"" alt=""Follow Kai on Twitter"" width=""16"" />](https://twitter.com/intent/follow?screen_name=KJellinghaus)\n- [Thomas Mizrahi](https://github.com/ThomasMiz)\n- [Beyley Thomas](https://github.com/Beyley)\n\nIn addition, the Silk.NET working group help drive larger user-facing changes providing key consultation from the perspective of dedicated users and professionals.\n\n<h1 align=""center"">Building from source</h1>\n\n**Prerequisites**\n\n- .NET 6 SDK and .NET 7 SDK\n- Android, iOS, and Mac Catalyst workloads (use `dotnet workload install android ios maccatalyst` to install them)\n    - On Linux, `ios` and `maccatalyst` should be omitted as they are not available\n- Android SDK versions 31, 33, and 34 with NDK tools installed\n    - On Windows, for best results, this should be installed into `C:\ProgramData\Android\android-sdk`\n- Java JDK 11+\n- Visual Studio 2022 Community version 17.0 or later (optional)\n\n**Instructions**\n\n- Clone the repository\n    - Note: Avoid performing a recursive clone as the submodules are not necessary for a normal build\n- Run `build.sh`, `build.cmd`, `build.ps1`, or `nuke compile`\n    - On Linux, you may need to pass `--msbuild-properties AndroidSdkDirectory=/path/to/android/sdk`\n- Use the built assemblies\n    - To get `.nupkg`s that you can use with NuGet instead, use `nuke pack`\n\nThere are more advanced build actions you can do too, such as `FullBuild`, `Pack`, `FullPack`, among others which you can view by doing `nuke --plan`.\n\n<h1 align=""center"">Contributing</h1>\n\nSilk.NET uses and encourages [Early Pull Requests](https://medium.com/practical-blend/pull-request-first-f6bb667a9b6). Please don't wait until you're done to open a PR!\n\n1. [Fork Silk.NET](https://github.com/dotnet/Silk.NET/fork)\n2. Add an empty commit to a new branch to start your work off: `git commit --allow-empty -m ""start of [thing you're working on]""`\n3. Once you've pushed a commit, open a [**draft pull request**](https://github.blog/2019-02-14-introducing-draft-pull-requests/). Do this **before** you actually start working.\n4. Make your commits in small, incremental steps with clear descriptions.\n5. Tag a maintainer when you're done and ask for a review!\n\nThe Silk.NET solution is **very large**. Learn about how you can combat this using our build process in [CONTRIBUTING.md](CONTRIBUTING.md).\n\n<!-- End exclude from NuGet readme. -->\n\n<h2 align=""center"">Funding</h2>\nSilk.NET requires significant effort to maintain, as such we greatly appreciate any financial support you are able to provide!\n\nThis helps ensure Silk.NET's long term viability, and to help support the developers who maintain Silk.NET in their free time. [Kai](https://github.com/sponsors/HurricanKai) is accepting GitHub Sponsorships.\n\n<h1 align=""center"">Further resources</h1>\n\n- Several examples can be found in the [examples folder](https://github.com/dotnet/Silk.NET/tree/master/examples)\n- Come chat with us on [Discord](https://discord.gg/DTHHXRt)!\n\n<h1 align=""center"">Licensing and governance</h1>\n\nSilk.NET is distributed under the very permissive MIT/X11 license and all dependencies are distributed under MIT-compatible licenses.\n\nSilk.NET is a [.NET Foundation](https://www.dotnetfoundation.org/projects) project, and has adopted the code of conduct defined by the [Contributor Covenant](http://contributor-covenant.org/) to clarify expected behavior in our community. For more information, see the [.NET Foundation Code of Conduct](http://www.dotnetfoundation.org/code-of-conduct).\n\n<!-- Begin exclude from NuGet readme. -->\n\n---\n\n<div>\n    <a href=""https://www.jetbrains.com/?from=Silk.NET"" align=""right""><img src=""https://raw.githubusercontent.com/dotnet/Silk.NET/main/documentation/readme/jetbrains.svg"" alt=""JetBrains"" class=""logo-footer"" width=""72"" align=""left"">\n    <a><br/>\n\nSpecial thanks to [JetBrains](https://www.jetbrains.com/?from=Silk.NET) for supporting us with open-source licenses for their IDEs. </a>\n</div>\n\n<!-- End exclude from NuGet readme. -->\n\n",3925,graphics,C#,16,C#,Batchfile,Shell,GLSL,PowerShell,C,C++,Makefile,CMake,Java,HLSL,HTML,CSS,JavaScript,Zig,WGSL,,,,,,,,,,,,,1569,407,1148,14,39,102,303,1528338,377,515,461,54,b079b28cd51ce447183cfedde0a85412b9b226ee,Fix #2149 (#2237),2024-07-15T03:01:36Z,Dylan Perks,11160611+Perksey@users.noreply.github.com,Perksey,2.21 April 2024 Update,"Silk.NET April 2024 Update\r\n\r\n- Added nint overloads for the cases where OpenGL represents a native-sized integer as a void pointer.\r\n- Added support for executing Windowing loops during repaint and drag events on GLFW in some cases. (thanks @otac0n)\r\n- Added a distinction between ""unset"" (-1) and ""default"" (null) for Windowing depth/stencil buffer bits, defaulting to 24/8 on all platforms unless overridden.\r\n- Added support for 64-bit Arm Linux for Assimp, GLFW (+ Windowing &amp; Input), OpenAL Soft, SDL, Vkd3d, Vulkan Loader, SwiftShader, and WGPU. (thanks @alexrp)\r\n- Added support for Apple Silicon Macs for OpenAL Soft, Vulkan Loader, and SwiftShader. (thanks @alexrp)\r\n- Added support for 32-bit Arm Linux for Assimp, GLFW, OpenAL Soft, SDL, Shaderc, SPIR-V Cross, SPIR-V Reflect, Vkd3d, SwiftShader, and WGPU. (thanks @alexrp)\r\n- Added support for 64-bit Windows on Arm for Assimp, OpenAL Soft, VulkanLoader, and SwiftShader. (thanks @alexrp)\r\n- Added limited native library support (SPIR-V Tools and Shaderc) for glibc versions as old as 2.17 on Linux platforms.\r\n- Added support for new standard cursor shapes in Input and GLFW.\r\n- Updated to use `ref readonly` instead of `in`.\r\n- Updated to OpenXR 1.1.36.\r\n- Updated to DirectStorage 1.2.2.\r\n- Updated to Vulkan 1.3.281.\r\n- Updated to SDL 2.30.1.\r\n- Updated to latest OpenCL specifications.\r\n- Updated to latest SPIR-V Reflect.\r\n- Updated to latest WebGPU/Dawn/WGPU headers.\r\n- Updated Vkd3d native binaries.\r\n- Fixed string marshalling occasionally resulting in memory access violations on newer .NET 8 versions.\r\n- Fixed Windowing crashing on .NET Framework builds with prefer 32-bit set using GLFW.\r\n- Fixed extension loading generic type usage issues with full IL trimming enabled.\r\n- Fixed erroneous BreakneckLock acquisition logic slowing input events on mobile/SDL. (thanks @ZingBallyhoo)\r\n- Fixed windowing losing events when using multiple windows on desktop with the mobile/SDL implementation. (thanks @ZingBallyhoo)\r\n- Fixed some packages not referencing their native counterparts where appropriate. (thanks @alexrp)\r\n- Fixed Input events for standalone joysticks not being tracked on GLFW.\r\n- Fixed Input events no longer being tracked after 3-10 reopens of the same window on GLFW.\r\n- Fixed V-Sync configuration not persisting over window state changes in Windowing.\r\n- Fixed Circle.Contains erroneously comparing the squared vector distance from its center against its non-squared radius. (thanks @djoyahoy)\r\n\r\nAfter this update:\r\n- Apple Silicon support is complete except for missing ANGLE binaries.\r\n- Arm Linux support is complete except for missing DXVK, ANGLE, and Vulkan Loader (missing on 32-bit only) binaries.\r\n- 64-bit Windows on Arm support is complete except for missing DXVK and ANGLE binaries.\r\n- 32-bit Windows on Arm remains unsupported except for DirectStorage.",v2.21.0,Dylan Perks,,Perksey,MIT License,Silk.NET,dotnet,39,opengl,csharp,vulkan,openal,opencl,native,graphics,3d,graphics-library,game-development,scientific-visualization,glfw,silk,haptics,audio,webgpu,wgpu,,,,/dotnet/Silk.NET,47,69,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/dockstore/dockstore,https://github.com/dockstore/dockstore,0,Tool for creating scientific workflows,,0,1,1,0,0,0,0,0,0,0,0,Our VM/Docker sharing infrastructure and management component,"[![CircleCI](https://circleci.com/gh/dockstore/dockstore.svg?style=svg)](https://circleci.com/gh/dockstore/dockstore)\n[![codecov](https://codecov.io/gh/dockstore/dockstore/branch/develop/graph/badge.svg)](https://codecov.io/gh/dockstore/dockstore)\n[![Website](https://img.shields.io/website/https/dockstore.org.svg)](https://dockstore.org)\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7259426.svg)](https://doi.org/10.5281/zenodo.7259426)\n[![Uptime Robot status](https://img.shields.io/uptimerobot/status/m779655940-a297af07d1cac2d6ad40c491.svg)]()\n[![license](https://img.shields.io/hexpm/l/plug.svg?maxAge=2592000)](LICENSE)\n[![Documentation Status](https://readthedocs.org/projects/dockstore/badge/?version=stable)](https://dockstore.readthedocs.io/en/stable/?badge=stable)\n\n\n# Dockstore\n\nDockstore provides a place for users to share tools encapsulated in Docker and described with the Common \nWorkflow Language (CWL), WDL (Workflow Description Language), Nextflow, or Galaxy. This enables scientists to share analytical \nworkflows so that they are  machine readable as well as runnable in a variety of environments. While the \nDockstore is focused on serving researchers in the biosciences, the combination of Docker + workflow languages can be used by \nanyone to describe the tools and services in their Docker images in a standardized, machine-readable way.  \nDockstore is also a leading implementor of the GA4GH API standard for container registries, [TRS](https://www.ga4gh.org/news/tool-registry-service-api-enabling-an-interoperable-library-of-genomics-analysis-tools/). The usage of this is to enumerate the docker containers \n(from quay.io and docker hub) and the workflows (from github/bitbucket/local) that are available \nto users of Dockstore.org.\n\nFor the live site see [dockstore.org](https://dockstore.org)\n\nThis repo contains the web service component for Dockstore as well as collecting issues for the project as a whole. \n\nFor the related web UI see the [dockstore-ui](https://github.com/dockstore/dockstore-ui2) project.\nFor the related CLI see the [cli](https://github.com/dockstore/cli) project.\n\n## For Dockstore Users\n\nThe following section is useful for users of Dockstore (e.g. those that want to browse, register, and \nlaunch tools). \n\nAfter registering at [dockstore.org](https://dockstore.org), you will be able to download the Dockstore \nCLI at https://dockstore.org/onboarding\n\n### Configuration File\n\nA basic Dockstore configuration file is available/should be created in `~/.dockstore/config` and contains the following\nat minimum:\n```\ntoken = <your token generated by the dockstore site>\nserver-url = https://www.dockstore.org/api\n```\n\n## For Dockstore Developers\n\nThe following section is useful for Dockstore developers (e.g. those that want to improve or fix the Dockstore web service and UI)\n\n### Dependencies\n\nThe dependency environment for Dockstore is described by our \n[CircleCI config](https://github.com/dockstore/dockstore/blob/develop/.circleci/config.yml) or [docker compose](docker-compose.yml). In addition to the dependencies for \nDockstore users, note the setup instructions for postgres. Specifically, you will need to have postgres installed \nand setup with the database user specified in [.circleci/config.yml](https://github.com/dockstore/dockstore/blob/1.11.10/.circleci/config.yml#L279) (ideally, postgres is needed only for integration tests but not unit tests).\n\n### Building\n\nAs an alternative to the following commands, if you do not have Maven installed you can use the maven wrapper as a substitute. For example:\n\n    ./mvnw clean install\n    # instead of\n    mvn clean install\n\nIf you maven build in the root directory this will build all modules:\n\n    mvn clean install\n    # or\n    mvn clean install -Punit-tests\n    \nConsider the following if you need to build a specific version (such as in preparation for creating a tag for a release):\n\n    mvnw clean install  -Dchangelist=.0-beta.5 #or whatever version you need \n    \nIf you're running tests on CircleCI (or otherwise have access to the confidential data bundle) Run them via:\n\n    mvn clean install -Pintegration-tests\n    \nThere are also certain categories for tests that they can be added to when writing new tests. \nCategories include:\n\n1. `ToilCompatibleTest` are tests that can be run with our default cwltool and with Toil\n2. `ConfidentialTest` are tests that require access to our confidential testing bundle (ask a member of the development team if you're on the team)\n\n### Running Locally\n\nYou can also run it on your local computer but will need to setup postgres separately.\n\n1. Fill in the template dockstore.yml and stash it somewhere outside the git repo (like ~/.dockstore)\n2. The dockstore.yml is mostly a standard [Dropwizard configuration file](https://www.dropwizard.io/en/release-2.0.x/manual/configuration.html). \nRefer to the linked document to setup httpClient and database. \n3. Start with `java -jar dockstore-webservice/target/dockstore-webservice-*.jar   server ~/.dockstore/dockstore.yml`\n4. If you need integration with GitHub.com, Quay.io. or Bitbucket for your work, you will need to follow the appropriate \nsections below and then fill out the corresponding fields in your \n[dockstore.yml](https://github.com/dockstore/dockstore/blob/develop/dockstore-integration-testing/src/test/resources/dockstore.yml). \n\nOne alternative if you prefer running things in containers would be using [docker-compose](docker-compose.yml)\n\n### View Swagger UI\n\nThe Swagger UI is reachable while the Dockstore webservice is running. This allows you to explore available web resources.\n\n1. Browse to [http://localhost:8080/static/swagger-ui/index.html](http://localhost:8080/static/swagger-ui/index.html)\n\n### Commits using `[skipTests]`\nIf you would like to save build minutes on CircleCI (particularly for changes that do not affect code), consider adding \nthe `[skipTests]` tag to your commit message. When included in the most recent commit, a partial CI pipeline will be run,\nconsisting of only the build and unit tests.\n\n`[skipTests]` acts as an alternative to `[skip ci]`, which is provided by CircleCI. This is because our automatic \ndeployment process requires a build to be run on every tag.\n\n\n## Development\n\n### Coding Standards\n\n[codestyle.xml](codestyle.xml) defines the coding style for Dockstore as an IntelliJ Code Style XML file that should be imported into IntelliJ IDE. \nWe also have a matching [checkstyle.xml](checkstyle.xml) that can be imported into other IDEs and is run during the build.  \n\nFor users of Intellij or comparable IDEs, we also suggest loading the checkstyle.xml with a plugin in order to display warnings and errors while coding live rather than encountering them later when running a build. \n\n#### Installing git-secrets\n\nDockstore uses git-secrets to help make sure that keys and private data stay out\nof the source tree. For information on installing it on your platform check <https://github.com/awslabs/git-secrets#id6> .\n\nIf you're on mac with homebrew use `brew install git-secrets`.\n\n### Dockstore Command Line\n\nThe dockstore command line should be installed in a location in your path.\n\n  /dockstore-client/bin/dockstore\n\nYou then need to setup a `~/.dockstore/config` file with the following contents:\n\n```\ntoken: <dockstore_token_from_web_app>\nserver-url: http://www.dockstore.org:8080\n```\n\nIf you are working with a custom-built or updated dockstore client you will need to update the jar in: `~/.dockstore/config/self-installs`.\n\n### Swagger Client Generation \n\nWe use the swagger-codegen-maven-plugin to generate several sections of code which are not checked in. \nThese include\n1. All of swagger-java-client (talks to our webservice for the CLI via Swagger 2.0)\n2. All of openapi-java-client (talks to our webservice for the CLI, but in OpenAPI 3.0)\n3. The Tool Registry Server components (serves up the TRS endpoints)\n\nTo update these, you will need to point at a new version of the swagger.yaml provided by a service. For example, update the equivalent of [inputSpec](https://github.com/dockstore/dockstore/blob/0afe35682bdfb6fa7285b2acab8f80648346e835/dockstore-webservice/pom.xml#L854) in your branch.  \n\n### Encrypted Documents for CircleCI\n\nEncrypted documents necessary for confidential testing are decrypted via [decrypt.sh](scripts/decrypt.sh) with access being granted to developers at UCSC and OICR.\n\nA convenience script is provided as [encrypt.sh](encrypt.sh) which will compress confidential files, encrypt them, and then update an encrypted archive on GitHub. Confidential files should also be added to .gitignore to prevent accidental check-in. The unencrypted secrets.tar should be privately distributed among members of the team that need to work with confidential data. When using this script you will likely want to alter the [CUSTOM\_DIR\_NAME](https://github.com/dockstore/dockstore/blob/0b59791440af6e3d383d1aede1774c0675b50404/encrypt.sh#L13). This is necessary since running the script will overwrite the existing encryption keys, instantly breaking existing builds using that key. Our current workaround is to use a new directory when providing a new bundle. \n\n### Adding Copyright header to all files with IntelliJ\n\nTo add copyright headers to all files with IntelliJ\n\n1. Ensure the Copyright plugin is installed (Settings -> Plugins)\n2. Create a new copyright profile matching existing copyright header found on all files, name it Dockstore (Settings -> Copyright -> Copyright Profiles -> Add New)\n3. Set the default project copyright to Dockstore (Settings -> Copyright)\n\n[//]: # (DO_NOT_DELETE_START_MACOS_INSTRUCTIONS: This section is automatically generated by scripts/macos-instructions.sh)\n### Setting up a Mac for Dockstore development\nInstall Docker (Be sure to click on 'Mac with Apple chip' if you have Apple silicon)\nhttps://docs.docker.com/desktop/mac/install/\n\nInstall Brew\nhttps://brew.sh/\n```\n/bin/bash -c ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)""\n```\n\nRun 'git' to trigger the install of Xcode or the Command Line Tools which will install and or update git\nhttps://developer.apple.com/forums/thread/672087?answerId=659036022#659036022\n```\ngit\n```\n_(If that doesn't work install git manually https://git-scm.com/download/mac)_\n\n\nSetup git user information\n```\ngit config --global user.email ""you@example.com""\ngit config --global user.name ""Your Name""\n```\n[Read about git token requirements](https://github.blog/2020-12-15-token-authentication-requirements-for-git-operations/)\n\n[Setup personal access token for git CLI](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token)\n\n[It's helpful to cache your git personal access token](https://docs.github.com/en/get-started/getting-started-with-git/caching-your-github-credentials-in-git)\n\nInstall Hubflow\nhttps://datasift.github.io/gitflow/TheHubFlowTools.html\n```\ngit clone https://github.com/datasift/gitflow\ncd gitflow\nsudo ./install.sh\n```\n\nInstall JDK 17\nhttps://formulae.brew.sh/formula/openjdk@17\n```\nbrew install openjdk@17\n```\nDownload and install node.js\nhttps://github.com/nvm-sh/nvm\n```\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash\n```\nInstall git secrets\nhttps://github.com/awslabs/git-secrets\n```\nbrew install git-secrets\n```\nInstall wget\n```\nbrew install wget\n```\n\nInstall jq\n```\nbrew install jq\n```\n#### Build the webservice\n(cd to where you cloned the dockstore/dockstore repo)\n```\n./mvnw clean install\n```\n\n#### Build the UI\n(cd to where you cloned the dockstore/dockstore-ui2 repo)\n\nSet up UI requirements\nNOTE: You must use the --legacy-peer-deps switch due to using npm version 8.11.0 (> npm 6) \nfor reasons mentioned in [this post](https://stackoverflow.com/questions/66239691/what-does-npm-install-legacy-peer-deps-do-exactly-when-is-it-recommended-wh)\n```\nnpm ci --legacy-peer-deps\n```\n\nRun build\n```\nnpm run build\n```\n#### Optional\nInstall IntelliJ _(if on Apple Silicon, select the .dmg (Apple Silicon), otherwise select .dmg(Intel)_\n\nhttps://www.jetbrains.com/idea/download/#section=mac\n\nAdd the Scala plugin to IntelliJ\nhttps://www.jetbrains.com/help/idea/managing-plugins.html\n\n[//]: # (DO_NOT_DELETE_END_MACOS_INSTRUCTIONS: This section is automatically generated by scripts/macos-instructions.sh)\n\n\n### Database Schema Documentation \n\nThis is autogenerated at [https://dockstore.github.io/dockstore/](https://dockstore.github.io/dockstore/)\n\n\n### Legacy Material\n\nAdditional documentation on developing Dockstore is available at [legacy.md](https://github.com/dockstore/dockstore/blob/develop/legacy.md)\n",116,bioinformatics,Java,10,Java,HTML,Shell,Scala,Common Workflow Language,Nextflow,Dockerfile,Mustache,Jupyter Notebook,WDL,,,,,,,,,,,,,,,,,,,2486,429,2050,7,31,42,75,42417,27,3449,2939,510,3c672b6fa1ee52fab8ab71923eb842c49f42052f,"add ""state"" property to SourceFile",2024-07-18T18:56:14Z,Steve Von Worley,svonworl@ucsc.edu,svonworl,1.15.3,## What's Changed\r\n* Make athena life easier - duration in seconds by @denis-yuen in https://github.com/dockstore/dockstore/pull/5842\r\n* Update deprecated CircleCI images by @denis-yuen in https://github.com/dockstore/dockstore/pull/5896\r\n* fix regex for comments in .dockstore.yml and add test by @denis-yuen in https://github.com/dockstore/dockstore/pull/5910\r\n\r\n\r\n**Full Changelog**: https://github.com/dockstore/dockstore/compare/1.15.2...1.15.3,1.15.3,Denis Yuen,,denis-yuen,Apache License 2.0,dockstore,dockstore,147,dockstore,docker,cwl,wdl,containers,bioinformatics,nextflow,workflow,,,,,,,,,,,,,/dockstore/dockstore,351,22,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/DLR-SC/tigl,https://github.com/DLR-SC/tigl,1,,,1,1,1,1,0,0,0,0,0,0,1,The TiGL Geometry Library to process aircraft geometries in pre-design.,"<p><img src=""doc/images/logo.png"" alt=""TiGL Logo"" title=""TiGL Logo"" style=""background-color:white;padding:5px;""/></p>\n\n[![CI workflow for main branch](https://github.com/DLR-SC/tigl/actions/workflows/main.yml/badge.svg)](https://github.com/DLR-SC/tigl/actions/workflows/main.yml)\n[![codecov](https://codecov.io/gh/dlr-sc/tigl/branch/master/graph/badge.svg)](https://codecov.io/gh/dlr-sc/tigl)\n[![Apache 2.0](https://img.shields.io/crates/l/k)](https://github.com/DLR-SC/tigl/blob/cpacs_3/LICENSE.txt)\n[![Install with conda](https://anaconda.org/dlr-sc/tigl3/badges/version.svg)](https://anaconda.org/dlr-sc/tigl3/badges/version.svg)\n[![Cite-us](https://img.shields.io/badge/doi-10.1007%2Fs11786--019--00401--y-blue)](https://doi.org/10.1007/s11786-019-00401-y) \n[![Documentation](https://img.shields.io/badge/docs-online-green)](https://dlr-sc.github.io/tigl/doc/latest/) \n\nThe **Ti**GL **G**eometry **L**ibrary can be used for the computation and processing of aircraft geometries \nstored inside [CPACS](https://github.com/DLR-LY/CPACS) files. TiGL offers many geometry related functions such as\n - Point retrieval functions to compute points on the aircraft surface\n - Intersection functions to compute the intersection of the aircraft with planes\n - Export functions for standard CAD file formats (STEP + IGES) or mesh formats, \n   including VTK, Collada, and STL.\n   \nThe TiGL library uses the OpenCASCADE CAD kernel to represent the airplane geometry \nby NURBS surfaces. The library provides external interfaces for C, C++, Python, Java, MATLAB, and FORTRAN.\n\nTiGL is shipped with the Qt based _TiGL Viewer_ for visualizing aircraft\ngeometries or viewing CAD files.\n\n![Screenshot of the TiGL Viewer](doc/images/tiglviewer-web.jpg)\n\n# Downloads\n\n - Pre-Compiled Releases:  https://github.com/DLR-SC/tigl/wiki/Downloads\n - Nightly Builds:    https://github.com/DLR-SC/tigl/actions?query=workflow%3A%22Continuous+Integration%22+event%3Aschedule\n\n# News\n\nPlease head over to our TiGL website: https://dlr-sc.github.io/tigl/#news\n\n# Cite us\n\nTiGL is available as Open Source and we encourage anyone to make use of it. If you are applying TiGL in a scientific environment and publish any related work, please cite the following article:\n\nSiggel, M., Kleinert, J., Stollenwerk, T. et al.:  *TiGL: An Open Source Computational Geometry Library for Parametric Aircraft Design*, Math.Comput.Sci. (2019). https://doi.org/10.1007/s11786-019-00401-y\n\nA free copy of the paper is offered here: https://rdcu.be/bIGUH \n\n\n",231,geometry,C++,11,CMake,JavaScript,C++,C,Python,Java,Shell,Makefile,GLSL,Batchfile,SWIG,,,,,,,,,,,,,,,,,,398,24,371,3,26,25,67,179424,60,608,503,105,fd4cf85a498f1c2b453542997457fc3e27b18199,Merge pull request #1017 from DLR-SC/occ_cmake,2024-07-17T15:49:31Z,Jan Kleinert,jan.kleinert@dlr.de,joergbrech,TiGL 3.3.1,"Version 3.3.1\r\n-------------\r\n14/03/2024\r\n\r\n - General changes:\r\n\r\n   - Improve documentation and update installation instructions.\r\n   - TiGL now supports Opencascade Technology 7.6.2 (#973).\r\n   - Extension of the functionality of `::tiglwinginterpolatexsi` to work on a single segment that is not part of a component segment (#970). The function\r\n   is now generalized, so that the start and end uid can be either a segment or component segment and the two do not have to be related to each other.\r\n   - TiGL now supplies an environment.yml file that can be used with conda to install all build requirements of tigl into an Anaconda environment.\r\n   - Renamed the function `::CCPACSConfiguration::GetAirplaneLenth` to `::CCPACSConfiguration::GetAirplaneLength` (#992).\r\n   \r\n - Fixes:\r\n\r\n   - Fix bug in guide curve transformation. The rX-Direction was wrongly interpreted with respect to global coordinates, not wing coordinates (#975).\r\n   - Fix for hard crash if guiceCurves node is present but empty in wing segment (#962).\r\n   - Fix bug related to the calculation of Euler angles from a given rotation matrix (#870).\r\n\r\n - TiGLViewer:\r\n\r\n   - Improved visual quality of zebra stripes (#974).\r\n\r\n\r\n## Contributors\r\n@MarAlder, @svengoldberg, @joergbrech, @AntonReiswich, @merakulix, @rainman110 \r\n\r\n**Full Changelog**: https://github.com/DLR-SC/tigl/compare/v3.3.0...v3.3.1",v3.3.1,Jan Kleinert,,joergbrech,Apache License 2.0,tigl,DLR-SC,33,opencascade,aircraft,c-plus-plus,geometry,b-spline,cpacs,nurbs,,,,,,,,,,,,,,/DLR-SC/tigl,37,31,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/DiligentGraphics/DiligentCore,https://github.com/DiligentGraphics/DiligentCore,0,,,0,0,0,0,0,0,1,1,0,0,0,A modern cross-platform low-level graphics API,"# Diligent Core [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=An%20easy-to-use%20cross-platform%20graphics%20library%20that%20takes%20full%20advantage%20of%20%23Direct3D12%20and%20%23VulkanAPI&url=https://github.com/DiligentGraphics/DiligentEngine) <img src=""media/diligentgraphics-logo.png"" height=64 align=""right"" valign=""middle"">\n\nDiligent Core is a modern cross-platfrom low-level graphics API that makes the foundation of the [Diligent Engine](https://github.com/DiligentGraphics/DiligentEngine).\nThe module implements Direct3D11, Direct3D12, OpenGL, OpenGLES, and Vulkan rendering backends (Metal implementation is available for commercial clients),\nas well as basic platform-specific utilities. It is self-contained and can be built by its own.\nPlease refer to the [main repository](https://github.com/DiligentGraphics/DiligentEngine) for information about the supported platforms and features,\nbuild instructions, etc.\n\n| Platform             | Build Status  |\n| ---------------------| ------------- |\n|<img src=""media/windows-logo.png"" width=24 valign=""middle""> Win32               | [![Build Status](https://github.com/DiligentGraphics/DiligentCore/actions/workflows/build-windows.yml/badge.svg?branch=master)](https://github.com/DiligentGraphics/DiligentCore/actions/workflows/build-windows.yml?query=branch%3Amaster) |\n|<img src=""media/uwindows-logo.png"" width=24 valign=""middle""> Universal Windows  | [![Build Status](https://github.com/DiligentGraphics/DiligentCore/actions/workflows/build-windows.yml/badge.svg?branch=master)](https://github.com/DiligentGraphics/DiligentCore/actions/workflows/build-windows.yml?query=branch%3Amaster) |\n|<img src=""media/linux-logo.png"" width=24 valign=""middle""> Linux                 | [![Build Status](https://github.com/DiligentGraphics/DiligentCore/actions/workflows/build-linux.yml/badge.svg?branch=master)](https://github.com/DiligentGraphics/DiligentCore/actions/workflows/build-linux.yml?query=branch%3Amaster) |\n|<img src=""media/android-logo.png"" width=24 valign=""middle""> Android             | [![Build Status](https://github.com/DiligentGraphics/DiligentCore/actions/workflows/build-android.yml/badge.svg?branch=master)](https://github.com/DiligentGraphics/DiligentCore/actions/workflows/build-android.yml?query=branch%3Amaster) |\n|<img src=""media/macos-logo.png"" width=24 valign=""middle""> MacOS                 | [![Build Status](https://github.com/DiligentGraphics/DiligentCore/actions/workflows/build-apple.yml/badge.svg?branch=master)](https://github.com/DiligentGraphics/DiligentCore/actions/workflows/build-apple.yml?query=branch%3Amaster) |\n|<img src=""media/apple-logo.png"" width=24 valign=""middle""> iOS                   | [![Build Status](https://github.com/DiligentGraphics/DiligentCore/actions/workflows/build-apple.yml/badge.svg?branch=master)](https://github.com/DiligentGraphics/DiligentCore/actions/workflows/build-apple.yml?query=branch%3Amaster) |\n|<img src=""media/tvos-logo.png"" width=24 valign=""middle""> tvOS                   | [![Build Status](https://github.com/DiligentGraphics/DiligentCore/actions/workflows/build-apple.yml/badge.svg?branch=master)](https://github.com/DiligentGraphics/DiligentCore/actions/workflows/build-apple.yml?query=branch%3Amaster) |\n|<img src=""media/emscripten-logo.png"" width=24 valign=""middle""> Emscripten       | [![Build Status](https://github.com/DiligentGraphics/DiligentCore/actions/workflows/build-emscripten.yml/badge.svg?branch=master)](https://github.com/DiligentGraphics/DiligentCore/actions/workflows/build-emscripten.yml?query=branch%3Amaster) | \n\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](License.txt)\n[![Chat on Discord](https://img.shields.io/discord/730091778081947680?logo=discord)](https://discord.gg/t7HGBK7)\n[![Appveyor Build Status](https://ci.appveyor.com/api/projects/status/github/DiligentGraphics/DiligentCore?svg=true)](https://ci.appveyor.com/project/DiligentGraphics/diligentcore)\n[![CodeQL Scanning](https://github.com/DiligentGraphics/DiligentCore/actions/workflows/codeql.yml/badge.svg?branch=master)](https://github.com/DiligentGraphics/DiligentCore/actions/workflows/codeql.yml?query=branch%3Amaster)\n[![MSVC Analysis](https://github.com/DiligentGraphics/DiligentCore/actions/workflows/msvc_analysis.yml/badge.svg?branch=master)](https://github.com/DiligentGraphics/DiligentCore/actions/workflows/msvc_analysis.yml?query=branch%3Amaster)\n[![Lines of Code](https://tokei.rs/b1/github.com/DiligentGraphics/DiligentCore)](https://github.com/DiligentGraphics/DiligentCore)\n\n\n# Table of Contents\n\n- [Cloning the Repository](#cloning)\n- [API Basics](#api_basics)\n  - [Initializing the Engine](#initialization)\n    - [Win32](#initialization_win32)\n    - [Universal Windows Platform](#initialization_uwp)\n    - [Linux](#initialization_linux)\n    - [MacOS](#initialization_macos)\n    - [Android](#initialization_android)\n    - [iOS](#initialization_ios)\n    - [Emscripten](#initialization_emscripten)\n    - [Destroying the Engine](#initialization_destroying)\n  - [Creating Resources](#creating_resources)\n  - [Creating Shaders](#creating_shaders)\n  - [Initializing Pipeline State](#initializing_pso)\n    - [Pipeline Resource Layout](#pipeline_resource_layout)\n  - [Binding Shader Resources](#binding_resources)\n  - [Setting the Pipeline State and Invoking Draw Command](#draw_command)\n- [Low-level API interoperability](#low_level_api_interoperability)\n- [NuGet package build instructions](#nuget_build_instructions)\n- [License](#license)\n- [Contributing](#contributing)\n- [Release History](#release_history)\n\n\n<a name=""cloning""></a>\n# Cloning the Repository\n\nTo get the repository and all submodules, use the following command:\n\n```\ngit clone --recursive https://github.com/DiligentGraphics/DiligentCore.git\n```\n\nTo build the module, see \n[build instructions](https://github.com/DiligentGraphics/DiligentEngine/blob/master/README.md#build-and-run-instructions) \nin the master repository.\n\n<a name=""api_basics""></a>\n# API Basics\n\n<a name=""initialization""></a>\n## Initializing the Engine\n\nBefore you can use any functionality provided by the engine, you need to create a render device, an immediate context and a swap chain.\n\n<a name=""initialization_win32""></a>\n### Win32\nOn Win32 platform, you can create OpenGL, Direct3D11, Direct3D12 or Vulkan device as shown below:\n\n```cpp\nvoid InitializeDiligentEngine(HWND NativeWindowHandle)\n{\n    SwapChainDesc SCDesc;\n    // RefCntAutoPtr<IRenderDevice>  m_pDevice;\n    // RefCntAutoPtr<IDeviceContext> m_pImmediateContext;\n    // RefCntAutoPtr<ISwapChain>     m_pSwapChain;\n    switch (m_DeviceType)\n    {\n        case RENDER_DEVICE_TYPE_D3D11:\n        {\n            EngineD3D11CreateInfo EngineCI;\n#    if ENGINE_DLL\n            // Load the dll and import GetEngineFactoryD3D11() function\n            auto* GetEngineFactoryD3D11 = LoadGraphicsEngineD3D11();\n#    endif\n            auto* pFactoryD3D11 = GetEngineFactoryD3D11();\n            pFactoryD3D11->CreateDeviceAndContextsD3D11(EngineCI, &m_pDevice, &m_pImmediateContext);\n            Win32NativeWindow Window{hWnd};\n            pFactoryD3D11->CreateSwapChainD3D11(m_pDevice, m_pImmediateContext, SCDesc,\n                                                FullScreenModeDesc{}, Window, &m_pSwapChain);\n        }\n        break;\n\n        case RENDER_DEVICE_TYPE_D3D12:\n        {\n#    if ENGINE_DLL\n            // Load the dll and import GetEngineFactoryD3D12() function\n            auto GetEngineFactoryD3D12 = LoadGraphicsEngineD3D12();\n#    endif\n            EngineD3D12CreateInfo EngineCI;\n\n            auto* pFactoryD3D12 = GetEngineFactoryD3D12();\n            pFactoryD3D12->CreateDeviceAndContextsD3D12(EngineCI, &m_pDevice, &m_pImmediateContext);\n            Win32NativeWindow Window{hWnd};\n            pFactoryD3D12->CreateSwapChainD3D12(m_pDevice, m_pImmediateContext, SCDesc,\n                                                FullScreenModeDesc{}, Window, &m_pSwapChain);\n        }\n        break;\n\n    case RENDER_DEVICE_TYPE_GL:\n    {\n#    if EXPLICITLY_LOAD_ENGINE_GL_DLL\n        // Load the dll and import GetEngineFactoryOpenGL() function\n        auto GetEngineFactoryOpenGL = LoadGraphicsEngineOpenGL();\n#    endif\n        auto* pFactoryOpenGL = GetEngineFactoryOpenGL();\n\n        EngineGLCreateInfo EngineCI;\n        EngineCI.Window.hWnd = hWnd;\n\n        pFactoryOpenGL->CreateDeviceAndSwapChainGL(EngineCI, &m_pDevice, &m_pImmediateContext,\n                                                   SCDesc, &m_pSwapChain);\n    }\n    break;\n\n    case RENDER_DEVICE_TYPE_VULKAN:\n    {\n#    if EXPLICITLY_LOAD_ENGINE_VK_DLL\n        // Load the dll and import GetEngineFactoryVk() function\n        auto GetEngineFactoryVk = LoadGraphicsEngineVk();\n#    endif\n        EngineVkCreateInfo EngineCI;\n\n        auto* pFactoryVk = GetEngineFactoryVk();\n        pFactoryVk->CreateDeviceAndContextsVk(EngineCI, &m_pDevice, &m_pImmediateContext);\n        Win32NativeWindow Window{hWnd};\n        pFactoryVk->CreateSwapChainVk(m_pDevice, m_pImmediateContext, SCDesc, Window, &m_pSwapChain);\n    }\n    break;\n\n    default:\n        std::cerr << ""Unknown device type"";\n    }\n}\n```\n\nOn Windows, the engine can be statically linked to the application or built as a separate DLL. In the first case,\nfactory functions `GetEngineFactoryOpenGL()`, `GetEngineFactoryD3D11()`, `GetEngineFactoryD3D12()`, and `GetEngineFactoryVk()`\ncan be called directly. In the second case, you need to load the DLL into the process's address space using `LoadGraphicsEngineOpenGL()`,\n`LoadGraphicsEngineD3D11()`, `LoadGraphicsEngineD3D12()`, or `LoadGraphicsEngineVk()` function. Each function loads appropriate\ndynamic library and imports the functions required to initialize the engine. You need to include the following headers:\n\n```cpp\n#include ""EngineFactoryD3D11.h""\n#include ""EngineFactoryD3D12.h""\n#include ""EngineFactoryOpenGL.h""\n#include ""EngineFactoryVk.h""\n\n```\n\nYou also need to add the following directories to the include search paths:\n\n* `DiligentCore/Graphics/GraphicsEngineD3D11/interface`\n* `DiligentCore/Graphics/GraphicsEngineD3D12/interface`\n* `DiligentCore/Graphics/GraphicsEngineOpenGL/interface`\n* `DiligentCore/Graphics/GraphicsEngineVulkan/interface`\n\nAs an alternative, you may only add the path to the root folder and\nthen use include paths relative to it.\n\nEnable `Diligent` namespace:\n\n```cpp\nusing namespace Diligent;\n```\n\n`IEngineFactoryD3D11::CreateDeviceAndContextsD3D11()`, `IEngineFactoryD3D12::CreateDeviceAndContextsD3D12()`, and \n`IEngineFactoryVk::CreateDeviceAndContextsVk()` functions can also create a specified number of immediate and deferred contexts,\nwhich can be used for asynchronous rendering and multi-threaded command recording. The contexts may only be created during\nthe initialization of the engine. The function populates an array of pointers to the contexts, where the immediates contexts go first,\nfollowed by all deferred contexts.\n\nFor more details, take a look at \n[Tutorial00_HelloWin32.cpp](https://github.com/DiligentGraphics/DiligentSamples/blob/master/Tutorials/Tutorial00_HelloWin32/src/Tutorial00_HelloWin32.cpp) \nfile.\n\n<a name=""initialization_uwp""></a>\n### Universal Windows Platform\n\nOn Universal Windows Platform, you can create Direct3D11 or Direct3D12 device. Initialization is performed the same\nway as on Win32 Platform. The difference is that you first create the render device and device contexts by\ncalling `IEngineFactoryD3D11::CreateDeviceAndContextsD3D11()` or `IEngineFactoryD3D12::CreateDeviceAndContextsD3D12()`.\nThe swap chain is created later by a call to `IEngineFactoryD3D11::CreateSwapChainD3D11()` or `IEngineFactoryD3D12::CreateSwapChainD3D12()`.\nPlease look at\n[SampleAppUWP.cpp](https://github.com/DiligentGraphics/DiligentSamples/blob/master/SampleBase/src/UWP/SampleAppUWP.cpp) \nfile for more details.\n\n<a name=""initialization_linux""></a>\n### Linux\n\nOn Linux platform, the engine supports OpenGL and Vulkan backends. Initialization of GL context on Linux is tightly\ncoupled with window creation. As a result, Diligent Engine does not initialize the context, but\nattaches to the one initialized by the app. An example of the engine initialization on Linux can be found in\n[Tutorial00_HelloLinux.cpp](https://github.com/DiligentGraphics/DiligentSamples/blob/master/Tutorials/Tutorial00_HelloLinux/src/Tutorial00_HelloLinux.cpp).\n\n<a name=""initialization_macos""></a>\n### MacOS\n\nOn MacOS, Diligent Engine supports OpenGL, Vulkan and Metal backends. Initialization of GL context on MacOS is\nperformed by the application, and the engine attaches to the context created by the app; see\n[GLView.mm](https://github.com/DiligentGraphics/DiligentTools/blob/master/NativeApp/Apple/Source/Classes/OSX/GLView.mm)\nfor details. Vulkan backend is initialized similar to other platforms. See \n[MetalView.mm](https://github.com/DiligentGraphics/DiligentTools/blob/master/NativeApp/Apple/Source/Classes/OSX/MetalView.mm).\n\n<a name=""initialization_android""></a>\n### Android\n\nOn Android, you can create OpenGLES or Vulkan device. The following code snippet shows an example:\n\n```cpp\nauto* pFactoryOpenGL = GetEngineFactoryOpenGL();\nEngineGLCreateInfo EngineCI;\nEngineCI.Window.pAWindow = NativeWindowHandle;\npFactoryOpenGL->CreateDeviceAndSwapChainGL(\n    EngineCI, &m_pDevice, &m_pContext, SCDesc, &m_pSwapChain);\n```\n\nIf the engine is built as dynamic library, the library needs to be loaded by the native activity. The following code shows one possible way:\n\n```java\nstatic\n{\n    try{\n        System.loadLibrary(""GraphicsEngineOpenGL"");\n    } catch (UnsatisfiedLinkError e) {\n        Log.e(""native-activity"", ""Failed to load GraphicsEngineOpenGL library.\n"" + e);\n    }\n}\n```\n\n<a name=""initialization_ios""></a>\n### iOS\n\niOS implementation supports OpenGLES, Vulkan and Metal backend. Initialization of GL context on iOS is\nperformed by the application, and the engine attaches to the context initialized by the app; see\n[EAGLView.mm](https://github.com/DiligentGraphics/DiligentTools/blob/master/NativeApp/Apple/Source/Classes/iOS/EAGLView.mm)\nfor details.\n\n<a name=""initialization_emscripten""></a>\n### Emscripten\n\nOn Emscripten, you can create OpenGLES device. The following code snippet shows an example:\n```cpp\n//You need to pass the id of the canvas to NativeWindow\nauto* pFactoryOpenGL = GetEngineFactoryOpenGL();\nEngineGLCreateInfo EngineCI = {};\nEngineCI.Window = NativeWindow{""#canvas""};\npFactoryOpenGL->CreateDeviceAndSwapChainGL(EngineCI, &m_pDevice, &m_pContext, SCDesc, &m_pSwapChain);\n```\nIf you are using SDL or GLFW with existing context, you can provide null as the native window handle:\n`EngineCI.Window = NativeWindow{nullptr}`\n\n<a name=""initialization_destroying""></a>\n### Destroying the Engine\n\nThe engine performs automatic reference counting and shuts down when the last reference to an engine object is released.\n\n<a name=""creating_resources""></a>\n## Creating Resources\n\nDevice resources are created by the render device. The two main resource types are buffers,\nwhich represent linear memory, and textures, which use memory layouts optimized for fast filtering.\nTo create a buffer, you need to populate `BufferDesc` structure and call `IRenderDevice::CreateBuffer()`.\nThe following code creates a uniform (constant) buffer:\n\n```cpp\nBufferDesc BuffDesc;\nBuffDesc.Name           = ""Uniform buffer"";\nBuffDesc.BindFlags      = BIND_UNIFORM_BUFFER;\nBuffDesc.Usage          = USAGE_DYNAMIC;\nBuffDesc.uiSizeInBytes  = sizeof(ShaderConstants);\nBuffDesc.CPUAccessFlags = CPU_ACCESS_WRITE;\nm_pDevice->CreateBuffer(BuffDesc, nullptr, &m_pConstantBuffer);\n```\n\nSimilar, to create a texture, populate `TextureDesc` structure and call `IRenderDevice::CreateTexture()` as in the following example:\n\n```cpp\nTextureDesc TexDesc;\nTexDesc.Name      = ""My texture 2D"";\nTexDesc.Type      = TEXTURE_TYPE_2D;\nTexDesc.Width     = 1024;\nTexDesc.Height    = 1024;\nTexDesc.Format    = TEX_FORMAT_RGBA8_UNORM;\nTexDesc.Usage     = USAGE_DEFAULT;\nTexDesc.BindFlags = BIND_SHADER_RESOURCE | BIND_RENDER_TARGET | BIND_UNORDERED_ACCESS;\nTexDesc.Name      = ""Sample 2D Texture"";\nm_pRenderDevice->CreateTexture(TexDesc, nullptr, &m_pTestTex);\n```\n\nThere is only one function `CreateTexture()` that is capable of creating all types of textures. Type, format,\narray size and all other parameters are specified by the members of the `TextureDesc` structure.\n\nFor every bind flag specified during the texture creation time, the texture object creates a default view.\nDefault shader resource view addresses the entire texture, default render target and depth stencil views reference\nall array slices in the most detailed mip level, and unordered access view references the entire texture. To get a\ndefault view from the texture, use `ITexture::GetDefaultView()` function. Note that this function does not increment\nthe reference counter of the returned interface. You can create additional texture views using `ITexture::CreateView()`.\nUse `IBuffer::CreateView()` to create additional views of a buffer.\n\n<a name=""creating_shaders""></a>\n## Creating Shaders\n\nTo create a shader, populate `ShaderCreateInfo` structure:\n\n```cpp\nShaderCreateInfo ShaderCI;\n```\n\nThere are three ways to create a shader. The first way is to provide a pointer to the shader source code through \n`ShaderCreateInfo::Source` member. The second way is to provide a file name. The third way is to provide a pointer\nto the compiled byte code through `ShaderCreateInfo::ByteCode` member. Graphics Engine is entirely decoupled\nfrom the platform. Since the host file system is platform-dependent, the structure exposes\n`ShaderCreateInfo::pShaderSourceStreamFactory` member that is intended to give the engine access to the file system.\nIf you provided the source file name, you must also provide a non-null pointer to the shader source stream factory.\nIf the shader source contains any `#include` directives, the source stream factory will also be used to load these\nfiles. The engine provides default implementation for every supported platform that should be sufficient in most cases.\nYou can however define your own implementation.\n\nAn important member is `ShaderCreateInfo::SourceLanguage`. The following are valid values for this member:\n\n* `SHADER_SOURCE_LANGUAGE_DEFAULT` - The shader source format matches the underlying graphics API: HLSL for D3D11 or D3D12 mode, and GLSL for OpenGL, OpenGLES, and Vulkan modes.\n* `SHADER_SOURCE_LANGUAGE_HLSL`    - The shader source is in HLSL. For OpenGL and OpenGLES modes, the source code will be \n                                     converted to GLSL. In Vulkan back-end, the code will be compiled to SPIRV directly.\n* `SHADER_SOURCE_LANGUAGE_GLSL`    - The shader source is in GLSL.\n* `SHADER_SOURCE_LANGUAGE_GLSL_VERBATIM` - The shader source language is GLSL and should be compiled verbatim.\n* `SHADER_SOURCE_LANGUAGE_MSL`     - The source language is Metal Shading Language.\n\nOther members of the `ShaderCreateInfo` structure define the shader include search directories, shader macro definitions,\nshader entry point and other parameters.\n\n```cpp\nShaderMacroHelper Macros;\nMacros.AddShaderMacro(""USE_SHADOWS"", 1);\nMacros.AddShaderMacro(""NUM_SHADOW_SAMPLES"", 4);\nMacros.Finalize();\nShaderCI.Macros = Macros;\n```\n\nWhen everything is ready, call `IRenderDevice::CreateShader()` to create the shader object:\n\n```cpp\nShaderCreateInfo ShaderCI;\nShaderCI.Desc.Name         = ""MyPixelShader"";\nShaderCI.FilePath          = ""MyShaderFile.fx"";\nShaderCI.EntryPoint        = ""MyPixelShader"";\nShaderCI.Desc.ShaderType   = SHADER_TYPE_PIXEL;\nShaderCI.SourceLanguage    = SHADER_SOURCE_LANGUAGE_HLSL;\nconst auto* SearchDirectories = ""shaders;shaders\\inc;"";\nRefCntAutoPtr<IShaderSourceInputStreamFactory> pShaderSourceFactory;\nm_pEngineFactory->CreateDefaultShaderSourceStreamFactory(SearchDirectories, &pShaderSourceFactory);\nShaderCI.pShaderSourceStreamFactory = pShaderSourceFactory;\nRefCntAutoPtr<IShader> pShader;\nm_pDevice->CreateShader(ShaderCI, &pShader);\n```\n\n<a name=""initializing_pso""></a>\n## Initializing Pipeline State\n\nDiligent Engine follows Direct3D12/Vulkan style to configure the graphics/compute pipeline. One monolithic Pipelines State Object (PSO)\nencompasses all required states (all shader stages, input layout description, depth stencil, rasterizer and blend state\ndescriptions etc.). To create a graphics pipeline state object, define an instance of `GraphicsPipelineStateCreateInfo` structure:\n\n```cpp\nGraphicsPipelineStateCreateInfo PSOCreateInfo;\nPipelineStateDesc&              PSODesc = PSOCreateInfo.PSODesc;\n\nPSODesc.Name = ""My pipeline state"";\n```\n\nDescribe the pipeline specifics such as the number and format of render targets as well as depth-stencil format:\n\n```cpp\n// This is a graphics pipeline\nPSODesc.PipelineType                            = PIPELINE_TYPE_GRAPHICS;\nPSOCreateInfo.GraphicsPipeline.NumRenderTargets = 1;\nPSOCreateInfo.GraphicsPipeline.RTVFormats[0]    = TEX_FORMAT_RGBA8_UNORM_SRGB;\nPSOCreateInfo.GraphicsPipeline.DSVFormat        = TEX_FORMAT_D32_FLOAT;\n```\n\nInitialize depth-stencil state description `DepthStencilStateDesc`. Note that the constructor initializes\nthe members with default values and you may only set the ones that are different from default.\n\n```cpp\n// Init depth-stencil state\nDepthStencilStateDesc& DepthStencilDesc = PSOCreateInfo.GraphicsPipeline.DepthStencilDesc;\nDepthStencilDesc.DepthEnable            = true;\nDepthStencilDesc.DepthWriteEnable       = true;\n```\n\nInitialize blend state description `BlendStateDesc`:\n\n```cpp\n// Init blend state\nBlendStateDesc& BSDesc = PSOCreateInfo.GraphicsPipeline.BlendDesc;\nBSDesc.IndependentBlendEnable = False;\nauto &RT0 = BSDesc.RenderTargets[0];\nRT0.BlendEnable           = True;\nRT0.RenderTargetWriteMask = COLOR_MASK_ALL;\nRT0.SrcBlend              = BLEND_FACTOR_SRC_ALPHA;\nRT0.DestBlend             = BLEND_FACTOR_INV_SRC_ALPHA;\nRT0.BlendOp               = BLEND_OPERATION_ADD;\nRT0.SrcBlendAlpha         = BLEND_FACTOR_SRC_ALPHA;\nRT0.DestBlendAlpha        = BLEND_FACTOR_INV_SRC_ALPHA;\nRT0.BlendOpAlpha          = BLEND_OPERATION_ADD;\n```\n\nInitialize rasterizer state description `RasterizerStateDesc`:\n\n```cpp\n// Init rasterizer state\nRasterizerStateDesc& RasterizerDesc = PSOCreateInfo.GraphicsPipeline.RasterizerDesc;\nRasterizerDesc.FillMode              = FILL_MODE_SOLID;\nRasterizerDesc.CullMode              = CULL_MODE_NONE;\nRasterizerDesc.FrontCounterClockwise = True;\nRasterizerDesc.ScissorEnable         = True;\nRasterizerDesc.AntialiasedLineEnable = False;\n```\n\nInitialize input layout description `InputLayoutDesc`:\n\n```cpp\n// Define input layout\nInputLayoutDesc& Layout = PSOCreateInfo.GraphicsPipeline.InputLayout;\nLayoutElement LayoutElems[] =\n{\n    LayoutElement( 0, 0, 3, VT_FLOAT32, False ),\n    LayoutElement( 1, 0, 4, VT_UINT8,   True ),\n    LayoutElement( 2, 0, 2, VT_FLOAT32, False ),\n};\nLayout.LayoutElements = LayoutElems;\nLayout.NumElements    = _countof(LayoutElems);\n```\n\nDefine primitive topology and set shader pointers:\n\n```cpp\n// Define shader and primitive topology\nPSOCreateInfo.GraphicsPipeline.PrimitiveTopology = PRIMITIVE_TOPOLOGY_TRIANGLE_LIST;\nPSOCreateInfo.pVS = m_pVS;\nPSOCreateInfo.pPS = m_pPS;\n```\n\n<a name=""pipeline_resource_layout""></a>\n### Pipeline Resource Layout\n\nPipeline resource layout informs the engine how the application is going to use different shader resource variables.\nTo allow grouping of resources based on the expected frequency of resource bindings changes, Diligent Engine introduces\nclassification of shader variables:\n\n* **Static variables** (`SHADER_RESOURCE_VARIABLE_TYPE_STATIC`) are variables that are expected to be set only once.\n  They may not be changed once a resource is bound to the variable. Such variables are intended to hold global constants such \n  as camera attributes or global light attributes constant buffers. Note that it is the *resource binding* that may not change,\n  while the contents of the resource is allowed to change according to its usage.\n* **Mutable variables** (`SHADER_RESOURCE_VARIABLE_TYPE_MUTABLE`) define resources that are expected to change on a per-material frequency.\n  Examples may include diffuse textures, normal maps etc. Again updates to the contents of the resource are orthogobal\n  to the binding changes.\n* **Dynamic variables** (`SHADER_RESOURCE_VARIABLE_TYPE_DYNAMIC`) are expected to change frequently and randomly.\n\nTo define variable types, prepare an array of `ShaderResourceVariableDesc` structures and\ninitialize `PSODesc.ResourceLayout.Variables` and `PSODesc.ResourceLayout.NumVariables` members. Also\n`PSODesc.ResourceLayout.DefaultVariableType` can be used to set the type that will be used if a variable name is not provided.\n\n```cpp\nShaderResourceVariableDesc ShaderVars[] =\n{\n    {SHADER_TYPE_PIXEL, ""g_StaticTexture"",  SHADER_RESOURCE_VARIABLE_TYPE_STATIC},\n    {SHADER_TYPE_PIXEL, ""g_MutableTexture"", SHADER_RESOURCE_VARIABLE_TYPE_MUTABLE},\n    {SHADER_TYPE_PIXEL, ""g_DynamicTexture"", SHADER_RESOURCE_VARIABLE_TYPE_DYNAMIC}\n};\nPSODesc.ResourceLayout.Variables           = ShaderVars;\nPSODesc.ResourceLayout.NumVariables        = _countof(ShaderVars);\nPSODesc.ResourceLayout.DefaultVariableType = SHADER_RESOURCE_VARIABLE_TYPE_STATIC;\n```\n\nWhen creating a pipeline state, textures can be permanently assigned immutable samplers. If an immutable sampler is assigned to a texture,\nit will always be used instead of the one initialized in the texture shader resource view. To define immutable samplers,\nprepare an array of `ImmutableSamplerDesc` structures and initialize `PSODesc.ResourceLayout.ImmutableSamplers` and\n`PSODesc.ResourceLayout.NumImmutableSamplers` members. Notice that immutable samplers can be assigned to a texture variable of any type,\nnot necessarily static, so that the texture binding can be changed at run-time, while the sampler will stay immutable.\nIt is highly recommended to use immutable samplers whenever possible.\n\n```cpp\nImmutableSamplerDesc ImtblSampler;\nImtblSampler.ShaderStages   = SHADER_TYPE_PIXEL;\nImtblSampler.Desc.MinFilter = FILTER_TYPE_LINEAR;\nImtblSampler.Desc.MagFilter = FILTER_TYPE_LINEAR;\nImtblSampler.Desc.MipFilter = FILTER_TYPE_LINEAR;\nImtblSampler.TextureName    = ""g_MutableTexture"";\nPSODesc.ResourceLayout.NumImmutableSamplers = 1;\nPSODesc.ResourceLayout.ImmutableSamplers    = &ImtblSampler;\n```\n\n[This document](https://github.com/DiligentGraphics/DiligentCore/blob/master/doc/TextureSamplers.md) provides a detailed\ninformation about working with texture samplers.\n\nWhen all required fields of PSO description structure are set, call `IRenderDevice::CreateGraphicsPipelineState()`\nto create the PSO object:\n\n```cpp\nm_pDevice->CreateGraphicsPipelineState(PSOCreateInfo, &m_pPSO);\n```\n\n<a name=""binding_resources""></a>\n## Binding Shader Resources\n\nAs mentioned above, [shader resource binding in Diligent Engine](http://diligentgraphics.com/2016/03/23/resource-binding-model-in-diligent-engine-2-0/)\nis based on grouping variables in 3 different groups (static, mutable and dynamic). Static variables are variables that are\nexpected to be set only once. They may not be changed once a resource is bound to the variable. Such variables are intended\nto hold global constants such as camera attributes or global light attributes constant buffers. They are bound directly to the\nPipeline State Object:\n\n```cpp\nm_pPSO->GetStaticShaderVariable(SHADER_TYPE_PIXEL, ""g_tex2DShadowMap"")->Set(pShadowMapSRV);\n```\n\nMutable and dynamic variables are bound via a new object called Shader Resource Binding (SRB), which is created by the pipeline state\n(`IPipelineState::CreateShaderResourceBinding()`), or pipeline resource signature in advanced use cases:\n\n```cpp\nm_pPSO->CreateShaderResourceBinding(&m_pSRB, true);\n```\n\nThe second parameter tells the system to initialize internal structures in the SRB object\nthat reference static variables in the PSO.\n\nDynamic and mutable resources are then bound through the SRB object:\n\n```cpp\nm_pSRB->GetVariable(SHADER_TYPE_PIXEL,  ""tex2DDiffuse"")->Set(pDiffuseTexSRV);\nm_pSRB->GetVariable(SHADER_TYPE_VERTEX, ""cbRandomAttribs"")->Set(pRandomAttrsCB);\n```\n\nThe difference between mutable and dynamic resources is that mutable resources can only be set once per instance\nof a shader resource binding. Dynamic resources can be set multiple times. It is important to properly set the variable type as\nthis affects performance. Static and mutable variables are more efficient. Dynamic variables are more expensive\nand introduce some run-time overhead.\n\nAn alternative way to bind shader resources is to create an `IResourceMapping` interface that maps resource literal names to the\nactual resources:\n\n```cpp\nResourceMappingEntry Entries[] =\n{\n    {""g_Texture"", pTexture->GetDefaultView(TEXTURE_VIEW_SHADER_RESOURCE)}\n};\nResourceMappingCreateInfo ResMappingCI;\nResMappingCI.pEntries   = Entries;\nResMappingCI.NumEntries = _countof(Entries);\nRefCntAutoPtr<IResourceMapping> pResMapping;\npRenderDevice->CreateResourceMapping(ResMappingCI, &pResMapping);\n```\n\nThe resource mapping can then be used to bind all static resources in a pipeline state (`IPipelineState::BindStaticResources()`):\n\n```cpp\nm_pPSO->BindStaticResources(SHADER_TYPE_VERTEX | SHADER_TYPE_PIXEL, pResMapping,\n                            BIND_SHADER_RESOURCES_VERIFY_ALL_RESOLVED);\n```\n\nor all mutable and dynamic resources in a shader resource binding (`IShaderResourceBinding::BindResources()`):\n\n```cpp\nm_pSRB->BindResources(SHADER_TYPE_VERTEX | SHADER_TYPE_PIXEL, pResMapping,\n                      BIND_SHADER_RESOURCES_VERIFY_ALL_RESOLVED);\n```\n\nThe last parameter to all `BindResources()` functions defines how resources should be resolved:\n\n* `BIND_SHADER_RESOURCES_UPDATE_STATIC` - Indicates that static variable bindings are to be updated.\n* `BIND_SHADER_RESOURCES_UPDATE_MUTABLE` - Indicates that mutable variable bindings are to be updated.\n* `BIND_SHADER_RESOURCES_UPDATE_DYNAMIC` -Indicates that dynamic variable bindings are to be updated.\n* `BIND_SHADER_RESOURCES_UPDATE_ALL` - Indicates that all variable types (static, mutable and dynamic) are to be updated.\n   Note that if none of `BIND_SHADER_RESOURCES_UPDATE_STATIC`, `BIND_SHADER_RESOURCES_UPDATE_MUTABLE`, and \n   `BIND_SHADER_RESOURCES_UPDATE_DYNAMIC` flags are set, all variable types are updated as if `BIND_SHADER_RESOURCES_UPDATE_ALL` was specified.\n* `BIND_SHADER_RESOURCES_KEEP_EXISTING` - If this flag is specified, only unresolved bindings will be updated. All existing bindings will keep their original values. If this flag is not specified, every shader variable will be updated if the mapping contains corresponding resource.\n* `BIND_SHADER_RESOURCES_VERIFY_ALL_RESOLVED` - If this flag is specified, all shader bindings are expected be resolved after the call. If this is not the case, an error will be reported.\n\n`BindResources()` may be called several times with different resource mappings to bind resources.\nHowever, it is recommended to use one large resource mapping as the size of the mapping does not affect element search time.\n\nThe engine performs run-time checks to verify that correct resources are being bound. For example, if you try to bind\na constant buffer to a shader resource view variable, an error will be output to the debug console.\n\n<a name=""draw_command""></a>\n## Setting the Pipeline State and Invoking Draw Command\n\nBefore any draw command can be invoked, all required vertex and index buffers as well as the pipeline state should\nbe bound to the device context:\n\n```cpp\n// Set render targets before issuing any draw command.\nauto* pRTV = m_pSwapChain->GetCurrentBackBufferRTV();\nauto* pDSV = m_pSwapChain->GetDepthBufferDSV();\nm_pContext->SetRenderTargets(1, &pRTV, pDSV, RESOURCE_STATE_TRANSITION_MODE_TRANSITION);\n\n// Clear render target and depth-stencil\nconst float zero[4] = {0, 0, 0, 0};\nm_pContext->ClearRenderTarget(pRTV, ClearColor, RESOURCE_STATE_TRANSITION_MODE_TRANSITION);\nm_pContext->ClearDepthStencil(pDSV, CLEAR_DEPTH_FLAG, 1.f, 0, RESOURCE_STATE_TRANSITION_MODE_TRANSITION);\n\n// Set vertex and index buffers\nIBuffer* buffer[] = {m_pVertexBuff",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/devkitPro/deko3d,https://github.com/devkitPro/deko3d,0,,,0,0,0,0,0,0,1,1,0,0,0,Homebrew low level graphics API for Nintendo Switch (Nvidia Tegra X1),"# 凸 deko3d ( ͡° ͜ʖ ͡°)\n\n![deko3d logo](res/logo-300w.png)\n\ndeko3d is a brand new homebrew low level 3D graphics API targetting the Nvidia Tegra X1 processor found inside the Nintendo Switch. It provides the following features:\n\n- Low level and explicit API reminiscent of Vulkan®: deko3d is modeled after popular low level concepts such as devices, queues, command buffers, and much more. deko3d thus provides a window into the inner workings of the GPU.\n- Low overhead and explicit resource management: deko3d lets developers take control of resource management, and make decisions that make most sense in each case. In addition, care was taken to avoid unnecessary dynamic memory allocations and other undesired behavior that can result in degraded application performance.\n- Control over command generation: deko3d grants developers the ability to manage and record command lists that can be submitted to the GPU, as well as the ability to maintain multiple parallel queues of command processing, each with its own internal (GPU-managed) state.\n- Ease of use and convenience: despite being a low level API, care was taken to avoid introducing arbitrary restrictions in the API (such as giant immutable state objects or overengineered abstractions) that do not correspond to how the hardware works. deko3d aims to expose every useful thing that the GPU can do, in the most straightforward way possible. Between pedanticness and programmer happiness, the latter was selected.\n- Compile-time resource management: deko3d provides and promotes a workflow involving the compilation and conversion of assets such as shaders or graphics at compile time, therefore avoiding runtime resource waste.\n- Debug version of the library, providing parameter and state checking, as well as warning/error messages. The release version of the library omits validation checks and is compiled with extra optimization flags for maximum speed.\n- Ability to use a plain C version of the API, as well as a rich C++ wrapper inspired by Vulkan-Hpp.\n\nIn conclusion, deko3d is the homebrew answer to Nintendo and Nvidia's proprietary NVN graphics API. deko3d is the culmination of a year-and-a-half long (and still ongoing!) reverse engineering effort centered around NVN, which has in turn heavily influenced its design and implementation. The source code of deko3d is intended to also serve a double purpose as a public example of how to use an Nvidia GPU, as well as aiming to be an accurate-as-possible public repository of register definitions and other hardware information.\n\n## Installation\n\nUsers don't need to do anything special on their own to install deko3d, as this is a core homebrew library in the `switch-dev` group supplied through [devkitPro pacman](https://devkitpro.org/wiki/devkitPro_pacman). However, users who haven't received deko3d because they set up their toolchain installation prior to its release may want to run these commands:\n\n	(sudo) (dkp-)pacman -Syu\n	(sudo) (dkp-)pacman -S --noconfirm --needed switch-dev\n\nNonetheless for documentation's sake it is pointed out that building deko3d from source requires building and installing [dekotools](https://github.com/fincs/dekotools). No support nor precompiled binaries are provided for these tools though, since users are expected and encouraged to use the prebuilt binaries on devkitPro's pacman repository. Developers wishing to contribute to deko3d are kindly invited to talk to us at devkitPro first, through the usual hacking channels :)\n\n## Preemptively Answered Questions (PAQ)\n\n### Can I use the shader compiler inside my program?\n\nThe short answer is no: deko3d's shader compiler (UAM) is a PC tool that runs at compile time as opposed to at runtime, and this is a *main design goal* of deko3d.\n\nThe long answer is: in order to provide a lightweight API that doesn't make compromises, certain hindersome aspects of typical cross-platform graphics APIs such as forceful runtime compilation of shaders were left out. This doesn't mean that it is *impossible* to dynamically generate shaders; it only means that deko3d only provides the means to use native code. Users could target the native shader instruction set in their JIT engines; or even write a new shader compiler from scratch that can be made to run on the Switch. (If anyone is willing to make that happen, please talk to us as we would be *very* interested in it!)\n\n### Is this a clone of NVN?\n\ndeko3d is not a clone of NVN. However, deko3d was developed as a result of NVN reverse engineering work, and as such it borrows many implementation ideas. Essentially, deko3d was designed to fulfill the same place in homebrew that NVN does in official software.\n\n### Why make yet another API instead of reimplementing NVN?\n\nFor the same reason that libnx is not a carbon copy of the official SDK. deko3d is intended to be an independent homebrew thing, and it is designed specifically to cater to homebrew needs, workflows and standards.\n\n### Why isn't this a Vulkan driver?\n\nThe Vulkan API is designed in a way that adds artificial restrictions and imposes workflows that do not correspond to how Nvidia hardware works; and many Nvidia hardware features aren't exposed in an optimal way (if at all). deko3d on the other hand tries to be as honest as possible with the reality of Nvidia hardware. What's more, cross platform APIs are designed in a way that does not play nice in certain resource constrained scenarios (such as reimplementing parts of the Switch's operating system), where there is essentially no room for any kind of waste.\n\nAdditionally, Nvidia GPUs are poorly understood pieces of hardware due to the vendor's refusal to publish hardware documentation, specifically 3D class methods and the shader ISA. For this reason, reverse engineering NVN and implementing a homebrew counterpart (deko3d) is a precondition for doing anything that requires a better understanding of the hardware (and this also includes writing a Vulkan driver). This process has shed a considerable amount of light on this topic, and there's even the chance of a homebrew Vulkan driver (running on top of deko3d) being developed in the future.\n\n### Why and when should I use deko3d instead of switch-mesa's OpenGL driver?\n\ndeko3d is a lightweight low level API; several orders of magnitude more lightweight than switch-mesa. A typical application using switch-mesa will usually be several megabytes in size (around 8 MB or larger); while a typical application using deko3d will clock at around 500 KB or even smaller. The reason for this stems from the fact that mesa is designed in an environment where code size is not an issue, and shared libraries are used to load graphics drivers, which is the opposite situation that exists on Switch. Also, deko3d has native support for many Kepler/Maxwell performance-enhancing hardware features that nouveau currently doesn't have such as Zcull, the tiled cache, compressed render targets (decompressed prior to presentation), several optimizations in the shader compiler, and more.\n\nOn the other hand, switch-mesa provides a standard OpenGL API, while deko3d is a Switch homebrew specific API. Users would be advised to stick to switch-mesa (or libraries using it such as SDL) if a standard API is desired for cross-platform support. Alternatively users may want to implement a separate deko3d backend for their applications, however this also increases the amount of maintenance work.\n\n### Is deko3d faster than OpenGL?\n\nAs mentioned previously, deko3d has native support for several Kepler/Maxwell performance-enhancing features that nouveau currently hasn't been updated to use. These features were figured out by reverse engineering NVN, and the results of that work are present in deko3d. In addition, there is substantially lower CPU overhead compared to OpenGL, since deko3d is a low-level graphics API. The same reasons that would prompt users to switch to Vulkan for the purposes of reducing CPU/GPU overhead apply to deko3d as well.\n\n### Why does this project exist instead of contributing to nouveau, such as by improving the existing code or writing a Vulkan driver?\n\nThe way mesa and nouveau are designed and developed makes it difficult to add support for certain missing hardware features, and carries a risk of introducing unforeseen regressions in other parts of the system. Adding and testing support for newer stuff has quickly become an uphill battle too, especially due to the reclocking issue which renders nouveau virtually unusable on anything newer than Kepler. Additionally, Vulkan for nouveau is essentially a major undertaking that requires major architectural changes. As an example, nouveau's Linux kernel interface wasn't designed with low level APIs in mind, which would need a substantial redesign of the way nouveau talks to the underlying OS.\n\nThe NVN reverse engineering route has resulted in much cleaner, straightforward, and fruitful results - the homebrew community has received a more suitable and lightweight low level API, and without needing to depend on nouveau's uncertain future. In fact, at the time of writing, it is unfortunately not clear if nouveau will ever receive Vulkan support.\n\nWith that said, deko3d (like mesa/nouveau) is released under permissive licensing terms. All developers are free to study and use it as reference material, including nouveau developers; in fact they are very much welcome to do so. Specifically, there are certain improvements in the shader compiler (which *is* based off mesa/nouveau sources) which we would be more than happy to upstream; as well as assisting with anything else we might be asked about.\n\n## Future plans\n\nIn its current form, deko3d provides support for around 80% of most common hardware features. However, deko3d is still a work in progress, and the following items are still waiting to be implemented (in no particular order):\n\n- A texture conversion tool\n- Event synchronization primitives\n- Conditional render\n- Conservative raster\n- Zcull state save/restore\n- Calling command lists from other command lists\n- Add support for X/Y flip and the 2D engine to dkCmdBufCopyImage\n- Registering custom zero-bandwidth-clear color/depth values\n- Transform feedback\n- Multidraw\n- Variable group size in compute shaders\n- NV_draw_texture equivalent functionality\n- Passthrough geometry shaders\n- `gl_ViewportMask[]` (also known as NV_viewport_array2)\n- Reading sample positions in shaders (directly or indirectly)\n- Fragment shader interlock\n- Sparse textures\n- ""Color reduction"" (?)\n- ""Stencil cull criteria"" (?)\n- An unidentified special Zcull mode\n\n## Credits and acknowledgements\n\ndeko3d is presently developed and maintained by **fincs**.<br>\nIn no particular order, I would like to credit the following people:\n\n- [WinterMute](https://github.com/WinterMute) from [devkitPro](https://devkitpro.org/) for making the homebrew dream a reality through their incredible toolchains and ecosystem.\n- [plutoo](https://github.com/plutooo) from [switchbrew](https://github.com/switchbrew) for kickstarting GPU and NVN reverse engineering efforts.\n- [SciresM](https://github.com/SciresM) and [hexkyz](https://github.com/hexkyz) from the [Atmosphère](https://github.com/Atmosphere-NX) project for their invaluable contributions in the reverse engineering of Horizon OS.\n- [Rodrigo](https://github.com/ReinUsesLisp) and [Blinkhawk](https://github.com/FernandoS27) from the [yuzu](https://github.com/yuzu-emu) project for tirelessly listening to my rants and assisting me with feedback and GPU knowledge.\n- [Thog](https://github.com/Thog) and [gdkchan](https://github.com/gdkchan) from the [Ryujinx](https://github.com/Ryujinx) project for tirelessly listening to my rants and assisting me with feedback and GPU knowledge.\n\nIn addition, deko3d would have never been a thing without these organizations:\n\n- [NVIDIA](https://github.com/NVIDIA) for designing and creating the Tegra X1 GPU, as well as the NVN API.\n- [Nintendo](https://github.com/Nintendo) for designing and creating Horizon OS.\n- [nouveau](https://nouveau.freedesktop.org/wiki/) for creating and maintaining the only open source OpenGL driver for Nvidia hardware.\n",315,graphics,C++,3,Makefile,C,C++,,,,,,,,,,,,,,,,,,,,,,,,,,11,2,9,0,1,9,0,640,22,7,6,1,bcc3efb0191b4e5501663e4af3e7d081647ac076,dkCmdBufPushConstants: check for pushbuffer overflow,2024-04-30T19:02:21Z,averne,averne381@gmail.com,averne,v0.5.0,## What's Changed\r\n* Report counters and gpu timestamp by @averne in https://github.com/devkitPro/deko3d/pull/18\r\n* Implement dkCmdBufCopyImageToBuffer and dkCmdBufCopyImage by @RSDuck in https://github.com/devkitPro/deko3d/pull/11\r\n\r\n## New Contributors\r\n* @averne made their first contribution in https://github.com/devkitPro/deko3d/pull/18\r\n* @RSDuck made their first contribution in https://github.com/devkitPro/deko3d/pull/11,v0.5.0,Dave Murphy,,WinterMute,zlib License,deko3d,devkitPro,5,devkitpro,nx,nintendo-switch,nvidia,3d,graphics,deko3d,low-level,lightweight,homebrew,horizon-os,,,,,,,,,,/devkitPro/deko3d,5,16,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/devkitPro/citro2d,https://github.com/devkitPro/citro2d,0,,,0,0,0,0,0,0,1,1,0,0,0,Library for drawing 2D graphics using the Nintendo 3DS's PICA200 GPU,"# citro2d\n\n**( ͡° ͜ʖ ͡°)**\n\n*Library for drawing 2D graphics using the Nintendo 3DS's PICA200 GPU.*\n\nThis library contains optimized routines that allow 3DS homebrew developers to\ndevelop applications that take full advantage of the GPU to draw 2D graphics.\nThe routines in this library have been carefully designed and optimized for\nthe purpose of removing bottlenecks and allowing higher GPU throughput.\n\ncitro2d uses [citro3d](https://github.com/fincs/citro3d) under the hood to\ntalk to the GPU. It is possible to use citro2d on its own, or use it alongside\ncitro3d to draw mixed 2D and 3D content.\n\nFeatures:\n- Lightweight and straightforward API\n- Full doxygen documentation\n- Drawing on any surface (C3D_RenderTarget)\n- Drawing images and sprites (the latter contain state whereas the former don't)\n- Drawing text using the system font\n- Spritesheet/texture atlas support using [tex3ds](https://github.com/mtheall/tex3ds)\n- Scaling, flipping, rotation\n- Drawing untextured triangles and rectangles\n- Per-vertex tinting with configurable blend factor (additive color blending with user specified colors)\n- Flexible and configurable gradients\n- Full-screen fade-out/fade-in transitions (to any color)\n- Concurrent usage of citro2d and citro3d\n",153,graphics,C,2,C,CMake,,,,,,,,,,,,,,,,,,,,,,,,,,,21,1,17,3,2,14,27,263,25,26,17,9,4e415d7707fe31c2caa3bb5153da81870956b65c,Build with catnip,2024-05-25T21:26:15Z,fincs,fincs@devkitpro.org,fincs,citro2d v1.6.0,- Added C2D_SetTintMode: switchable tinting modes (solid tint/multiplicative tint/grayscale tint)\r\n- Added C2D_FontSetFilter\r\n- Minor behind-the-scenes performance improvements\r\n- Miscellaneous bugfixes\r\n- Further improvements to overall system stability and other minor adjustments have been made to enhance the user experience.,v1.6.0,,,fincs,zlib License,citro2d,devkitPro,6,gpu,pica200,nintendo,nintendo-3ds,libctru,citro3d,citro2d,devkitpro,homebrew,graphics,2d,sprite,sprites,,,,,,,,/devkitPro/citro2d,8,17,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/dellytools/delly,https://github.com/dellytools/delly,1,,1,1,1,1,1,0,0,0,0,0,0,1,DELLY2: Structural variant discovery by integrated paired-end and split-read analysis,"<p align=""center"">\n  <a href=""https://academic.oup.com/bioinformatics/article/28/18/i333/245403/DELLY-structural-variant-discovery-by-integrated"">\n    <img height=""150"" src=""https://raw.githubusercontent.com/dellytools/assets/main/delly-logo/delly-logo-539x600.png"">\n  </a>\n  <h1 align=""center"">Delly</h1>\n</p>\n\n[![install with bioconda](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?style=flat-square)](http://bioconda.github.io/recipes/delly/README.html)\n[![Anaconda-Server Badge](https://anaconda.org/bioconda/delly/badges/downloads.svg)](https://anaconda.org/bioconda/delly)\n[![C/C++ CI](https://github.com/dellytools/delly/workflows/C/C++%20CI/badge.svg)](https://github.com/dellytools/delly/actions)\n[![Docker CI](https://github.com/dellytools/delly/workflows/Docker%20CI/badge.svg)](https://hub.docker.com/r/dellytools/delly/)\n[![GitHub license](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](https://github.com/dellytools/delly/blob/main/LICENSE)\n[![GitHub Releases](https://img.shields.io/github/release/dellytools/delly.svg)](https://github.com/dellytools/delly/releases)\n\nDelly is an integrated structural variant (SV) prediction method that can discover, genotype and visualize deletions, tandem duplications, inversions and translocations at single-nucleotide resolution in short-read and long-read massively parallel sequencing data. It uses paired-ends, split-reads and read-depth to sensitively and accurately delineate genomic rearrangements throughout the genome.\n\n# Installing Delly\n\nDelly is available as a [statically linked binary](https://github.com/dellytools/delly/releases/), a [singularity container (SIF file)](https://github.com/dellytools/delly/releases/), a [docker container](https://hub.docker.com/r/dellytools/delly/) or via [Bioconda](https://anaconda.org/bioconda/delly). You can also build Delly from source using a recursive clone and make. \n\n`git clone --recursive https://github.com/dellytools/delly.git`\n\n`cd delly/`\n\n`make all`\n\nThere is a Delly discussion group [delly-users](http://groups.google.com/d/forum/delly-users) for usage and installation questions.\n\n\n# Delly multi-threading mode\n\nDelly supports parallel computing using the OpenMP API (www.openmp.org).\n\n`make PARALLEL=1 src/delly`\n\nYou can set the number of threads using the environment variable OMP_NUM_THREADS.\n\n`export OMP_NUM_THREADS=2`\n\nDelly primarily parallelizes on the sample level. Hence, OMP_NUM_THREADS should be always smaller or equal to the number of input samples. \n\n\n# Running Delly\n\nDelly needs a sorted, indexed and duplicate marked bam file for every input sample.\nAn indexed reference genome is required to identify split-reads.\nCommon workflows for germline and somatic SV calling are outlined below.\n\n`delly call -g hg38.fa input.bam > delly.vcf`\n\nYou can also specify an output file in [BCF](http://samtools.github.io/bcftools/) format.\n\n`delly call -o delly.bcf -g hg38.fa input.bam`\n\n`bcftools view delly.bcf > delly.vcf`\n\n\nExample\n-------\n\nA small example is included for short-read, long-read and copy-number variant calling.\n\n`delly call -g example/ref.fa -o sr.bcf example/sr.bam`\n\n`delly lr -g example/ref.fa -o lr.bcf example/lr.bam`\n\n`delly cnv -g example/ref.fa -m example/map.fa.gz -c out.cov.gz -o cnv.bcf example/sr.bam`\n\nMore in-depth tutorials for SV calling are available here:\n\n* Short-read SV calling: [https://github.com/tobiasrausch/vc](https://github.com/tobiasrausch/vc)\n\n* Long-read SV calling: [https://github.com/tobiasrausch/sv](https://github.com/tobiasrausch/sv)\n\n\nSomatic SV calling\n------------------\n\n* At least one tumor sample and a matched control sample are required for SV discovery\n\n`delly call -x hg38.excl -o t1.bcf -g hg38.fa tumor1.bam control1.bam`\n\n* Somatic pre-filtering requires a tab-delimited sample description file where the first column is the sample id (as in the VCF/BCF file) and the second column is either tumor or control.\n\n`delly filter -f somatic -o t1.pre.bcf -s samples.tsv t1.bcf`\n\n* Genotype pre-filtered somatic sites across a larger panel of control samples to efficiently filter false postives and germline SVs. For performance reasons, this can be run in parallel for each sample of the control panel and you may want to combine multiple pre-filtered somatic site lists from multiple tumor samples.\n\n`delly call -g hg38.fa -v t1.pre.bcf -o geno.bcf -x hg38.excl tumor1.bam control1.bam ... controlN.bam`\n\n* Post-filter for somatic SVs using all control samples.\n\n`delly filter -f somatic -o t1.somatic.bcf -s samples.tsv geno.bcf`\n\n\n\nGermline SV calling\n-------------------\n\n* SV calling is done by sample for high-coverage genomes or in small batches for low-coverage genomes\n\n`delly call -g hg38.fa -o s1.bcf -x hg38.excl sample1.bam`\n\n* Merge SV sites into a unified site list \n\n`delly merge -o sites.bcf s1.bcf s2.bcf ... sN.bcf`\n\n* Genotype this merged SV site list across all samples. This can be run in parallel for each sample.\n\n`delly call -g hg38.fa -v sites.bcf -o s1.geno.bcf -x hg38.excl s1.bam`\n\n`delly call -g hg38.fa -v sites.bcf -o sN.geno.bcf -x hg38.excl sN.bam`\n\n* Merge all genotyped samples to get a single VCF/BCF using bcftools merge\n\n`bcftools merge -m id -O b -o merged.bcf s1.geno.bcf s2.geno.bcf ... sN.geno.bcf`\n\n* Apply the germline SV filter which requires at least 20 unrelated samples\n\n`delly filter -f germline -o germline.bcf merged.bcf`\n\n\nDelly for long reads from PacBio or ONT\n---------------------------------------\n\nDelly also supports long-reads for SV discovery.\n\n`delly lr -y ont -o delly.bcf -g hg38.fa input.bam`\n\n`delly lr -y pb -o delly.bcf -g hg38.fa input.bam`\n\n\nRead-depth profiles and copy-number variant calling\n---------------------------------------------------\n\nYou can generate read-depth profiles with delly. This requires a mappability map which can be downloaded here:\n\n[Mappability Maps](https://gear-genomics.embl.de/data/delly/)\n\nThe command to count reads in 10kbp mappable windows and normalize the coverage is:\n\n`delly cnv -a -g hg38.fa -m hg38.map -c out.cov.gz -o out.bcf input.bam`\n\nThe output file `out.cov.gz` can be plotted using [R](https://www.r-project.org/) to generate normalized copy-number profiles and segment the read-depth information:\n\n`Rscript R/rd.R out.cov.gz`\n\nInstead of segmenting the read-depth information, you can also visualize the CNV calls.\n\n`bcftools query -f ""%CHROM\t%POS\t%INFO/END\t%ID[\t%RDCN]\n"" out.bcf > seg.bed`\n\n`Rscript R/rd.R out.cov.gz seg.bed`\n\nWith `-s` you can output a statistics file with GC bias information.\n\n`delly cnv -g hg38.fa -m hg38.map -c out.cov.gz -o out.bcf -s stats.gz input.bam`\n\n`zcat stats.gz | grep ""^GC"" > gc.bias.tsv`\n\n`Rscript R/gcbias.R gc.bias.tsv`\n\n\nGermline CNV calling\n--------------------\n\nDelly uses GC and mappability fragment correction to call CNVs. This requires a [mappability map](https://gear-genomics.embl.de/data/delly/).\n\n* Call CNVs for each sample and optionally refine breakpoints using delly SV calls\n\n`delly cnv -o c1.bcf -g hg38.fa -m hg38.map -l delly.sv.bcf input.bam`\n\n* Merge CNVs into a unified site list\n\n`delly merge -e -p -o sites.bcf -m 1000 -n 100000 c1.bcf c2.bcf ... cN.bcf`\n\n* Genotype CNVs for each sample\n\n`delly cnv -u -v sites.bcf -g hg38.fa -m hg38.map -o geno1.bcf input.bam`\n\n* Merge genotypes using [bcftools](https://github.com/samtools/bcftools)\n\n`bcftools merge -m id -O b -o merged.bcf geno1.bcf ... genoN.bcf`\n\n* Filter for germline CNVs\n\n`delly classify -f germline -o filtered.bcf merged.bcf`\n\n* Optional: Plot copy-number distribution for large number of samples (>>100)\n\n`bcftools query -f ""%ID[\t%RDCN]\n"" filtered.bcf > plot.tsv`\n\n`Rscript R/cnv.R plot.tsv`\n\n\nSomatic copy-number alterations (SCNAs)\n---------------------------------------\n\n* For somatic copy-number alterations, delly first segments the tumor genome (`-u` is required). Depending on the coverage, tumor purity and heterogeneity you can adapt parameters `-z`, `-t` and `-x` which control the sensitivity of SCNA detection.\n\n`delly cnv -u -z 10000 -o tumor.bcf -c tumor.cov.gz -g hg38.fa -m hg38.map tumor.bam`\n\n* Then these tumor SCNAs are genotyped in the control sample (`-u` is required).\n\n`delly cnv -u -v tumor.bcf -o control.bcf -g hg38.fa -m hg38.map control.bam`\n\n* The VCF IDs are matched between tumor and control. Thus, you can merge both files using [bcftools](https://github.com/samtools/bcftools).\n\n`bcftools merge -m id -O b -o tumor_control.bcf tumor.bcf control.bcf`\n\n* Somatic filtering requires a tab-delimited sample description file where the first column is the sample id (as in the VCF/BCF file) and the second column is either tumor or control.\n\n`delly classify -p -f somatic -o somatic.bcf -s samples.tsv tumor_control.bcf`\n\n* Optional: Plot the SCNAs using bcftools and R.\n\n`bcftools query -s tumor -f ""%CHROM\t%POS\t%INFO/END\t%ID[\t%RDCN]\n"" somatic.bcf > segmentation.bed`\n\n`Rscript R/rd.R tumor.cov.gz segmentation.bed`\n\n\nFAQ\n---\n* Visualization of SVs      \nYou may want to try out [wally](https://github.com/tobiasrausch/wally) to plot candidate structural variants. The paired-end coloring is explained in [wally's README](https://github.com/tobiasrausch/wally#paired-end-view) file.\n\n* What is the smallest SV size Delly can call?  \nFor short-reads, this depends on the sharpness of the insert size distribution. For an insert size of 200-300bp with a 20-30bp standard deviation, Delly starts to call reliable SVs >=300bp. Delly also supports calling of small InDels using soft-clipped reads only, the smallest SV size called is 15bp. For long-reads, delly calls SVs >=30bp.\n\n* Can Delly be used on a non-diploid genome?  \nYes and no. The SV site discovery works for any ploidy. However, Delly's genotyping model assumes diploidy (hom. reference, het. and hom. alternative). The CNV calling allows to set the baseline ploidy on the command-line.\n\n* Delly is running too slowly what can I do?    \nYou should exclude telomere and centromere regions and also all unplaced contigs (`-x` command-line option). In addition, you can filter input reads more stringently using -q 20 and -s 15. Lastly, `-z` can be set to 5 for high-coverage data.\n\n* Are non-unique alignments, multi-mappings and/or multiple split-read alignments allowed?  \nDelly expects two alignment records in the bam file for every paired-end, one for the first and one for the second read. Multiple split-read alignment records of a given read are allowed if and only if one of them is a primary alignment whereas all others are marked as secondary or supplementary. This is the default for bwa, minimap2 and many other aligners.\n\n* What pre-processing of bam files is required?    \nBam files need to be sorted, indexed and ideally duplicate marked.\n\n* Usage/discussion mailing list?         \nThere is a delly discussion group [delly-users](http://groups.google.com/d/forum/delly-users).\n\n* Docker/Singularity support?            \nThere is a delly [docker container](https://hub.docker.com/r/dellytools/delly/) and [singularity container (*.sif file)](https://github.com/dellytools/delly/releases) available.\n\n* How can I compute a mappability map?               \nA basic mappability map can be built using [dicey](https://github.com/gear-genomics/dicey), [samtools](https://github.com/samtools/samtools) and [bwa](https://github.com/lh3/bwa) with the below commands (as an example for the sacCer3 reference):\n```\ndicey chop sacCer3.fa\nbwa index sacCer3.fa\nbwa mem sacCer3.fa read1.fq.gz read2.fq.gz | samtools sort -@ 8 -o srt.bam -\nsamtools index srt.bam \ndicey mappability2 srt.bam \ngunzip map.fa.gz && bgzip map.fa && samtools faidx map.fa.gz \n```\n\n* Bioconda support?              \nDelly is available via [bioconda](http://bioconda.github.io/recipes/delly/README.html).\n\n\nCitation\n--------\n\nTobias Rausch, Thomas Zichner, Andreas Schlattl, Adrian M. Stuetz, Vladimir Benes, Jan O. Korbel.      \nDELLY: structural variant discovery by integrated paired-end and split-read analysis.     \nBioinformatics. 2012 Sep 15;28(18):i333-i339.       \n[https://doi.org/10.1093/bioinformatics/bts378](https://doi.org/10.1093/bioinformatics/bts378)\n\nLicense\n-------\nDelly is distributed under the BSD 3-Clause license. Consult the accompanying [LICENSE](https://github.com/dellytools/delly/blob/main/LICENSE) file for more details.\n",416,cancer-genomics,C++,6,Makefile,C++,Dockerfile,R,C,Python,,,,,,,,,,,,,,,,,,,,,,,16,5,11,0,6,8,0,39239,136,355,305,50,c187a90a260bdb6d037b5b8bcf9a534f8e91712a,fix macosx,2024-07-18T13:25:39Z,Tobias Rausch,rauschtobi@gmail.com,tobiasrausch,delly v1.2.6,,v1.2.6,Tobias Rausch,,tobiasrausch,"BSD 3-Clause ""New"" or ""Revised"" License",delly,dellytools,42,structural-variation,sv-discovery,delly-users,delly,rearrangement,genomic,cancer-genomics,germline,tumor,svs,,,,,,,,,,,/dellytools/delly,42,33,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/deeptools/deepTools,https://github.com/deeptools/deepTools,0,,,0,1,0,0,0,0,1,0,0,0,0,Tools to process and analyze deep sequencing data.,"# deepTools\n[![Documentation Status](https://readthedocs.org/projects/deeptools/badge/)](http://deeptools.readthedocs.org/) \n[![PyPI Version](https://img.shields.io/pypi/v/deeptools.svg?style=plastic)](https://pypi.org/project/deepTools/) \n[![install with bioconda](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?style=flat)](http://bioconda.github.io/recipes/deeptools/README.html)\n[![European Galaxy server](https://img.shields.io/badge/usegalaxy-.eu-brightgreen?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABgAAAASCAYAAABB7B6eAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAACXBIWXMAAAsTAAALEwEAmpwYAAACC2lUWHRYTUw6Y29tLmFkb2JlLnhtcAAAAAAAPHg6eG1wbWV0YSB4bWxuczp4PSJhZG9iZTpuczptZXRhLyIgeDp4bXB0az0iWE1QIENvcmUgNS40LjAiPgogICA8cmRmOlJERiB4bWxuczpyZGY9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiPgogICAgICA8cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0iIgogICAgICAgICAgICB4bWxuczp0aWZmPSJodHRwOi8vbnMuYWRvYmUuY29tL3RpZmYvMS4wLyI+CiAgICAgICAgIDx0aWZmOlJlc29sdXRpb25Vbml0PjI8L3RpZmY6UmVzb2x1dGlvblVuaXQ+CiAgICAgICAgIDx0aWZmOkNvbXByZXNzaW9uPjE8L3RpZmY6Q29tcHJlc3Npb24+CiAgICAgICAgIDx0aWZmOk9yaWVudGF0aW9uPjE8L3RpZmY6T3JpZW50YXRpb24+CiAgICAgICAgIDx0aWZmOlBob3RvbWV0cmljSW50ZXJwcmV0YXRpb24+MjwvdGlmZjpQaG90b21ldHJpY0ludGVycHJldGF0aW9uPgogICAgICA8L3JkZjpEZXNjcmlwdGlvbj4KICAgPC9yZGY6UkRGPgo8L3g6eG1wbWV0YT4KD0UqkwAAAn9JREFUOBGlVEuLE0EQruqZiftwDz4QYT1IYM8eFkHFw/4HYX+GB3/B4l/YP+CP8OBNTwpCwFMQXAQPKtnsg5nJZpKdni6/6kzHvAYDFtRUT71f3UwAEbkLch9ogQxcBwRKMfAnM1/CBwgrbxkgPAYqlBOy1jfovlaPsEiWPROZmqmZKKzOYCJb/AbdYLso9/9B6GppBRqCrjSYYaquZq20EUKAzVpjo1FzWRDVrNay6C/HDxT92wXrAVCH3ASqq5VqEtv1WZ13Mdwf8LFyyKECNbgHHAObWhScf4Wnj9CbQpPzWYU3UFoX3qkhlG8AY2BTQt5/EA7qaEPQsgGLWied0A8VKrHAsCC1eJ6EFoUd1v6GoPOaRAtDPViUr/wPzkIFV9AaAZGtYB568VyJfijV+ZBzlVZJ3W7XHB2RESGe4opXIGzRTdjcAupOK09RA6kzr1NTrTj7V1ugM4VgPGWEw+e39CxO6JUw5XhhKihmaDacU2GiR0Ohcc4cZ+Kq3AjlEnEeRSazLs6/9b/kh4eTC+hngE3QQD7Yyclxsrf3cpxsPXn+cFdenF9aqlBXMXaDiEyfyfawBz2RqC/O9WF1ysacOpytlUSoqNrtfbS642+4D4CS9V3xb4u8P/ACI4O810efRu6KsC0QnjHJGaq4IOGUjWTo/YDZDB3xSIxcGyNlWcTucb4T3in/3IaueNrZyX0lGOrWndstOr+w21UlVFokILjJLFhPukbVY8OmwNQ3nZgNJNmKDccusSb4UIe+gtkI+9/bSLJDjqn763f5CQ5TLApmICkqwR0QnUPKZFIUnoozWcQuRbC0Km02knj0tPYx63furGs3x/iPnz83zJDVNtdP3QAAAABJRU5ErkJggg==)](https://usegalaxy.eu/root?tool_id=deeptools_compute_matrix)\n![test](https://github.com/deeptools/deepTools/actions/workflows/test.yml/badge.svg)\n\n\n## User-friendly tools for exploring deep-sequencing data\n\ndeepTools addresses the challenge of handling the large amounts of data that are now routinely generated from DNA sequencing centers. deepTools contains useful modules to process the mapped reads data for multiple quality checks, creating **normalized coverage files** in standard bedGraph and bigWig file formats, that allow comparison between different files (for example, treatment and control). Finally, using such normalized and standardized files, deepTools can create many publication-ready  **visualizations** to identify enrichments and for functional annotations of the genome.\n\nFor support or questions please post to [Biostars](http://biostars.org). For bug reports and feature requests please open an issue [on github](http://github.com/deeptools/deeptools).\n\n\n### Citation:\n\nRamírez F, Ryan DP, Grüning B, Bhardwaj V, Kilpert F, Richter AS, Heyne S, Dündar F, Manke T. [deepTools2: a next generation web server for deep-sequencing data analysis.](https://nar.oxfordjournals.org/content/early/2016/04/12/nar.gkw257.abstract) Nucleic Acids Research. 2016 Apr 13:gkw257.\n\n### Documentation:\n\nOur [documentation](http://deeptools.readthedocs.org/) contains more details on the [individual tool scopes and usages](http://deeptools.readthedocs.org/en/latest/content/list_of_tools.html) and an [introduction to our deepTools Galaxy web server](http://deeptools.readthedocs.org/en/latest/content/help_galaxy_intro.html) including [step-by-step protocols](http://deeptools.readthedocs.org/en/latest/content/example_usage.html).\n\n>Please see also the [FAQ](http://deeptools.readthedocs.org/en/latest/content/help_faq.html), which we update regularly.\nOur [Gallery](http://deeptools.readthedocs.org/en/latest/content/example_gallery.html) may give you some more ideas about the scope of deepTools.\n\n>For more specific **troubleshooting, feedback, and tool suggestions**, please post [to Biostars](http://biostars.org).\n\n\n-------------------------------------------------------------------------------------------------------------------\n\n### Installation\n\ndeepTools are available for:\n\n* Command line usage (via pip / conda / github)\n* Integration into Galaxy servers (via toolshed/API/web-browser)\n\nThere are many easy ways to install deepTools. More details can be found [here](https://deeptools.readthedocs.io/en/latest/content/installation.html).\n\nIn Brief:\n\n**Install through pypi**\n\n	$ pip install deeptools\n\n**Install via conda**\n\n	$ conda install -c bioconda deeptools\n\n**Install by cloning the repository**\n\n	$ git clone https://github.com/deeptools/deepTools\n	$ cd deepTools\n	$ pip install .\n\n<a name=""galaxy""/></a>\n### Galaxy Installation\n\ndeepTools can be easily integrated into [Galaxy](http://galaxyproject.org). Please see the [installation instructions in our documentation](http://deeptools.readthedocs.io/en/latest/content/installation.html#galaxy-installation) for further details.\n\n**Note:** From version 2.3 onwards, deepTools support **python3**.\n\n------------------------------------\n\nThis tool suite is developed by the [Bioinformatics Facility](http://www1.ie-freiburg.mpg.de/bioinformaticsfac) at the [Max Planck Institute for Immunobiology and Epigenetics, Freiburg](http://www1.ie-freiburg.mpg.de/).\n\n[Documentation](http://deeptools.readthedocs.org/en/latest/index.html) | [deepTools Galaxy](http://deeptools.ie-freiburg.mpg.de) | [FAQ](http://deeptools.readthedocs.org/en/latest/content/help_faq.html)\n",662,bioinformatics,Python,2,Python,Shell,,,,,,,,,,,,,,,,,,,,,,,,,,,388,42,340,6,4,41,0,123476,205,930,802,128,c24522b9ed6fa08cdac01d56aa9e9e9bb8953050,Merge pull request #1295 from deeptools/develop,2024-03-08T15:41:55Z,Ward D,ward@deboutte.be,WardDeb,03.05.2005,"## What's Changed\r\n* drop support for python 3.7\r\n* doc fixes (argparse properly displayed, minor changes in installation instructions)\r\n* deepblue support stops\r\n* initiate deprecation of tight_layout in plotheatmap, in favor of constrained_layout. Minor changes in paddings, etc can occur (but for the better).\r\n* documentation changes to improve ESS tab, table constraints have been lifted & sphinx_rtd_theme to v2.0.0\r\n* upload artifact in gh test runner pinned to 3\r\n* Try to get the number of processors from sched_getaffinity, to avoid using to many in job submissions for example. #1199\r\n* Fix typo in estimateScaleFactor that fixes broken argparsing. #1286\r\n\r\n\r\n**Full Changelog**: https://github.com/deeptools/deepTools/compare/3.5.4...3.5.5",03.05.2005,Ward D,,WardDeb,Other,deepTools,deeptools,58,python,bioinformatics,genomics,ngs,rna-seq,chip-seq,,,,,,,,,,,,,,,/deeptools/deepTools,62,37,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/deephyper/deephyper,https://github.com/deephyper/deephyper,0,,,0,0,0,0,0,0,1,0,0,0,0,DeepHyper: Scalable Asynchronous Neural Architecture and Hyperparameter Search for Deep Neural Networks,"<p align=""center"">\n<img src=""docs/_static/logo/medium.png"">\n</p>\n\n[![DOI](https://zenodo.org/badge/156403341.svg)](https://zenodo.org/badge/latestdoi/156403341)\n![GitHub tag (latest by date)](https://img.shields.io/github/tag-date/deephyper/deephyper.svg?label=version)\n[![Documentation Status](https://readthedocs.org/projects/deephyper/badge/?version=latest)](https://deephyper.readthedocs.io/en/latest/?badge=latest)\n![PyPI - License](https://img.shields.io/pypi/l/deephyper.svg)\n![PyPI - Downloads](https://img.shields.io/pypi/dm/deephyper.svg?label=Pypi%20downloads)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deephyper/tutorials/blob/main/tutorials/colab/DeepHyper_101.ipynb)\n<!-- [![Build Status](https://travis-ci.com/deephyper/deephyper.svg?branch=develop)](https://travis-ci.com/deephyper/deephyper) -->\n\n## What is DeepHyper?\n\nDeepHyper is a powerful Python package for automating machine learning tasks, particularly focused on optimizing hyperparameters, searching for optimal neural architectures, and quantifying uncertainty through the use of deep ensembles. With DeepHyper, users can easily perform these tasks on a single machine or distributed across multiple machines, making it ideal for use in a variety of environments. Whether you're a beginner looking to optimize your machine learning models or an experienced data scientist looking to streamline your workflow, DeepHyper has something to offer. So why wait? Start using DeepHyper today and take your machine-learning skills to the next level!\n\n## Install Instructions\n\nInstallation with `pip`:\n\n```console\n# For the most basic set of features (hyperparameter search)\npip install deephyper\n\n# For the default set of features including:\n# - hyperparameter search with transfer-learning\n# - neural architecture search\n# - deep ensembles\n# - Ray-based distributed computing\n# - Learning-curve extrapolation for multi-fidelity hyperparameter search\npip install ""deephyper[default]""\n```\n\nMore details about the installation process can be found at [DeepHyper Installations](https://deephyper.readthedocs.io/en/latest/install/index.html).\n\n## Quickstart\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deephyper/tutorials/blob/main/tutorials/colab/DeepHyper_101.ipynb)\n\nThe black-box function named `run` is defined by taking an input job named `job` which contains the different variables to optimize `job.parameters`. Then the run-function is bound to an `Evaluator` in charge of distributing the computation of multiple evaluations. Finally, a Bayesian search named `CBO` is created and executed to find the values of config which **MAXIMIZE** the return value of `run(job)`.\n\n```python\ndef run(job):\n    # The suggested parameters are accessible in job.parameters (dict)\n    x = job.parameters[""x""]\n    b = job.parameters[""b""]\n\n    if job.parameters[""function""] == ""linear"":\n        y = x + b\n    elif job.parameters[""function""] == ""cubic"":\n        y = x**3 + b\n\n    # Maximization!\n    return y\n\n\n# Necessary IF statement otherwise it will enter in a infinite loop\n# when loading the 'run' function from a new process\nif __name__ == ""__main__"":\n    from deephyper.problem import HpProblem\n    from deephyper.search.hps import CBO\n    from deephyper.evaluator import Evaluator\n\n    # define the variable you want to optimize\n    problem = HpProblem()\n    problem.add_hyperparameter((-10.0, 10.0), ""x"") # real parameter\n    problem.add_hyperparameter((0, 10), ""b"") # discrete parameter\n    problem.add_hyperparameter([""linear"", ""cubic""], ""function"") # categorical parameter\n\n    # define the evaluator to distribute the computation\n    evaluator = Evaluator.create(\n        run,\n        method=""process"",\n        method_kwargs={\n            ""num_workers"": 2,\n        },\n    )\n\n    # define your search and execute it\n    search = CBO(problem, evaluator, random_state=42)\n\n    results = search.search(max_evals=100)\n    print(results)\n```\n\nWhich outputs the following results where the best parameters are with `function == ""cubic""`, \n`x == 9.99` and `b == 10`.\n\n```verbatim\n    p:b p:function       p:x    objective  job_id  m:timestamp_submit  m:timestamp_gather\n0     7     linear  8.831019    15.831019       1            0.064874            1.430992\n1     4     linear  9.788889    13.788889       0            0.064862            1.453012\n2     0      cubic  2.144989     9.869049       2            1.452692            1.468436\n3     9     linear -9.236860    -0.236860       3            1.468123            1.483654\n4     2      cubic -9.783865  -934.550818       4            1.483340            1.588162\n..  ...        ...       ...          ...     ...                 ...                 ...\n95    6      cubic  9.862098   965.197192      95           13.538506           13.671872\n96   10      cubic  9.997512  1009.253866      96           13.671596           13.884530\n97    6      cubic  9.965615   995.719961      97           13.884188           14.020144\n98    5      cubic  9.998324  1004.497422      98           14.019737           14.154467\n99    9      cubic  9.995800  1007.740379      99           14.154169           14.289366\n```\n\nThe code defines a function `run` that takes a RunningJob `job` as input and returns the maximized objective `y`. The `if` block at the end of the code defines a black-box optimization process using the `CBO` (Centralized Bayesian Optimization) algorithm from the `deephyper` library.\n\nThe optimization process is defined as follows:\n\n1. A hyperparameter optimization problem is created using the `HpProblem` class from `deephyper`. In this case, the problem has three variables. The `x` hyperparameter is a real variable in a range from -10.0 to 10.0. The `b` hyperparameter is a discrete variable in a range from 0 to 10. The `function` hyperparameter is a categorical variable with two possible values.\n2. An evaluator is created using the `Evaluator.create` method. The evaluator will be used to evaluate the function `run` with different configurations of suggested hyperparameters in the optimization problem. The evaluator uses the `process` method to distribute the evaluations across multiple worker processes, in this case, 2 worker processes.\n3. A search object is created using the `CBO` class, the problem and evaluator defined earlier. The `CBO` algorithm is a derivative-free optimization method that uses a Bayesian optimization approach to explore the hyperparameter space.\n4. The optimization process is executed by calling the `search.search` method, which performs the evaluations of the `run` function with different configurations of the hyperparameters until a maximum number of evaluations (100 in this case) is reached.\n5. The results of the optimization process, including the optimal configuration of the hyperparameters and the corresponding objective value, are printed to the console.\n\n## How do I learn more?\n\n* Documentation: <https://deephyper.readthedocs.io>\n\n* GitHub repository: <https://github.com/deephyper/deephyper>\n\n* Blog: <https://deephyper.github.io>\n\n## Contributions\n\nFind the list of contributors on the [DeepHyper Authors](https://deephyper.github.io/aboutus) page of the Documentation.\n\n## Citing DeepHyper\n\nIf you wish to cite the Software, please use the following:\n\n```\n@misc{deephyper_software,\n    title = {""DeepHyper: A Python Package for Scalable Neural Architecture and Hyperparameter Search""},\n    author = {Balaprakash, Prasanna and Egele, Romain and Salim, Misha and Maulik, Romit and Vishwanath, Venkat and Wild, Stefan and others},\n    organization = {DeepHyper Team},\n    year = 2018,\n    url = {https://github.com/deephyper/deephyper}\n} \n```\n\nFind all our publications on the [Research & Publication](https://deephyper.github.io/papers) page of the Documentation.\n\n## How can I participate?\n\nQuestions, comments, feature requests, bug reports, etc. can be directed to:\n\n* Issues on GitHub\n\nPatches through pull requests are much appreciated on the software itself as well as documentation.\nOptionally, please include in your first patch a credit for yourself in the list above.\n\nThe DeepHyper Team uses git-flow to organize the development: [Git-Flow cheatsheet](https://danielkummer.github.io/git-flow-cheatsheet/). For tests we are using: [Pytest](https://docs.pytest.org/en/latest/).\n\n## Acknowledgments\n\n* Scalable Data-Efficient Learning for Scientific Domains, U.S. Department of Energy 2018 Early Career Award funded by the Advanced Scientific Computing Research program within the DOE Office of Science (2018--Present)\n* Argonne Leadership Computing Facility: This research used resources of the Argonne Leadership Computing Facility, which is a DOE Office of Science User Facility supported under Contract DE-AC02-06CH11357.\n* SLIK-D: Scalable Machine Learning Infrastructures for Knowledge Discovery, Argonne Computing, Environment and Life Sciences (CELS) Laboratory Directed Research and Development (LDRD) Program (2016--2018)\n\n## Copyright and license\n\nCopyright © 2019, UChicago Argonne, LLC\n\nDeepHyper is distributed under the terms of BSD License. See [LICENSE](https://github.com/deephyper/deephyper/blob/master/LICENSE.md)\n\nArgonne Patent & Intellectual Property File Number: SF-19-007\n",267,uncertainty-quantification,Python,3,Python,Shell,Dockerfile,,,,,,,,,,,,,,,,,,,,,,,,,,72,9,62,1,5,26,0,44703,60,146,125,21,739bf6629a1a0681b16996dc8bcf381a6ef4a224,Merge branch 'release/0.7.0',2024-04-04T07:05:19Z,Deathn0t,romainegele@gmail.com,Deathn0t,Changelog - DeepHyper 0.7.0,"## deephyper.search\r\n\r\n- If a `results.csv` file already exists in the `log_dir` folder, it is renamed instead of overwritten.\r\n- Parallel and asynchronous standard experimental design can now be used through DeepHyper to perform Random, Grid, or Quasi-Monte-Carlo evaluations: [Example on Standard Experimental Design (Grid Search)](https://deephyper.readthedocs.io/en/latest/examples/plot_experimental_design.html).\r\n\r\n### Bayesian optimization (CBO and MPIDistributedBO)\r\n\r\n- New optimizers of the acquisition function: the acquisition function for non-derivable surrogate models (e.g., `""RF"", ""ET""`) can now be optimized with `acq_optimizer=""ga""` or `acq_optimizer=""mixedga""`. This makes BO iterations more efficient but implies an additional overhead (negligible if the evaluated function is slow). The `acq_optimizer_freq=2` parameter can be used to amortize this overhead.\r\n- New exploration/exploitation scheduler: the periodic exponential decay scheduler can now be specified with its initial and final values `CBO(..., kappa=10, scheduler={""type"": ""periodic-exp-decay"", ""period"": 25, ""kappa_final"": 1.96})`. This mechanism allows to escape local optimum.\r\n- New family of acquisition functions for Random-Forests surrogate models. `acq_func=""UCBd""` or `""EId""` or `""PId""`. The `""d""` postfix stands for ""deterministic"" because in this case, the acquisition function will only use epistemic uncertainty from the black-box function to evaluate the acquisition function and it will ignore aleatoric uncertainty (i.e., noise estimation).\r\n- The default surrogate model was renamed `""ET""` which stands for ""Extremely Randomized Trees"" to better match the machine learning literature. This surrogate model provides better epistemic uncertainty estimates than the standard `""RF""` which stands for ""Random Forest"". It is also a type of randomized ensemble of trees but it uses a randomized split decision rule instead of an optimized split decision rule.\r\n- `HpProblem` based on `ConfigSpace` objects using constraints now uses the lower bound of each hyperparameter as a slack value.\r\n\r\n## deephyper.stopper\r\n\r\n- The stopper based on learning curve extrapolation has an improved fit and speed.",0.7.0,Romain Egele,,Deathn0t,"BSD 3-Clause ""New"" or ""Revised"" License",deephyper,deephyper,15,automl,scalability,neural-architecture-search,hyperparameter-optimization,python,tensorflow,keras,deep-learning,hpc,machine-learning,multi-fidelity,pytorch,uncertainty-quantification,,,,,,,,/deephyper/deephyper,29,18,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/DedalusProject/dedalus,https://github.com/DedalusProject/dedalus,1,,,1,1,1,0,0,0,0,0,0,0,0,A flexible framework for solving PDEs with modern spectral methods.,"<!-- Title -->\r\n<h1 align=""center"">\r\n  Dedalus Project\r\n</h1>\r\n\r\n<!-- Information badges -->\r\n<p align=""center"">\r\n  <a href=""https://www.repostatus.org/#active"">\r\n    <img alt=""Repo status"" src=""https://www.repostatus.org/badges/latest/active.svg"" />\r\n  </a>\r\n  <a href=""http://dedalus-project.readthedocs.org"">\r\n    <img alt=""Read the Docs"" src=""https://img.shields.io/readthedocs/dedalus-project"">\r\n  </a>\r\n    <img alt=""PyPI - Python Version"" src=""https://img.shields.io/pypi/pyversions/dedalus"">\r\n  <a href=""https://pypi.org/project/dedalus/"">\r\n    <img alt=""PyPI"" src=""https://img.shields.io/pypi/v/dedalus"">\r\n  </a>\r\n  <a href=""https://github.com/conda-forge/dedalus-feedstock"">\r\n  <img alt=""Conda Version"" src=""https://img.shields.io/conda/vn/conda-forge/dedalus"">\r\n  </a>\r\n  <a href=""https://www.gnu.org/licenses/gpl-3.0.en.html"">\r\n    <img alt=""PyPI - License"" src=""https://img.shields.io/pypi/l/dedalus"">\r\n  </a>\r\n</p>\r\n\r\nDedalus is a flexible framework for solving partial differential equations using modern spectral methods.\r\nThe code is open-source and developed by a team of researchers studying astrophysical, geophysical, and biological fluid dynamics.\r\n\r\nDedalus is written primarily in Python and features an easy-to-use interface with symbolic vectorial equation specification.\r\nFor example, to simulate incompressible hydrodynamics in a ball, you can symbolically enter the equations, including [gauge conditions](https://dedalus-project.readthedocs.io/en/latest/pages/gauge_conditions.html) and [boundary conditions enforced with the tau method](https://dedalus-project.readthedocs.io/en/latest/pages/tau_method.html), as:\r\n\r\n```python\r\nproblem.add_equation(""div(u) + tau_p = 0"")\r\nproblem.add_equation(""dt(u) - nu*lap(u) + grad(p) + lift(tau_u) = - u@grad(u)"")\r\nproblem.add_equation(""u(r=1) = 0"")\r\nproblem.add_equation(""integ(p) = 0"")\r\n```\r\n\r\nOur numerical algorithms produce sparse and spectrally accurate discretizations of PDEs on simple domains, including Cartesian domains of any dimension, disks, annuli, spheres, spherical shells, and balls:\r\n\r\n<table style=""background-color:#FFFFFF;"">\r\n  <tr>\r\n    <th width=""25%"">\r\n      <a href=""https://dedalus-project.readthedocs.io/en/latest/pages/examples/ivp_1d_kdv_burgers.html"">\r\n        <figure>\r\n          <img src=""https://raw.githubusercontent.com/DedalusProject/dedalus/master/docs/pages/examples/images/kdv_burgers.png"">\r\n          <figcaption>KdV-Burgers equation (1D IVP)</figcaption>\r\n        </figure>\r\n      </a>\r\n    </th>\r\n    <th width=""25%"">\r\n      <a href=""https://dedalus-project.readthedocs.io/en/latest/pages/examples/ivp_2d_rayleigh_benard.html"">\r\n        <figure>\r\n          <img src=""https://raw.githubusercontent.com/DedalusProject/dedalus/master/docs/pages/examples/images/rayleigh_benard.png"">\r\n          <figcaption>Rayleigh-Benard convection (2D IVP)</figcaption>\r\n        </figure>\r\n      </a>\r\n    </th>\r\n    <th width=""25%"">\r\n      <a href=""https://dedalus-project.readthedocs.io/en/latest/pages/examples/ivp_2d_shear_flow.html"">\r\n        <figure>\r\n            <img src=""https://raw.githubusercontent.com/DedalusProject/dedalus/master/docs/pages/examples/images/shear_flow.png"">\r\n            <figcaption>Periodic shear flow (2D IVP)</figcaption>\r\n        </figure>\r\n      </a>\r\n    </th>\r\n    <th width=""25%"">\r\n      <a href=""https://dedalus-project.readthedocs.io/en/latest/pages/examples/lbvp_2d_poisson.html"">\r\n        <figure>\r\n            <img src=""https://raw.githubusercontent.com/DedalusProject/dedalus/master/docs/pages/examples/images/poisson.png"">\r\n            <figcaption>Poisson equation (2D LBVP)</figcaption>\r\n        </figure>\r\n      </a>\r\n    </th>\r\n  </tr>\r\n  <tr>\r\n  </tr>\r\n  <tr>\r\n    <th width=""25%"">\r\n      <a href=""https://dedalus-project.readthedocs.io/en/latest/pages/examples/ivp_disk_libration.html"">\r\n        <figure>\r\n          <img src=""https://raw.githubusercontent.com/DedalusProject/dedalus/master/docs/pages/examples/images/libration.png"">\r\n          <figcaption>Librational instability (disk IVP)</figcaption>\r\n        </figure>\r\n      </a>\r\n    </th>\r\n    <th width=""25%"">\r\n      <a href=""https://dedalus-project.readthedocs.io/en/latest/pages/examples/ivp_sphere_shallow_water.html"">\r\n        <figure>\r\n          <img src=""https://raw.githubusercontent.com/DedalusProject/dedalus/master/docs/pages/examples/images/shallow_water.png"">\r\n          <figcaption>Spherical shallow water (sphere IVP)</figcaption>\r\n        </figure>\r\n      </a>\r\n    </th>\r\n    <th width=""25%"">\r\n      <a href=""https://dedalus-project.readthedocs.io/en/latest/pages/examples/ivp_shell_convection.html"">\r\n        <figure>\r\n            <img src=""https://raw.githubusercontent.com/DedalusProject/dedalus/master/docs/pages/examples/images/shell_convection.png"">\r\n            <figcaption>Spherical shell convection (shell IVP)</figcaption>\r\n        </figure>\r\n      </a>\r\n    </th>\r\n    <th width=""25%"">\r\n      <a href=""https://dedalus-project.readthedocs.io/en/latest/pages/examples/ivp_ball_internally_heated_convection.html"">\r\n        <figure>\r\n            <img src=""https://raw.githubusercontent.com/DedalusProject/dedalus/master/docs/pages/examples/images/internally_heated_convection.png"">\r\n            <figcaption>Internally heated convection (ball IVP)</figcaption>\r\n        </figure>\r\n      </a>\r\n    </th>\r\n  </tr>\r\n</table>\r\n\r\nThe resulting systems are efficiently solved using compiled libraries and are automatically parallelized using MPI.\r\nSee the [documentation](http://dedalus-project.readthedocs.org) for tutorials and additional examples.\r\n\r\n## Links\r\n\r\n* Project homepage: <http://dedalus-project.org>\r\n* Code repository: <https://github.com/DedalusProject/dedalus>\r\n* Documentation: <http://dedalus-project.readthedocs.org>\r\n* Mailing list: <https://groups.google.com/forum/#!forum/dedalus-users>\r\n\r\n## Developers\r\n\r\n* [Keaton Burns (@kburns)](https://github.com/kburns)\r\n* [Geoff Vasil (@geoffvasil)](https://github.com/geoffvasil)\r\n* [Jeff Oishi (@jsoishi)](https://github.com/jsoishi)\r\n* [Daniel Lecoanet (@lecoanet)](https://github.com/lecoanet/)\r\n* [Ben Brown (@bpbrown)](https://github.com/bpbrown)\r\n",471,spectral-methods,Python,2,Python,Cython,,,,,,,,,,,,,,,,,,,,,,,,,,,99,12,81,6,19,23,0,62401,109,200,159,41,47355012c85917a5ffd293482b6b1481b655823a,Modified polar component operators so they work on tensors.,2024-07-18T19:25:55Z,Daniel Lecoanet,dlecoanet@gmail.com,lecoanet,Dedalus v3.0.2,"## What's Changed\r\n* Fixed bug in setting scales with arrays (e.g. when loading data from hdf5) (https://github.com/DedalusProject/dedalus/commit/d2532390b6164e537b9dc41bea359860abac6c83)\r\n* Updated sparse matrix routines for compatibility with scipy v1.12 (https://github.com/DedalusProject/dedalus/commit/1885f707936f058fea3d4ab80ee6c567182843ce)\r\n* Added MPI explicitly to linking arguments during build, to help when not explicitly compiling with mpicc (https://github.com/DedalusProject/dedalus/commit/bbb4f93e204fac08d442ad61c8263732dd033bc5)\r\n\r\n\r\n**Full Changelog**: https://github.com/DedalusProject/dedalus/compare/v3.0.1...v3.0.2",v3.0.2,Keaton J. Burns,,kburns,GNU General Public License v3.0,dedalus,DedalusProject,7,spectral-methods,fluid-dynamics,pde-solver,,,,,,,,,,,,,,,,,,/DedalusProject/dedalus,8,20,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/datoviz/datoviz,https://github.com/datoviz/datoviz,0,"Scientific data, but visualization??",,0,1,0,0,0,0,0,1,0,0,0,⚡ High-performance GPU interactive scientific data visualization,"# Datoviz: GPU interactive scientific data visualization with Vulkan\n\n**Datoviz** is an open-source **high-performance interactive scientific data visualization library** leveraging the graphics processing unit (**GPU**) for speed, visual quality, and scalability. It supports both 2D and 3D rendering, as well as minimal graphical user interfaces (using the [Dear ImGUI library](https://github.com/ocornut/imgui)).\n\n**Written in C/C++**, Datoviz has been designed from the ground up for **performance**. It provides native **Python bindings** (based on Cython). Bindings to other languages could be developed thanks to community efforts (Julia, R, MATLAB, Rust, C#, and so on). Datoviz uses the [**Vulkan graphics API**](https://www.khronos.org/vulkan/) created by the Khronos consortium, successor of OpenGL. Supporting other modern graphics API, such as WebGPU, would constitute interesting developments.\n\nDatoviz is currently being developed mostly by [Cyrille Rossant](https://cyrille.rossant.net) at the [International Brain Laboratory](http://internationalbrainlab.org/), a consortium of neuroscience research labs around the world.\n\n> **Note**: Although its inception and development occurred over a long history spanning more than a decade, Datoviz is still an **early stage library** that would now benefit from increased community feedback. The current [**v0.1x version**](https://datoviz.org/) was released in 2021 and is available in this main branch. The upcoming **v0.2x version** is now available (in a [separate git branch](https://github.com/datoviz/datoviz/tree/v0.2x)) to audacious users for early testing and community feedback.\n\n\n\n## Screenshots\n\n![](https://raw.githubusercontent.com/datoviz/data/master/screenshots/datoviz.jpg)\n*Credits: mouse brain volume: [Allen SDK](https://alleninstitute.github.io/AllenSDK/). France: [Natural Earth](https://www.naturalearthdata.com/). Molecule: [Crystal structure of S. pyogenes Cas9 from PDB](https://www.rcsb.org/structure/4cmp) (thanks to Eric for conversion to OBJ mesh). Earth: [Pixabay](https://pixabay.com/fr/illustrations/terre-planet-monde-globe-espace-1617121/). Raster plot: IBL. 3D human brain: [Anneke Alkemade et al. 2020](https://www.frontiersin.org/articles/10.3389/fnana.2020.536838/full), thanks to Pierre-Louis Bazin and Julia Huntenburg.*\n\n\n## Code example\n\n```python\nimport numpy as np\nfrom datoviz import canvas, run, colormap\n\npanel = canvas(show_fps=True).scene().panel()\nvisual = panel.visual('marker')\n\nN = 10_000\npos = np.random.randn(N, 3)\nms = np.random.uniform(low=2, high=35, size=N)\ncolor = colormap(np.random.rand(N), cmap='viridis')\n\nvisual.data('pos', pos)\nvisual.data('ms', ms)\nvisual.data('color', color)\n\nrun()\n```\n\n\n## Documentation\n\nThe documentation is divided into:\n\n* **[Installation](https://datoviz.org/tutorials/install/)**: install guide,\n* **[Quick start tutorial](https://datoviz.org/tutorials/quickstart)** with Python,\n* **[Examples](https://datoviz.org/examples/)**: gallery of Python examples,\n* **[How to guides](https://datoviz.org/howto/)**: advanced topics explaining how to use the C API, how to create custom visuals...\n* **[Reference](https://datoviz.org/reference/)**: comprehensive list of included colormaps, visuals, and graphics pipelines,\n* **[Discussions](https://datoviz.org/discussions/)**: high-level discussions, Vulkan crash course...\n* **[C API reference](https://datoviz.org/api/)**,\n\n<!-- NOTE: we use absolute URLs so that the links work on both the GitHub README and the website -->\n\n\n## Preliminary performance results\n\n* scatter plot with 10M points: **250 FPS** (`point` visual)\n* high-resolution 3D mesh with 10M triangles and 5M vertices: **400 FPS**\n* 1000 signals with 30K points each (30M vertices): **200 FPS**\n\n*GPU: 2019 NVIDIA GeForce RTX 2070 SUPER. Window size: 1024x768.*\n\n\n## Features\n\n### Current features\n\n* **High-quality antialiased 2D visuals**: markers, paths, lines (contributions by Nicolas P. Rougier, code from [Glumpy](https://glumpy.github.io/))\n* **3D visuals**: meshes\n* **Mixing 2D and 3D** plots seamlessly in the same window\n* **~150 colormaps** included (from matplotlib, colorcet, MATLAB)\n* **High-level interactivity**: pan & zoom, mouse arcball, first-person cameras\n* **Axes**: ticks, grids, labels\n* **Subplots** organized in a grid layout\n* **GUIs** integrated via the **Dear ImGUI** C++ library (Qt or other backends not required)\n* **Custom visuals**, with custom shaders and/or custom data transformations\n\n### Experimental/work-in-progress features\n\n* Multiple canvases\n* Builtin creencasts and video recording with ffmpeg (optional dependency)\n* Offscreen rendering and CPU emulation via swiftshader\n* Mouse picking\n* IPython event-loop integration\n* DPI-aware canvases\n* Continuous integration and continuous building\n* Panel linking, shared axes\n\n### Longer-term features\n\n* More visuals: arrows, triangulations, planar straight-line graphs (PSLG), histograms, areas, graphs, fake 3D spheres...\n* Further data transformations: logarithmic, polar, basic Earth coordinate systems for geographical data\n* Fixed aspect ratio\n* Colorbars\n* 3D axes\n* Deep zooming\n* CUDA-Vulkan interoperability example\n* Better support of multiple GPUs\n* Qt integration\n* Bindings in other languages (Julia, R, MATLAB, Rust...)\n* Automated benchmarkes\n* Distributed architecture (integration in the web browser, Jupyter...)\n\n\n## History and roadmap\n\n* late 2019: first experiments with using Vulkan for scientific visualization\n* 2020: multiple cycles of prototyping and refactoring\n* **17 Feb 2021**: first public experimental release (manual compilation required)\n* **31 May 2021**: first experimental release `v0.1.0-alpha.0` with precompiled pip wheels for Linux, Windows, macOS\n* **20 Oct 2021**: new experimental release `v0.1.0-alpha.1` with bug fixes and minor improvements\n* **early 2022?**: final `v0.1.0` release\n* **late 2022?**: redesigned internal architecture for multithreading and distributed environments (still a work-in-progress)\n\n\n\n## Credits and related projects\n\nDatoviz borrows heavily ideas and code from other projects.\n\n\n### VisPy\n\n[**VisPy**](https://vispy.org/) is a Python scientific visualization library created in 2013 by Luke Campagnola (developer of [**pyqtgraph**](http://www.pyqtgraph.org/)), Almar Klein (developer of [**visvis**](https://github.com/almarklein/visvis)), Nicolas Rougier (developer of [**glumpy**](https://glumpy.github.io/)), and myself (Cyrille Rossant, developer of **galry**). We joined forces to create a single library unifying all of our approaches. There is today a community of users and projects based on VisPy ([napari](https://napari.org/)). David Hoese and some of the original VisPy developers are currently maintaining the library. The current version of VisPy suffers from the limitations of OpenGL, a 30-year-old technology.\n\nIn 2020, VisPy received a 1-year [funding from the **Chan Zuckerberg Initiative (CZI)**](https://chanzuckerberg.com/eoss/proposals/rebuilding-the-community-behind-vispys-fast-interactive-visualizations/) to improve the documentation and knowledge base.\n\nIn 2021, VisPy received another [CZI grant](https://chanzuckerberg.com/eoss/proposals/vispy-2-0-next-generation-interactive-scientific-visualization-in-python/) for building VisPy 2.0, a completely redesigned library leveraging newer GPU technology such as Vulkan and WebGPU. The current strategy is to rebuild VisPy on top of Datoviz, pygfx (a WebGPU-based library developed by Almar Klein), and other backends.\n\n\n### Glumpy\n\nGlumpy, developed by Nicolas Rougier, provides [efficient implementations of high-quality 2D visuals on the GPU](https://www.labri.fr/perso/nrougier/python-opengl/), using algorithms from the antigrain geometry library. The GPU code of most 2D visuals in Datoviz comes directly from Glumpy.\n\n\n### Dependencies and algorithms\n\n* [LunarG Vulkan SDK](https://www.lunarg.com/vulkan-sdk/) **(mandatory)**\n* [GLFW](https://www.glfw.org/) **(mandatory)** (support for alternative window backends will be considered)\n* [ffmpeg](https://ffmpeg.org/) (optional), for making live screencasts\n* [libpng](http://www.libpng.org/pub/png/libpng.html) (optional), for making PNG screenshots\n* [glslang](https://github.com/KhronosGroup/glslang) (optional), for compiling GLSL shaders to SPIR-V on the fly\n* [earcut](https://github.com/mapbox/earcut) (included), developed by Mapbox, for polygon triangulations\n* [triangle](https://www.cs.cmu.edu/~quake/triangle.html) (included), developed by Jonathan Richard Shewchuk, for Delaunay triangulations\n* [extended Wilkinson algorithm](http://vis.stanford.edu/papers/tick-labels) (included) for tick placement\n* [Dear ImGUI](https://github.com/ocornut/imgui) (included)\n* [antigrain geometry](https://en.wikipedia.org/wiki/Anti-Grain_Geometry) (GLSL implementation included)\n* [msdfgen](https://github.com/Chlumsky/msdfgen): multi-channel signed distance field (to do: bundle as submodule?)\n* [freetype](https://www.freetype.org/) (optional)\n\n\n### Related projects\n\n* [mayavi](https://docs.enthought.com/mayavi/mayavi/)\n* [VTK](https://vtk.org/)\n* [napari](https://napari.org/)\n* [vedo](https://github.com/marcomusy/vedo)\n* [ipygany](https://ipygany.readthedocs.io/en/latest/)\n* [ipyvolume](https://github.com/maartenbreddels/ipyvolume)\n* [Makie.jl](http://makie.juliaplots.org/stable/)\n\n\n## Funding\n\nDatoviz is being developed by [Cyrille Rossant](https://cyrille.rossant.net/) at the [International Brain Laboratory](https://www.internationalbrainlab.com/), with funding from the Simons Foundation, the Flatiron Institute, the Wellcome Trust, INCF. The logo was graciously created by [Chiara Zini](https://www.linkedin.com/in/czini/).\n\n![](https://raw.githubusercontent.com/datoviz/datoviz/main/docs/images/simons.png)\n![](https://raw.githubusercontent.com/datoviz/datoviz/main/docs/images/flatiron.png)\n![](https://raw.githubusercontent.com/datoviz/datoviz/main/docs/images/wellcome.jpg)\n![](https://raw.githubusercontent.com/datoviz/datoviz/main/docs/images/incf.jpg)\n\n\nGlossary:\n\n*[Vulkan]: Low-level graphics API created by Khronos, and successor of OpenGL\n*[shaders]: code written in GLSL and executed on the GPU to customize the graphics pipeline\n*[GLSL]: OpenGL shading language, the C-like language used to write shaders\n",505,graphics,C,11,CMake,Julia,Python,Rust,C,C++,Shell,GLSL,Batchfile,Dockerfile,Cython,,,,,,,,,,,,,,,,,,14,1,10,3,10,8,0,23717,25,38,16,22,ccb3283e6e96e52077d671c42382600e63c376d2,Add link to v0.2x branch in README,2024-07-04T12:25:08Z,Cyrille Rossant,rossant@users.noreply.github.com,rossant,,,,,,,MIT License,datoviz,datoviz,2,visualization,graphics,data-visualization,scientific-visualization,gpu,vulkan,c,cpp,python,scientific-computing,rendering,data-viz,,,,,,,,,/datoviz/datoviz,2,23,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/danforthcenter/plantcv,https://github.com/danforthcenter/plantcv,1,,,1,1,1,1,0,0,0,0,0,0,1,Plant phenotyping with image analysis,"![builds](https://github.com/danforthcenter/plantcv/workflows/builds/badge.svg)\n[![DeepSource](https://app.deepsource.com/gh/danforthcenter/plantcv.svg/?label=code+coverage&show_trend=true&token=og8rSyKxywOCGkIk8UNiF7B_)](https://app.deepsource.com/gh/danforthcenter/plantcv/)\n[![DeepSource](https://app.deepsource.com/gh/danforthcenter/plantcv.svg/?label=active+issues&show_trend=true&token=og8rSyKxywOCGkIk8UNiF7B_)](https://app.deepsource.com/gh/danforthcenter/plantcv/)\n[![Documentation Status](https://readthedocs.org/projects/plantcv/badge/?version=stable)](https://plantcv.readthedocs.io/en/stable/?badge=stable)\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/danforthcenter/plantcv-binder.git/master?filepath=index.ipynb)\n[![Docker Pulls](https://img.shields.io/docker/pulls/danforthcenter/plantcv.svg)](https://hub.docker.com/r/danforthcenter/plantcv/)\n[![GitHub release](https://img.shields.io/github/release/danforthcenter/plantcv.svg)](https://github.com/danforthcenter/plantcv/releases)\n[![PyPI version](https://badge.fury.io/py/plantcv.svg)](https://badge.fury.io/py/plantcv)\n![Conda](https://img.shields.io/conda/v/conda-forge/plantcv)\n[![license](https://img.shields.io/github/license/danforthcenter/plantcv.svg)](https://github.com/danforthcenter/plantcv/blob/main/LICENSE)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](docs/CODE_OF_CONDUCT.md)\n<!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section -->\n[![All Contributors](https://img.shields.io/badge/all_contributors-56-orange.svg?style=flat-square)](#contributors-)\n<!-- ALL-CONTRIBUTORS-BADGE:END -->\n\n# PlantCV: Plant phenotyping using computer vision\n\nPlease use, cite, and [contribute to](http://plantcv.readthedocs.io/en/latest/CONTRIBUTING/) PlantCV!\nIf you have questions, please submit them via the\n[GitHub issues page](https://github.com/danforthcenter/plantcv/issues).\nFollow us on twitter [@plantcv](https://twitter.com/plantcv).\n\n***\n\n## Introduction to PlantCV\n\nPlantCV is an open-source image analysis software package targeted for plant phenotyping. PlantCV provides a common\nprogramming and documentation interface to a collection of image analysis techniques that are integrated from a variety\nof source packages and algorithms. PlantCV utilizes a modular architecture that enables flexibility in the design of\nanalysis workflows and rapid assimilation and integration of new methods. For more information about the project,\nlinks to recorded presentations, and publications using PlantCV, please visit our homepage: \n<https://plantcv.danforthcenter.org/>.\n\n### Quick Links\n\n* [Documentation](http://plantcv.readthedocs.io/)\n* [Interactive Documentation](https://mybinder.org/v2/gh/danforthcenter/plantcv-binder.git/master?filepath=index.ipynb)\n* [Installation Instructions](https://plantcv.readthedocs.io/en/stable/installation/)\n* [Updating/Changelog](https://plantcv.readthedocs.io/en/stable/updating/)\n* [Public Image Datasets](http://plantcv.danforthcenter.org/pages/data.html)\n* [Contribution Guide](https://plantcv.readthedocs.io/en/stable/CONTRIBUTING/)\n* [Code of Conduct](https://plantcv.readthedocs.io/en/stable/CODE_OF_CONDUCT/)\n* Downloads\n  * [GitHub](https://github.com/danforthcenter/plantcv)\n  * [PyPI](https://pypi.org/project/plantcv/)\n  * [Conda-forge](https://anaconda.org/conda-forge/plantcv)\n  * [Docker](https://hub.docker.com/r/danforthcenter/plantcv)\n  * [Zenodo](https://doi.org/10.5281/zenodo.595522)\n\n### Citing PlantCV\n\nIf you use PlantCV, please cite the [PlantCV publications](https://plantcv.danforthcenter.org/#plantcv-publications)\nrelevant to your work. To see how others have used PlantCV in their research, check out our list of \n[publications using PlantCV](https://plantcv.danforthcenter.org/#publications-using-plantcv).\n\n***\n\n## Issues with PlantCV\n\nPlease file any PlantCV suggestions/issues/bugs via our \n[GitHub issues page](https://github.com/danforthcenter/plantcv/issues). Please check to see if any related \nissues have already been filed.\n\n***\n\n## Contributors\n\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n<table>\n  <tbody>\n    <tr>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/AFis-245""><img src=""https://avatars.githubusercontent.com/u/145135602?v=4?s=100"" width=""100px;"" alt=""AFis-245""/><br /><sub><b>AFis-245</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=AFis-245"" title=""Code"">💻</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://www.adonline.id.au/""><img src=""https://avatars.githubusercontent.com/u/5926320?v=4?s=100"" width=""100px;"" alt=""Adam Dimech""/><br /><sub><b>Adam Dimech</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=AdamDimech"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=AdamDimech"" title=""Documentation"">📖</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=AdamDimech"" title=""Tests"">⚠️</a> <a href=""#ideas-AdamDimech"" title=""Ideas, Planning, & Feedback"">🤔</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://alexanderkutschera.com/""><img src=""https://avatars.githubusercontent.com/u/20026476?v=4?s=100"" width=""100px;"" alt=""Alexander Kutschera""/><br /><sub><b>Alexander Kutschera</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=vektorious"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=vektorious"" title=""Documentation"">📖</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=vektorious"" title=""Tests"">⚠️</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/aapokor""><img src=""https://avatars.githubusercontent.com/u/39534960?v=4?s=100"" width=""100px;"" alt=""Alexandria Pokorny""/><br /><sub><b>Alexandria Pokorny</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=aapokor"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=aapokor"" title=""Documentation"">📖</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=aapokor"" title=""Tests"">⚠️</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/AzlinII""><img src=""https://avatars.githubusercontent.com/u/15007726?v=4?s=100"" width=""100px;"" alt=""Andy Lin""/><br /><sub><b>Andy Lin</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=AzlinII"" title=""Code"">💻</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""http://calizarr.github.io/resume""><img src=""https://avatars.githubusercontent.com/u/3262069?v=4?s=100"" width=""100px;"" alt=""Cesar Lizarraga""/><br /><sub><b>Cesar Lizarraga</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=calizarr"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=calizarr"" title=""Documentation"">📖</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=calizarr"" title=""Tests"">⚠️</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/cluebbert""><img src=""https://avatars.githubusercontent.com/u/47461392?v=4?s=100"" width=""100px;"" alt=""Collin Luebbert""/><br /><sub><b>Collin Luebbert</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=cluebbert"" title=""Code"">💻</a></td>\n    </tr>\n    <tr>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://www.linkedin.com/in/david-peery-344882126/""><img src=""https://avatars.githubusercontent.com/u/63679152?v=4?s=100"" width=""100px;"" alt=""David Peery""/><br /><sub><b>David Peery</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=jdavidpeery"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=jdavidpeery"" title=""Documentation"">📖</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=jdavidpeery"" title=""Tests"">⚠️</a> <a href=""#ideas-jdavidpeery"" title=""Ideas, Planning, & Feedback"">🤔</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/dhiraj-ms""><img src=""https://avatars.githubusercontent.com/u/39856917?v=4?s=100"" width=""100px;"" alt=""Dhiraj Srivastava""/><br /><sub><b>Dhiraj Srivastava</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=dhiraj-ms"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=dhiraj-ms"" title=""Documentation"">📖</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=dhiraj-ms"" title=""Tests"">⚠️</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://data-folks.masto.host/@dschneiderch""><img src=""https://avatars.githubusercontent.com/u/7461221?v=4?s=100"" width=""100px;"" alt=""Dominik Schneider""/><br /><sub><b>Dominik Schneider</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=dschneiderch"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=dschneiderch"" title=""Documentation"">📖</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=dschneiderch"" title=""Tests"">⚠️</a> <a href=""#ideas-dschneiderch"" title=""Ideas, Planning, & Feedback"">🤔</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://rayngrowingsystems.com/""><img src=""https://avatars.githubusercontent.com/u/129968640?v=4?s=100"" width=""100px;"" alt=""Dr. Alexander Kutschera""/><br /><sub><b>Dr. Alexander Kutschera</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=rayn-alex"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/issues?q=author%3Arayn-alex"" title=""Bug reports"">🐛</a> <a href=""#question-rayn-alex"" title=""Answering Questions"">💬</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/ic""><img src=""https://avatars.githubusercontent.com/u/64160?v=4?s=100"" width=""100px;"" alt=""Eric Platon""/><br /><sub><b>Eric Platon</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=ic"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=ic"" title=""Documentation"">📖</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=ic"" title=""Tests"">⚠️</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://www.datamaplab.com""><img src=""https://avatars.githubusercontent.com/u/2442821?v=4?s=100"" width=""100px;"" alt=""Fabian Dubois""/><br /><sub><b>Fabian Dubois</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=fabid"" title=""Code"">💻</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://fabiobarbero.eu""><img src=""https://avatars.githubusercontent.com/u/20724986?v=4?s=100"" width=""100px;"" alt=""Fabio Barbero""/><br /><sub><b>Fabio Barbero</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=fbarbe00"" title=""Code"">💻</a></td>\n    </tr>\n    <tr>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/ygarrot""><img src=""https://avatars.githubusercontent.com/u/33935239?v=4?s=100"" width=""100px;"" alt=""Garrot Yoan""/><br /><sub><b>Garrot Yoan</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=ygarrot"" title=""Documentation"">📖</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/GrantKonkel""><img src=""https://avatars.githubusercontent.com/u/78433847?v=4?s=100"" width=""100px;"" alt=""GrantKonkel""/><br /><sub><b>GrantKonkel</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=GrantKonkel"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=GrantKonkel"" title=""Documentation"">📖</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/HaleySchuhl""><img src=""https://avatars.githubusercontent.com/u/44006936?v=4?s=100"" width=""100px;"" alt=""Haley Schuhl""/><br /><sub><b>Haley Schuhl</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=HaleySchuhl"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=HaleySchuhl"" title=""Documentation"">📖</a> <a href=""#maintenance-HaleySchuhl"" title=""Maintenance"">🚧</a> <a href=""#tutorial-HaleySchuhl"" title=""Tutorials"">✅</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=HaleySchuhl"" title=""Tests"">⚠️</a> <a href=""#talk-HaleySchuhl"" title=""Talks"">📢</a> <a href=""#ideas-HaleySchuhl"" title=""Ideas, Planning, & Feedback"">🤔</a> <a href=""#question-HaleySchuhl"" title=""Answering Questions"">💬</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/DannieSheng""><img src=""https://avatars.githubusercontent.com/u/28633120?v=4?s=100"" width=""100px;"" alt=""Hudanyun Sheng""/><br /><sub><b>Hudanyun Sheng</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=DannieSheng"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=DannieSheng"" title=""Documentation"">📖</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=DannieSheng"" title=""Tests"">⚠️</a> <a href=""#ideas-DannieSheng"" title=""Ideas, Planning, & Feedback"">🤔</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://jakejasper.xyz""><img src=""https://avatars.githubusercontent.com/u/35571955?v=4?s=100"" width=""100px;"" alt=""Jake""/><br /><sub><b>Jake</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=Jake-Jasper"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=Jake-Jasper"" title=""Documentation"">📖</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/jberry47""><img src=""https://avatars.githubusercontent.com/u/20114985?v=4?s=100"" width=""100px;"" alt=""Jeffrey Berry""/><br /><sub><b>Jeffrey Berry</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=jberry47"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=jberry47"" title=""Documentation"">📖</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=jberry47"" title=""Tests"">⚠️</a> <a href=""#ideas-jberry47"" title=""Ideas, Planning, & Feedback"">🤔</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/JoeDuenwald""><img src=""https://avatars.githubusercontent.com/u/101900627?v=4?s=100"" width=""100px;"" alt=""JoeDuenwald""/><br /><sub><b>JoeDuenwald</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=JoeDuenwald"" title=""Documentation"">📖</a></td>\n    </tr>\n    <tr>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://sites.google.com/view/jorge-gutierrezor/home""><img src=""https://avatars.githubusercontent.com/u/26798565?v=4?s=100"" width=""100px;"" alt=""Jorge Gutierrez""/><br /><sub><b>Jorge Gutierrez</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=JorgeGtz"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=JorgeGtz"" title=""Documentation"">📖</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=JorgeGtz"" title=""Tests"">⚠️</a> <a href=""#talk-JorgeGtz"" title=""Talks"">📢</a> <a href=""#ideas-JorgeGtz"" title=""Ideas, Planning, & Feedback"">🤔</a> <a href=""#question-JorgeGtz"" title=""Answering Questions"">💬</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/joshqsumner""><img src=""https://avatars.githubusercontent.com/u/51797700?v=4?s=100"" width=""100px;"" alt=""Josh Sumner""/><br /><sub><b>Josh Sumner</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=joshqsumner"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=joshqsumner"" title=""Documentation"">📖</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=joshqsumner"" title=""Tests"">⚠️</a> <a href=""#tutorial-joshqsumner"" title=""Tutorials"">✅</a> <a href=""#ideas-joshqsumner"" title=""Ideas, Planning, & Feedback"">🤔</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/kaitlynrying""><img src=""https://avatars.githubusercontent.com/u/156353832?v=4?s=100"" width=""100px;"" alt=""Kaitlyn Ying""/><br /><sub><b>Kaitlyn Ying</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=kaitlynrying"" title=""Code"">💻</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://www.danforthcenter.org/our-work/core-facilities/phenotyping/""><img src=""https://avatars.githubusercontent.com/u/58490237?v=4?s=100"" width=""100px;"" alt=""Katie Murphy""/><br /><sub><b>Katie Murphy</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=kmurphy61"" title=""Documentation"">📖</a> <a href=""#tutorial-kmurphy61"" title=""Tutorials"">✅</a> <a href=""#talk-kmurphy61"" title=""Talks"">📢</a> <a href=""#ideas-kmurphy61"" title=""Ideas, Planning, & Feedback"">🤔</a> <a href=""#promotion-kmurphy61"" title=""Promotion"">📣</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/k034b363""><img src=""https://avatars.githubusercontent.com/u/156026125?v=4?s=100"" width=""100px;"" alt=""KeelyBrown""/><br /><sub><b>KeelyBrown</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=k034b363"" title=""Code"">💻</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""http://gehan-lab.org""><img src=""https://avatars.githubusercontent.com/u/6978303?v=4?s=100"" width=""100px;"" alt=""Malia Gehan""/><br /><sub><b>Malia Gehan</b></sub></a><br /><a href=""#projectManagement-maliagehan"" title=""Project Management"">📆</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=maliagehan"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=maliagehan"" title=""Documentation"">📖</a> <a href=""#fundingFinding-maliagehan"" title=""Funding Finding"">🔍</a> <a href=""#mentoring-maliagehan"" title=""Mentoring"">🧑‍🏫</a> <a href=""#talk-maliagehan"" title=""Talks"">📢</a> <a href=""#tutorial-maliagehan"" title=""Tutorials"">✅</a> <a href=""#ideas-maliagehan"" title=""Ideas, Planning, & Feedback"">🤔</a> <a href=""#question-maliagehan"" title=""Answering Questions"">💬</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=maliagehan"" title=""Tests"">⚠️</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/maldil""><img src=""https://avatars.githubusercontent.com/u/20140049?v=4?s=100"" width=""100px;"" alt=""Malinda""/><br /><sub><b>Malinda</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=maldil"" title=""Code"">💻</a></td>\n    </tr>\n    <tr>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/afinit""><img src=""https://avatars.githubusercontent.com/u/6353304?v=4?s=100"" width=""100px;"" alt=""Mark Wilson""/><br /><sub><b>Mark Wilson</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=afinit"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=afinit"" title=""Documentation"">📖</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=afinit"" title=""Tests"">⚠️</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/maxjfeldman""><img src=""https://avatars.githubusercontent.com/u/6977566?v=4?s=100"" width=""100px;"" alt=""Max""/><br /><sub><b>Max</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=maxjfeldman"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=maxjfeldman"" title=""Documentation"">📖</a> <a href=""#ideas-maxjfeldman"" title=""Ideas, Planning, & Feedback"">🤔</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""http://danforthcenter.org""><img src=""https://avatars.githubusercontent.com/u/6233508?v=4?s=100"" width=""100px;"" alt=""Noah Fahlgren""/><br /><sub><b>Noah Fahlgren</b></sub></a><br /><a href=""#projectManagement-nfahlgren"" title=""Project Management"">📆</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=nfahlgren"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=nfahlgren"" title=""Documentation"">📖</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=nfahlgren"" title=""Tests"">⚠️</a> <a href=""#fundingFinding-nfahlgren"" title=""Funding Finding"">🔍</a> <a href=""#mentoring-nfahlgren"" title=""Mentoring"">🧑‍🏫</a> <a href=""#talk-nfahlgren"" title=""Talks"">📢</a> <a href=""#tutorial-nfahlgren"" title=""Tutorials"">✅</a> <a href=""#ideas-nfahlgren"" title=""Ideas, Planning, & Feedback"">🤔</a> <a href=""#question-nfahlgren"" title=""Answering Questions"">💬</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/Sanazjd""><img src=""https://avatars.githubusercontent.com/u/40071989?v=4?s=100"" width=""100px;"" alt=""Sanazjd""/><br /><sub><b>Sanazjd</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=Sanazjd"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=Sanazjd"" title=""Documentation"">📖</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=Sanazjd"" title=""Tests"">⚠️</a> <a href=""#ideas-Sanazjd"" title=""Ideas, Planning, & Feedback"">🤔</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/SethPolydore""><img src=""https://avatars.githubusercontent.com/u/65034463?v=4?s=100"" width=""100px;"" alt=""SethPolydore""/><br /><sub><b>SethPolydore</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=SethPolydore"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=SethPolydore"" title=""Documentation"">📖</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=SethPolydore"" title=""Tests"">⚠️</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://orcid.org/0000-0002-1338-8900""><img src=""https://avatars.githubusercontent.com/u/2046665?v=4?s=100"" width=""100px;"" alt=""Steen Hoyer""/><br /><sub><b>Steen Hoyer</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=jshoyer"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=jshoyer"" title=""Documentation"">📖</a> <a href=""#ideas-jshoyer"" title=""Ideas, Planning, & Feedback"">🤔</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/stiphyMT""><img src=""https://avatars.githubusercontent.com/u/15986230?v=4?s=100"" width=""100px;"" alt=""Stephan Summerer""/><br /><sub><b>Stephan Summerer</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=stiphyMT"" title=""Code"">💻</a></td>\n    </tr>\n    <tr>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/stevenhwu""><img src=""https://avatars.githubusercontent.com/u/1005078?v=4?s=100"" width=""100px;"" alt=""Steven Wu""/><br /><sub><b>Steven Wu</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=stevenhwu"" title=""Code"">💻</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/Stylopidae1793""><img src=""https://avatars.githubusercontent.com/u/128506162?v=4?s=100"" width=""100px;"" alt=""Stylopidae1793""/><br /><sub><b>Stylopidae1793</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=Stylopidae1793"" title=""Documentation"">📖</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://tj-schultz.github.io/""><img src=""https://avatars.githubusercontent.com/u/73593696?v=4?s=100"" width=""100px;"" alt=""TJ Schultz""/><br /><sub><b>TJ Schultz</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=tj-schultz"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/issues?q=author%3Atj-schultz"" title=""Bug reports"">🐛</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/adrianethompson""><img src=""https://avatars.githubusercontent.com/u/85243018?v=4?s=100"" width=""100px;"" alt=""adrianethompson""/><br /><sub><b>adrianethompson</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=adrianethompson"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=adrianethompson"" title=""Documentation"">📖</a> <a href=""#tutorial-adrianethompson"" title=""Tutorials"">✅</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/annacasto""><img src=""https://avatars.githubusercontent.com/u/61164490?v=4?s=100"" width=""100px;"" alt=""annacasto""/><br /><sub><b>annacasto</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=annacasto"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=annacasto"" title=""Documentation"">📖</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=annacasto"" title=""Tests"">⚠️</a> <a href=""#tutorial-annacasto"" title=""Tutorials"">✅</a> <a href=""#talk-annacasto"" title=""Talks"">📢</a> <a href=""#ideas-annacasto"" title=""Ideas, Planning, & Feedback"">🤔</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/bganglia""><img src=""https://avatars.githubusercontent.com/u/48276939?v=4?s=100"" width=""100px;"" alt=""bganglia""/><br /><sub><b>bganglia</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=bganglia"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=bganglia"" title=""Documentation"">📖</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=bganglia"" title=""Tests"">⚠️</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/ghernandez-57""><img src=""https://avatars.githubusercontent.com/u/170016226?v=4?s=100"" width=""100px;"" alt=""ghernandez-57""/><br /><sub><b>ghernandez-57</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=ghernandez-57"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=ghernandez-57"" title=""Documentation"">📖</a></td>\n    </tr>\n    <tr>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/jgerardhodge""><img src=""https://avatars.githubusercontent.com/u/11335765?v=4?s=100"" width=""100px;"" alt=""jgerardhodge""/><br /><sub><b>jgerardhodge</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=jgerardhodge"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=jgerardhodge"" title=""Documentation"">📖</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=jgerardhodge"" title=""Tests"">⚠️</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/jmgordon1223""><img src=""https://avatars.githubusercontent.com/u/128613726?v=4?s=100"" width=""100px;"" alt=""jmgordon1223""/><br /><sub><b>jmgordon1223</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=jmgordon1223"" title=""Documentation"">📖</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/jwheeler5""><img src=""https://avatars.githubusercontent.com/u/86489506?v=4?s=100"" width=""100px;"" alt=""jwheeler5""/><br /><sub><b>jwheeler5</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=jwheeler5"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=jwheeler5"" title=""Documentation"">📖</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=jwheeler5"" title=""Tests"">⚠️</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/kbgilbert""><img src=""https://avatars.githubusercontent.com/u/13541110?v=4?s=100"" width=""100px;"" alt=""kbgilbert""/><br /><sub><b>kbgilbert</b></sub></a><br /><a href=""#design-kbgilbert"" title=""Design"">🎨</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/lacostag""><img src=""https://avatars.githubusercontent.com/u/72940710?v=4?s=100"" width=""100px;"" alt=""lacostag""/><br /><sub><b>lacostag</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=lacostag"" title=""Documentation"">📖</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/lchavez037""><img src=""https://avatars.githubusercontent.com/u/18586794?v=4?s=100"" width=""100px;"" alt=""lchavez037""/><br /><sub><b>lchavez037</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=lchavez037"" title=""Documentation"">📖</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/leowlima""><img src=""https://avatars.githubusercontent.com/u/123584304?v=4?s=100"" width=""100px;"" alt=""leowlima""/><br /><sub><b>leowlima</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=leowlima"" title=""Documentation"">📖</a></td>\n    </tr>\n    <tr>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/mtwatso2-eng""><img src=""https://avatars.githubusercontent.com/u/55025422?v=4?s=100"" width=""100px;"" alt=""mtwatso2-eng""/><br /><sub><b>mtwatso2-eng</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=mtwatso2-eng"" title=""Code"">💻</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/natejly""><img src=""https://avatars.githubusercontent.com/u/141955513?v=4?s=100"" width=""100px;"" alt=""natejly""/><br /><sub><b>natejly</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=natejly"" title=""Code"">💻</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/scallen81""><img src=""https://avatars.githubusercontent.com/u/21960021?v=4?s=100"" width=""100px;"" alt=""scallen81""/><br /><sub><b>scallen81</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=scallen81"" title=""Code"">💻</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/sdkenney42""><img src=""https://avatars.githubusercontent.com/u/128510721?v=4?s=100"" width=""100px;"" alt=""sdkenney42""/><br /><sub><b>sdkenney42</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=sdkenney42"" title=""Documentation"">📖</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/typelogic""><img src=""https://avatars.githubusercontent.com/u/45341970?v=4?s=100"" width=""100px;"" alt=""typelogic""/><br /><sub><b>typelogic</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=typelogic"" title=""Code"">💻</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/wurDevTim""><img src=""https://avatars.githubusercontent.com/u/81219429?v=4?s=100"" width=""100px;"" alt=""wurDevTim""/><br /><sub><b>wurDevTim</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=wurDevTim"" title=""Code"">💻</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=wurDevTim"" title=""Documentation"">📖</a> <a href=""https://github.com/danforthcenter/plantcv/commits?author=wurDevTim"" title=""Tests"">⚠️</a></td>\n      <td align=""center"" valign=""top"" width=""14.28%""><a href=""https://github.com/zeeuqsze""><img src=""https://avatars.githubusercontent.com/u/107494588?v=4?s=100"" width=""100px;"" alt=""zeeuqsze""/><br /><sub><b>zeeuqsze</b></sub></a><br /><a href=""https://github.com/danforthcenter/plantcv/commits?author=zeeuqsze"" title=""Documentation"">📖</a> <a href=""#tutorial-zeeuqsze"" title=""Tutorials"">✅</a> <a href=""#talk-zeeuqsze"" title=""Talks"">📢</a> <a href=""#mentoring-zeeuqsze"" title=""Mentoring"">🧑‍🏫</a></td>\n    </tr>\n  </tbody>\n</table>\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n<!-- prettier",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/Dana-Farber-AIOS/pathml,https://github.com/Dana-Farber-AIOS/pathml,1,,,1,1,1,1,0,0,0,0,0,0,1,Tools for computational pathology,"🤖🔬 **PathML: Tools for computational pathology**\n\n[![Downloads](https://static.pepy.tech/badge/pathml)](https://pepy.tech/project/pathml)\n[![Documentation Status](https://readthedocs.org/projects/pathml/badge/?version=latest)](https://pathml.readthedocs.io/en/latest/?badge=latest)\n[![codecov](https://codecov.io/gh/Dana-Farber-AIOS/pathml/branch/master/graph/badge.svg?token=UHSQPTM28Y)](https://codecov.io/gh/Dana-Farber-AIOS/pathml)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![PyPI version](https://img.shields.io/pypi/v/pathml)](https://pypi.org/project/pathml/)\n![tests](https://github.com/Dana-Farber-AIOS/pathml/actions/workflows/tests-conda.yml/badge.svg?branch=master)\n![dev-tests](https://github.com/Dana-Farber-AIOS/pathml/actions/workflows/tests-conda.yml/badge.svg?branch=dev)\n\n⭐ **PathML objective is to lower the barrier to entry to digital pathology**\n\nImaging datasets in cancer research are growing exponentially in both quantity and information density. These massive datasets may enable derivation of insights for cancer research and clinical care, but only if researchers are equipped with the tools to leverage advanced computational analysis approaches such as machine learning and artificial intelligence. In this work, we highlight three themes to guide development of such computational tools: scalability, standardization, and ease of use. We then apply these principles to develop PathML, a general-purpose research toolkit for computational pathology. We describe the design of the PathML framework and demonstrate applications in diverse use cases. \n\n🚀 **The fastest way to get started?**\n\n    docker pull pathml/pathml && docker run -it -p 8888:8888 pathml/pathml\n\ndone, what analyses can I write now? 👉 <a href=""https://chat.openai.com/g/g-L1IbnIIVt-digital-pathology-assistant-v3-0"" target=""_blank""><img src=""https://github.com/Dana-Farber-AIOS/pathml/assets/25375373/7fdc35b4-fede-431b-a8d5-324bea1873e4"" width=""30%""/></a>\n\n<table> \n<tr>\n    <td> <img src=""https://github.com/Dana-Farber-AIOS/pathml/assets/25375373/7b1b7293-03cd-4ef1-91d3-8f2efde0899a""/> </td>\n    <td>\n        \nThis AI will:\n        \n- 🤖 write digital pathology analyses for you\n- 🔬 walk you through the code, step-by-step\n- 🎓 be your teacher, as you embark on your digital pathology journey ❤️\n\nMore usage examples [here](./ai-digital-pathology-assistant-v3).\n  \n</td>\n</tr>\n</table>\n\n\n\n📖 **Official PathML Documentation**\n\nView the official [PathML Documentation on readthedocs](https://pathml.readthedocs.io/en/latest/)\n\n🔥 **Examples! Examples! Examples!**\n\n[↴ Jump to the gallery of examples below](#3-examples)\n\n<br>\n\n<img src=https://raw.githubusercontent.com/Dana-Farber-AIOS/pathml/master/docs/source/_static/images/logo.png width=""300""> \n\n<img src=https://raw.githubusercontent.com/Dana-Farber-AIOS/pathml/master/docs/source/_static/images/overview.png width=""750"">\n\n# 1. Installation\n\nThere are several ways to install `PathML`:\n\n1. `pip install` from PyPI (**recommended for users**)\n2. Clone repo to local machine and install from source (recommended for developers/contributors)\n3. Use the PathML Docker container\n4. Install in Google Colab\n\nOptions (1), (2), and (4) require that you first install all external dependencies:\n* openslide\n* JDK 8\n\nWe recommend using conda for environment management. \nDownload Miniconda [here](https://docs.conda.io/en/latest/miniconda.html)\n\n## 1.1 Installation option 1: pip install\n\nCreate conda environment, this step is common to all platforms (Linux, Mac, Windows):\n````\nconda create --name pathml python=3.8\nconda activate pathml\n````\n\nInstall external dependencies (for Linux) with [Apt](https://ubuntu.com/server/docs/package-management):\n````\nsudo apt-get install openslide-tools g++ gcc libblas-dev liblapack-dev\n````\n\nInstall external dependencies (for MacOS) with [Brew](https://www.brew.sh):\n````\nbrew install openslide\n````\n\nInstall external dependencies (for Windows) with [vcpkg](https://vcpkg.io/en/):\n````\nvcpkg install openslide\n````\n\nInstall [OpenJDK 8](https://openjdk.java.net/), this step is common to all platforms (Linux, Mac, Windows):\n````\nconda install openjdk==8.0.152\n````\n\nOptionally install CUDA (instructions [here](#CUDA))\n\nInstall `PathML` from PyPI:\n````\npip install pathml\n````\n\n## 1.2 Installation option 2: clone repo and install from source\n\nClone repo:\n````\ngit clone https://github.com/Dana-Farber-AIOS/pathml.git\ncd pathml\n````\n\nCreate conda environment:\n````\nconda env create -f environment.yml\nconda activate pathml\n````\n\nOptionally install CUDA (instructions [here](#CUDA))\n\nInstall `PathML` from source: \n````\npip install -e .\n````\n\n## 1.3 Installation option 3: Docker\n\nFirst, download or build the PathML Docker container:\n\n![pathml-docker-installation](https://user-images.githubusercontent.com/25375373/191053363-477497a1-9804-48f3-91f9-767dc7f859ed.gif)\n\n- *Step 1:* download PathML container from Docker Hub\n   ````\n   docker pull pathml/pathml:latest\n   ````\n  Optionally specify a tag for a particular version, e.g. `docker pull pathml/pathml:2.0.2`. To view possible tags, \n  please refer to the [PathML DockerHub page](https://hub.docker.com/r/pathml/pathml).\n  \n- *Alternative Step 1* if you have custom hardware: build docker container from source\n   ````\n   git clone https://github.com/Dana-Farber-AIOS/pathml.git\n   cd pathml\n   docker build -t pathml/pathml .\n   ````\n\n- *Step 2:* Then connect to the container:\n  ````\n  docker run -it -p 8888:8888 pathml/pathml\n  ````\n\nThe above command runs the container, which is configured to spin up a jupyter lab session and expose it on port 8888. \nThe terminal should display a URL to the jupyter lab session starting with `http://127.0.0.1:8888/lab?token=<.....>`. \nNavigate to that page and you should connect to the jupyter lab session running on the container with the pathml \nenvironment fully configured. If a password is requested, copy the string of characters following the `token=` in the \nurl.\n\nNote that the docker container requires extra configurations to use with GPU.  \nNote that these instructions assume that there are no other processes using port 8888.\n\nPlease refer to the `Docker run` [documentation](https://docs.docker.com/engine/reference/run/) for further instructions\non accessing the container, e.g. for mounting volumes to access files on a local machine from within the container.\n\n## 1.4 Installation option 4: Google Colab\n\nTo get PathML running in a Colab environment:\n\n````\nimport os\n!pip install openslide-python\n!apt-get install openslide-tools\n!apt-get install openjdk-8-jdk-headless -qq > /dev/null\nos.environ[""JAVA_HOME""] = ""/usr/lib/jvm/java-8-openjdk-amd64""\n!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n!java -version\n!pip install pathml\n````\n\nPathML Tutorials we published in Google Colab\n1. [PathML Tutorial Colab #1 - Load an SVS image in PathML and see the image descriptors](https://colab.research.google.com/drive/12ICBsJLCvuubTqb42-Wr5k-2EVDPbbNQ#scrollTo=Qog8Y6wARMgW)\n2. [Now that you have PathML installed, all our other examples would work too](https://github.com/Dana-Farber-AIOS/pathml#3-examples) - Only make sure you select an appropriately sized backend or VM in CoLab (i.e., RAM, CPU, Disk, and GPU if necessary) \n\n*Thanks to all of our open-source collaborators for helping maintain these installation instructions!*  \n*Please open an issue for any bugs or other problems during installation process.*\n\n## 1.5 CUDA (optional)\n\nTo use GPU acceleration for model training or other tasks, you must install CUDA. \nThis guide should work, but for the most up-to-date instructions, refer to the [official PyTorch installation instructions](https://pytorch.org/get-started/locally/).\n\nCheck the version of CUDA:\n````\nnvidia-smi\n````\n\nInstall correct version of `cudatoolkit`:\n````\n# update this command with your CUDA version number\nconda install cudatoolkit=11.0\n````\n\nAfter installing PyTorch, optionally verify successful PyTorch installation with CUDA support: \n````\npython -c ""import torch; print(torch.cuda.is_available())""\n````\n\n# 2. Using with Jupyter (optional)\n\nJupyter notebooks are a convenient way to work interactively. To use `PathML` in Jupyter notebooks: \n\n## 2.1 Set JAVA_HOME environment variable\n\nPathML relies on Java to enable support for reading a wide range of file formats.\nBefore using `PathML` in Jupyter, you may need to manually set the `JAVA_HOME` environment variable \nspecifying the path to Java. To do so:\n\n1. Get the path to Java by running `echo $JAVA_HOME` in the terminal in your pathml conda environment (outside of Jupyter)\n2. Set that path as the `JAVA_HOME` environment variable in Jupyter:\n    ````\n    import os\n    os.environ[""JAVA_HOME""] = ""/opt/conda/envs/pathml"" # change path as needed\n    ````\n\n## 2.2 Register environment as an IPython kernel\n````\nconda activate pathml\nconda install ipykernel\npython -m ipykernel install --user --name=pathml\n````\nThis makes the pathml environment available as a kernel in jupyter lab or notebook.\n\n# 3. Examples\n\nNow that you are all set with ``PathML`` installation, let's get started with some analyses you can easily replicate:\n\n<table style=""border: 0px !important;"">\n    <tr>\n    <td> \n        \n1. [Load over 160+ different types of pathology images using PathML](https://github.com/Dana-Farber-AIOS/pathml/blob/master/examples/loading_images_vignette.ipynb)\n2. [H&E Stain Deconvolution and Color Normalization](https://github.com/Dana-Farber-AIOS/pathml/blob/master/examples/stain_normalization.ipynb)\n3. [Brightfield imaging pipeline: load an image, preprocess it on a local cluster, and get it read for machine learning analyses in PyTorch](https://github.com/Dana-Farber-AIOS/pathml/blob/master/examples/workflow_HE_vignette.ipynb)\n4. [Multiparametric Imaging: Quickstart & single-cell quantification](https://github.com/Dana-Farber-AIOS/pathml/blob/master/examples/multiplex_if.ipynb)\n5. [Multiparametric Imaging: CODEX & nuclei quantization](https://github.com/Dana-Farber-AIOS/pathml/blob/master/examples/codex.ipynb)\n6. [Train HoVer-Net model to perform nucleus detection and classification, using data from PanNuke dataset](https://github.com/Dana-Farber-AIOS/pathml/blob/master/examples/train_hovernet.ipynb)\n7. [Gallery of PathML preprocessing and transformations](https://github.com/Dana-Farber-AIOS/pathml/blob/master/examples/pathml_gallery.ipynb)\n\n</td>                                                                                                                             \n        <td>\n<img src=""https://github.com/Dana-Farber-AIOS/pathml/assets/25375373/502c9e69-e988-4d61-b50f-0d6bfc8af251"" width=""1000px"" />\n\n   </td>\n</tr>\n</table>\n\n\n# 4. Citing & known uses\n\nIf you use ``PathML`` please cite:\n\n- [**J. Rosenthal et al., ""Building tools for machine learning and artificial intelligence in cancer research: best practices and a case study with the PathML toolkit for computational pathology."" Molecular Cancer Research, 2022.**](https://doi.org/10.1158/1541-7786.MCR-21-0665)\n\nSo far, **PathML** was referenced in 20+ manuscripts:\n\n-   [H. Pakula et al. **Nature Communications**, 2024](https://www.nature.com/articles/s41467-023-44210-1)\n-   [B. Ricciuti et al. **Journal of Clinical Oncology**, 2024](https://ascopubs.org/doi/full/10.1200/JCO.23.00580)\n-   [A. Song et al. **Nature Reviews Bioengineering**, 2023](https://www.nature.com/articles/s44222-023-00096-8)\n-   [I. Virshup et al. **Nature Bioengineering**, 2023](https://www.nature.com/articles/s41587-023-01733-8)\n-   [A. Karargyris et al. **Nature Machine Intelligence**, 2023](https://www.nature.com/articles/s42256-023-00652-2)\n-   [S. Pati et al. **Nature Communications Engineering**, 2023](https://www.nature.com/articles/s44172-023-00066-3)\n-   [C. Gorman et al. **Nature Communications**, 2023](https://www.nature.com/articles/s41467-023-37224-2)\n-   [J. Nyman et al. **Cell Reports Medicine**, 2023](https://doi.org/10.1016/j.xcrm.2023.101189)\n-   [A. Shmatko et al. **Nature Cancer**, 2022](https://www.nature.com/articles/s43018-022-00436-4)\n-   [J. Pocock et al. **Nature Communications Medicine**, 2022](https://www.nature.com/articles/s43856-022-00186-5)\n-   [S. Orsulic et al. **Frontiers in Oncology**, 2022](https://www.frontiersin.org/articles/10.3389/fonc.2022.924945/full)\n-   [J. Linares et al. **Molecular Cell**, 2021](https://doi.org/10.1016/j.molcel.2021.08.039)\n-   the list continues [**here** **🔗**](https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1157052756975292108)\n\n# 5. Users\n\n<table style=""border: 0px !important;""><tr><td>This is where in the world our most enthusiastic supporters are located:\n   <br/><br/>\n<img src=""https://user-images.githubusercontent.com/25375373/208137141-e450aa86-8433-415a-9cc7-c4274139bdc2.png"" width=""500px"">\n   </td><td>   \nand this is where they work:\n   <br/><br/>\n<img src=""https://user-images.githubusercontent.com/25375373/208137644-f73c86d0-c5c7-4094-80d9-ea11e0edbdc5.png"" width=""400px"">\n</td>                                                                                                                             \n</tr>\n</table>\n\nSource: https://ossinsight.io/analyze/Dana-Farber-AIOS/pathml#people\n\n# 6. Contributing\n\n``PathML`` is an open source project. Consider contributing to benefit the entire community!\n\nThere are many ways to contribute to `PathML`, including:\n\n* Submitting bug reports\n* Submitting feature requests\n* Writing documentation and examples\n* Fixing bugs\n* Writing code for new features\n* Sharing workflows\n* Sharing trained model parameters\n* Sharing ``PathML`` with colleagues, students, etc.\n\nSee [contributing](https://github.com/Dana-Farber-AIOS/pathml/blob/master/CONTRIBUTING.rst) for more details.\n\n\n# 7. License\n\nThe GNU GPL v2 version of PathML is made available via Open Source licensing. \nThe user is free to use, modify, and distribute under the terms of the GNU General Public License version 2.\n\nCommercial license options are available also.\n\n# 8. Contact\n\nQuestions? Comments? Suggestions? Get in touch!\n\n[pathml@dfci.harvard.edu](mailto:pathml@dfci.harvard.edu)\n\n<img src=https://raw.githubusercontent.com/Dana-Farber-AIOS/pathml/master/docs/source/_static/images/dfci_cornell_joint_logos.png width=""750""> \n",372,pathology,Python,2,Python,Dockerfile,,,,,,,,,,,,,,,,,,,,,,,,,,,230,33,187,10,27,22,25,228172,81,205,163,42,f86c5c6c38beebcce9cbd6309408946bc9277138,Update README.md w/ dpa button,2024-04-10T16:43:48Z,Dana-Farber,Dana-Farber@users.noreply.github.com,Dana-Farber,v2.1.1,## What's Changed\r\n* use current year in docs by @jacob-rosenthal in https://github.com/Dana-Farber-AIOS/pathml/pull/311\r\n* pin protobuf to 3.20.1 by @jacob-rosenthal in https://github.com/Dana-Farber-AIOS/pathml/pull/314\r\n* Make error message for extract_region more informative by @jacob-rosenthal in https://github.com/Dana-Farber-AIOS/pathml/pull/320\r\n* Add action to enforce pre-commit hooks by @tddough98 in https://github.com/Dana-Farber-AIOS/pathml/pull/333\r\n* Merge main branch into dev by @tddough98 in https://github.com/Dana-Farber-AIOS/pathml/pull/334\r\n* Bump ipython from 7.31.1 to 8.10.0 in /docs by @dependabot in https://github.com/Dana-Farber-AIOS/pathml/pull/346\r\n* Fixed docs by @sreekarreddydfci in https://github.com/Dana-Farber-AIOS/pathml/pull/348\r\n\r\n## New Contributors\r\n* @tddough98 made their first contribution in https://github.com/Dana-Farber-AIOS/pathml/pull/333\r\n\r\n**Full Changelog**: https://github.com/Dana-Farber-AIOS/pathml/compare/v2.1.0...v2.1.1,v2.1.1,,,sreekarreddydfci,GNU General Public License v2.0,pathml,Dana-Farber-AIOS,16,machine-learning,digital-pathology,computational-pathology,biomedical-image-processing,pathology,histopathology,spatial-transcriptomics,image-analysis,microscopy,fluorescence-microscopy-imaging,deep-learning,python,pytorch,research,pathml,,,,,,/Dana-Farber-AIOS/pathml,19,13,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/cyberbotics/webots,https://github.com/cyberbotics/webots,1,,,1,1,1,0,0,0,0,0,0,0,0,Webots Robot Simulator,"# Webots: open-source robot simulator\n\n[![Webots](https://img.shields.io/github/v/release/cyberbotics/webots)](https://github.com/cyberbotics/webots/releases/latest)\n[![Software License](https://img.shields.io/badge/license-Apache%202.0-blue)](LICENSE)\n[![User Guide](https://img.shields.io/badge/doc-guide-blue)](https://cyberbotics.com/doc/reference/index)\n[![Reference Manual](https://img.shields.io/badge/doc-reference-blue.svg)](https://cyberbotics.com/doc/reference/index)<br>\n[![Stars](https://img.shields.io/github/stars/cyberbotics/webots)](https://github.com/cyberbotics/webots/stargazers)\n[![Downloads](https://img.shields.io/github/downloads/cyberbotics/webots/total?color=blue)](https://hanadigital.github.io/grev/?user=cyberbotics&repo=webots)\n[![Contributions](https://img.shields.io/github/commit-activity/m/cyberbotics/webots.svg)](https://github.com/cyberbotics/webots/graphs/commit-activity)\n[![Contributors](https://img.shields.io/github/contributors/cyberbotics/webots?color=blue)](https://github.com/cyberbotics/webots/graphs/contributors)\n[![GitHub Discussions](https://img.shields.io/github/discussions/cyberbotics/webots)](https://github.com/cyberbotics/webots/discussions)\n[![Chat](https://img.shields.io/discord/565154702715518986?color=blue)](https://discordapp.com/invite/nTWbN9m)\n\n\n![Webots Screenshot](docs/guide/images/main_window.png?raw=true ""Webots Screenshot"")\n\nWebots provides a complete development environment to model, program and simulate robots, vehicles and mechanical systems.\n\n- See the [Webots introduction video](https://www.youtube.com/watch?v=O7U3sX_ubGc).\n- View online Webots simulations at [webots.cloud](https://webots.cloud).\n- Participate in the [IROS 2023 Simulated Humanoid Robot Wrestling Competition](https://webots.cloud/run?version=R2023a&url=https%3A%2F%2Fgithub.com%2Fcyberbotics%2Fwrestling%2Fblob%2Fmain%2Fworlds%2Fwrestling.wbt&type=competition) and win 1 Ethereum.\n\n### Download\n\nGet pre-compiled binaries for the [latest release](https://github.com/cyberbotics/webots/releases/latest), as well as [older releases and nightly builds](https://github.com/cyberbotics/webots/releases).\n\nCheck out installation instructions:\n\n[![Linux](https://img.shields.io/badge/Linux-0f80c0?logo=linux&logoColor=white)](https://cyberbotics.com/doc/guide/installation-procedure#installation-on-linux)\n[![Windows](https://img.shields.io/badge/Windows-0f80c0?logo=windows&logoColor=white)](https://cyberbotics.com/doc/guide/installation-procedure#installation-on-windows)\n[![macOS](https://img.shields.io/badge/macOS-0f80c0?logo=apple&logoColor=white)](https://cyberbotics.com/doc/guide/installation-procedure#installation-on-macos)\n\n### Build from Source\n\nIf you prefer to [compile Webots from source](https://github.com/cyberbotics/webots/wiki), read the [contributing guidelines](CONTRIBUTING.md).\n\n### Continuous Integration Nightly Tests\n\n[![master branch](https://img.shields.io/badge/branch-master-blue)](https://github.com/cyberbotics/webots/tree/master)\n[![Linux build (master)](https://github.com/cyberbotics/webots/actions/workflows/test_suite_linux.yml/badge.svg?event=schedule)](https://github.com/cyberbotics/webots/actions/workflows/test_suite_linux.yml?query=event%3Aschedule)\n[![Windows build (master)](https://github.com/cyberbotics/webots/actions/workflows/test_suite_windows.yml/badge.svg?event=schedule)](https://github.com/cyberbotics/webots/actions/workflows/test_suite_windows.yml?query=event%3Aschedule)\n[![macOS build (master)](https://github.com/cyberbotics/webots/actions/workflows/test_suite_mac.yml/badge.svg?event=schedule&label=macOS)](https://github.com/cyberbotics/webots/actions/workflows/test_suite_mac.yml?query=event%3Aschedule)<br>\n[![develop branch](https://img.shields.io/badge/branch-develop-blue)](https://github.com/cyberbotics/webots/tree/develop)\n[![Linux build (develop)](https://github.com/cyberbotics/webots/actions/workflows/test_suite_linux_develop.yml/badge.svg?event=schedule)](https://github.com/cyberbotics/webots/actions/workflows/test_suite_linux_develop.yml?query=event%3Aschedule)\n[![Windows build (develop)](https://github.com/cyberbotics/webots/actions/workflows/test_suite_windows_develop.yml/badge.svg?event=schedule)](https://github.com/cyberbotics/webots/actions/workflows/test_suite_windows_develop.yml?query=event%3Aschedule)\n[![macOS build (develop)](https://github.com/cyberbotics/webots/actions/workflows/test_suite_mac_develop.yml/badge.svg?event=schedule)](https://github.com/cyberbotics/webots/actions/workflows/test_suite_mac_develop.yml?query=event%3Aschedule)\n\n### About us\n\nWebots was originally designed at [EPFL](https://epfl.ch) in 1996 and then further developed and commercialized by [Cyberbotics](https://cyberbotics.com) since 1998. In December 2018, Webots was open sourced. Since then, [Cyberbotics](https://cyberbotics.com) continues to develop Webots thanks to paid customer support, training, consulting for industry and academic research projects.\n\n[Contact us](mailto:info@cyberbotics.com) to discuss your custom robot simulation projects.\n",3130,fluid-dynamics,C++,21,Makefile,HTML,C++,C,Shell,M,MATLAB,Python,Java,JavaScript,Objective-C,CSS,Assembly,Mercury,PHP,Lua,GLSL,M4,Batchfile,SWIG,Dockerfile,,,,,,,,4471,359,4106,6,12,174,102,4287678,1656,1789,1595,194,a01ffad93a4d908912925d68eff10b213417c03e,Fix #6578 `Device`s with Duplicate Names Cause Unexpected Behavior (#…,2024-07-10T10:11:53Z,CoolSpy3,55305038+CoolSpy3@users.noreply.github.com,CoolSpy3,R2023b,"\r\n![R2023b](https://github.com/cyberbotics/webots/assets/25938827/61bfd8ba-5b79-4e38-8c35-e47df03788fb)\r\n\r\nThis major release comes with many improvements: a new robot, a new devices and a new node.\r\n\r\nIt is recommended to proceed with this update.\r\n\r\nSee the [official announcement](https://cyberbotics.com/doc/blog/Webots-2023-b-release) and the [complete change log](https://cyberbotics.com/doc/reference/changelog-r2023) for details.\r\n\r\n| OS | File(s) |\r\n| :---: | :---: |\r\n| ![Windows](https://user-images.githubusercontent.com/2461619/60157660-95571180-97ef-11e9-8173-fa41a4092345.png)<br>**Windows 10**<br><sub>Intel/AMD 64-bit</sub>  | [webots-R2023b_setup.exe](https://github.com/cyberbotics/webots/releases/download/R2023b/webots-R2023b_setup.exe) |\r\n| ![Linux](https://user-images.githubusercontent.com/1264964/189833997-ff4d3b66-2419-4010-8c84-9051569cf610.png)<br>**Linux**<br><sub>Intel/AMD 64-bit</sub> | [webots_2023b_amd64.deb](https://github.com/cyberbotics/webots/releases/download/R2023b/webots_2023b_amd64.deb) (Ubuntu 20.04 & 22.04)<br>[webots-R2023b-x86-64.tar.bz2](https://github.com/cyberbotics/webots/releases/download/R2023b/webots-R2023b-x86-64.tar.bz2) (Ubuntu 20.04 & 22.04)<br>[![Webots](https://snapcraft.io/webots/badge.svg)](https://snapcraft.io/webots) (Linux snap package)<br>[![Webots](https://user-images.githubusercontent.com/2461619/92602550-8fdbcf00-f2ae-11ea-95cf-a580dc5e763e.png)](https://hub.docker.com/r/cyberbotics/webots) (Docker Image) |\r\n| ![macOS](https://user-images.githubusercontent.com/2461619/60157679-a2740080-97ef-11e9-9cf0-30e3712d8fff.png)<br> **macOS**<br><sub>Apple Silicon & Intel</sub> | [webots-R2023b.dmg](https://github.com/cyberbotics/webots/releases/download/R2023b/webots-R2023b.dmg) |\r\n",R2023b,,,github-actions[bot],Apache License 2.0,webots,cyberbotics,16,robotics,robot,simulator,simulation,ros,physics-engine,3d-engine,fluid-dynamics,multi-platform,autonomous-vehicles,computer-vision,ai,webots,robot-simulator,simulated-robots,robotics-simulation,robots,open-source,robot-simulation,ros2,/cyberbotics/webots,18,77,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/CyberAgentGameEntertainment/NovaShader,https://github.com/CyberAgentGameEntertainment/NovaShader,0,,,0,0,0,0,0,0,1,0,0,0,0,Multi-functional shader for the Particle System that supports Universal Render Pipeline (URP) of Unity.,,1068,graphics,C#,3,C#,HLSL,ShaderLab,,,,,,,,,,,,,,,,,,,,,,,,,,66,4,62,0,4,7,0,56287,69,11,9,2,68d2e08ef47afb47ad035c0b5090846c5ec4e345,Merge pull request #77 from CyberAgentGameEntertainment/feature/add_T…,2024-05-21T06:48:32Z,ZYB,119645979+S20817@users.noreply.github.com,S20817,02.02.2003,## What's Changed\r\n* Added note regarding PreviewRenderUtility by @CA-Tatami in https://github.com/CyberAgentGameEntertainment/NovaShader/pull/74\r\n* TintColorMapのOffsetにCustomCoord追加 by @S20817 in https://github.com/CyberAgentGameEntertainment/NovaShader/pull/77\r\n\r\n\r\n**Full Changelog**: https://github.com/CyberAgentGameEntertainment/NovaShader/compare/2.2.2...2.2.3,02.02.2003,ZYB,,S20817,MIT License,NovaShader,CyberAgentGameEntertainment,24,unity,shaders,graphics,vfx,particles,particlesystem,urp,,,,,,,,,,,,,,/CyberAgentGameEntertainment/NovaShader,25,24,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/CSBiology/BioFSharp,https://github.com/CSBiology/BioFSharp,0,,,0,1,0,0,0,0,1,0,0,0,0,Open source bioinformatics and computational biology toolbox written in F#.,"\n![Logo](docs/img/Logo_large.png)\n\n\n[![Nuget](https://img.shields.io/nuget/v/BioFSharp)](https://www.nuget.org/packages/BioFSharp/)\n[![Made with F#](https://img.shields.io/badge/Made%20with-FSharp-rgb(184,69,252).svg)](https://fsharp.org/)\n\n\nBioFSharp is an open source bioinformatics and computational biology toolbox written in F#. <https://csbiology.github.io/BioFSharp/>\n\n[![Gitter](https://badges.gitter.im/CSBiology/BioFSharp.svg)](https://gitter.im/CSBiology/BioFSharp?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n![GitHub contributors](https://img.shields.io/github/contributors/CSBiology/BioFSharp)\n\n| Build status (ubuntu and windows) | Test Coverage |\n|---|---|\n| ![](https://github.com/CSBiology/BioFSharp/actions/workflows/build-test.yml/badge.svg) | [![codecov](https://codecov.io/gh/CSBiology/BioFSharp/branch/developer/graph/badge.svg)](https://codecov.io/gh/CSBiology/BioFSharp) |\n\n\n\nCore functionality\n------------------\n\nIn its core namespace, BioFSharp contains the basic data structures for common biological objects and their modification. Our type modeling starts at chemical elements, abstracts those to form formulas, and finally molecules of high biological relevance such as amino acids and nucleotides. Sequences of these molecules are modelled by BioCollections, which provide extensive functionality for investigating their real life counterparts.\n\n![Data model](docs/img/Core.png)\n\nAdditionally, core algorithms for biological sequences such as alignments and pattern matching algorithms are implemented.\n\nBesides the core functionality, BioFSharp has several namespaces as sub-projects with different scopes:\n\nIO functionality\n----------------\n\nThe IO namespace aims to make data available and ease further processing. It contains read/write functions for a diverse set of biological file formats such as [Fasta](https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&PAGE_TYPE=BlastDocs&DOC_TYPE=BlastHelp), [FastQ](https://www.ncbi.nlm.nih.gov/sra/docs/submitformats/#fastq-files), [GeneBank](https://www.ncbi.nlm.nih.gov/Sitemap/samplerecord.html) or [GFF](https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md), as well as helper function for searching on or transforming the input data. Wrappers for commonly used command line tools like [NCBI's Blast](https://www.ncbi.nlm.nih.gov/books/NBK153387/) assure interoperability with an array of existing bioinformatic workflows\n\nBioDB functionality\n-------------------\n\nThe BioDB namespace offers API access to powerful popular databases like [GEO](https://www.ncbi.nlm.nih.gov/geo/) and [EBI(including SwissProt/Expasy)](https://www.ebi.ac.uk/). We additionally provide an API access for [FATool](http://iomiqsweb1.bio.uni-kl.de/), a webservice by our workgroup for querying functional annotations of proteins.\n\nThis project is netframework only and has a new home here: https://github.com/CSBiology/BioFSharp.BioDB\n\nBioContainers functionality\n----------------------\n\nThe BioContainers namespace is our newest BioFSharp project and we are very excited about it! It is all about making common bioinformatics tools programmatically accessible from F#. \nThis is realized by making the containerized tool accessible via the Docker daemon. We wrap some functionality from\n[Docker.DotNet](https://github.com/microsoft/Docker.DotNet) to communicate with the docker API while providing extensive, type safe bindings for already 9 tools, including Blast, ClustalO, and TMHMM\n\nML functionality\n----------------\n\nMake your workflow ML ready with BioFSharp.ML. Currently contains helper functionf for [CNTK](https://docs.microsoft.com/en-us/cognitive-toolkit/) and a pre-trained model we used in our [publication about predicting peptide observability](https://www.frontiersin.org/articles/10.3389/fpls.2018.01559/full).\n\nStats functionality\n----------------------\n\nThe Stats namespace contains statistical functions with a clear biological focus such as functions for calculating Gene Ontology Enrichments.\n\n\nDocumentation\n-------------\n\nFunctions, types and Classes contained in BioFSharp come with short explanatory description, which can be found in the [API Reference](https://csbiology.github.io/BioFSharp/reference/index.html).\n\nMore indepth explanations, tutorials and general information about the project can be found [here](http://csbiology.github.io/BioFSharp).\n\nThe documentation and tutorials for this library are automatically generated (using the F# Formatting) from *.fsx and *.md files in the docs folder. If you find a typo, please submit a pull request!\n\nContributing\n------------\n\nPlease refer to the [Contribution guidelines](.github/CONTRIBUTING.md)\n\nCommunity/Social\n----------------\nWant to get in touch with us? We recently joined the twitter crowd:\n\n[![Twitter Follow](https://img.shields.io/twitter/follow/BioFSharp.svg?style=social)](https://twitter.com/biofsharp)\n\n[![Twitter Follow](https://img.shields.io/twitter/follow/cs_biology.svg?style=social)](https://twitter.com/cs_biology)",105,bioinformatics,F#,7,Batchfile,F#,Shell,Perl,Dockerfile,PowerShell,Jupyter Notebook,,,,,,,,,,,,,,,,,,,,,,70,13,55,2,19,21,154,325585,32,66,32,34,5af7fe338f5ca35213d37d8624f35139568ef80f,Removed submodule,2024-05-14T07:38:30Z,Kevin Schneider,schneider.kev@outlook.de,kMutagene,"1.2.0 - Monday, March 30",\r\n**Additions:**\r\n * **BioFSharp.BioDD:**\r\n   * [Full low-level DSL for generating Entrez queries for NCBI cgis](https://github.com/CSBiology/BioFSharp/issues/84):\r\n     * [einfo](https://github.com/CSBiology/BioFSharp/commit/311e2fea029eb536fb21016e33cc60e6aed7b63f)\r\n     * [esearch](https://github.com/CSBiology/BioFSharp/commit/7d000e24a40cba6a6944f7fefd56fb9aca5a696f)\r\n     * [epost](https://github.com/CSBiology/BioFSharp/commit/d8e0daf8b6e80cb6f79740a68a20dfffea2a98d7)\r\n     * [esummary](https://github.com/CSBiology/BioFSharp/commit/d8e0daf8b6e80cb6f79740a68a20dfffea2a98d7)\r\n     * [efetch](https://github.com/CSBiology/BioFSharp/commit/07ab9b96ae761ab0d393e936bb8de42b25cfb7c9)\r\n     * [elink](https://github.com/CSBiology/BioFSharp/commit/8b0d1649c2b22d93f02541da7944a2bb9e441c65)\r\n     * [egquery](https://github.com/CSBiology/BioFSharp/commit/147d8d01c295f2326e123b1588bdcad2c69cda6e)\r\n     * [espell](https://github.com/CSBiology/BioFSharp/commit/147d8d01c295f2326e123b1588bdcad2c69cda6e)\r\n     * [ecitmatch](https://github.com/CSBiology/BioFSharp/commit/147d8d01c295f2326e123b1588bdcad2c69cda6e)\r\n * **Bugfixes:**\r\n   * BioFSharp.BioContainers:\r\n     * [fix fastP DSL command line generation](https://github.com/CSBiology/BioFSharp/commit/b1768ae3d728cdd59cdfe1e23aae2ff5554f11b9) (missing `=` signs)\r\n,01.02.2000,Kevin Schneider,,kMutagene,MIT License,BioFSharp,CSBiology,7,bioinformatics,biostatistics,datascience,biology,dataprocessing,fsharp,amino-acids,nucleotides,sequence-analysis,docker,bioinformatics-containers,biocontainers,,,,,,,,,/CSBiology/BioFSharp,10,15,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/cosmonium/cosmonium,https://github.com/cosmonium/cosmonium,0,not really scientific,,0,0,1,0,0,0,0,1,0,0,0,3D astronomy and space exploration program.,"![Cosmonium](textures/cosmonium-name.png)\n\n[![Build Status](https://github.com/cosmonium/cosmonium/workflows/build/badge.svg)](https://github.com/cosmonium/cosmonium/actions)\n[![Latest release](https://img.shields.io/github/v/release/cosmonium/cosmonium?label=Lastest%20release)](https://github.com/cosmonium/cosmonium/wiki/Download)\n[![Latest build](https://img.shields.io/github/v/release/cosmonium/cosmonium?include_prereleases&label=Lastest%20build)](https://github.com/cosmonium/cosmonium/wiki/Download)\n[![GitHub](https://img.shields.io/github/license/cosmonium/cosmonium)](https://github.com/cosmonium/cosmonium/blob/master/COPYING.md)\n\nCosmonium is a 3D astronomy and space exploration program. With Cosmonium you can navigate in our solar system and discover all the planets and their moons. You can also visit the neighboring stars and discover the true size of our galaxy and the Universe.\n\nCosmonium supports (or will support) the creation of fictional planets, stellar systems nebulaes, ... using procedural generation.\n\nCosmonium also already supports some Celestia addons (though CMOD and CelX are not yet supported).\n\n### Requirements\n\nCosmonium runs on Windows (Vista or above), Linux (CentOS 5, Ubuntu 14 or above) or macOS (mac0S 10.9 or above)\nwith a graphic card supporting OpenGL 2.1 or better (OpenGL 4.5 is recommended) and at least 512MB of disk\n(up to 4GB if the HD and UHD textures are installed).\n\n### Installation \n\nDownload the installer or package for your platform from the [download](https://github.com/cosmonium/cosmonium/wiki/Download) page and see the [[Installation]] page\nThe package contains only low resolution textures, see [here](https://github.com/cosmonium/cosmonium/wiki/Download#extra-textures) to install extra HD and UHD textures.\n\n### Screenshots\n\nSee in the [Wiki](https://github.com/cosmonium/cosmonium/wiki/Screenshots) some screenshots of the application with views of\n[Saturn](https://github.com/cosmonium/cosmonium/wiki/Screenshots#rings-of-saturn),\n[Jupiter](https://github.com/cosmonium/cosmonium/wiki/Screenshots#io-casting-a-shadow-on-jupiter),\n[Mars](https://github.com/cosmonium/cosmonium/wiki/Screenshots#phobos-over-mars),\nthe [Moon](https://github.com/cosmonium/cosmonium/wiki/Screenshots#moon-crescent),\n[procedural planets](https://github.com/cosmonium/cosmonium/wiki/Screenshots#procedural-planet), ...\n\n![Jupiter](https://github.com/cosmonium/cosmonium/wiki/screenshots/Io+Jupiter.png)\n\n### Launch\n\nSimply starts cosmonium from your application menu or from the cosmonium folder. See also the [installation](https://github.com/cosmonium/cosmonium/wiki/Installation) page for more options.\n\n### User interface\n\nCosmonium user interface is still heavily based on Celestia, most of the command and keyboard shortcuts work the same.\nGo to [First steps](https://github.com/cosmonium/cosmonium/wiki/First-steps) to have an explanation of the basic command or see the [Control](https://github.com/cosmonium/cosmonium/wiki/Control) page for an exhaustive list.\n\n### Full documentation\n\nCosmonium is still in its infancy, but it is already usable to explore all the planets and the moons of our solar system, all the neighbor or visible stars and much more.\nIt also support custom content and addons, either as Cosmonium or Celestia addons.\n\nThe full documentation is available in the [Wiki](https://github.com/cosmonium/cosmonium/wiki)\n\n### Bugs\n\nIf you encounter any problem to install or run Cosmonium, please don't hesitate to fill a bug report in the [issue tracker](https://github.com/cosmonium/cosmonium/issues) here on Github.\n\n## License \n\nCosmonium is (C) 2018-2022 Laurent Deru.\n\nThis program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 3 of the License, or (at your option) any later version.\n\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details, which you should have received along with this program. If not, request a copy from: Free Software Foundation, Inc. 59 Temple Place - Suite 330 Boston, MA 02111-1307 USA.\n\nCosmonium uses several third-party libraries which are subject to their own licenses,  see [Third-Party.md](Third-Party.md) for the complete list.\n\nCosmonium data (textures, models, orbital elements,..) come from many sources. Their respective copyright holder, license and reference are available in the info panel of the displayed object and in the related yaml file.\n\n## Powered by\n\n[![Python](https://github.com/cosmonium/cosmonium/wiki/images/python-powered-w-200x80.png)](http://www.python.org)\n\n[![Panda3D](https://github.com/cosmonium/cosmonium/wiki/images/panda3d_logo.png)](http://www.panda3d.org)\n",214,astronomy,Python,7,Python,GLSL,C++,C,Makefile,Shell,SWIG,,,,,,,,,,,,,,,,,,,,,,12,7,4,1,5,4,0,16656,17,25,23,2,53b8e2b131beaa8b8fec7ad7b62f1db09e268d01,ralph: Allow to create physics shape from config,2024-07-11T21:16:40Z,LD,laurent.deru@gmail.com,el-dee,Cosmonium V0.2.1.1,## Critical bugfix release for Windows 10 20H2\r\n\r\nThis release includes a workaround for a fatal bug in the Windows Runtime impacting Numpy (see #21 and https://tinyurl.com/y3dm3h86),v0.2.1.1,LD,,el-dee,GNU General Public License v3.0,cosmonium,cosmonium,9,astronomy,space,planetarium,procedural-generation,3d,panda3d,,,,,,,,,,,,,,,/cosmonium/cosmonium,9,11,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/coq-community/math-classes,https://github.com/coq-community/math-classes,0,,,0,1,0,0,0,0,1,0,0,0,0,"A library of abstract interfaces for mathematical structures in Coq [maintainer=@spitters,@Lysxia]","<!---\nThis file was generated from `meta.yml`, please do not edit manually.\nFollow the instructions on https://github.com/coq-community/templates to regenerate.\n--->\n# Math Classes\n\n[![Docker CI][docker-action-shield]][docker-action-link]\n[![Contributing][contributing-shield]][contributing-link]\n[![Code of Conduct][conduct-shield]][conduct-link]\n[![Zulip][zulip-shield]][zulip-link]\n[![DOI][doi-shield]][doi-link]\n\n[docker-action-shield]: https://github.com/coq-community/math-classes/actions/workflows/docker-action.yml/badge.svg?branch=master\n[docker-action-link]: https://github.com/coq-community/math-classes/actions/workflows/docker-action.yml\n\n[contributing-shield]: https://img.shields.io/badge/contributions-welcome-%23f7931e.svg\n[contributing-link]: https://github.com/coq-community/manifesto/blob/master/CONTRIBUTING.md\n\n[conduct-shield]: https://img.shields.io/badge/%E2%9D%A4-code%20of%20conduct-%23f15a24.svg\n[conduct-link]: https://github.com/coq-community/manifesto/blob/master/CODE_OF_CONDUCT.md\n\n[zulip-shield]: https://img.shields.io/badge/chat-on%20zulip-%23c1272d.svg\n[zulip-link]: https://coq.zulipchat.com/#narrow/stream/237663-coq-community-devs.20.26.20users\n\n\n[doi-shield]: https://zenodo.org/badge/DOI/10.1017/S0960129511000119.svg\n[doi-link]: https://doi.org/10.1017/S0960129511000119\n\nMath classes is a library of abstract interfaces for mathematical\nstructures, such as:\n\n*  Algebraic hierarchy (groups, rings, fields, …)\n*  Relations, orders, …\n*  Categories, functors, universal algebra, …\n*  Numbers: N, Z, Q, …\n*  Operations, (shift, power, abs, …)\n\nIt is heavily based on Coq’s new type classes in order to provide:\nstructure inference, multiple inheritance/sharing, convenient\nalgebraic manipulation (e.g. rewriting) and idiomatic use of\nnotations.\n\n\n## Meta\n\n- Author(s):\n  - Eelis van der Weegen (initial)\n  - Bas Spitters (initial)\n  - Robbert Krebbers (initial)\n- Coq-community maintainer(s):\n  - Bas Spitters ([**@spitters**](https://github.com/spitters))\n- License: [MIT License](LICENSE)\n- Compatible Coq versions: Coq 8.18 or later (use releases for other Coq versions)\n- Additional dependencies:\n  - [BigNums](https://github.com/coq/bignums)\n- Coq namespace: `MathClasses`\n- Related publication(s):\n  - [Type Classes for Mathematics in Type Theory](https://arxiv.org/abs/1102.1323) doi:[10.1017/S0960129511000119](https://doi.org/10.1017/S0960129511000119)\n\n## Building and installation instructions\n\nThe easiest way to install the latest released version of Math Classes\nis via [OPAM](https://opam.ocaml.org/doc/Install.html):\n\n```shell\nopam repo add coq-released https://coq.inria.fr/opam/released\nopam install coq-math-classes\n```\n\nTo instead build and install manually, do:\n\n``` shell\ngit clone https://github.com/coq-community/math-classes.git\ncd math-classes\n./configure.sh\nmake   # or make -j <number-of-cores-on-your-machine>\nmake install\n```\n\n\n## Directory structure\n\n### categories/\nProofs that certain structures form categories.\n\n### functors/\n\n### interfaces/\nDefinitions of abstract interfaces/structures.\n\n### implementations/\nDefinitions of concrete data structures and algorithms, and proofs that they are instances of certain structures (i.e. implement certain interfaces).\n\n### misc/\nMiscellaneous things.\n\n### orders/\nTheory about orders on different structures.\n\n### quote/\nPrototype implementation of type class based quoting. To be integrated.\n\n### theory/\nProofs of properties of structures.\n\n### varieties/\nProofs that certain structures are varieties, and translation to/from type classes dedicated to these structures (defined in interfaces/).\n\nThe reason we treat categories and varieties differently from other structures\n(like groups and rings) is that they are like meta-interfaces whose implementations\nare not concrete data structures and algorithms but are themselves abstract structures.\n\nTo be able to distinguish the various arrows, we recommend using a variable width font.\n\n",158,mathematics,Coq,4,Python,Coq,Shell,Makefile,,,,,,,,,,,,,,,,,,,,,,,,,95,12,82,1,3,41,0,2985,42,33,23,10,2a8e12360cceee510f39e3ef4d0a7472d70fa684,Merge pull request #127 from coq-community/coq_18880,2024-04-11T13:10:33Z,Pierre Roux,pierre.roux@onera.fr,proux01,Math-Classes 8.19.0,## What's Changed\r\n* Adapt to https://github.com/coq/coq/pull/6134 by @proux01 in https://github.com/coq-community/math-classes/pull/118\r\n* Remove deprecated files in Coq.Arith by @Villetaneuse in https://github.com/coq-community/math-classes/pull/121\r\n* Fix for a new rapply tactic by @JasonGross in https://github.com/coq-community/math-classes/pull/78\r\n* Add Module instance for polynomials by @tymmym in https://github.com/coq-community/math-classes/pull/50\r\n* Adapt to Coq PR #17832: syntax of choice in rewstrategy expects arguments at atomic level by @herbelin in https://github.com/coq-community/math-classes/pull/122\r\n* Adapt to https://github.com/coq/coq/pull/18590 by @proux01 in https://github.com/coq-community/math-classes/pull/123\r\n* Remove the few instances of non-global hint declarations. by @ppedrot in https://github.com/coq-community/math-classes/pull/126\r\n* Adapt to https://github.com/coq/coq/pull/18880 by @proux01 in https://github.com/coq-community/math-classes/pull/127\r\n\r\n**Full Changelog**: https://github.com/coq-community/math-classes/compare/8.18.0...8.19.0,8.19.0,Karl Palmskog,,palmskog,MIT License,math-classes,coq-community,11,coq-library,typeclasses,mathematics,coq,coq-ci,coq-platform,,,,,,,,,,,,,,,/coq-community/math-classes,11,16,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/ComputationalRadiationPhysics/picongpu,https://github.com/ComputationalRadiationPhysics/picongpu,1,,,1,1,1,1,0,0,0,0,0,0,1,Performance-Portable Particle-in-Cell Simulations for the Exascale Era :sparkles:,"PIConGPU - Particle-in-Cell Simulations for the Exascale Era\n============================================================\n\n[![Code Status dev](https://gitlab.com/hzdr/crp/picongpu/badges/dev/pipeline.svg?key_text=dev)](https://gitlab.com/hzdr/crp/picongpu/pipelines/dev/latest)\n[![Documentation Status](https://readthedocs.org/projects/picongpu/badge/?version=latest)](http://picongpu.readthedocs.io)\n[![Doxygen](https://img.shields.io/badge/API-Doxygen-blue.svg)](http://computationalradiationphysics.github.io/picongpu)\n[![Language](https://img.shields.io/badge/language-C%2B%2B17-orange.svg)](https://isocpp.org/)\n[![License PIConGPU](https://img.shields.io/badge/license-GPLv3-blue.svg?label=PIConGPU)](https://www.gnu.org/licenses/gpl-3.0.html)\n[![License PMacc](https://img.shields.io/badge/license-LGPLv3-blue.svg?label=PMacc)](https://www.gnu.org/licenses/lgpl-3.0.html)\n\n[![PIConGPU Presentation Video](http://img.youtube.com/vi/nwZuG-XtUDE/0.jpg)](http://www.youtube.com/watch?v=nwZuG-XtUDE)\n[![PIConGPU Release](docs/logo/pic_logo_vert_158x360.png)](http://www.youtube.com/watch?v=nwZuG-XtUDE)\n\nIntroduction\n------------\n\nPIConGPU is a fully relativistic,\n[manycore](https://en.wikipedia.org/wiki/Manycore_processor),\n3D3V particle-in-cell ([PIC](http://en.wikipedia.org/wiki/Particle-in-cell))\ncode. The Particle-in-Cell algorithm is a central tool in plasma physics.\nIt describes the dynamics of a plasma by computing the motion of\nelectrons and ions in the plasma based on\n[Maxwell's equations](http://en.wikipedia.org/wiki/Maxwell%27s_equations).\n\nPIConGPU implements various numerical schemes to solve the PIC cycle.\nIts features for the electro-magnetic PIC algorithm include:\n- a central or Yee-lattice for fields\n- particle pushers that solve the equation of motion for charged and neutral\n  particles, e.g., the *Boris-* and the\n  [*Vay-Pusher*](http://dx.doi.org/10.1063/1.2837054)\n- Maxwell field solvers, e.g.\n  [*Yee's*](http://dx.doi.org/10.1109/TAP.1966.1138693) and\n  [*Lehe's*](http://dx.doi.org/10.1103/PhysRevSTAB.16.021301) scheme\n- rigorously charge conserving current deposition schemes, such as\n  [*Esirkepov*](http://dx.doi.org/10.1016/S0010-4655%2800%2900228-9)\n  and *EZ* (Esirkepov meets ZigZag)\n- macro-particle form factors ranging from NGP (0th order), CIC (1st),\n  TSC (2nd), PQS (3rd) to PCS (4th)\n\nand the electro-magnetic PIC algorithm is further self-consistently coupled to:\n- classical radiation reaction\n  ([DOI:10.1016/j.cpc.2016.04.002](http://dx.doi.org/10.1016/j.cpc.2016.04.002))\n- advanced field ionization methods\n  ([DOI:10.1103/PhysRevA.59.569](http://dx.doi.org/10.1103/PhysRevA.59.569),\n   [LV Keldysh](http://www.jetp.ac.ru/cgi-bin/dn/e_020_05_1307.pdf), BSI)\n\nBesides the electro-magnetic PIC algorithm and extensions to it, we developed\na wide range of tools and diagnostics, e.g.:\n- online, far-field radiation diagnostics for coherent and incoherent radiation\n  emitted by charged particles\n- full restart and output capabilities via [openPMD](http://openPMD.org),\n  including [parallel HDF5](http://hdfgroup.org/)\n- 2D and 3D live view and diagnostics tools\n- a large selection of extensible\n  [online-plugins](http://picongpu.readthedocs.io/en/latest/usage/plugins.html)\n\nAs one of our supported compute platforms, GPUs provide a computational\nperformance of several\n[TFLOP/s](http://en.wikipedia.org/wiki/FLOPS) at considerable lower invest and\nmaintenance costs compared to multi CPU-based compute architectures of similar\nperformance. The latest high-performance systems\n([TOP500](http://www.top500.org/)) are enhanced by accelerator hardware that\nboost their peak performance up to the multi-PFLOP/s level. With its\noutstanding performance and scalability to more than 18'000 GPUs,\nPIConGPU was one of the **finalists** of the 2013\n[Gordon Bell Prize](http://sc13.supercomputing.org/content/acm-gordon-bell-prize).\n\nPIConGPU is developed and maintained by the\n[Computational Radiation Physics Group](https://www.hzdr.de/db/Cms?pNid=2097)\nat the [Institute for Radiation Physics](http://www.hzdr.de/db/Cms?pNid=132)\nat [HZDR](http://www.hzdr.de/) in close collaboration with the Center\nfor Information Services and High Performance Computing\n([ZIH](http://tu-dresden.de/die_tu_dresden/zentrale_einrichtungen/zih)) of the\nTechnical University Dresden ([TUD](http://www.tu-dresden.de)). We are a\nmember of the [Dresden GPU Center of Excellence](http://ccoe-dresden.de/) that\ncooperates on a broad range of scientific GPU and manycore applications,\nworkshops and teaching efforts.\n\nAttribution\n-----------\n\nPIConGPU is a *scientific project*. If you **present and/or publish** scientific\nresults that used PIConGPU, you should set a **reference** to show your support.\n\nOur according **up-to-date publication** at **the time of your publication**\nshould be inquired from:\n- [REFERENCE.md](https://raw.githubusercontent.com/ComputationalRadiationPhysics/picongpu/master/REFERENCE.md)\n\nPlease also consider adding yourself to our [community map](https://github.com/ComputationalRadiationPhysics/picongpu-communitymap).\nWe would love to hear from you!\n\nOral Presentations\n------------------\n\nThe following slide should be part of **oral presentations**. It is intended to\nacknowledge the team maintaining PIConGPU and to support our community:\n\n(*coming soon*) presentation_picongpu.pdf\n(svg version, key note version, png version: 1920x1080 and 1024x768)\n\nSoftware License\n----------------\n\n*PIConGPU* is licensed under the **GPLv3+**. Furthermore, you can develop your\nown particle-mesh algorithms based on our general library *PMacc* that is\nshipped alongside PIConGPU. *PMacc* is *dual licensed* under both the\n**GPLv3+ and LGPLv3+**.\nFor a detailed description, please refer to [LICENSE.md](LICENSE.md)\n\n********************************************************************************\n\nInstall\n-------\n\nSee our notes in [INSTALL.rst](INSTALL.rst).\n\nUsers\n-----\n\nDear User, we hereby emphasize that we are still actively developing PIConGPU at great\nspeed and do, from time to time, break backwards compatibility.\n\nWhen using this software, please stick to the latest release or use the `dev` branch containing the\nlatest changes. It also contains a file `CHANGELOG.md` with the\nlatest changes (and how to update your simulations). Read it first before\nupdating between two versions! Also, we add a git `tag` according to a version\nnumber for each release.\n\nFor any questions regarding the usage of PIConGPU please **do not** contact the\ndevelopers and maintainers directly.\n\nInstead, please [open an issue on GitHub](https://github.com/ComputationalRadiationPhysics/picongpu/issues/new).\n\nBefore you post a question, browse the PIConGPU\n[documentation](https://github.com/ComputationalRadiationPhysics/picongpu/search?l=markdown),\n[wiki](https://github.com/ComputationalRadiationPhysics/picongpu/wiki) and the\n[issue tracker](https://github.com/ComputationalRadiationPhysics/picongpu/issues)\nto see if your question has been answered, already.\n\nPIConGPU is a collaborative project.\nWe thus encourage users to engage in answering questions of other users and post solutions to problems to the list.\nA problem you have encountered might be the future problem of another user.\n\nIn addition, please consider using the collaborative features of GitHub if you have questions or comments on code or documentation.\nThis will allow other users to see the piece of code or documentation you are referring to.\n\nMain ressources are in our [online manual](https://picongpu.readthedocs.io), the [user section](https://github.com/ComputationalRadiationPhysics/picongpu/wiki) of our wiki, documentation files in [`.md` (Markdown)](http://commonmark.org/help/) and [`.rst` (reStructuredText)](http://www.sphinx-doc.org/en/stable/rest.html) format in this repository and a [getting started video](http://www.youtube.com/watch?v=7ybsD8G4Rsk).\nFeel free to visit [picongpu.hzdr.de](http://picongpu.hzdr.de) to learn more about the PIC algorithm.\n\nSoftware Upgrades\n-----------------\n\nPIConGPU ships new and frequent changes to the code in the development branch `dev`.\n\nFrom time to time we publish a new release\nof PIConGPU. Before you pull the changes in, please read our\n[ChangeLog](CHANGELOG.md)!\nYou may have to update some of your simulation `.param` and `.cfg` files by\nhand since PIConGPU is an active project and new features often require changes\nin input files. Additionally, a full description of new features and fixed bugs\nin comparison to the previous release is provided in that file.\n\nIn case you decide to use *new, potentially buggy and experimental* features\nfrom our `dev` branch, be aware that you must participate or at least follow the development yourself. \nSyntax changes and in-development bugs will *not* be announced outside of their according pull\nrequests and issues.\n\nBefore drafting a new release, we open a new `release-*` branch from `dev` with\nthe `*` being the version number of the upcoming release. This branch only\nreceives bug fixes (feature freeze) and users are welcome to try it out\n(however, the change log and a detailed announcement might still be missing in\nit).\n\nDevelopers\n----------\n\n### How to participate\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md)\n\nIf you like to jump in right away, see  \n[![open ""good first issue"" issues](https://img.shields.io/github/issues-raw/ComputationalRadiationPhysics/picongpu/good%20first%20issue.svg?color=56cbef)](https://github.com/ComputationalRadiationPhysics/picongpu/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n\nActive Team\n-----------\n\n### Scientific Supervision\n\n- Dr. Michael Bussmann\n\n### Maintainers* and core developers\n\n- Dr. Sergei Bastrakov*\n- Finn-Ole Carstens\n- Dr. Alexander Debus\n- Dr. Marco Garten*\n- Dr. Axel Huebl*\n- Brian Edward Marre\n- Pawel Ordyna \n- Dr. Richard Pausch*\n- Franz Poeschel\n- Dr. Klaus Steiniger*\n- Rene Widera*\n\n### Former Members, Contributions and Thanks\n\nThe PIConGPU Team expresses its gratitude to:\n\nFlorian Berninger, Heiko Burau, Fabia Dietrich, Robert Dietrich, Carlchristian Eckert,\nSimeon Ehrig, Wen Fu, Ph.D., Alexander Grund, Sebastian Hahn, Anton Helm, Wolfgang Hoehnig,\nDr.-Ing. Guido Juckeland, Jeffrey Kelling, Maximilian Knespel, Dr. Remi Lehe,\nFelix Schmitt, Frank Winkler, Benjamin Schneider, Joseph Schuchart, Conrad Schumann,\nStefan Tietze, Marija Vranic, Ph.D., Benjamin Worpitz, Erik Zenker,\nSophie Rudat, Sebastian Starke, Alexander Matthes, Kseniia Bastrakova, \nBernhard Manfred Gruber, Jakob Trojok, Anton Lebedev, Nils Prinz,\nFelix Meyer, Lennert Sprenger, Manhui Wang, Maxence Thevenet, Ilja Goethel,\nMika Soren Voß, Lei Bifeng, Andrei Berceanu, Felix Meyer,\nLennert Sprenger and Nico Wrobel.\n\nKudos to everyone, mentioned or unmentioned, who contributed further in any\nway!\n\n********************************************************************************\n\n![image of an lwfa](docs/images/lwfa_iso.png ""LWFA"")\n![image of our strong scaling](docs/images/StrongScalingPIConGPU_log.png ""Strong Scaling"")\n",689,physics,C++,8,Shell,C++,Python,Awk,Gnuplot,CMake,Jupyter Notebook,Mustache,,,,,,,,,,,,,,,,,,,,,3535,203,3313,19,3,65,6,51853,216,1462,1065,397,c16e76a00dc36fe413dbbaae7d8611a5c732169d,Merge pull request #4991 from BrianMarre/topic-switchToNewSchemaResolver,2024-07-17T15:07:35Z,Tapish Narwal,10693329+ikbuibui@users.noreply.github.com,ikbuibui,"C++17, lasers via incidentField, DispersivePulse laser profile, sub-stepping field solver, and PICMI support","With this release, laser initialization switches to an incidentField approach, which allows easier initialization of arbitrary Maxwell-compliant electromagnetic fields in a simulation. Old laser.param files won't work anymore. See the documentation! This update of the laser implementation was also used to add a new profile 'DispersivePulse' and to rename the existing laser profiles. The new profile allows the initialization of Gaussian laser pulses with arbitrary first, second and third order dispersion, while the renaming aims at a more precise expression of the laser profile.\r\n\r\nBasic [PICMI](https://picmi-standard.github.io/#) support is also added to PIConGPU, as well as time substepping for field solvers and improved handling of currents in PML boundaries. Again, see the documentation for details.\r\n\r\nMajor code refactorings also come with this release. These include a cleanup of the code base, in particular removal of unused plugins and CuSTL, performance improvements, and a switch to C++17 as the minimum required version.\r\n\r\nAs usual, this release contains fixes to PIConGPU and enhancements and clarifications to the documentation.\r\n\r\nThanks to Sergei Bastrakov, Finn-Ole Carstens, Alexander Debus, Fabia Dietrich, Simeon Ehrig, Marco Garten, Bernhard Manfred Gruber, Axel Huebl, Jeffrey Kelling, Anton Lebedev, Brian Edward Marre, Paweł Ordyna, Richard Pausch, Franz Pöschel, Klaus Steiniger, Jan Stephan, Hannes Tröpgen, Mika Soren Voß, and René Widera for their contributions to this release!",0.7.0,René Widera,,psychocoderHPC,Other,picongpu,ComputationalRadiationPhysics,26,laser,plasma,physics,gpu,physics-simulation,gpu-computing,particle-accelerator,particle-in-cell,pic,research,,,,,,,,,,,/ComputationalRadiationPhysics/picongpu,31,52,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/compas-dev/compas,https://github.com/compas-dev/compas,0,,0,0,0,0,0,0,0,1,0,0,0,0,Core packages of the COMPAS framework.,"# The COMPAS framework\n\n![build](https://github.com/compas-dev/compas/workflows/build/badge.svg)\n[![codecov](https://codecov.io/github/compas-dev/compas/graph/badge.svg?token=wpkfew9szQ)](https://codecov.io/github/compas-dev/compas)\n[![GitHub - License](https://img.shields.io/github/license/compas-dev/compas.svg)](https://github.com/compas-dev/compas)\n[![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/compas)](https://anaconda.org/conda-forge/compas)\n[![pip downloads](https://img.shields.io/pypi/dm/compas)](https://pypi.python.org/project/COMPAS)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/COMPAS.svg)](https://pypi.python.org/project/COMPAS)\n[![PyPI - Latest Release](https://img.shields.io/pypi/v/COMPAS.svg)](https://pypi.python.org/project/COMPAS)\n[![Conda (channel only)](https://img.shields.io/conda/vn/conda-forge/compas)](https://anaconda.org/conda-forge/compas)\n[![Conda - Platform](https://img.shields.io/conda/pn/conda-forge/compas)](https://anaconda.org/conda-forge/compas)\n[![DOI](https://zenodo.org/badge/104857648.svg)](https://zenodo.org/badge/latestdoi/104857648)\n[![Twitter Follow](https://img.shields.io/twitter/follow/compas_dev?style=social)](https://twitter.com/compas_dev)\n\nThe **COMPAS** framework is an open-source, Python-based framework for computational research and collaboration in architecture, engineering, digital fabrication and construction.\n\nThe framework consists of a general-purpose core library, written in pure Python, and a growing collection of extensions that provide easy access to peer-reviewed research, state-of-the-art external libraries such as CGAL, libigl and Triangle, and tools with specialized functionality for AEFC applications such as Abaqus, ANSYS, SOFISTIK, ROS, etc.\n\nCOMPAS has dedicated packages for working with Rhino, Grasshopper, and Blender, but it can be used in any environment that supports Python scripting. It is available on PyPI and conda-forge and can be easily installed using popular package managers on multiple platforms.\n\n## COMPAS 2.0!\n\nWe are working on a new major release of the framework, COMPAS 2.0!\nTherefore, be aware that the current version of the repository already contains some changes that are incompatible with the version 1 releases.\nThe documentation of the latest stable version (COMPAS 1.17.9) [is available here](https://compas.dev/compas/1.17.9).\n\n## Installation\n\nThe recommended way to install **COMPAS** is to use [Anaconda/conda](https://conda.io/docs/):\n\n```bash\nconda config --add channels conda-forge\nconda install compas\n```\n\nFor other installation options, [see the user guide](https://compas.dev/compas/latest/userguide/installation.html)\n\n## First Steps\n\n* [First steps](https://compas.dev/compas/latest/userguide/firststeps.html)\n* [Tutorials: geometry basics](https://compas.dev/compas/latest/userguide/basics.geometry.html)\n* [Tutorials: datastructures](https://compas.dev/compas/latest/userguide/basics.datastructures.html)\n* [API Reference](https://compas.dev/compas/latest/api/index.html)\n\n## Questions and feedback\n\nThe **COMPAS** framework has a [forum for questions and discussions](https://forum.compas-framework.org/).\n\n## Issue tracker\n\nIf you find a bug, please help us solve it by [filing a report](https://github.com/compas-dev/compas/issues).\n\n## Contributing\n\nIf you want to contribute, check out the [contribution guidelines](https://compas.dev/compas/latest/devguide/index.html).\n\n## Changelog\n\nSee changes between releases on the [changelog](https://github.com/compas-dev/compas/blob/main/CHANGELOG.md).\n\n## License\n\nThe main library of **COMPAS** is [released under the MIT license](https://compas.dev/compas/latest/userguide/license.html).\n\n## Credits\n\nCOMPAS is developed by a small team of core developers (`compas-dev`) and with the support of contributers from the open source community.\nSee the [list of authors](https://github.com/compas-dev/compas/blob/main/AUTHORS.md) for a complete overview...\n",305,geometry,Python,1,Python,,,,,,,,,,,,,,,,,,,,,,,,,,,,759,92,660,7,85,50,1341,2384252,104,611,525,86,82af17eb81b9ce333854d585098f92ea96ecb01b,Merge pull request #1380 from compas-dev/yck011522/test_tol_format_nu…,2024-07-18T06:08:07Z,Victor LEUNG,yck011522@gmail.com,yck011522,COMPAS 2.3.0,"### Added\r\n\r\n* Added code coverage report uploads to codecov.io.\r\n* Added `compas.geometry.surfaces.surface.Surface.from_native`.\r\n* Added `compas.geometry.surfaces.nurbs.NurbsSurface.from_plane`.\r\n* Added `compas.geometry.surfaces.nurbs.NurbsSurface.from_cylinder`.\r\n* Added `compas.geometry.surfaces.nurbs.NurbsSurface.from_extrusion`.\r\n* Added `compas.geometry.surfaces.nurbs.NurbsSurface.from_frame`.\r\n* Added `compas.geometry.surfaces.nurbs.NurbsSurface.from_interpolation`.\r\n* Added `compas.geometry.surfaces.nurbs.NurbsSurface.from_revolution`.\r\n* Added `compas.geometry.surfaces.nurbs.NurbsSurface.from_sphere`.\r\n* Added `compas.geometry.surfaces.nurbs.NurbsSurface.from_torus`.\r\n* Added `compas_rhino.geometry.surfaces.surface_from_native`.\r\n* Added `compas_rhino.geometry.surfaces.nurbssurface_from_native`.\r\n* Added `compas_rhino.geometry.surfaces.nurbssurface_from_cylinder`.\r\n* Added `compas_rhino.geometry.surfaces.nurbssurface_from_fill`.\r\n* Added `compas_rhino.geometry.surfaces.nurbssurface_from_torus`.\r\n* Added `compas_rhino.geometry.surfaces.nurbs.NurbsSurface.from_corners`.\r\n* Added `compas_rhino.geometry.surfaces.nurbs.NurbsSurface.from_cylinder`.\r\n* Added `compas_rhino.geometry.surfaces.nurbs.NurbsSurface.from_frame`.\r\n* Added `compas_rhino.geometry.surfaces.nurbs.NurbsSurface.from_sphere`.\r\n* Added `compas_rhino.geometry.surfaces.nurbs.NurbsSurface.from_torus`.\r\n* Added `compas.geometry.curves.curve.Curve.from_native`.\r\n* Added `compas_rhino.geometry.curves.curve.Curve.from_native`.\r\n* Added `compas_rhino.geometry.curves.nurbs.NurbsCurve.from_native`.\r\n* Added `compas_rhino.conversions.breps.brep_to_compas_mesh`.\r\n* Added `compas_rhino.conversions.docobjects.brepobject_to_compas`.\r\n* Added `compas_rhino.conversions.docobjects.curveobject_to_compas`.\r\n* Added `compas_rhino.conversions.docobjects.meshobject_to_compas`.\r\n* Added `compas_rhino.conversions.docobjects.pointobject_to_compas`.\r\n* Added `compas.datastructures.HashTree` and `compas.datastructures.HashNode`.\r\n\r\n### Changed\r\n\r\n* Fixed bug in `compas.geometry.curves.curve.Curve.reversed` by adding missing parenthesis.\r\n* Fixed all doctests so we can run `invoke test --doctest`.\r\n* Changed `compas.geometry.surfaces.surface.Surface.__new__` to prevent instantiation of `Surface` directly.\r\n* Changed `compas.geometry.surfaces.nurbs.NurbsSurface.__new__` to prevent instantiation of `NurbsSurface` directly.\r\n* Fixed bug in `compas.geometry.surfaces.nurbs.NurbsSurface.__data__`.\r\n* Changed `compas.geometry.surfaces.nurbs.new_nurbssurface_from_...` to `nurbssurface_from_...`.\r\n* Changed `compas.geometry.curves.curve.Curve.__new__` to prevent instantiation of `Curve` directly.\r\n* Changed `compas.geometry.curves.nurbs.new_nurbscurve_from_...` to `nurbscurve_from_...`.\r\n* Changed `compas.geometry.curves.nurbs.NurbsCurve.__new__` to prevent instantiation of `NurbsCurve` directly.\r\n* Changed `compas_rhino.geometry.curves.new_nurbscurve_from_...` to `nurbscurve_from_...`.\r\n* Fixed `compas_ghpython` Grasshopper components not included in published pakcage.\r\n* Changed `compas.colors.Color.coerce` to take color as is, if it is already an instance of `compas.colors.Color`.\r\n* Changed `compas_rhino.conversions.surfaces.surface_to_compas` to work only with surface geometry.\r\n* Changed `compas_rhino.conversions.curves.curve_to_compas_line` to work only with geometry.\r\n* Changed `compas_rhino.conversions.curves.curve_to_compas_circle` to work only with geometry.\r\n* Changed `compas_rhino.conversions.curves.curve_to_compas_ellipse` to work only with geometry.\r\n* Changed `compas_rhino.conversions.curves.curve_to_compas_polyline` to work only with geometry.\r\n* Changed `compas_rhino.objects.get_point_coordinates` to deprecated (removed in v2.3).\r\n* Changed `compas_rhino.objects.get_line_coordinates` to deprecated (removed in v2.3).\r\n* Changed `compas_rhino.objects.get_polyline_coordinates` to deprecated (removed in v2.3).\r\n* Changed `compas_rhino.objects.get_polygon_coordinates` to deprecated (removed in v2.3).\r\n* Fixed a bug in `worldtransformation` of `compas.scene.SceneObject` to include the object's own frame.\r\n\r\n### Removed\r\n\r\n* Removed pluggable `compas.geometry.surfaces.surface.new_surface`.\r\n* Removed pluggable `compas.geometry.surfaces.surface.new_surface_from_plane`.\r\n* Removed `compas.geometry.surfaces.surface.Surface.from_plane`.\r\n* Removed `compas.geometry.surfaces.surface.ConicalSurface.__new__`.\r\n* Removed `compas.geometry.surfaces.surface.CylindricalSurface.__new__`.\r\n* Removed `compas.geometry.surfaces.surface.PlanarSurface.__new__`.\r\n* Removed `compas.geometry.surfaces.surface.SphericalSurface.__new__`.\r\n* Removed `compas.geometry.surfaces.surface.ToroidalSurface.__new__`.\r\n* Removed `compas.geometry.surfaces.nurbs.NurbsSurface.__init__`.\r\n* Removed `compas_rhino.geometry.surfaces.new_surface`.\r\n* Removed `compas_rhino.geometry.surfaces.new_nurbssurface`.\r\n* Removed `compas_rhino.geometry.surfaces.nurbs.NurbsSurface.__from_data__`.\r\n* Removed `compas_rhino.geometry.surfaces.surface.Surface.from_corners`.\r\n* Removed `compas_rhino.geometry.surfaces.surface.Surface.from_cylinder`.\r\n* Removed `compas_rhino.geometry.surfaces.surface.Surface.from_frame`.\r\n* Removed `compas_rhino.geometry.surfaces.surface.Surface.from_sphere`.\r\n* Removed `compas_rhino.geometry.surfaces.surface.Surface.from_torus`.\r\n* Removed `compas.geometry.curves.arc.Arc.__new__`.\r\n* Removed `compas.geometry.curves.bezier.Bezier.__new__`.\r\n* Removed `compas.geometry.curves.conic.Conic.__new__`.\r\n* Removed `compas.geometry.curves.polyline.Polyline.__new__`.\r\n* Removed `compas.geometry.curves.curve.new_curve`.\r\n* Removed `compas.geometry.curves.curve.new_nurbscurve`.\r\n* Removed `compas_rhino.geometry.curves.new_curve`.\r\n* Removed `compas_rhino.geometry.curves.new_nurbscurve`.\r\n* Removed `compas_rhino.conversions.surfaces.data_to_rhino_surface`.\r\n* Removed `compas_rhino.conversions.surfaces.surface_to_compas_data`.\r\n* Removed `compas_rhino.conversions.surfaces.surface_to_compas_quadmesh`.\r\n* Removed `compas_rhino.conversions.curves.curve_to_compas_data`.",v2.3.0,,,github-actions[bot],MIT License,compas,compas-dev,76,geometry,datastructures,data,grasshopper3d,rhino3d,blender3d,aec,rpc,,,,,,,,,,,,,/compas-dev/compas,185,25,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/CommonWealthRobotics/BowlerStudio,https://github.com/CommonWealthRobotics/BowlerStudio,0.5,Is it science?,0,0,0,1,0,0,0,0,0,0,0,0,A Full-Stack Robotics Development Environment,"BowlerStudio\n==========\n\n[![Join the chat at https://gitter.im/CommonWealthRobotics/BowlerStudio](https://badges.gitter.im/CommonWealthRobotics/BowlerStudio.svg)](https://gitter.im/CommonWealthRobotics/BowlerStudioDevelopment?utm_source=share-link&utm_medium=link&utm_campaign=share-link)\n[![Test Build](https://github.com/CommonWealthRobotics/BowlerStudio/actions/workflows/verify.yml/badge.svg)](https://github.com/CommonWealthRobotics/BowlerStudio/actions/workflows/verify.yml)\n[![Github All Releases](https://img.shields.io/github/downloads/CommonWealthRobotics/BowlerStudio/total.svg)](https://github.com/CommonWealthRobotics/BowlerStudio/releases)\n\n\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/2904/badge)](https://bestpractices.coreinfrastructure.org/projects/2904)\n\n# [Download Latest](https://commonwealthrobotics.com/#downloads)\n\n							\n\n# What is BowlerStudio?\n\nBowlerStudio assists you in every step of a robotics project from concept to completion. Tools enable users to:\n* Interface with motors, sensors, and other electronics hardware.\n* Create 3d models for fabrication, and for simulating the motions of your project.\n* Give your robot sight with image processing on camera feeds and Kinect data.\n* Operate 3d printers and other CNC machines.\n* Create custom graphical user interfaces to control yours robots.\n* Create and control animations.\n\n==========\n\n## The Nitty-Gritty Version\n\nBowlerStudio Robotics development IDE is based on\n* [JCSG](https://github.com/miho/JCSG)\n* [Java-Bowler](https://github.com/NeuronRobotics/java-bowler)\n* [Jinput](https://github.com/jinput/jinput)\n* [motej](http://motej.sourceforge.net/)\n* [Usb4Java](https://github.com/usb4java/usb4java)\n* [NrJavaSerial](https://github.com/NeuronRobotics/nrjavaserial)\n* [BlueCove](https://github.com/hcarver/bluecove)\n* JavaFX 8 3d engine. \n* [JBullet](http://jbullet.advel.cz/) physics engine ported from the popular Bullet C++ framework.\n* [Jetty](http://www.eclipse.org/jetty/) Web Framework\n\nBowlerStudio is a device manager, scripting engine, CAD package, and simulation tool all in one application. A user can develop the kinematic of an robot arm using the D-H parameters-based automatic kinematics engine. With this kinematics model, the user can then generate the CAD for new unique parts to match the kinematic model. The user can then export the model to an STL, and connect a Bowler 3d printer to BowlerStudio. The printer can print out the part (using the newly generated STL) while the user connects a DyIO and begins testing the servos with the kinematics model. When the print is done, the user can assemble the arm with the tested servos and run the model again to control the arm with Cartesian instructions. Once this is complete, the user can then attach a wiimote to train the robot arm through a set of tasks, recording them with the animation framework built into BowlerStudio. To be sure the arm is moving to the right place, the user can attach a webcam to the end and use OpenCV to verify the arm's position, or use the arm (in conjunction with the webcam with OpenCV enabled) to track and grab objects (IE ""eye-in-hand"" tracking). \n\nEvery step of this task can be performed from within BowlerStudio!\n\nLet's go through the main features:\n\n# Scripting With Gist\n### About scripts and Gist\n   Scripts are bits of code that BowlerStudio can load and run. BowlerStudio allows you to open a local file and run it, but BowlerStudio is most powerful when the code lives on Github Gist (a code snippet hosting service from Github). Simply give BowlerStudio the URL for a Gist you want to load and execute. Gists can be selected and edited using the built in browser, or inline in another script using the Gist ID.   \n### Java and Groovy\n   BowlerStudio can load and run scripts written in Java, Groovy, and Python. Which parser is used is determined by the file extension. Files that end in .java or .groovy will be run through the Groovy compiler. These Groovy scripts are compiled fully and run directly in the JVM. This means they will execute at full speed, just like a regular application.  \n   \n### Python\n   Python, on the other hand, by virtue of its structure, will generally execute much slower then Java. With the reduction in speed you get lots of flexibility and a clean and easy to understand syntax. The python code can also create and return objects to BowlerStudio (such as CAD CSG objects, or UI Tabs).  \n\n### Return Objects\n   A script can return a few object types that will be handled by BowlerStudio:\n   Objects of type ""CSG"" and ""MeshView"" will be added to the 3d display. If a transform is added to either of these and updated by a script the user can move an object in the 3d view. \n   Objects of type ""Tab"" will be added to the Tabmanager and displayed in BowlerStudio. This is an easy way to make control panels or state displays and monitors. \n   Objects of type ""BowlerAbstractDevice"" (or any subclass) will be added to the connections manager and made available to all other scripts. These can be external devices or virtual communication bus devices. A bowler Server/Client pair is the preferred mechanism for communication between scripts. \n### Device Access\n   All scripts are passed all connected devices by name when the script is run. The name associated with the device in the connections tab is the name to use in the script to access that device. A script can also create and return a device (EG to connect to a specific device in order to give that device a specific name). The device returned will be added to the list of available devices and be available to other scripts. A user can define their own devices to facilitate communication between scripts. \n   \n# Bowler Devices\n   BowlerDevices (such as the Neuron Robotics DyIO) are devices that implement the Bowler Communication System. BowlerDevices are servers of features to applications. The DyIO, for example, is a server of microcontroller features. These devices implement a micro domain-specific language as a protocol. This language synchronizes the device with the application by building the communication system at runtime using a namespace/RPC system. \n   As such, the device is treated as a collection of namespaces. Each namespace has a set of RPCs for communication, some synchronous (IE they are application initiated) and some asynchronous (IE device initiated). In addition, each RPC has full method introspection. This means that all parameters and datatypes (including how to pack and interpret all packets) are able to be queried over the communication system. A Library need only implement the core packet parser and every device will assemble its own communication layer live.  \n\n# Cameras\n   Cameras can be connected to Bowler Studio using one of 3 supported drivers:\n   OpenCV's native Java bindings are provided by installing OpenCV using your OS specific installer (unfortunately not available for Mac at this time). \n   JavaCV is a meta-library that adds support for a wide range of camera device integrations and image processing options. This is a big project and integrated now with a full scripting system. \n   CHDK-PTP-Java is a Java library that adds support for SLR Cannon cameras. CHDK is a camera OS that makes the cameras features available over USB, and the Java library makes those images and controls available to Java and or scripting engine. \n\n# Image processing\n   Image processing is provided be a variety of libraries included in this application. OpenCV and ARToolkit are some of the most widely used image recognition libraries available, and now you can use them directly from our scripting environment! \n   \n# Kinematics Engine\n   The Bowler Kinematics engine is based on [D-H parameters](https://www.youtube.com/embed/rA9tm0gTln8)- the standard mathematical definition of kinematics chains. This standard simplifies the calculation process and allows us to run forward kinematics equations for arbitrary defined chains in real time. For inverse kinematics, a collection of kinematic engines are available for optimizing for speed and accuracy. \n\n# Real-Time validated\n   For applications where real time is required, there is no need to leave the Bowler OS. The Bowler Java stack has been validated as real-time capable when run on JamaicaVM (the real-time Java implementation). The Bowler Kinematics engine is run in a real-time loop for neurosurgery applications (Bowler and Java are fast and reliable enough for brain surgery!)\n   \n# 3D CAD\n   Users can write scripts using Java, Groovy or Python to generate CSG style CAD. This programmatic CAD engine JCSG was inspired by OpenSCAD, but implemented in pure Java with JavaFX visualizations. JCSG implements all basic shape generation and manipulation, using Java's library packaging and distribution for libraries of parts. Gist hosting of parts can also simplify sharing and loading of dependent libraries. \n   \n# Virtualization\n### Virtual links\n   Virtual PID devices allow users to make applications that can be simulated with virtual links before ever connecting a real device. \n   Users can also use the PIDLab to learn about designing and implementing PID controllers with a built in motor physics simulation. \n### Virtual Camera\n   Coming soon! Soon users will be able to interact with the 3D environment camera from their code just like a real camera. Users will be able to manipulate it just like it was another 3d object. \n   \n### Virtual Sensors\n   Coming soon! Soon we will be able to provide virtual sensor devices, simulating real world sensors within the 3d environment. \n\n# Make A Contribution\nBowlerStudio is an open source project and is always looking for help with both the application code and the tutorials content. \n\n### Java Contributions\n\nIf you are a Java developmer, skip ahead to [The Build Instructions](#command-line). The application is a light plugin framework for UI, 3D and Device interaction. You can look at this repository for issues. \n\n### Adding Tutorials\n\nAll of the content for BowlerStudio Tutorials is housed on our [Neuronrobotics.github.io](https://github.com/CommonWealthRobotics/CommonWealthRobotics.github.io) web page. Fork that repository and make contributions based on the README.md file in the root of the repository. To merge the changes into the main website, send a pull request with your changes to official repository. \n\nExamples of tutorials that need to be added are [A simple Java Programming Introduction](https://github.com/CommonWealthRobotics/CommonWealthRobotics.github.io/issues/59). This tutorial set would go through the basic syntax of java and what all of the symbols mean and how to use them. \n\nAnother example of a tutorial that could be added is one for [JavaCad Cheatsheet](https://github.com/CommonWealthRobotics/CommonWealthRobotics.github.io/issues/58) where you would add a 'cheat sheet' of commands to use in the JavaCad system. \n\nIf a tutorial is missing and not described as needed by an issue, feel free to add additional issues. \n\n## How to Build BowlerStudio\n\n### Requirements\n\n- Internet connection (dependencies are downloaded automatically)\n- IDE: [Gradle](http://www.gradle.org/) Plugin (not necessary for command line usage)\n- The Binary installer for BowlerStudio must be installed before develpment. \n\n### IDE\n\nOpen the `BowlerStudio` [Gradle](http://www.gradle.org/) project in your favorite IDE (tested with NetBeans 7.4) and build \nby calling the `assemble` task.\n\n#### Windows Setup Instructions\n\n- Download and install Sloeber Eclipse or an alternative Java IDE of your choice. \n- Install BowlerStudio via the provided installer. This will ensure that the correct JVM is accessible. \n- Clone this repository via HTTPS and use your [GitHub personal access token](https://github.com/settings/tokens).\n  - Make sure to enable the option to clone submodules. \n- Set the default JVM in Eclipse to the following file path: `C:\Program Files (x86)\Commonwealth Robotics BowlerStudio\BowlerStudioApp\jre`. (Window > Preferences > Java > Installed JREs)\n- In the Eclipse gradle configuration, set the `javahome` path to the following file path: `C:\Program Files (x86)\Commonwealth Robotics BowlerStudio\BowlerStudioApp\jre`. \n- Import the cloned project into Eclipse using gradle.\n\n\n\n### Command Line\nNavigate to the [Gradle](http://www.gradle.org/) project (e.g., `path/to/BowlerStudio`) and enter the following command:\n\n#### Bash (Linux/OS X/Cygwin/other Unix-like shell)\n##### Ubuntu 18.04 Dependencies\n\n```\nsudo apt install  curl\n```\n\n##### Ubuntu 16.04 Dependencies\n```   \n    sudo apt-get install git gradle\n```\n##### Ubuntu 14.04, install extra dependencies\n```\n	sudo add-apt-repository ""deb http://ppa.launchpad.net/mad-hephaestus/commonwealthrobotics/ubuntu xenial main"" -y\n	sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 71EA898B \n	sudo add-apt-repository ""deb http://us.archive.ubuntu.com/ubuntu/ trusty universe multiverse""\n	sudo apt-get update -qq\n	sudo apt-get install -y --force-yes gradle \n```\n##### All Unix  \n```  \n    bowlerstudio # run this to get the JVM downloaded into $HOME/bin/java8/jre\n\n    git clone https://github.com/CommonWealthRobotics/BowlerStudio.git\n    \n    JAVA_HOME=$HOME/bin/java8/jre\n\n    export JAVA_HOME\n   \n    cd BowlerStudio\n    \n    git submodule update --init --recursive\n    \n    git submodule update  --recursive\n    \n    ./gradlew shadowJar\n    \n    $JAVA_HOME/bin/java -jar build/libs/BowlerStudio*.jar\n```\nNow you can use the Eclipse Marketplace to install the Gradle Plugin\n    \n#### Windows (CMD)\n\n    gradlew jar\n    \n# History\n\nBowler Studio began [Feb 11, 2015](https://github.com/CommonWealthRobotics/BowlerStudio/releases/tag/0.0.1)  with the goal of making a robotics IDE. \n\n",131,physics,Java,2,Java,Shell,,,,,,,,,,,,,,,,,,,,,,,,,,,61,2,57,2,25,15,0,182369,29,353,240,113,12543aefa14e7b4484481b61f10a49aada977775,removing the internal jar deps from the kernel,2024-07-15T19:41:36Z,Kevin Harrington,kharrington@bancroftschool.org,,,,2.35.5,,,github-actions[bot],GNU Lesser General Public License v3.0,BowlerStudio,CommonWealthRobotics,585,kinematics-engine,kinematics-model,cad,robotics,scripting-engine,robot-arm,physics-simulation,physics,jbullet,javafx-application,javafx-3d,groovy-language,clojure,python,chdk,java-8,bowler,java,java-library,java-bowler,/CommonWealthRobotics/BowlerStudio,771,11,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/COMBINE-lab/salmon,https://github.com/COMBINE-lab/salmon,1,,,1,1,1,1,0,0,0,0,0,0,1,🐟 🍣 🍱 Highly-accurate & wicked fast transcript-level quantification from RNA-seq reads using selective alignment,"<img alt=""salmon logo"" src=""https://github.com/COMBINE-lab/salmon/raw/master/doc/salmon_logo.png"" width=""600"">\n\n[![Documentation Status](https://readthedocs.org/projects/salmon/badge/?version=latest)](http://salmon.readthedocs.org/en/latest)\n[![install with bioconda](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?style=flat-square)](http://bioconda.github.io/recipes/salmon/README.html)\n![GitHub tag (latest SemVer)](https://img.shields.io/github/v/tag/combine-lab/salmon?style=flat-square)\n\n\n**Try out the new [alevin-fry](https://alevin-fry.readthedocs.io/en/latest/) framework for single-cell analysis; tutorials can be found [here](https://combine-lab.github.io/alevin-fry-tutorials/)!**\n\n**Help guide the development of Salmon, [take our survey](https://docs.google.com/forms/d/e/1FAIpQLSeWhBNE_fA_0uVHvbAlAulDmfmowv7rAYla879DZpqCARyRTQ/viewform)**\n\n\nWhat is Salmon?\n===============\n\nSalmon is a **wicked**-fast program to produce a highly-accurate, transcript-level quantification estimates from \nRNA-seq data.  Salmon achieves its accuracy and speed via a number of different innovations, including the \nuse of *selective-alignment* (accurate but fast-to-compute proxies for traditional read alignments), and \nmassively-parallel stochastic collapsed variational inference.  The result is a versatile tool that fits nicely\ninto many different pipelines.  For example, you can choose to make use of our *selective-alignment* algorithm by providing Salmon with raw sequencing reads, or, if it is more convenient, you can provide Salmon with regular alignments (e.g. an **unsorted** BAM file with alignments to the transcriptome produced with your favorite aligner), and it will use the same **wicked**-fast, state-of-the-art inference algorithm to estimate transcript-level abundances for your experiment.\n\nGive salmon a try!  You can find the latest binary releases [here](https://github.com/COMBINE-lab/salmon/releases).\n\nThe current version number of the master branch of Salmon can be found [**here**](http://combine-lab.github.io/salmon/version_info/latest)\n\nDocumentation\n==============\n\nThe documentation for Salmon is available on [ReadTheDocs](http://readthedocs.org), check it out [here](http://salmon.readthedocs.org).\n\nSalmon is, and will continue to be, [freely and actively supported on a best-effort basis](https://oceangenomics.com/about/#open).\nIf you need industrial-grade technical support, please consider the options at [oceangenomics.com/contact](http://oceangenomics.com/contact).\n\nDecoy sequences in transcriptomes\n=================================\n\ntl;dr: fast is good but fast and accurate is better!\n[Alignment and mapping methodology influence transcript abundance estimation](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02151-8), and accounting for the [accounting for fragments of unexpected origin can improve transcript quantification](https://www.biorxiv.org/content/10.1101/2021.01.17.426996v1).  To this end, salmon provides the ability to index both the transcriptome as well as decoy seuqence that can be considered during mapping and quantification.  The decoy sequence accounts for reads that might otherwise be (spuriously) attributed to some annotated transcript. This [tutorial](https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/) provides a step-by-step guide on how to efficiently index the reference transcriptome and genome to produce a decoy-aware index.  Specifically, there are 3 possible ways in which the salmon index can be created:\n\n* cDNA-only index : salmon_index - https://combine-lab.github.io/salmon/getting_started/. This method will result in the smallest index and require the least resources to build, but will be the most prone to possible spurious alignments.\n\n* SA mashmap index: salmon_partial_sa_index - (regions of genome that have high sequence similarity to the transcriptome) - Details can be found in [this README](https://github.com/COMBINE-lab/SalmonTools/blob/master/README.md) and using [this script](https://raw.githubusercontent.com/COMBINE-lab/SalmonTools/master/scripts/generateDecoyTranscriptome.sh). While running mashmap can require considerable resources, the resulting decoy files are fairly small.  This will result in an index bigger than the cDNA-only index, but still mucch smaller than the full genome index below.  It will confer many, though not all, of the benefits of using the entire genome as a decoy sequence.\n\n* SAF genome index: salmon_sa_index - (the full genome is used as decoy) - The tutorial for creating such an index can be found [here](https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/).  This will result in the largest index, but likely does the best job in avoiding spurious alignments to annotated transcripts. \n\n**Facing problems with Indexing?**, Check if anyone else already had this problem in the issues section or fill the index generation [request form](https://forms.gle/3baJc5SYrkSWb1z48)\n\n### **NOTE**:\nIf you are generating an index to be used for single-cell or single-nucleus quantification with [alevin-fry](https://github.com/COMBINE-lab/alevin-fry), then we recommend you consider building a spliced+intron (_splici_) reference.  This serves much of the purpose of a decoy-aware index when quantifying with alevin-fry, while also providing the capability to attribute splicing status to mapped fragments.  More details about the _splici_ reference and the Unspliced/Spliced/Ambiguous quantification mode it enables can be found [here](https://combine-lab.github.io/alevin-fry-tutorials/2021/improving-txome-specificity/).\n\nChat live about Salmon\n======================\n\nYou can chat with the Salmon developers and other users via Gitter (**Note**: Gitter is much less frequently monitored than GitHub, so if you have an important problem or question, please consider opening an issue here on GitHub)!\n\n[![Join the chat at https://gitter.im/COMBINE-lab/salmon](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/COMBINE-lab/salmon?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n",751,bioinformatics,C++,8,CMake,C++,C,Shell,Python,Standard ML,Nextflow,Dockerfile,,,,,,,,,,,,,,,,,,,,,87,15,70,2,33,42,128,219297,159,813,475,338,a2f6912b3f9f9af91e3a4b0d74adcb3bdc4c9a32,Merge pull request #917 from COMBINE-lab/develop,2024-03-15T20:01:14Z,Rob Patro,rob-p@users.noreply.github.com,rob-p,Salmon v1.10.1,"This release is a *very* minor update, intended entirely to address #835 (a problem raised by deb med maintainers running into build problems upstream).  This release bumps the included version of the `cereal` headers in the corresponding `pufferfish` tag to `v1.3.2` and also updates the required version for `salmon` to match this (i.e. `cereal v1.3.2`).  Since the prior version included in `pufferfish` in past releases, the `cereal` library had made 2 *patch* releases which, nonetheless, were *not* backwards compatible. This lead to problems when mixing `cereal v1.3.2` with `v1.3.0`.  This release bumps everything to `v1.3.2` to match the latest package on debian testing.  If `salmon 1.10.0` is working fine for you, there's no need to update to this release (but obviously no harm in doing so). It adds no new features or bug fixes within `salmon` itself.",v1.10.1,Rob Patro,,rob-p,GNU General Public License v3.0,salmon,COMBINE-lab,46,quasi-mapping,bioinformatics,rna-seq,rnaseq,salmon,quantification,sailfish,c-plus-plus,gene-expression,scrna-seq,single-cell,10x,selective-alignment,rna-seq-quantification,single-cell-rna-seq,transcriptome,,,,,/COMBINE-lab/salmon,54,39,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/colmap/colmap,https://github.com/colmap/colmap,0,"big project, has complex calculations, but not really scientific software",,0,0,1,1,0,0,0,0,0,0,0,COLMAP - Structure-from-Motion and Multi-View Stereo,"COLMAP\n======\n\nAbout\n-----\n\nCOLMAP is a general-purpose Structure-from-Motion (SfM) and Multi-View Stereo\n(MVS) pipeline with a graphical and command-line interface. It offers a wide\nrange of features for reconstruction of ordered and unordered image collections.\nThe software is licensed under the new BSD license. If you use this project for\nyour research, please cite:\n\n    @inproceedings{schoenberger2016sfm,\n        author={Sch\""{o}nberger, Johannes Lutz and Frahm, Jan-Michael},\n        title={Structure-from-Motion Revisited},\n        booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},\n        year={2016},\n    }\n\n    @inproceedings{schoenberger2016mvs,\n        author={Sch\""{o}nberger, Johannes Lutz and Zheng, Enliang and Pollefeys, Marc and Frahm, Jan-Michael},\n        title={Pixelwise View Selection for Unstructured Multi-View Stereo},\n        booktitle={European Conference on Computer Vision (ECCV)},\n        year={2016},\n    }\n\nIf you use the image retrieval / vocabulary tree engine, please also cite:\n\n    @inproceedings{schoenberger2016vote,\n        author={Sch\""{o}nberger, Johannes Lutz and Price, True and Sattler, Torsten and Frahm, Jan-Michael and Pollefeys, Marc},\n        title={A Vote-and-Verify Strategy for Fast Spatial Verification in Image Retrieval},\n        booktitle={Asian Conference on Computer Vision (ACCV)},\n        year={2016},\n    }\n\nThe latest source code is available at https://github.com/colmap/colmap. COLMAP\nbuilds on top of existing works and when using specific algorithms within\nCOLMAP, please also cite the original authors, as specified in the source code,\nand consider citing relevant third-party dependencies.\n\n\nDownload\n--------\n\n* Binaries for **Windows** and other resources can be downloaded\n  from https://github.com/colmap/colmap/releases.\n* Binaries for **Linux/Unix/BSD** are available at\n  https://repology.org/metapackage/colmap/versions.\n* Pre-built **Docker** images are available at\n  https://hub.docker.com/r/colmap/colmap.\n* **Python bindings** are available at https://pypi.org/project/pycolmap.\n* To **build from source**, please see https://colmap.github.io/install.html.\n\n\nGetting Started\n---------------\n\n1. Download pre-built binaries or build from source.\n2. Download one of the provided datasets at https://demuc.de/colmap/datasets/\n   or use your own images.\n3. Use the **automatic reconstruction** to easily build models\n   with a single click or command.\n\n\nDocumentation\n-------------\n\nThe documentation is available at https://colmap.github.io/.\n\n\nSupport\n-------\n\nPlease, use GitHub Discussions at https://github.com/colmap/colmap/discussions\nfor questions and the GitHub issue tracker at https://github.com/colmap/colmap\nfor bug reports, feature requests/additions, etc.\n\n\nAcknowledgments\n---------------\n\nCOLMAP was originally written by [Johannes Schönberger](https://demuc.de/) with\nfunding provided by his PhD advisors Jan-Michael Frahm and Marc Pollefeys.\n\nThe Python bindings in PyCOLMAP were originally added by\n[Mihai Dusmanu](https://github.com/mihaidusmanu),\n[Philipp Lindenberger](https://github.com/Phil26AT), and\n[Paul-Edouard Sarlin](https://github.com/Skydes).\n\nThe project has also benefitted from countless community contributions, including\nbug fixes, improvements, new features, third-party tooling, and community\nsupport (special credits to [Torsten Sattler](https://tsattler.github.io)).\n\n\nContribution\n------------\n\nContributions (bug reports, bug fixes, improvements, etc.) are very welcome and\nshould be submitted in the form of new issues and/or pull requests on GitHub.\n\n\nLicense\n-------\n\nThe COLMAP library is licensed under the new BSD license. Note that this text\nrefers only to the license for COLMAP itself, independent of its thirdparty\ndependencies, which are separately licensed. Building COLMAP with these\ndependencies may affect the resulting COLMAP license.\n\n    Copyright (c) 2023, ETH Zurich and UNC Chapel Hill.\n    All rights reserved.\n\n    Redistribution and use in source and binary forms, with or without\n    modification, are permitted provided that the following conditions are met:\n\n        * Redistributions of source code must retain the above copyright\n          notice, this list of conditions and the following disclaimer.\n\n        * Redistributions in binary form must reproduce the above copyright\n          notice, this list of conditions and the following disclaimer in the\n          documentation and/or other materials provided with the distribution.\n\n        * Neither the name of ETH Zurich and UNC Chapel Hill nor the names of\n          its contributors may be used to endorse or promote products derived\n          from this software without specific prior written permission.\n\n    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n    AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n    IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n    ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE\n    LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n    CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n    SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n    INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n    CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n    ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n    POSSIBILITY OF SUCH DAMAGE.\n\n",7178,geometry,C++,11,CMake,MATLAB,Python,Shell,C++,Cuda,C,GLSL,Batchfile,Dockerfile,PowerShell,,,,,,,,,,,,,,,,,,620,76,515,29,14,115,0,14821,1461,1961,1116,845,b5c381ad71e6a970266a1f1280de523c0b10f107,Declare PosePrior::IsValid as const (#2653),2024-07-15T07:38:39Z,Johannes Schönberger,joschonb@microsoft.com,ahojnnes,03.09.2001,## What's Changed\r\n* Version 3.9 changelog by @ahojnnes in https://github.com/colmap/colmap/pull/2325\r\n* Fully encapsulate freeimage in bitmap library (#2332) by @ahojnnes in https://github.com/colmap/colmap/pull/2334\r\n\r\n\r\n**Full Changelog**: https://github.com/colmap/colmap/compare/3.9...3.9.1,03.09.2001,Johannes Schönberger,,ahojnnes,Other,colmap,colmap,20,structure-from-motion,multi-view-stereo,reconstruction,geometry,computer-vision,,,,,,,,,,,,,,,,/colmap/colmap,20,172,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/CliMA/Oceananigans.jl,https://github.com/CliMA/Oceananigans.jl,1,,,1,1,1,1,0,0,0,0,0,0,1,"🌊  Julia software for fast, friendly, flexible, ocean-flavored fluid dynamics on CPUs and GPUs","<!-- Title -->\n<h1 align=""center"">\n  Oceananigans.jl\n</h1>\n\n<!-- description -->\n<p align=""center"">\n  <strong>🌊 Fast and friendly ocean-flavored Julia software for simulating incompressible fluid dynamics in Cartesian and spherical shell domains on CPUs and GPUs. https://clima.github.io/OceananigansDocumentation/stable</strong>\n</p>\n\n<!-- Information badges -->\n<p align=""center"">\n  <a href=""https://www.repostatus.org/#active"">\n    <img alt=""Repo status"" src=""https://www.repostatus.org/badges/latest/active.svg?style=flat-square"" />\n  </a>\n  <a href=""https://mit-license.org"">\n    <img alt=""MIT license"" src=""https://img.shields.io/badge/License-MIT-blue.svg?style=flat-square"">\n  </a>\n  <a href=""https://github.com/CliMA/Oceananigans.jl/discussions"">\n    <img alt=""Ask us anything"" src=""https://img.shields.io/badge/Ask%20us-anything-1abc9c.svg?style=flat-square"">\n  </a>\n  <a href=""https://github.com/SciML/ColPrac"">\n    <img alt=""ColPrac: Contributor's Guide on Collaborative Practices for Community Packages"" src=""https://img.shields.io/badge/ColPrac-Contributor's%20Guide-blueviolet?style=flat-square"">\n  </a>\n  <a href=""https://doi.org/10.21105/joss.02018"">\n    <img alt=""JOSS"" src=""https://joss.theoj.org/papers/10.21105/joss.02018/status.svg"">\n  </a>\n</p>\n\n<!-- Version and documentation badges -->\n<p align=""center"">\n  <a href=""https://github.com/CliMA/Oceananigans.jl/releases"">\n    <img alt=""GitHub tag (latest SemVer pre-release)"" src=""https://img.shields.io/github/v/tag/CliMA/Oceananigans.jl?include_prereleases&label=latest%20version&logo=github&sort=semver&style=flat-square"">\n  </a>\n  <a href=""https://clima.github.io/OceananigansDocumentation/stable"">\n    <img alt=""Stable documentation"" src=""https://img.shields.io/badge/documentation-stable%20release-blue?style=flat-square"">\n  </a>\n  <a href=""https://clima.github.io/OceananigansDocumentation/dev"">\n    <img alt=""Development documentation"" src=""https://img.shields.io/badge/documentation-in%20development-orange?style=flat-square"">\n  </a>\n</p>\n\n<!-- CI/CD badges -->\n<p align=""center"">\n  <a href=""https://buildkite.com/clima/oceananigans"">\n    <img alt=""Buildkite CPU+GPU build status"" src=""https://img.shields.io/buildkite/4d921fc17b95341ea5477fb62df0e6d9364b61b154e050a123/main?logo=buildkite&label=Buildkite%20CPU%2BGPU&style=flat-square"">\n  </a>\n  <a href=""https://hub.docker.com/r/aliramadhan/oceananigans"">\n    <img alt=""Docker build status"" src=""https://img.shields.io/docker/cloud/build/aliramadhan/oceananigans?label=Docker&logo=docker&logoColor=white&style=flat-square"">\n  </a>\n</p>\n\nOceananigans is a fast, friendly, flexible software package for finite volume simulations of the nonhydrostatic\nand hydrostatic Boussinesq equations on CPUs and GPUs.\nIt runs on GPUs (wow, [fast!](http://arxiv.org/abs/2309.06662)), though we believe Oceananigans makes the biggest waves\nwith its ultra-flexible user interface that makes simple simulations easy, and complex, creative simulations possible.\n\nOceananigans.jl is developed by the [Climate Modeling Alliance](https://clima.caltech.edu) and heroic external collaborators.\n\n## Contents\n\n* [Installation instructions](#installation-instructions)\n* [Running your first model](#running-your-first-model)\n* [The Oceananigans knowledge base](#the-oceananigans-knowledge-base)\n* [Citing](#citing)\n* [Contributing](#contributing)\n* [Movies](#movies)\n* [Performance benchmarks](#performance-benchmarks)\n\n## Installation instructions\n\nOceananigans is a [registered Julia package](https://julialang.org/packages/). So to install it,\n\n1. [Download Julia](https://julialang.org/downloads/) (version 1.9 or later).\n\n2. Launch Julia and type\n\n```julia\njulia> using Pkg\n\njulia> Pkg.add(""Oceananigans"")\n```\n\nThis installs the latest version that's _compatible with your current environment_.\nDon't forget to *be careful* 🏄 and check which Oceananigans you installed:\n\n```julia\njulia> Pkg.status(""Oceananigans"")\n```\n\n## Running your first model\n\nLet's run a two-dimensional, horizontally-periodic simulation of turbulence using 128² finite volume cells for 4 non-dimensional time units:\n\n```julia\nusing Oceananigans\ngrid = RectilinearGrid(CPU(), size=(128, 128), x=(0, 2π), y=(0, 2π), topology=(Periodic, Periodic, Flat))\nmodel = NonhydrostaticModel(; grid, advection=WENO())\nϵ(x, y) = 2rand() - 1\nset!(model, u=ϵ, v=ϵ)\nsimulation = Simulation(model; Δt=0.01, stop_time=4)\nrun!(simulation)\n```\n\nBut there's more: changing `CPU()` to `GPU()` makes this code on a CUDA-enabled Nvidia GPU.\n\nDive into [the documentation](https://clima.github.io/OceananigansDocumentation/stable/) for more code examples and tutorials.\nBelow, you'll find movies from GPU simulations along with CPU and GPU [performance benchmarks](https://github.com/clima/Oceananigans.jl#performance-benchmarks).\n\n## The Oceananigans knowledge base\n\nIt's _deep_ and includes:\n\n* [Documentation](https://clima.github.io/OceananigansDocumentation/stable) that provides\n    * example Oceananigans scripts,\n    * tutorials that describe key Oceananigans objects and functions,\n    * explanations of Oceananigans finite-volume-based numerical methods,\n    * details of the dynamical equations solved by Oceananigans models, and\n    * a library documenting all user-facing Oceananigans objects and functions.\n* [Discussions on the Oceananigans github](https://github.com/CliMA/Oceananigans.jl/discussions), covering topics like\n    * [""Computational science""](https://github.com/CliMA/Oceananigans.jl/discussions/categories/computational-science), or how to science and set up numerical simulations in Oceananigans, and\n    * [""Experimental features""](https://github.com/CliMA/Oceananigans.jl/discussions?discussions_q=experimental+features), which covers new and sparsely-documented features for those who like to live dangerously.\n  \n    If you've got a question or something, anything! to talk about, don't hesitate to [start a new discussion](https://github.com/CliMA/Oceananigans.jl/discussions/new?).\n* The [Oceananigans wiki](https://github.com/CliMA/Oceananigans.jl/wiki) contains practical tips for [getting started with Julia](https://github.com/CliMA/Oceananigans.jl/wiki/Installation-and-getting-started-with-Oceananigans), [accessing and using GPUs](https://github.com/CliMA/Oceananigans.jl/wiki/Accessing-GPUs-and-using-Oceananigans-on-GPUs), and [productive workflows when using Oceananigans](https://github.com/CliMA/Oceananigans.jl/wiki/Productive-Oceananigans-workflows-and-Julia-environments).\n* The `#oceananigans` channel on the [Julia Slack](https://julialang.org/slack/), which accesses ""institutional knowledge"" stored in the minds of the amazing Oceananigans community.\n* [Issues](https://github.com/CliMA/Oceananigans.jl/issues) and [pull requests](https://github.com/CliMA/Oceananigans.jl/pulls) also contain lots of information about problems we've found, solutions we're trying to implement, and dreams we're dreaming to make tomorrow better 🌈.\n\n## Citing\n\nIf you use Oceananigans.jl as part of your research, teaching, or other activities, we would be grateful if you could cite our work and mention Oceananigans.jl by name.\n\n```bibtex\n@article{OceananigansJOSS,\n  doi = {10.21105/joss.02018},\n  url = {https://doi.org/10.21105/joss.02018},\n  year = {2020},\n  publisher = {The Open Journal},\n  volume = {5},\n  number = {53},\n  pages = {2018},\n  author = {Ali Ramadhan and Gregory LeClaire Wagner and Chris Hill and Jean-Michel Campin and Valentin Churavy and Tim Besard and Andre Souza and Alan Edelman and Raffaele Ferrari and John Marshall},\n  title = {{Oceananigans.jl: Fast and friendly geophysical fluid dynamics on GPUs}},\n  journal = {Journal of Open Source Software}\n}\n```\n\nWe also maintain a [list of publications using Oceananigans.jl](https://clima.github.io/OceananigansDocumentation/stable/#Papers-and-preprints-using-Oceananigans). If you have work using Oceananigans.jl that you would like to have listed there, please open a pull request to add it or let us know!\n\n## Contributing\n\nIf you're interested in contributing to the development of Oceananigans we want your help no matter how big or small a contribution you make!\nCause we're all in this together.\n\nIf you'd like to work on a new feature, or if you're new to open source and want to crowd-source neat projects that fit your interests, you should [start a discussion](https://github.com/CliMA/Oceananigans.jl/discussions/new?) right away.\n\nFor more information check out our [contributor's guide](https://clima.github.io/OceananigansDocumentation/stable/contributing/).\n\n## Movies\n\n### [Deep convection](https://www.youtube.com/watch?v=kpUrxnKKMjI)\n\n[![Watch deep convection in action](https://raw.githubusercontent.com/ali-ramadhan/ali-ramadhan.Github.io/master/img/surface_temp_3d_00130_halfsize.png)](https://www.youtube.com/watch?v=kpUrxnKKMjI)\n\n### [Free convection](https://www.youtube.com/watch?v=yq4op9h3xcU)\n\n[![Watch free convection in action](https://raw.githubusercontent.com/ali-ramadhan/ali-ramadhan.Github.io/master/img/free_convection_0956.png)](https://www.youtube.com/watch?v=yq4op9h3xcU)\n\n### [Winds blowing over the ocean](https://www.youtube.com/watch?v=IRncfbvuiy8)\n\n[![Watch winds blowing over the ocean](https://raw.githubusercontent.com/ali-ramadhan/ali-ramadhan.Github.io/master/img/wind_stress_0400.png)](https://www.youtube.com/watch?v=IRncfbvuiy8)\n\n### [Free convection with wind stress](https://www.youtube.com/watch?v=ob6OMQgPfI4)\n\n[![Watch free convection with wind stress in action](https://raw.githubusercontent.com/ali-ramadhan/ali-ramadhan.Github.io/master/img/wind_stress_unstable_7500.png)](https://www.youtube.com/watch?v=ob6OMQgPfI4)\n\n## Performance benchmarks\n\nWe've performed some preliminary performance benchmarks (see the [performance benchmarks](https://clima.github.io/OceananigansDocumentation/stable/appendix/benchmarks/) section of the documentation) by initializing models of various sizes and measuring the wall clock time taken per model iteration (or time step).\n\nThis is not really a fair comparison as we haven't parallelized across all the CPU's cores so we will revisit these benchmarks once Oceananigans.jl can run on multiple CPUs and GPUs.\n\nTo make full use of or fully saturate the computing power of a GPU such as an Nvidia Tesla V100 or\na Titan V, the model should have around ~10 million grid points or more.\n\nSometimes counter-intuitively running with `Float32` is slower than `Float64`. This is likely due\nto type mismatches causing slowdowns as floats have to be converted between 32-bit and 64-bit, an\nissue that needs to be addressed meticulously. Due to other bottlenecks such as memory accesses and\nGPU register pressure, `Float32` models may not provide much of a speedup so the main benefit becomes\nlower memory costs (by around a factor of 2).\n\n![Performance benchmark plots](https://user-images.githubusercontent.com/20099589/89906791-d2c85b00-dbb9-11ea-969a-4b8db2c31680.png)\n",928,fluid-dynamics,Julia,5,Julia,Dockerfile,Mathematica,TeX,Python,,,,,,,,,,,,,,,,,,,,,,,,2003,302,1634,67,363,79,408,120721,188,1419,1260,159,d6e63e53e795272378b7657c4a6f32da2d62d6f9,`CuArray` `times` for `Cyclical` field time series (#3639),2024-07-16T00:22:15Z,Simone Silvestri,silvestri.simone0@gmail.com,simone-silvestri,v0.91.4,## Oceananigans v0.91.4\n\n[Diff since v0.91.3](https://github.com/CliMA/Oceananigans.jl/compare/v0.91.3...v0.91.4)\n\n\n**Merged pull requests:**\n- Remove performance-/precompilation-time harmful `@eval` (#3556) (@simone-silvestri)\n- Enzyme: try bump without tuple changes (#3618) (@wsmoses)\n- Make `Base.axes(f::Field)` type-inferable for windowed fields (#3624) (@glwagner)\n- `fill_halo_regions!` for velocities in implicit free surface correctly (#3629) (@glwagner)\n- Fix a bug for SpecifiedTimes time-step alignment (#3634) (@glwagner)\n- Remove vestigial code (#3636) (@glwagner)\n- Update list of papers using Oceananigans (#3641) (@navidcy)\n- (0.91.4) Fix interpolations in off-diagonal components of strain-rate tensor (#3648) (@tomchor)\n\n**Closed issues:**\n- `@eval` considered harmful (#3555)\n- Huge memory use of Average associated with compilation (#3621),v0.91.4,,,github-actions[bot],MIT License,Oceananigans.jl,CliMA,245,climate,ocean,fluid-dynamics,julia,gpu,climate-change,machine-learning,data-assimilation,,,,,,,,,,,,,/CliMA/Oceananigans.jl,246,27,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/clEsperanto/pyclesperanto_prototype,https://github.com/clEsperanto/pyclesperanto_prototype,0.5,Image processing?,1,1,1,1,1,0,0,0,0,0,0,1,GPU-accelerated bio-image analysis focusing on 3D+t microscopy image data,"# py-clesperanto\n[![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&url=https%3A%2F%2Fforum.image.sc%2Ftag%2Fclesperanto.json&query=%24.topic_list.tags.0.topic_count&colorB=brightgreen&suffix=%20topics&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tag/clesperanto)\n[![website](https://img.shields.io/website?url=http%3A%2F%2Fclesperanto.net)](http://clesperanto.net)\n[![PyPI](https://img.shields.io/pypi/v/pyclesperanto-prototype.svg?color=green)](https://pypi.org/project/pyclesperanto-prototype)\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/pyclesperanto-prototype/badges/version.svg)](https://anaconda.org/conda-forge/pyclesperanto-prototype)\n[![Contributors](https://img.shields.io/github/contributors-anon/clEsperanto/pyclesperanto_prototype)](https://github.com/clEsperanto/pyclesperanto_prototype/graphs/contributors)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/pyclesperanto_prototype)](https://pypistats.org/packages/pyclesperanto_prototype)\n[![GitHub stars](https://img.shields.io/github/stars/clEsperanto/pyclesperanto_prototype?style=social)](https://github.com/clEsperanto/pyclesperanto_prototype/)\n[![GitHub forks](https://img.shields.io/github/forks/clEsperanto/pyclesperanto_prototype?style=social)](https://github.com/clEsperanto/pyclesperanto_prototype/)\n[![License](https://img.shields.io/pypi/l/pyclesperanto_prototype.svg?color=green)](https://github.com/haesleinhuepf/pyclesperanto_prototype/raw/master/LICENSE)\n[![Python Version](https://img.shields.io/pypi/pyversions/pyclesperanto-prototype.svg?color=green)](https://python.org)\n[![tests](https://github.com/clesperanto/pyclesperanto_prototype/workflows/tests/badge.svg)](https://github.com/clesperanto/pyclesperanto_prototype/actions)\n[![codecov](https://codecov.io/gh/clesperanto/pyclesperanto_prototype/branch/master/graph/badge.svg)](https://codecov.io/gh/clesperanto/pyclesperanto_prototype)\n[![Development Status](https://img.shields.io/pypi/status/pyclesperanto_prototype.svg)](https://en.wikipedia.org/wiki/Software_release_life_cycle#Alpha)\n[![DOI](https://zenodo.org/badge/248206619.svg)](https://zenodo.org/badge/latestdoi/248206619)\n\npy-clesperanto is a prototype for [clesperanto](http://clesperanto.net) - a multi-platform multi-language framework for GPU-accelerated image processing. \nWe mostly use it in the life sciences for analysing 3- and 4-dimensional microsopy data, e.g. as we face it developmental biology when segmenting cells and studying\ntheir individual properties as well as properties of compounds of cells forming tissues.\n\n![](https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/banner.png)\nImage data source: Daniela Vorkel, Myers lab, MPI-CBG, rendered using [napari](https://github.com/napari/napari)\n\nclesperanto uses [OpenCL kernels](https://github.com/clEsperanto/clij-opencl-kernels/tree/development/src/main/java/net/haesleinhuepf/clij/kernels) from [CLIJ](http://clij.github.io/).\n\nFor users convenience, there are code generators available for [napari](https://clesperanto.github.io/napari_pyclesperanto_assistant/) and [Fiji](https://clij.github.io/assistant/).\nAlso check out the [napari workflow optimizer](https://github.com/haesleinhuepf/napari-workflow-optimizer) for semi-automatic parameter tuning of clesperanto-functions.\n\n## Reference\nThe preliminary API reference is available [here](https://clesperanto.github.io/pyclesperanto_prototype/docs/_build/html/).\nFurthermore, parts of the [reference](https://clij.github.io/clij2-docs/reference__pyclesperanto) are also available within the CLIJ2 documentation.\n\n## Installation\n* Get a conda/python environment, e.g. via [mamba-forge](https://github.com/conda-forge/miniforge#mambaforge). \n* If you never used python/conda environments before, please follow [these instructions](https://biapol.github.io/blog/mara_lampert/getting_started_with_mambaforge_and_python/readme.html) first.\n\n```\nconda create --name cle_39 python=3.9\nconda activate cle_39\n```\n\n* Install pyclesperanto-prototype using [mamba / conda](https://focalplane.biologists.com/2022/12/08/managing-scientific-python-environments-using-conda-mamba-and-friends/):\n\n```\nmamba install -c conda-forge pyclesperanto-prototype\n```\n\nOR using pip:\n\n```\npip install pyclesperanto-prototype\n```\n\n## Troubleshooting: Graphics cards drivers\n\nIn case error messages contain ""ImportError: DLL load failed while importing cl: The specified procedure could not be found"" [see also](https://github.com/clEsperanto/pyclesperanto_prototype/issues/55) or ""clGetPlatformIDs failed: PLATFORM_NOT_FOUND_KHR"", please install recent drivers for your graphics card and/or OpenCL device. Select the right driver source depending on your hardware from this list:\n\n* [AMD drivers](https://www.amd.com/en/support)\n* [NVidia drivers](https://www.nvidia.com/download/index.aspx)\n* [Intel GPU drivers](https://www.intel.com/content/www/us/en/download/726609/intel-arc-graphics-windows-dch-driver.html)\n* [Microsoft Windows OpenCL support](https://www.microsoft.com/en-us/p/opencl-and-opengl-compatibility-pack/9nqpsl29bfff)\n\nSometimes, mac-users need to install this:\n\n    mamba install -c conda-forge ocl_icd_wrapper_apple\n\nSometimes, linux users need to install this:\n\n    mamba install -c conda-forge ocl-icd-system\n\nLinux user may have to install packages such as `intel-opencl-icd` or `rocm-opencl-runtime` depending on their GPU.\n\n## Computing on Central Processing units (CPUs)\n\nIf no OpenCL-compatible GPU is available, pyclesperanto-prototype can make use of CPUs instead. \nJust install [oclgrind](https://github.com/jrprice/Oclgrind)\nor [pocl](http://portablecl.org/), e.g. using mamba / conda. Oclgrind is recommended for Windows systems, PoCL for Linux. MacOS typically comes with OpenCL support for CPUs.\n\n```\nmamba install  oclgrind -c conda-forge\n```\n\nOR \n\n```\nmamba install  pocl -c conda-forge\n```\n\nOwners of compatible Intel Xeon CPUs can also install a driver to use them for computing:\n* [Intel CPU OpenCL drivers](https://www.intel.com/content/www/us/en/developer/articles/tool/opencl-drivers.html#latest_CPU_runtime)\n\n## Example code\nA basic image processing workflow loads blobs.gif and counts the number of objects:\n\n```python\nimport pyclesperanto_prototype as cle\n\nfrom skimage.io import imread, imsave\n\n# initialize / select GPU with ""TX"" in their name\ndevice = cle.select_device(""TX"")\nprint(""Used GPU: "", device)\n\n# load data\nimage = imread('https://imagej.nih.gov/ij/images/blobs.gif')\n\n# process the image\ninverted = cle.subtract_image_from_scalar(image, scalar=255)\nblurred = cle.gaussian_blur(inverted, sigma_x=1, sigma_y=1)\nbinary = cle.threshold_otsu(blurred)\nlabeled = cle.connected_components_labeling_box(binary)\n\n# The maximium intensity in a label image corresponds to the number of objects\nnum_labels = labeled.max()\nprint(f""Number of objects in the image: {num_labels}"")\n\n# save image to disc\nimsave(""result.tif"", labeled)\n```\n\n## Example gallery \n\n<table border=""0"">\n<tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/screenshot_select_GPU.png"" width=""300""/>\n\n</td><td>\n\n[Select GPU](https://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/basics/select_GPU.py)\n\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/jupyter.png"" width=""300""/>\n\n</td><td>\n\n[Image processing in Jupyter Notebooks](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/interoperability/jupyter.ipynb)\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/screenshot_count_blobs.png"" width=""300""/>\n\n</td><td>\n\n[Counting blobs](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/basics/count_blobs.ipynb)\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/voronoi_otsu_labeling.png"" width=""300""/>\n\n</td><td>\n\n[Voronoi-Otsu labeling](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/segmentation/voronoi_otsu_labeling.ipynb)\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/segmentation_3d.png"" width=""300""/>\n\n</td><td>\n\n[3D Image segmentation ](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/segmentation/Segmentation_3D.ipynb)\n\n</td></tr>\n\n<tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/segmentation_2d_membranes.png"" width=""300""/>\n\n</td><td>\n\n[Cell segmentation based on membranes](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/segmentation/segmentation_2d_membranes.ipynb)\n\n</td></tr>\n\n<tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/counting_nuclei_multichannel.png"" width=""300""/>\n\n</td><td>\n\n[Counting nuclei according to expression in multiple channels](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/measurement/counting_nuclei_multichannel.ipynb)\n\n</td></tr>\n\n\n<tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/differentiate_nuclei_intensity.png"" width=""300""/>\n\n</td><td>\n\n[Differentiating nuclei according to signal intensity](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/measurement/differentiate_nuclei_intensity.ipynb)\n\n</td></tr>\n\n\n<tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/bead_segmentation.png"" width=""300""/>\n\n</td><td>\n\n[Detecting beads and measuring their size](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/segmentation/bead_segmentation.ipynb)\n\n</td></tr>\n\n<tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/label_statistics.png"" width=""300""/>\n\n</td><td>\n\n[Label statistics](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/basics/label_statistics.ipynb)\n\n</td></tr>\n\n<tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/parametric_maps.png"" width=""300""/>\n\n</td><td>\n\n[Parametric maps](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/tissues/parametric_maps.ipynb)\n\n</td></tr>\n\n\n<tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/intensities_along_lines.png"" width=""300""/>\n\n</td><td>\n\n[Measure intensity along lines](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/measurement/intensities_along_lines.ipynb)\n\n</td></tr>\n\n\n<tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/screenshot_crop_and_paste_images.png"" width=""300""/>\n\n</td><td>\n\n[Crop and paste images](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/basics/crop_and_paste_images.ipynb)\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/screenshot_inspecting_3d_images.png"" width=""300""/>\n\n</td><td>\n\n[Inspecting 3D image data](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/basics/inspecting_3d_images.ipynb)\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/screenshot_affine_transforms.png"" width=""300""/>\n\n</td><td>\n\n[Rotation, scaling, translation, affine transforms](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/transforms/affine_transforms.ipynb)\n\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/screenshot_deskew.png"" width=""300""/>\n\n</td><td>\n\n[Deskewing](https://github.com/clEsperanto/pyclesperanto_prototype/blob/master/demo/transforms/deskew.ipynb)\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/screenshot_multiply_vectors_and_matrices.png"" width=""300""/>\n\n</td><td>\n\n[Multiply vectors and matrices](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/basics/multiply_vectors_and_matrices.ipynb)\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/screenshot_multiply_matrices.png"" width=""300""/>\n\n</td><td>\n\n[Matrix multiplication](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/basics/multiply_matrices.ipynb)\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/screenshot_spots_pointlists_matrices_tables.png"" width=""300""/>\n\n</td><td>\n\n* [Working with spots, pointlist and matrices](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/basics/spots_pointlists_matrices_tables.ipynb)\n* [Lists of nonzero pixel coordinates](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/basics/nonzero.ipynb)\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/mesh_between_centroids.png"" width=""300""/>\n\n</td><td>\n\n[Mesh between centroids](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/neighbors/mesh_between_centroids.ipynb)\n\n</td></tr><tr><td>\n\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/mesh_between_touching_neighbors.png"" width=""300""/>\n\n</td><td>\n\n[Mesh between touching neighbors](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/neighbors/mesh_between_touching_neighbors.ipynb)\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/mesh_with_distances.png"" width=""300""/>\n\n</td><td>\n\n[Mesh with distances](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/neighbors/mesh_with_distances.ipynb)\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/mesh_nearest_neighbors.png"" width=""300""/>\n\n</td><td>\n\n[Mesh nearest_neighbors](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/neighbors/mesh_nearest_neighbors.ipynb)\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/ipgraph_networkx.png"" width=""300""/>\n\n</td><td>\n\n[Export to igraph and networkx](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/neighbors/ipgraph_networkx.ipynb)\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/neighborhood_definitions.png"" width=""300""/>\n\n</td><td>\n\n[Neighborhood definitions](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/neighbors/neighborhood_definitions.ipynb)\n\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/tissue_neighborhood_quantification.png"" width=""300""/>\n\n</td><td>\n\n[Tissue neighborhood quantification](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/tissues/tissue_neighborhood_quantification.ipynb)\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/neighbors_of_neighbors.png"" width=""300""/>\n\n</td><td>\n\n[Neighbors of neighbors](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/neighbors/neighbors_of_neighbors.ipynb)\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/screenshot_voronoi_diagrams.png"" width=""300""/>\n\n</td><td>\n\n[Voronoi diagrams](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/basics/voronoi_diagrams.ipynb)\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/shape_descriptors_based_on_neighborhood_graphs.png"" width=""300""/>\n\n</td><td>\n\n[Shape descriptors based on neighborhood graphs](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/neighbors/shape_descriptors_based_on_neighborhood_graphs.ipynb)\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/distance_to_other_labels.png"" width=""300""/>\n\n</td><td>\n\n[Measuring distances between labels in two label images](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/neighbors/distance_to_other_labels.ipynb)\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/screenshot_tribolium_napari.png"" width=""300""/>\n\n</td><td>\n\n[Tribolium morphometry + Napari](https://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/tribolium_morphometry/tribolium.py)\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/screenshot_tribolium_morphometry.png"" width=""300""/>\n\n</td><td>\n\n[Tribolium morphometry](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/tribolium_morphometry/tribolium_morphometry2.ipynb)\n[(archived version)](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/tribolium_morphometry/tribolium_morphometry.ipynb)\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/screenshot_napari_dask.png"" width=""300""/>\n\n</td><td>\n\n[napari+dask timelapse processing](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/napari_gui/napari_dask.ipynb)\n\n</td></tr>\n</table>\n\n## Technical insights\n\n<table border=""0""><tr><td>\n<tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/browse_operations.png"" width=""300""/>\n\n</td><td>\n\n[Browsing operations](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/basics/browse_operations.ipynb)\n\n</td></tr>\n<tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/interactive_widgets.gif"" width=""300""/>\n\n</td><td>\n\n[Interactive widgets](https://colab.research.google.com/github/clEsperanto/pyclesperanto_prototype/tree/master/demo/basics/browse_operations.ipynb)\n\n</td></tr>\n<tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/optimize_blobs_segmentation.png"" width=""300""/>\n\n</td><td>\n\n[Automatic workflow optimization](https://colab.research.google.com/github/clEsperanto/pyclesperanto_prototype/tree/master/demo/optimization/optimize_blobs_segmentation.ipynb)\n\n</td></tr>\n\n<tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/memory_management.png"" width=""300""/>\n\n</td><td>\n\n[Tracing memory consumtion on NVidia GPUs](https://github.com/clEsperanto/pyclesperanto_prototype/blob/master/demo/optimization/memory_management.ipynb)\n\n</td></tr>\n\n<tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/switching_gpus.png"" width=""300""/>\n\n</td><td>\n\n[Exploring and switching between GPUs](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/basics/switching_gpus.ipynb)\n\n</td></tr>\n<tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/cupy_compatibility.png"" width=""300""/>\n\n</td><td>\n\n[Interoperability with cupy](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/basics/interoperability_cupy.ipynb)\n\n[Using the cupy backend](http://github.com/clEsperanto/pyclesperanto_prototype/tree/master/demo/basics/select_backend.ipynb)\n\n</td></tr>\n<tr><td>\n\n<img src=""./docs/images/dask.jpg"" width=""300""/>\n\n</td><td>\n\n[Big data handling with Dask GPU clusters](./demo/interoperability/dask.ipynb)\n\n\n</td></tr>\n</table>\n\n## Related projects\n\n<table border=""0""><tr><td>\n\n<img src=""https://github.com/clesperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot.png"" width=""300""/>\n\n</td><td>\n\n[napari-pyclesperanto-assistant](https://github.com/clesperanto/napari_pyclesperanto_assistant):\nA graphical user interface for general purpose GPU-accelerated image processing and analysis in napari.\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification/raw/main/images/screenshot.png"" width=""300""/>\n\n</td><td>\n\n[napari-accelerated-pixel-and-object-classification](https://github.com/haesleinhuepf/napari-accelerated-pixel-and-object-classification):\nGPU-accelerated Random Forest Classifiers for pixel and labeled object classification\n\n</td></tr><tr><td>\n\n<img src=""https://github.com/clEsperanto/pyclesperanto_prototype/raw/master/docs/images/napari-clusters-plotter.png"" width=""300""/>\n\n</td><td>\n\n[napari-clusters-plotter](https://github.com/BiAPoL/napari-clusters-plotter):\nClustering of objects according to their quantitative properties\n\n</td></tr></table>\n\n## Benchmarking\nWe implemented some basic benchmarking notebooks allowing to see performance differences between pyclesperanto and \nsome other image processing libraries, typically using the CPU. Such benchmarking results vary heavily depending on \nimage size, kernel size, used operations, parameters and used hardware. Feel free to use those notebooks, adapt them to \nyour use-case scenario and benchmark on your target hardware. If you have different scenarios or use-cases, you are very \nwelcome to submit your notebook as pull-request!\n\n* [Affine transforms](http://github.com/clEsperanto/pyclesperanto_prototype/blob/master/benchmarks/affine_transforms.ipynb)\n* [Background subtraction](http://github.com/clEsperanto/pyclesperanto_prototype/blob/master/benchmarks/top_hat.ipynb)\n* [Gaussian blur](http://github.com/clEsperanto/pyclesperanto_prototype/blob/master/benchmarks/gaussian_blur.ipynb)\n* [Convolution](http://github.com/clEsperanto/pyclesperanto_prototype/blob/master/benchmarks/convolution.ipynb)\n* [Otsu's thresholding](http://github.com/clEsperanto/pyclesperanto_prototype/blob/master/benchmarks/threshold_otsu.ipynb)\n* [Connected component labeling](http://github.com/clEsperanto/pyclesperanto_prototype/blob/master/benchmarks/connected_component_labeling.ipynb)  \n* [Extend labels](http://github.com/clEsperanto/pyclesperanto_prototype/blob/master/benchmarks/extend_labels.ipynb)\n* [Statistics of labeled pixels / regionprops](http://github.com/clEsperanto/pyclesperanto_prototype/blob/master/benchmarks/statistics_of_labeled_pixels.ipynb)\n* [Histograms](http://github.com/clEsperanto/pyclesperanto_prototype/blob/master/benchmarks/histograms.ipynb)\n* [Matrix multiplication](http://github.com/clEsperanto/pyclesperanto_prototype/blob/master/benchmarks/matrix_multiplication.ipynb)\n* [Pixel-wise comparison](http://github.com/clEsperanto/pyclesperanto_prototype/blob/master/benchmarks/pixelwise_comparison.ipynb)\n* [Intensity projections](http://github.com/clEsperanto/pyclesperanto_prototype/blob/master/benchmarks/intensity_projections.ipynb)\n* [Axis transposition](http://github.com/clEsperanto/pyclesperanto_prototype/blob/master/benchmarks/transpose.ipynb)\n* [Nonzero](http://github.com/clEsperanto/pyclesperanto_prototype/blob/master/benchmarks/nonzero.ipynb)\n\n## See also\nThere are other libraries for code acceleration and GPU-acceleration for image processing.\n* [numba](https://numba.pydata.org/)\n* [cupy](https://cupy.dev)\n* [cucim](https://github.com/rapidsai/cucim)\n* [clij](https://clij.github.io)\n\n## Feedback welcome!\nclesperanto is developed in the open because we believe in the open source community. See our [community guidelines](https://clij.github.io/clij2-docs/community_guidelines). Feel free to drop feedback as [github issue](https://github.com/clEsperanto/pyclesperanto_prototype/issues) or via [image.sc](https://image.sc)\n",201,bioimage-analysis,Jupyter Notebook,3,Python,C,Jupyter Notebook,,,,,,,,,,,,,,,,,,,,,,,,,,200,6,183,11,90,15,340,231508,44,139,74,65,b1481b35e8a578155c0c211be6225350e92cdb31,bump version,2024-05-12T12:16:04Z,Robert Haase,haesleinhuepf@users.noreply.github.com,haesleinhuepf,,,,,,,"BSD 3-Clause ""New"" or ""Revised"" License",pyclesperanto_prototype,clEsperanto,66,bioimage-analysis,gpu-acceleration,microscopy,,,,,,,,,,,,,,,,,,/clEsperanto/pyclesperanto_prototype,66,14,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/CindyJS/CindyJS,https://github.com/CindyJS/CindyJS,0,,,0,0,0,0,0,0,1,1,0,0,0,A JavaScript framework for interactive (mathematical) content.,"# CindyJS\n\n**CindyJS is a framework to create interactive\n(mathematical) content for the web.**\n\nIt aims to be compatible with [Cinderella](http://cinderella.de/),\nproviding an interpreter for the scripting language CindyScript\nas well as a set of geometric operations which can be used to describe\nconstructions.\nTogether, these components make it very easy to visualize various\nconcepts, from geometry in particular and mathematics in general,\nbut also from various other fields.\n\nSee also our [project page](https://cindyjs.org).\n\n## Examples\n\nExamples on the web can be seen [here](https://cindyjs.org/gallery/main/).\n\nThere is also [an `examples` directory](https://github.com/CindyJS/CindyJS/tree/master/examples)\ninside the repository, demonstrating individual functions and operations.\n\nDevelopers can run these examples from their local development copy.\nSome examples may require a webserver-like environment to avoid\ntriggering browser security measures associated with local files.\nTo do so, one can run <code>node_modules/.bin/st -l -nc</code>\nin the root of the development tree, and then visit\n[the local copy of the examples directory](http://127.0.0.1:1337/examples/).\n\n## Building\n\nIf you have `npm` installed, running `npm install`\nin the top level source directory should just work.\n\nIf you lack a compatible setup of `npm` and `node`,\nrunning `make build=release` in the top level source directory should\nbe able to get a suitable setup installed inside the project directory tree.\nIn general, all required third-party tools should be automatically downloaded\nand installed inside the project directory tree.\nOne exception is a Java Runtime Environment, which has to be installed before\n(because users have to manually accept the terms and conditions before\nbeing allowed to download a JRE).\n\nIf `npm` resp. `make` terminated successfully, then `build/js` will contain\nthe artefacts which you'd likely want to include in your web site.\nIf you are building from an official commit, then `make build=release deploy`\nwill create `build/deploy` which is even better suited to be put on a web server,\nsince it references the commit at GitHub which may help diagnose problems.\n\n### Building on Windows\n\nThe description above uses `make` mostly for convenience.\nPretty much all the commands are in fact passed on to\na JavaScript-based build system contained in the `make` directory.\nIf you don't have `make` available on Windows,\nyou can call `node make` instead.\nSo a standard release build would be `node make build=release`.\n\nNote that you should have the following software installed:\n\n-   A recent Java Runtime Environment (JRE)\n-   Node.js with the `node` command added to the PATH\n-   Git for Windows with the `git` command usable from the Windows Command Prompt\n\n## Contributing\n\nWhen you work on the code base the simple `make` or `node make`\nwill give you a build which is fast to compile and easy to debug.\nIn contrast to this, `node make build=release` will\nperform additional compilation steps like running the Closure Compiler.\nIt may issue more warnings, which in turn might be useful when developing.\nYou should make sure that your code works in both build modes.\n\nIf you are confident that your work is done, call `make alltests`\nafter you did `git add` to stage your changes.\nThat will ensure that your modifications pass all kinds of tests.\nThe same tests will be run automatically on pull requests.\nOnce your modifications satisfy your expectations, pass these tests\nand are accompanied by a suitable test case or demonstrating example\n(where appropriate), you may file a pull request for your changes.\n\n## Documentation\n\n[The CindyJS API documentation](https://github.com/CindyJS/CindyJS/blob/master/ref/createCindy.md)\ndescribes how to create a widget on an HTML page using this framework.\n\nOther documentation in [the `ref` directory](https://github.com/CindyJS/CindyJS/tree/master/ref) describes\nlarge portions of the CindyScript programming language. This\ndocumentation, however, started as a copy of\n[the corresponding Cinderella documentation](http://doc.cinderella.de/tiki-index.php?page=CindyScript). It\nis currently meant as a goal of what functionality _should_ be\nsupported, while actual support might still be lagging behind. If there\nis a particular feature you'd need for your work, don't hesitate to\n[file a feature request](https://github.com/CindyJS/CindyJS/issues) for it.\n\n## License\n\nCindyJS is licensed under the\n[Apache 2 license](http://www.apache.org/licenses/LICENSE-2.0.html).\n",645,mathematics,JavaScript,11,JavaScript,HTML,Makefile,CSS,Python,Java,GLSL,C++,Shell,SCSS,TypeScript,,,,,,,,,,,,,,,,,,622,71,551,0,16,18,0,32241,50,306,166,140,eed61f78507528033e9ea00e1a255c905abd98fb,Merge pull request #927 from CindyJS/array-index-bugfix,2024-06-17T19:18:57Z,Bernhard Werner,bernhard.werner@hm.edu,BernhardWerner,CindyJS v0.8.29,## What's Changed\r\n* Name variables for export functions by @BernhardWerner in https://github.com/CindyJS/CindyJS/pull/921\r\n* Text outline by @BernhardWerner  in https://github.com/CindyJS/CindyJS/pull/924\r\n* Optional indices in array functions + compatability fix for max/min by @BernhardWerner in https://github.com/CindyJS/CindyJS/pull/923\r\n* Bug fix and new modifier in min/max by @BernhardWerner in https://github.com/CindyJS/CindyJS/pull/925\r\n* Bug fix for text outline by @BernhardWerner in https://github.com/CindyJS/CindyJS/pull/926\r\n* Fixed modifier handling in min/max by @BernhardWerner in https://github.com/CindyJS/CindyJS/pull/927\r\n\r\n\r\n**Full Changelog**: https://github.com/CindyJS/CindyJS/compare/v0.8.28...v0.8.29,v0.8.29,,,kortenkamp,Apache License 2.0,CindyJS,CindyJS,40,geometry,mathematics,javascript,,,,,,,,,,,,,,,,,,/CindyJS/CindyJS,48,26,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/chemprop/chemprop,https://github.com/chemprop/chemprop,1,"However, based on Neural networks",,1,1,1,0,0,0,0,0,0,0,0,Message Passing Neural Networks for Molecule Property Prediction,"![ChemProp Logo](docs/source/_static/images/logo/chemprop_logo.svg)\n# Chemprop\n\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/chemprop)](https://badge.fury.io/py/chemprop)\n[![PyPI version](https://badge.fury.io/py/chemprop.svg)](https://badge.fury.io/py/chemprop)\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/chemprop/badges/version.svg)](https://anaconda.org/conda-forge/chemprop)\n[![Build Status](https://github.com/chemprop/chemprop/workflows/tests/badge.svg)](https://github.com/chemprop/chemprop/actions/workflows/tests.yml)\n[![Documentation Status](https://readthedocs.org/projects/chemprop/badge/?version=main)](https://chemprop.readthedocs.io/en/main/?badge=main)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Downloads](https://static.pepy.tech/badge/chemprop)](https://pepy.tech/project/chemprop)\n[![Downloads](https://static.pepy.tech/badge/chemprop/month)](https://pepy.tech/project/chemprop)\n[![Downloads](https://static.pepy.tech/badge/chemprop/week)](https://pepy.tech/project/chemprop)\n\nChemprop is a repository containing message passing neural networks for molecular property prediction.\n\nDocumentation can be found [here](https://chemprop.readthedocs.io/en/main/).\n\nThere are tutorial notebooks in the [`examples/`](https://github.com/chemprop/chemprop/tree/main/examples) directory.\n\nChemprop recently underwent a ground-up rewrite and new major release (v2.0.0). A helpful transition guide from Chemprop v1 to v2 can be found [here](https://docs.google.com/spreadsheets/u/3/d/e/2PACX-1vRshySIknVBBsTs5P18jL4WeqisxDAnDE5VRnzxqYEhYrMe4GLS17w5KeKPw9sged6TmmPZ4eEZSTIy/pubhtml). This includes a side-by-side comparison of CLI argument options, a list of which arguments will be implemented in later versions of v2, and a list of changes to default hyperparameters.\n\n**License:** Chemprop is free to use under the [MIT License](LICENSE.txt). The Chemprop logo is free to use under [CC0 1.0](docs/source/_static/images/logo/LICENSE.txt).\n\n**References**: Please cite the appropriate papers if Chemprop is helpful to your research.\n\n- Chemprop was initially described in the papers [Analyzing Learned Molecular Representations for Property Prediction](https://pubs.acs.org/doi/abs/10.1021/acs.jcim.9b00237) for molecules and [Machine Learning of Reaction Properties via Learned Representations of the Condensed Graph of Reaction](https://doi.org/10.1021/acs.jcim.1c00975) for reactions.\n- The interpretation functionality (available in v1, but not yet implemented in v2) is based on the paper [Multi-Objective Molecule Generation using Interpretable Substructures](https://arxiv.org/abs/2002.03244).\n- Chemprop now has its own dedicated manuscript that describes and benchmarks it in more detail: [Chemprop: A Machine Learning Package for Chemical Property Prediction](https://doi.org/10.1021/acs.jcim.3c01250).\n- A paper describing and benchmarking the changes in v2.0.0 is forthcoming.\n\n**Selected Applications**: Chemprop has been successfully used in the following works.\n\n- [A Deep Learning Approach to Antibiotic Discovery](https://www.cell.com/cell/fulltext/S0092-8674(20)30102-1) - _Cell_ (2020): Chemprop was used to predict antibiotic activity against _E. coli_, leading to the discovery of [Halicin](https://en.wikipedia.org/wiki/Halicin), a novel antibiotic candidate. Model checkpoints are availabile on [Zenodo](https://doi.org/10.5281/zenodo.6527882).\n- [Discovery of a structural class of antibiotics with explainable deep learning](https://www.nature.com/articles/s41586-023-06887-8) - _Nature_ (2023): Identified a structural class of antibiotics selective against methicillin-resistant _S. aureus_ (MRSA) and vancomycin-resistant enterococci using ensembles of Chemprop models, and explained results using Chemprop's interpret method.\n- [ADMET-AI: A machine learning ADMET platform for evaluation of large-scale chemical libraries](https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/btae416/7698030?utm_source=authortollfreelink&utm_campaign=bioinformatics&utm_medium=email&guestAccessKey=f4fca1d2-49ec-4b10-b476-5aea3bf37045): Chemprop was trained on 41 absorption, distribution, metabolism, excretion, and toxicity (ADMET) datasets from the [Therapeutics Data Commons](https://tdcommons.ai). The Chemprop models in ADMET-AI are available both as a web server at [admet.ai.greenstonebio.com](https://admet.ai.greenstonebio.com) and as a Python package at [github.com/swansonk14/admet_ai](https://github.com/swansonk14/admet_ai).\n- A more extensive list of successful Chemprop applications is given in our [2023 paper](https://doi.org/10.1021/acs.jcim.3c01250)\n\n## Version 1.x\n\nFor users who have not yet made the switch to Chemprop v2.0, please reference the following resources.\n\n### v1 Documentation\n\n- Documentation of Chemprop v1 is available [here](https://chemprop.readthedocs.io/en/v1.7.1/). Note that the content of this site is several versions behind the final v1 release (v1.7.1) and does not cover the full scope of features available in chemprop v1.\n- The v1 [README](https://github.com/chemprop/chemprop/blob/v1.7.1/README.md) is the best source for documentation on more recently-added features.\n- Please also see descriptions of all the possible command line arguments in the v1 [`args.py`](https://github.com/chemprop/chemprop/blob/v1.7.1/chemprop/args.py) file.\n\n### v1 Tutorials and Examples\n\n- [Benchmark scripts](https://github.com/chemprop/chemprop_benchmark) - scripts from our 2023 paper, providing examples of many features using Chemprop v1.6.1\n- [ACS Fall 2023 Workshop](https://github.com/chemprop/chemprop-workshop-acs-fall2023) - presentation, interactive demo, exercises on Google Colab with solution key\n- [Google Colab notebook](https://colab.research.google.com/github/chemprop/chemprop/blob/v1.7.1/colab_demo.ipynb) - several examples, intended to be run in Google Colab rather than as a Jupyter notebook on your local machine\n- [nanoHUB tool](https://nanohub.org/resources/chempropdemo/) - a notebook of examples similar to the Colab notebook above, doesn't require any installation\n  - [YouTube video](https://www.youtube.com/watch?v=TeOl5E8Wo2M) - lecture accompanying nanoHUB tool\n- These [slides](https://docs.google.com/presentation/d/14pbd9LTXzfPSJHyXYkfLxnK8Q80LhVnjImg8a3WqCRM/edit?usp=sharing) provide a Chemprop tutorial and highlight additions as of April 28th, 2020\n\n### v1 Known Issues\n\nWe have discontinued support for v1 since v2 has been released, but we still appreciate v1 bug reports and will tag them as [`v1-wontfix`](https://github.com/chemprop/chemprop/issues?q=label%3Av1-wontfix+) so the community can find them easily.\n",1635,chemistry,Python,3,Python,Dockerfile,Shell,,,,,,,,,,,,,,,,,,,,,,,,,,455,68,367,20,7,42,0,782845,553,494,448,46,3213210c3519c7dc4e5ce452a57087f91b524115,Fix CGR featurizer behavior when bond features matrix is empty,2024-07-17T21:35:21Z,Jonathan Zheng,jonzheng@mit.edu,jonwzheng,v2.0.3,"## Notable changes\r\nThe `mfs` argument of `MoleculeDatapoint` was removed in #876. This argument accepted functions which generated molecular features to use as extra datapoint descriptors. When using chemprop in a notebook, users should first manually generate their molecule features and pass them into the datapoints using `x_d` which stands for (extra) datapoint descriptors. This is demonstrated in the `extra_features_descriptors.ipynb` notebook under examples. CLI users will see no change as the CLI will still automatically calculate molecule features using user specified featurizers. The `--features-generators` flag has been deprecated though in favor of the more descriptive `--molecule-featurizers`. Available molecule features can be found in the help text generated by `chemprop train -h`.\r\n\r\n\r\nThe default aggregation was changed to norm in #946. This was meant to be change in version 2.0.0, but got missed. Norm aggregation was used in all the benchmarking of version 1 as it performs better than mean aggregation when predicting properties that are extensive in the number of atoms. \r\n\r\nMore documentation for the CLI `hpopt` and `fingerprint` commands have been added and can be viewed [here](https://chemprop.readthedocs.io/en/main/tutorial/cli/hpopt.html) and [here](https://chemprop.readthedocs.io/en/main/tutorial/cli/fingerprint.html). \r\n\r\nThe individual predictions of an ensemble of models are now automatically averaged and the individual predictions are saved in a separate file. #919\r\n\r\n## What's Changed\r\n* Change the installed numpy version in pyproject by @shihchengli in https://github.com/chemprop/chemprop/pull/922\r\n* Explicitly double save scalers/criterion by @KnathanM in https://github.com/chemprop/chemprop/pull/898\r\n* Add `--show-individual-scores` CLI flag by @shihchengli in https://github.com/chemprop/chemprop/pull/920\r\n* Set Ray Train's trainer resources to 0 by @hwpang in https://github.com/chemprop/chemprop/pull/928\r\n* Save individual and average predictions into different files by @shihchengli in https://github.com/chemprop/chemprop/pull/919\r\n* Add CLI pages for hpopt and fingerprint by @jonwzheng in https://github.com/chemprop/chemprop/pull/914\r\n* Make fingerprint CLI consistent with predict CLI by @hwpang in https://github.com/chemprop/chemprop/pull/927\r\n* Fix issue related to target column for fingerprint by @hwpang in https://github.com/chemprop/chemprop/pull/939\r\n* build molecule featurizer in parsing by @KnathanM in https://github.com/chemprop/chemprop/pull/875\r\n* Remove featurizing from datapoint by @KnathanM in https://github.com/chemprop/chemprop/pull/876\r\n* change aggregation default to norm by @KnathanM in https://github.com/chemprop/chemprop/pull/946\r\n* Use mol.GetBonds() instead of for loop by @KnathanM in https://github.com/chemprop/chemprop/pull/931\r\n\r\n\r\n**Full Changelog**: https://github.com/chemprop/chemprop/compare/v2.0.2...v2.0.3",v2.0.3,Nathan Morgan,,KnathanM,Other,chemprop,chemprop,22,machine-learning,chemistry,neural-networks,drug-discovery,,,,,,,,,,,,,,,,,/chemprop/chemprop,24,37,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/chemfiles/chemfiles,https://github.com/chemfiles/chemfiles,0,,,0,0,0,0,0,0,1,0,0,0,0,Library for reading and writing chemistry files,"## Chemfiles: a library for reading and writing chemistry files\n\n[![Documentation](https://img.shields.io/badge/docs-latest-brightgreen.svg)](http://chemfiles.org/chemfiles/)\n[![Build Status](https://img.shields.io/travis/chemfiles/chemfiles/master.svg)](https://travis-ci.org/chemfiles/chemfiles)\n[![Code Coverage](http://codecov.io/github/chemfiles/chemfiles/coverage.svg?branch=master)](http://codecov.io/github/chemfiles/chemfiles?branch=master)\n[![Gitter](https://badges.gitter.im/chemfiles/chemfiles.svg)](https://gitter.im/chemfiles/chemfiles)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3653157.svg)](https://doi.org/10.5281/zenodo.3653157)\n\nChemfiles is a high-quality library for reading and writing trajectory files\ncreated by computational chemistry simulations programs. To help you access\ninformation (atomic positions, velocities, names, topology, etc.) about these\nfiles, Chemfiles provides a **simple** and **unified** interface to a variety of\nfile formats.\n\n- **unified**: the same code will work with all supported formats;\n- **simple**: the interface is easy to use and extensively documented.\n\nYou can use Chemfiles to conduct post-processing analysis and extract physical\ninformation about the systems you're simulating, to convert files from one\nformat to another, to write trajectories with your own simulation software, and\nanything that requires reading or writing the file formats used in computational\nchemistry.\n\nChemfiles is used in multiple scientific software\n- [cfiles](https://github.com/chemfiles/cfiles) provides ready-to-use analysis\n  algorithms simulations trajectories as a command line tool;\n- [lemon](https://github.com/chopralab/lemon) is a framework for rapidly mining\n  structural information from the Protein Data Bank;\n- [lumol](https://github.com/lumol-org/lumol) is a prototype of universal\n  extensible molecular simulation engine, supporting both molecular dynamics\n  and Metropolis Monte Carlo simulations;\n- [ANA](https://ana.run/) detects cavities, calculates their volume and their\n  flexibility in macromolecular structures and molecular dynamics trajectories;\n\nThis repository contains the core of the chemfiles library — written in C++11,\nwith a C99 interface. You can also use chemfiles from other languages: [Python\n2&3](https://github.com/chemfiles/chemfiles.py),\n[Fortran](https://github.com/chemfiles/chemfiles.f03),\n[Rust](https://github.com/chemfiles/chemfiles.rs), and\n[Julia](https://github.com/chemfiles/chemfiles.jl).\n\n## Quick Links\n\n- [Is chemfiles for you?](#is-chemfiles-for-you)\n- [Main features of chemfiles](#chemfiles-features)\n- [Contact / Contribute / Cite](#contact-contribute-cite)\n- [Getting Started](#getting-started)\n- [Supported File Formats](http://chemfiles.org/chemfiles/latest/formats.html)\n- [Full documentation](http://chemfiles.org/chemfiles/)\n- Documentation for using Chemfiles from various languages:\n    - [Python 2 and 3](http://chemfiles.org/chemfiles.py/)\n    - [Fortran](http://chemfiles.org/chemfiles.f03/)\n    - [C and C++](http://chemfiles.org/chemfiles/)\n    - [Julia](http://chemfiles.org/Chemfiles.jl/)\n    - [Rust](http://chemfiles.org/chemfiles.rs/)\n\n## Is chemfiles for you?\n\nYou might want to use chemfiles if any of these points appeals to you:\n\n- you don't want to spend time writing and debugging a file parser;\n- you use binary formats because they are faster and take up less disk space;\n- you write analysis algorithms and want to read more than one trajectory\n  format;\n- you write simulation software and want to use more than one format for input\n  or output.\n\nThere are [other libraries](http://chemfiles.org/chemfiles/latest/others.html)\ndoing the roughly the same job as chemfiles, have a look at them if chemfiles is\nnot for you. Here we also say why we could not use them instead of creating a\nnew library.\n\n- [OpenBabel](https://openbabel.org/wiki/Main_Page) is a C++ library providing\n  convertions between more than 110 formats. It is more complex than chemfiles,\n  and distributed under the GPL license.\n- [VMD molfile plugins](http://www.ks.uiuc.edu/Research/vmd/) are a collection\n  of plugins witten in C and C++ used by VMD to read/write trajectory files.\n  They do not support a variable number of atoms in a trajectory.\n- [MDTraj](http://mdtraj.org/latest/), [MDAnalyis](http://www.mdanalysis.org/),\n  [cclib](https://cclib.github.io/) are Python libraries providing analysis and\n  read capacities for trajectories. Unfortunely, they are only usable from\n  Python.\n\n## Chemfiles Features\n\n- Reads both text (XYZ, PDB, ...) and binary (NetCDF, TNG, ...) file formats;\n- Transparently read and write compressed files (`.gz`, `.xz` and `.bz2`);\n- Filters atoms with a rich selection language, including constrains on\n  multiple atoms;\n- Supports non-constant numbers of atoms in trajectories;\n- Easy-to-use programming interface in Python, C++, C, Fortran 95, Julia and\n  Rust;\n- Cross-platform and usable from Linux, OS X and Windows;\n- Open source and freely available (3-clauses BSD license);\n\n## Contact / Contribute / Cite\n\nChemfiles is free and open source. Your [contributions](Contributing.md) are\nalways welcome!\n\nIf you have questions or suggestions, or need help, please open an [issue] or\njoin us on our [Gitter] chat room.\n\nIf you are using Chemfiles in a published scientific study, please cite us using\nthe following DOI: https://doi.org/10.5281/zenodo.3653157.\n\n## Getting Started\n\nHere, we'll help you get started with the C++ and C interface. If you want to\nuse Chemfiles with another language, please refer to the corresponding\ndocumentation.\n\n### Installing Compiled Packages\n\nWe provide compiled packages of the latest Chemfiles release for Linux\ndistributions. You can use your package manager to download them\n[here][OSB-download].\n\nWe also provide conda packages in the `conda-forge` community channel for Linux\nand OS X. This package provides the C++, C and Python interfaces. Install the conda package by running:\n\n```\nconda install -c conda-forge chemfiles\n```\n\nFind more information about pre-compiled packages in the [documentation][install].\n\n### Building from Source\n\nYou will need [cmake](http://cmake.org/) and a C++11 compiler.\n\n```bash\ngit clone https://github.com/chemfiles/chemfiles\ncd chemfiles\nmkdir build\ncd build\ncmake ..\nmake\nmake install\n```\n\n### Usage Examples\n\nThis is what the interface looks like in C++:\n\n```cpp\n#include <iostream>\n#include ""chemfiles.hpp""\n\nint main() {\n    chemfiles::Trajectory trajectory(""filename.xyz"");\n\n    auto frame = trajectory.read();\n    std::cout << ""There are "" << frame.size() << "" atoms in the frame"" << std::endl;\n\n    auto positions = frame.positions();\n    // Do awesome science with the positions here !\n}\n```\n\n## License\n\nGuillaume Fraux created and maintains Chemfiles, which is distributed under the\n[3 clauses BSD license](LICENSE). By contributing to Chemfiles, you agree to\ndistribute your contributions under the same license. \n\nChemfiles depends on multiple external libraries, which are distributed under [their\nrespective licenses](external/README.md). All external libraries licenses should\nbe compatible with chemfiles's 3 clauses BSD. One notable execption depending on\nyour use case is [Gemmi](https://gemmi.readthedocs.io) which is distributed\nunder the Mozilla Public License version 2. You can use `CHFL_DISABLE_GEMMI=ON`\nCMake flag to remove this dependency.\n\nThe [AUTHORS](AUTHORS) file lists all contributors to Chemfiles. Many thanks to\nall of them!\n\n[Gitter]: https://gitter.im/chemfiles/chemfiles\n[issue]: https://github.com/chemfiles/chemfiles/issues/new\n[install]: http://chemfiles.org/chemfiles/latest/installation.html\n[OpenSuseBuild]: https://build.opensuse.org/package/show/home:Luthaf/chemfiles\n[OSB-download]: https://software.opensuse.org/download.html?project=home%3ALuthaf&package=chemfiles\n",159,chemistry,C++,5,CMake,C++,Python,Shell,C,,,,,,,,,,,,,,,,,,,,,,,,309,17,288,4,3,15,264,104864,47,182,141,41,9d56df21948a4d4b4d60d312b57b03c5b85d28d9,Fix PDB reading error message,2024-03-13T16:39:32Z,pgbarletta,pbarletta@gmail.com,pgbarletta,Version 0.10.4,"This is a patch update of chemfiles, incorporating some fixes for the new DCD reader/writer (#465, #467); and adding support for GROMACS TPR files (#459).\r\n\r\n",0.10.4,Guillaume Fraux,,Luthaf,"BSD 3-Clause ""New"" or ""Revised"" License",chemfiles,chemfiles,23,computational-chemistry,library,files,compchem,cheminformatics,chemistry,hacktoberfest,,,,,,,,,,,,,,/chemfiles/chemfiles,41,12,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/Chaste/Chaste,https://github.com/Chaste/Chaste,1,,,1,1,1,1,0,0,0,0,0,0,1,Chaste - Cancer Heart And Soft Tissue Environment - main public repository.,"[![DOI](https://joss.theoj.org/papers/10.21105/joss.01848/status.svg)](https://doi.org/10.21105/joss.01848)[![Build chaste/develop](https://github.com/Chaste/Chaste/actions/workflows/docker-develop-image.yml/badge.svg)](https://github.com/Chaste/Chaste/actions/workflows/docker-develop-image.yml)\n\n# Welcome to Chaste\n\nIf you are new to Chaste please see [our Getting Started wiki page](https://chaste.github.io/docs/).\n\nThe files you have downloaded contain the source code for all Chaste functionality. \nChaste makes use of a variety of external libraries and packages that need to be installed on your machine. \nThe [Install Guide webpage](https://chaste.github.io/docs/installguides/) \nprovides a comprehensive guide on how to install these external tools.\n\nChaste is distributed as open source software under the [3-clause BSD licence](https://opensource.org/licenses/BSD-3-Clause). \nFor full details see the file [Copying.pdf](docs/licencing/Copying.pdf).\nChaste uses various third party libraries which have their own licences. \nFor details of these licences and the impact they may have on your use of \nChaste please see [Licences.html](docs/licencing/Licences.html).\n\nChaste includes a complete test suite covering all the source code. \nThe easiest way to use existing source codes is to create a test file \nwhich can call upon any of the source files.  \nThe Chaste build system can build this file for you and handle \nall of the dependencies and library calls.\n\nWe suggest you use the projects directory in this manner to store your own \nsource and test files if you do not wish to modify the Chaste source code. \nFor more information, see the [User Projects webpage](https://chaste.github.io/docs/user-guides/user-projects/).\n\nFor more information please refer to the Chaste website at: http://www.cs.ox.ac.uk/chaste/\n\nInformation on changes in this release can be found on the [release notes webpage](https://chaste.github.io/docs/release-notes/release-notes/).\n\nTutorial examples for this release are available at:\nhttps://chaste.github.io/releases/2024.1/user-tutorials/\n\nAPI documentation generated from the code by Doxygen is available at:\nhttps://chaste.github.io/doxygen-releases/release_2024.1\n\nChaste welcomes contributions from the community.\nFor information on how to contribute to Chaste, and for support and bug reports, please see the file [CONTRIBUTING.md](docs/CONTRIBUTING.md).\n\nA number of external libraries have been created that build on the Chaste trunk code. These include the following:\n * Microvessel Chaste (https://jmsgrogan.github.io/MicrovesselChaste/)\n * ChemChaste (https://github.com/OSS-Lab/ChemChaste)\n\nNote that, while Chaste developers may have been contributed to the development of these external libraries, we are unable to offer any support in their maintenance, testing or usage. If you have any questions about one of these external libraries, please contact that library's lead developer directly.\n",119,hpc-applications,C++,18,CMake,Python,Java,Shell,MATLAB,C++,NetLinx,C,Perl,HTML,Makefile,DIGITAL Command Language,GLSL,Awk,Roff,sed,Dockerfile,Edge,,,,,,,,,,,122,9,106,7,54,38,0,334923,55,141,97,44,7164e4b51c1b3061ca46da3d5ccade266953e68f,Merge pull request #282 from Chaste/267-command-line-tutorial,2024-07-02T09:20:41Z,Jack,44051158+BJackal@users.noreply.github.com,BJackal,Chaste 2024.1,,2024.1,Fergus Cooper,,fcooper8472,Other,Chaste,Chaste,11,mathematical-modelling,mathematical-biology,computational-biology,cell-based,electrophysiology,hpc-applications,physiology,cancer-research,developmental-biology,c-plus-plus,,,,,,,,,,,/Chaste/Chaste,21,17,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/CGNS/CGNS,https://github.com/CGNS/CGNS,0,,,0,0,0,0,0,0,1,0,0,0,0,"The CFD General Notation System (CGNS) provides a standard for recording and recovering computer data associated with the numerical solution of fluid dynamics equations. All development work and bug fixes should be based off the 'develop' branch, CGNS uses the branching model Gitflow.","# CGNS\n[![Actions Status](https://github.com/CGNS/CGNS/workflows/cgns/badge.svg)](https://github.com/CGNS/CGNS/actions)\n[![Build Status: Windows](https://ci.appveyor.com/api/projects/status/jux83kxj0r234oy6?svg=true)](https://ci.appveyor.com/project/brtnfld/cgns)\n\n<a href=""https://scan.coverity.com/projects/cgns-cgns"">\n  <img alt=""Coverity Scan Build Status""\n       src=""https://scan.coverity.com/projects/7923/badge.svg""/>\n</a>\n\n## About\n\nThe CFD General Notation System (CGNS) provides a standard for recording and recovering computer data associated with the numerical solution of fluid dynamics equations.\n\n## Bugs/Feature and issue tracking\n\nhttps://cgnsorg.atlassian.net\n\n## Installation\n\n### Installation Instructions using `cmake`\n\n1. Install HDF5 on your system.\n  \n   - HDF5 can use the standard GNU autotools, so `./configure`, `make`, `sudo make install` should install HDF5 without problems on most systems.\n2. Unpack the tar ball containing the source code into some directory.\n3. Create a new directory in which to build the library.\n4. Use `cmake` to initialize the build tree.\n   ```shell\n   user@hostname:build_path$ cmake /path/to/cgns/sources/\n   ```\n5. Use `ccmake` to edit the control variables as needed.\n   ```shell\n   user@hostname:build_path$ ccmake .\n   ```\n   - The path to the HDF5 library should be specified with `CMAKE_PREFIX_PATH=$HDF_DIR` for linking with a specific HDF5 version.\n     - If HDF5 is built with parallel-IO support via MPI, the `HDF5_NEED_MPI` flag must be set to `true`.\n     - If HDF5 is built with `zlib` and `szip` support, these need to be flagged with `HDF5_NEED_ZLIB` and `HDF5_NEED_SZIP` as well as the paths for those libraries.\n   - Fortran can be enabled by toggling the `CGNS_ENABLE_FORTRAN` variable.\n     - A view of the attempt to autodetect the correct interface between Fortran and C is show, setting the value of `FORTRAN_NAMING`.\n     - For `gfortran` and `pgf90` the value of `FORTRAN_NAMING` should be `LOWERCASE_`.\n   - The build system must be reconfigured after variable changes by pressing `c`. Variables who's value has changed are marked with a `*` in the interface.\n   - After configuration, the `Makefile`s must be generated by pressing `g`.\n6. Use `make` to build the library.\n   ```shell\n   user@hostname:build_path$ make\n   ```\n   - A colorized review of the build process should follow.\n7. Installation of the library is accomplished with the `install` target of the makefile.\n   ```shell\n   user@hostname:build_path$ make install\n   ```\n   - You must have permissions to alter the directory where CGNS will be installed.\n\n### Installation Instructions using `make`\n\n1. Install HDF5 on your system.\n   - HDF5 can use the standard GNU autotools, so `./configure`, `make`, `sudo make install` should install HDF5 without problems on most systems.\n2. Typically the standard `./configure`, `make`, `make install` will suffice.  \n3. Sample scripts for building parallel CGNS can be found in `src/SampleScripts`.\n\n## Usage\n\n## License\n\nThe distribution and use of the CGNS software is covered by the\nfollowing license:\n\n-----------------------------------------------------------------------\nThis software is provided 'as-is', without any express or implied\nwarranty. In no event will the authors be held liable for any damages\narising from the use of this software.\n\nPermission is granted to anyone to use this software for any purpose,\nincluding commercial applications, and to alter it and redistribute it\nfreely, subject to the following restrictions:\n\n1. The origin of this software must not be misrepresented; you must\n   not claim that you wrote the original software. If you use this\n   software in a product, an acknowledgment in the product documentation would be appreciated but is not required.\n\n2. Altered source versions must be plainly marked as such, and must not be misrepresented as being the original software.\n\n3.  This notice may not be removed or altered from any source distribution.\n\n----------------------------------------------------------------------\n\nThis license is borrowed from the zlib/libpng License:\n\n    http://www.opensource.org/licenses/zlib-license.php\n\nand supersedes the GNU Lesser General Public License (LGPL) which\npreviously governed the use and distribution of the software.\n\nFor details on the policy governing the distribution of the CGNS\nstandard and software see:\n\n    http://www.grc.nasa.gov/www/cgns/charter/principles.html\n\n## Development\nCGNS uses the branching/release model as summarized at:\n\nhttp://nvie.com/posts/a-successful-git-branching-model/\n  \n\n![image](https://github.com/CGNS/cgns.github.io/blob/master/git-model.png)\n",218,fluid-dynamics,C,10,CMake,Makefile,Fortran,C,Shell,Tcl,Batchfile,M4,Gnuplot,Pascal,,,,,,,,,,,,,,,,,,,338,55,277,6,18,36,0,19065,100,338,175,163,e3f6c95fe85da67e29c370acc24cd63e5bb4ff21,[pcgns] add poly elements read/parent elements readwrite,2024-07-12T16:17:10Z,ndelling,122032328+ndelling@users.noreply.github.com,ndelling,CGNS version 4.4.0,"<h2>INTRODUCTION</h2>\r\n<p>This document describes the difference between CGNS 4.3.0 and<br>\r\nCGNS 4.4.0 and contains information on known problems in<br>\r\nCGNS 4.4.0.</p>\r\n<p>Links to the CGNS current released source code can be found at:</p>\r\n<p><a href=""http://cgns.org/download.html"" rel=""nofollow"">http://cgns.org/download.html</a></p>\r\n<p>User documentation for the current release can be found at:</p>\r\n<p><a href=""http://cgns.org/CGNS_docs_current/midlevel/index.html"" rel=""nofollow"">http://cgns.org/CGNS_docs_current/midlevel/index.html</a></p>\r\n<p>For more information, see the CGNS home page:</p>\r\n<p><a href=""http://cgns.org"" rel=""nofollow"">http://cgns.org</a></p>\r\n<h2>CONTENTS</h2>\r\n<ul>\r\n<li>New Features</li>\r\n<li>Bug Fixes since CGNS 4.3.0</li>\r\n<li>Known Problems</li>\r\n</ul>\r\n<h1>New Features</h1>\r\n\r\nConfiguration:\r\n-------------\r\nN/A\r\n\r\nLibrary:\r\n--------\r\nN/A\r\n\r\nParallel Library:\r\n-----------------\r\nAdded new parallel APIs  (C and Fortran: cgp_coord_multi_read_data, cgp_coord_multi_write_data, cgp_field_multi_read_data,  cgp_field_multi_write_data, cgp_array_multi_write_data and cgp_array_multi_read_data) which use HDF5's multi-dataset read and write APIs. The new  HDF5 feature (version 1.14.0) allows access to multiple datasets with a single I/O call. The use of these new CGNS routines can improve performance when data is accessed across several datasets from all processes since the data can be aggregated in the HDF5 or MPI-I/O library.\r\n\r\nFortran Library:\r\n----------------\r\nN/A\r\n\r\nTools:\r\n------\r\nN/A\r\n\r\n<h1>Bug Fixes since CGNS 4.3.0 release</h1>\r\n\r\nLibrary:\r\n-------\r\n- Removed possible overflow in cgi_error, #343 #701 \r\n- Increased maximum open file limit default\r\n- Added control for HDF5 tuning routine ""H5Pset_elink_file_cache_size,""  which is useful when many external links are used, #351\r\n- Fixed backward compatibility issues #702:\r\n   (a) Older CGNS files may not have enabled creation ordering in HDF5, causing issues with the number of  Bases returned.\r\n   (b) Fixed H5Fset_libver_bounds for 1.8 HDF5 when trying to read a CGNS file in 1.10 HDF5 file format and added a check which opens, for modifying, a CGNS file that uses HDF5 1.10 format. It will maintain 1.10 format capability since downgrading the file format to HDF5 1.8 requires repacking it (see h5repack from HDF5).\r\n\r\nFortran:\r\n--------\r\n- Removed ""NULL"" and ""UserDefined"" Fortran parameters #349 \r\n\r\n  There is a ""NULL"" function in the Fortran Standard so it could conflict with this definition. Therefore, CG_NULL should be used instead. Older codes need to update to use CG_NULL instead.\r\n  Removed the ""UserDefined"" parameter. CG_UserDefined should be used instead. Older codes need to update to use CG_UserDefined.\r\n\r\nConfiguration:\r\n-------------\r\n\r\n\r\n\r\nTools:\r\n------\r\n\r\n\r\n</code></pre></div>\r\n<h1>Known Problems</h1>\r\n\r\n************ FORTRAN ************\r\n\r\n* A gfortran bug in version 10.2 broke Fortran mapping and caused cg_goto_f \r\n to segfault. All other versions of gfortran are suitable.\r\n (ref. CGNS-246, GNU BUG 100149)\r\n\r\n* A bug in gfortran (all versions) causes cg_configure_f to fail,\r\n  GNU BUG 99982. Other Fortran compilers are OK.\r\n\r\n************ FORTRAN END ********\r\n\r\n************ CGNSVIEW ************\r\n\r\n* cgnsview for OSX is not viewing properly, and cgnsview under Windows \r\n  may fail to compile due to tcl/tk incompatibility. \r\n\r\n************ CGNSVIEW END ********\r\n\r\n* For other issues, see https://github.com/CGNS/CGNS/issues\r\n</code></pre></div>\r\n<h1>Supported Platforms</h1>\r\n<p>The following platforms are supported and have been tested for this release.<br>\r\nThey are built with autotools unless specified otherwise.</p>\r\n<div class=""snippet-clipboard-content position-relative overflow-auto""><pre><code>Linux 3.10.0-1127.10.1.el7    gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)\r\n#1 SMP ppc64 GNU/Linux        g++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)\r\n(echidna)                     GNU Fortran (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)\r\n\r\nLinux 2.6.32-573.18.1.el6     IBM XL C/C++ V13.1\r\n#1 SMP ppc64 GNU/Linux        IBM XL Fortran V15.1\r\n(ostrich)                     \r\n\r\nLinux 3.10.0-327.10.1.el7     GNU C (gcc), Fortran (gfortran), C++ (g++)\r\n#1 SMP x86_64 GNU/Linux       compilers:\r\n(jelly/kituo/moohan)          Version 4.8.5 20150623 (Red Hat 4.8.5-39)\r\n                                 Versions 4.9.3, 5.3.0, 6.3.0, 7.2.0\r\n                                          8.3.0, 9.1.0\r\n                              Intel(R) C (icc), C++ (icpc), Fortran (icc)\r\n                              compilers:\r\n                                 Version 17.0.0.098 Build 20160721\r\n                              MPICH 3.3 compiled with GCC 7.2.0\r\n                              OpenMPI 4.0.0 compiled with GCC 7.2.0\r\n                              NAG Fortran Compiler Release 7.0(Yurakuchho) Build 7011\r\n\r\nSunOS 5.11 11.4.5.12.5.0      Sun C 5.15 SunOS_sparc 2017/05/30\r\n32- and 64-bit                Studio 12.6 Fortran 95 8.8 SunOS_sparc 2017/05/30\r\n(hedgehog)                    Sun C++ 5.15 SunOS_sparc 2017/05/30\r\n\r\nWindows 10 x64                Visual Studio 2015 w/ Intel Fortran 18 (CMake)\r\n                              Visual Studio 2017 w/ Intel Fortran 19 (CMake)\r\n                              Visual Studio 2019 w/ Intel Fortran 19 (CMake)\r\n                              Visual Studio 2019 w/ MSMPI 10.1 (CMake)\r\n\r\nmacOS High Sierra 10.13.6     Apple LLVM version 10.0.0 (clang-1000.10.44.4)\r\n64-bit                        gfortran GNU Fortran (GCC) 6.3.0\r\n(bear)                        Intel icc/icpc/ifort version 19.0.4.233 20190416\r\n\r\nmacOS Mojave 10.14.6          Apple LLVM version 10.0.1 (clang-1001.0.46.4)\r\n64-bit                        gfortran GNU Fortran (GCC) 6.3.0\r\n(bobcat)                      Intel icc/icpc/ifort version 19.0.4.233 20190416\r\n</code></pre></div>\r\n<h1>Tested Configuration Features Summary</h1>\r\n<div class=""snippet-clipboard-content position-relative overflow-auto""><pre><code>In the table below\r\n      y   = tested\r\n      n   = not tested in this release\r\n      x   = not working in this release\r\n</code></pre></div>\r\n\r\n\r\nPlatform | C | C[1] | Fortran | Fortran [1]\r\n-- | -- | -- | -- | --\r\nSunOS 5.11 32-bit | y | n | y | n\r\nSunOS 5.11 64-bit | y | n | y | n\r\nWindows 10 | y | n | n | n\r\nWindows 10 x64 | y | n | n | n\r\nWindows 10 Cygwin | n | n | x | n\r\nMac OS X El Capitan 10.11.6 64-bit | y | n | y | n\r\nMac OS Sierra 10.12.6 64-bit | y | n | y | n\r\nMac OS X High Sierra 10.13.6 64-bit | y | n | y | n\r\nMac OS X Mojave 10.14.6 64-bit | y | n | y | n\r\nCentOS 7.2 Linux 3.10.0 x86_64 PGI | y | n | y | n\r\nCentOS 7.2 Linux 3.10.0 x86_64 GNU | y | y | y | y\r\nCentOS 7.2 Linux 3.10.0 x86_64 Intel | y | y | y | y\r\nLinux 2.6.32-573.18.1.el6.ppc64 | y | n | y | n\r\n\r\n\r\n<p>[1] Parallel",v4.4.0,Scot Breitenfeld,,brtnfld,Other,CGNS,CGNS,15,cgns,hdf5,c,fortran,cfd-general-notation,fluid-dynamics,cfd,parallel-io,,,,,,,,,,,,,/CGNS/CGNS,25,22,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/CGAL/cgal,https://github.com/CGAL/cgal,0,"collection of algos, not really a project",,0,0,1,0,0,0,1,0,0,0,0,"The public CGAL repository, see the README below","![CGAL](Installation/doc_html/images/cgal_2013_grey.png)\n\nThe Computational Geometry Algorithms Library (CGAL) is a C++ library that\naims to provide easy access to efficient and reliable algorithms in\ncomputational geometry.\n\nCGAL Releases\n=============\nThe primary vector of distribution of CGAL are source tarballs, released\ntwice a year, announced on [the web site of CGAL](https://www.cgal.org/).\n\nGetting Started with CGAL\n=========================\n\n**Since version 5.0, CGAL is a header-only library, meaning that\nit is no longer needed to build CGAL libraries before it can be used.**\n\nHead over to the [CGAL manual](https://doc.cgal.org/latest/Manual/general_intro.html)\nfor usage guides and tutorials that will get you started smoothly.\n\nLicense\n=======\nSee the file [LICENSE.md](LICENSE.md).\n\nCGAL Git Repository Layout\n==========================\n\nThe Git repository of CGAL has a different layout from release tarballs. It\ncontains a `CMakeLists.txt` file that serves as anchor for configuring and building programs,\nand a set of subfolders, so called *packages*. Most packages\nimplement a data structure or an algorithm for CGAL (e.g., `Convex_hull_2`,\nor `Triangulation_3`); however some packages serve special needs:\n\n* `Installation` - meta-files and CMake-support\n* `Maintenance` - infrastructural support\n* `Core`, `CGALimageIO`, `Qt_widget`, `GraphicsView` - component libraries\n* `Scripts` - scripts to simplify developer's and user's work\n* `Testsuite` - infrastructure for testsuite\n* `Documentation` - infrastructure for CGAL's manual\n* `STL_Extension` - extensions to the standard template library\n\nMore Information\n================\n* [The CGAL web site](https://www.cgal.org/)\n* [Latest CGAL release documentation pages](https://doc.cgal.org/)\n* [Latest CGAL master documentation pages, updated once a week](https://cgal.geometryfactory.com/CGAL/doc/master/)\n* [CGAL daily testsuite results](https://cgal.geometryfactory.com/CGAL/testsuite/)\n* [Guidelines for CGAL developers](https://github.com/CGAL/cgal/wiki/Guidelines) and [Information for new developers](https://github.com/CGAL/cgal/wiki/Information-for-New-Developers)\n",4708,geometry,C++,24,C++,CMake,Shell,HTML,C,Makefile,TeX,Gnuplot,Lua,Arc,Python,CSS,Perl,Ruby,Batchfile,Lex,Yacc,GLSL,JavaScript,Roff,sed,TypeScript,JetBrains MPS,Pyret,,,,,4975,315,4532,128,4,202,0,688417,1353,3076,2574,502,847df9a0c9cbfee1168b4d3f969e082d431a39c9,better error handling for doxygen testsuite,2024-07-09T09:48:41Z,Laurent Rineau,laurent.rineau@cgal.org,lrineau,CGAL 5.6.1,CGAL-5.6.1 is a bug-fix release.\r\n\r\nSee on Github [the list of bugs that were solved] since [CGAL-5.6].\r\n\r\n[the list of bugs that were solved]: https://github.com/CGAL/cgal/issues?q=sort%3Aupdated-desc+label%3AMerged_in_5.6.1+-label%3AMerged_in_5.6\r\n[CGAL-5.6]: https://github.com/CGAL/cgal/releases/tag/v5.6\r\n,v5.6.1,Laurent Rineau,,lrineau,Other,cgal,CGAL,81,cgal,c-plus-plus,geometry,algorithms,library,cpp,template-library,geometry-processing,computational-geometry,triangulation,mesh-processing,arrange,point-cloud,boolean-operations,polygon,meshes,voronoi-diagram,,,,/CGAL/cgal,147,165,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/cemu-project/cemu_graphic_packs,https://github.com/cemu-project/cemu_graphic_packs,0,,,0,0,0,0,0,0,1,1,0,0,0,Community Graphic Packs for Cemu,"[![Github Actions Build Status Badge](https://github.com/cemu-project/cemu_graphic_packs/workflows/Build%20Process/badge.svg)](https://github.com/cemu-project/cemu_graphic_packs/actions)\n[![Github Releases Badge](https://img.shields.io/github/downloads/cemu-project/cemu_graphic_packs/total.svg)](https://github.com/cemu-project/cemu_graphic_packs/releases/latest)\n[![Contributions Welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/cemu-project/cemu_graphic_packs/issues)\n\n------\n### Information\n------\nCemu Graphic Packs is a repository where you can find graphic packs that can upscale, modify or improve most Wii U games that work on [Cemu](https://github.com/cemu-project/Cemu/) ([Website](https://cemu.info/)).\n\nIt's made by the Cemu community so you're also free to [contribute to the project](https://github.com/cemu-project/cemu_graphic_packs/wiki/How-to-create-resolution-packs) if you wish.\n\n### Downloads\nYou can download the latest graphic packs via Cemu's Graphic Pack window directly or download them manually from [our website](https://cemu-project.github.io/cemu_graphic_packs/)!\n",1275,graphics,Assembly,2,GLSL,Assembly,,,,,,,,,,,,,,,,,,,,,,,,,,,274,56,217,1,1,76,1360,18804,583,348,280,68,5d0254be896804a9dbfa74129947493a1d007fc0,[WWHD] Add 32:9 ultrawide resolutions (#589),2024-07-14T09:14:50Z,victoria4dx,99297572+victoria4dx@users.noreply.github.com,victoria4dx,Cemu Graphic Packs: v913,Commited at 2024-07-14 11:14:50 +0200 by **GitHub** in commit 5d0254b\n### [WWHD] Add 32:9 ultrawide resolutions (#589)\n```\nAdded 32:9 superwide monitor resolutions\n```,Github913,,,github-actions[bot],Creative Commons Zero v1.0 Universal,cemu_graphic_packs,cemu-project,163,cemu,graphics,graphic-packs,cemu-emulator,emulator,game,cemu-folder,emulation,wii-u,wiiu,wiiu-emulator,wii-u-emulator,wii-u-emulation,wiiu-emulation,,,,,,,/cemu-project/cemu_graphic_packs,659,74,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/CelestiaProject/Celestia,https://github.com/CelestiaProject/Celestia,0.5,"Visualization of space, not sure about scientific?",0,0,0,1,0,0,0,0,1,0,0,0,Real-time 3D visualization of space.,"| **`Release`** | **`Localized`** | **`License`** | **`Contribute`** |\n|-------------------|---------------|---------------|---------------|\n|[![GitHub release](https://img.shields.io/github/v/release/CelestiaProject/Celestia?label=Release)](https://celestiaproject.space/download.html) | [![Localization](https://img.shields.io/badge/Localized-85%25-green.svg)](#) | [![License](https://img.shields.io/github/license/CelestiaProject/Celestia?label=License)](https://github.com/CelestiaProject/Celestia/blob/master/COPYING) | [![Contribute](https://img.shields.io/badge/PRs-Welcome-brightgreen.svg)](#contributing) |\n\n# Celestia\n![Celestia](celestia-logo.png)<br>\n**A real-time space simulation that lets you experience our universe in three dimensions.**\n\n**Copyright © 2001-2023, Celestia Development Team**<br>\n**Celestia website: https://celestiaproject.space**<br>\n**Celestia Wikibook: https://en.wikibooks.org/wiki/Celestia**<br>\n**Celestia forums: https://celestiaproject.space/forum/**<br>\n**Celestia Subreddit: https://www.reddit.com/r/Celestiasoftware/**<br>\n**Celestia Archive Repository: https://github.com/Anthony-B-Russo10/Celestia-Archive**\n## License\n\nThis program is free software; you can redistribute it and/or modify it under\nthe terms of the GNU General Public License as published by the Free Software Foundation;\neither version 2 of the License, or (at your option) any later version.\n\nThis program is distributed in the hope that it will be useful, but WITHOUT\nANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\nFOR A PARTICULAR PURPOSE. See the GNU General Public License for more details,\nwhich you should have received along with this program (filename: COPYING).\nIf not, request a copy from:<br>\nFree Software Foundation, Inc.<br>\n59 Temple Place - Suite 330<br>\nBoston, MA  02111-1307<br>\nUSA\n\n## Getting started\n\nCelestia will start up in a window, and if everything is working correctly,\nyou'll see Earth in front of a field of stars.  Displayed on-screen, is some\ninformation about your target (Earth), your speed, and the current time\n(Universal Time, so it'll probably be a few hours off from your computer's\nclock).\n\nRight drag the mouse to orbit Earth and you might see the Moon and some\nfamiliar constellations.  Left dragging the mouse changes your orientation\nalso, but the camera rotates about its center instead of rotating around\nEarth.  Rolling the mouse wheel will change your distance to Earth--you can\nmove light years away, then roll the wheel in the opposite direction to get\nback to your starting location.  If your mouse lacks a wheel, you can use the\nHome and End keys instead.\n\nWhen running Celestia, you will usually have some object selected.  Currently,\nit's Earth, but it could also be a star, moon, spacecraft, galaxy, or some\nother object.  The simplest way to select an object is to click on it.  Try\nclicking on a star to select it.  The information about Earth is replaced with\nsome details about the star.  Press G (or use the Navigation menu), and you'll\nzoom through space toward the selected star.  If you press G again, you'll\napproach the star even closer.\n\nPress H to select our Sun, and then G to go back to our Sun.  Right click on\nthe sun to bring up a menu of planets and other objects in the solar system.\nAfter selecting a planet from the menu, hit G again to travel toward it.  Once\nthere, hold down the right mouse button and drag to orbit the planet.\n\nThe Tour Guide is a list of some of the more interesting objects you can visit\nin Celestia.  Select the Tour Guide option in the Navigation menu to display\nthe Tour Guide window.  Choose a destination from the list, click the Goto\nbutton, and you're off.\n\nThat covers the very basics.  For a more in-depth look at Celestia and the\ncontrols available to you, download the ""Celestia User's Guide"" (written by\nFrank Gregorio), available in several languages, from:<br>\n  https://celestiaproject.space/guides.html<br>\nThis web page also includes links to the Celestia README file translated into\nJapanese.\n\n### Star browser\nBy default, the Star Browser window displays a table of the 100 nearest stars,\nalong with their Distance, Apparent and Absolute Magnitude, and Type. Clicking\non the column headers will sort the stars.  The table is not continuously\nupdated, so if you travel to another star, you should press the Refresh button\nto update the table for your current position.  The radio buttons beneath the\ntable let you switch between viewing a list of Nearest, Brightest, or 'With\nplanets' stars.  As with the solar system browser, clicking on any star name\nin the table will select it.  Use this feature along with the Center and Go\nTo buttons to tour the stars visible from any night sky in the galaxy.\n\n### Solar system browser\nThe Solar System Browser displays a window with a tree view of all the objects\nin the nearest solar system (if there is one within a light year of your current\nposition.)  Clicking on the name of any object in the window will select it.\nYou can then use the Center or Go To buttons to display that object in the main\nCelestia window.\n\n### Selecting objects by name\nCelestia provides several ways to select an object by name...\n1. Choose 'Select Object' from the Navigation menu, type in the object name, and click OK.\n2. Press Enter, type in the entire object name, and press Enter again.\n3. Press Enter, type in the first few characters of the object name,\npress the Tab key to move through the displayed listing until the object is highlighted,\nthen press Enter again.\n\nYou can use common names, Bayer designations or catalog numbers for stars.\nCelestia currently supports the HIP, HD and SAO catalogs. Catalog numbers must\nbe entered with a space between the prefix and the catalog number.\n\n### Known issues\nFor up-to-the-minute answers to some common problems encountered when running\nCelestia, please view either the FAQ in the Help menu or take a look at the\n""Celestia User's FAQ"" located on the Celestia User's Forum:\nhttps://celestiaproject.space/forum/\n\n### User modifiable elements\nYou can modify how Celestia starts up each time you run it, by defining your\nown start-up settings.  Simply open the file ""start.cel"" in a plain text\neditor and follow the in-file instructions.  Also, view the celestia.cfg file\nin a plain text editor to see additional settings.\n\nCelestia allows you to easily add real, hypothetical, or fictional objects\nby creating new catalog files. It is *not* recommended that you alter the\nbuilt-in data files; nearly all desired modifications and additions can be\nmade by placing new catalog files in Celestia's extras folders. There are three\ntypes of catalog files:\n* ssc (solar system catalog: planets, moons, spacecraft, etc.)\n* stc (star catalog)\n* dsc (deep sky catalog: galaxies, star clusters, and nebulae)\n\nAll three types of catalog file are text files that can be updated with your\nfavorite text editing program.\n\n### Building from sources\nSee instructions in file [INSTALL.md](INSTALL.md).\n\n## Contributions\n| **`Authors`** | **`Contributors`** | **`Documentation`** | **`Other`** |\n|-----------------|---------------------|------------------|-------------------|\n| Chris Laurel, Clint Weisbrod, Fridger Schrempp, Bob Ippolito, Christophe Teyssier, Hank Ramsey, Grant Hutchison, Pat Suwalski, Toti, Da Woon Jung, Vincent Giangiulio, Andrew Tribick, Hleb Valoshka, Łukasz Buczyński, Li Linfeng | Deon Ramsey, Christopher Andre, Colin Walters, Peter Chapman, James Holmes, Harald Schmidt, Nils Larsson, Sergey Leonov, Alexell, Dmitry Brant, Janus | Selden Ball, Frank Gregorio, Hitoshi Suzuki, Christophe Teyssier, Diego Rodriguez, Don Goyette, Harald Schmidt | Creators of scientific database, texture maps, 3D models and used libraries, you can see in full README.|\n\n### Contributing\n\n**We welcome feedback, bug reports, and pull requests!**\n\nFor pull requests, please stick to the following guidelines:\n* Be sure to test your code changes.\n* Follow the existing code style (e.g., indents).\n* Put a lot of comments into the code, if necessary.\n* Separate unrelated changes into multiple pull requests.\n",1769,astronomy,C++,16,Makefile,Perl,Shell,Inno Setup,HTML,C,C++,Batchfile,CMake,GLSL,Objective-C++,CSS,PowerShell,Roff,Python,POV-Ray SDL,,,,,,,,,,,,,1653,279,1357,17,21,48,1,175613,301,545,409,136,26db10ead8479a7ed0bc185b6b157a9a65942256,Split DSODatabaseBuilder and DynamicOctree into separate files,2024-07-14T21:30:08Z,Andrew Tribick,ajtribick@googlemail.com,ajtribick,,- Update site URL to [celestiaproject.space](https://celestiaproject.space)\r\n- Fix building with Lua 5.4,01.06.2004,Hleb Valoshka,,375gnu,GNU General Public License v2.0,Celestia,CelestiaProject,8,celestia,astronomy,planetarium,educational,free-software,open-source,opengl,opengl-es,space,,,,,,,,,,,,/CelestiaProject/Celestia,24,62,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/cds-astro/ipyaladin,https://github.com/cds-astro/ipyaladin,0,,0,0,0,0,0,0,0,1,1,0,0,0,"An IPython Widget for Aladin Lite, the sky viewer.","# ipyaladin\n\nA bridge between Jupyter and Aladin Lite, enabling interactive sky visualization in IPython notebooks.\nWith a couple of lines, you can display Aladin Lite, center it on the target of your choice, and overlay an Astropy table:\n\n![ipyaladin example](assets/ipyaladin-screencast.gif)\n\n- [ipyaladin](#ipyaladin)\n  - [Examples](#examples)\n  - [Installation](#installation)\n  - [Development installation](#development-installation)\n  - [How does it work?](#how-does-it-work)\n  - [Acknowledging ipyaladin](#acknowledging-ipyaladin)\n\n## Examples\n\nSome example notebooks can be found in the [examples directory](examples).\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/cds-astro/ipyaladin/master). You can also try it directly [in mybinder](https://mybinder.org/v2/gh/cds-astro/ipyaladin/master), without installing anything.\n\n## Installation\n\nTo install use pip or conda :\n\n```shell\n> pip install ipyaladin\n```\n\nYou can already try to load ipyaladin in a notebook.\n\n```python\nfrom ipyaladin import Aladin\naladin = Aladin()\naladin\n```\n\n## Development installation\n\nFirst, make sure you have installed jupyter in your python environnement: `pip install jupyter`.\nFor a development installation [Node.js](https://nodejs.org) and [Yarn version 1](https://classic.yarnpkg.com/) are also required,\n\n```shell\n> git clone https://github.com/cds-astro/ipyaladin.git\n> cd ipyaladin\n> npm install\n> npm run dev\n```\n\nAnd you are ready to develop! Any change done in the python, javascript, or css files should\nbe directly reflected in the notebook editor of your choice (JupyterLab, VSCode,...)!\n\n## How does it work?\n\nIpyaladin brings [Aladin Lite](https://github.com/cds-astro/aladin-lite) into notebooks thanks to\n[Anywidget](https://anywidget.dev/).\n\nCorrespondence table between ipyaladin versions and Aladin Lite versions:\n\n| ipyaladin | Aladin-Lite |\n| --------- | ----------- |\n| 0.3.0     | 3.3.3-dev   |\n| 0.4.0     | 3.4.4-beta  |\n\n> [!TIP]\n> This can always be read like so\n>\n> ```python\n> from ipyaladin import __version__, __aladin_lite_version__\n> print(""version:"", __version__, ""running Aladin Lite:"", __aladin_lite_version__)\n> ```\n>\n> ```\n> version: 0.4.0 running Aladin Lite: 3.4.4-beta\n> ```\n\n## Acknowledging ipyaladin\n\nIf you use ipyaladin for your work or research, we kindly ask you to cite it with the following acknowledgment:\n\n> This research made use of ipyaladin, developed by CDS, Strasbourg Astronomical Observatory, France (DOI: [10.26093/kpaw-kb74](https://doi.org/10.26093/kpaw-kb74) ).\n>\n> [2020ASPC..522..117B](https://ui.adsabs.harvard.edu/abs/2020ASPC..522..117B) - ipyaladin: Enabling Aladin Lite in Jupyter Notebooks (Boch T. et al.)\n",117,astronomy,Python,4,Shell,Python,JavaScript,CSS,,,,,,,,,,,,,,,,,,,,,,,,,34,7,22,5,1,6,0,36741,24,66,47,19,1a61e29e22ac48dec78288bb9a3d46217ba13292,feat: adding FITS to the view with new add_fits method,2024-07-16T13:39:51Z,Tom Czekaj,47594493+Xen0Xys@users.noreply.github.com,Xen0Xys,v0.4.0,"## What's Changed\r\n\r\n### Added\r\n\r\n- attribute `__aladin_lite_version__` added to point to the corresponding Aladin Lite released version\r\n- Support for `astropy.coordinates.SkyCoord` for assigning and reading the `target` property (#80)\r\n- Support for `astropy.coordinates.Angle` for reading the `fov` property (#83)\r\n- Support for `regions.LineSkyRegion`, `regions.CircleSkyRegion`, `regions.EllipseSkyRegion`, `regions.PolygonSkyRegion`, `regions.RectangleSkyRegion`, `regions.Regions` with `add_graphic_overlay_from_region` (#88)\r\n\r\n### Fixed\r\n\r\n- `clicked_object` was not properly updated after a click\r\n- Fix asynchronous update for the `target` property (#80)\r\n- some options were not accepted in snake_case anymore in `add_moc` and in `add_catalog_from_url` (#82)\r\n\r\n### Changed\r\n\r\n- Change the jslink target trait from `target` to `shared_target` (#80)\r\n- Change the jslink fov trait from `fov` to `shared_fov` (#83)\r\n- Upgrade Aladin Lite version to 3.4.1-beta (#88)\r\n- Add support for list of strings in `add_overlay_from_stcs` (#88)\r\n\r\n### Deprecated\r\n\r\n- Deprecate `add_overlay_from_stcs` in favor of `add_graphic_overlay_from_stcs` (#88)\r\n- Deprecate the `add_listener` method for a preferred use of `set_listener` method (#82)\r\n\r\n**Full Changelog**: https://github.com/cds-astro/ipyaladin/compare/v0.3.0...v0.4.0\r\n\r\n## Contributions\r\n\r\n@Xen0Xys made their first contribution in this release :sun_with_face: ",v0.4.0,Manon Marchand,,ManonMarchand,"BSD 3-Clause ""New"" or ""Revised"" License",ipyaladin,cds-astro,7,aladin,astronomy,astrophysics,ipywidgets,jupyterlab-extension,python,visualization,,,,,,,,,,,,,,/cds-astro/ipyaladin,8,15,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/cdk/cdk,https://github.com/cdk/cdk,1,,,1,1,1,1,0,0,0,0,0,0,1,The Chemistry Development Kit,"[![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.openscience.cdk/cdk/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.openscience.cdk/cdk) [![build](https://github.com/cdk/cdk/actions/workflows/maven.yml/badge.svg)](https://github.com/cdk/cdk/actions/workflows/maven.yml) [![Bugs](https://sonarcloud.io/api/project_badges/measure?project=cdk&metric=bugs)](https://sonarcloud.io/summary/overall?id=cdk)\n\n\n# The Chemistry Development Kit (CDK)\n \nCopyright &copy; 1997-2024 The CDK Development Team\n\nLicense: LGPL v2, see LICENSE.txt\n\n[Home Page](https://cdk.github.io/) | [JavaDoc](http://cdk.github.io/cdk/latest/docs/api/index.html?overview-summary.html) | [Wiki](https://github.com/cdk/cdk/wiki) | [Issues](https://github.com/cdk/cdk/issues) | [Mailing List](https://sourceforge.net/projects/cdk/lists/cdk-user)\n\n## Introduction\n\nThe CDK is an open-source Java library for cheminformatics and bioinformatics.\n\nKey Features:\n  * Molecule and reaction valence bond representation.\n  * Read and write file formats: SMILES, SDF, InChI, Mol2, CML, and others.\n  * Efficient molecule processing algorithms: Ring Finding, Kekulisation, Aromaticity.\n  * Coordinate generation and rendering.\n  * Canonical identifiers for fast exact searching.\n  * Substructure and SMARTS pattern searching.\n  * ECFP, Daylight, MACCS, and other fingerprint methods for similarity searching.\n  * QSAR descriptor calculations\n\n## Install\n\nThe CDK is a class library intended to be used by other programs, it will not run as a stand-alone program. \n\nThe library is built with Apache Maven and currently requires Java 1.7 or later. From the root of the project run to build the JAR files for each module. The ``bundle/target/`` directory contains the main JAR with all dependencies included:\n\n```bash\n$ mvn install\n```\n\nYou can also download a pre-built library JAR from [releases](https://github.com/cdk/cdk/releases). \n\nInclude the main JAR on the Java classpath when compiling and running your code:\n\n```bash\n$ javac -cp cdk-2.9.jar MyClass.java\n$ java -cp cdk-2.9.jar:. MyClass\n```\n\nIf you are using Maven, you can use the **uber** ``cdk-bundle`` to grab \neverything, note it is much more efficient to use include the modules you need:\n\n```xml\n<dependency>\n  <artifactId>cdk-bundle</artifactId>\n  <groupId>org.openscience.cdk</groupId>\n  <version>2.9</version>\n</dependency>\n```\n\nIf you are a Python user, the Cinfony project provides access via [Jython](http://www.redbrick.dcu.ie/~noel/CDKJython.html).\nNoel O'Boyle's [Cinfony](http://cinfony.github.io/) provides a wrapper around the CDK and over toolkits exposing core\nfunctionality as a consistent API. `ScyJava` can also be used, as explain in [ChemPyFormatics](https://egonw.github.io/chempyformatics/).\n\nFurther details on building the project in integrated development environments (IDEs) are available on the wiki:\n * [Building the CDK](https://github.com/cdk/cdk/wiki/Building-CDK)\n * [Maven Reporting Plugins](https://github.com/cdk/cdk/wiki/Maven-Reporting-Plugins)\n\n## Getting Help\n\nThe [Toolkit-Rosetta Wiki Page](https://github.com/cdk/cdk/wiki/Toolkit-Rosetta) provides some examples for common tasks. If you need help using the CDK and have questions please use the user mailing list, [``cdk-user@lists.sf.net``](mailto:cdk-user@lists.sf.net) (**you must [subscribe here]( https://sourceforge.net/projects/cdk/lists/cdk-user) first to post**).\n \n## Acknowledgments\n\n![YourKit Logo](https://www.yourkit.com/images/yklogo.png)\n\nThe CDK developers use YourKit to profile and optimise code.\n\nYourKit supports open source projects with its full-featured Java Profiler.\nYourKit, LLC is the creator of <a href=""https://www.yourkit.com/java/profiler/index.jsp"">YourKit Java Profiler</a>\nand <a href=""https://www.yourkit.com/.net/profiler/index.jsp"">YourKit .NET Profiler</a>,\ninnovative and intelligent tools for profiling Java and .NET applications.\n",479,bioinformatics,Java,3,Java,HTML,Pawn,,,,,,,,,,,,,,,,,,,,,,,,,,809,203,598,8,81,68,29,247427,155,284,253,31,2ca277409fb73e947fa6a540c54309d77a0e6dcf,"Actually use the new smart ""requires"" flag.",2024-07-16T17:31:12Z,John Mayfield,john@nextmovesoftware.com,johnmay,CDK 2.9,"[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.8270947.svg)](https://doi.org/10.5281/zenodo.8270947)\r\n\r\n# Summary\r\n\r\n- Improved abbreviation handling\r\n- More arrow types\r\n- Multi-step Reaction SMILES\r\n- Reaction Set and Multi-step depiction\r\n- More correct PubChemFingerprinter\r\n- Universal (InChI) SMILES for large molecules\r\n- Dependency updates and stability improvements, huge kudos to @uli-f for finding some longstanding issues\r\n\r\n## Improved abbreviation handling\r\n\r\n#991. The Abbreviation handling has been tweaked with more and cleaner options:\r\n\r\n```java\r\nAbbreviations  abbreviations = new Abbreviations();\r\n// abbreviations.setContractToSingleLabel(true); // old (still supported)\r\nabbreviations.with(Abbreviations.Option.ALLOW_SINGLETON); // new\r\n// abbreviations.setContractOnHetero(true); // old (still supported)\r\nabbreviations.with(Abbreviations.Option.AUTO_CONTRACT_HETERO); // new\r\n```\r\n\r\nThe full options are described here: [Abbreviations.Option](http://cdk.github.io/cdk/latest/docs/api/org/openscience/cdk/depict/Abbreviations.Option.html).\r\n\r\n## More arrow types\r\n\r\nNow includes NoGo/Equilibrium/RetroSynthetic - #927. See [IReaction.Direction](http://cdk.github.io/cdk/latest/docs/api/org/openscience/cdk/interfaces/IReaction.Direction.html). Examples:\r\n\r\n![#1 (2)](https://github.com/cdk/cdk/assets/983232/968004e7-1747-4bc0-950d-1adbf5d08991)\r\n\r\n![#1 (3)](https://github.com/cdk/cdk/assets/983232/18db4d41-d93d-4ce1-adc9-89367378d98c)\r\n\r\n## Multi-step Reaction SMILES\r\n\r\nhttps://github.com/cdk/cdk/pull/986\r\n\r\nAn new entry point to the SMILES parser has been added to parse into a ""multi-step"" reaction where by the product of one step is the reactant the the next. The basic idea is to allow more than two '>'. Parts at even positions are reactants/products and odd positions are agents/catalysts/solvents.\r\n\r\nBasic idea:\r\n\r\n```java\r\nSmilesParser sp = new SmilesParser(SilentChemObjectBuilder.getInstance());\r\nIReactionSet rset = sp.parseReactionSetSmiles(""[Pb]>>[Ag]>>[Au] lead-to-silver-to-gold"");\r\n```\r\n\r\nReal example (see next bullet for depiction): \r\n\r\n```\r\nClC1=NC=2N(C(=C1)N(CC3=CC=CC=C3)CC4=CC=CC=C4)N=CC2C(OCC)=O>C1(=CC(=CC(=N1)C)N)N2C[C@H](CCC2)O.O1CCOCC1.CC1(C2=C(C(=CC=C2)P(C3=CC=CC=C3)C4=CC=CC=C4)OC5=C(C=CC=C15)P(C6=CC=CC=C6)C7=CC=CC=C7)C.C=1C=CC(=CC1)\C=C\C(=O)\C=C\C2=CC=CC=C2.C=1C=CC(=CC1)\C=C\C(=O)\C=C\C2=CC=CC=C2.C=1C=CC(=CC1)\C=C\C(=O)\C=C\C2=CC=CC=C2.[Pd].[Pd].[Cs]OC(=O)O[Cs]>C1(=CC(=CC(=N1)C)NC2=NC=3N(C(=C2)N(CC4=CC=CC=C4)CC5=CC=CC=C5)N=CC3C(OCC)=O)N6C[C@H](CCC6)O>CO.C1CCOC1.O.O[Li]>C1(=CC(=CC(=N1)C)NC2=NC=3N(C(=C2)N(CC4=CC=CC=C4)CC5=CC=CC=C5)N=CC3C(O)=O)N6C[C@H](CCC6)O>CN(C)C(=[N+](C)C)ON1C2=C(C=CC=N2)N=N1.F[P-](F)(F)(F)(F)F.[NH4+].[Cl-].CN(C)C=O.CCN(C(C)C)C(C)C>C1(=CC(=CC(=N1)C)NC2=NC=3N(C(=C2)N(CC4=CC=CC=C4)CC5=CC=CC=C5)N=CC3C(N)=O)N6C[C@H](CCC6)O>>C1(=CC(=CC(=N1)C)NC2=NC=3N(C(=C2)N)N=CC3C(N)=O)N4C[C@H](CCC4)O |f:4.5.6.7.8,16.17,18.19|  US20190241576A1\r\n```\r\n## Reaction Set and Multi-step depiction\r\n\r\nhttps://github.com/cdk/cdk/pull/986\r\n\r\nThe [``DepictionGenerator``](http://cdk.github.io/cdk/latest/docs/api/org/openscience/cdk/depict/DepictionGenerator.html#depict(org.openscience.cdk.interfaces.IReactionSet)) has been extended to depict reaction sets. If the product of the previous reaction is the same as the reactant in the next (object identity) it is omitted for a terser depiction:\r\n\r\n![US20190241576A1 (3)](https://github.com/cdk/cdk/assets/983232/b7585122-1816-4007-a647-863a852f1dc8)\r\n\r\n## More correct PubChemFingerprinter\r\n\r\nExplicit hydrogens are not longer required and there is an option to use a more correct ring set definition matching closer the original CACTVS substructure keys. This is now on by default:\r\n\r\n```\r\nIChemObject builder = SilentChemObjectBuilder.getInstance();\r\nnew PubchemFingerprinter(builder); // new - default is to use ""ESSSR-like"" ring set\r\nnew PubchemFingerprinter(builder, false); // old - for backwards compatible with FP generated with older CDK versions\r\n```\r\n\r\n## Universal (InChI) SMILES for large molecules\r\n\r\n#979.\r\n\r\nThe InChI now supports > 999 atoms, we have the option to generate a SMILES using the InChI canonical labelling, it makes sense to use the larger molecules flag and support more. \r\n\r\n# New Contributors\r\n\r\n* @Mailaender made their first contribution in https://github.com/cdk/cdk/pull/934\r\n* @parit made their first contribution in https://github.com/cdk/cdk/pull/980\r\n\r\n# All Contributors\r\n\r\n```\r\n  75 John Mayfield\r\n  17 Egon Willighagen\r\n   6 Uli Fechner\r\n   4 Mark J. Williamson\r\n   3 Mark Williamson\r\n   1 Parit Bansal\r\n   1 Matthias Mailänder\r\n```\r\n\r\n**Full Changelog**: https://github.com/cdk/cdk/compare/cdk-2.8...cdk-2.9",cdk-2.9,John Mayfield,,johnmay,GNU Lesser General Public License v2.1,cdk,cdk,20,code4lib,chemistry,java,cheminformatics,bioinformatics,blueobelisk,,,,,,,,,,,,,,,/cdk/cdk,75,41,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/cBioPortal/cbioportal,https://github.com/cBioPortal/cbioportal,0,,,0,1,0,0,0,0,0,0,0,0,0,cBioPortal for Cancer Genomics,"# cBioPortal\n\nThe cBioPortal for Cancer Genomics provides visualization, analysis, and download of large-scale cancer genomics data sets. For a short intro on cBioPortal, see [these introductory slides](https://docs.google.com/presentation/d/1hm0G77UklZnpQfFvywBfW2ZIsy8deKi5r1RfJarOPLg/edit?usp=sharing).\n\nIf you would like to know how to setup a private instance of the portal and/or get set up for developing, see the [documentation](https://docs.cbioportal.org). For details on contributing code changes via pull requests, see our [Contributing document](CONTRIBUTING.md).\n\nIf you are interested in coordinating the development of new features, please contact cbioportal@cbioportal.org or reach out on https://slack.cbioportal.org.\n\n## 📘 Documentation\nSee [https://docs.cbioportal.org](https://docs.cbioportal.org)\n\n## 🤝 License\nSee [LICENSE](./LICENSE)\n\n## 💻 Run Backend\ncBioPortal consists of several components, please read the [Architecture docs](https://docs.cbioportal.org/architecture-overview/) to figure out what repo would be relevant to edit. If you e.g. only want to make frontend changes, one can directly edit [the frontend repo](https://github.com/cbioportal/cbioportal-frontend) instead. Read the instructions in that repo for more info on how to do frontend development. This repo only contains the backend part. Before editing the backend, it's good to read the [backend code organization](docs/Backend-Code-Organization.md).\n\n### Local Development\n#### What MySQL database to use\nWe recommend to set up a MySQL database using [Docker Compose](https://github.com/cBioPortal/cbioportal-docker-compose). It's useful to know how to do this as it allows you to import any dataset of your choice. For debugging production issues, we also have a database available with all the data on https://cbioportal.org that one can connect to directly. Please reach out on slack to get the credentials.\n\n#### Command Line\nIf you want to run the cBioPortal web app from the command line please follow these instructions. First, we want to make sure that all ports are open for the services set up through [docker compose](https://github.com/cBioPortal/cbioportal-docker-compose) (i.e. not just accessible to other containers within the same Docker Compose file). To do so, in the [docker compose repo](https://github.com/cBioPortal/cbioportal-docker-compose) run:\n\n```\ndocker compose -f docker-compose.yml -f open-ports.yml up\n```\nThis should open the ports. Now we are ready to run the cBioPortal web app locally. You can compile the backend code with:\n\n```\n\njava -Xms2g -Xmx4g \\n     -Dauthenticate=noauthsessionservice \\n     -Dsession.service.url=http://localhost:5000/api/sessions/my_portal/ \\n     -Dsession.service.origin='*' \\n     -Dspring.datasource.username=cbio_user \\n     -Dspring.datasource.password=somepassword \\n     -Dspring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver \\n     -Dspring.jpa.database-platform=org.hibernate.dialect.MySQL5InnoDBDialect \\n     -Dspring.datasource.url='jdbc:mysql://cbio_user:somepassword@localhost:3306/cbioportal?useSSL=false&allowPublicKeyRetrieval=true' \\n     -Dshow.civic=true \\n     -Dskin.footer='' \\n     -Dapp.name='my-portal' \\n     -Ddbconnector=dbcp \\n     -cp ""$PWD:$PWD/BOOT-INF/lib/*"" \\n     org.cbioportal.PortalApplication\n```\n\nThe app should now show up at http://localhost:8080.\n\n#### Deploy your development image inside Docker Compose\nAnother option is to deploy your development image directly into the [docker-compose](https://github.com/cBioPortal/cbioportal-docker-compose/blob/5da068f0eb9b4f42db52ab5e91321b26a1826d7a/docker-compose.yml#L6) file. First build the image like this\n\n```\ndocker build -t cbioportal/cbioportal:my-dev-cbioportal-image -f docker/web-and-data/Dockerfile .\n```\n\nThen change the [env file](https://github.com/cBioPortal/cbioportal-docker-compose/blob/master/.env) to use `cbioportal/cbioportal:my-dev-cbioportal-image`.\n\n### Local Development\n\n\nNote: internally we have a dev database available with the public data set that one can connect to directly. Please reach out on slack to get the credentials. It is usually best to use a small test dataset, but if a copy of the production database is necessary for e.g. fixing a bug specific to production data that can be useful.\n\n### 🕵️‍♀️ Debugging\n\nIf you want to attach a debugger you can change the `docker-compose.yml` file to include the parameters: `-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=5005` (make sure to expose the debug port by adding `5005:5005` in the ports section of the cbioportal container). If you are running the java app outside of docker you can add the same parameters to the java command line arguments instead.\n\nYou can then use a JAVA IDE to connect to that port. E.g. in [VSCode](https://code.visualstudio.com/), one would add the following configuration to `launch.json` to connect:\n\n```\n{\n    ""version"": ""0.2.0"",\n    ""configurations"": [\n        {\n            ""type"": ""java"",\n            ""name"": ""Debug (Attach)"",\n            ""request"": ""attach"",\n            ""hostName"": ""localhost"",\n            ""port"": 5005,\n            ""projectName"": ""cbioportal""\n        }\n    ]\n}\n```\n\n## 🌳 Branch Information\n\n| | main branch | upcoming release branch | later release candidate branch |\n| --- | --- | --- | --- |\n| Branch name | [`master`](https://github.com/cBioPortal/cbioportal/tree/master) |  -- |  [`rc`](https://github.com/cBioPortal/cbioportal/tree/rc) |\n| Description | All bug fixes and features not requiring database migrations go here. This code is either already in production or will be released this week | Next release that requires database migrations. Thorough manual product review often takes place for this branch before release | Later releases with features that require database migrations. This is useful to allow merging in new features without affecting the upcoming release. Could be seen as a development branch, but note that only high quality pull requests are merged. That is the feature should be pretty much ready for release after merge. |\n| Live instance | https://www.cbioportal.org / https://master.cbioportal.org | -- | https://rc.cbioportal.org |\n| Live instance version | https://www.cbioportal.org/api/info / https://master.cbioportal.org/api/info | -- | https://rc.cbioportal.org/api/info |\n| Docker Image | cbioportal/cbioportal:master | --| cbioportal/cbioportal:rc |\n| Kubernetes Config | [production](https://github.com/knowledgesystems/knowledgesystems-k8s-deployment/blob/master/cbioportal/cbioportal_spring_boot.yaml) / [master](https://github.com/knowledgesystems/knowledgesystems-k8s-deployment/blob/master/cbioportal/cbioportal_backend_master.yaml) | -- | [rc](https://github.com/knowledgesystems/knowledgesystems-k8s-deployment/blob/master/cbioportal/cbioportal_backend_rc.yaml) |\n| Status | [![master build status](https://github.com/cbioportal/cbioportal/workflows/Core%20tests/badge.svg)](https://github.com/cBioPortal/cbioportal/actions/workflows/core-test.yml?query=branch%3Amaster) [![master build status](https://github.com/cbioportal/cbioportal/workflows/Integration%20tests/badge.svg)](https://github.com/cBioPortal/cbioportal/actions/workflows/integration-test.yml?query=branch%3Amaster) [![master build status](https://github.com/cbioportal/cbioportal/workflows/Docker%20Image%20CI/badge.svg)](https://github.com/cBioPortal/cbioportal/actions/workflows/dockerimage.yml?query=branch%3Amaster) [![master build status](https://github.com/cbioportal/cbioportal/workflows/Python%20validator/badge.svg)](https://github.com/cBioPortal/cbioportal/actions/workflows/validate-data.yml?query=branch%3Amaster) [![CircleCI](https://circleci.com/gh/cBioPortal/cbioportal/tree/master.svg?style=svg)](https://app.circleci.com/pipelines/github/cBioPortal/cbioportal?branch=master&filter=all) | -- | -- |\n\n## 🚀 Releases\nRelease Notes on GitHub:\n\nhttps://github.com/cBioPortal/cbioportal/releases\n\nSee also the cBioPortal News section for user focused release information:\n\nhttps://www.cbioportal.org/news\n\nDocker Images are available for each tag and branch:\n\nhttps://hub.docker.com/repository/docker/cbioportal/cbioportal/tags\n\n## 👉 Other Repos\nRead the [Architecture docs](https://docs.cbioportal.org/2.1-deployment/architecture-overview) to see how these relate:\n\n- https://github.com/cBioPortal/cbioportal-frontend\n- https://github.com/cbioportal/session-service\n- https://github.com/cBioPortal/datahub/\n",581,cancer-genomics,Java,7,Java,Shell,Perl,HTML,JavaScript,ActionScript,Dockerfile,,,,,,,,,,,,,,,,,,,,,,4118,813,3283,22,58,118,9139,330160,448,6031,5818,213,d8e6a1af74d07bb0461ddcfc244c888c4b998fc1,Switch off spring security on public VS endpoints (#10893),2024-07-17T09:35:25Z,Ruslan Forostianov,ruslan@se4.bio,forus,v6.0.8,## 🐛 Bug Fixes\r\n\r\n- Set default API springdoc properties to match those prior to v6 @inodb (#10800)\r\n- Fix LazyMobXTable countHeader when only has 1 page @fuzhaoyuan ([cbioportal-frontend#4911](https://github.com/cBioPortal/cbioportal-frontend/pull/4911))\r\n\r\n## 📘 Documentation\r\n\r\n- update news page with March & April releases @tmazor (#10801)\r\n\r\n## 🕵️‍♀️ Full commit logs\r\n\r\n- Backend: https://github.com/cBioPortal/cbioportal/compare/v6.0.7...v6.0.8\r\n- Frontend: https://github.com/cBioPortal/cbioportal-frontend/compare/v6.0.7...v6.0.8\r\n\r\n## 🏷Notes on versioning and release procedure\r\nhttps://docs.cbioportal.org/development/release-procedure/#a-note-on-versioning\r\n,v6.0.8,,,github-actions[bot],GNU Affero General Public License v3.0,cbioportal,cBioPortal,294,cancer-genomics,science,visualization,precision-medicine,,,,,,,,,,,,,,,,,/cBioPortal/cbioportal,295,53,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/cbg-ethz/V-pipe,https://github.com/cbg-ethz/V-pipe,1,,,1,1,1,1,0,0,0,0,0,0,1,V-pipe is a pipeline designed for analysing NGS data of short viral genomes,"<!-- markdownlint-disable MD013 MD041 -->\n\n![Logo](https://cbg-ethz.github.io/V-pipe/img/logo.svg)\n\n[![bio.tools](https://img.shields.io/badge/bio-tools-blue.svg)](https://bio.tools/V-Pipe)\n[![Snakemake](https://img.shields.io/badge/snakemake-≥7.11.0-blue.svg)](https://snakemake.github.io/snakemake-workflow-catalog/?usage=cbg-ethz/V-pipe)\n[![Deploy Docker image](https://github.com/cbg-ethz/V-pipe/actions/workflows/deploy-docker.yaml/badge.svg)](https://github.com/cbg-ethz/V-pipe/pkgs/container/v-pipe)\n[![Tests](https://github.com/cbg-ethz/V-pipe/actions/workflows/run_regression_tests.yaml/badge.svg)](https://github.com/cbg-ethz/V-pipe/actions/workflows/run_regression_tests.yaml)\n[![Mega-Linter](https://github.com/cbg-ethz/V-pipe/actions/workflows/mega-linter.yml/badge.svg)](https://github.com/cbg-ethz/V-pipe/actions/workflows/mega-linter.yml)\n[![License: Apache-2.0](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n\nV-pipe is a workflow designed for the analysis of next generation sequencing (NGS) data from viral pathogens. It produces a number of results in a curated format (e.g., consensus sequences, SNV calls, local/global haplotypes).\nV-pipe is written using the Snakemake workflow management system.\n\n## Usage\n\nDifferent ways of initializing V-pipe are presented below. We strongly encourage you to deploy it [using the quick install script](#using-quick-install-script), as this is our preferred method.\n\nTo configure V-pipe refer to the documentation present in [config/README.md](config/README.md).\n\nV-pipe expects the input samples to be organized in a [two-level](config/README.md#samples) directory hierarchy,\nand the sequencing reads must be provided in a sub-folder named `raw_data`. Further details can be found on the [website](https://cbg-ethz.github.io/V-pipe/usage/).\nCheck the utils subdirectory for [mass-importers tools](utils/README.md#samples-mass-importers) that can assist you in generating this hierarchy.\n\nWe provide [virus-specific base configuration files](config/README.md#virus-base-config) which contain handy defaults for, e.g., HIV and SARS-CoV-2. Set the virus in the general section of the configuration file:\n\n```yaml\ngeneral:\n  virus_base_config: hiv\n```\n\nAlso see [snakemake's documentation](https://snakemake.readthedocs.io/en/stable/executing/cli.html) to learn more about the command-line options available when executing the workflow.\n\n\n### Tutorials\n\nTutorials for your first steps with V-pipe for different scenarios are available in the [docs/](docs/README.md) subdirectory.\n\n\n### Using quick install script\n\nTo deploy V-pipe, use the [installation script](utils/README.md#quick-installer) with the following parameters:\n\n```bash\ncurl -O 'https://raw.githubusercontent.com/cbg-ethz/V-pipe/master/utils/quick_install.sh'\n./quick_install.sh -w work\n```\n\nThis script will download and install miniconda, checkout the V-pipe git repository (use `-b` to specify which branch/tag) and setup a work directory (specified with `-w`) with an executable script that will execute the workflow:\n\n```bash\ncd work\n# edit config.yaml and provide samples/ directory\n./vpipe --jobs 4 --printshellcmds --dry-run\n```\n\nTest data to test your installation is available with the tutorials provided in the [docs/](docs/README.md) subdirectory.\n\n### Using Docker\n\nNote: the [docker image](https://github.com/cbg-ethz/V-pipe/pkgs/container/v-pipe) is only setup with components to run the workflow for HIV and SARS-CoV-2 virus base configurations.\nUsing V-pipe with other viruses or configurations might require internet connectivity for additional software components.\n\nCreate `config.yaml` or `vpipe.config` and then populate the `samples/` directory.\nFor example, the following config file could be used:\n\n```yaml\ngeneral:\n  virus_base_config: hiv\n\noutput:\n  snv: true\n  local: true\n  global: false\n  visualization: true\n  QA: true\n```\n\nThen execute:\n\n```bash\ndocker run --rm -it -v $PWD:/work ghcr.io/cbg-ethz/v-pipe:master --jobs 4 --printshellcmds --dry-run\n```\n\n### Using Snakedeploy\n\nFirst install [mamba](https://github.com/conda-forge/miniforge#mambaforge), then create and activate an environment with Snakemake and Snakedeploy:\n\n```bash\nmamba create -c conda-forge -c bioconda --name snakemake snakemake snakedeploy\nconda activate snakemake\n```\n\nSnakemake's [official workflow installer Snakedeploy](https://snakemake.github.io/snakemake-workflow-catalog/?usage=cbg-ethz/V-pipe) can now be used:\n\n```bash\nsnakedeploy deploy-workflow https://github.com/cbg-ethz/V-pipe --tag master .\n# edit config/config.yaml and provide samples/ directory\nsnakemake --use-conda --jobs 4 --printshellcmds --dry-run\n```\n\n## Dependencies\n\n- **[Conda](https://conda.io/docs/index.html)**\n\n  Conda is a cross-platform package management system and an environment manager application. Snakemake uses mamba as a package manager.\n\n- **[Snakemake](https://snakemake.readthedocs.io/)**\n\n  Snakemake is the central workflow and dependency manager of V-pipe. It determines the order in which individual tools are invoked and checks that programs do not exit unexpectedly.\n\n- **[VICUNA](https://www.broadinstitute.org/viral-genomics/vicuna)**\n\n  VICUNA is a _de novo_ assembly software designed for populations with high mutation rates. It is used to build an initial reference for mapping reads with ngshmmalign aligner when a `references/cohort_consensus.fasta` file is not provided. Further details can be found in the [wiki](https://github.com/cbg-ethz/V-pipe/wiki/getting-started#input-files) pages.\n\n### Computational tools\n\nOther dependencies are managed by using isolated conda environments per rule, and below we list some of the computational tools integrated in V-pipe:\n\n- **[FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)**\n\n  FastQC gives an overview of the raw sequencing data. Flowcells that have been overloaded or otherwise fail during sequencing can easily be determined with FastQC.\n\n- **[PRINSEQ](http://prinseq.sourceforge.net/)**\n\n  Trimming and clipping of reads is performed by PRINSEQ. It is currently the most versatile raw read processor with many customization options.\n\n- **[ngshmmalign](https://github.com/cbg-ethz/ngshmmalign)**\n\n  We perform the alignment of the curated NGS data using our custom ngshmmalign that takes structural variants into account. It produces multiple consensus sequences that include either majority bases or ambiguous bases.\n\n- **[bwa](https://github.com/lh3/bwa)**\n\n  In order to detect specific cross-contaminations with other probes, the Burrows-Wheeler aligner is used. It quickly yields estimates for foreign genomic material in an experiment.\n  Additionally, It can be used as an alternative aligner to ngshmmalign.\n\n- **[MAFFT](http://mafft.cbrc.jp/alignment/software/)**\n\n  To standardise multiple samples to the same reference genome (say HXB2 for HIV-1), the multiple sequence aligner MAFFT is employed. The multiple sequence alignment helps in determining regions of low conservation and thus makes standardisation of alignments more robust.\n\n- **[Samtools and bcftools](https://www.htslib.org/)**\n\n  The Swiss Army knife of alignment postprocessing and diagnostics. bcftools is also used to generate consensus sequence with indels.\n\n- **[SmallGenomeUtilities](https://github.com/cbg-ethz/smallgenomeutilities)**\n\n  We perform genomic liftovers to standardised reference genomes using our in-house developed python library of utilities for rewriting alignments.\n\n- **[ShoRAH](https://github.com/cbg-ethz/shorah)**\n\n  ShoRAh performs SNV calling and local haplotype reconstruction by using bayesian clustering.\n\n- **[LoFreq](https://csb5.github.io/lofreq/)**\n\n  LoFreq (version 2) is SNVs and indels caller from next-generation sequencing data, and can be used as an alternative engine for SNV calling.\n\n- **[SAVAGE](https://bitbucket.org/jbaaijens/savage) and [Haploclique](https://github.com/cbg-ethz/haploclique)**\n\n  We use HaploClique or SAVAGE to perform global haplotype reconstruction for heterogeneous viral populations by using an overlap graph.\n\n## Citation\n\nIf you use this software in your research, please cite:\n\nFuhrmann, L., Jablonski, K. P., Topolsky, I., Batavia, A. A., Borgsmueller, N., Icer Baykal, P., Carrara, M. ... & Beerenwinkel, (2023).\n""V-Pipe 3.0: A Sustainable Pipeline for Within-Sample Viral Genetic Diversity Estimation.""\n_bioRxiv_, doi:[10.1101/2023.10.16.562462](https://doi.org/10.1101/2023.10.16.562462).\n\n## Contributions\n\n- [Ivan Topolsky\* ![orcid]](https://orcid.org/0000-0002-7561-0810), [![github]](https://github.com/dryak)\n- [Pelin Icer Baykal ![orcid]](https://orcid.org/0000-0002-9542-5292), [![github]](https://github.com/picerbaykal)\n- [Kim Philipp Jablonski ![orcid]](https://orcid.org/0000-0002-4166-4343), [![github]](https://github.com/kpj)\n- [Lara Fuhrmann ![orcid]](https://orcid.org/0000-0001-6405-0654), [![github]](https://github.com/LaraFuhrmann)\n- [Uwe Schmitt ![orcid]](https://orcid.org/0000-0002-4658-0616), [![github]](https://github.com/uweschmitt)\n- [Michal Okoniewski ![orcid]](https://orcid.org/0000-0003-4722-4506), [![github]](https://github.com/michalogit)\n- [Monica Dragan ![orcid]](https://orcid.org/0000-0002-7719-5892), [![github]](https://github.com/monicadragan)\n- [Susana Posada Céspedes ![orcid]](https://orcid.org/0000-0002-7459-8186), [![github]](https://github.com/sposadac)\n- [David Seifert ![orcid]](https://orcid.org/0000-0003-4739-5110), [![github]](https://github.com/SoapZA)\n- Tobias Marschall\n- [Niko Beerenwinkel\*\* ![orcid]](https://orcid.org/0000-0002-0573-6119)\n\n\* software maintainer ;\n\** group leader\n\n[github]: https://cbg-ethz.github.io/V-pipe/img/mark-github.svg\n[orcid]: https://cbg-ethz.github.io/V-pipe/img/ORCIDiD_iconvector.svg\n\n## Contact\n\nWe encourage users to use the [issue tracker](https://github.com/cbg-ethz/V-pipe/issues). For further enquiries, you can also contact the V-pipe Dev Team <v-pipe@bsse.ethz.ch>.\n",129,snakemake,Jupyter Notebook,7,Shell,Python,HTML,Dockerfile,CSS,Perl,Jupyter Notebook,,,,,,,,,,,,,,,,,,,,,,100,31,66,3,25,18,75,15600,44,56,27,29,70376758b506122d7c476925e196e982c32d735f,[bump] LolliPop v0.4.1,2024-07-06T12:30:23Z,Ivan Blagoev Topolsky,ivan.topolsky@sib.swiss,DrYak,V-pipe 3.0 - prerelease 1,Peer-review for publication,v3.0.0.pre1,DrYak,,DrYak,Apache License 2.0,V-pipe,cbg-ethz,10,ngs,snakemake,conda,biohackeu20,virus,sequencing,bioinformatics,bioinformatics-pipeline,biohackcovid20,sars-cov-2,sarscov2,hiv,genomics,biohackeu21,biohackeu22,,,,,,/cbg-ethz/V-pipe,15,10,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/CasparCG/server,https://github.com/CasparCG/server,0,,,0,1,0,0,0,0,1,1,0,0,0,"CasparCG Server is a Windows and Linux software used to play out professional graphics, audio and video to multiple outputs. It has been in 24/7 broadcast production since 2006. Ready-to-use downloads are available under the Releases tab https://casparcg.com.","CasparCG Server\n===============\n\nThank you for your interest in CasparCG Server, a professional software used to\nplay out and record professional graphics, audio and video to multiple outputs.\nCasparCG Server has been in 24/7 broadcast production since 2006.\n\nThe CasparCG Server works on Windows and Linux.\n\nSystem Requirements\n-------------------\n\n- A graphics card (GPU) capable of OpenGL 4.5 is required.\n- An Nvidia GPU is recommended, but other GPU's will likely work fine.\n- Only Intel CPU's have been tested and are known to work\n\n### Windows\n\n - Only Windows 10 is supported\n\n### Linux\n\n - Ubuntu 22.04 is recommended\n - Other distributions and releases will work but have not been tested\n\nGetting Started\n---------------\n\n1. Download a release from (http://casparcg.com/downloads).\n   Alternatively, newer testing versions can be downloaded from (http://builds.casparcg.com) or [built from source](BUILDING.md)\n\n2. Install any optional non-GPL modules\n    - Flash template support (Windows only):\n\n    1. Uninstall any previous version of the Adobe Flash Player using this file:\n        (http://download.macromedia.com/get/flashplayer/current/support/uninstall_flash_player.exe)\n\n    2. Download and unpack\n        (http://download.macromedia.com/pub/flashplayer/installers/archive/fp_11.8.800.94_archive.zip)\n\n    3. Install Adobe Flash Player 11.8.800.94 from the unpacked archive:\n        fp_11.8.800.94_archive\11_8_r800_94\flashplayer11_8r800_94_winax.exe\n\n3. Configure the server by editing the self-documented ""casparcg.config"" file in\n   a text editor.\n\n4.\n   1. Windows: start `casparcg_auto_restart.bat`, or `casparcg.exe` and `scanner.exe` separately.\n   1. Linux: start the `run.sh` program or use tools/linux/start_docker.sh to run within docker (documentation is at the top of the file).\n\n5. Connect to the Server from a client software, such as the ""CasparCG Client""\n   which is available as a separate download.\n\nDocumentation\n-------------\n\nThe most up-to-date documentation is always available at\nhttps://github.com/CasparCG/help/wiki\n\nAsk questions in the forum: https://casparcgforum.org/\n\nDevelopment\n-----------\n\nSee [BUILDING](BUILDING.md) for instructions on how to build the CasparCG Server from source manually.\n\nLicense\n---------\n\nCasparCG Server is distributed under the GNU General Public License GPLv3 or\nhigher, see [LICENSE](LICENSE) for details.\n\nCasparCG Server uses the following third party libraries:\n- FFmpeg (http://ffmpeg.org/) under the GPLv2 Licence.\n  FFmpeg is a trademark of Fabrice Bellard, originator of the FFmpeg project.\n- Threading Building Blocks (http://www.threadingbuildingblocks.org/) library under the GPLv2 Licence.\n- FreeImage (http://freeimage.sourceforge.net/) under the GPLv2 License.\n- SFML (http://www.sfml-dev.org/) under the zlib/libpng License.\n- GLEW (http://glew.sourceforge.net) under the modified BSD License.\n- boost (http://www.boost.org/) under the Boost Software License, version 1.0.\n",890,graphics,C++,7,Shell,C++,C,CMake,Batchfile,Dockerfile,GLSL,,,,,,,,,,,,,,,,,,,,,,362,106,241,15,15,47,0,263571,270,1185,977,208,e4e9ed299e1a0f9c87c8709d19b0585a5703102a,fix: ffmpeg producer reject ndi://,2024-06-11T16:38:44Z,Jesper Ek,deadbeef84@gmail.com,deadbeef84,v2.4.0 Stable,"## Changes since 2.4.0 RC 1:\r\n\r\n### Core\r\n##### Improvements\r\n* Support 4K DCI frames\r\n* Build: Allow configuring diag font path at build time \r\n* AMCP: Add CALLBG command to perform CALL on background producer\r\n##### Fixes\r\n* OSC: Background state not being produced\r\n* Scanner: resolve issues with database not being updated\r\n### Producers\r\n##### Improvements\r\n* HTML: Expose `cache-path` setting\r\n##### Fixes\r\n* FFmpeg: Unable to play files with extra dots in filenames\r\n* FFmpeg: Support parameters with name containing a dash\r\n* FFmpeg: Audio channels being swapped unexpectedly\r\n\r\n## Changes since 2.3.3\r\n\r\n### Core\r\n##### Improvements\r\n* Custom resultions can be specified in casparcg.config\r\n* Interlaced mixer pipeline to ensure field accuracy\r\n* Preserve unicode characters in console input/output\r\n* Producers to be run at startup can be defined in casparcg.config\r\n* Support 8K frames\r\n* Support 4K DCI frames\r\n* Remove undocumented CII and CLK protocol implementations\r\n* Config parameter can be an absolute system path, not just relative to the working directory\r\n* AMCP: Add CLEAR ALL command\r\n* AMCP: Command batching syntax\r\n* AMCP: LOAD/LOADBG/PLAY commands accept a CLEAR_ON_404 parameter, to instruct the layer to be cleared when the requested file was not found\r\n* AMCP: Add commands to subscribe and unsubscribe to OSC on any port number\r\n* AMCP: Add CALLBG command to perform CALL on background producer\r\n* Build: Require C++17 for building\r\n* Build: Support newer versions of Boost\r\n* Build: Support newer versions of TBB\r\n* Build: Disable precompiled headers for linux\r\n* Build: Support VS2022\r\n* Build: Replace nuget and locally committed dependencies with direct http downloads\r\n* Build: Allow configuring diag font path at build time \r\n* Linux: Support setting thread priorities\r\n* Linux: Initial ARM64 compatibility\r\n* Linux: Rework build to always use system boost\r\n* Linux: Rework build process to better support being build as a system package\r\n* Logging: add config option to disable logging to file and to disable column alignment \r\n* Transitions: Support additional audio fade properties for STING transition\r\n##### Fixes\r\n* Crash upon exiting if HTML producer was running\r\n* AMCP: Ensure all consumers and producers are reported in `INFO` commands\r\n* AMCP: Deferred mixer operations were not being cleared after being applied\r\n* AMCP: `LOAD` command would show a frame or two of black while new producer was loading\r\n* OpenGL: Fix support for recent Linux drivers\r\n* Linux: Fix endless looping on stdin\r\n* Route: Fix error when clearing layer\r\n* Transitions: Fix wipe duration\r\n\r\n### Producers\r\n##### Improvements\r\n* Decklink: Require driver 11.0 or later\r\n* Decklink: Scale received frames on GPU\r\n* FFmpeg: Update to v5.1\r\n* FFmpeg: Improve performance\r\n* FFmpeg: Allow specifying both SEEK and IN for PLAY commands\r\n* HTML: Update to CEF 117\r\n* HTML: `CALL 1-10 RELOAD` to reload a renderer\r\n* HTML: Expose `cache-path` setting\r\n* NDI: Upgrade to NDI5\r\n* System Audio: Allow specifying output device to use\r\n##### Fixes\r\n* Decklink: Log spamming when using some input formats\r\n* FFmpeg: Prevent loading unreadable files\r\n* FFmpeg: Unable to play files with unicode filenames\r\n* FFmpeg: Don't lowercase filter parameters\r\n* FFmpeg: Support parameters with name containing a dash\r\n* HTML: media-stream permission denied\r\n* HTML: Expose angle backend config field, the best backend varies depending on the templates and machine\r\n* HTML: Crash when multiple iframes were loaded within a renderer\r\n* Image: Improve file loading algorithm to match the case insensitive and absolute path support already used by ffmpeg\r\n\r\n### Consumers\r\n##### Improvements\r\n* Artnet: New artnet consumer\r\n* Decklink: Configure device duplex modes in casparcg.config\r\n* Decklink: Output a subregion of the channel\r\n* Decklink: Add secondary outputs in a consumer, to ensure sync when used within a single card\r\n* iVGA: Remove consumer\r\n* NDI: Upgrade to NDI5\r\n##### Fixes\r\n* Decklink: Fix stutter when loading clips\r\n* FFmpeg: Fix RTMP streaming missing headers\r\n* NDI: dejitter",v2.4.0-stable,Julian Waller,,Julusian,GNU General Public License v3.0,server,CasparCG,16,casparcg,casparcg-server,c-plus-plus,professional,broadcast,broadcasting,video-player,video-recording,video-streaming,streaming-video,graphics,graphics-engine,,,,,,,,,/CasparCG/server,43,102,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/casadi/casadi,https://github.com/casadi/casadi,1,"Framework, but pretty big. Not sure",,1,1,1,0,0,0,0,0,0,0,0,"CasADi is a symbolic framework for numeric optimization implementing automatic differentiation in forward and reverse modes on sparse matrix-valued computational graphs. It supports self-contained C-code generation and interfaces state-of-the-art codes such as SUNDIALS, IPOPT etc. It can be used from C++, Python or Matlab/Octave.",![GitHub all releases](https://img.shields.io/github/downloads/casadi/casadi/total?label=github%20downloads)\n![PyPI - Downloads](https://img.shields.io/pypi/dm/casadi?label=pypi%20downloads)\n\n\nLearn all about CasADi at the [homepage](http://casadi.org) or jump to [install instructions](http://install.casadi.org)...\n,1628,mathematics,C++,12,CMake,C++,C,Python,Makefile,Fortran,MATLAB,Shell,M,Objective-C,SWIG,Raku,,,,,,,,,,,,,,,,,202,66,111,25,208,52,0,140267,364,3207,2495,712,72764dc73eab7b935db7e302278508855815883b,automated commit by docs target [skip ci],2024-06-25T08:51:07Z,casadibot,casaditestbot@gmail.com,casadibot,03.06.2005,"# Install\r\n\r\nGrab a binary from the table:\r\n<table>\r\n  <tr><th></th><th>Windows</th><th>Linux</th><th>Mac classic (High Sierra or above)</th><th>Mac M1</th></tr>\r\n  <tr>\r\n    <th>Matlab</th>\r\n    <td><a href=""https://github.com/casadi/casadi/releases/download/3.6.5/casadi-3.6.5-windows64-matlab2018b.zip"">R2018b</a> or later</td>\r\n    <td><a href=""https://github.com/casadi/casadi/releases/download/3.6.5/casadi-3.6.5-linux64-matlab2018b.zip"">R2018b</a> or later</td>\r\n    <td><a href=""https://github.com/casadi/casadi/releases/download/3.6.5/casadi-3.6.5-osx64-matlab2018b.zip"">R2018b</a> or later</td>\r\n    <td><a href=""https://github.com/casadi/casadi/releases/download/3.6.5/casadi-3.6.5-osx64-matlab2018b.zip"">R2020b</a> or later (normal Matlab)<br/><a href=""https://github.com/casadi/casadi/releases/download/3.6.5/casadi-3.6.5-osx_arm64-matlab2018b.zip"">R2018b</a> or later (<a href=""https://nl.mathworks.com/support/apple-silicon-r2022b-beta.html"">Open Beta/M1</a>)</td>\r\n  </tr>\r\n  <tr>\r\n    <th>Octave</th>\r\n    <td><a href=""https://github.com/casadi/casadi/releases/download/3.6.5/casadi-3.6.5-windows64-octave7.3.0.zip"">6.2.0</a> or later</td>\r\n    <td><a href=""https://github.com/casadi/casadi/releases/download/3.6.5/casadi-3.6.5-linux64-octave7.3.0.zip"">6.2.0</a> or later</td>\r\n    <td><a href=""https://github.com/casadi/casadi/releases/download/3.6.5/casadi-3.6.5-osx64-octave7.3.0.zip"">6.2.0</a> or later</td>\r\n    <td><a href=""https://github.com/casadi/casadi/releases/download/3.6.5/casadi-3.6.5-osx_arm64-octave7.3.0.zip"">6.2.0</a> or later</td>\r\n  </tr>\r\n  <tr>\r\n    <th>Python</th>\r\n    <td colspan=""4""><code>pip install casadi</code> (needs <code>pip -V</code>>=8.1)</td>\r\n  </tr>\r\n</table>\r\n\r\nFor Matlab/Octave, unzip in your home directory and adapt the path:\r\n<pre>\r\n<code>\r\naddpath('&lt;yourpath>/casadi-3.6.5-windows64-matlab2018b')\r\n</code>\r\n</pre>\r\n\r\nCheck your installation:\r\n<table>\r\n<tr><th>Matlab/Octave</th><th>Python</th><tr>\r\n<tr><td>\r\n<pre>\r\n<code>\r\nimport casadi.*\r\nx = MX.sym('x')\r\ndisp(jacobian(sin(x),x))\r\n</code>\r\n</pre>\r\n</td><td>\r\n\r\n<pre>\r\n<code>\r\nfrom casadi import *\r\nx = MX.sym(""x"")\r\nprint(jacobian(sin(x),x))\r\n</code>\r\n</pre>\r\n</td></tr>\r\n</table>\r\n\r\nGet started with the [example pack](https://github.com/casadi/casadi/releases/download/3.6.5/casadi-example_pack-v3.6.5.zip). Onboarding pointers have been gathered by the community at our [wiki](https://github.com/casadi/casadi/wiki/Onboarding-Guide).\r\n\r\n\r\n# Troubleshooting\r\n\r\n * KNITRO on linux crashes with a segmentation fault without `LD_PRELOAD=<knitro_lin_path>/libiomp5.so`.\r\n * Callbacks with one argument are broken in Matlab CasADi\r\n\r\n# Release notes\r\n\r\n## Symbolic expressions\r\n * Added SX/MX/DM operations [#1595](https://github.com/casadi/casadi/issues/1595):\r\n    * `hypot(x,y) = sqrt(x*x+y*y)`\r\n    * `log1p(x) = log(1+x)`\r\n    * `expm1(x) = exp(x-1)`\r\n * Added operation `remainder` with the semantics of the [C operation](https://cplusplus.com/reference/cmath/remainder/)\r\n * **breaking** AD rule of `fmin/`fmax` is now symmetric:\r\n  ```jacobian(fmin(x,y),vertcat(x,y))``` used to be [1 0] for x==y. Now yields [0.5 0.5].\r\n * Added AD rules for `mmin`/`mmax`\r\n * Added `logsumexp` which behaves like `log(sum(exp(x)))` but is numerically more accurate (and no overflow issues).\r\n * **breaking** `vertcat`/`vcat`,`horzcat`/`hcat`, etc now return a `DM` type instead of a `Sparsity` type  [#2549](https://github.com/casadi/casadi/issues/2549)\r\n * **breaking** CasADi-Matlab `mod` has been renamed to `rem`, because its numerical behaviour is like the builtin-Matlab `rem`. The builtin-Matlab `mod` has no CasADi counterpart. CasADi-Python `mod` has been removed, because its numerical behaviour is not like `numpy.mod`. [#2767](https://github.com/casadi/casadi/issues/2767). `numpy.mod` has no counterpart in CasADi; only `fmod` is equivalent.\r\n * DAE index reduction support (Pantelides structural algorithm) See https://github.com/casadi/casadi/blob/3.6.0/docs/examples/python/dae_reduced_index.py\r\n * Fixed flaw in codegen with MX if_else\r\n### Common subexpression elimination\r\n * Added Common Subexpression Elimination [#1540](https://github.com/casadi/casadi/issues/1540) for MX and SX.\r\n  CasADi can now efficiently eliminate redundant computation by inspecting an expression graph and removing redundant nodes.\r\n\r\nBefore, CasADi internals would avoid introducing redundant nodes during operations on a given expression, but the user was responsible to avoid duplication when constructing that expression.\r\n\r\nThere is a function `cse()` that you may apply to expressions:\r\n```python\r\nx = MX.sym('x')\r\n\r\n# User responsibility\r\nsx = sin(x)\r\ny = sqrt(sx)+sx # MX(@1=sin(x), (sqrt(@1)+@1))\r\n\r\n# cse\r\ny = sqrt(sin(x))+sin(x) # MX((sqrt(sin(x))+sin(x)))\r\ny = cse(y) # MX(@1=sin(x), (sqrt(@1)+@1))\r\n```\r\nThere is a boolean option `cse` that may be used when constructing a `Function`:\r\n```python\r\nx = MX.sym('x')\r\n\r\nf = Function('f',[x],[sqrt(sin(x))+sin(x)],{""cse"":True})\r\nf.disp(True)\r\n```\r\n\r\n```\r\nf:(i0)->(o0) MXFunction\r\nAlgorithm:\r\n@0 = input[0][0]\r\n@0 = sin(@0)\r\n@1 = sqrt(@0)\r\n@1 = (@1+@0)\r\noutput[0][0] = @1\r\n```\r\n\r\nThe technique scales favorably for large graphs.\r\n\r\n### Triangular solve triangular solve nodes in MX\r\nMX how has atomic support for solving upper and lower triangular linear systems without allocating any linear solver instance. The operation handles the case with unity diagonal separately for efficiency and supports C code generation. To use the feature, call `casadi.solve(A, b)` (Python or MATLAB/Octave)\r\n```python\r\n# Python\r\nimport casadi\r\nA = casadi.MX.sym('A', casadi.Sparsity.upper(2))\r\nb = casadi.MX.sym('b', 2)\r\nx = casadi.solve(A, b)\r\n```\r\n```c++\r\n// C++\r\ncasadi::MX A = casadi::MX::sym(""A"", casadi::Sparsity::upper(2));\r\ncasadi::MX b = casadi::MX::sym(""b"", 2);\r\ncasadi::MX x = solve(A, b);  // for argument-dependent lookup, alternatively casadi::MX::solve(A, b) for static function\r\n```\r\nCf. [#2688](https://github.com/casadi/casadi/issues/2688).\r\n\r\n\r\n## Function\r\n * **breaking** `SX`/`MX` `Function` construction with free variables (i.e. symbols used in the output expressions that are not declared as inputs) now fails immediately unless the `allow_free` option is used.\r\n * **breaking** `SX`/`MX` `Function` construction now fails if there are duplicates in input names or output names, unless the `allow_duplicate_io_names` option is used [#2604](https://github.com/casadi/casadi/issues/2604).\r\n * **breaking** Serialization: files saved with CasADi 3.5.5 will load in CasADi 3.6.0 (unittested), except for Functions that include a 'mumps' linear solver since serialization of this solver was deficient, and except for Functions that include an Integrator.\r\n * **breaking** `custom_jacobian` semantics changed. The Function must now return individual blocks (Jacobian of an output w.r.t. to an input)\r\n * **breaking** Changed API part for Jacobian sparsity (relevant for advanced use through `external` or `Callback`)\r\n```cpp\r\nbool has_jac_sparsity(casadi_int oind, casadi_int iind) const override;\r\nSparsity get_jac_sparsity(casadi_int oind, casadi_int iind, bool symmetric) const override;\r\n```\r\n * `Function.find_function` Can be used to retrieve Functions in a hierarchy.\r\n * Avoid truncation in printing [#2452](https://github.com/casadi/casadi/issues/2452)\r\n * **breaking**: Function outputs that are not used (passed a null pointer internally) will be logged (`dump_in` option ) as `nan` instead of earlier `0`. E.g. Ipopt `nlp_grad_f` has two outputs, `f` and `grad_f_x`. The `f` output is not used internally, so will be logged as `nan`.\r\n\r\n## Code-generation\r\n * `Function` objects with an `external` call can now be codegenerated.\r\n * `mmin`/`mmax` now support codegeneration\r\n \r\n## Solvers/plugins\r\n * `nlpsol`/`Opti.solver` can now take an option 'detect_simple_bounds' (default `False`) that will promote general constraints to simple bounds (lbx/ubx).\r\n * Added SPRAL linear solver for Ipopt\r\n * Added QP solvers HPIPM, Proxqp, Highs\r\n * CPLEX interface will dynamically load `libcplex<CPLEX_VERSION>`, where CPLEX_VERSION is read from environmental variables. Same strategy for `Gurobi`.\r\n * SqpMethod Eigen-reflect/eigen-clip incorrect [#2896](https://github.com/casadi/casadi/issues/2896)\r\n\r\n### Generalized integrator support\r\nThe `Integrator` class, which solves initial-value problems in ODEs and DAEs has been thoroughly refactored. Changes include:\r\n * The integrator class now has a much more mature support for returning the IVP solution at multiple time points. It can now be obtained by providing a time grid to the `integrator` constructor. Unlike before, this support should now work in combination with forward/adjoint sensitivity analysis (to any order) and sparsity pattern calculations. Cf. [#2823](https://github.com/casadi/casadi/issues/2823).\r\n * The integrator class now includes support for a piecewise constant control (`u`). The interface will keep track of changes to `u` and avoid integrating past such changes; for the Sundials (CVODES/IDAS) interfaces by setting a ""stop time"", for fixed step integrators by aligning the integration points with the grid points. Cf. [#3025](https://github.com/casadi/casadi/issues/3025). Development versions of CasADi included support for this in a dedicated class, called `Simulator`, but this class has now been removed (**breaking**) and the functionality has been ported to the `Integrator` class.\r\n  If you had code looking like `cs.integrator('sim_function', 'cvodes', dae, tgrid, opts)`, you may replace it by `cs.integrator('sim_function', 'cvodes', dae, 0, tgrid[1:], opts)`.\r\n * The Integrator class now much better exploits the problem structure in the sensitivity calculations, especially adjoint (and forward-over-adjoint, adjoint-over-adjoint) sensitivity calculations. Cf. [#2823](https://github.com/casadi/casadi/issues/2823), [#3047](https://github.com/casadi/casadi/issues/3047). The sensitivity analysis relies to a much less extent on symbolic reformulations and instead uses calls to the `Function` class for derivative calculations - this makes the class now more efficient for use with non-symbolic DAEs, including FMUs or other external models.\r\n * **breaking** The options `t0`, `tf`, `output_t0` and `grid` have been deprecated and will result in a warning if used. Instead, the user can provide equivalent information via the `integrator` constructor, cf. previous point.\r\n * The `backward states` are no longer part of the DAE formulation. They are now derived from a user specified number of sensitivity equations (`nadj`). This is a slight restriction in the possible problem formulations, but on the other hand allows for a much better exploitation of adjoint sensitivity structure. The the backward states remain in the integrator class function inputs and outputs, but have now been renamed to align with their meaning; `adj_xf` means the adjoint seeds corresponding to `xf` (before they were called `rx0`), `adj_p` are the adjoint sensitivities corresponding to `p` (before called `rqf` and so on.\r\n* An option `scale_abstol` has been added to the Sundials integrators. If this is set to true, nominal values for the differential state and algebraic variables will be passed on to the solver. Cf. [#3046](https://github.com/casadi/casadi/issues/3046)\r\n\r\nSee ""multipoint_simulation"" in the example pack for a good starting point.\r\n\r\n## Function factory\r\n * **breaking*** Hessian blocks are now symmetric by default instead of returning only the upper triangular part. Prefix with `triu:` to get the old behavior.\r\n * **breaking** Multiple Jacobian and/or Hessian blocks can now be calculated more efficiently. Rather than calculating the blocks separately, calculation is done for multiple blocks at once, whenever possible.\r\n   Cf. [#2696](https://github.com/casadi/casadi/issues/2696).\r\n\r\n## DaeBuilder / FMI interoperability\r\n\r\n* The dependent parameters `d` and local dependent variables `w` have been replaced by the single dependent variables `v`.\r\n* The DaeBuilder::create function has been reimplemented and now uses the updated Function::factory support (above).\r\n* New proof-of-concept support for export of models in FMI 3.0 format, cf. [#3009](https://github.com/casadi/casadi/issues/3009)\r\n* New binary interface to standard FMI, including analytic validated first derivatives and validated hybrid second derivatives, cf. [#2779](https://github.com/casadi/casadi/issues/2779)\r\n* The Integrator class has been refactored to efficiently support non-symbolic DAEs, including from FMI - see below.\r\n\r\n## Binaries\r\n * Adding Python interfaces for versions 3.10 and 3.11 \r\n * Adding builds for Mac silicon\r\n * Octave interface will now dynamically couple with the correct versioned `octaveinterp` version, such that the new binaries work with future releases of Octave that increment the `octaveinterp` ABI version number.\r\n\r\n## CLI\r\n * There is now a CasADi command line interface, `casadi-cli`. At the moment, functionality is very limited, just `eval_dump`, to evaluate Function that have been dumped to the disk (options `dump`,`dump_in`)\r\n\r\n## Documentation\r\n * There was a substantial effort to create an onboarding guide: https://github.com/casadi/casadi/wiki/Onboarding-Guide\r\n\r\n## Building\r\n * Source builds are no longer dependent on SWIG since Python and Matlab interface files generated by SWIG are now shipped in source archives.\r\n * Source builds can now build and integrate a range third-party open-source solver automatically. E.g. `-DWITH_IPOPT=ON -DWITH_BUILD_REQUIRED=ON`\r\n * Source builds can now use mockups for a range of third-party commercial solvers. E.g. `-DWITH_CPLEX=ON -DWITH_MOCKUP_CPLEX=ON`\r\n * Source packages for python `pip` are now available\r\n\r\n## Plugin versions used in binaries\r\n\r\n### 3.6.0\r\n\r\n * dynamic-loading, Compile with support for dynamic loading of FMU libraries\r\n * sundials-interface, Interface to the ODE/DAE integrator suite SUNDIALS.\r\n * csparse-interface, Interface to the sparse direct linear solver CSparse.\r\n * superscs-interface, Interface to QP solver SUPERSCS.\r\n * osqp-interface, Interface to QP solver OSQP.\r\n * tinyxml-interface, Interface to the XML parser TinyXML.\r\n * qpoases-interface, Interface to the active-set QP solver qpOASES.\r\n * blocksqp-interface, Interface to the NLP solver blockSQP.\r\n * cplex-mockup-build, Use mockup CPLEX (BUILD_MOCKUPS_VERSION=v60) from downloaded source (BUILD_MOCKUPS_GIT_REPO=https://github.com/casadi/mockups.git).\r\n * snopt-mockup-build, Use mockup SNOPT (BUILD_MOCKUPS_VERSION=v60) from downloaded source (BUILD_MOCKUPS_GIT_REPO=https://github.com/casadi/mockups.git).\r\n * knitro-mockup-build, Use mockup KNITRO (BUILD_MOCKUPS_VERSION=v60) from downloaded source (BUILD_MOCKUPS_GIT_REPO=https://github.com/casadi/mockups.git).\r\n * gurobi-mockup-build, Use mockup GUROBI (BUILD_MOCKUPS_VERSION=v60) from downloaded source (BUILD_MOCKUPS_GIT_REPO=https://github.com/casadi/mockups.git).\r\n * worhp-mockup-build, Use mockup WORHP (BUILD_MOCKUPS_VERSION=v60) from downloaded source (BUILD_MOCKUPS_GIT_REPO=https://github.com/casadi/mockups.git).\r\n * hsl-mockup-build, Use mockup WORHP (BUILD_MOCKUPS_VERSION=v60) from downloaded source (BUILD_MOCKUPS_GIT_REPO=https://github.com/casadi/mockups.git).\r\n * highs-sourcebuild, Build HiGHS (BUILD_HIGHS_VERSION=v1.4.1) from downloaded source (BUILD_HIGHS_GIT_REPO=https://github.com/ERGO-Code/HiGHS).\r\n * proxqp-sourcebuild, Build PROXQP (BUILD_PROXQP_VERSION=v0.3.2) from downloaded source (BUILD_PROXQP_GIT_REPO=https://github.com/Simple-Robotics/proxsuite.git).\r\n * osqp-sourcebuild, Build OSQP (BUILD_OSQP_VERSION=v0.6.2) from downloaded source (BUILD_OSQP_GIT_REPO=https://github.com/osqp/osqp.git).\r\n * superscs-sourcebuild, Build SuperSCS (BUILD_SUPERSCS_VERSION=4d2d1bd03ed4cf93e684a880b233760ce34ca69c) from downloaded source (BUILD_SUPERSCS_GIT_REPO=https://github.com/jgillis/scs.git).\r\n * bonmin-sourcebuild, Build BONMIN (BUILD_BONMIN_VERSION=releases/1.8.8) from downloaded source (BUILD_BONMIN_GIT_REPO=https://github.com/coin-or/Bonmin.git).\r\n * ipopt-sourcebuild, Build IPOPT (BUILD_IPOPT_VERSION=3.14.11.mod) from downloaded source (BUILD_IPOPT_GIT_REPO=https://github.com/jgillis/Ipopt-1.git).\r\n * cbc-sourcebuild, Build CBC (BUILD_CBC_VERSION=releases/2.10.6) from downloaded source.\r\n * clp-sourcebuild, Build CLP (BUILD_CLP_VERSION=releases/1.17.7) from downloaded source (BUILD_CLP_GIT_REPO=https://github.com/coin-or/Clp.git).\r\n * mumps-sourcebuild, Build MUMPS (BUILD_MUMPS_TP_VERSION=releases/3.0.2) from downloaded source (BUILD_MUMPS_TP_GIT_REPO=https://github.com/coin-or-tools/ThirdParty-Mumps.git).\r\n * spral-sourcebuild, Build SPRAL (BUILD_SPRAL_VERSION=d385d2c9e858366d257cafaaf05760ffa6543e26) from downloaded source (BUILD_SPRAL_GIT_REPO=https://github.com/ralna/spral.git).\r\n * metis-sourcebuild, Build METIS (BUILD_METIS_TP_VERSION=releases/2.0.0) from downloaded source.\r\n * hpipm-sourcebuild, Build HPIPM (BUILD_HPIPM_VERSION=0e0c9f4e0d4081dceafa9b37c396db50bce0e81a) from downloaded source (BUILD_HPIPM_GIT_REPO=https://github.com/jgillis/hpipm.git).\r\n * blasfeo-sourcebuild, Build BLASFEO (BUILD_BLASFEO_VERSION=edf92b396adddd9e548b9786f87ad290a0971329) from downloaded source (BUILD_BLASFEO_GIT_REPO=https://github.com/giaf/blasfeo.git).\r\n * lapack-sourcebuild, Download and install OpenBLAS for LAPACK+BLAS\r\n * eigen3-sourcebuild, Build Eigen (BUILD_EIGEN3_VERSION=3.4.0) from downloaded source (BUILD_EIGEN3_GIT_REPO=https://gitlab.com/libeigen/eigen.git).\r\n * simde-sourcebuild, Build Simde (BUILD_SIMDE_VERSION=v0.7.2) from downloaded source (BUILD_SIMDE_GIT_REPO=https://github.com/simd-everywhere/simde.git).\r\n * cplex-interface, Interface to the QP solver CPLEX.\r\n * gurobi-interface, Interface to the (mixed-integer) QP solver GUROBI\r\n * knitro-interface, Interface to the NLP solver KNITRO.\r\n * snopt-interface, Interface to the NLP solver SNOPT.\r\n * worhp-interface, Interface to the NLP solver Worhp (requires gfortran, gomp).\r\n * lapack-interface, Interface to LAPACK.\r\n * mumps-interface, Interface to MUMPS.\r\n * spral-interface, Interface to SPRAL.\r\n * coinutils-sourcebuild, Build COINUTILS (BUILD_COINUTILS_VERSION=releases/2.11.6) from downloaded source.\r\n * osi-sourcebuild, Build OSI (BUILD_OSI_VERSION=releases/0.108.7) from downloaded source.\r\n * clp-interface, Interface to the LP solver CLP.\r\n * cgl-sourcebuild, Build CGL (BUILD_CGL_VERSION=releases/0.60.4) from downloaded source.\r\n * cbc-interface, Interface to the LP solver CBC.\r\n * ipopt-interface, Interface to the NLP solver Ipopt.\r\n * bonmin-interface, Interface to the MINLP framework Bonmin.\r\n * highs-interface, Interface to the MILP / QP solver HiGHS.\r\n * proxqp-interface, Interface to QP solver PROXQP.\r\n * ampl-interface, Interface to the AMPL solver library.\r\n \r\n### 3.6.4\r\n * alpaqa-sourcebuild, Build Alpaqa (BUILD_ALPAQA_VERSION=develop) from downloaded source (BUILD_ALPAQA_GIT_REPO=https://github.com/jgillis/alpaqa).\r\n * highs-sourcebuild, Build HiGHS (BUILD_HIGHS_VERSION=v1.6.0) from downloaded source (BUILD_HIGHS_GIT_REPO=https://github.com/ERGO-Code/HiGHS).\r\n * sleqp-sourcebuild, Build SLEQP (BUILD_SLEQP_VERSION=patch-1) from downloaded source (BUILD_SLEQP_GIT_REPO=https://github.com/jgillis/sleqp.git).\r\n * bonmin-sourcebuild, Build BONMIN (BUILD_BONMIN_VERSION=releases/1.8.9) from downloaded source (BUILD_BONMIN_GIT_REPO=https://github.com/coin-or/Bonmin.git).\r\n * cbc-sourcebuild, Build CBC (BUILD_CBC_VERSION=releases/2.10.11) from downloaded source.\r\n * clp-sourcebuild, Build CLP (BUILD_CLP_VERSION=releases/1.17.9) from downloaded source (BUILD_CLP_GIT_REPO=https://github.com/coin-or/Clp.git).\r\n * trlib-sourcebuild, Build TRLIB (BUILD_TRLIB_VERSION=c7632b8b14152e78bc21721a3bd1a2432586b824) from downloaded source (BUILD_TRLIB_GIT_REPO=https://github.com/jgillis/trlib.git).\r\n * coinutils-sourcebuild, Build COINUTILS (BUILD_COINUTILS_VERSION=releases/2.11.10) from downloaded source.\r\n * osi-sourcebuild, Build OSI (BUILD_OSI_VERSION=releases/0.108.9) from downloaded source.\r\n * cgl-sourcebuild, Build CGL (BUILD_CGL_VERSION=releases/0.60.8) from downloaded source.\r\n * sleqp-interface, Interface to the NLP solver SLEQP.\r\n * alpaqa-interface, Interface to the NLP solver Alpaqa.\r\n\r\n## Changes in 3.6.1\r\n * Various bugfixes and patches https://github.com/casadi/casadi/milestone/24?closed=1\r\n * **breaking** serialization of integrator is not compatible with 3.6.0 due to bugfixes\r\n * git: `master` branch has been renamed to `main`, and has different semantics: it will be the branch where new features are added regularly before they become an official release. Latest official release is available as `latest` branch.\r\n\r\n## Changes in 3.6.2\r\n * Various bugfixes and patches https://github.com/casadi/casadi/milestone/25?closed=1\r\n * **breaking** serialization of integrator is not compatible with 3.6.0 or 3.6.1 due to bugfixes\r\n\r\n## Changes in 3.6.3\r\n * Various bugfixes and patches https://github.com/casadi/casadi/milestone/26?closed=1\r\n\r\n## Changes in 3.6.4\r\n * Parallelization is now officially supported for some plugins (see https://github.com/jgillis/mip_casadi): HiGHS (on the LP level, options ""threads"",""parallel"",""simplex_min_concurrency"",""simplex_max_concurrency"",""simplex_strategy""), CbC & Gurobi (on the branching level of MILP, option ""threads"")\r\n * Various bugfixes and patches https://github.com/casadi/casadi/milestone/27?closed=1\r\n \r\n## Changes in 3.6.5\r\n * Various bugfixes and patches https://github.com/casadi/casadi/milestone/28?closed=1\r\n * IPOPT solver can now be code-generated, with a dependency on ipopt.dll",03.06.2005,Joris Gillis,,jgillis,GNU Lesser General Public License v3.0,casadi,casadi,92,optimization,nonlinear,derivatives,code-generation,numerical-calculations,academic-project,scientific-computing,mathematics,symbolic-manipulation,parameter-estimation,optimal-control,modular,matlab,octave,python,c-plus-plus,library,algorithmic-differentation,nonlinear-programming,,/casadi/casadi,156,57,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/CaNS-World/CaNS,https://github.com/CaNS-World/CaNS,1,,,1,1,1,1,0,0,0,0,0,0,1,"A code for fast, massively-parallel direct numerical simulations (DNS) of canonical flows","<!--- the logo -->\n<img src=""assets/img/CaNS-logo.png"" height=100>\n\n## Synopsis\n\n**CaNS (Canonical Navier-Stokes)** is a code for massively-parallel numerical simulations of fluid flows. It aims at solving any fluid flow of an incompressible, Newtonian fluid that can benefit from a FFT-based solver for the second-order finite-difference Poisson equation in a 3D Cartesian grid. In two directions the grid is regular and the solver supports the following combination of (homogeneous) boundary conditions:\n\n * Neumann-Neumann\n * Dirichlet-Dirichlet\n * Neumann-Dirichlet\n * Periodic\n\nIn the third domain direction, the solver is more flexible as it uses Gauss elimination. There the grid can also be non-uniform (e.g. fine at the boundary and coarser in the center).\n\nCaNS also allows for choosing an implicit temporal discretization of the momentum diffusion terms, either fully implicit or only along the last domain direction. This results in solving a 3D/1D Helmholtz equation per velocity component. In the fully implicit case, FFT-based solvers are also used, and the same options described above for pressure boundary conditions apply to the velocity.\n\n**Reference**\n\nP. Costa. *A FFT-based finite-difference solver for massively-parallel direct numerical simulations of turbulent flows.* *Computers & Mathematics with Applications* 76: 1853--1862 (2018). [doi:10.1016/j.camwa.2018.07.034](https://doi.org/10.1016/j.camwa.2018.07.034) [[arXiv preprint]](https://arxiv.org/abs/1802.10323)\n\n## News\n\n**[10/08/2023]:** The input files `dns.in` and `cudecomp.in` have been replaced with the namelist file `input.nml`, which makes parsing of input files and extensions in more complex solvers based on CaNS simpler. See the updated [`docs/INFO_INPUT.md`](docs/INFO_INPUT.md) file for more details. Additionally, we have added a new input parameter, `gtype` to explicitly select the type of grid stretching function.\n\n**[03/02/2023]:** The input file `dns.in` has been simplified to avoid a common source of confusion. Instead of prescribing `uref`, `lref`, and `rey` (reference velocity and length scales, and Reynolds number) to calculate the fluid viscosity as `visc = uref*lref/rey`, we directly prescribe the inverse of the viscosity, `visci` (`visc = visci**(-1)`), so all inputs are dimensional (see the updated [`docs/INFO_INPUT.md`](docs/INFO_INPUT.md) file). Note that `visci` has the same value as the flow Reynolds number for all files under `examples`, as `uref` and `lref` were always equal to `1`. *This change is backwards-incompatible - former input files should be updated from [v2.2.0](https://github.com/CaNS-World/CaNS/tree/v2.2.0) onward!*\n\n**[24/10/2022]:** Option `SINGLE_PRECISION_POISSON` has been removed from the `main` branch. While solving the Poisson in lower precision equation yields excellent results for many benchmarks, several of these cases also perform well when the whole calculation is performed in lower precision (see https://github.com/CaNS-World/CaNS/pull/42). Since this mode introduces significant complexity, it will be removed from the main branch for now in favor of a more readable code, a decision that can be reconsidered in the future. This option can still be explored in [v2.0.1](https://github.com/CaNS-World/CaNS/tree/v2.0.1), and is valuable for, e.g., setups with high Reynolds numbers and/or with extremely fine grids.\n\n### _Major Update:_ [`CaNS 2.0`](docs/CaNS-2.0.md) _is finally out!_ :tada:\n**`CaNS 2.0` has many new features, being the result of the most significant revision effort undertaken so far.** It includes major improvements in performance and robustness, and a fresh hardware-adaptive many-GPU parallelization using the [*cuDecomp*](https://github.com/NVIDIA/cuDecomp) library. See [`docs/CaNS-2.0.md`](docs/CaNS-2.0.md) for a detailed description of all new features. CaNS 2.0 has been tested and observed to run efficiently on some major GPU-accelerated clusters such as Perlmutter, Summit, and Marconi 100.\n\n## Features\n\nSome features are:\n\n * Hybrid MPI/OpenMP parallelization\n * FFTW guru interface / cuFFT used for computing multi-dimensional vectors of 1D transforms\n * The right type of transformation (Fourier, cosine, sine, etc) is automatically determined form the input file\n * [cuDecomp](https://github.com/NVIDIA/cuDecomp) pencil decomposition library for _hardware-adaptive_ distributed memory calculations on _many GPUs_\n * [2DECOMP&FFT](https://github.com/xcompact3d/2decomp-fft) library used for performing global data transpositions on CPUs and some of the data I/O\n * GPU acceleration using OpenACC directives\n * A different canonical flow can be simulated just by changing the input files\n\nSome examples of flows that this code can solve are:\n\n * periodic or developing channel\n * periodic or developing square duct\n * tri-periodic domain\n * lid-driven cavity\n\n## Motivation\n\nThis project aimed first at being a modern alternative to the well-known FISHPACK routines (Paul Swarztrauber & Roland Sweet, NCAR) for solving a three-dimensional Helmholtz equation. After noticing some works simulating canonical flows with iterative solvers -- when faster direct solvers could have been used instead -- it seemed natural to create a versatile tool and make it available. This code can be used as a first base code for which solvers for more complex flows can be developed (e.g. extensions with fictitious domain methods).\n\n## Method\n\nThe fluid flow is solved with a second-order finite difference pressure correction scheme, discretized in a MAC grid arrangement. Time is advanced with a three-step low storage Runge-Kutta scheme. Optionally, for increased stability at low Reynolds numbers, at the price of higher computational demand, the diffusion term can be treated implicitly. See the reference above for details.\n\n## Usage\n\n### Downloading *CaNS*\n\nSince *CaNS* loads the external pencil decomposition libraries as Git Submodules, the repository should be cloned as follows:\n```bash\ngit clone --recursive https://github.com/CaNS-World/CaNS\n```\nso the libraries are downloaded too. Alternatively, in case the repository has already been cloned without the Submodules (i.e., folders `cuDecomp` and `2decomp-fft` under `dependencies/` are empty), the following command can be used to update them:\n```bash\ngit submodule update --init --recursive\n```\n\n### Compilation\n\n#### Prerequisites\nThe prerequisites for compiling CaNS are the following:\n\n * MPI\n * FFTW3/cuFFT library for CPU/GPU runs\n * The `nvfortran` compiler (for GPU runs)\n * NCCL and NVSHMEM (optional, may be exploited by the cuDecomp library)\n * OpenMP (optional)\n\n#### In short\nFor most systems, CaNS can be compiled from the root directory with the following commands `make libs && make`, which will compile the 2DECOMP&FFT/cuDecomp libraries, and CaNS.\n\n#### Detailed instructions\nThe `Makefile` in root directory is used to compile the code, and is expected to work out-of-the-box for most systems. The `build.conf` file in the root directory can be used to choose the Fortran compiler (MPI wrapper), and a few pre-defined profiles depending on the nature of the run (e.g., production vs debugging), and pre-processing options, see [`INFO_COMPILING.md`](docs/INFO_COMPILING.md) for more details. Concerning the pre-processing options, the following are available:\n\n * `DEBUG`                    : performs some basic checks for debugging purposes\n * `TIMING`                   : wall-clock time per time step is computed\n * `IMPDIFF`                  : diffusion terms are integrated implicitly in time (thereby improving the stability of the numerical algorithm for viscous-dominated flows)\n * `IMPDIFF_1D`               : same as above, but with implicit diffusion *only* along Z; *for optimal parallel performance this option should be combined with* `PENCIL_AXIS=3`\n * `PENCIL_AXIS`              : sets the default pencil direction, one of [1,2,3] for [X,Y,Z]-aligned pencils; X-aligned is the default and should be optimal for all cases except for Z implicit diffusion, where using Z-pencils is recommended\n * `SINGLE_PRECISION`         : calculation will be carried out in single precision (the default precision is double)\n * `GPU`                      : enable GPU-accelerated runs\n * `USE_NVTX`                 : enable [NVTX](https://s.nvidia.com/nsight-visual-studio-edition/nvtx) tags for profiling\n\n### Input file\n\nThe input file `input.nml` sets the physical and computational parameters. In the `examples/` folder are examples of input files for several canonical flows. See [`INFO_INPUT.md`](docs/INFO_INPUT.md) for a detailed description of the input file.\n\nFiles `out1d.h90`, `out2d.h90` and `out3d.h90` in `src/` set which data are written in 1-, 2- and 3-dimensional output files, respectively. *The code should be recompiled after editing out?d.h90 files*.\n\n### Running the code\n\nRun the executable with `mpirun` with a number of tasks complying to what has been set in the input file `dns.in`. Data will be written by default in a folder named `data/`, which must be located where the executable is run (by default in the `run/` folder).\n\n### Visualizing field data\n\nSee [`INFO_VISU.md`](docs/INFO_VISU.md).\n\n## Contributing\n\nWe appreciate any contributions and feedback that can improve CaNS. If you wish to contribute to the tool, please get in touch with the maintainers or open an Issue in the repository / a thread in Discussions. Pull Requests are welcome, but please propose/discuss the changes in an linked Issue first.\n\n## Final notes\n\nPlease read the `ACKNOWLEDGEMENTS`, `LICENSE` files.\n",189,fluid-dynamics,Fortran,6,Fortran,Makefile,Python,Shell,MATLAB,Awk,,,,,,,,,,,,,,,,,,,,,,,81,2,79,0,2,6,0,862,66,40,36,4,4b17b50d6e0c3c7bf6d8bb72fd25af187dfb522e,3D Antuono vortex initial field. (#123),2024-07-18T16:48:47Z,gianlupo,40678236+gianlupo@users.noreply.github.com,gianlupo,v2.3.4,## What's Changed\r\n* Add option of task-local checkpointing. by @p-costa in https://github.com/CaNS-World/CaNS/pull/110\r\n* Updated cuDecomp submodule. by @p-costa in https://github.com/CaNS-World/CaNS/pull/117\r\n\r\n**Full Changelog**: https://github.com/CaNS-World/CaNS/compare/v2.3.2...v2.3.4,v2.3.4,Pedro Costa,,p-costa,MIT License,CaNS,CaNS-World,19,fluid-dynamics,fluid-simulation,computational-fluid-dynamics,turbulence,high-performance-computing,cfd,fortran,gpu,gpu-computing,,,,,,,,,,,,/CaNS-World/CaNS,20,15,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/camicroscope/caMicroscope,https://github.com/camicroscope/caMicroscope,0.5,"Image viewer with extra features, but used in scintific purpose. ",0,0,1,0,0,0,0,1,0,0,0,0,Digital pathology image viewer with support for human/machine generated annotations and markups.,"<h2 align=""center"">\n  <a href=""http://camicroscope.org/""><img src=""https://avatars2.githubusercontent.com/u/12075069?s=400&v=4"" style=""background-color:rgba(0,0,0,0);"" height=230 alt=""camicroscope: a web-based image viewer optimized for large bio-medical image data viewing""></a>\n</h2>\n\ncaMicroscope is a web-based biomedical image and data viewer, with a strong emphasis on cancer pathology WSI (Whole Slide Imaging).\nThis guide has sections for different kinds of use of the platform. The [User Guide](#user-guide) covers the basics on how to use caMicroscope viewer. <a href=""https://github.com/SBU-BMI/Nanoborb#nanoborb"" target=""_blank"" rel=""noopener noreferrer"">nanoBorb</a> covers nanoBorb, the version of caMicroscope designed as a standalone application for individual users without a server. [Hosted Setup](#hosted-setup) covers how to set up caMicroscope for multiple users on a server. [Developer Guide](#developer-guide) covers the broad strokes on how to add new functionality to caMicroscope.\n\n![View Slides](docs/img/View.gif)\n![Measure Features](docs/img/Measure.gif)\n![Annotate Areas of Interest](docs/img/Draw.gif)\n![Alternate Annotation Method](docs/img/Paint.gif)\n![Automatic Object Detection](docs/img/Segment.gif)\n![Test Classification Models](docs/img/Predict.gif)\n\n# User Guide\n\n## Selecting an Image\nDepending on what is providing the image metadata, a different login process may be necessary. For public instances, no log in is necessary, and you can proceed to view slides. Use of other tools, such as annotations may or may not require login in this case.\nFor slim instances, login should be done through a redirect directly. For pathDB instances, login should be done on the login link on the main page.\nAt this point, select a collection, if applicable, and proceed to open or ""view"" the image of your choice.\n\n## Viewing an Image\nOnce an image is open, you can pan around the image by either clicking and dragging (when no conflicting tool, such as the pen, is open), or by moving the red bounding box in the viewport in the bottom right.\nZooming can be accomplished through the scroll wheel, pinch events on a touch screen, by using the zoom slider or its associated buttons, or by clicking on the zoom number and inputting a different number.\n\n## Using Tools\nThe toolbar is in the top-left of the main content window. Use the toolbar buttons to manipulate the slide. To close any toolbar button, click the same button again or a new button.\n\n| Tool  | Name        | Function  |\n| ----- |-------------| -----|\n| ![](https://fonts.gstatic.com/s/i/materialicons/apps/v4/24px.svg)      | Annotations | Opens the Annotation panel, where you can select which annotation set to view, name that annotation set, add optional notes about the annotation set, save the annotation set, and reset the panel to its original state. |\n| ![](https://fonts.gstatic.com/s/i/materialicons/view_list/v4/24px.svg)      | Layer Manager      | Opens the Layers Manager panel, where you can select which layers to view. |\n| ![](https://fonts.gstatic.com/s/i/materialicons/home/v4/24px.svg)      | Home      | Return to the data table so that you can open another slide.|\n| ![](https://fonts.gstatic.com/s/i/materialicons/create/v4/24px.svg)      | Draw      |  Draw thin lines, thick lines, or polygons on the image. Annotations can also be computer aided using the Smart-pen tool. Draw them, stretch them, remove them. To maintain the integrity of measurements, avoid drawing shapes that overlap or intersect one another. |\n| ![](https://fonts.gstatic.com/s/i/materialicons/colorize/v4/24px.svg)      | Preset Labels      |  Use a preset annotation type immediately to quickly annotate a slide consistently. |\n| ![](https://fonts.gstatic.com/s/i/materialicons/search/v4/24px.svg)       | Magnifier      |The Magnifier works like a magnifying glass and allows you to see the slide at normal magnification (1.0), low magnification (0.5), or high magnification (2.0). Click a magnification level and place the bounding box on the area of the slide you want to magnify. |\n| ![](https://fonts.gstatic.com/s/i/materialicons/space_bar/v4/24px.svg)      | Measurement      | Drag this tool on the slide to learn the measurement in micrometers. |\n| ![](https://fonts.gstatic.com/s/i/materialicons/share/v4/24px.svg)      | Share View      |Opens a window with a URL to the current presentation state of the slide including the magnification level, layers that are currently open, and your position on the image.|\n| ![](https://fonts.gstatic.com/s/i/materialicons/view_carousel/v4/24px.svg)      | Side by Side Viewer     |Shows the Layer Manager panel, the left and right layers, and inset window. For the right and left layer, select which layer you want to view. |\n| ![](https://fonts.gstatic.com/s/i/materialicons/satellite/v4/24px.svg)      | Heatmap     | For a slide with heatmap data, opens the choices of heatmaps available, as well as ways of displaying the heatmaps. The gradient shows all of the values on the selected spectrum for the field you selected. Contains a heatmap edit pen function.|\n| ![](https://fonts.gstatic.com/s/i/materialicons/label/v4/24px.svg)      | Labeling      |Use this tool to draw a circle or rectangle around a tumor region, measure an area on the slide, download labels, and submit a bug report. The Labeling tool has its own toolbar with tools in the following order from left to right: return to the previous slide, place a square on the slide, place a circle on the slide, measure an area, download labels, and submit a bug report. Click the left arrow at the far right of the toolbar to hide it, then click the right arrow to show it. |\n| ![](https://fonts.gstatic.com/s/i/materialicons/timeline/v6/24px.svg)      | Segment      | This tool allows you to display, count, and export nuclear segmentations on the image. Clicking this tool opens the following custom toolbar. |\n| ![](https://fonts.gstatic.com/s/i/materialicons/aspect_ratio/v4/24px.svg)      | Model      | Show results from a pre-trained tensorflow compatible model on a ROI of the slide. |\n| ![](https://fonts.gstatic.com/s/i/materialicons/get_app/v4/24px.svg)      | Download Slide      | Download the slide image to your system |\n| ![](https://fonts.gstatic.com/s/i/materialicons/playlist_add_check/v8/24px.svg)      | Mark Reviewed      | Use to signify the completion of review of a slide. |\n| ![](https://fonts.gstatic.com/s/i/materialicons/bug_report/v4/24px.svg)      | Bug Report      | Report a bug or give feedback. |\n| ![](https://fonts.gstatic.com/s/i/materialicons/camera_enhance/v4/24px.svg)      | Slide Capture      | Click to take a screenshot of the slide and annotations on it. |\n| ![](https://fonts.gstatic.com/s/i/materialicons/help/v4/24px.svg)      | Tutorial      | Click to view a guided tour of the viewer tools. |\n\n\n## Toolbar Shortcuts\n\n| Tool         | Shortcut  |\n|------------- |-----------|\n| Annotation   |  Ctrl + a |\n| Magnifier    |  Ctrl + m |\n| Measurement    |  Ctrl + r |\n| Side-by-Side |  Ctrl + s |\n| Close all tools |  ESC   |\n\n# Hosted Setup\nThe full distribution repository for hosted caMicroscope is [here](https://github.com/camicroscope/Distro/).\nrun with `docker-compose -f caMicroscope.yml up`\n\nthis will build all services and run in the foreground.\nUse `docker-compose -f caMicroscope.yml build` to rebuild the services.\n\nOnce everything is up, go to \<the host this is running on\>:4010/ to see the landing page.\n\n# Other Resources\n- **Slack:** <http://bit.ly/camicroscope>\n- **Discussion mailing list:** <https://groups.google.com/g/camicroscope>\n- **Sample Tensorflow Models:** <https://github.com/camicroscope/tfjs-models>\n\n# Developer Guide\nWe are collecting feedback to write this section in more detail. Please add your suggestions [here](https://github.com/camicroscope/caMicroscope/issues/267).\n\ncaMicroscope is open source software. Any involvement and contribution with the caMicroscope project is greatly appreciated. Feel free to get directly involved in any of the repositories in the caMicroscope organization. New developers may find the notes in [CONTRIBUTING](https://github.com/camicroscope/caMicroscope/blob/master/CONTRIBUTING.md) helpful to start contributing to caMicroscope.\n\nIt is highly recommended to make any changes off of the develop branch of a repository, and, when ready, create a PR to the develop branch of the source repository. Before sending the PR, make sure that there are no linting errors by running ```npm install``` and then ```npm run lint```  to see the errors and ```npm run lint-fix``` to automatically fix the errors in the repository folder.\n\nSource code organization ie the file structure of caMicroscope can be found in [file structure](https://github.com/camicroscope/caMicroscope/blob/master/docs/file_structure.md)\n\n## Fast Local Changes\nWhen using the hosted setup, you can have the distribution host the changes from your local. Follow these steps :\n- Clone this repository, the [Caracal repository](https://github.com/camicroscope/Caracal/) and [the distribution](https://github.com/camicroscope/Distro/) in the same parent directory\n- Set the build to build your local changes instead of the hosted git versions by editing the ca-back container section of your develop.yml. Replace the build context section with the path to your caracal checkout (""../Caracal""), and add `- ../caMicroscope:/src/camicroscope` to the volumes.\n- Remove this line from 'Dockerfile' in Caracal repository :\n```\nRUN git clone https://github.com/${fork:-camicroscope}/camicroscope.git --branch=${viewer:-master}\n```\n- In Distro repository, enter the following commands :\n```\ndocker-compose -f develop.yml build\ndocker-compose -f develop.yml up\n```\n",233,whole-slide-imaging,JavaScript,4,CSS,HTML,JavaScript,Dockerfile,,,,,,,,,,,,,,,,,,,,,,,,,660,158,447,55,18,65,0,104936,286,286,198,88,a9058bd34392c9398625b669d795f7d0500e7ff9,Merge pull request #667 from camicroscope/develop,2024-05-02T16:55:56Z,Birm,birm@rbirm.us,birm,03.12.2000,"### Accessibility and Standards Compliance\r\n\r\n- Created GitHub workflow for accessibility checks\r\n- Jest test for DrawerHelper JS\r\n- Various HTML pages updated to comply with W3C standards\r\n\r\n### UI/UX Improvements and Fixes\r\n\r\n- Adjustments for mobile view (e.g., overflow table scroll, button UI improvements)\r\n- Enhanced responsiveness (e.g., Workbench HTML page, option dropdown UI)\r\n- Clarity improvements (e.g., clearer instructions, fixing overlapping buttons)\r\n- Loader addition and UI adjustments\r\n- Transitioned from Google Form to GitHub issue link for feedback\r\n\r\n### Feature Additions and Enhancements\r\n\r\n- Added Aria Labels to Image Links\r\n- Added go back button and meta tag\r\n- Improved signup page user experience and consistency\r\n- Improved Slides Page Layout and User Guidance\r\n- (Secret) dicom connect demo\r\n\r\n### Bug Fixes and Error Handling\r\n\r\n- Fixed disabled zooming in mobile view\r\n- Fixed various linting errors and tile loading issues\r\n\r\n### Code Refactoring and Maintenance\r\n\r\n- Refactoring for improved footer reusability component\r\n- Improved config handling and file organization\r\n\r\n",v3.12.0,Birm,,birm,"BSD 3-Clause ""New"" or ""Revised"" License",caMicroscope,camicroscope,58,whole-slide-imaging,digital-pathology,camicroscope,slide-images,,,,,,,,,,,,,,,,,/camicroscope/caMicroscope,63,25,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/c3js/c3,https://github.com/c3js/c3,0,,,0,0,0,0,0,0,1,1,0,0,0,:bar_chart: A D3-based reusable chart library,"# c3\n\n[![CircleCI](https://circleci.com/gh/c3js/c3.svg?style=shield)](https://circleci.com/gh/c3js/c3)\n[![license](http://img.shields.io/badge/license-MIT-brightgreen.svg?style=flat)](https://github.com/c3js/c3/blob/master/LICENSE)\n[![codecov.io](https://codecov.io/github/c3js/c3/coverage.svg?branch=master)](https://codecov.io/github/c3js/c3?branch=master)\n\n[![jsDelivr Hits](https://data.jsdelivr.com/v1/package/npm/c3/badge?style=rounded)](https://www.jsdelivr.com/package/npm/c3)\n\n> c3 is a D3-based reusable chart library that enables deeper integration of charts into web applications.\n\nFollow the link for more information: [http://c3js.org](http://c3js.org/)\n\n## Documentation\n\n+ [Getting Started](http://c3js.org/gettingstarted.html)\n+ [Examples](http://c3js.org/examples.html)\n+ [Full API Reference](https://c3js.org/reference.html)\n\nAdditional samples can be found in this repository:\n+ [https://github.com/c3js/c3/tree/master/htdocs/samples](https://github.com/c3js/c3/tree/master/htdocs/samples)\n\nYou can run these samples as:\n```\n$ npm run serve-static\n```\n\n## Google Group\nFor general C3.js-related discussion, please visit our [Google Group at https://groups.google.com/forum/#!forum/c3js](https://groups.google.com/forum/#!forum/c3js).\n\n## Gitter\n[![Join the chat at https://gitter.im/c3js/c3](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/c3js/c3?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n## Using the issue queue\nThe [issue queue](https://github.com/c3js/c3/issues) is to be used for reporting defects and problems with C3.js, in addition to feature requests and ideas. It is **not** a catch-all support forum. **For general support enquiries, please use the [Google Group](https://groups.google.com/forum/#!forum/c3js) at https://groups.google.com/forum/#!forum/c3js.** All questions involving the interplay between C3.js and any other library (such as AngularJS) should be posted there first!\n\nBefore reporting an issue, please do the following:\n\n1. [Search for existing issues](https://github.com/c3js/c3/issues) to ensure you're not posting a duplicate.\n\n1.  [Search the Google Group](https://groups.google.com/forum/#!forum/c3js) to ensure it hasn't been addressed there already.\n\n1. Create a JSFiddle or Plunkr highlighting the issue. Please don't include any unnecessary dependencies so we can isolate that the issue is in fact with C3. *Please be advised that custom CSS can modify C3.js output!*\n\n1. When posting the issue, please use a descriptive title and include the version of C3 (or, if cloning from Git, the commit hash — C3 is under active development and the master branch contains the latest dev commits!), along with any platform/browser/OS information that may be relevant.\n\n## Pull requests\nPull requests are welcome, though please post an issue first to see whether such a change is desirable.\nIf you choose to submit a pull request, please do not bump the version number unless asked to, and please include test cases for any new features. Squash all your commits as well, please.\n\n## Playground\nPlease fork this fiddle:\n\n+ http://jsfiddle.net/7kYJu/4742/\n\n## Dependency\n\n+ [D3.js](https://github.com/mbostock/d3) `^5.0.0`\n\n## License\n\nMIT\n",9334,graphics,JavaScript,6,JavaScript,CSS,HTML,Ruby,TypeScript,SCSS,,,,,,,,,,,,,,,,,,,,,,,648,185,409,54,13,148,61,15038,1391,2245,1517,728,e8efa2996ed41d84a2bc4ec8a0460aa2e583d6b8,chore(deps): bump socket.io from 4.4.1 to 4.7.5 (#2896),2024-06-20T06:12:31Z,dependabot[bot],49699333+dependabot[bot]@users.noreply.github.com,dependabot[bot],,# Changes\r\n\r\n- Fix config subchart_axis_x_show (#2806)\r\n- Fix pan of y-axis when zoomed and rotated=true #2799 (#2805),v0.7.20,Yoshiya Hinosawa,,kt3k,MIT License,c3,c3js,94,chart,d3,graphics,data-visualization,svg,graph,interactive-visualizations,,,,,,,,,,,,,,/c3js/c3,124,268,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/bwa-mem2/bwa-mem2,https://github.com/bwa-mem2/bwa-mem2,0.5,"Scientific, but small",0,0,1,1,0,0,0,0,0,0,0,0,The next version of bwa-mem,"[![GitHub Downloads](https://img.shields.io/github/downloads/bwa-mem2/bwa-mem2/total?label=GitHub%20Downloads)](https://github.com/bwa-mem2/bwa-mem2/releases)\n[![BioConda Install](https://img.shields.io/conda/dn/bioconda/bwa-mem2?label=BioConda%20Installs)](https://anaconda.org/bioconda/bwa-mem2)\n\n## Important Information\n\n***We are happy to announce that the index size on disk is down by 8 times and in memory by 4 times due to moving to only one type of FM-index (2bit.64 instead of 2bit.64 and 8bit.32) and 8x compression of suffix array. For example, for human genome, index size on disk is down to ~10GB from ~80GB and memory footprint is down to ~10GB from ~40GB.***\n***There is a substantial reduction in index IO time due to the reduction and hardly any performance impact on read mapping.***\n***Due to this change in index structure (in commit #4b59796, 10th October 2020), you will need to rebuild the index.***\n\n***Added MC flag in the output sam file in commit a591e22. Output should match original bwa-mem version 0.7.17.***\n\n***As of commit e0ac59e, we have a git submodule safestringlib. To get it, use --recursive while cloning or use ""git submodule init"" and ""git submodule update"" in an already cloned repository (See below for more details).***\n\n\n## Getting Started\n```sh\n# Use precompiled binaries (recommended)\ncurl -L https://github.com/bwa-mem2/bwa-mem2/releases/download/v2.2.1/bwa-mem2-2.2.1_x64-linux.tar.bz2 \\n  | tar jxf -\nbwa-mem2-2.2.1_x64-linux/bwa-mem2 index ref.fa\nbwa-mem2-2.2.1_x64-linux/bwa-mem2 mem ref.fa read1.fq read2.fq > out.sam\n\n# Compile from source (not recommended for general users)\n# Get the source\ngit clone --recursive https://github.com/bwa-mem2/bwa-mem2\ncd bwa-mem2\n# Or\ngit clone https://github.com/bwa-mem2/bwa-mem2\ncd bwa-mem2\ngit submodule init\ngit submodule update\n# Compile and run\nmake\n./bwa-mem2\n```\n\n## Introduction\n\nThe tool bwa-mem2 is the next version of the bwa-mem algorithm in [bwa][bwa]. It\nproduces alignment identical to bwa and is ~1.3-3.1x faster depending on the use-case, dataset and the running machine.\n\nThe original bwa was developed by Heng Li (@lh3). Performance enhancement in\nbwa-mem2 was primarily done by Vasimuddin Md (@yuk12) and Sanchit Misra (@sanchit-misra)\nfrom Parallel Computing Lab, Intel.\nbwa-mem2 is distributed under the MIT license.\n\n## Installation\n\nFor general users, it is recommended to use the precompiled binaries from the\n[release page][rel]. These binaries were compiled with the Intel compiler and\nruns faster than gcc-compiled binaries. The precompiled binaries also\nindirectly support CPU dispatch. The `bwa-mem2` binary can automatically choose\nthe most efficient implementation based on the SIMD instruction set available\non the running machine. Precompiled binaries were generated on a CentOS7\nmachine using the following command line:\n```sh\nmake CXX=icpc multi\n```\n\n[bwa]: https://github.com/lh3/bwa\n[rel]: https://github.com/bwa-mem2/bwa-mem2/releases\n\n## Usage\n\nThe usage is exactly same as the original BWA MEM tool. Here is a brief synopsys. Run ./bwa-mem2 for available commands.\n\n```sh\n# Indexing the reference sequence (Requires 28N GB memory where N is the size of the reference sequence).\n./bwa-mem2 index [-p prefix] <in.fasta>\nWhere \n<in.fasta> is the path to reference sequence fasta file and \n<prefix> is the prefix of the names of the files that store the resultant index. Default is in.fasta.\n\n# Mapping \n# Run ""./bwa-mem2 mem"" to get all options\n./bwa-mem2 mem -t <num_threads> <prefix> <reads.fq/fa> > out.sam\nWhere <prefix> is the prefix specified when creating the index or the path to the reference fasta file in case no prefix was provided.\n```\n\n## Performance\n\nDatasets:  \nReference Genome: human_g1k_v37.fasta\n\n Alias	    |  Dataset source				|  No. of reads	| Read length \n --------- | --------- | --------- | --------- \n D1	|  Broad Institute				|  2 x 2.5M	bp	|	151bp\n D2	|  SRA: SRR7733443				|  2 x 2.5M	bp	|	151bp  \n D3	|  SRA: SRR9932168				|  2 x 2.5M	bp	|	151bp  \n D4	|  SRA: SRX6999918				|  2 x 2.5M	bp	|	151bp  \n\n\n\nMachine details:  \nProcessor: Intel(R) Xeon(R) 8280 CPU @ 2.70GHz  \nOS: CentOS Linux release 7.6.1810  \nMemory: 100GB  \n\n\nWe followed the steps below to collect the performance results:  \nA. Data download steps:\n1. Download SRA toolkit from https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software#header-global    \n2. tar xfzv sratoolkit.2.10.5-centos_linux64.tar.gz  \n3. Download D2: sratoolkit.2.10.5-centos_linux64/bin/fastq-dump --split-files SRR7733443   \n4. Download D3: sratoolkit.2.10.5-centos_linux64/bin/fastq-dump --split-files SRR9932168   \n5. Download D4: sratoolkit.2.10.5-centos_linux64/bin/fastq-dump --split-files SRX6999918   \n\n\n\nB. Alignment steps:   \n1. git clone https://github.com/bwa-mem2/bwa-mem2.git   \n2. cd bwa-mem2   \n3. ```make CXX=icpc``` (using intel C/C++ compiler)   \nor   ```make``` (using gcc compiler)   \n4. ./bwa-mem2 index <ref.fa>   \n5. ./bwa-mem2 mem [-t <#threads>] <ref.fa> <in_1.fastq> [<in_2.fastq>]  >  <output.sam>   \n\nFor example,  in our double socket (56 threads each) and double numa compute node, we used the following command line to align D2 to human_g1k_v37.fasta reference genome.  \n```\nnumactl -m 0 -C 0-27,56-83 ./bwa-mem2 index human_g1k_v37.fasta  \nnumactl -m 0 -C 0-27,56-83 ./bwa-mem2 mem -t 56 human_g1k_v37.fasta SRR7733443_1.fastq SRR7733443_2.fastq > d2_align.sam\n```\n\n<p align=""center"">\n<img src=""https://github.com/bwa-mem2/bwa-mem2/blob/master/images/bwa-mem2-1.png"" height=""400""/a></br>\n<img src=""https://github.com/bwa-mem2/bwa-mem2/blob/master/images/bwa-mem2-2.png"" height=""400""/a></br>\n<img src=""https://github.com/bwa-mem2/bwa-mem2/blob/master/images/bwa-mem2-3.png"" height=""400""/a></br>\n<img src=""https://github.com/bwa-mem2/bwa-mem2/blob/master/images/bwa-mem2-4.png"" height=""400""/a></br>\n</p> \n\n## bwa-mem2 seeding phase accelerated using LISA (Learned-Indexes for Sequence Analysis)\n\nbwa-mem2-lisa is an accelerated version of bwa-mem2 where we apply learned-indexes to the seeding phase. bwa-mem2-lisa branch contains the source code of the implementation. Following are the features of bwa-mem2-lisa:\n1. Exact same output as bwa-mem2.\n2. All command-lines for creating an index and the read mapping are exactly same as bwa-mem2.\n3. bwa-mem2-lisa accelerates seeding phase (one of the major bottlenecks in bwa-mem2) by up to 4.5x compared to bwa-mem2.\n4. The memory footprint of bwa-mem2-lisa index is ~120GB for human genome.\n5. The code is present in bwa-mem2-lisa branch: https://github.com/bwa-mem2/bwa-mem2/tree/bwa-mem2-lisa\n\n\n## bwa-mem2 seeding speedup with Enumerated Radix Trees (Code in ert branch)\n\nThe ert branch of bwa-mem2 repository contains codebase of enuerated radix tree based acceleration of bwa-mem2. The ert code is built on the top of bwa-mem2 (thanks to the hard work by @arun-sub). \nThe following are the highlights of the ert based bwa-mem2 tool: \n1. Exact same output as bwa-mem(2) \n2. The tool has two additional flags to enable the use of ert solution (for index creation and mapping), else it runs in vanilla bwa-mem2 mode \n3. It uses 1 additional flag to create ert index (different from bwa-mem2 index) and 1 additional flag for using that ert index (please see the readme of ert branch) \n4. The ert solution is 10% - 30% faster (tested on above machine configuration) in comparison to vanilla bwa-mem2 -- users are adviced to use option `-K 1000000` to see the speedups \n5. The memory foot print of the ert index is ~60GB \n6. The code is present in ert branch: https://github.com/bwa-mem2/bwa-mem2/tree/ert\n\n\n## Citation\n\nVasimuddin Md, Sanchit Misra, Heng Li, Srinivas Aluru.\n<b> Efficient Architecture-Aware Acceleration of BWA-MEM for Multicore Systems. </b>\n<i> IEEE Parallel and Distributed Processing Symposium (IPDPS), 2019. [10.1109/IPDPS.2019.00041](https://doi.org/10.1109/IPDPS.2019.00041) </i>\n",698,bioinformatics,C++,3,Makefile,C++,C,,,,,,,,,,,,,,,,,,,,,,,,,,52,8,37,7,9,13,0,2945,93,210,127,83,7aa5ff6c3330490e5629ab9b7327683d2dce02d6,Merge pull request #229 from gh-jphan/fix_n_processed,2024-05-08T18:49:51Z,Vasimuddin Md,wasim.mzr@gmail.com,yuk12,v2.2.1: Hotfix for v2.2,Hotfix: Fixed the bug mentioned in #135.,v2.2.1,,,sanchit-misra,Other,bwa-mem2,bwa-mem2,6,bioinformatics,genomics,sequence-alignment,,,,,,,,,,,,,,,,,,/bwa-mem2/bwa-mem2,6,36,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/BVE-Reborn/rend3,https://github.com/BVE-Reborn/rend3,0,,,0,0,0,0,1,0,1,1,0,0,0,"MAINTENCE MODE ---- Easy to use, customizable, efficient 3D renderer library built on wgpu.","# rend3\n\n![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/BVE-Reborn/rend3/ci.yml)\n[![Crates.io](https://img.shields.io/crates/v/rend3)](https://crates.io/crates/rend3)\n[![Documentation](https://docs.rs/rend3/badge.svg)](https://docs.rs/rend3)\n![License](https://img.shields.io/crates/l/rend3)\n[![Matrix](https://img.shields.io/static/v1?label=rend3%20dev&message=%23rend3&color=blueviolet&logo=matrix)](https://matrix.to/#/#rend3:matrix.org)\n[![Matrix](https://img.shields.io/static/v1?label=rend3%20users&message=%23rend3-users&color=blueviolet&logo=matrix)](https://matrix.to/#/#rend3-users:matrix.org)\n[![Discord](https://img.shields.io/discord/451037457475960852?color=7289DA&label=discord)](https://discord.gg/mjxXTVzaDg)\n\n\nEasy to use, customizable, efficient 3D renderer library built on wgpu.\n\nLibrary is under active development. While internals are might change in the\nfuture, the external api remains stable, with only minor changes occuring as\nfeatures are added.\n\n## Examples\n\nTake a look at the [examples] for getting started with the api. The examples\nwill show how the core library and helper crates can be used.\n\n[examples]: https://github.com/BVE-Reborn/rend3/tree/trunk/examples\n\n#### Screenshots\n\nThese screenshots are from the scene_viewer example.\n\n![scifi-base](https://raw.githubusercontent.com/BVE-Reborn/rend3/trunk/examples/src/scene_viewer/scifi-base.jpg)\n![example](https://raw.githubusercontent.com/BVE-Reborn/rend3/trunk/examples/src/scene_viewer/screenshot.jpg)\n![bistro](https://raw.githubusercontent.com/BVE-Reborn/rend3/trunk/examples/src/scene_viewer/bistro.jpg)\n![emerald-square](https://raw.githubusercontent.com/BVE-Reborn/rend3/trunk/examples/src/scene_viewer/emerald-square.jpg)\n\n## Crates\n\nThe `rend3` ecosystem is composed of a couple core crates which provide most\nof the functionality and exensibility to the library, extension crates, and\nintegration crates\n\n#### Core\n\n- `rend3`: The core crate. Performs all handling of world data, provides the\n  Renderer and RenderGraph and defines vocabulary types.\n- `rend3-routine`: Implementation of various ""Render Routines"" on top of the\n  RenderGraph. Also provides for re-usable graphics work. Provides PBR\n  rendering, Skyboxes, Shadow Rendering, and Tonemapping.\n\n#### Extensions\n\nThere are extension crates that are not required, but provide pre-made bits\nof useful code that I would recommend using.\n\n- `rend3-framework`: Vastly simplifies correct handling of the window and\n  surface across platforms.\n- `rend3-gltf`: Modular gltf file and scene loader.\n\n#### Integration\n\nIntegration with other external libraries are also offered. Due to external\ndependencies, the versions of these may increase at a much higher rate than\nthe rest of the ecosystem.\n\n- `rend3-anim`: Skeletal animation playback utilities. Currently tied to rend3-gltf.\n- `rend3-egui`: Integration with the [egui](https://github.com/emilk/egui)\n  immediate mode gui.\n\n## Purpose\n\n`rend3` tries to fulfill the following usecases:\n 1. Games and visualizations that need a customizable, and efficient renderer.\n 2. Projects that just want to put objects on screen, but want lighting and effects.\n 3. A small cog in a big machine: a renderer that doesn't interfere with the rest of the program.\n\n`rend3` is not:\n 1. A framework or engine. It does not include all the parts needed to make an\n    advanced game or simulation nor care how you structure your program.\n    If you want a very basic framework to deal with windowing and event loop management,\n    `rend3-framework` can help you. This will always be optional and is just there to help\n    with the limited set of cases it can help.\n\n## Future Plans\n\nI have grand plans for this library. An overview can be found in the issue\ntracker under the [enhancement] label.\n\n[enhancement]: https://github.com/BVE-Reborn/rend3/labels/enhancement\n\n## Matrix Chatroom\n\nWe have a matrix chatroom that you can come and join if you want to chat\nabout using rend3 or developing it:\n\n[![Matrix](https://img.shields.io/static/v1?label=rend3%20dev&message=%23rend3&color=blueviolet&logo=matrix)](https://matrix.to/#/#rend3:matrix.org)\n[![Matrix](https://img.shields.io/static/v1?label=rend3%20users&message=%23rend3-users&color=blueviolet&logo=matrix)](https://matrix.to/#/#rend3-users:matrix.org)\n\nIf discord is more your style, our meta project has a channel which mirrors\nthe matrix rooms:\n\n[![Discord](https://img.shields.io/discord/451037457475960852?color=7289DA&label=discord)](https://discord.gg/mjxXTVzaDg)\n\n## Helping Out\n\nWe welcome all contributions and ideas. If you want to participate or have\nideas for this library, we'd love to hear them!\n\nLicense: MIT OR Apache-2.0 OR Zlib\n",1049,graphics,Rust,3,Rust,Shell,WGSL,,,,,,,,,,,,,,,,,,,,,,,,,,403,142,247,14,36,29,0,47391,59,189,150,39,d088a841b0469d07d5a7ff3f4d784e97b4a194d5,Fix Transparency (#598),2024-05-03T06:10:43Z,Connor Fitzgerald,connorwadefitzgerald@gmail.com,cwfitzgerald,v0.3.0 - RenderGraphs and Animations,"Released 2022-02-11\r\n\r\nThis is by far the largest release of rend3 since 0.1. I have done my best to document everything that has changed.\r\n\r\n### Major Changes\r\n- rend3-pbr got renamed to rend3-routine and will host all render routines, not just PBR related ones.\r\n\r\n### Added\r\n- rend3-egui: An integration with the immediate mode GUI [egui](https://github.com/emilk/egui) @MindSwipe\r\n- rend3-textured-quad: Add example of simple 2D rendering.\r\n- rend3: Allow duplicating objects overriding some of their properties. @setzer22\r\n- rend3: Implement mesh skinning. @setzer22\r\n- rend3: Added `CameraProjection::Raw`.\r\n- rend3: The renderer now has a `handedness` value that determines the handedness of its coordinate system.\r\n  This allows the renderer to deal with both DX-convention coordinate systems and OpenGL convention coordinate systems.\r\n- rend3-gltf: Load gltf animation data @setzer22\r\n\r\n### Changes\r\n- rend3: Instead of passing a render routine to the render function, \r\n  you now add them to a rendergraph, then pass that rendergraph into the renderer.\r\n  - rend3-routine:\r\n    - The old `PbrRoutine` has been replaced with the more vercitile `BaseRenderGraph`.\r\n    - This base rendergraph will put all the parts together through it's `add_to_graph` function.\r\n    - Split the PbrRoutine into two parts `add_prepass_to_graph` and `add_forward_to_graph`.\r\n    - Split out the skybox renderer into `SkyboxRoutine`.\r\n    - Split out tonemapping into the `TonemappingRoutine`.\r\n- rend3: Renamed the old `RendererMode` to `RendererProfile` and adjusted verbiage to refer to them as profiles.\r\n  - ""CpuPowered"" => ""CpuDriven"".\r\n  - ""GpuPowered"" => ""GpuDriven"".\r\n- rend3: All meshes now require validation.\r\n  - `MeshBuilder::new()` now takes a handedness of the mesh. If it doesn't match the handedness of the renderer, it flips the winding order.\r\n  - `MeshBuilder::build()` now returns a `Result<Mesh, MeshValidationError>`. This validation can be unsafely omitted.\r\n  - The implementation functions `Mesh::calculate_normals{,_for_buffers}` and `Mesh::calculate_tangents{,_for_buffers}` are now unsafe.\r\n- rend3: Renamed `CameraProjection::Projection` to `CameraProjection::Perspective`.\r\n- rend3: Allow requesting device features explicitly in `rend3::create_iad`. @setzer22\r\n- rend3: Objects now take a `mesk_kind` allowind definition of static or animated meshes. @setzer22\r\n- rend3-routine: All transparency now has backface culling enabled. Use `Mesh::double_side` or `MeshBuilder::with_double_side` to re-enable double sided transparency.\r\n- rend3-gltf: Split return value of `load_gltf` into per-scene data and per-instance data. @setzer22\r\n- rend3-gltf: The `nodes` vector is now flat instead of nested. Hierarchy is represented using indices. @setzer22\r\n\r\n### Fixes\r\n- rend3: Fixed objects becoming invalid after a vertex buffer resize occured.\r\n- rend3: Get vertex/index counts from RangeAllocator. @jamen\r\n- rend3: Fix compatibility comparison for RenderPassTargets. @setzer22",v0.3.0,Connor Fitzgerald,,cwfitzgerald,Apache License 2.0,rend3,BVE-Reborn,14,rust,wgpu,3d-graphics,rendering,vulkan,d3d12,metal,hacktoberfest,graphics,gamedev,rendergraph,,,,,,,,,,/BVE-Reborn/rend3,14,16,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/B-UMMI/chewBBACA,https://github.com/B-UMMI/chewBBACA,1,,,1,1,1,1,0,0,0,0,0,0,1,BSR-Based Allele Calling Algorithm,"\n[![PyPI](https://img.shields.io/badge/Install%20with-PyPI-blue)](https://pypi.org/project/chewBBACA/#description)\n[![Bioconda](https://img.shields.io/badge/Install%20with-bioconda-green)](https://anaconda.org/bioconda/chewbbaca)\n[![Conda](https://img.shields.io/conda/dn/bioconda/chewbbaca?color=green)](https://anaconda.org/bioconda/chewbbaca)\n[![chewBBACA](https://github.com/B-UMMI/chewBBACA/workflows/chewbbaca/badge.svg)](https://github.com/B-UMMI/chewBBACA/actions?query=workflow%3Achewbbaca)\n[![Documentation Status](https://readthedocs.org/projects/chewbbaca/badge/?version=latest)](https://chewbbaca.readthedocs.io/en/latest/?badge=latest)\n[![License: GPL v3](https://img.shields.io/github/license/B-UMMI/chewBBACA)](https://www.gnu.org/licenses/gpl-3.0)\n[![DOI:10.1099/mgen.0.000166](https://img.shields.io/badge/DOI-10.1099%2Fmgen.0.000166-blue)](http://mgen.microbiologyresearch.org/content/journal/mgen/10.1099/mgen.0.000166)\n\n# chewBBACA\n\n**chewBBACA** is a software suite for the creation and evaluation of core genome and whole genome MultiLocus Sequence \nTyping (cg/wgMLST) schemas and results. The ""BBACA"" stands for ""BSR-Based Allele Calling Algorithm"". BSR stands for \nBLAST Score Ratio as proposed by [Rasko DA et al.](http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-6-2). The ""chew"" part adds extra coolness to the name and could be thought of as ""Comprehensive and Highly Efficient Workflow"". chewBBACA allows to define the target loci in a schema based on multiple genomes (e.g. define target loci based on the distinct loci identified in a dataset of high-quality genomes for a species or lineage of interest) and performs allele calling to determine the allelic profiles of bacterial strains, easily scaling to thousands of genomes with modest computational resources. chewBBACA includes functionalities to annotate the schema loci, compute the set of loci that constitute the core genome for a given dataset, and generate interactive reports for schema and allele calling results evaluation to enable an intuitive analysis of the results in surveillance and outbreak detection settings or population studies. Pre-defined cg/wgMLST schemas can be downloaded from [Chewie-NS ](https://chewbbaca.online/) or adapted from other cg/wgMLST platforms.\n\n### Check the [documentation](https://chewbbaca.readthedocs.io/en/latest/index.html) for implementation details and guidance on using chewBBACA.\n\n## News\n\n## 3.3.9 - 2024-07-16\n\n- Fixed an issue related to sequence IDs interpreted by BLAST as PDB chain IDs.\n\n- Fixed an issue related to CDS counting when gene prediction returns no CDSs for one or more inputs.\n\nCheck our [Changelog](https://github.com/B-UMMI/chewBBACA/blob/master/CHANGELOG.md) to learn about the latest changes.\n\n## Citation\n\nWhen using chewBBACA, please use the following citation:\n\n> Silva M, Machado MP, Silva DN, Rossi M, Moran-Gilad J, Santos S, Ramirez M, Carriço JA. 2018. chewBBACA: A complete suite for gene-by-gene schema creation and strain identification. Microb Genom 4:000166. [doi:10.1099/mgen.0.000166](doi:10.1099/mgen.0.000166)\n",129,bioinformatics,HTML,7,Python,CSS,JavaScript,HTML,Makefile,Batchfile,Dockerfile,,,,,,,,,,,,,,,,,,,,,,70,10,58,2,6,14,0,76945,26,132,121,11,ee97bdb70eddfcfb80fc827468415cf0fc8ee8af,Updated Readme and Changelog.,2024-07-16T13:39:19Z,rfm-targa,rmamede@medicina.ulisboa.pt,rfm-targa,v3.3.9,#NAME?,v3.3.9,Rafael Mamede,,rfm-targa,GNU General Public License v3.0,chewBBACA,B-UMMI,18,wgmlst,cgmlst,mlst,genomics,outbreak-detection,bacterial-genome-analysis,allele-calling,bioinformatics,bacteria,bacterial-typing,,,,,,,,,,,/B-UMMI/chewBBACA,19,18,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/broadinstitute/gatk-sv,https://github.com/broadinstitute/gatk-sv,1,,,1,1,1,1,0,0,0,0,0,0,1,A structural variation pipeline for short-read sequencing,"# GATK-SV\n\nA structural variation discovery pipeline for Illumina short-read whole-genome sequencing (WGS) data.\n\n## Table of Contents\n* [Requirements](#requirements)\n* [Citation](#citation)\n* [Acknowledgements](#acknowledgements)\n* [Quickstart](#quickstart)\n* [Pipeline Overview](#overview)\n    * [Cohort mode](#cohort-mode)\n    * [Single-sample mode](#single-sample-mode)\n    * [gCNV model](#gcnv-training-overview)\n    * [Generating a reference panel](#reference-panel-generation)\n* [Module Descriptions](#descriptions)\n    * [GatherSampleEvidence](#gather-sample-evidence) - Raw callers and evidence collection\n    * [EvidenceQC](#evidence-qc) - Batch QC\n    * [TrainGCNV](#gcnv-training) - gCNV model creation\n    * [GatherBatchEvidence](#gather-batch-evidence) - Batch evidence merging, BAF generation, and depth callers\n    * [ClusterBatch](#cluster-batch) - Site clustering\n    * [GenerateBatchMetrics](#generate-batch-metrics) - Site metrics\n    * [FilterBatch](#filter-batch) - Filtering\n    * [MergeBatchSites](#merge-batch-sites) - Cross-batch site merging\n    * [GenotypeBatch](#genotype-batch) - Genotyping\n    * [RegenotypeCNVs](#regenotype-cnvs) - Genotype refinement (optional)\n    * [MakeCohortVcf](#make-cohort-vcf) - Cross-batch integration, complex event resolution, and VCF cleanup\n    * [Module 07](#module07) - Downstream Filtering\n    * [AnnotateVcf](#annotate-vcf) - Annotation\n    * [Module 09](#module09) - QC and Visualization\n    * Additional modules - Mosaic and de novo\n* [CI/CD](#cicd)\n* [Troubleshooting](#troubleshooting)\n\n\n## <a name=""requirements"">Requirements</a>\n\n\n### Deployment and execution:\n* A [Google Cloud](https://cloud.google.com/) account.\n* A workflow execution system supporting the [Workflow Description Language](https://openwdl.org/) (WDL), either:\n  * [Cromwell](https://github.com/broadinstitute/cromwell) (v36 or higher). A dedicated server is highly recommended.\n  * or [Terra](https://terra.bio/) (note preconfigured GATK-SV workflows are not yet available for this platform)\n* Recommended: [MELT](https://melt.igs.umaryland.edu/). Due to licensing restrictions, we cannot provide a public docker image or reference panel VCFs for this algorithm.\n* Recommended: [cromshell](https://github.com/broadinstitute/cromshell) for interacting with a dedicated Cromwell server.\n* Recommended: [WOMtool](https://cromwell.readthedocs.io/en/stable/WOMtool/) for validating WDL/json files.\n\n#### Alternative backends\nBecause GATK-SV has been tested only on the Google Cloud Platform (GCP), we are unable to provide specific guidance or support for other execution platforms including HPC clusters and AWS. Contributions from the community to improve portability between backends will be considered on a case-by-case-basis. We ask contributors to please adhere to the following guidelines when submitting issues and pull requests:\n\n1. Code changes must be functionally equivalent on GCP backends, i.e. not result in changed output\n2. Increases to cost and runtime on GCP backends should be minimal\n3. Avoid adding new inputs and tasks to workflows. Simpler changes are more likely to be approved, e.g. small in-line changes to scripts or WDL task command sections\n4. Avoid introducing new code paths, e.g. conditional statements\n5. Additional backend-specific scripts, workflows, tests, and Dockerfiles will not be approved\n6. Changes to Dockerfiles may require extensive testing before approval\n\nWe still encourage members of the community to adapt GATK-SV for non-GCP backends and share code on forked repositories. Here are a some considerations:\n* Refer to Cromwell's [documentation](https://cromwell.readthedocs.io/en/stable/backends/Backends/) for configuration instructions.\n* The handling and ordering of `glob` commands may differ between platforms.\n* Shell commands that are potentially destructive to input files (e.g. `rm`, `mv`, `tabix`) can cause unexpected behavior on shared filesystems. Enabling [copy localization](https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options) may help to more closely replicate the behavior on GCP.\n* For clusters that do not support Docker, Singularity is an alternative. See [Cromwell documentation on Singularity](https://cromwell.readthedocs.io/en/stable/tutorials/Containers/#singularity).\n* The GATK-SV pipeline takes advantage of the massive parallelization possible in the cloud. Local backends may not have the resources to execute all of the workflows. Workflows that use fewer resources or that are less parallelized may be more successful. For instance, some users have been able to run [GatherSampleEvidence](#gather-sample-evidence) on a SLURM cluster.\n\n### Data:\n* Illumina short-read whole-genome CRAMs or BAMs, aligned to hg38 with [bwa-mem](https://github.com/lh3/bwa). BAMs must also be indexed.\n* Family structure definitions file in [PED format](#ped-format).\n\n#### <a name=""ped-format"">PED file format</a>\nThe PED file format is described [here](https://gatk.broadinstitute.org/hc/en-us/articles/360035531972-PED-Pedigree-format). Note that GATK-SV imposes additional requirements:\n* The file must be tab-delimited.\n* The sex column must only contain 0, 1, or 2: 1=Male, 2=Female, 0=Other/Unknown. Sex chromosome aneuploidies (detected in [EvidenceQC](#evidence-qc)) should be entered as sex = 0.\n* All family, individual, and parental IDs must conform to the [sample ID requirements](#sampleids).\n* Missing parental IDs should be entered as 0.\n* Header lines are allowed if they begin with a # character.\nTo validate the PED file, you may use `src/sv-pipeline/scripts/validate_ped.py -p pedigree.ped -s samples.list`.\n\n#### <a name=""sample-exclusion"">Sample Exclusion</a>\nWe recommend filtering out samples with a high percentage of improperly paired reads (>10% or an outlier for your data) as technical outliers prior to running [GatherSampleEvidence](#gather-sample-evidence). A high percentage of improperly paired reads may indicate issues with library prep, degradation, or contamination. Artifactual improperly paired reads could cause incorrect SV calls, and these samples have been observed to have longer runtimes and higher compute costs for [GatherSampleEvidence](#gather-sample-evidence).\n\n#### <a name=""sampleids"">Sample ID requirements:</a>\n\nSample IDs must:\n* Be unique within the cohort\n* Contain only alphanumeric characters and underscores (no dashes, whitespace, or special characters)\n\nSample IDs should not:\n* Contain only numeric characters\n* Be a substring of another sample ID in the same cohort\n* Contain any of the following substrings: `chr`, `name`, `DEL`, `DUP`, `CPX`, `CHROM`\n\nThe same requirements apply to family IDs in the PED file, as well as batch IDs and the cohort ID provided as workflow inputs.\n\nSample IDs are provided to [GatherSampleEvidence](#gather-sample-evidence) directly and need not match sample names from the BAM/CRAM headers. `GetSampleID.wdl` can be used to fetch BAM sample IDs and also generates a set of alternate IDs that are considered safe for this pipeline; alternatively, [this script](https://github.com/talkowski-lab/gnomad_sv_v3/blob/master/sample_id/convert_sample_ids.py) transforms a list of sample IDs to fit these requirements. Currently, sample IDs can be replaced again in [GatherBatchEvidence](#gather-batch-evidence). \n\nThe following inputs will need to be updated with the transformed sample IDs:\n* Sample ID list for [GatherSampleEvidence](#gather-sample-evidence) or [GatherBatchEvidence](#gather-batch-evidence)\n* PED file\n\n\n## <a name=""citation"">Citation</a>\nPlease cite the following publication:\n[Collins, Brand, et al. 2020. ""A structural variation reference for medical and population genetics."" Nature 581, 444-451.](https://doi.org/10.1038/s41586-020-2287-8)\n\nAdditional references:\n[Werling et al. 2018. ""An analytical framework for whole-genome sequence association studies and its implications for autism spectrum disorder."" Nature genetics 50.5, 727-736.](http://dx.doi.org/10.1038/s41588-018-0107-y)\n\n\n## <a name=""acknowledgements"">Acknowledgements</a>\nThe following resources were produced using data from the All of Us Research Program and have been approved by the Program for public dissemination:\n\n* Genotype filtering model: ""aou_recalibrate_gq_model_file"" in ""inputs/values/resources_hg38.json""\n\nThe All of Us Research Program is supported by the National Institutes of Health, Office of the Director: Regional Medical Centers: 1 OT2 OD026549; 1 OT2 OD026554; 1 OT2 OD026557; 1 OT2 OD026556; 1 OT2 OD026550; 1 OT2 OD 026552; 1 OT2 OD026553; 1 OT2 OD026548; 1 OT2 OD026551; 1 OT2 OD026555; IAA #: AOD 16037; Federally Qualified Health Centers: HHSN 263201600085U; Data and Research Center: 5 U2C OD023196; Biobank: 1 U24 OD023121; The Participant Center: U24 OD023176; Participant Technology Systems Center: 1 U24 OD023163; Communications and Engagement: 3 OT2 OD023205; 3 OT2 OD023206; and Community Partners: 1 OT2 OD025277; 3 OT2 OD025315; 1 OT2 OD025337; 1 OT2 OD025276. In addition, the All of Us Research Program would not be possible without the partnership of its participants.\n\n\n## <a name=""quickstart"">Quickstart</a>\n\n#### WDLs\nThere are two scripts for running the full pipeline:\n* `wdl/GATKSVPipelineBatch.wdl`: Runs GATK-SV on a batch of samples.\n* `wdl/GATKSVPipelineSingleSample.wdl`: Runs GATK-SV on a single sample, given a reference panel\n\n#### Building inputs\nExample workflow inputs can be found in `/inputs`. Build using `scripts/inputs/build_default_inputs.sh`, which \ngenerates input jsons in `/inputs/build`. Except the MELT docker image, all required resources are available in public \nGoogle buckets. \n\nSome workflows require a Google Cloud Project ID to be defined in a cloud environment parameter group. Workspace builds \nrequire a Terra billing project ID as well. An example is  provided at `/inputs/values/google_cloud.json` but should \nnot be used, as modifying this file will cause tracked changes in the repository. Instead, create a copy in the same \ndirectory with the format `google_cloud.my_project.json` and modify as necessary.\n\nNote that these inputs are required only when certain data are located in requester pays buckets. If this does not \napply, users may use placeholder values for the cloud configuration and simply delete the inputs manually.\n\n#### MELT\n**Important**: The example input files contain MELT inputs that are NOT public (see [Requirements](#requirements)). These include:\n\n* `GATKSVPipelineSingleSample.melt_docker` and `GATKSVPipelineBatch.melt_docker` - MELT docker URI (see [Docker readme](https://github.com/talkowski-lab/gatk-sv-v1/blob/master/dockerfiles/README.md))\n* `GATKSVPipelineSingleSample.ref_std_melt_vcfs` - Standardized MELT VCFs ([GatherBatchEvidence](#gather-batch-evidence))\n\nThe input values are provided only as an example and are not publicly accessible. In order to include MELT, these values must be provided by the user. MELT can be disabled by deleting these inputs and setting `GATKSVPipelineBatch.use_melt` to `false`.\n\n#### Execution\nWe recommend running the pipeline on a dedicated [Cromwell](https://github.com/broadinstitute/cromwell) server with a [cromshell](https://github.com/broadinstitute/cromshell) client. A batch run can be started with the following commands:\n\n```\n> mkdir gatksv_run && cd gatksv_run\n> mkdir wdl && cd wdl\n> cp $GATK_SV_ROOT/wdl/*.wdl .\n> zip dep.zip *.wdl\n> cd ..\n> echo '{ ""google_project_id"": ""my-google-project-id"", ""terra_billing_project_id"": ""my-terra-billing-project"" }' > inputs/values/google_cloud.my_project.json\n> bash scripts/inputs/build_default_inputs.sh -d $GATK_SV_ROOT -c google_cloud.my_project\n> cp $GATK_SV_ROOT/inputs/build/ref_panel_1kg/test/GATKSVPipelineBatch/GATKSVPipelineBatch.json GATKSVPipelineBatch.my_run.json\n> cromshell submit wdl/GATKSVPipelineBatch.wdl GATKSVPipelineBatch.my_run.json cromwell_config.json wdl/dep.zip\n```\n\nwhere `cromwell_config.json` is a Cromwell [workflow options file](https://cromwell.readthedocs.io/en/stable/wf_options/Overview/). Note users will need to re-populate batch/sample-specific parameters (e.g. BAMs and sample IDs).\n\n## <a name=""overview"">Pipeline Overview</a>\nThe pipeline consists of a series of modules that perform the following:\n* [GatherSampleEvidence](#gather-sample-evidence): SV evidence collection, including calls from a configurable set of algorithms (Manta, MELT, and Wham), read depth (RD), split read positions (SR), and discordant pair positions (PE).\n* [EvidenceQC](#evidence-qc): Dosage bias scoring and ploidy estimation\n* [GatherBatchEvidence](#gather-batch-evidence): Copy number variant calling using cn.MOPS and GATK gCNV; B-allele frequency (BAF) generation; call and evidence aggregation\n* [ClusterBatch](#cluster-batch): Variant clustering\n* [GenerateBatchMetrics](#generate-batch-metrics): Variant filtering metric generation\n* [FilterBatch](#filter-batch): Variant filtering; outlier exclusion\n* [GenotypeBatch](#genotype-batch): Genotyping\n* [MakeCohortVcf](#make-cohort-vcf): Cross-batch integration; complex variant resolution and re-genotyping; vcf cleanup\n* [Module 07](#module07): Downstream filtering, including minGQ, batch effect check, outlier samples removal and final recalibration;\n* [AnnotateVcf](#annotate-vcf): Annotations, including functional annotation, allele frequency (AF) annotation and AF annotation with external population callsets;\n* [Module 09](#module09): Visualization, including scripts that generates IGV screenshots and rd plots.\n* Additional modules to be added: de novo and mosaic scripts \n\n\nRepository structure:\n* `/dockerfiles`: Resources for building pipeline docker images\n* `/inputs`: files for generating workflow inputs\n  * `/templates`: Input json file templates\n  * `/values`: Input values used to populate templates\n* `/wdl`: WDLs running the pipeline. There is a master WDL for running each module, e.g. `ClusterBatch.wdl`.\n* `/scripts`: scripts for running tests, building dockers, and analyzing cromwell metadata files\n* `/src`: main pipeline scripts\n  * `/RdTest`: scripts for depth testing\n  * `/sv-pipeline`: various scripts and packages used throughout the pipeline\n  * `/svqc`: Python module for checking that pipeline metrics fall within acceptable limits\n  * `/svtest`: Python module for generating various summary metrics from module outputs\n  * `/svtk`: Python module of tools for SV-related datafile parsing and analysis\n  * `/WGD`: whole-genome dosage scoring scripts\n\n\n## <a name=""cohort-mode"">Cohort mode</a>\nA minimum cohort size of 100 is required, and a roughly equal number of males and females is recommended. For modest cohorts (~100-500 samples), the pipeline can be run as a single batch using `GATKSVPipelineBatch.wdl`.\n\nFor larger cohorts, samples should be split up into batches of about 100-500 samples. Refer to the [Batching](#batching) section for further guidance on creating batches.\n\nThe pipeline should be executed as follows:\n* Modules [GatherSampleEvidence](#gather-sample-evidence) and [EvidenceQC](#evidence-qc) can be run on arbitrary cohort partitions\n* Modules [GatherBatchEvidence](#gather-batch-evidence), [ClusterBatch](#cluster-batch), [GenerateBatchMetrics](#generate-batch-metrics), and [FilterBatch](#filter-batch) are run separately per batch\n* [GenotypeBatch](#genotype-batch) is run separately per batch, using filtered variants ([FilterBatch](#filter-batch) output) combined across all batches\n* [MakeCohortVcf](#make-cohort-vcf) and beyond are run on all batches together\n\nNote: [GatherBatchEvidence](#gather-batch-evidence) requires a [trained gCNV model](#gcnv-training).\n\n#### <a name=""batching"">Batching</a>\nFor larger cohorts, samples should be split up into batches of about 100-500 samples with similar characteristics. We recommend batching based on overall coverage and dosage score (WGD), which can be generated in [EvidenceQC](#evidence-qc). An example batching process is outlined below:\n1. Divide the cohort into PCR+ and PCR- samples\n2. Partition the samples by median coverage from [EvidenceQC](#evidence-qc), grouping samples with similar median coverage together. The end goal is to divide the cohort into roughly equal-sized batches of about 100-500 samples; if your partitions based on coverage are larger or uneven, you can partition the cohort further in the next step to obtain the final batches. \n3. Optionally, divide the samples further by dosage score (WGD) from [EvidenceQC](#evidence-qc), grouping samples with similar WGD score together, to obtain roughly equal-sized batches of about 100-500 samples\n4. Maintain a roughly equal sex balance within each batch, based on sex assignments from [EvidenceQC](#evidence-qc)\n\n\n## <a name=""sample-sample-mode"">Single-sample mode</a>\n`GATKSVPipelineSingleSample.wdl` runs the pipeline on a single sample using a fixed reference panel. An example run with reference panel containing 156 samples from the [NYGC 1000G Terra workspace](https://app.terra.bio/#workspaces/anvil-datastorage/1000G-high-coverage-2019) can be found in `inputs/build/NA12878/test` after [building inputs](#building-inputs)).\n\n## <a name=""gcnv-training-overview"">gCNV Training</a>\nBoth the cohort and single-sample modes use the [GATK-gCNV](https://gatk.broadinstitute.org/hc/en-us/articles/360035531152) depth calling pipeline, which requires a [trained model](#gcnv-training) as input. The samples used for training should be technically homogeneous and similar to the samples to be processed (i.e. same sample type, library prep protocol, sequencer, sequencing center, etc.). The samples to be processed may comprise all or a subset of the training set. For small, relatively homogenous cohorts, a single gCNV model is usually sufficient. If a cohort contains multiple data sources, we recommend training a separate model for each [batch](#batching) or group of batches with similar dosage score (WGD). The model may be trained on all or a subset of the samples to which it will be applied; a reasonable default is 100 randomly-selected samples from the batch (the random selection can be done as part of the workflow by specifying a number of samples to the `n_samples_subsample` input parameter in `/wdl/TrainGCNV.wdl`).\n\n## <a name=""reference-panel-generation"">Generating a reference panel</a>\nNew reference panels can be generated easily from a single run of the `GATKSVPipelineBatch` workflow. If using a Cromwell server, we recommend copying the outputs to a permanent location by adding the following option to the workflow configuration file:\n```\n  ""final_workflow_outputs_dir"" : ""gs://my-outputs-bucket"",\n  ""use_relative_output_paths"": false,\n```\nHere is an example of how to generate workflow input jsons from `GATKSVPipelineBatch` workflow metadata:\n```\n> cromshell -t60 metadata 38c65ca4-2a07-4805-86b6-214696075fef > metadata.json\n> python scripts/inputs/create_test_batch.py \\n    --execution-bucket gs://my-exec-bucket \\n    --final-workflow-outputs-dir gs://my-outputs-bucket \\n    metadata.json \\n    > inputs/values/my_ref_panel.json\n> # Define your google project id (for Cromwell inputs) and Terra billing project (for workspace inputs)\n> echo '{ ""google_project_id"": ""my-google-project-id"", ""terra_billing_project_id"": ""my-terra-billing-project"" }' > inputs/values/google_cloud.my_project.json\n> # Build test files for batched workflows (google cloud project id required)\n> python scripts/inputs/build_inputs.py \\n    inputs/values \\n    inputs/templates/test \\n    inputs/build/my_ref_panel/test \\n    -a '{ ""test_batch"" : ""ref_panel_1kg"", ""cloud_env"": ""google_cloud.my_project"" }'\n> # Build test files for the single-sample workflow\n> python scripts/inputs/build_inputs.py \\n    inputs/values \\n    inputs/templates/test/GATKSVPipelineSingleSample \\n    inputs/build/NA19240/test_my_ref_panel \\n    -a '{ ""single_sample"" : ""test_single_sample_NA19240"", ""ref_panel"" : ""my_ref_panel"" }'\n> # Build files for a Terra workspace\n> python scripts/inputs/build_inputs.py \\n    inputs/values \\n    inputs/templates/terra_workspaces/single_sample \\n    inputs/build/NA12878/terra_my_ref_panel \\n    -a '{ ""single_sample"" : ""test_single_sample_NA12878"", ""ref_panel"" : ""my_ref_panel"" }'\n```\nNote that the inputs to `GATKSVPipelineBatch` may be used as resources for the reference panel and therefore should also be in a permanent location.\n\n## <a name=""descriptions"">Module Descriptions</a>\nThe following sections briefly describe each module and highlights inter-dependent input/output files. Note that input/output mappings can also be gleaned from `GATKSVPipelineBatch.wdl`, and example input templates for each module can be found in `/inputs/templates/test`.\n\n## <a name=""gather-sample-evidence"">GatherSampleEvidence</a>\n*Formerly Module00a*\n\nRuns raw evidence collection on each sample with the following SV callers: [Manta](https://github.com/Illumina/manta), [Wham](https://github.com/zeeev/wham), and/or [MELT](https://melt.igs.umaryland.edu/). For guidance on pre-filtering prior to `GatherSampleEvidence`, refer to the [Sample Exclusion](#sample-exclusion) section.\n\nNote: a list of sample IDs must be provided. Refer to the [sample ID requirements](#sampleids) for specifications of allowable sample IDs. IDs that do not meet these requirements may cause errors.\n\n#### Inputs:\n* Per-sample BAM or CRAM files aligned to hg38. Index files (`.bai`) must be provided if using BAMs.\n\n#### Outputs:\n* Caller VCFs (Manta, MELT, and/or Wham)\n* Binned read counts file\n* Split reads (SR) file\n* Discordant read pairs (PE) file\n\n## <a name=""evidence-qc"">EvidenceQC</a>\n*Formerly Module00b*\n\nRuns ploidy estimation, dosage scoring, and optionally VCF QC. The results from this module can be used for QC and batching.\n\nFor large cohorts, this workflow can be run on arbitrary cohort partitions of up to about 500 samples. Afterwards, we recommend using the results to divide samples into smaller batches (~100-500 samples) with ~1:1 male:female ratio. Refer to the [Batching](#batching) section for further guidance on creating batches.\n\nWe also recommend using sex assignments generated from the ploidy estimates and incorporating them into the PED file, with sex = 0 for sex aneuploidies.\n\n#### Prerequisites:\n* [GatherSampleEvidence](#gather-sample-evidence)\n\n#### Inputs:\n* Read count files ([GatherSampleEvidence](#gather-sample-evidence))\n* (Optional) SV call VCFs ([GatherSampleEvidence](#gather-sample-evidence))\n\n#### Outputs:\n* Per-sample dosage scores with plots\n* Median coverage per sample\n* Ploidy estimates, sex assignments, with plots\n* (Optional) Outlier samples detected by call counts\n\n#### <a name=""prelim-sample-qc"">Preliminary Sample QC</a>\nThe purpose of sample filtering at this stage after EvidenceQC is to prevent very poor quality samples from interfering with the results for the rest of the callset. In general, samples that are borderline are okay to leave in, but you should choose filtering thresholds to suit the needs of your cohort and study. There will be future opportunities (as part of [FilterBatch](#filter-batch)) for filtering before the joint genotyping stage if necessary. Here are a few of the basic QC checks that we recommend:\n* Look at the X and Y ploidy plots, and check that sex assignments match your expectations. If there are discrepancies, check for sample swaps and update your PED file before proceeding.\n* Look at the dosage score (WGD) distribution and check that it is centered around 0 (the distribution of WGD for PCR- samples is expected to be slightly lower than 0, and the distribution of WGD for PCR+ samples is expected to be slightly greater than 0. Refer to the [gnomAD-SV paper](https://doi.org/10.1038/s41586-020-2287-8) for more information on WGD score). Optionally filter outliers.\n* Look at the low outliers for each SV caller (samples with much lower than typical numbers of SV calls per contig for each caller). An empty low outlier file means there were no outliers below the median and no filtering is necessary. Check that no samples had zero calls.\n* Look at the high outliers for each SV caller and optionally filter outliers; samples with many more SV calls than average may be poor quality.\n* Remove samples with autosomal aneuploidies based on the per-batch binned coverage plots of each chromosome.\n\n\n## <a name=""gcnv-training"">TrainGCNV</a>\nTrains a [gCNV](https://gatk.broadinstitute.org/hc/en-us/articles/360035531152) model for use in [GatherBatchEvidence](#gather-batch-evidence). The WDL can be found at `/wdl/TrainGCNV.wdl`. See the [gCNV training overview](#gcnv-training-overview) for more information.\n\n#### Prerequisites:\n* [GatherSampleEvidence](#gather-sample-evidence)\n* (Recommended) [EvidenceQC](#evidence-qc)\n\n#### Inputs:\n* Read count files ([GatherSampleEvidence](#gather-sample-evidence))\n\n#### Outputs:\n* Contig ploidy model tarball\n* gCNV model tarballs\n\n\n## <a name=""gather-batch-evidence"">GatherBatchEvidence</a>\n*Formerly Module00c*\n\nRuns CNV callers ([cn.MOPS](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3351174/), [GATK-gCNV](https://gatk.broadinstitute.org/hc/en-us/articles/360035531152)) and combines single-sample raw evidence into a batch. See [above](#cohort-mode) for more information on batching.\n\n#### Prerequisites:\n* [GatherSampleEvidence](#gather-sample-evidence)\n* (Recommended) [EvidenceQC](#evidence-qc)\n* [gCNV training](#gcnv-training)\n\n#### Inputs:\n* PED file (updated with [EvidenceQC](#evidence-qc) sex assignments, including sex = 0 for sex aneuploidies. Calls will not be made on sex chromosomes when sex = 0 in order to avoid generating many confusing calls or upsetting normalized copy numbers for the batch.)\n* Read count, BAF, PE, SD, and SR files ([GatherSampleEvidence](#gather-sample-evidence))\n* Caller VCFs ([GatherSampleEvidence](#gather-sample-evidence))\n* Contig ploidy model and gCNV model files ([gCNV training](#gcnv-training))\n\n#### Outputs:\n* Combined read count matrix, SR, PE, and BAF files\n* Standardized call VCFs\n* Depth-only (DEL/DUP) calls\n* Per-sample median coverage estimates\n* (Optional) Evidence QC plots\n\n\n## <a name=""cluster-batch"">ClusterBatch</a>\n*Formerly Module01*\n\nClusters SV calls across a batch.\n\n#### Prerequisites:\n* [GatherBatchEvidence](#gather-batch-evidence)\n\n#### Inputs:\n* Standardized call VCFs ([GatherBatchEvidence](#gather-batch-evidence))\n* Depth-only (DEL/DUP) calls ([GatherBatchEvidence](#gather-batch-evidence))\n\n#### Outputs:\n* Clustered SV VCFs\n* Clustered depth-only call VCF\n\n\n## <a name=""generate-batch-metrics"">GenerateBatchMetrics</a>\n*Formerly Module02*\n\nGenerates variant metrics for filtering.\n\n#### Prerequisites:\n* [ClusterBatch](#cluster-batch)\n\n#### Inputs:\n* Combined read count matrix, SR, PE, and BAF files ([GatherBatchEvidence](#gather-batch-evidence))\n* Per-sample median coverage estimates ([GatherBatchEvidence](#gather-batch-evidence))\n* Clustered SV VCFs ([ClusterBatch](#cluster-batch))\n* Clustered depth-only call VCF ([ClusterBatch](#cluster-batch))\n\n#### Outputs:\n* Metrics file\n\n\n## <a name=""generate-batch-metrics"">FilterBatch</a>\n*Formerly Module03*\n\nFilters poor quality variants and filters outlier samples. This workflow can be run all at once with the WDL at `wdl/FilterBatch.wdl`, or it can be run in two steps to enable tuning of outlier filtration cutoffs. The two subworkflows are:\n1. FilterBatchSites: Per-batch variant filtration. Visualize SV counts per sample per type to help choose an IQR cutoff for outlier filtering, and preview outlier samples for a given cutoff\n2. FilterBatchSamples: Per-batch outlier sample filtration; provide an appropriate `outlier_cutoff_nIQR` based on the SV count plots and outlier previews from step 1. Note that not removing high outliers can result in increased compute cost and a higher false positive rate in later steps.\n\n#### Prerequisites:\n* [GenerateBatchMetrics](#generate-batch-metrics)\n\n#### Inputs:\n* Batch PED file\n* Metrics file ([GenerateBatchMetrics](#generate-batch-metrics))\n* Clustered SV and depth-only call VCFs ([ClusterBatch](#cluster-batch))\n\n#### Outputs:\n* Filtered SV (non-depth-only a.k.a. ""PESR"") VCF with outlier samples excluded\n* Filtered depth-only call VCF with outlier samples excluded\n* Random forest cutoffs file\n* PED file with outlier samples excluded\n\n\n## <a name=""merge-batch-sites"">MergeBatchSites</a>\n*Formerly MergeCohortVcfs*\n\nCombines filtered variants across batches. The WDL can be found at: `/wdl/MergeBatchSites.wdl`.\n\n#### Prerequisites:\n* [FilterBatch](#filter-batch)\n\n#### Inputs:\n* List of filtered PESR VCFs ([FilterBatch](#filter-batch))\n* List of filtered depth VCFs ([FilterBatch](#filter-batch))\n\n#### Outputs:\n* Combined cohort PESR and depth VCFs\n\n\n## <a name=""genotype-batch"">GenotypeBatch</a>\n*Formerly Module04*\n\nGenotypes a batch of samples across unfiltered variants combined across all batches.\n\n#### Prerequisites:\n* [FilterBatch](#filter-batch)\n* [MergeBatchSites](#merge-batch-sites)\n\n#### Inputs:\n* Batch PESR and depth VCFs ([FilterBatch](#filter-batch))\n* Cohort PESR and depth VCFs ([MergeBatchSites](#merge-batch-sites))\n* Batch read count, PE, and SR files ([GatherBatchEvidence](#gather-batch-evidence)) \n\n#### Outputs:\n* Filtered SV (non-depth-only a.k.a. ""PESR"") VCF with outlier samples excluded\n* Filtered depth-only call VCF with outlier samples excluded\n* PED file with outlier samples excluded\n* List of SR pass variants\n* List of SR fail variants\n* (Optional) Depth re-genotyping intervals list\n\n\n## <a name=""regenotype-cnvs"">RegenotypeCNVs</a>\n*Formerly Module04b*\n\nRe-genotypes probable mosaic variants across multiple batches.\n\n#### Prerequisites:\n* [GenotypeBatch](#genotype-batch)\n\n#### Inputs:\n* Per-sample median coverage estimates ([GatherBatchEvidence](#gather-batch-evidence))\n* Pre-genotyping depth VCFs ([FilterBatch](#filter-batch))\n* Batch PED files ([FilterBatch](#filter-batch))\n* Cohort depth VCF ([MergeBatchSites](#merge-batch-sites))\n* Genotyped depth VCFs ([GenotypeBatch](#genotype-batch))\n* Genotyped depth RD cutoffs file ([GenotypeBatch](#genotype-batch))\n\n#### Outputs:\n* Re-genotyped depth VCFs\n\n\n## <a name=""make-cohort-vcf"">MakeCohortVcf</a>\n*Formerly Module0506*\n\nCombines variants across multiple batches, resolves complex variants, re-genotypes, and performs final VCF clean-up.\n\n#### Prerequisites:\n* [GenotypeBatch](#genotype-batch)\n* (Optional) [RegenotypeCNVs](#regenotype-cnvs)\n\n#### Inputs:\n* RD, PE and SR file URIs ([GatherBatchEvidence](#gather-batch-evidence))\n* Batch filtered PED file URIs ([FilterBatch](#filter-batch))\n* Genotyped PESR VCF URIs ([GenotypeBatch](#genotype-batch))\n* Genotyped depth VCF URIs ([GenotypeBatch](#genotype-batch) or [RegenotypeCNVs](#regenotype-cnvs))\n* SR pass variant file URIs ([GenotypeBatch](#genotype-batch))\n* SR fail variant file URIs ([GenotypeBatch](#genotype-batch))\n* Genotyping cutoff file URIs ([GenotypeBatch](#genotype-batch))\n* Batch IDs\n* Sample ID list URIs\n\n#### Outputs:\n* Finalized ""cleaned"" VCF and QC plots\n\n## <a name=""module07"">Module 07</a> (in development)\nApply downstream filtering steps to the cleaned VCF to further control the false discovery rate; all steps are optional and users should decide based on the specific purpose of their projects.\n\nFiltering methods include:\n* minGQ - remove variants based on the genotype quality across populations.\nNote: Trio families are required to build the minGQ filtering model in this step. We provide tables pre-trained with the 1000 genomes samples at different FDR thresholds for projects that lack family structures, and they can be found at the paths below.  These tables assume that GQ has a scale of [0,999], so they will not work with newer VCFs where GQ has a scale of [0,99].\n```\ngs://gatk-sv-resources-public/hg38/v0/sv-resources/ref-panel/1KG/v2/mingq/1KGP_2504_and_698_with_GIAB.10perc_fdr.PCRMINUS.minGQ.filter_lookup_table.txt\ngs://gatk-sv-resources-public/hg38/v0/sv-resources/ref-panel/1KG/v2/mingq/1KGP_2504_and_698_with_GIAB.1perc_fdr.PCRMINUS.minGQ.filter_lookup_table.txt\ngs://gatk-sv-resources-public/hg38/v0/sv-resources/ref-panel/1KG/v2/mingq/1KGP_2504_and_698_with_GIAB.5perc_fdr.PCRMINUS.minGQ.filter_lookup_table.txt\n```\n\n* BatchEffect - remove variants that show significant discrepancies in allele frequencies across batche",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/broadinstitute/gatk,https://github.com/broadinstitute/gatk,1,,,1,1,1,1,0,0,0,0,0,0,1,Official code repository for GATK versions 4 and up,"[![Build Status](https://github.com/broadinstitute/gatk/actions/workflows/gatk-tests.yml/badge.svg?branch=master)](https://github.com/broadinstitute/gatk/actions/workflows/gatk-tests.yml)\n[![Maven Central](https://img.shields.io/maven-central/v/org.broadinstitute/gatk.svg)](https://maven-badges.herokuapp.com/maven-central/org.broadinstitute/gatk)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n\n***Please see the [GATK website](http://www.broadinstitute.org/gatk), where you can download a precompiled executable, read documentation, ask questions, and receive technical support. For GitHub basics, see [here](https://software.broadinstitute.org/gatk/documentation/article?id=23405).***\n\n### GATK 4\n\nThis repository contains the next generation of the Genome Analysis Toolkit (GATK). The contents\nof this repository are 100% open source and released under the Apache 2.0 license (see [LICENSE.TXT](https://github.com/broadinstitute/gatk/blob/master/LICENSE.TXT)).\n\nGATK4 aims to bring together well-established tools from the [GATK](http://www.broadinstitute.org/gatk) and\n[Picard](http://broadinstitute.github.io/picard/) codebases under a streamlined framework,\nand to enable selected tools to be run in a massively parallel way on local clusters or in the cloud using\n[Apache Spark](http://spark.apache.org/). It also contains many newly developed tools not present in earlier\nreleases of the toolkit.\n\n## Table of Contents\n* [Requirements](#requirements)\n* [Quick Start Guide](#quickstart)\n* [Downloading GATK4](#downloading)\n  * [Tools Included in Docker Image](#dockerSoftware)\n* [Building GATK4](#building)\n* [Running GATK4](#running)\n    * [Passing JVM options to gatk](#jvmoptions)\n    * [Passing a configuration file to gatk](#configFileOptions)\n    * [Running GATK4 with inputs on Google Cloud Storage](#gcs)\n    * [Running GATK4 Spark tools locally](#sparklocal)\n    * [Running GATK4 Spark tools on a Spark cluster](#sparkcluster)\n    * [Running GATK4 Spark tools on Google Cloud Dataproc](#dataproc)\n    * [Using R to generate plots](#R)\n    * [GATK Tab Completion for Bash](#tab_completion)\n* [For GATK Developers](#developers)\n    * [General guidelines for GATK4 developers](#dev_guidelines)\n    * [Testing GATK4](#testing)\n    * [Using Git LFS to download and track large test data](#lfs)\n    * [Creating a GATK project in the IntelliJ IDE](#intellij)\n    * [Setting up debugging in IntelliJ](#debugging)\n    * [Updating the Intellij project when dependencies change](#intellij_gradle_refresh)\n    * [Setting up profiling using JProfiler](#jprofiler)\n    * [Uploading Archives to Sonatype](#sonatype)\n    * [Building GATK4 Docker images](#docker_building)\n    * [Releasing GATK4](#releasing_gatk)\n    * [Generating GATK4 documentation](#gatkdocs)\n    * [Generating GATK4 WDL Wrappers](#gatkwdlgen)\n    * [Using Zenhub to track github issues](#zenhub)\n* [Further Reading on Spark](#spark_further_reading)\n* [How to contribute to GATK](#contribute)\n* [Discussions](#discussions)\n* [Authors](#authors)\n* [License](#license)\n\n## <a name=""requirements"">Requirements</a>\n* To run GATK:\n    * Java 17 is needed to run or build GATK. \n    We recommend one of the following:\n        * Download the Eclipse Foundation's distribution of OpenJDK 17 from [adoptium.net](https://adoptium.net/). Navigate to the [release archive](https://adoptium.net/temurin/archive/?version=17) to find downloads for Java 17.\n        * On Mac OS, you can install the [Homebrew package manager](https://brew.sh/) and run `brew tap homebrew/cask-versions` followed by `brew install --cask temurin17` to install the Eclipse Foundation's OpenJDK 17. \n    * Python 2.6 or greater (required to run the `gatk` frontend script)\n    * Python 3.6.2, along with a set of additional Python packages, is required to run some tools and workflows.\n      See [Python Dependencies](#python) for more information.\n    * R 3.2.5 (needed for producing plots in certain tools)\n* To build GATK:\n    * A Java 17 JDK\n    * Git 2.5 or greater\n    * [git-lfs](https://git-lfs.github.com/) 1.1.0 or greater. Required to download the large files used to build GATK, and\n      test files required to run the test suite. Run `git lfs install` after downloading, followed by `git lfs pull` from\n      the root of your git clone to download all of the large files, including those required to run the test suite. The\n      full download is approximately 5 gigabytes. Alternatively, if you are just building GATK and not running the test\n      suite, you can skip this step since the build itself will use git-lfs to download the minimal set of large `lfs`\n      resource files required to complete the build. The test resources will not be downloaded, but this greatly reduces\n      the size of the download.\n    * Gradle 5.6. We recommend using the `./gradlew` script which will\n      download and use an appropriate gradle version automatically (see examples below).\n    * R 3.2.5 (needed for running the test suite)\n* Pre-packaged Docker images with all needed dependencies installed can be found on\n  [our dockerhub repository](https://hub.docker.com/r/broadinstitute/gatk/). This requires a recent version of the\n   docker client, which can be found on the [docker website](https://www.docker.com/get-docker).\n* Python Dependencies:<a name=""python""></a>\n    * GATK4 uses the [Conda](https://conda.io/docs/index.html) package manager to establish and manage the\n      Python environment and dependencies required by GATK tools that have a Python dependency. This environment also \n      includes the R dependencies used for plotting in some of the tools. The ```gatk``` environment \n      requires hardware with AVX support for tools that depend on TensorFlow (e.g. CNNScoreVariant). The GATK Docker image \n      comes with the ```gatk``` environment pre-configured.\n      	* At this time, the only supported platforms are 64-bit Linux distributions. The required Conda environment is not\n	  currently supported on OS X/macOS. \n    * To establish the environment when not using the Docker image, a conda environment must first be ""created"", and\n      then ""activated"":\n        * First, make sure [Miniconda or Conda](https://conda.io/docs/index.html) is installed (Miniconda is sufficient).\n        * To ""create"" the conda environment:\n            * If running from a zip or tar distribution, run the command ```conda env create -f gatkcondaenv.yml``` to\n              create the ```gatk``` environment.\n            * If running from a cloned repository, run ```./gradlew localDevCondaEnv```. This generates the Python\n              package archive and conda yml dependency file(s) in the build directory, and also creates (or updates)\n              the local  ```gatk``` conda environment.\n        * To ""activate"" the conda environment (the conda environment must be activated within the same shell from which\n          GATK is run):\n             * Execute the shell command ```source activate gatk``` to activate the ```gatk``` environment.\n        * See the [Conda](https://conda.io/docs/user-guide/tasks/manage-environments.html) documentation for\n          additional information about using and managing Conda environments.\n\n## <a name=""quickstart"">Quick Start Guide</a>\n\n* Build the GATK: `./gradlew bundle` (creates `gatk-VERSION.zip` in `build/`)\n* Get help on running the GATK: `./gatk --help`\n* Get a list of available tools: `./gatk --list`\n* Run a tool: `./gatk PrintReads -I src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O output.bam`\n* Get help on a particular tool: `./gatk PrintReads --help`\n\n## <a name=""downloading"">Downloading GATK4</a>\n\nYou can download and run pre-built versions of GATK4 from the following places:\n\n* A zip archive with everything you need to run GATK4 can be downloaded for each release from the [github releases page](https://github.com/broadinstitute/gatk/releases). We also host unstable archives generated nightly in the Google bucket gs://gatk-nightly-builds.\n\n* You can download a GATK4 docker image from [our dockerhub repository](https://hub.docker.com/r/broadinstitute/gatk/). We also host unstable nightly development builds on [this dockerhub repository](https://hub.docker.com/r/broadinstitute/gatk-nightly/).\n    * Within the docker image, run gatk commands as usual from the default startup directory (/gatk).\n\n### <a name=""dockerSoftware"">Tools Included in Docker Image</a>\n\nOur docker image contains the following bioinformatics tools, which can be run by invoking the tool name from the command line:\n* bedtools (v2.30.0)\n* samtools (1.13)\n* bcftools (1.13)\n* tabix (1.13+ds)\n\nWe also include an installation of Python3 (3.6.10) with the following popular packages included:\n* numpy\n* scipy\n* tensorflow\n* pymc3\n* keras\n* scikit-learn\n* matplotlib\n* pandas\n* biopython\n* pyvcf\n* pysam\n\nWe also include an installation of R (3.6.2) with the following popular packages included:\n* data.table\n* dplyr\n* ggplot2\n\nFor more details on system packages, see the GATK [Base Dockerfile](scripts/docker/gatkbase/Dockerfile) and for more details on the Python3/R packages, see the [Conda environment setup file](scripts/gatkcondaenv.yml.template). Versions for the Python3/R packages can be found there.\n\n## <a name=""building"">Building GATK4</a>\n\n* **To do a full build of GATK4, first clone the GATK repository using ""git clone"", then run:**\n\n        ./gradlew bundle\n        \n  Equivalently, you can just type:\n  \n        ./gradlew\n        \n    * This creates a zip archive in the `build/` directory with a name like `gatk-VERSION.zip` containing a complete standalone GATK distribution, including our launcher `gatk`, both the local and spark jars, and this README.    \n    * You can also run GATK commands directly from the root of your git clone after running this command.\n    * Note that you *must* have a full git clone in order to build GATK, including the git-lfs files in `src/main/resources/large`. The zipped source code alone is not buildable.\n    * The large files under `src/main/resources/large/` are required to build GATK, since they are packaged inside the GATK jar and used by tools at runtime. These include things like ML models and native C/C++ libraries used for acceleration of certain tools.\n    * The large files under `src/test/resources/large/`, on the other hand, are only required by the test suite when running tests, and are not required to build GATK.\n\n* **Other ways to build:**\n    * `./gradlew installDist`  \n        * Does a *fast* build that only lets you run GATK tools from inside your git clone, and locally only (not on a cluster). Good for developers! \n    * `./gradlew installAll`\n        * Does a *semi-fast* build that only lets you run GATK tools from inside your git clone, but works both locally and on a cluster. Good for developers!\n    * `./gradlew localJar`\n        * Builds *only* the GATK jar used for running tools locally (not on a Spark cluster). The resulting jar will be in `build/libs` with a name like `gatk-package-VERSION-local.jar`, and can be used outside of your git clone.\n    * `./gradlew sparkJar`\n        * Builds *only* the GATK jar used for running tools on a Spark cluster (rather than locally). The resulting jar will be in `build/libs` with a name like `gatk-package-VERSION-spark.jar`, and can be used outside of your git clone. \n        * This jar will not include Spark and Hadoop libraries, in order to allow the versions of Spark and Hadoop installed on your cluster to be used.\n\n* **To remove previous builds, run:** \n\n        ./gradlew clean\n\n* For faster gradle operations, add `org.gradle.daemon=true` to your `~/.gradle/gradle.properties` file.\n  This will keep a gradle daemon running in the background and avoid the ~6s gradle start up time on every command.\n\n* Gradle keeps a cache of dependencies used to build GATK.  By default this goes in `~/.gradle`.  If there is insufficient free space in your home directory, you can change the location of the cache by setting the `GRADLE_USER_HOME` environment variable.\n\n* The version number is automatically derived from the git history using `git describe`, you can override it by setting the `versionOverride` property.\n  ( `./gradlew -DversionOverride=my_weird_version printVersion` )\n\n## <a name=""running"">Running GATK4</a>\n\n* The standard way to run GATK4 tools is via the **`gatk`** wrapper script located in the root directory of a clone of this repository.\n    * Requires Python 2.6 or greater (this includes Python 3.x)\n    * You need to have built the GATK as described in the [Building GATK4](#building) section above before running this script.\n    * There are several ways `gatk` can be run:\n        * Directly from the root of your git clone after building\n        * By extracting the zip archive produced by `./gradlew bundle` to a directory, and running `gatk` from there\n        * Manually putting the `gatk` script within the same directory as fully-packaged GATK jars produced by `./gradlew localJar` and/or `./gradlew sparkJar`\n        * Defining the environment variables `GATK_LOCAL_JAR` and `GATK_SPARK_JAR`, and setting them to the paths to the GATK jars produced by `./gradlew localJar` and/or `./gradlew sparkJar` \n    * `gatk` can run non-Spark tools as well as Spark tools, and can run Spark tools locally, on a Spark cluster, or on Google Cloud Dataproc.\n    * ***Note:*** running with `java -jar` directly and bypassing `gatk` causes several important system properties to not get set, including htsjdk compression level!\n    \n* For help on using `gatk` itself, run **`./gatk --help`**\n\n* To print a list of available tools, run **`./gatk --list`**.\n    * Spark-based tools will have a name ending in `Spark` (eg., `BaseRecalibratorSpark`). Most other tools are non-Spark-based.\n\n* To print help for a particular tool, run **`./gatk ToolName --help`**.\n\n* To run a non-Spark tool, or to run a Spark tool locally, the syntax is: **`./gatk ToolName toolArguments`**.\n\n* Tool arguments that allow multiple values, such as -I, can be supplied on the command line using a file with the extension "".args"". Each line of the file should contain a\n  single value for the argument.\n\n* Examples:\n\n  ```\n  ./gatk PrintReads -I input.bam -O output.bam\n  ```\n\n  ```\n  ./gatk PrintReadsSpark -I input.bam -O output.bam\n  ```\n\n#### <a name=""jvmoptions"">Passing JVM options to gatk</a>\n\n* To pass JVM arguments to GATK, run `gatk` with the `--java-options` argument: \n\n    ```\n    ./gatk --java-options ""-Xmx4G"" <rest of command>\n     \n    ./gatk --java-options ""-Xmx4G -XX:+PrintGCDetails"" <rest of command>\n    ```\n#### <a name=""configFileOptions"">Passing a configuration file to gatk</a>\n\n* To pass a configuration file to GATK, run `gatk` with the `--gatk-config-file` argument: \n\n	```\n	./gatk --gatk-config-file GATKProperties.config <rest of command>\n	```\n\n	An example GATK configuration file is packaged with each release as `GATKConfig.EXAMPLE.properties`\n	This example file contains all current options that are used by GATK and their default values.\n\n#### <a name=""gcs"">Running GATK4 with inputs on Google Cloud Storage:</a>\n\n* Many GATK4 tools can read BAM or VCF inputs from a Google Cloud Storage bucket. Just use the ""gs://"" prefix:\n  ```\n  ./gatk PrintReads -I gs://mybucket/path/to/my.bam -L 1:10000-20000 -O output.bam\n  ```\n* ***Important:*** You must set up your credentials first for this to work! There are three options:\n    * Option (a): run in a Google Cloud Engine VM\n        * If you are running in a Google VM then your credentials are already in the VM and will be picked up by GATK, you don't need to do anything special.\n    * Option (b): use your own account\n        * Install [Google Cloud SDK](https://cloud.google.com/sdk/)\n        * Log into your account:\n        ```\n        gcloud auth application-default login\n        ```\n        * Done! GATK will use the application-default credentials you set up there.\n    * Option (c): use a service account\n        * Create a new service account on the Google Cloud web page and download the JSON key file\n        * Install [Google Cloud SDK](https://cloud.google.com/sdk/)\n        * Tell gcloud about the key file:\n        ```\n        gcloud auth activate-service-account --key-file ""$PATH_TO_THE_KEY_FILE""\n        ```\n        * Set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to point to the file\n        ```\n        export GOOGLE_APPLICATION_CREDENTIALS=""$PATH_TO_THE_KEY_FILE""\n        ```\n        * Done! GATK will pick up the service account. You can also do this in a VM if you'd like to override the default credentials.\n\n#### <a name=""sparklocal"">Running GATK4 Spark tools locally:</a>\n\n* GATK4 Spark tools can be run in local mode (without a cluster). In this mode, Spark will run the tool\n  in multiple parallel execution threads using the cores in your CPU. You can control how many threads\n  Spark will use via the `--spark-master` argument.\n  \n* Examples:\n\n  Run `PrintReadsSpark` with 4 threads on your local machine:\n  ``` \n    ./gatk PrintReadsSpark -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O output.bam \\n        -- \\n        --spark-runner LOCAL --spark-master 'local[4]'\n  ```\n  Run `PrintReadsSpark` with as many worker threads as there are logical cores on your local machine:\n  ``` \n    ./gatk PrintReadsSpark -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O output.bam \\n        -- \\n        --spark-runner LOCAL --spark-master 'local[*]'\n  ```   \n  \n* Note that the Spark-specific arguments are separated from the tool-specific arguments by a `--`.\n\n#### <a name=""sparkcluster"">Running GATK4 Spark tools on a Spark cluster:</a>\n\n**`./gatk ToolName toolArguments -- --spark-runner SPARK --spark-master <master_url> additionalSparkArguments`**\n* Examples:\n\n  ```\n  ./gatk PrintReadsSpark -I hdfs://path/to/input.bam -O hdfs://path/to/output.bam \\n      -- \\n      --spark-runner SPARK --spark-master <master_url>\n  ```\n\n    ```\n    ./gatk PrintReadsSpark -I hdfs://path/to/input.bam -O hdfs://path/to/output.bam \\n      -- \\n      --spark-runner SPARK --spark-master <master_url> \\n      --num-executors 5 --executor-cores 2 --executor-memory 4g \\n      --conf spark.executor.memoryOverhead=600\n    ```\n\n* You can also omit the ""--num-executors"" argument to enable [dynamic allocation](https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation) if you configure the cluster properly (see the Spark website for instructions).\n* Note that the Spark-specific arguments are separated from the tool-specific arguments by a `--`.\n* Running a Spark tool on a cluster requires Spark to have been installed from http://spark.apache.org/, since\n   `gatk` invokes the `spark-submit` tool behind-the-scenes.\n* Note that the examples above use YARN but we have successfully run GATK4 on Mesos as well.\n\n#### <a name=""dataproc"">Running GATK4 Spark tools on Google Cloud Dataproc:</a>\n  * You must have a [Google cloud services](https://cloud.google.com/) account, and have spun up a Dataproc cluster\n    in the [Google Developer's console](https://console.developers.google.com). You may need to have the ""Allow API access to all Google Cloud services in the same project"" option enabled (settable when you create a cluster).\n  * You need to have installed the Google Cloud SDK from [here](https://cloud.google.com/sdk/), since\n    `gatk` invokes the `gcloud` tool behind-the-scenes. As part of the installation, be sure\n      that you follow the `gcloud` setup instructions [here](https://cloud.google.com/sdk/gcloud/). As this library is frequently updated by Google, we recommend updating your copy regularly to avoid any version-related difficulties.\n  * Your inputs to the GATK when running on dataproc are typically in Google Cloud Storage buckets, and should be specified on\n    your GATK command line using the syntax `gs://my-gcs-bucket/path/to/my-file`\n  * You can run GATK4 jobs on Dataproc from your local computer or from the VM (master node) on the cloud.\n\n  Once you're set up, you can run a Spark tool on your Dataproc cluster using a command of the form:\n\n  **`./gatk ToolName toolArguments -- --spark-runner GCS --cluster myGCSCluster additionalSparkArguments`**\n\n  * Examples:\n\n      ```      \n      ./gatk PrintReadsSpark \\n          -I gs://my-gcs-bucket/path/to/input.bam \\n          -O gs://my-gcs-bucket/path/to/output.bam \\n          -- \\n          --spark-runner GCS --cluster myGCSCluster\n      ```\n\n      ```\n      ./gatk PrintReadsSpark \\n          -I gs://my-gcs-bucket/path/to/input.bam \\n          -O gs://my-gcs-bucket/path/to/output.bam \\n          -- \\n          --spark-runner GCS --cluster myGCSCluster \\n          --num-executors 5 --executor-cores 2 --executor-memory 4g \\n          --conf spark.yarn.executor.memoryOverhead=600\n      ```\n  * When using Dataproc you can access the web interfaces for YARN, Hadoop and HDFS by opening an SSH tunnel and connecting with your browser.  This can be done easily using included `gcs-cluster-ui` script.\n  \n    ```\n    scripts/dataproc-cluster-ui myGCSCluster\n    ```\n    Or see these [these instructions](https://cloud.google.com/dataproc/cluster-web-interfaces) for more details.\n  * Note that the spark-specific arguments are separated from the tool-specific arguments by a `--`.\n  * If you want to avoid uploading the GATK jar to GCS on every run, set the `GATK_GCS_STAGING`\n    environment variable to a bucket you have write access to (eg., `export GATK_GCS_STAGING=gs://<my_bucket>/`)\n  * Dataproc Spark clusters are configured with [dynamic allocation](https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation) so you can omit the ""--num-executors"" argument and let YARN handle it automatically.\n\n#### <a name=""R"">Using R to generate plots</a>\nCertain GATK tools may optionally generate plots using the R installation provided within the conda environment.  If you are uninterested in plotting, R is still required by several of the unit tests.  Plotting is currently untested and should be viewed as a convenience rather than a primary output.\n\n#### <a name=""tab_completion"">Bash Command-line Tab Completion (BETA)</a>\n\n* A tab completion bootstrap file for the bash shell is now included in releases.  This file allows the command-line shell to complete GATK run options in a manner equivalent to built-in command-line tools (e.g. grep).  \n\n* This tab completion functionality has only been tested in the bash shell, and is released as a beta feature.\n\n* To enable tab completion for the GATK, open a terminal window and source the included tab completion script:\n\n```\nsource gatk-completion.sh\n```\n\n* Sourcing this file will allow you to press the tab key twice to get a list of options available to add to your current GATK command.  By default you will have to source this file once in each command-line session, then for the rest of the session the GATK tab completion functionality will be available.  GATK tab completion will be available in that current command-line session only.\n\n* Note that you must have already started typing an invocation of the GATK (using gatk) for tab completion to initiate:\n\n```\n./gatk <TAB><TAB>\n```\n\n* We recommend adding a line to your bash settings file (i.e. your ~/.bashrc file) that sources the tab completion script.  To add this line to your bash settings / bashrc file you can use the following command:\n\n```\necho ""source <PATH_TO>/gatk-completion.sh"" >> ~/.bashrc\n```\n\n* Where ```<PATH_TO>``` is the fully qualified path to the ```gatk-completion.sh``` script.\n\n## <a name=""developers"">For GATK Developers</a>\n\n#### <a name=""dev_guidelines"">General guidelines for GATK4 developers</a>\n\n* **Do not put private or restricted data into the repo.**\n\n* **Try to keep datafiles under 100kb in size.** Larger test files should go into `src/test/resources/large` (and subdirectories) so that they'll be stored and tracked by git-lfs as described [above](#lfs).\n\n* GATK4 is Apache 2.0 licensed.  The license is in the top level LICENSE.TXT file.  Do not add any additional license text or accept files with a license included in them.\n\n* Each tool should have at least one good end-to-end integration test with a check for expected output, plus high-quality unit tests for all non-trivial utility methods/classes used by the tool. Although we have no specific coverage target, coverage should be extensive enough that if tests pass, the tool is guaranteed to be in a usable state.\n\n* All newly written code must have good test coverage (>90%).\n\n* All bug fixes must be accompanied by a regression test.\n\n* All pull requests must be reviewed before merging to master (even documentation changes).\n\n* Don't issue or accept pull requests that introduce warnings. Warnings must be addressed or suppressed.\n\n* Don't issue or accept pull requests that significantly decrease coverage (less than 1% decrease is sort of tolerable). \n\n* Don't use `toString()` for anything other than human consumption (ie. don't base the logic of your code on results of `toString()`.)\n\n* Don't override `clone()` unless you really know what you're doing. If you do override it, document thoroughly. Otherwise, prefer other means of making copies of objects.\n\n* For logging, use [org.apache.logging.log4j.Logger](https://logging.apache.org/log4j/2.0/log4j-api/apidocs/org/apache/logging/log4j/Logger.html)\n\n* We mostly follow the [Google Java Style guide](https://google.github.io/styleguide/javaguide.html)\n\n* Git: Don't push directly to master - make a pull request instead. \n\n* Git: Rebase and squash commits when merging.\n\n* If you push to master or mess up the commit history, you owe us 1 growler or tasty snacks at happy hour. If you break the master build, you owe 3 growlers (or lots of tasty snacks). Beer may be replaced by wine (in the color and vintage of buyer's choosing) in proportions of 1 growler = 1 bottle. \n\n#### <a name=""testing"">Testing GATK</a>\n\n* Before running the test suite, be sure that you've installed `git lfs` and downloaded the large test data, following the [git lfs setup instructions](#lfs)\n\n* To run the test suite, run **`./gradlew test`**.\n    * Test report is in `build/reports/tests/test/index.html`.\n    * What will happen depends on the value of the `TEST_TYPE` environment variable: \n       * unset or any other value         : run non-cloud unit and integration tests, this is the default\n       * `cloud`, `unit`, `integration`, `conda`, `spark`   : run only the cloud, unit, integration, conda (python + R), or Spark tests\n       * `all`                            : run the entire test suite\n    * Cloud tests require being logged into `gcloud` and authenticated with a project that has access\n      to the cloud test data.  They also require setting several certain environment variables.\n      * `HELLBENDER_JSON_SERVICE_ACCOUNT_KEY` : path to a local JSON file with [service account credentials](https://cloud.google.com/storage/docs/authentication#service_accounts) \n      * `HELLBENDER_TEST_PROJECT` : your google cloud project \n      * `HELLBENDER_TEST_STAGING` : a gs:// path to a writable location\n      * `HELLBENDER_TEST_INPUTS` : path to cloud test data, ex: gs://hellbender/test/resources/ \n    * Setting the environment variable `TEST_VERBOSITY=minimal` will produce much less output from the test suite \n\n* To run a subset of tests, use gradle's test filtering (see [gradle doc](https://docs.gradle.org/current/userguide/java_plugin.html)):\n    * You can use `--tests` with a wildcard to run a specific test class, method, or to select multiple test classes:\n        * `./gradlew test --tests *SomeSpecificTestClass`\n        * `./gradlew test --tests *SomeTest.someSpecificTestMethod`\n        * `./gradlew test --tests all.in.specific.package*`\n\n* To run tests and compute coverage reports, run **`./gradlew jacocoTestReport`**. The report is then in `build/reports/jacoco/test/html/index.html`.\n  (IntelliJ has a good coverage tool that is preferable for development).\n\n* We use [Github Actions](https://github.com/broadinstitute/gatk/actions/workflows/gatk-tests.yml) as our continuous integration provider.\n\n    * Before merging any branch make sure that all required tests pass on Github.\n    * Every Actions build will upload the test results to our GATK Google Cloud Storage bucket and a zipped artifact upload.\n      A link to the uploaded report will appear at the very bottom of the github actions log.\n      Look for the line that says `See the test report at`.\n      Test github actions test artifacts will not show up on the webpage until the entire test has concluded.\n      If TestNG itself crashes there will be no report generated.\n\n* We use [Broad Jenkins](https://gatk-jenkins.broadinstitute.org/view/Performance/) for our long-running tests and performance tests.\n    * To add a performance test (requires Broad-ID), you need to make a ""new item"" in Jenkins and make it a ""copy"" instead of a blank project. You need to base it on either the ""-spark-"" jobs or the other kind of jobs and alter the commandline. \n\n* To output stack traces for `UserException` set the environment variable `GATK_STACKTRACE_ON_USER_EXCEPTION=true`\n\n#### <a name=""lfs"">Using Git LFS to download and track large test data</a>\n\nWe use [git-lfs](https://git-lfs.github.com/) to version and distribute test data that is too large to check into our repository directly. You must install and configure it in order to be able to run our test suite.\n\n* After installing [git-lfs](https://git-lfs.github.com/), run `git lfs install`\n    * This adds hooks to your git configuration that will cause git-lfs files to be checked out for you automatically in the future.\n    \n* To manually retrieve the large test data, run `git lfs pull` from the root of your GATK git clone.\n    * The download size is approximately 5 gigabytes.\n    \n* To add a new large file to be tracked by git-lfs, simply:\n    * Put the new file(s) in `src/test/resources/large` (or a subdirectory)\n    * `git add` the file(s), then `git commit -a`\n    * That's it! Do ***not*** run `git lfs track` on the files manually: all files in `src/test/resources/large` are tracked by git-lfs automatically. \n\n#### <a name=""intellij"">Creating a GATK project in the IntelliJ IDE (last tested with version 2016.2.4):</a>\n\n* Ensure that you have `gradle` and the Java 17 JDK installed\n\n* You may need to install the TestNG and Gradle plugins (in preferences)\n\n* Clone the GATK repository using git\n\n* In IntelliJ, click on ""Import Project"" in the home screen or go to File -> New... -> Project From Existing Sources...\n\n* Select the root directory of your GATK clone, then click on ""OK""\n\n* Select ""Import project from external model"", then ""Gradle"", then click on ""Next""\n\n* Ensure that ""Gradle project"" points to the build.gradle file in the root of your GATK clone\n\n* Select ""Use auto-import"" and ""Use default gradle wrapper"".\n\n* Make sure the Gradle JVM points to Java 17. You may need to set this manually after creating the project, to do so find the gradle settings by clicking the wrench icon in the gradle tab on the right bar, from there edit ""Gradle JVM"" argument to point to Java 17.\n\n* Click ""Finish""\n\n* After downloading project dependencies, IntelliJ should open a new window with your GATK project\n\n* Make sure that the Java version is set correctly by going to File -> ""Project Structure"" -> ""Project"". Check that the ""Project SDK"" is set to your Java 17 JDK, and ""Project language level"" to 17 (you may need to add your Java 17 JDK under ""Platform Settings"" -> SDKs if it isn't there already). Then click ""Apply""/""Ok"".\n\n#### <a name=""debugging"">Setting up debugging in IntelliJ</a>\n\n* Follow the instructions above for creating an IntelliJ project for GATK\n\n* Go to Run -> ""Edit Configurations"", then click ""+"" and add a new ""Application"" configuration\n\n* Set the name of the new configuration to something like ""GATK debug""\n\n* For ""Main class"", enter `org.broadinstitute.hellbender.Main`\n\n* Ensure that ""Use classpath of module:"" is set to use the ""gatk"" module's classpath\n\n* Enter the arguments for the command you want to debug in ""Program Arguments""\n\n* Click ""Apply",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/broadinstitute/depmap_omics,https://github.com/broadinstitute/depmap_omics,0,,,0,1,0,1,0,0,0,0,0,0,0,What you need to process the Quarterly DepMap-Omics releases from Terra,"# depmap_omics\n\n![](documentation/depmap-logo_white.png)\n\nThis repository contains code that processes data for the biannual [DepMap](https://www.depmap.org) data release. State of the pipeline for each release can be found under the ""Releases"" tab in this repo.\n\n## Table of Contents\n- [Getting Started](#quickstart)\n  - [Installation](#installation)\n- [Repository File Structure](#file-structure)\n- [Running the Pipeline](#running-pipeline)\n  - [Uploading and Preprocessing](#upload-preprocess)\n  - [Running Terra Pipelines](#running-terra-pipelines)\n  - [Downloading and Postprocessing](#downloading-postprocessing)\n  - [QC, Grouping and Uploading](#qc-grouping-uploading)\n\n## Getting Started <a name=""quickstart""></a>\n\nThe processing pipeline relies on the following tools:\n\n- [python](https://www.learnpython.org/)\n- [R](https://www.codecademy.com/learn/learn-r)\n- [jupyter](https://jupyter.org/index.html)\n- [WDL](https://software.broadinstitute.org/wdl/documentation/)\n- [gcp](https://cloud.google.com/sdk/docs/quickstart-macos)\n- [docker](https://docs.docker.com/get-started/)\n- [Terra](https://software.broadinstitute.org/firecloud/documentation/)\n- [The Terra Convention: The dos and donts for maintaining a cleaner terra.](https://docs.google.com/document/d/1zTtaN-Px64f8JvgydZNdBbzBpFWyZzEpshSNxQh43Oc/edit#heading=h.dz5wh0l4bu9g)\n- [dalmatian](https://github.com/broadinstitute/dalmatian)\n\n### Installatiion <a name=""installation""></a>\n\n`git clone http://github.com/BroadInstitute/depmap_omics.git && cd depmap_omics`\n\n`pip install -e .`\n\n### :warning: This repository needs other repos\n\nSome important data and code from the [genepy Library](https://github.com/broadinstitute/genepy).\n\nUse the instructions in the genepy page to install the package.\n\n### :warning: You need the following R and python packages\n\n1. You will need to install jupyter notetbooks and google cloud sdk\n  - install [Google Cloud SDK](https://cloud.google.com/sdk/docs/downloads-interactive).\n  - authenticate my SDK account by running `gcloud auth application-default login` in the terminal, and follow the instrucion to log in.\n\n2. and GSVA for ssGSEA in R `R` run `R -e 'if(!requireNamespace(""BiocManager"", quietly = TRUE)){install.packages(""BiocManager"")};BiocManager::install(c(""GSEABase"", ""erccdashboard"", ""GSVA"", ""DESeq2""));'`\n\n3. For Python use the requirements.txt file `pip install -r requirements.txt` \n\n### :warning: Follow instructions [here](documentation/getting_started.md) to set up Terra and obtain access to services required for running the pipeline.\n\n## Repository File Structure <a name=""file-structure""></a>\n\n__ccle_tasks/__ Contains a notebook for each of the different additional processing that the CCLE team has to perform as well as one-off tasks run by the omics team\n\n__data/__ Contains important information used for processing, including terra workspace configurations from past quarters\n\n__depmapomics/__ Contains the core python code used in the pipeline and called by the processing jupyter notebooks\n\n__\*\_pipeline/__ Contains some of the workflows' wdl files and script files used by these workflows \n\n__temp/__ Contains the temp file that can get removed after processing (should be empty)\n\n__documentation/__ Contains some additional files and diagrams for documenting the pipelines\n\n__tests/__ Contains automated pytest functions used internally for development\n\n__jupyter notebooks:__ `RNA_CCLE.ipynb` contains the DepMap processing pipelines for Expression and Fusion (from RNAseq data), and `WGS_CCLE.ipynb` contains the DepMap processing pipelines for Copy number and Mutations (from WGS/WES data)\n\n## Pipeline Walkthrough <a name=""running-pipeline""></a>\n\nThe processing pipelines are encapsulated in two jupyter notebooks (`RNA_CCLE.ipynb` and `WGS_CCLE.ipynb`). Each is divided into four steps: uploading, running Terra pipelines, local postprocessing, and uploading. Here is a detailed walkthrough (_Note that the steps that are ""internal only"" are run as part of DepMap's data processing, but not meant for external users to reproduce due to various dependencies that are unique to our team at the Broad. The ""internal only"" functions below can be found in the [depmap_omics_upload repo](https://github.com/broadinstitute/depmap_omics_upload)_):\n\n### 1. Uploading and Preprocessing (internal only) <a name=""upload-preprocess""></a>\n\nCurrently, sequenced data for DepMap is generated by the Genomics Platform (GP) at the Broad who deposits them into several different Terra workspaces. Therefore, the first step of this pipeline is to look at these workspaces and\n\n- identify new samples by looking at the bam files and compare them with bams we have already onboarded\n- remove duplicates and ones with broken file paths\n- onboard new samples and new versions of old cell lines if we find any\n\n### 2. Running Terra Pipelines <a name=""running-terra-pipelines""></a>\n\nWe are using Dalmatian to send requests to Terra, so before running this part, external users need to make sure that the dalmatian `WorkspaceManager` object is initialized with the right workspace and that the functions are taking the correct workflow names as inputs.\nYou can then run the RNAseq and/or WGS pipelines on your samples.\n\n**For a more in-depth documentation on what our pipelines contain, including the packages, input references, and parameters, please refer to this [summary of DepMap processing pipeline](documentation/DepMap_processing_pipeline.md).**\n\n### 3. Downloading and Postprocessing (sections under **on local** in the notebooks) <a name=""downloading-postprocessing""></a>\n\nThis step will do a set of tasks:\n- clean the workspaces by deleting large useless files, including unmapped bams.\n- retrieve from the workspace interesting QC results.\n- copy realigned bam files to our own data storage bucket (internal only).\n- download the outputs from Terra pipelines.\n\nThe main postprocessing steps for each pipeline are as followed:\n\n#### Copy Number\n\n`copynumbers.py` contains the main postprocessing function `postProcess()` responsible for postprocessing segments and creating gene-level (relative and absolute) CN files and genomic feature table. Gene mapping information is retrieved from BioMart version `nov2020`. The function also applies the following filters to segment and CN data:\n\n* Remove chrY segments from cell lines where their chrY segment count is bigger than 150\n* Mark samples that have more than 1500 segments as QC fails and remove them\n* Remove genes whose Entrez ID is NaN in BioMart in the gene-level matrices\n\n_Internal only: `dm_omics.cnPostProcessing()` calls the above function on both WES and WGS data, merges them, renames the indices into ProfileIDs, and upload them to taiga._\n\nNote: to get the exact same results as in DepMap, be sure to run `genecn = genecn.apply(lambda x: np.log2(1+x))` to the genecn dataframe in the CNV pipeline\n\n#### Mutation\n\n`mutations.py` contains `postProcess()`, the function responsible for postprocessing aggregated MAF files, genotyped mutation matrices (hot spot and damaging), binary guide mutation matrices, and structural variants (SVs). \n\n_Internal only: `dm_omics.mutationPostProcessing()` calls the above function on both WES and WGS data, merges them, renames the indices into ProfileIDs, removes genes whose hugo symbol is not in biomart, generates individual mutation datasets for variant types, and uploads them to taiga. It also generates and uploads a binary matrix for germline mutations._\n\n#### Expression\n\n`expressions.py` contains the main postprocessing function responsible for postprocessing aggregated expression data from RSEM, which removes duplicates and QC failures, renames genes, filters and log transforms values, and generates transcrip-level, gene-level, and protein-coding gene-level expression data files. Gene mapping information is retrieved from BioMart version `nov2020`. Optionally, in addition, it also generates Single-sample GSEA (ssGSEA) data.\n\n_Internal only: `dm_omics.expressionPostProcessing()` is a wrapper for the above function. It renames the indices into ProfileIDs and uploads the files to taiga._\n\n#### Fusion\n\nFunctions that postprocess aggregated fusion data can be found in `fusions.py`. We want to apply filters to the fusion table to reduce the number of artifacts in the dataset. Specifically, we filter the following:\n\n* Remove fusions involving mitochondrial chromosomes, or HLA genes, or immunoglobulin genes\n* Remove red herring fusions (from STAR-Fusion annotations column)\n* Remove fusions recurrent in CCLE (>= 25 samples)\n* Remove fusions that have (SpliceType="" INCL_NON_REF_SPLICE"" AND LargeAnchorSupport=""No"" AND FFPM < 0.1)\n* Remove fusions with FFPM < 0.05 (STAR-Fusion suggests using 0.1, but looking at the translocation data, this looks like it might be too aggressive)\n\n_Internal only: `dm_omics.fusionPostProcessing()` is a wrapper for the above function. It renames the indices into ProfileIDs and uploads the data to taiga._\n\n\n### 4. QC, Grouping and Uploading to the Portal (internal use only) <a name=""qc-grouping-uploading""></a>\n\nWe then perform the following QC tasks for each dataset:\n\n#### CN\n\nOnce the CN files are saved, we load them back in python and do some validations, in brief:\n\n- mean, max, var...\n- to previous release: same mean, max, var...\n- checkAmountOfSegments: flag any samples with a very high number of segments\n\n#### Mutation\n\n__Compare to previous release (broad only)__\n\nWe compare the results to the previous releases MAF. Namely:\n\n- Count the total number of mutations per cell line, split by type (SNP, INS, DEL)\n- Count the total number of mutations observed by position (group by chromosome, start position, end position and count the number of mutations)\n\n\n##### REMARK:\n\nOverall the filters applied after the CGA pipeline are the following:\n\nWe remove everything that:\n- has AF<.1 \n- OR coverage <4\n- OR alt cov=1 \n- OR is not in coding regions \n- OR is in Exac with a frequency of >0.005% \n  - except if it is either \n    - in TCGA > 3 times \n    - OR in Cosmic > 10 times \n  - AND in a set of known cancer regions.\n- OR exist in >5% of the CCLE samples\n  - except if they are in TCGA >5 times\n\n#### RNA\n\nOnce the expression files are saved, we do the following validations:\n- mean, max, var...\n- comparison to previous release: same mean, max, var...\n- we QC on the amount of genes with 0 counts for each samples\n\n\nAfter QC, data is uploaded to taiga for all portal audiences according to release dates in Gumbo.\n\n\n@[jkobject](https://www.jkobject.com)\n@gkugener\n@gmiller\n@5im1z\n@__[BroadInsitute](https://www.broadinstitute.org)\n\nIf you have any feedback or run into any issues, feel free to post an issue on the github repo.\n",103,cancer-genomics,HTML,8,Jupyter Notebook,R,HTML,WDL,Python,Shell,Dockerfile,Makefile,,,,,,,,,,,,,,,,,,,,,166,38,123,5,104,38,0,430889,22,50,48,2,cf950f8c695cbf61db39d16718f999c622e1f16d,Merge pull request #217 from broadinstitute/lauren_tcga_vcf_dockstore…,2024-07-09T15:39:39Z,Alvin Qin,884386+qinqian@users.noreply.github.com,qinqian,23Q4,23Q4 DepMap Omics release,23Q4,Simone Zhang,,5im1z,,depmap_omics,broadinstitute,4,depmap,cancer-genomics,cloud-computing,data-science,,,,,,,,,,,,,,,,,/broadinstitute/depmap_omics,9,18,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/broadinstitute/cromwell,https://github.com/broadinstitute/cromwell,1,,,1,1,1,1,0,0,0,0,0,0,1,Scientific workflow engine designed for simplicity & scalability. Trivially transition between one off use cases to massive scale production environments,"[![codecov](https://codecov.io/gh/broadinstitute/cromwell/branch/develop/graph/badge.svg)](https://codecov.io/gh/broadinstitute/cromwell)\n\n## Welcome to Cromwell\n\nCromwell is an open-source Workflow Management System for bioinformatics. Licensing is [BSD 3-Clause](LICENSE.txt).\n\nThe [Cromwell documentation has a dedicated site](https://cromwell.readthedocs.io/en/stable).\n\nFirst time to Cromwell? Get started with [Tutorials](https://cromwell.readthedocs.io/en/stable/tutorials/FiveMinuteIntro/).\n\n### Community\n\nThinking about contributing to Cromwell? Get started by reading our [Contributor Guide](CONTRIBUTING.md).\n\nCromwell has a growing ecosystem of community-backed projects to make your experience even better! Check out our [Ecosystem](https://cromwell.readthedocs.io/en/stable/Ecosystem/) page to learn more.\n\nTalk to us:\n- [Join the Cromwell Slack workspace](https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g) to discuss the Cromwell workflow engine.\n- [Join the OpenWDL Slack workspace](https://join.slack.com/t/openwdl/shared_invite/zt-ctmj4mhf-cFBNxIiZYs6SY9HgM9UAVw) to discuss the evolution of the WDL language itself.\n    - More information about WDL is available in [that project's repository](https://github.com/openwdl/wdl).  \n\n### Capabilities and roadmap\n\nMany users today run their WDL workflows in [Terra](https://app.terra.bio/), a managed cloud bioinformatics platform with built-in WDL support provided by Cromwell. See [here](https://support.terra.bio/hc/en-us/articles/360036379771-Get-started-running-workflows) for a quick-start guide.\n\nUsers with specialized needs who wish to install and maintain their own Cromwell instances can [download](https://github.com/broadinstitute/cromwell/releases) a JAR or Docker image. The development team accepts reproducible bug reports from self-managed instances, but cannot feasibly provide direct support.\n\n[Cromwell's backends](https://cromwell.readthedocs.io/en/stable/backends/Backends/) receive development resources proportional to user demand. The team is actively developing for Google Cloud and Microsoft Azure (see [Cromwell on Azure](https://github.com/microsoft/CromwellOnAzure)). Maintenance of other backends is primarily community-based.\n\nCromwell [supports](https://cromwell.readthedocs.io/en/stable/LanguageSupport/) the WDL workflow language. Cromwell version 80 and above no longer support CWL.\n\nCWL will be re-introduced at a later date in the [Terra platform](https://terra.bio/), using a solution other than Cromwell. See the blog post [""Terra’s roadmap to supporting more workflow languages""](https://terra.bio/terras-roadmap-to-supporting-more-workflow-languages/) for details.\n\n### Security reports\n\nIf you believe you have found a security issue please contact `infosec@broadinstitute.org`.\n\n### Issue tracking\n\nNeed to file an issue? Head over to [Github Issues](https://github.com/broadinstitute/cromwell/issues).\n\nIf you previously filed an issue in JIRA, the link is [here](https://broadworkbench.atlassian.net/jira/software/c/projects/CROM/issues). New signups are no longer accepted.\n\n![Jamie, the Cromwell pig](docs/jamie_the_cromwell_pig.png)\n",974,bioinformatics,Scala,7,Scala,Java,Shell,HTML,WDL,Dockerfile,Python,,,,,,,,,,,,,,,,,,,,,,4625,1262,3321,42,553,266,0,47083,355,2836,2109,727,90ca58db0aa0a9b99c1591ced6292778cd912b5a,[WX-1700] Fix to string & optional concatenation (#7471),2024-07-18T20:11:04Z,Tom Wiseman,twiseman@broadinstitute.org,THWiseman,87,"## 87 Release Notes\r\n\r\n### GCP Batch\r\n\r\n * Added Nvidia driver install (default 418) ([#7235](https://github.com/broadinstitute/cromwell/pull/7235}))\r\n * Fixed Docker mounting volumes with extra colon ([#7240](https://github.com/broadinstitute/cromwell/pull/7240))\r\n * Fixed issue with multiple zones defined in config ([#7240](https://github.com/broadinstitute/cromwell/pull/7240))\r\n * Fixed Batch label regex ([#7355](https://github.com/broadinstitute/cromwell/pull/7355))\r\n\r\n### Progress toward WDL 1.1 Support\r\n\r\nWDL 1.1 support is in progress. Users that would like to try out the current partial support can do so by using\r\nWDL version `development-1.1`. As of Cromwell 87, `development-1.1` includes:\r\n * Engine functions:\r\n   * Added `suffix` ([#7363](https://github.com/broadinstitute/cromwell/pull/7363))\r\n   * Added `unzip` ([#7363](https://github.com/broadinstitute/cromwell/pull/7368))\r\n   * Added `quote` and `squote` ([#7375](https://github.com/broadinstitute/cromwell/pull/7375))\r\n   * Updated `sub` to expect POSIX-flavor regex ([#7374](https://github.com/broadinstitute/cromwell/pull/7374))\r\n * Struct literals can be included in WDLs ([#7391](https://github.com/broadinstitute/cromwell/pull/7391)) ([#7402](https://github.com/broadinstitute/cromwell/pull/7402))\r\n * Added `returnCodes` runtime attribute ([#7389](https://github.com/broadinstitute/cromwell/pull/7389))\r\n\r\n### `upgrade` command removed from Womtool\r\n\r\nWomtool previously supported a `womtool upgrade` command for upgrading draft-2 WDLs to 1.0. With WDL 1.1 soon to \r\nbecome the latest supported version, this functionality is retiring. ([#7382](https://github.com/broadinstitute/cromwell/pull/7382))\r\n\r\n### Replacement of `gsutil` with `gcloud storage`\r\n\r\nIn this release ([#7359](https://github.com/broadinstitute/cromwell/pull/7359)), all **localization** functionality on the GCP backend migrates to use the more modern and performant `gcloud storage`. With sufficiently powerful worker VMs, Cromwell can now localize at up to 1200 MB/s [0][1][2].\r\n\r\nIn a future release, **delocalization** will also migrate to `gcloud storage`. As part of that upcoming change, we are considering turning on [parallel composite uploads](https://cromwell.readthedocs.io/en/stable/backends/Google/#parallel-composite-uploads) by default to maximize performance. Delocalized composite objects will no longer have an md5 checksum in their metadata; refer to the matrix below [3]. If you have compatibility concerns for your workflow, please [submit an issue](https://github.com/broadinstitute/cromwell/issues).\r\n\r\n| Delocalization Strategy | Performance   | crc32c | md5 |\r\n|-------------------------|---------------|--------|-----|\r\n| Classic                 | Baseline/slow | ✅     | ✅  |\r\n| Parallel Composite      | Fast          | ✅     | ❌  |\r\n\r\n[0] Tested with Intel Ice Lake CPU platform, 16 vCPU, 32 GB RAM, 2500 GB SSD\r\n\r\n[1] [Throughput scales with vCPU count](https://cloud.google.com/compute/docs/disks/performance#n2_vms) with a plateau at 16 vCPUs.\r\n\r\n[2] [Throughput scales with disk size and type](https://cloud.google.com/compute/docs/disks/performance#throughput_limits_for_zonal) with at a plateau at 2.5 TB SSD. Worked example: 1200 MB/s ÷ 0.48 MB/s per GB = 2500 GB.\r\n\r\n[3] Cromwell itself uses crc32c hashes for call caching and is not affected\r\n\r\n### Other Improvements\r\n * In certain cases DRS downloads have been found to hang forever. Cromwell will now time these out. ([#7416](https://github.com/broadinstitute/cromwell/pull/7416))\r\n * Increased default Akka `client.parsing.max-response-reason-length` to 1024 ([#7406](https://github.com/broadinstitute/cromwell/pull/7406))\r\n * Workflow Completion Callback bodies now include fully-qualified output names ([#7234](https://github.com/broadinstitute/cromwell/pull/7234))\r\n * Improved workflow abort error handling ([#7245](https://github.com/broadinstitute/cromwell/pull/7245))\r\n * Improved logging for troubleshooting ([#7246](https://github.com/broadinstitute/cromwell/pull/7246)) ([#7253](https://github.com/broadinstitute/cromwell/pull/7253)) ([#7388](https://github.com/broadinstitute/cromwell/pull/7388))\r\n * Support for Intel Ice Lake chips in Life Sciences backend ([#7252](https://github.com/broadinstitute/cromwell/pull/7252))\r\n * Fix workflows getting stuck in Aborting when WDL has a type error ([#7385](https://github.com/broadinstitute/cromwell/pull/7385))\r\n * Updates to dependencies to fix security vulnerabilities.",87,Adam Nichols,,aednichols,"BSD 3-Clause ""New"" or ""Revised"" License",cromwell,broadinstitute,92,workflow-execution,workflow,cloud,hpc,bioinformatics,executor,scala,docker,ga4gh,containers,wdl,workflow-description-language,application,,,,,,,,/broadinstitute/cromwell,98,113,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/brick/geo,https://github.com/brick/geo,0,,,0,0,0,0,0,0,1,0,0,0,0,GIS geometry library for PHP,"Brick\Geo\n=========\n\n<img src=""https://raw.githubusercontent.com/brick/brick/master/logo.png"" alt="""" align=""left"" height=""64"">\n\nA GIS geometry library for PHP.\n\n[![Build Status](https://github.com/brick/geo/workflows/CI/badge.svg)](https://github.com/brick/geo/actions)\n[![Coverage Status](https://coveralls.io/repos/github/brick/geo/badge.svg?branch=master)](https://coveralls.io/github/brick/geo?branch=master)\n[![Latest Stable Version](https://poser.pugx.org/brick/geo/v/stable)](https://packagist.org/packages/brick/geo)\n[![Total Downloads](https://poser.pugx.org/brick/geo/downloads)](https://packagist.org/packages/brick/geo)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](http://opensource.org/licenses/MIT)\n\nIntroduction\n------------\n\nThis library is a PHP implementation of the [OpenGIS specification](http://www.opengeospatial.org/standards/sfa).\n\nIt provides [Geometry classes](#geometry-hierarchy) (`Point`, `LineString`, `Polygon`, etc.), and can natively read/write many formats: WKB, WKT, EWKB, EWKT, and GeoJSON.\n\nIt also provides a `GeometryEngine` interface for advanced calculations (`length`, `area`, `union`, `intersection`, etc.),\ntogether with implementations that delegate these operations to a third-party GIS engine: the [GEOS](https://git.osgeo.org/gitea/geos/php-geos) extension, or a GIS-enabled database such as MySQL or PostgreSQL.\n\nRequirements and installation\n-----------------------------\n\nThis library requires PHP 8.1.\nFor PHP 8.0, you can use version `0.9`.\nFor PHP 7.4, you can use version `0.7`.\n\nInstall the library with [Composer](https://getcomposer.org/):\n\n```bash\ncomposer require brick/geo\n```\n\nIf you only need basic operations such as building Geometry objects, importing from / exporting to one of the supported formats (WKB, WKT, EWKB, EWKT, or GeoJSON), then you're all set.\n\nIf you need advanced features, such as `length()`, `union()`, `intersection`, etc., head on to the [Configuration](#configuration) section to choose a `GeometryEngine` implementation.\n\nProject status & release process\n--------------------------------\n\nThis library is still under development.\n\nThe current releases are numbered `0.x.y`. When a non-breaking change is introduced (adding new methods, optimizing existing code, etc.), `y` is incremented.\n\n**When a breaking change is introduced, a new `0.x` version cycle is always started.**\n\nIt is therefore safe to lock your project to a given release cycle, such as `0.11.*`.\n\nIf you need to upgrade to a newer release cycle, check the [release history](https://github.com/brick/geo/releases) for a list of changes introduced by each further `0.x.0` version.\n\nQuick start\n-----------\n\n```php\nuse Brick\Geo\LineString;\nuse Brick\Geo\Point;\nuse Brick\Geo\Polygon;\n\n// Building geometries from coordinates\n\n$lineString = LineString::of(\n    Point::xy(1, 2),\n    Point::xy(3, 4),\n);\n\necho $lineString->asText(); // LINESTRING (1 2, 3 4)\n\n// Importing geometries\n\n$point = Point::fromText('POINT (1 2)');\n\necho $point->x(); // 1\necho $point->y(); // 2\n\n// Using advanced calculations from a GeometryEngine\n// (see the Configuration section)\n\n$polygon = Polygon::fromText('POLYGON ((0 0, 0 3, 3 3, 0 0))');\necho $geometryEngine->area($polygon); // 4.5\n\n$centroid = $geometryEngine->centroid($polygon);\necho $centroid->asText(); // POINT (1 2)\n```\n\nConfiguration\n-------------\n\nAdvanced calculations are available through the `GeometryEngine` interface. The library ships with the following implementations:\n\n- `PDOEngine`: communicates with a GIS-compatible database over a `PDO` connection.  \n  This engine currently supports the following databases:\n  - [MySQL](http://php.net/manual/en/ref.pdo-mysql.php) version 5.6 or greater (*2D geometries only*)\n  - MariaDB version 5.5 or greater\n  - [PostgreSQL](http://php.net/manual/en/ref.pdo-pgsql.php) with the [PostGIS](http://postgis.net/install) extension.\n- `SQLite3Engine`: communicates with a [SQLite3](http://php.net/manual/en/book.sqlite3.php) database with the [SpatiaLite](https://www.gaia-gis.it/fossil/libspatialite/index) extension.\n- `GEOSEngine`: uses the [GEOS](https://git.osgeo.org/gitea/geos/php-geos) PHP extension\n\nYour choice for the right implementation should be guided by two criteria:\n\n- **availability**: if you already use a GIS-enabled database such as MySQL, this may be an easy choice;\n- **capabilities**: not all databases offer the same GIS capabilities:\n  - some functions may be available on PostgreSQL but not on other databases (see the [Spatial Function Reference](#spatial-function-reference) section)\n  - some functions may be restricted to certain geometry types and/or SRIDs; for example, `buffer()` works on MySQL, but would fail with a `Polygon` on SRID 4326 (GPS coordinates, distance in meters)\n  - some databases may return distances in meters on SRID 4326, while others may return distances in degrees\n\nYou should probably start with the easiest method that works for you, and test if this setup matches your expectations.\n\nFollowing is a step-by-step guide for all possible configurations:\n\n### Using PDO and MySQL 5.6 or greater\n\n<details>\n<summary>Click to expand</summary>\n\n- Ensure that your MySQL version is at least `5.6`.  \n  Earlier versions only have partial GIS support based on bounding boxes and are not supported.\n- Use this bootstrap code in your project:\n\n    ```php\n    use Brick\Geo\Engine\PDOEngine;\n    \n    $pdo = new PDO('mysql:host=localhost', 'root', '');\n    $geometryEngine = new PDOEngine($pdo);\n    ```\n\nUpdate the code with your own connection parameters, or use an existing `PDO` connection if you have one (recommended).\n</details>\n\n### Using PDO and MariaDB 5.5 or greater\n\n<details>\n<summary>Click to expand</summary>\n\nMariaDB is a fork of MySQL, so you can follow the same procedure as for MySQL.\nJust ensure that your MariaDB version is `5.5` or greater.\n</details>\n\n### Using PDO and PostgreSQL with PostGIS\n\n<details>\n<summary>Click to expand</summary>\n\n- Ensure that [PostGIS is installed](http://postgis.net/install/) on your server\n- Enable PostGIS on the database server if needed:\n\n        CREATE EXTENSION postgis;\n\n- Use this bootstrap code in your project:\n\n    ```php\n    use Brick\Geo\Engine\PDOEngine;\n    \n    $pdo = new PDO('pgsql:host=localhost', 'postgres', '');\n    $geometryEngine = new PDOEngine($pdo);\n    ```\n\nUpdate the code with your own connection parameters, or use an existing `PDO` connection if you have one (recommended).\n</details>\n\n### Using PDO and SQLite with SpatiaLite\n\n<details>\n<summary>Click to expand</summary>\n\nDue to [limitations in the PDO_SQLITE driver](https://bugs.php.net/bug.php?id=64810), it is currently not possible<sup>*</sup> to load the SpatiaLite extension with a\n`SELECT LOAD_EXTENSION()` query, hence you cannot use SpatiaLite with the PDO driver.\n\nYou need to use the SQLite3 driver instead. Note that you can keep using your existing PDO_SQLITE code,\nall you need to do is create an additional in-memory SQLite3 database just to power the geometry engine.\n\n<sup>* It actually *is* possible, using [moxio/sqlite-extended-api](https://github.com/Moxio/sqlite-extended-api), which uses FFI and [Z-Engine](https://github.com/lisachenko/z-engine), but beware that this library is still experimental!</sup>\n</details>\n\n### Using SQLite3 with SpatiaLite\n\n<details>\n<summary>Click to expand</summary>\n\n- Ensure that [SpatiaLite is installed](https://www.gaia-gis.it/fossil/libspatialite/index) on your system.\n- Ensure that the SQLite3 extension is enabled in your `php.ini`:\n\n        extension=sqlite3.so\n\n- Ensure that the SQLite3 extension dir where SpatiaLite is installed is configured in your `php.ini`:\n\n        [sqlite3]\n        sqlite3.extension_dir = /usr/lib\n\n- Use this bootstrap code in your project:\n\n    ```php\n    use Brick\Geo\Engine\SQLite3Engine;\n    \n    $sqlite3 = new SQLite3(':memory:');\n    $sqlite3->loadExtension('mod_spatialite.so');\n    $geometryEngine = new SQLite3Engine($sqlite3);\n    ```\n\n- Depending on the functions you use, you will probably need to initialize the spatial metadata by running this query:\n\n    ```sql\n    SELECT InitSpatialMetaData();\n    ```\n  \n  You only need to run this query once if your database is persisted, but **if your database is in-memory, you'll need to run it on every connection**. Be aware that this may hurt performance.\n\nIn this example we have created an in-memory database for our GIS calculations, but you can also use an existing `SQLite3` connection.\n</details>\n\n### Using GEOS PHP bindings\n\n<details>\n<summary>Click to expand</summary>\n\n- Ensure that [the PHP bindings for GEOS](https://git.osgeo.org/gitea/geos/php-geos) are installed on your server (GEOS 3.6.0 onwards; previous versions require compiling GEOS with the `--enable-php` flag).\n- Ensure that the GEOS extension is enabled in your `php.ini`:\n\n        extension=geos.so\n\n- Use this bootstrap code in your project:\n\n    ```php\n    use Brick\Geo\Engine\GEOSEngine;\n    \n    $geometryEngine = new GEOSEngine();\n    ```\n</details>\n\nGeometry hierarchy\n------------------\n\nAll geometry objects reside in the `Brick\Geo` namespace, and extend a base `Geometry` class:\n\n- [Geometry](https://github.com/brick/geo/blob/master/src/Geometry.php) `abstract`\n  - [Point](https://github.com/brick/geo/blob/master/src/Point.php)\n  - [Curve](https://github.com/brick/geo/blob/master/src/Curve.php) `abstract`\n    - [LineString](https://github.com/brick/geo/blob/master/src/LineString.php)\n    - [CompoundCurve](https://github.com/brick/geo/blob/master/src/CompoundCurve.php)\n    - [CircularString](https://github.com/brick/geo/blob/master/src/CircularString.php)\n  - [Surface](https://github.com/brick/geo/blob/master/src/Surface.php) `abstract`\n    - [Polygon](https://github.com/brick/geo/blob/master/src/Polygon.php)\n      - [Triangle](https://github.com/brick/geo/blob/master/src/Triangle.php)\n    - [CurvePolygon](https://github.com/brick/geo/blob/master/src/CurvePolygon.php)\n    - [PolyhedralSurface](https://github.com/brick/geo/blob/master/src/PolyhedralSurface.php)\n      - [TIN](https://github.com/brick/geo/blob/master/src/TIN.php)\n  - [GeometryCollection](https://github.com/brick/geo/blob/master/src/GeometryCollection.php)\n    - [MultiPoint](https://github.com/brick/geo/blob/master/src/MultiPoint.php)\n    - [MultiCurve](https://github.com/brick/geo/blob/master/src/MultiCurve.php) `abstract`\n      - [MultiLineString](https://github.com/brick/geo/blob/master/src/MultiLineString.php)\n    - [MultiSurface](https://github.com/brick/geo/blob/master/src/MultiSurface.php) `abstract`\n      - [MultiPolygon](https://github.com/brick/geo/blob/master/src/MultiPolygon.php)\n\nGeometry exceptions\n-------------------\n\nAll geometry exceptions reside in the `Brick\Geo\Exception` namespace, and extend a base `GeometryException` object.\n\nGeometry exceptions are fine-grained: only subclasses of the base `GeometryException` class are thrown throughout\nthe project. This leaves to the user the choice to catch only specific exceptions, or all geometry-related exceptions.\n\nHere is a list of all exceptions:\n\n- `CoordinateSystemException` is thrown when mixing objects with different SRID or dimensionality (e.g. XY with XYZ)\n- `EmptyGeometryException` is thrown when trying to access a non-existent property on an empty geometry\n- `GeometryEngineException` is thrown when a functionality is not supported by the current geometry engine\n- `GeometryIOException` is thrown when an error occurs while reading or writing (E)WKB/T data\n- `InvalidGeometryException` is thrown when creating an invalid geometry, such as a `LineString` with only one `Point`\n- `NoSuchGeometryException` is thrown when attempting to get a geometry at a non-existing index in a collection\n- `UnexpectedGeometryException` is thrown when a geometry is not an instance of the expected sub-type, for example when\ncalling `Point::fromText()` with a `LineString` WKT.\n\nGeometryEngine methods reference\n--------------------------------\n\nThis is a list of all methods available in the `GeometryEngine` interface. Some methods are only available\nif you use a specific geometry engine, sometimes with a minimum version.\n\n| Function Name      | GEOS | PostGIS | MySQL  | MariaDB | SpatiaLite |\n|--------------------|------|---------|--------|---------|------------|\n| `area`             | ✓    | ✓       | ✓      | ✓       | ✓          |\n| `azimuth`          |      | ✓       |        |         | ✓          |\n| `boundary`         | ✓    | ✓       |        |         | ✓          |\n| `buffer`           | ✓    | ✓       | ✓      | ✓       | ✓          |\n| `centroid`         | ✓    | ✓       | ✓      | ✓       | ✓          |\n| `contains`         | ✓    | ✓       | ✓      | ✓       | ✓          |\n| `convexHull`       | ✓    | ✓       | 5.7.6  |         | ✓          |\n| `crosses`          | ✓    | ✓       | ✓      | ✓       | ✓          |\n| `difference`       | ✓    | ✓       | ✓      | ✓       | ✓          |\n| `disjoint`         | ✓    | ✓       | ✓      | ✓       | ✓          |\n| `distance`         | ✓    | ✓       | ✓      | ✓       | ✓          |\n| `envelope`         | ✓    | ✓       | ✓      | ✓       | ✓          |\n| `equals`           | ✓    | ✓       | ✓      | ✓       | ✓          |\n| `intersection`     | ✓    | ✓       | ✓      | ✓       | ✓          |\n| `intersects`       | ✓    | ✓       | ✓      | ✓       | ✓          |\n| `isClosed`         | ✓    | ✓       | ✓      | ✓       | ✓          |\n| `isRing`           | ✓    | ✓       | ✓      | ✓       | ✓          |\n| `isSimple`         | ✓    | ✓       | ✓      | ✓       | ✓          |\n| `isValid`          | ✓    | ✓       | 5.7.6  |         | ✓          |\n| `length`           | ✓    | ✓       | ✓      | ✓       | ✓          |\n| `locateAlong`      |      | ✓       |        |         | ✓          |\n| `locateBetween`    |      | ✓       |        |         | ✓          |\n| `makeValid`        |      | ✓       |        |         | ✓          |\n| `maxDistance`      |      | ✓       |        |         | ✓          |\n| `overlaps`         | ✓    | ✓       | ✓      | ✓       | ✓          |\n| `pointOnSurface`   | ✓    | ✓       |        |         | ✓          |\n| `relate`           | ✓    | ✓       |        |         | ✓          |\n| `simplify`         | ✓    | ✓       | 5.7.6  |         | 4.1.0      |\n| `snapToGrid`       |      | ✓       |        |         | ✓          |\n| `split`            |      | ✓       |        |         | ✓          |\n| `symDifference`    | ✓    | ✓       | ✓      | ✓       | ✓          |\n| `touches`          | ✓    | ✓       | ✓      | ✓       | ✓          |\n| `transform`        | ✓    | ✓       | 8.0.13 | ✓       | ✓          |\n| `union`            | ✓    | ✓       | ✓      | ✓       | ✓          |\n| `within`           | ✓    | ✓       | ✓      | ✓       | ✓          |\n\nImporting and exporting geometries\n----------------------------------\n\nThis library supports importing from and exporting to the following formats:\n\n- WKT\n- WKB\n- EWKT\n- EWKB\n- GeoJSON\n\n### WKT\n\nWell-Known Text is the standard text format for geometries.\n\nEvery Geometry class provides a convenience method `fromText()`, that accepts a WKT string and an optional SRID, and\nreturns a Geometry object:\n\n```php\nuse Brick\Geo\Point;\n\n$point = Point::fromText('POINT (1.5 2.5)', 4326);\n```\n\nGeometries can be converted to WKT using the convenience method `asText()`:\n\n```php\necho $point->asText(); // POINT (1.5 2.5)\n```\n\nYou can alternatively use the [WKTReader](https://github.com/brick/geo/blob/master/src/IO/WKTReader.php) and\n[WKTWriter](https://github.com/brick/geo/blob/master/src/IO/WKTWriter.php) classes directly; the latter allows you to\npretty-print the output.\n\n### WKB\n\nWell-Known Binary is the standard binary format for geometries.\n\nEvery Geometry class provides a convenience method `fromBinary()`, that accepts a WKB binary string and an optional\nSRID, and returns a Geometry object:\n\n```php\nuse Brick\Geo\Point;\n\n$point = Point::fromBinary(hex2bin('0101000000000000000000f83f0000000000000440'), 4326);\n\necho $point->asText(); // POINT (1.5 2.5)\necho $point->SRID(); // 4326\n```\n\nGeometries can be converted to WKB using the convenience method `asBinary()`:\n\n```php\necho bin2hex($point->asBinary()); // 0101000000000000000000f83f0000000000000440\n```\n\nYou can alternatively use the [WKBReader](https://github.com/brick/geo/blob/master/src/IO/WKBReader.php) and\n[WKBWriter](https://github.com/brick/geo/blob/master/src/IO/WKBWriter.php) classes directly; the latter allows you to\nchoose the endianness of the output (big endian or little endian).\n\n### EWKT\n\nExtended WKT is a PostGIS-specific text format that includes the SRID of the geometry object, which is missing from the\nstandard WKT format. You can import from and export to this format using the\n[EWKTReader](https://github.com/brick/geo/blob/master/src/IO/EWKTReader.php) and\n[EWKTWriter](https://github.com/brick/geo/blob/master/src/IO/EWKTWriter.php) classes:\n\n```php\nuse Brick\Geo\Point;\nuse Brick\Geo\IO\EWKTReader;\nuse Brick\Geo\IO\EWKTWriter;\n\n$reader = new EWKTReader();\n$point = $reader->read('SRID=4326; POINT (1.5 2.5)');\n\necho $point->asText(); // POINT (1.5 2.5)\necho $point->SRID(); // 4326\n\n$writer = new EWKTWriter();\necho $writer->write($point); // SRID=4326; POINT (1.5 2.5)\n```\n\n### EWKB\n\nExtended WKB is a PostGIS-specific binary format that includes the SRID of the geometry object, which is missing from\nthe standard WKB format. You can import from and export to this format using the\n[EWKBReader](https://github.com/brick/geo/blob/master/src/IO/EWKBReader.php) and\n[EWKBWriter](https://github.com/brick/geo/blob/master/src/IO/EWKBWriter.php) classes:\n\n```php\nuse Brick\Geo\Point;\nuse Brick\Geo\IO\EWKBReader;\nuse Brick\Geo\IO\EWKBWriter;\n\n$reader = new EWKBReader();\n$point = $reader->read(hex2bin('0101000020e6100000000000000000f83f0000000000000440'));\n\necho $point->asText(); // POINT (1.5 2.5)\necho $point->SRID(); // 4326\n\n$writer = new EWKBWriter();\necho bin2hex($writer->write($point)); // 0101000020e6100000000000000000f83f0000000000000440\n```\n\n### GeoJSON\n\nGeoJSON is an open standard format designed for representing simple geographical features, based on JSON, and\nstandardized in [RFC 7946](https://tools.ietf.org/html/rfc7946).\n\nThis library supports importing geometries from, and exporting them to GeoJSON documents using the\n[GeoJSONReader](https://github.com/brick/geo/blob/master/src/IO/GeoJSONReader.php) and\n[GeoJSONWriter](https://github.com/brick/geo/blob/master/src/IO/GeoJSONWriter.php) classes:\n\n```php\nuse Brick\Geo\Point;\nuse Brick\Geo\IO\GeoJSONReader;\nuse Brick\Geo\IO\GeoJSONWriter;\n\n$reader = new GeoJSONReader();\n$point = $reader->read('{ ""type"": ""Point"", ""coordinates"": [1, 2] }');\n\necho $point->asText(); // POINT (1 2)\necho $point->SRID(); // 4326\n\n$writer = new GeoJSONWriter();\necho $writer->write($point); // {""type"":""Point"",""coordinates"":[1,2]}\n```\n\nThe library supports reading and writing `Feature` and `FeatureCollection` objects, together with custom properties.\n\nGeoJSON aims to support WGS84 only, and as such all Geometries are imported using [SRID 4326](https://epsg.io/4326).\n\n## Doctrine mappings\n\nYou can use `brick/geo` types in your Doctrine entities using the [brick/geo-doctrine](https://github.com/brick/geo-doctrine) package.\n",215,geometry,PHP,1,PHP,,,,,,,,,,,,,,,,,,,,,,,,,,,,20,6,11,3,6,5,0,1349,32,30,22,8,28b5f1ced22c82bafff51ad0892150cdc7ea7cad,Prepare for release,2024-06-07T20:30:04Z,Benjamin Morel,benjamin.morel@gmail.com,BenMorel,0.11.0,💥 **Breaking changes**\r\n\r\n- interface `GeometryEngine` has a new method: `split()`\r\n- method `GeometryEngine::boundingPolygons()` has been removed\r\n\r\n✨ **New features**\r\n\r\n- New engine method: `GeometryEngine::split()`\r\n,0.11.0,Benjamin Morel,,BenMorel,MIT License,geo,brick,23,php,geometry,gis,,,,,,,,,,,,,,,,,,/brick/geo,23,16,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/boutproject/BOUT-dev,https://github.com/boutproject/BOUT-dev,1,,,1,1,1,0,0,0,0,0,0,0,0,BOUT++: Plasma fluid finite-difference simulation code in curvilinear coordinate systems,"# BOUT++\n\n<!---Build nice shields at shields.io-->\n[![Build Status](https://github.com/boutproject/BOUT-dev/actions/workflows/tests.yml/badge.svg?branch=next)](https://github.com/boutproject/BOUT-dev/actions)\n[![License](https://img.shields.io/badge/license-LGPL-blue.svg)](https://www.gnu.org/licenses/lgpl-3.0.en.html)\n[![py3comp](https://img.shields.io/badge/py3-compatible-brightgreen.svg)](https://img.shields.io/badge/py3-compatible-brightgreen.svg)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.8369888.svg)](https://doi.org/10.5281/zenodo.8369888)\n\n```\n.______     ______    __    __  .___________.\n|   _  \   /  __  \  |  |  |  | |           |  _     _\n|  |_)  | |  |  |  | |  |  |  | `---|  |----`_| |_ _| |_\n|   _  <  |  |  |  | |  |  |  |     |  |    |_   _|_   _|\n|  |_)  | |  `--'  | |  `--'  |     |  |      |_|   |_|\n|______/   \______/   \______/      |__|\n```\n\nBOUT++ is a framework for writing fluid and plasma simulations in\ncurvilinear geometry. It is intended to be quite modular, with a\nvariety of numerical methods and time-integration solvers\navailable. BOUT++ is primarily designed and tested with reduced plasma\nfluid models in mind, but it can evolve any number of equations, with\nequations appearing in a readable form.\n\nFor example, the following set of equations for magnetohydrodynamics\n(MHD):\n\n![ddt_rho](http://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20%5Crho%7D%7B%5Cpartial%20t%7D%20%3D%20-%5Cmathbf%7Bv%7D%5Ccdot%5Cnabla%5Crho%20-%20%5Crho%5Cnabla%5Ccdot%5Cmathbf%7Bv%7D)\n![ddt_p](http://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20p%7D%7B%5Cpartial%20t%7D%20%3D%20-%5Cmathbf%7Bv%7D%5Ccdot%5Cnabla%20p%20-%20%5Cgamma%20p%5Cnabla%5Ccdot%5Cmathbf%7Bv%7D)\n![ddt_v](http://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20%5Cmathbf%7Bv%7D%7D%7B%5Cpartial%20t%7D%20%3D%20-%5Cmathbf%7Bv%7D%5Ccdot%5Cnabla%5Cmathbf%7Bv%7D%20&plus;%20%5Cfrac%7B1%7D%7B%5Crho%7D%28-%5Cnabla%20p%20&plus;%20%28%5Cnabla%5Ctimes%5Cmathbf%7BB%7D%29%5Ctimes%5Cmathbf%7BB%7D%29)\n![ddt_B](http://latex.codecogs.com/png.latex?%7B%7B%5Cfrac%7B%5Cpartial%20%5Cmathbf%7BB%7D%7D%7B%5Cpartial%20t%7D%7D%7D%20%3D%20%5Cnabla%5Ctimes%28%5Cmathbf%7Bv%7D%5Ctimes%5Cmathbf%7BB%7D%29)\n\ncan be written simply as:\n\n```cpp\nddt(rho) = -V_dot_Grad(v, rho) - rho*Div(v);\nddt(p)   = -V_dot_Grad(v, p) - g*p*Div(v);\nddt(v)   = -V_dot_Grad(v, v) + (cross(Curl(B),B) - Grad(p))/rho;\nddt(B)   = Curl(cross(v,B));\n```\n\nThe full code for this example can be found in the [orszag-tang\nexample](examples/orszag-tang/mhd.cxx).\n\nJointly developed by University of York (UK), LLNL, CCFE, DCU, DTU,\nand other international partners.\n\n\nHomepage found at [http://boutproject.github.io/](http://boutproject.github.io/)\n\n## Table of Contents\n* [Requirements](#requirements)\n* [Usage and installation](#usage-and-installation)\n* [Terms of use](#terms-of-use)\n* [Overview of files](#overview-of-files)\n* [Contributing](#contributing)\n* [License](#license)\n\n## Requirements\n\nBOUT++ needs the following:\n\n* A C++14 compiler\n* MPI\n* NetCDF\n\nBOUT++ has the following optional dependencies:\n\n* FFTW3 (strongly recommended!)\n* OpenMP\n* PETSc\n* SLEPc\n* ARKODE\n* IDA\n* CVODE\n* LAPACK\n* Score-p (for performance diagnostics)\n\n## Usage and installation\nPlease see the [users manual](http://bout-dev.readthedocs.io)\n\n## Terms of use\n\nBOUT++ is released under the LGPL, but since BOUT++ is a\nscientific code we also ask that you show professional courtesy\nwhen using this code:\n\n1. Since you are benefiting from work on BOUT++, we ask that you\n   submit any improvements you make to the code to us by submitting a\n   pull request to this repository\n2. If you use BOUT++ results in a paper or professional publication,\n   we ask that you send your results to one of the BOUT++ authors\n   first so that we can check them. It is understood that in most cases\n   if one or more of the BOUT++ team are involved in preparing results\n   then they should appear as co-authors.\n3. If you use BOUT++ in your work, please help ensure that all the\n   authors get the credit they deserve by citing BOUT++, preferably\n   using the DOI of the version you used. See the file\n   [CITATION.cff](CITATION.cff) for details. In addition, you may also\n   cite either of the two main papers: B. Dudson et al,\n   Comp. Phys. Comm. 2009, and B. Dudson et al, Phys. of Plasmas 2016\n\nYou can convert the CITATION.cff file into a Bibtex file as follows:\n\n    pip3 install --user cffconvert\n    cffconvert -if CITATION.cff -f bibtex -of CITATION.bib\n\n## Overview of files\n\nThis directory contains\n\n* **bin**                   Files for setting the BOUT++ configuration\n* **examples**              Example models and test codes\n* **externalpackages**      External packages needed for installing BOUT++\n* **include**               Header files used in BOUT++\n* **manual**                Manuals and documentation (also [doxygen](http://www.stack.nl/~dimitri/doxygen/) documentation)\n* **src**                   The main code directory\n* **CITATION**              Contains the paper citation for BOUT++\n* **LICENSE**               LGPL license\n* **LICENSE.GPL**           GPL license\n* **tools**                 Tools for helping with analysis, mesh generation, and data managment\n\n  * **archiving**               Routines for managing input/output files e.g. compressing data, converting formats, and managing runs\n  * **cyl_and_helimak_grids**   IDL codes for generating cylindrical and helimak grids\n  * **eigensolver**             Matlab routines for solving eigenmodes\n  * **idllib**                  Analysis codes in IDL. Add this to your IDL_PATH environment variable\n  * **line_tracing**            IDL routines for line tracing of field lines\n  * **line_tracing_v2**         Newer version of the IDL routines for line tracing of field lines\n  * **mathematicalib**          Library for post processing using Mathematica\n  * **matlablib**               Library for post processing using MATLAB\n  * **numlib**                  Numerical IDL routines\n  * **octave**                  Routines for post processing using octave\n  * **plasmalib**               IDL routines for calculation of plasma parameters\n  * **pdb2idl**                 Library to read Portable Data Binary (PDB) files into IDL\n  * **pylib**                   Analysis codes in Python\n\n    * **boutdata**        Routines to simplify accessing BOUT++ output\n    * **boututils**       Some useful routines for accessing and plotting data\n    * **post_bout**       Routines for post processing in BOUT++\n\n  * **slab**              IDL routine for grid generation of a slab\n  * **tokamak_grids**     Code to generate input grids for tokamak equilibria\n\n    * **gridgen**         Grid generator in IDL. Hypnotoad GUI for converting G-EQDSK files into a flux-aligned orthogonal grid.\n    * **elite**           Convert ELITE .eqin files into an intermediate binary file\n    * **gato**            Convert DSKGATO files into intermediate binary format\n    * **all**             Convert the intermediate binary file into BOUT++ input grid\n    * **coils**           Routines for calculating the field due to external RMP coils and adding to existing equilibria\n    * **cyclone**         Generate cyclone test cases (concentric circle ""equilibrium"" for local flux-surface calculations)\n    * **py_gridgen**      Translation"" into python of the corresponding IDL routines in the folder gridgen\n    * **shifted_circle**  Produce shifted cirle equilibria input grids\n\n\n## Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md).\n\n## License\nCopyright 2010 B.D.Dudson, S.Farley, M.V.Umansky, X.Q.Xu\n\nBOUT++ is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nBOUT++ is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Lesser General Public License for more details.\n\nA copy of the LGPL license is in [LICENSE](LICENSE). Since this is based\non (and refers to) the GPL, this is included in [LICENSE.GPL](LICENSE.GPL).\n\nSome of the autoconf macros under [m4](m4) are licensed under\nGPLv3. These are not necessary to either build or run BOUT++, but are\nused in the creation of [configure](configure) from\n[configure.ac](configure.ac), and are provided as a courtesy to\ndevelopers. You are free to substitute them with other autoconf macros\nthat provide equivalent functionality.\n\nBOUT++ links by default with some GPL licensed libraries. Thus if you\ncompile BOUT++ with any of them, BOUT++ will automatically be licensed\nas GPL. Thus if you want to use BOUT++ with GPL non-compatible code,\nmake sure to compile without GPLed code.\n\n",179,physics,C++,19,Shell,C++,Makefile,Python,TeX,Jupyter Notebook,M4,IDL,Prolog,Perl,C,MATLAB,Mathematica,Objective-C,CMake,Jinja,CSS,Cython,Dockerfile,,,,,,,,,,2119,271,1813,35,183,52,0,62322,93,827,633,194,c193e73fbeea206f42f69107395dcafc43f9c861,Merge pull request #2913 from boutproject/dependabot/github_actions/Z…,2024-05-16T17:13:31Z,Ben Dudson,dudson2@llnl.gov,bendudson,v5.1.0,"## What's Changed\r\n* SNB and InvertParDiv for nonuniform grids by @bendudson in https://github.com/boutproject/BOUT-dev/pull/2572\r\n* Fix error message by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2642\r\n* Expose mixed derivatives by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2639\r\n* Cleanup syntax by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2638\r\n* Merge v5 into next - resolve conflicts by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2658\r\n* Merge v5 into next by @ZedThree in https://github.com/boutproject/BOUT-dev/pull/2655\r\n* Squash optional by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2659\r\n* Always provide applyParallelBoundary by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2640\r\n* Fix getUniform warning by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2644\r\n* Handle missing python better for finding numpy by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2643\r\n* Fix some warnings and deprecated headers by @ZedThree in https://github.com/boutproject/BOUT-dev/pull/2666\r\n* Fix versions after release by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2669\r\n* Improve docs + fix boutpp docs (master) by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2664\r\n* Fix some warnings by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2661\r\n* switch to BOUT_FOR and simplify Div_a_Grad_perp by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2641\r\n* fix boutconfig's `has` by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2647\r\n* Fix versions after release (next) by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2670\r\n* Compute modulo only once by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2645\r\n* Set some _ROOT vars by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2652\r\n* Add dependabot config by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2680\r\n* Some fixes for tokamak-2fluid example by @ZedThree in https://github.com/boutproject/BOUT-dev/pull/2668\r\n* Error early with in-source-builds by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2660\r\n* More forgiving restart inputs by @bendudson in https://github.com/boutproject/BOUT-dev/pull/2676\r\n* apply clang-format v15.0.7 and .clang-format@d8f14fdd by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2662\r\n* Python: If we are on a tag, we should use that as version by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2683\r\n* Recommend `requirements.txt` for dependencies by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2689\r\n* Add `plt.show()` to example by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2687\r\n* CI: switch to openmpi for fedora GHA by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2682\r\n* Fix pvode warnings by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2684\r\n* Track more build options by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2634\r\n* Revert ""Workaround for test-restart-io for recent boututils"" by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2685\r\n* Update boututils and boutdata to working versions by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2679\r\n* Update bundled boututils and boutdata by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2657\r\n* isUniform: Do not fail if numbers are only almost equal by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2693\r\n* Docs for docs improvements by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2690\r\n* Fix SUNDIALS `Context` construction for MPICH by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2678\r\n* Improve checking in coords constructor by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2650\r\n* CI: do not run on removed old ubuntu by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2695\r\n* Fall back to pkgconfig for finding netcdf-cxx4 by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2686\r\n* CI: Update runs-on (next) by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2696\r\n* Docs: Fix default branch for ""suggest edit""  by @ZedThree in https://github.com/boutproject/BOUT-dev/pull/2697\r\n* Improve invSg calculation by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2646\r\n* Add support for PETSc & SLEPc 3.18 / 3.19 by @ZedThree in https://github.com/boutproject/BOUT-dev/pull/2704\r\n* provide VectorMetric in vectormetric.hxx by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2706\r\n* Remove last uses of `bout::globals::dump` by @ZedThree in https://github.com/boutproject/BOUT-dev/pull/2699\r\n* Remove unnecessary check for aligned fields when applying twist-shift by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2711\r\n* The additional check is not needed by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2710\r\n* Get mesh outputs after physics init by @bendudson in https://github.com/boutproject/BOUT-dev/pull/2714\r\n* Solver improvements by @bendudson in https://github.com/boutproject/BOUT-dev/pull/2716\r\n* Review of PR #2514 (Switch to CMake) by @tomc271 in https://github.com/boutproject/BOUT-dev/pull/2694\r\n* Do not try to run SLEPc or PETSc in configure by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2709\r\n* Switch to CMake in CI and docs by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2514\r\n* Update `make dist` invocation by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2707\r\n* mesh:paralleltransform is a section by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2702\r\n* Remove references to build dir from install by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2628\r\n* CI install natsort by @bendudson in https://github.com/boutproject/BOUT-dev/pull/2723\r\n* Revert sundials detection by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2725\r\n* Fix static CI build by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2724\r\n* Bump `clang-tidy-review` version by @ZedThree in https://github.com/boutproject/BOUT-dev/pull/2726\r\n* CI: Install dnf5 for Fedora by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2731\r\n* Build containers in GHA CI by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2511\r\n* Generate BOUT++ tar via boutpp backend by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2708\r\n* Remove h5py - we only support netcdf by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2735\r\n* Update to dnf5 (master) by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2732\r\n* Improving compatibility with fmt 10 by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2730\r\n* Fix FV::Div_a_Grad_perp Y alignment by @bendudson in https://github.com/boutproject/BOUT-dev/pull/2722\r\n* Update RELEASE_HOWTO.md by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2741\r\n* 5.1 release candidate by @ZedThree in https://github.com/boutproject/BOUT-dev/pull/2720\r\n* CMake: Fix version detection from git tag by @ZedThree in https://github.com/boutproject/BOUT-dev/pull/2753\r\n* Add fallback if the version has been bumped bot not tagged by @dschwoerer in https://github.com/boutproject/BOUT-dev/pull/2754\r\n\r\n## New Contributors\r\n* @tomc271 made their first contribution in https://github.com/boutproject/BOUT-dev/pull/2694\r\n\r\n**Full Changelog**: https://github.com/boutproject/BOUT-dev/compare/v5.0.0...v5.1.0",v5.1.0,Peter Hill,,ZedThree,GNU Lesser General Public License v3.0,BOUT-dev,boutproject,20,plasma,physics,pde,numerical-modelling,physics-simulation-library,computational-physics,c-plus-plus,python,,,,,,,,,,,,,/boutproject/BOUT-dev,23,30,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/bmwcarit/ramses,https://github.com/bmwcarit/ramses,0,,,0,0,0,0,0,0,1,1,0,0,0,A distributed system for rendering 3D content with focus on bandwidth and resource efficiency,"<!-- RAMSES -->\n\n# Table of Contents\n* <project:#what-is-ramses>\n* <project:#obtaining-the-source-code>\n* <project:#build-instructions>\n* <project:#license>\n\n# What is RAMSES\n\nRAMSES is 3D rendering engine with focus on bandwidth and resource efficiency.\nFor a broader overview and introduction to the ecosystem and tools, visit our\n[homepage](https://ramses3d.org/).\n\nHave a look at our showcase video: [![RAMSES Distributed Rendering Engine Showcase Video](https://img.youtube.com/vi/tyzvEI25BMg/0.jpg)](https://www.youtube.com/watch?v=tyzvEI25BMg)\n\n# Obtaining the source code\nRAMSES can be cloned from its BMW Car IT repository using git:\n\n```\ngit clone --recurse-submodules https://github.com/bmwcarit/ramses <ramses-sdk>\n```\n\n# Build instructions\nGeneral building tips: RAMSES's build system is based on CMake. It has\nmandatory components and optional components which are built only if\nrequired dependencies and/or CMake flags are present. The CMake log will\nprovide info what was built and what not - a 'plus' indicating that something\nwas built, and 'minus' that it wasn't. If an optional component was not built,\nCMake will list the missing dependencies which were not found or not built.\nCheck the CMake logs! Typical build errors:\n- not able to find a compiler -> Check that you have a valid compiler!\n- not able to find something in external/ folder -> Check that you downloaded the submodules as shown in section 2.)\n- CMake can't identify the compiler of Visual studio Community edition -> You need to download a Windows SDK 8.1\n- No renderer was built on linux -> Check that you have installed some of the platform packages (x11-dev, egl, openGL, wayland)\n\nBuilding RAMSES on Windows:\n- start CMake GUI\n- select <ramses-sdk> as source path, choose arbitrary <build> folder.\nConfigure\nIf you want to build the tests, set 'ramses-sdk_BUILD_TESTS' to true in the CMake cache.\ngenerate -> open solution in Visual Studio.\n\nBuilding RAMSES on Linux with docker:\n\nWe prefer to build RAMSES in Docker because it abstracts the dependency installation\nand the CMake invocations away from the user. Docker is installed slightly differently\non different distros, check the docker manual for your distro. The instructions below\nare for Ubuntu 20.04 LTS:\n\n```\napt install docker.io\ngroupadd docker             #can fail if already exists\nusermod –aG docker $USER\nnewgrp docker               #logs you into the new group in the current terminal session\n\ndocker run hello-world      #check that docker works\n\n# build RAMSES docker container\ncd <ramses-sdk>/scripts/docker\n./build-basic-container.sh\n# Start RAMSES docker\n./start-basic-for-x11.sh    # (on wayland-enabled systems, optionally: ./start-basic-for-wayland.sh)\n# Inside docker container\n./build-ramses.sh\n# optionally - check if all RAMSES features work on your system\n./run-unittests.sh\n```\n\nBuilding RAMSES on Linux (without docker):\n\n-Install dependencies using Distro of choice package manager. For example, for Ubuntu:\n```\nsudo apt-get install libx11-dev libgles2-mesa-dev\nmkdir <build>\ncd <build>\ncmake <ramses-sdk>\n```\n\nYou can also check the docker container setup scripts for a reference how to build:\n```\n<ramses-sdk>/scripts/docker/ramses-basic/Dockerfile         -> contains info about build dependencies\n<ramses-sdk>/scripts/docker/runtime-files/build-ramses.sh   -> contains CMake command for building\n```\n\n# License\nRAMSES original code is copyright BMW Car IT or BMW AG\n\n```Copyright (C) 20xx BMW Car IT GmbH``` or ```Copyright (C) 20xx BMW AG```\n\nThe source code is licensed under the Mozilla Public License 2.0, please find a\ncopy of that license in the [LICENSE.txt](https://github.com/bmwcarit/ramses/blob/master/LICENSE.txt) file.\n\nRAMSES makes use of several open source libraries which can be found in the folder 'external'.\nSome of these are shipped directly with the sourcecode, others are included as git submodule references.\nRAMSES also includes some assets (e.g. font files) which are licensed under different open source licenses.\n\nDirectly included:\n- cityhash (Licensed under MIT License)\n- Khronos Headers (Licensed under Khronos Group License)\n- lodepng (Licensed under zlib License)\n- Wayland-IVI Extension (Licensed under MIT License)\n- wayland-zwp-linux-dmabuf-v1-extension (Licensed under MIT License)\n- Wayland-IVI example client (Licensed under MIT License)\n\nSubmodule reference:\n- Freetype 2 (Licensed under FTL, also containing code under BSD and ZLib)\n- GLSLang (Licensed under BSD-3 and Khronos Group License)\n- OpenGL Mathematics (Licensed under MIT License)\n- Googletest (Licensed under BSD-3)\n- Google Benchmark (Licensed under Apache-2.0)\n- Harfbuzz (Licensed under MIT and ISC; see external/harfbuzz/COPYING)\n- Asio (Boost Software License - Version 1.0)\n- LZ4 (Licensed under BSD-2; see also external/lz4/LICENSE for more details)\n- {fmt} (Licensed under MIT License)\n- ImGui (Licensed under MIT License)\n- Abseil (Licensed under Apache 2.0; see also external/abseil/LICENSE for more details)\n- CLI11 (Licensed under BSD-3)\n- Lua (Licensed under MIT)\n- Sol (Licensed under MIT)\n- Flatbuffers (Licensed under Apache-2.0)\n- Webkit ANGLE (Licensed under BSD-3)\n\nIncluded Assets:\n- Roboto Font (Licensed under Apache 2.0)\n- M+ Font (Licensed under Public Domain)\n- WenQuanYi MicroHei (Licensed under Apache 2.0)\n- Arimo Font (Licensed under Apache 2.0)\n- Droid Kufi Font (Licensed under Apache 2.0)\n- Droid Naskh Font (Licensed under Apache 2.0)\n- Satisfy Font (Licensed under Apache 2.0)\n- Noto Sans Thai Font (Licensed under SIL Open Font License 1.1)\n",362,graphics,C++,9,CMake,C++,C,GLSL,Python,Shell,Dockerfile,Batchfile,Objective-C++,,,,,,,,,,,,,,,,,,,,84,32,52,0,8,34,34,61281,58,36,27,9,068163a64bde899c55e05bfbc99fbfe379ccec7b,Oss release 28.2.0 created 2024-06-11-16-11 (#121),2024-06-12T14:21:52Z,Askanaz Torosyan,46795157+nVxx@users.noreply.github.com,nVxx,28.02.2000,,28.02.2000,,,github-actions[bot],Other,ramses,bmwcarit,11,genivi,genivi-alliance,graphics,,,,,,,,,,,,,,,,,,/bmwcarit/ramses,41,23,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/bioperl/bioperl-live,https://github.com/bioperl/bioperl-live,0,,,0,1,1,0,0,0,0,0,0,0,0,Core BioPerl 1.x code,"[![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.16344.svg)](http://dx.doi.org/10.5281/zenodo.16344)\n[![Build Status](https://github.com/bioperl/bioperl-live/actions/workflows/ci.yml/badge.svg)](https://github.com/bioperl/bioperl-live/actions/workflows/ci.yml)\n[![Coverage Status](https://coveralls.io/repos/bioperl/bioperl-live/badge.svg?branch=master)](https://coveralls.io/r/bioperl/bioperl-live?branch=master)\n[![Documentation Status](https://readthedocs.org/projects/bioperl/badge/?version=latest)](https://readthedocs.org/projects/bioperl/?badge=latest)\n\n# About BioPerl\n\nBioPerl is a project for development of free and open source Perl\ntools for computational molecular biology.  For example, it includes\nclasses for biological sequences, readers of multiple formats,\nsequence alignments, database searching objects, and interfaces to\nmultiple programs such as EMBOSS, ClustalW, and BLAST.\n\nThe BioPerl project has developed multiple module distributions for\ndifferent purposes.  The one named BioPerl (named after the project)\nprovides the foundation for all others distributions.\n\nThis is the repository for the BioPerl distribution only.  Other\ndistributions have [their own\nrepositories](https://github.com/bioperl/).\n\n# Installation\n\nBioPerl distribution has the same name as the BioPerl.  However, the\nBioPerl distribution only includes a subset of the project modules.\nBecause of this, the meaning of ""installing BioPerl"" is rarely clear.\nInstead of ""install BioPerl"", the aim must be ""install module X"".\n\n[CPAN.org](https://www.cpan.org/modules/INSTALL.html) provides an\noverview on how to install and manage Perl modules but the bottom-line\nis:\n\n1. find the module you need, for example `Bio::DB::EUtilities`\n2. install it with `cpanm`, for example `cpanm Bio::DB::EUtilities`\n\nAlternatively, some Linux distributions have packaged BioPerl and have\nit available through their package manager.\n\n# Documentation and Support\n\nDocumentation for individual modules is in POD and can also be read\nonline at [metacpan](https://metacpan.org/pod/BioPerl).  Useful\ndocumentation in the form of example code can also be found in the\n`examples/` and `bin/` directories.\n\nAdditional resources and information about the project is available on\nthe [project website](https://bioperl.org), with discussion happening\non the [bioperl-l@bioperl.org](mailto:bioperl-l@bioperl.org) mailing\nlist, and on the `#bioperl` channel of the freenode IRC server.\n\nBug reports are handle on the distribution github page.\n\n# Development\n\nSee the [`HACKING.md`](HACKING.md) file for details on the project\nstructure, such as building from source and running the test suite.\n",294,bioinformatics,Perl,6,Perl,Gnuplot,Parrot,Forth,Roff,F*,,,,,,,,,,,,,,,,,,,,,,,174,32,141,1,30,80,0,136197,181,209,156,53,9ce0d304f42ef3a6808e7e94be42cb56bb51068d,maint: remove Travis stuff which has been replaced with Github action…,2024-04-26T15:06:36Z,David Miguel Susano Pinto,carandraug+dev@gmail.com,carandraug,BioPerl 1.7.2,As released on CPAN.,release-1-7-2,Hilmar Lapp,,hlapp,,bioperl-live,bioperl,4,bioinformatics,bioperl,perl,toolkit,,,,,,,,,,,,,,,,,/bioperl/bioperl-live,96,54,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/BioPandas/biopandas,https://github.com/BioPandas/biopandas,0,,,0,1,0,0,0,0,1,0,0,0,0,Working with molecular structures in pandas DataFrames,"![Logo](./docs/img/logos/logo.png#gh-light-mode-only)![Logo](./docs/img/logos/logo_dark.png#gh-dark-mode-only)\n\n**Working with molecular structures in pandas DataFrames**\n\n\n[![Continuous Integration](https://travis-ci.org/rasbt/biopandas.svg?branch=master)](https://travis-ci.org/rasbt/biopandas)\n[![Build status](https://ci.appveyor.com/api/projects/status/jcp91fvbgmqws30p/branch/master?svg=true)](https://ci.appveyor.com/project/rasbt/biopandas/branch/master)\n[![Code Coverage](https://coveralls.io/repos/rasbt/biopandas/badge.svg?branch=master&service=github)](https://coveralls.io/github/rasbt/biopandas?branch=master)\n[![PyPI Version](https://img.shields.io/pypi/v/biopandas.svg)](https://pypi.python.org/pypi/biopandas/)\n[![License](https://img.shields.io/badge/license-new%20BSD-blue.svg)](https://github.com/rasbt/biopandas/blob/master/LICENSE)\n![Python 3](https://img.shields.io/badge/python-3-blue.svg)\n[![JOSS](http://joss.theoj.org/papers/10.21105/joss.00279/status.svg)](http://joss.theoj.org/papers/10.21105/joss.00279)\n[![Discuss](https://img.shields.io/badge/discuss-github-blue.svg)](https://github.com/rasbt/biopandas/discussions)\n\n<hr>\n\n## Links\n- Documentation: [https://BioPandas.github.io/biopandas/](https://BioPandas.github.io/biopandas/)\n- Source code repository: [https://github.com/rasbt/biopandas](https://github.com/rasbt/biopandas)\n- PyPI: [https://pypi.python.org/pypi/biopandas](https://pypi.python.org/pypi/biopandas)\n- How to contribute: [https://biopandas.github.io/biopandas/CONTRIBUTING/](https://biopandas.github.io/biopandas/CONTRIBUTING/)\n- Changelog: [./docs/sources/CHANGELOG.md](./docs/sources/CHANGELOG.md)\n\n<br>\n\nIf you are a computational biologist, chances are that you cursed one too many times about protein structure files. Yes, I am talking about ye Goode Olde Protein Data Bank format, aka ""PDB files."" Nothing against PDB, it's a neatly structured format (if deployed correctly); yet, it is a bit cumbersome to work with PDB files in ""modern"" programming languages -- I am pretty sure we all agree on this.\n\nAs machine learning and ""data science"" person, I fell in love with [pandas](http://pandas.pydata.org) DataFrames for handling just about everything that can be loaded into memory.  \nSo, why don't we take pandas to the structural biology world? Working with molecular structures of biological macromolecules (from PDB and MOL2 files) in pandas DataFrames is what BioPandas is all about!\n\n<br>\n\n## Examples\n\n![3eiy](./docs/img/index/3eiy.png#gh-light-mode-only)![3eiy](./docs/img/index/3eiy_dark.png#gh-dark-mode-only)\n\n```python\n# Initialize a new PandasPdb object\n# and fetch the PDB file from rcsb.org\n>>> from biopandas.pdb import PandasPdb\n>>> ppdb = PandasPdb().fetch_pdb('3eiy')\n>>> ppdb.df['ATOM'].head()\n```\n\n![3eiy head](./docs/img/index/3eiy_head.png#gh-light-mode-only)![3eiy head](./docs/img/index/3eiy_head_dark.png#gh-dark-mode-only)\n\n<br><br>\n<br><br>\n\n\n![3eiy head](./docs/img/index/ligand_rmsd.png#gh-light-mode-only)![3eiy head](./docs/img/index/ligand_rmsd_dark.png#gh-dark-mode-only)\n\n```python\n# Load structures from your drive and compute the\n# Root Mean Square Deviation\n>>> from biopandas.pdb import PandasPdb\n>>> pl1 = PandasPdb().read_pdb('./docking_pose_1.pdb')\n>>> pl2 = PandasPdb().read_pdb('./docking_pose_2.pdb')\n>>> r = PandasPdb.rmsd(pl1.df['HETATM'], pl2.df['HETATM'],\n                       s='hydrogen', invert=True)\n>>> print('RMSD: %.4f Angstrom' % r)\n\nRMSD: 2.6444 Angstrom\n```\n\n<br><br>\n<br><br>\n\n\n## Quick Install\n\n- install the latest version (from GitHub): `pip install git+git://github.com/rasbt/biopandas.git#egg=biopandas`\n- install the latest PyPI version: `pip install biopandas`\n- install biopandas via conda-forge: `conda install biopandas -c conda-forge`\n\n#### Requirements\n\n- [Python](https://www.python.org) >=3.7\n- [NumPy](http://www.numpy.org) >= 1.11.2\n- [SciPy](https://www.scipy.org/scipylib/index.html) >= 0.18.1\n- [Pandas](http://pandas.pydata.org) >= 0.19.1\n\n\nFor more information, please see [https://BioPandas.github.io/biopandas/installation/](https://BioPandas.github.io/biopandas/installation/).\n\n<br><br>\n<br><br>\n\n\n### Cite as\n\nIf you use BioPandas as part of your workflow in a scientific publication, please consider citing the BioPandas repository with the following DOI:\n\n- Sebastian Raschka. Biopandas: Working with molecular structures in pandas dataframes. *The Journal of Open Source Software*, 2(14), jun 2017. doi: 10.21105/joss.00279. URL http://dx.doi.org/10.21105/joss.00279.\n\n```\n@article{raschkas2017biopandas,\n  doi = {10.21105/joss.00279},\n  url = {http://dx.doi.org/10.21105/joss.00279},\n  year  = {2017},\n  month = {jun},\n  publisher = {The Open Journal},\n  volume = {2},\n  number = {14},\n  author = {Sebastian Raschka},\n  title = {BioPandas: Working with molecular structures in pandas DataFrames},\n  journal = {The Journal of Open Source Software}\n}\n```\n",698,bioinformatics,Python,3,Python,Shell,TeX,,,,,,,,,,,,,,,,,,,,,,,,,,82,2,77,3,26,14,40,23357,118,58,34,24,3e26557b46d7612a4b878c579487701e70bd5695,[feat] added get_model and get_models fct to mmcif (#145),2024-07-08T19:49:17Z,Kieran Didi,58345129+kierandidi@users.noreply.github.com,kierandidi,Version 0.5.1dev,,v0.5.1dev,Arian Jamasb,,a-r-j,"BSD 3-Clause ""New"" or ""Revised"" License",biopandas,BioPandas,17,molecular-structures,pandas-dataframe,pdb,mol2,protein-structure,molecules,pdb-files,drug-discovery,computational-biology,bioinformatics,molecule,,,,,,,,,,/BioPandas/biopandas,22,17,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/BioJulia/BioSequences.jl,https://github.com/BioJulia/BioSequences.jl,0,,,0,0,0,0,0,0,1,0,0,0,0,Biological sequences for the julia language,"# <img src=""./sticker.svg"" width=""30%"" align=""right"" /> BioSequences\n\n[![Latest Release](https://img.shields.io/github/release/BioJulia/BioSequences.jl.svg)](https://github.com/BioJulia/BioSequences.jl/releases/latest)\n[![MIT license](https://img.shields.io/badge/license-MIT-green.svg)](https://github.com/BioJulia/BioSequences.jl/blob/master/LICENSE)\n[![Documentation](https://img.shields.io/badge/docs-stable-blue.svg)](https://biojulia.github.io/BioSequences.jl/stable)\n[![Pkg Status](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n[![Chat](https://img.shields.io/gitter/room/BioJulia/BioSequences.svg)](https://gitter.im/BioJulia/BioSequences.jl)\n\n## Description\n\nBioSequences provides data types and methods for common operations with\nbiological sequences, including DNA, RNA, and amino acid sequences.\n\n\n## Installation\n\nYou can install BioSequences from the julia\nREPL. Press `]` to enter pkg mode, and enter the following:\n\n```julia\nadd BioSequences\n```\n\nIf you are interested in the cutting edge of the development, please check out\nthe master branch to try new features before release.\n\n\n## Testing\n\nBioSequences is tested against Julia `1.X` on Linux, OS X, and Windows.\n\n[![Unit tests](https://github.com/BioJulia/BioSequences.jl/workflows/Unit%20tests/badge.svg?branch=master)](https://github.com/BioJulia/BioSequences.jl/actions?query=workflow%3A%22Unit+tests%22+branch%3Amaster)\n[![Documentation](https://github.com/BioJulia/BioSequences.jl/workflows/Documentation/badge.svg?branch=master)](https://github.com/BioJulia/BioSequences.jl/actions?query=workflow%3ADocumentation+branch%3Amaster)\n[![](https://codecov.io/gh/BioJulia/BioSequences.jl/branch/master/graph/badge.svg)](https://codecov.io/gh/BioJulia/BioSequences.jl)\n[![Downstream](https://github.com/BioJulia/BioSequences.jl/actions/workflows/Downstream.yml/badge.svg)](https://github.com/BioJulia/BioSequences.jl/actions/workflows/Downstream.yml)\n\n\n## Contributing\n\nWe appreciate contributions from users including reporting bugs, fixing\nissues, improving performance and adding new features.\n\nTake a look at the [contributing files](https://github.com/BioJulia/Contributing)\ndetailed contributor and maintainer guidelines, and code of conduct.\n\n\n## Backers & Sponsors\n\nThank you to all our backers and sponsors!\n\n[![](https://opencollective.com/biojulia/sponsor/0/avatar.svg)](https://opencollective.com/biojulia/sponsor/0/website)\n[![](https://opencollective.com/biojulia/sponsor/1/avatar.svg)](https://opencollective.com/biojulia/sponsor/1/website)\n[![](https://opencollective.com/biojulia/sponsor/2/avatar.svg)](https://opencollective.com/biojulia/sponsor/2/website)\n[![](https://opencollective.com/biojulia/sponsor/3/avatar.svg)](https://opencollective.com/biojulia/sponsor/3/website)\n[![](https://opencollective.com/biojulia/sponsor/4/avatar.svg)](https://opencollective.com/biojulia/sponsor/4/website)\n[![](https://opencollective.com/biojulia/sponsor/5/avatar.svg)](https://opencollective.com/biojulia/sponsor/5/website)\n[![](https://opencollective.com/biojulia/sponsor/6/avatar.svg)](https://opencollective.com/biojulia/sponsor/6/website)\n[![](https://opencollective.com/biojulia/sponsor/7/avatar.svg)](https://opencollective.com/biojulia/sponsor/7/website)\n[![](https://opencollective.com/biojulia/sponsor/8/avatar.svg)](https://opencollective.com/biojulia/sponsor/8/website)\n[![](https://opencollective.com/biojulia/sponsor/9/avatar.svg)](https://opencollective.com/biojulia/sponsor/9/website)\n\n\n## Questions?\n\nIf you have a question about contributing or using BioJulia software, come\non over and chat to us on [the Julia Slack workspace](https://julialang.org/slack/), or you can try the\n[Bio category of the Julia discourse site](https://discourse.julialang.org/c/domain/bio).\n",146,bioinformatics,Julia,1,Julia,,,,,,,,,,,,,,,,,,,,,,,,,,,,209,31,171,7,8,28,288,3608,47,91,75,16,af67daf35a6e81d2dde6c4bf7894ec3351a8d99d,Add new ncbi translation code (#307),2024-06-24T14:57:40Z,Camilo García,ca.garcia2@uniandes.edu.co,camilogarciabotero,v3.1.6,## BioSequences v3.1.6\n\n[Diff since v3.1.5](https://github.com/BioJulia/BioSequences.jl/compare/v3.1.5...v3.1.6)\n\n\n**Merged pull requests:**\n- Improve heuristics of translating ambiguous nucs (#280) (@jakobnissen)\n- Decode nocheck (#281) (@jakobnissen)\n- Fix tests (#283) (@jakobnissen)\n- A few fixes for tests (#285) (@jakobnissen)\n- Bump Documenter to 1.0 (#286) (@jakobnissen)\n- Remove downstream BioMakie CI (#287) (@jakobnissen)\n- Allow translation of LongSubSeq (#288) (@jakobnissen)\n- Define compat for Random (#290) (@jakobnissen)\n\n**Closed issues:**\n- Translate subsequence  (#263),v3.1.6,,,github-actions[bot],MIT License,BioSequences.jl,BioJulia,28,biojulia,bioinformatics,biology,biological-sequences,,,,,,,,,,,,,,,,,/BioJulia/BioSequences.jl,28,15,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/biojava/biojava,https://github.com/biojava/biojava,1,framework for bio applications?? pipeline composer,,1,1,1,1,0,0,1,0,0,0,0,:book::microscope::coffee: BioJava is an open-source project dedicated to providing a Java library for processing biological data.,"# Welcome to <img src=""logo-full.png"" height=""35""/>\n\n![Build](https://github.com/biojava/biojava/actions/workflows/master.yml/badge.svg)\n[![Version](http://img.shields.io/badge/version-7.1.1-blue.svg?style=flat)](https://github.com/biojava/biojava/releases/tag/biojava-7.1.1) [![License](http://img.shields.io/badge/license-LGPL_2.1-blue.svg?style=flat)](https://github.com/biojava/biojava/blob/master/LICENSE) [![Join the chat at https://gitter.im/biojava/biojava](https://badges.gitter.im/biojava/biojava.svg)](https://gitter.im/biojava/biojava?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n\nBioJava is an open-source project dedicated to providing a Java framework for **processing biological data**. It provides analytical and statistical routines, parsers for common file formats, reference implementations of popular algorithms, and allows the manipulation of sequences and 3D structures. The goal of the biojava project is to facilitate rapid application development for bioinformatics.\n\nPlease visit our [homepage](http://biojava.org/).\n\n### Documentation\n\nThe [BioJava tutorial](https://github.com/biojava/biojava-tutorial) is a great place to get started. It is most complete for the biojava-structure module. \n\nThe [BioJava Cookbook](http://biojava.org/wiki/BioJava:CookBook4.0/) contains an older and slightly outdated collection of simple examples that teach the basics for how to work with BioJava.\n\nFull javadocs are available at the [BioJava website](http://biojava.org/docs/api).\n\n### Maven Repository\n\nBioJava release are available from Maven Central.\n\n### Quick Installation\n\nIf you are using Maven you can add the BioJava repository by adding the following XML to your project pom.xml file:\n\n```xml\n    <dependencies>\n      <dependency>\n        <groupId>org.biojava</groupId>\n        <artifactId>biojava-core</artifactId>\n        <version>7.1.1</version>\n      </dependency>\n      <!-- other biojava modules as needed -->\n    </dependencies>\n```\n\n### Mailing Lists\n\nBioJava has one main mailing list. In order to avoid SPAM the list only accepts postings from list members. Anybody can become a list member, so please subscribe before you post. If you send without being subscribed your mail might get stuck in the moderation loop, which can cause several weeks of delay (no fun to read through all that spam).\n\n#### biojava-l general discussion list\n\n* [biojava-l@biojava.org](http://lists.open-bio.org/mailman/listinfo/biojava-l)\n\nThis list is intended for general discussion, advice, questions, offers of help, announcements, expressions of appreciation, bugs found in release code and requests for features.\n\n#### biojava-dev developers list\n \nA [dev mailing list](http://lists.open-bio.org/mailman/listinfo/biojava-dev) used to exist, but it has now been shut down. For dev discussions we now use github issues. Please search existing issues and if you don't find the answer to your question submit a new issue.\n\n### Please cite\n\n**BioJava 5: A community driven open-source bioinformatics library**<br/>\n*Aleix Lafita, Spencer Bliven, Andreas Prlić, Dmytro Guzenko, Peter W. Rose, Anthony Bradley, Paolo Pavan, Douglas Myers-Turnbull, Yana Valasatava, Michael Heuer, Matt Larson, Stephen K. Burley, Jose M. Duarte* <br/>\n[PLOS Computational Biology 15(2): e1006791](http://dx.plos.org/10.1371/journal.pcbi.1006791) <br/>\n[![doi](http://img.shields.io/badge/doi-10.1371%2Fjournal.pcbi.1006791-blue.svg?style=flat)](https://doi.org/10.1371/journal.pcbi.1006791)\n",585,bioinformatics,Java,3,Java,Shell,Python,,,,,,,,,,,,,,,,,,,,,,,,,,645,78,566,1,16,88,0,50605,382,451,384,67,62b0426ebe278620ac2d18fe7327ac303790c732,Merge pull request #1095 from jlerbsc/s1319,2024-06-17T16:00:02Z,Jose Manuel Duarte,jose.m.duarte@gmail.com,josemduarte,BioJava 7.1.1,"### Fixed\r\n* Now mmCIF files that have no author fields in atom_site can be read (e.g. from PyMol or ESMAtlas) #775 #1083\r\n* No evaluations of arguments in debug level log statements #1086 #789\r\n\r\n### Removed\r\n* Minor removal from biojava-core: FileDownloadUtils::copy, replaced by Files::copy\r\n",biojava-7.1.1,Jose Manuel Duarte,,josemduarte,GNU Lesser General Public License v2.1,biojava,biojava,34,bioinformatics,java,protein-structure,genomics,protein-sequences,protein-modification,sequence-alignment,structure-alignment,protein-data-bank,pdb,parser-library,structural-bioinformatics,,,,,,,,,/biojava/biojava,82,79,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/bioinformatics-centre/kaiju,https://github.com/bioinformatics-centre/kaiju,0,,,0,0,0,0,0,0,1,0,0,0,0,Fast taxonomic classification of metagenomic sequencing reads using a protein reference database,"# Kaiju\n\n[![CI](https://github.com/bioinformatics-centre/kaiju/actions/workflows/ci.yaml/badge.svg)](https://github.com/bioinformatics-centre/kaiju/actions/workflows/ci.yaml)\n[![Kaiju](https://img.shields.io/conda/dn/bioconda/kaiju.svg?label=Bioconda )](https://anaconda.org/bioconda/kaiju)\n\nKaiju is a program for the taxonomic classification of high-throughput sequencing\nreads, e.g., Illumina or Roche/454, from whole-genome sequencing of\nmetagenomic DNA. Reads are directly assigned to taxa using the NCBI taxonomy and a\nreference database of protein sequences from microbial and viral genomes.\n\nThe program is described in [Menzel, P. et al. (2016) Fast and sensitive taxonomic classification for metagenomics with Kaiju. *Nat. Commun.* 7:11257](http://www.nature.com/ncomms/2016/160413/ncomms11257/full/ncomms11257.html) (open access).\n\nKaiju can be installed locally (see below) or used via a [web server](http://kaiju.binf.ku.dk/).\n\nSee the release notes for all releases [here](https://bioinformatics-centre.github.io/kaiju/index.html#releases).\n\n\n### Authors\nPeter Menzel <pmenzel@gmail.com>   \nAnders Krogh <krogh@binf.ku.dk>   \n\n\n### License\n\nCopyright (c) 2015-2024 Peter Menzel and Anders Krogh\n\nKaiju is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nKaiju is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  \nSee the file LICENSE for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with the source code.  If not, see <http://www.gnu.org/licenses/>.\n\n\n## Installation\n### Compiling Kaiju from source\nKaiju's source code can be downloaded directly from GitHub either as a\n[compressed archive](https://github.com/bioinformatics-centre/kaiju/archive/master.tar.gz) or\nusing the git command line client:\n```\ngit clone https://github.com/bioinformatics-centre/kaiju.git\n```\nThis will create the directory `kaiju` in the current directory.\n\nKaiju is written in C/C++11 for Linux. It uses the zlib library for reading gzip-compressed files.\nIf not already installed, it is necessary to install the zlib development library, e.g. on Ubuntu using:\n```\nsudo apt install libz-dev\n```\n\nFor compiling Kaiju and its associated programs, type:\n```\ncd kaiju/src\nmake\n```\n\nAfter compilation, Kaiju's executable files are available in the `kaiju/bin` directory.\nYou can add this directory to your shell's `$PATH` variable or copy all files from `kaiju/bin` to a directory in your `$PATH`.\n\n### Installation via Bioconda\n\nKaiju is also available via the `bioconda` channel and can be installed via\n```\nconda install -c bioconda kaiju\n# or\nmamba install -c bioconda kaiju\n```\n\n## Creating the Kaiju index\n\nBefore classification of reads, Kaiju's database index needs to be built from\nthe reference protein database.  You can either create a local index based on\nthe currently available reference databases, or [download a pre-built index](https://bioinformatics-centre.github.io/kaiju/downloads.html).\n\nFor creating a local index, the program `kaiju-makedb` in the `bin/` directory\nwill download a source database and the taxonomy files from the NCBI FTP server,\nconvert them into a protein database and construct Kaiju's index (the\nBurrows-Wheeler transform and the FM-index) in one go.\n\n`kaiju-makedb` needs `curl` and `wget` for downloading the reference databases.\n\nThe downloaded files can be very large, depending on the selected reference database.\nIt is therefore recommended to run `kaiju-makedb` in a directory with at least 500 GB of free space.\n\nExample usage:\n```\nmkdir kaijudb\ncd kaijudb\nkaiju-makedb -s <DB>\n```\nThe table below lists the available source databases.\nUse the database name shown in the first column as argument to option `-s` in `kaiju-makedb`.\nThe last column denotes the required memory for running Kaiju with the\nrespective index and for creating the index (in brackets).\n\n| Index name | Description | Sequences<sup>\*</sup> | RAM in GB (makedb)<sup>\*</sup> |\n| --- | --- | --- | --- |\n| `refseq` | Completely assembled and annotated reference genomes of Archaea, Bacteria, and viruses from the NCBI RefSeq database. |  127 M |  87 (112) |\n| `refseq_nr` | Sequences for Archaea, Bacteria, viruses and microbial eukaryotes from the [NCBI RefSeq non-redundant protein collection](https://www.ncbi.nlm.nih.gov/refseq/about/nonredundantproteins/). |  210 M |  116 (194) |\n| `refseq_ref` | Protein sequences from representative assemblies of Archaea and bacteria from NCBI RefSeq plus viruses from NCBI RefSeq. |  69 M |  49 (63) |\n| `progenomes` |  Representative set of genomes from the [proGenomes](http://progenomes.embl.de/) database and viruses from the NCBI RefSeq database. |  141 M | 102 (120) |\n| `viruses` |  Only viruses from the NCBI RefSeq database. | 0.65 M | 0.5  (0.5) |\n| `plasmids` |  Plasmid sequences from the NCBI RefSeq database. |  5.6 M | 4 (5) |\n| `fungi` |  [Fungi](ftp://ftp.ncbi.nlm.nih.gov/genomes/refseq/fungi) sequences from the NCBI RefSeq database. |  5.2 M | 5 (6.5) |\n| `nr` | Subset of NCBI BLAST _nr_ database containing all proteins belonging to Archaea, bacteria and viruses. |  283 M | 177 (308) |\n| `nr_euk` | Like option `-s nr` and additionally include proteins from fungi and microbial eukaryotes, see taxon list in `bin/kaiju-taxonlistEuk.tsv`. | 321 M | 204 (354) |\n| `rvdb` | Protein sequences from [RVDB-prot](https://rvdb-prot.pasteur.fr/) |  34 M | 75 (215) |\n\n\* as of Mai 2023.\n\nPre-built indexes for each reference database can be [downloaded](https://bioinformatics-centre.github.io/kaiju/downloads.html).\n\nBy default, `kaiju-makedb` uses 5 parallel threads for constructing the index, which can\nbe changed by using the option `-t`. Note that a higher number of threads\nincreases the memory usage during index construction, while reducing the number\nof threads decreases memory usage.\n\nAfter `kaiju-makedb` is finished, only the files `kaiju_db_*.fmi`, `nodes.dmp`,\nand `names.dmp` are needed to run Kaiju.\n\n### Custom database\nIt is also possible to make a custom database from a collection of protein sequences.\nThe format needs to be a FASTA file in which the headers are the numeric NCBI taxon identifiers of the protein sequences,\nwhich can optionally be prefixed by another identifier (e.g. a counter) followed by an underscore, for example:\n```\n>1_1358\nMAQQRRGGFKRRKKVDFIAANKIEVVDYKDTELLKRFISERGKILPRRVTGTSAKNQRKVVNAIKRARVMALLPFVAEDQN\n>2_44689\nMASTQNIVEEVQKMLDTYDTNKDGEITKAEAVEYFKGKKAFNPERSAIYLFQVYDKDNDGKITIKELAGDIDFDKALKEYKEKQAKSKQQEAEVEEDIEAFILRHNKDDNTDITKDELIQGFKETGAKDPEKSANFILTEMDTNKDGTITVKELRVYYQKVQKLLNPDQ\n>3_352472\nMKTKSSNNIKKIYYISSILVGIYLCWQIIIQIIFLMDNSIAILEAIGMVVFISVYSLAVAINGWILVGRMKKSSKKAQYEDFYKKMILKSKILLSTIIIVIIVVVVQDIVINFILPQNPQPYVYMIISNFIVGIADSFQMIMVIFVMGELSFKNYFKFKRIEKQKNHIVIGGSSLNSLPVSLPTVKSNESNESNTISINSENNNSKVSTDDTINNVM\n>4_91061\nMTNPFENDNYTYKVLKNEEGQYSLWPAFLDVPIGWNVVHKEASRNDCLQYVENNWEDLNPKSNQVGKKILVGKR\n...\n```\nThe taxon identifiers must be contained in the [NCBI taxonomy files](ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump.tar.gz) nodes.dmp and names.dmp.\nThen, Kaiju's index is created using the programs `kaiju-mkbwt` and `kaiju-mkfmi`. For example, if the database FASTA file is called `proteins.faa`, then run:\n```\nkaiju-mkbwt -n 5 -a ACDEFGHIKLMNPQRSTVWY -o proteins proteins.faa\nkaiju-mkfmi proteins\n```\nwhich creates the file proteins.fmi that is used by Kaiju.\nNote that the protein sequences may only contain the uppercase characters of the standard 20 amino acids, all other\ncharacters need to be removed.\n\n## Running Kaiju\nKaiju requires at least three arguments:\n```\nkaiju -t nodes.dmp -f kaiju_db_*.fmi -i inputfile.fastq\n```\nReplace `kaiju_db_*.fmi` by the actual `.fmi` file depending on the selected database.\nFor example, when running `kaiju-makedb -s refseq`, the corresponding index file is `refseq/kaiju_db_refseq.fmi`.\n\nFor paired-end reads use `-i firstfile.fastq` and `-j secondfile.fastq`.\n\nThe reads must be in the same order in both files. Kaiju will strip suffixes\nfrom the read names by deleting all characters after a `/` or space.  The read\nnames are then compared between the first and second file and an error is\nissued if they are not identical.\n\nKaiju can read input files in FASTQ and FASTA format, which may also be gzip-compressed.\n\nBy default, Kaiju will print the output to the terminal (STDOUT).\nThe output can also be written to a file using the `-o` option:\n```\nkaiju -t nodes.dmp -f kaiju_db.fmi -i inputfile.fastq -o kaiju.out\n```\n\nKaiju can use multiple parallel threads, which can be specified with the `-z` option, e.g. for using 25 parallel threads:\n```\nkaiju -z 25 -t nodes.dmp -f kaiju_db.fmi -i inputfile.fastq -o kaiju.out\n```\n\n### kaiju-multi\nWhile `kaiju` can only process one input, `kaiju-multi` can take a comma-separated list of input files (and optionally output files) for processing multiple samples at once:\n```\nkaiju-multi -z 25 -t nodes.dmp -f kaiju_db.fmi -i sample1_R1.fastq,sample2_R1.fastq,sample3_R1.fastq -j sample1_R2.fastq,sample2_R2.fastq,sample3_R2.fastq  -o sample1.out,sample2.out,sample3.out\n```\nThese lists must have the same length.\nIt's also possible to merge all outputs into one file using output redirection:\n```\nkaiju-multi -z 25 -t nodes.dmp -f kaiju_db.fmi -i sample1_R1.fastq,sample2_R1.fastq,sample3_R1.fastq -j sample1_R2.fastq,sample2_R2.fastq,sample3_R2.fastq > all_samples.out\n```\n\n### Run modes\nThe default run mode is **Greedy** with three allowed mismatches.\nThe number of allowed mismatches can be changed using option `-e`.\n\nIn Greedy mode, matches are filtered by a minimum length and score, but also by their E-value (similar to blastp), which can be adjusted with the option `-E`. The default value is 0.01.\nThe cutoffs for minimum required match length and match score can be changed using the options `-m` (default: 11) and `-s` (default: 65).\n\nThe run mode can be changed to **MEM** using option `-a`:\n```\nkaiju -t nodes.dmp -f kaiju_db.fmi -i inputfile.fastq -a mem\n```\n\nIf the input sequences are already protein sequences, use option `-p` to disable translation of the input.\n\nOption `-x` enables filtering of query sequences containing\nlow-complexity regions by using the SEG algorithm from the blast+ package.\nIt is enabled by default and can be disabled by the `-X` option.  SEG filtering\nis always recommended in order to avoid false positive taxon assignments that\nare caused by spurious matches due to simple repeat patterns or other\nsequencing noise.\n\n### Output format\nKaiju will print one line for each read or read pair.\nThe default output format contains three columns separated by tabs.\nUsing the option `-v` enables the verbose output, which will print additional columns:\n\n1. either C or U, indicating whether the read is classified or unclassified.\n2. name of the read\n3. NCBI taxon identifier of the assigned taxon\n4. the length or score of the best match used for classification\n5. the taxon identifiers of all database sequences with the best match\n6. the accession numbers of all database sequences with the best match\n7. matching fragment sequence(s)\n\nNB: Since the _nr_ database aggregates multiple genes of identical sequences, only the first accession number\nfor each sequence in the __nr__ source file is kept in Kaiju's database and therefore also in the output file.\n\nThe number of taxon identifiers (column 5) and accession numbers (column 5) is limited to 20 entries each in\norder to reduce large outputs produced by highly abundant protein sequences in _nr_, e.g. from HIV.\n\n## Classification accuracy\n\nThe accuracy of the classification depends both on the choice of the reference\ndatabase and the chosen options when running Kaiju. These choices also affect\nthe speed and memory usage of Kaiju.\n\nFor highest sensitivity, it is recommended to use the _nr_ database (+eukaryotes)\nas a reference database because it is the most comprehensive set of protein\nsequences. Alternatively, use proGenomes over Refseq for increased sensitivity.\n\nGreedy run mode yields a higher sensitivity compared with MEM mode.\n\nFor fastest classification, use MEM mode and multiple parallel threads\n(`-z`); and for lowest memory usage use the proGenomes reference\ndatabase. The number of parallel threads has only little impact on memory usage.\n\nFurther, the choice of the minimum required match length (`-m`) in MEM mode or\nmatch score (`-s`) in Greedy mode governs the trade-off between sensitivity and\nprecision of the classification. Please refer to the paper for a discussion on\nthis topic.\n\n## Helper programs\n### Creating input file for Krona\nThe program `kaiju2krona` can be used to convert Kaiju's tab-separated output file\ninto a tab-separated text file, which can be imported into [Krona](https://github.com/marbl/Krona/wiki/KronaTools). It requires the `nodes.dmp`\nand `names.dmp` files from the NCBI taxonomy for mapping the taxon identifiers from Kaiju's\noutput to the corresponding taxon names.\n```\nkaiju2krona -t nodes.dmp -n names.dmp -i kaiju.out -o kaiju.out.krona\n```\nThe file `kaiju.out.krona` can then be imported into Krona and converted into an HTML file using\nKrona's `ktImportText` program:\n```\nktImportText -o kaiju.out.html kaiju.out.krona\n```\n\n### Creating classification summary\nThe program `kaiju2table` converts Kaiju's output file(s) into a\nsummary table for a given taxonomic rank, e.g., genus. It requires the\n`nodes.dmp` and `names.dmp` files for mapping the taxon identifiers from the third column in the\nKaiju output to the corresponding taxon names.\n\nBasic usage:\n```\nkaiju2table -t nodes.dmp -n names.dmp -r genus -o kaiju_summary.tsv kaiju.out [kaiju2.out, ...]\n```\nThe program can also filter out taxa with low abundances, e.g. for only showing genera that\ncomprise at least 1 percent of the total reads:\n```\nkaiju2table -t nodes.dmp -n names.dmp -r genus -m 1.0 -o kaiju_summary.tsv kaiju.out [kaiju2.out, ...]\n```\nSimilarly, option `-c` can be used to specify the threshold by absolute read count.\n\nOption `-u` disables counting unclassified reads towards the total number of reads when calculating percentages.\n\nOption `-p` will print the full taxon path instead of just the taxon name.\n\nInstead of printing the full taxon path, option `-l` can be used to specify the\nranks to be printed by supplying a comma-separated list, for example:\n`-l superkingdom,phylum,class,order,family,genus,species`.\n\n### Adding taxa names to output file\nThe program `kaiju-addTaxonNames` appends the name that corresponds to the taxon id in\nKaiju's output file as an additional last column to the output.\n```\nkaiju-addTaxonNames -t nodes.dmp -n names.dmp -i kaiju.out -o kaiju.names.out\n```\nOption `-u` will omit unclassified reads.  \nOption `-p` will print the full taxon path instead of just the taxon name.  \nOption `-r` will print the path containing only to the specified ranks. For example,\n`-r phylum,genus` will append the names of phylum and genus to the end of each line.\n\n### Merging outputs\nThe program `kaiju-mergeOutputs` can merge two tab-separated output files in the\ncolumn format (see above) used by Kaiju and Kraken. Only the first three columns are used.\n\nThe files need to be sorted by the read name in the second column, for example by:\n```\nsort -k2,2 kaiju.out  >kaiju.out.sort\nsort -k2,2 kraken.out >kraken.out.sort\n```\nThen both files can be merged:\n```\nkaiju-mergeOutputs -i kaiju.out.sort -j kraken.out.sort -o combined.out -v\n```\nThe shell's process substitution can be used for sorting without creating intermediate files:\n```\nkaiju-mergeOutputs -i <(sort -k2,2 kaiju.out) -j <(sort -k2,2 kraken.out) -o combined.out -v\n```\nThe output file will be in the same column format as the input files (but only\ncontain the first three columns) and it will have the same length as the input\nfiles (which also have to be of same length).  In the case of conflicting taxon\nidentifiers for a classified read in both input files, `kaiju-mergeOutputs` will use the identifier found in the\nfirst input file (specified by `-i`).  This behavior can be changed using the\n`-c` option, which can take four possible values:\n- `1`: use taxon identifier from the first input file.\n- `2`: use taxon identifier from the second input file.\n- `lca`: use the least common ancestor of the taxon identifiers from both files (default).\n- `lowest`: use the lowest ranking of the two taxon identifiers if they are within the same lineage. Otherwise use the LCA.\n\nOptions `lca` and `lowest` require the path to the file `nodes.dmp` by using the `-t` option.\n\nWhen the two tab-separated output files contain the classification score in the 4th column (by running `kaiju -v`), then option `-s` can be used to give precedence to the classification result with the higher score.\n\n### KaijuX and KaijuP\n\nThe programs `kaijux` and `kaijup` can be used for finding the best matching\ndatabase sequence for each query sequence without taxonomic classification,\ni.e., they will just print the name of the database sequence. Thus, both\nprograms do not use the `nodes.dmp` file containing the taxonomy, but only need\nthe `.fmi` database file. While `kaijux` takes nucleotide sequences as input\nand translates them into the six reading frames like standard `kaiju`,\n`kaijup` takes protein sequences as input, which are directly searched in the\ndatabase.  All other parameters remain the same as in standard `kaiju`. In case\nof paired-end reads, both mates are searched independently.\n\nTo build an index for a custom database, all sequences need to be in a single\nFASTA file and may only contain the 20 letters from the standard protein\nalphabet `ACDEFGHIKLMNPQRSTVWY`.\n\nFor example, building the index (the Burrows-Wheeler transform and FM-index) from the\nfile with the protein sequences `proteins.faa` is done in two steps by the\nprograms `kaiju-mkbwt` and `kaiju-mkfmi`:\n```\nkaiju-mkbwt -n 5 -a ACDEFGHIKLMNPQRSTVWY -o proteins proteins.faa\nkaiju-mkfmi proteins\n```\nThis will create two intermediate files `proteins.bwt` and `proteins.sa`, and finally\nthe file `proteins.fmi`, which is used by Kaiju.\n\nThe option `-n` for `kaiju-mkbwt` specifies the number of parallel threads. The more\nthreads are used, the higher the memory consumption becomes.  The option `-e`\nfor `kaiju-mkbwt` specifies the exponent of the suffix array checkpoint distances and\ntherefore determines the trade-off between the size of the suffix array and the\nspeed of the search. The default value is 5.\n\n",258,bioinformatics,C,5,C++,Makefile,C,Perl,Shell,,,,,,,,,,,,,,,,,,,,,,,,17,3,13,1,2,8,20,991,68,266,218,48,55a0a14f454f86f09df6d424e39847d9ddc4ab7e,bump version to 1.10.1,2024-03-03T20:10:52Z,Peter Menzel,pmenzel@gmail.com,pmenzel,version 1.10.1,see [release notes](https://bioinformatics-centre.github.io/kaiju/index.html#releases),v1.10.1,Peter Menzel,,pmenzel,GNU General Public License v3.0,kaiju,bioinformatics-centre,30,metagenomics,next-generation-sequencing,bioinformatics,taxonomic-classification,taxonomy,,,,,,,,,,,,,,,,/bioinformatics-centre/kaiju,30,14,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/bioinform/somaticseq,https://github.com/bioinform/somaticseq,1,,,1,1,1,1,0,0,0,0,0,0,1,An ensemble approach to accurately detect somatic mutations using SomaticSeq,"# SomaticSeq\n\nSomaticSeq is an ensemble somatic SNV/indel caller that has the ability to use\nmachine learning to filter out false positives from other callers. The detailed\ndocumentation is located in [docs/Manual.pdf](docs/Manual.pdf ""User Manual"").\n\n-   It was published in\n    [Fang, L.T., Afshar, P.T., Chhibber, A. _et al_. An ensemble approach to accurately detect somatic mutations using SomaticSeq. _Genome Biol_ **16**, 197 (2015)](http://dx.doi.org/10.1186/s13059-015-0758-2 ""Fang LT, et al. Genome Biol (2015)"").\n-   Feel free to report issues and/or ask questions at the\n    [Issues](../../issues ""Issues"") page.\n\n## Training data for benchmarking and/or model building\n\nIn 2021, the\n[FDA-led MAQC-IV/SEQC2 Consortium](https://www.fda.gov/science-research/bioinformatics-tools/microarraysequencing-quality-control-maqcseqc#MAQC_IV)\nhas produced multi-center multi-platform whole-genome and whole-exome\n[sequencing data sets](https://identifiers.org/ncbi/insdc.sra:SRP162370) for a\npair of tumor-normal reference samples (HCC1395 and HCC1395BL), along with the\nhigh-confidence\n[somatic mutation call set](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/seqc/Somatic_Mutation_WG/release/latest/).\nThis work was published in\n[Fang, L.T., Zhu, B., Zhao, Y. _et al_. Establishing community reference samples, data and call sets for benchmarking cancer mutation detection using whole-genome sequencing. _Nat Biotechnol_ **39**, 1151-1160 (2021)](https://doi.org/10.1038/s41587-021-00993-6 ""Fang LT, et al. Nat Biotechnol (2021)"")\n/\n[PMID:34504347](http://identifiers.org/pubmed:34504347 ""Fang LT, et al. Nat Biotechnol (2021)"")\n/\n[Free Read-Only Link](https://bit.ly/2021nbt ""Fang LT, et al. Nat Biotechnol (2021)"").\nThe following are some of the use cases for these resources:\n\n-   Use high-confidence call set as the ""ground truth"" to investigate how\n    different sample preparations, sequencing library kits, and bioinformatic\n    algorithms affect the accuracy of the somatic mutation pipelines, and\n    develop best practices, e.g.,\n    [Xiao W. _et al_. Nat Biotechnol 2021](https://doi.org/10.1038/s41587-021-00994-5).\n-   Use high-confidence call set as the ""ground truth"" to build accurate and\n    robust machine learning models for somatic mutation detections, e.g.,\n    [Sahraeian S.M.E. _et al_. Genome Biol 2022](https://doi.org/10.1186/s13059-021-02592-9)\n\n#### Click for [more details of the SEQC2's somatic mutation project](docs/seqc2.md).\n\n#### [Recommendation](docs/train_for_classifiers.md) of how to use SEQC2 data to create SomaticSeq classifiers.\n\n<hr>\n<table style=""width: 100%;"">\n\n  <tr>\n    <td>Briefly explaining SomaticSeq v1.0</td>\n    <td>SEQC2 somatic mutation reference data and call sets</td>\n    <td>How to run <a href=""https://precision.fda.gov/home/apps/app-G7XVKQQ02v051q5PK3yQYJKJ-1"">SomaticSeq v3.6.3</a> on precisionFDA</td>\n\n  </tr>\n\n  <tr>\n    <td><a href=""https://youtu.be/MnJdTQWWN6w""><img src=""docs/SomaticSeqYoutube.png"" width=""400"" /></a></td>\n    <td><a href=""https://youtu.be/nn0BOAONRe8""><img src=""docs/workflow400.png"" width=""400"" /></a></td>\n    <td><a href=""https://youtu.be/fLKokuMGTvk""><img src=""docs/precisionfda.png"" width=""400"" /></a></td>\n\n  </tr>\n\n  <tr>\n    <td></td>\n    <td></td>\n    <td>Run in <a href=""https://youtu.be/F6TSdg0OffM"">train or prediction mode</a></td>\n\n  </tr>\n\n</table>\n<hr>\n\n# Installation\n\n## Dependencies\n\nThis [dockerfile](Dockerfiles/somaticseq.base-1.6.dockerfile) reveals the\ndependencies\n\n-   Python 3, plus pysam, numpy, scipy, pandas, and xgboost libraries.\n-   [BEDTools](https://bedtools.readthedocs.io/en/latest/): required when\n    parallel processing is invoked, and/or when any bed files are used as input\n    files.\n-   Optional: dbSNP VCF file (if you want to use dbSNP membership as a feature).\n-   Optional: R and [ada](https://cran.r-project.org/package=ada) are required\n    for AdaBoost, whereas XGBoost is implemented in python.\n-   To install SomaticSeq, clone this repo, `cd somaticseq`, and then run\n    `pip install .` or `./setup.py install`.\n\n## To install using pip\n\nMake sure to install `bedtools` separately.\n\n```\npip install somaticseq\n```\n\n## To install the bioconda version\n\nSomaticSeq can also be found on\n[![Anaconda-Server Badge](https://anaconda.org/bioconda/somaticseq/badges/version.svg)](https://anaconda.org/bioconda/somaticseq).\nTo\n[![install with bioconda](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?style=flat)](http://bioconda.github.io/recipes/somaticseq/README.html),\nwhich also automatically installs a bunch of 3rd-party somatic mutation callers:\n\n```\nconda install -c bioconda somaticseq\n```\n\n## To install from github source with conda\n\n```\nconda create --name my_env -c bioconda python bedtools\nconda activate my_env\ngit clone git@github.com:bioinform/somaticseq.git\ncd somaticseq\npip install -e .\n```\n\n### Test your installation\n\nThere are some toy data sets and test scripts in [**example**](example) that\nshould finish in <1 minute if installed properly.\n\n## Run SomaticSeq with an example command\n\n-   At minimum, given the results of the individual mutation caller(s),\n    SomaticSeq will extract sequencing features for the combined call set.\n    Required inputs are\n\n    -   `--output-directory` and `--genome-reference`, then\n    -   Either `paired` or `single` to invoke paired or single sample mode,\n        -   if `paired`: `--tumor-bam-file`, and `--normal-bam-file` are both\n            required.\n        -   if `single`: `--bam-file` is required.\n\n    Everything else is optional (though without a single VCF file from at least\n    one caller, SomaticSeq does nothing).\n\n-   The following four files will be created into the output directory:\n\n    -   `Consensus.sSNV.vcf`, `Consensus.sINDEL.vcf`, `Ensemble.sSNV.tsv`, and\n        `Ensemble.sINDEL.tsv`.\n\n-   If you're searching for pipelines to run those individual somatic mutation\n    callers, feel free to take advantage of our\n    [**Dockerized Somatic Mutation Workflow**](somaticseq/utilities/dockered_pipelines)\n    as a start.\n    -   Important note: multi-argument options (e.g., `--extra-hyperparameters`\n        or `--features-excluded`) cannot be placed immediately before `paired`\n        or `single`, because those options would try to ""grab"" `paired` or\n        `single` as an additional argument.\n\n```\n# Merge caller results and extract SomaticSeq features\nsomaticseq_parallel.py \\n  --output-directory  $OUTPUT_DIR \\n  --genome-reference  GRCh38.fa \\n  --inclusion-region  genome.bed \\n  --exclusion-region  blacklist.bed \\n  --threads           24 \\npaired \\n  --tumor-bam-file    tumor.bam \\n  --normal-bam-file   matched_normal.bam \\n  --mutect2-vcf       MuTect2/variants.vcf \\n  --varscan-snv       VarScan2/variants.snp.vcf \\n  --varscan-indel     VarScan2/variants.indel.vcf \\n  --jsm-vcf           JointSNVMix2/variants.snp.vcf \\n  --somaticsniper-vcf SomaticSniper/variants.snp.vcf \\n  --vardict-vcf       VarDict/variants.vcf \\n  --muse-vcf          MuSE/variants.snp.vcf \\n  --lofreq-snv        LoFreq/variants.snp.vcf \\n  --lofreq-indel      LoFreq/variants.indel.vcf \\n  --scalpel-vcf       Scalpel/variants.indel.vcf \\n  --strelka-snv       Strelka/variants.snv.vcf \\n  --strelka-indel     Strelka/variants.indel.vcf \\n  --arbitrary-snvs    additional_snv_calls_1.vcf.gz additional_snv_calls_2.vcf.gz ... \\n  --arbitrary-indels  additional_indel_calls_1.vcf.gz additional_indel_calls_2.vcf.gz ...\n```\n\n-   For all of those input VCF files, both `.vcf` and `.vcf.gz` are acceptable.\n    SomaticSeq also accepts `.cram`, but some callers may only take `.bam`.\n\n-   `--arbitrary-snvs` and `--arbitrary-indels` are added since v3.7.0. It\n    allows users to input **any** arbitrary VCF file(s) from caller(s) that we\n    did not explicitly incorporate. SNVs and indels have to be separated.\n\n    -   If your caller puts SNVs and indels in the same output VCF file, you may\n        split it using a SomaticSeq utility script, e.g.,\n        `splitVcf.py -infile small_variants.vcf -snv snvs.vcf -indel indels.vcf`.\n        As usual, input can be either `.vcf` or `.vcf.gz`, but output will be\n        `.vcf`.\n    -   For those VCF file(s), any calls **not** labeled REJECT or LowQual will\n        be considered a bona fide somatic mutation call. REJECT calls will be\n        skipped. LowQual calls will be considered, but will not have a value of\n        `1` in `if_Caller` machine learning feature.\n\n-   `--inclusion-region` or `--exclusion-region` will require `bedtools` in your\n    path.\n\n-   `--algorithm` defaults to `xgboost` as v3.6.0, but can also be `ada`\n    (AdaBoost in R). XGBoost supports multi-threading and can be orders of\n    magnitude faster than AdaBoost, and seems to be about the same in terms of\n    accuracy, so we changed the default from `ada` to `xgboost` as v3.6.0 and\n    that's what we recommend now.\n\n-   To split the job into multiple threads, place `--threads X` before the\n    `paired` option to indicate X threads. It simply creates multiple BED file\n    (each consisting of 1/X of total base pairs) for SomaticSeq to run on each\n    of those sub-BED files in parallel. It then merges the results. This\n    requires `bedtools` in your path.\n\nAdditional parameters to be specified **before** `paired` option to invoke\ntraining mode. In addition to the four files specified above, two classifiers\n(SNV and indel) will be created..\n\n-   `--somaticseq-train`: FLAG to invoke training mode with no argument, which\n    also requires ground truth VCF files.\n    -   `--extra-hyperparameters`: add hyperparameters for xgboost, e.g.,\n        `--extra-hyperparameters scale_pos_weight:0.1 grow_policy:lossguide max_leaves:12`.\n-   `--truth-snv`: if you have a ground truth VCF file for SNV\n-   `--truth-indel`: if you have a ground truth VCF file for INDEL\n\nAdditional input files to be specified **before** `paired` option invoke\nprediction mode (to use classifiers to score variants). Four additional files\nwill be created, i.e., `SSeq.Classified.sSNV.vcf`, `SSeq.Classified.sSNV.tsv`,\n`SSeq.Classified.sINDEL.vcf`, and `SSeq.Classified.sINDEL.tsv`.\n\n-   `--classifier-snv`: classifier previously built for SNV\n-   `--classifier-indel`: classifier previously built for INDEL\n\nWithout those paramters above to invoking training or prediction mode,\nSomaticSeq will default to majority-vote consensus mode.\n\nDo not worry if Python throws the following warning. This occurs when SciPy\nattempts a statistical test with empty data, e.g., z-scores between reference-\nand variant-supporting reads will be `nan` if there is no reference read at a\nposition.\n\n```\n  RuntimeWarning: invalid value encountered in double_scalars\n  z = (s - expected) / np.sqrt(n1*n2*(n1+n2+1)/12.0)\n```\n\n## To train for SomaticSeq classifiers with multiple data sets\n\nRun `somatic_xgboost.py train --help` to see the options, e.g.,\n\n```\nsomatic_xgboost.py train \\n  -tsvs SAMPLE_1/Ensemble.sSNV.tsv SAMPLE_2/Ensemble.sSNV.tsv ... SAMPLE_N/Ensemble.sSNV.tsv \\n  -out multiSample.SNV.classifier \\n  -threads 8 -depth 12 -seed 42 -method hist -iter 250 \\n  --extra-params scale_pos_weight:0.1 grow_policy:lossguide max_leaves:12\n```\n\n## Run SomaticSeq modules seperately\n\nMost SomaticSeq modules can be run on their own. They may be useful in debugging\ncontext, or be run for your own purposes. See [this page](MODULES.md) for your\noptions.\n\n## Dockerized workflows and pipelines\n\n### To run somatic mutation callers and then SomaticSeq\n\nWe have created a module (i.e., `makeSomaticScripts.py`) that can run all the\ndockerized somatic mutation callers and then SomaticSeq, described at\n[**somaticseq/utilities/dockered_pipelines**](somaticseq/utilities/dockered_pipelines).\nThere is also an alignment workflow described there. You need\n[docker](https://www.docker.com/) to run these workflows. Singularity is also\nsupported, but is not optimized. Let me know if you find bugs.\n\n### To create training data to create SomaticSeq classifiers\n\n-   I recommend [SEQC2 Somatic Mutation Working Group](docs/seqc2.md)'s\n    [reference sequencing data](https://identifiers.org/ncbi/insdc.sra:SRP162370)\n    and\n    [high-confidence somatic mutation call sets](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/seqc/Somatic_Mutation_WG/release/latest/).\n\n-   Before well characterized real data was available, we have dockerized\n    pipelines for _in silico_ mutation spike in at\n    [**somaticseq/utilities/dockered_pipelines/bamSimulator**](somaticseq/utilities/dockered_pipelines/bamSimulator).\n    These pipelines are based on\n    [BAMSurgeon](https://github.com/adamewing/bamsurgeon). We have used it to\n    create training set to build SomaticSeq classifiers, though it has not been\n    updated for a while.\n\n-   Combine both BAMSurgeon _in silico_ spike in and the real SEQC2 training\n    data **may** give you better model than using either, which was shown in\n    [Sahraeian S.M.E. _et al_. 2022](https://doi.org/10.1186/s13059-021-02592-9).\n    The reason may be that the real data's high-confidence call sets do not have\n    the most challenging genomic regions, whereas _in silico_ data do not have\n    the most realistic data characteristics. Combining both allows them to cover\n    each other's shortcomings.\n\n### Dockerized alignment pipeline based on GATK's best practices\n\nDescribed at\n[**somaticseq/utilities/dockered_pipelines**](somaticseq/utilities/dockered_pipelines).\nThe module is `makeAlignmentScripts.py`.\n\n### Utilities\n\nWe have some generally useful scripts in [utilities](somaticseq/utilities). Some\nof the more useful tools, e.g.,\n\n-   `lociCounterWithLabels.py` finds overlapping regions among multiple bed\n    files.\n-   `paired_end_bam2fastq.py` converts paired-end bam files into 1.fastq and\n    2.fastq files. It will not require an enormous amount of memory, nor will\n    the resulting files crap out on downstream GATK tools.\n-   `run_workflows.py` is a rudimentary workflow manager that executes multiple\n    scripts at once.\n-   `split_bed_into_equal_regions.py` splits one bed file into a number of\n    output bed files, where each output bed file will have the same total\n    length.\n",190,somatic-variants,Python,5,R,Shell,Python,Perl,Dockerfile,,,,,,,,,,,,,,,,,,,,,,,,41,1,40,0,5,7,38,68837,53,93,88,5,acd0a39c47265ad1a117c0daeac28cab7b83e00e,set upper limit for some versions to prevent incompatibility,2024-06-30T00:07:53Z,Li Tai Fang,litai.fang@freenome.com,lfang-fn,v3.8.0,"## Mostly maintenance\r\n- A lot of coding stylistic changes\r\n- Enforce versioning for some dependencies in setup.py, including python>=3.10\r\n- For XGBoost model, additionally output json file (i.e., decision trees)\r\n- Updated some docker files for 3rd party tools\r\n- remove `-d dbsnp.vcf.gz` parameter from tumor-only LoFreq command (that param is only meaningful for tumor-normal pair)\r\n\r\n**Full Changelog**: https://github.com/bioinform/somaticseq/compare/v3.7.4...v3.8.0",v3.8.0,Li Tai Fang,,litaifang,"BSD 2-Clause ""Simplified"" License",somaticseq,bioinform,58,somatic-variants,cancer-genomics,,,,,,,,,,,,,,,,,,,/bioinform/somaticseq,60,12,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/bgrabitmap/bgracontrols,https://github.com/bgrabitmap/bgracontrols,0,,,0,0,0,0,0,0,1,1,0,0,0,🆗 BGRA Controls is a set of graphical UI elements that you can use with Lazarus LCL applications.,"# BGRA Controls\n\nBGRA Controls is a set of graphical UI elements that you can use with Lazarus LCL applications.\n\n![BGRA Controls](https://raw.githubusercontent.com/bgrabitmap/bgracontrols/dev-bgracontrols/docs/img/logo.png)\n\n### Support Us\n\nIf you like BGRAControls please [support @circular17 with PayPal](https://sourceforge.net/p/lazpaint/donate/?source=navbar).\n\n### Installing\n\nUse the Online Package Manager to get BGRABitmap and BGRAControls.\n\nNotice that you must check only the packages ""bgrabitmappack.lpk"" and ""bgracontrols.lpk"" in the Online Package Manager. The other packages are optional and may need third party packages / libraries to work (OpenGL and PascalScript).\n\n### Optional Components\n\nSince v4.4 the components TBCDefaultThemeManager, TBCKeyboard and TBCNumericKeyboard are not installed by default to allow Linux users to get a seamless installation with the Online Package Manager not installing third party stuff. If you want these components turn on the ""Register unit"" in the package options for each file (bcdefaulthememanager.pas, bckeyboard.pas, bcnumerickeyboard.pas) then compile and rebuild Lazarus. On Linux you need to install libxtst-dev and libgl-dev first.\n\n### Screenshots macOS 64 Cocoa\n![Analog Controls](https://raw.githubusercontent.com/bgrabitmap/bgracontrols/dev-bgracontrols/docs/img/analogcontrols.png)\n![BCButton](https://raw.githubusercontent.com/bgrabitmap/bgracontrols/dev-bgracontrols/docs/img/bcbutton.png)\n![BCButtonFocus](https://raw.githubusercontent.com/bgrabitmap/bgracontrols/dev-bgracontrols/docs/img/bcbuttonfocus.png)\n![BCImageButton](https://raw.githubusercontent.com/bgrabitmap/bgracontrols/dev-bgracontrols/docs/img/bcimagebutton.png)\n![BCToolBar](https://raw.githubusercontent.com/bgrabitmap/bgracontrols/dev-bgracontrols/docs/img/bctoolbar.png)\n![BCXButton](https://raw.githubusercontent.com/bgrabitmap/bgracontrols/dev-bgracontrols/docs/img/bcxbutton.png)\n![BGRA Ribbon](https://raw.githubusercontent.com/bgrabitmap/bgracontrols/dev-bgracontrols/docs/img/bgraribbon.png)\n![ProgressBar](https://raw.githubusercontent.com/bgrabitmap/bgracontrols/dev-bgracontrols/docs/img/progressbar.png)\n\n### TBCButton\n\nA button control that can be styled through properties for each state like StateClicked, StateHover, StateNormal with settings like gradients, border and text with shadows. You can assign an already made style through the property AssignStyle.\n\nAuthor: Dibo.\n\n### TBCButtonFocus\n\nLike TBCButton but it supports focus like normal TButton.\n\nAuthor: Dibo.\n\n### TBCGameGrid\n\nA grid with custom width and height of items and any number of horizontal and vertical cells that can be drawn with BGRABitmap directly with the OnRenderControl event.\n\nAuthor: Lainz.\n\n### TBCImageButton\n\nA button control that can be styled with one image file, containing the drawing for each state Normal, Hovered, Active and Disabled. It supports 9-slice scaling feature. It supports a nice fading animation that can be turned on.\n\nAuthor: Lainz.\n\n### TBCXButton\n\nA button control that can be styled by code with the OnRenderControl event. Or even better create your own child control inheriting from this class.\n\nAuthor: Lainz.\n\n### TBCLabel\n\nA label control that can be styled through properties, it supports shadow, custom borders and background.\n\nAuthor: Dibo.\n\n### TBCMaterialDesignButton\n\nA button control that has an animation effect according to Google Material Design guidelines. It supports custom color for background and for the circle animation, also you can customize the shadow.\n\nAuthor: Lainz.\n\n### TBCMDButton\n\nA button control like TBCMaterialDesignButton, without shadow, but with more capabilities.\n\nAuthor: Lainz. Contributions by Fritz.\n\n### TBCPanel\n\nA panel control that can be styled through properties. You can assign an already made style through the property AssignStyle.\n\nAuthor: Dibo.\n\n### TBCRadialProgressBar\n\nA progress bar with radial style. You can set the color and text properties as you like.\n\nAuthor: Lainz.\n\n### TBCSVGButton\n\nButton made with SVG images for each state. Based on the SVG Viewer.\n\nAuthor: Josh.\n\n### TBCSVGViewer\n\nSVG viewer with several options.\n\nAuthor: Lainz, Circular.\n\n### TBCToolBar\n\nA TToolBar with an event OnRedraw to paint it using BGRABitmap. It supports also the default OnPaintButton to customize the buttons drawing. By default it comes with a Windows 7 like explorer toolbar style.\n\nAuthor: Lainz.\n\n### TBCTrackBarUpdown\n\nA control to input numeric values with works like a trackbar and a spinedit both in one control.\n\nAuthor: Circular.\n\n### TBGRAFlashProgressBar\n\nA progress bar with a default style inspired in the old Flash Player Setup for Windows progress dialog. You can change the color property to have different styles and also you can use the event OnRedraw to paint custom styles on it like text or override the entire default drawing.\n\nAuthor: Circular.\n\n### TBGRAGraphicControl\n\nIs like a paintbox. You can draw with transparency with this control using the OnRedraw event.\n\nAuthor: Circular.\n\n### TBGRAImageList\n\nAn image list that supports alpha in all supported platforms.\n\nAuthor: Dibo.\n\n### TBGRAImageManipulation\n\nA tool to manipulate pictures, see the demo that shows all the capability that comes with it.\n\nAuthor: Emerson Cavalcanti, maxm74 .\n\n### TBGRAKnob\n\nA knob that can be styled through properties.\n\nAuthor: Circular.\n\n### TBGRAResizeSpeedButton\n\nA speed button that can resize the glyph to fit in the entire control.\n\nAuthor: Fox (helix2001).\n\n### TBGRAShape\n\nA control with configurable shapes like polygon and ellipse that can be filled with gradients and can have custom borders and many other visual settings.\n\nAuthor: Circular.\n\n### TBGRASpeedButton\n\nA speed button that in GTK and GTK2 provides BGRABitmap powered transparency to the glyph.\n\nAuthor: Dibo.\n\n### TBGRASpriteAnimation\n\nA component that can be used as image viewer or animation viewer, supports the loading of gif files.\n\nAuthor: Lainz.\n\n### TBGRAVirtualScreen\n\nIs like a panel. You can draw this control using the OnRedraw event.\n\nAuthor: Circular.\n\n### TBCNumericKeyboard\n\nA panel with numeric buttons to store the input in a string. Then you can use the events to edit it to fit your needs and assign to other controls that value.\n\nAuthor: Lainz.\n\n### TBCRealNumericKeyboard\n\nA panel with numeric buttons to do the real input of the keys on keyboard. What you type is sent to the focused control directly.\n\nAuthor: Lainz. Esvignolo.\n\n### TBCDefaultThemeManager\n\nA component to style all the selected buttons in a form with the need to style only a single button. Can be used entirely with code.\n\nAuthor: Lainz.\n\n### TDTAnalogClock\n\nA clock.\n\nAuthor: Digeo.\n\n### TDTAnalogGaugue\n\nA gauge.\n\nAuthor: Digeo.\n\n### TDTThemedClock\n\nAnother clock.\n\nAuthor: Digeo.\n\n### TDTThemedGauge\n\nAnother gauge.\n\nAuthor: Digeo.\n\n### TPSImport_BGRAPascalScript\n\nA component to load BGRABitmap pascal script utilities.\n\nAuthor: Lainz, Circular.\n\n### TBCFluentProgressRing\n\nProgress control in a form of a circle. Also with animated indeterminated state.\n\nAuthor: hedgehog.\n\n### TBCFluentSlider\n\nSimple slider with modern design.\n\nAuthor: hedgehog.\n\n### TBCLeaTheme\n\nNon-visual theme-container that can be used with all the BCLea components. An editor, Theme Builder, is in the /test/test_bclea directory.\n\nAuthor: Boban Spasic.\n\n### TBCLeaLED and TBCLeaQLED\n\nRound and square LED controls. Can also be used as a switch (property clickable).\n\nAuthor: Boban Spasic.\n\n### TBCLeaLCDDisplay\n\nA LCD component with integrated font editor.\n\nAuthor: Boban Spasic, Werner Pamler.\n\n### TBCLeaSelector\n\nA knob to select between predefined items.\n\nAuthor: Boban Spasic.\n\n### TBCLeaRingSlider\n\nAn imitation of a potentiometer.\n\nAuthor: Boban Spasic.\n\n# BGRA Custom Drawn\nBGRA Custom Drawn is a set of controls inherited from Custom Drawn. These come with a default dark style that is like Photoshop.\n\nAuthor: Lainz.\n\n### TBCDButton\n\nA button control that is styled with TBGRADrawer.\n\n### TBCDEdit\n\nAn edit control that is styled with TBGRADrawer.\n\n### TBCDStaticText\n\nA label control that is styled with TBGRADrawer.\n\n### TBCDProgressBar\n\nA progress bar control that is styled with TBGRADrawer.\n\n### TBCDSpinEdit\n\nA spin edit control that is styled with TBGRADrawer.\n\n### TBCDCheckBox\n\nA check box control that is styled with TBGRADrawer.\n\n### TBCRadioButton\n\nA radio button that is styled with TBGRADrawer.\n\n### TBCDPanel\n\nA panel control that is styled in its own Paint event.\n\n# Sample code\n\nBGRA Controls comes with nice demos to show how to use the stuff and extra things you can use in your own projects.\n\nContributors: Lainz, Circular, Fred vS, Coasting and others.\n\n### Pascal Script Library\n\nPutting BGRABitmap methods into a .dll with c#, java and pascal headers.\n\n### BGRA Ribbon Custom\n\nHow to create a fully themed window using the controls to achieve a Ribbon like application.\n\n### Tests\n\nThere are test for analog controls (clock and gauge), BC prefixed controls, BGRA prefixed controls, BGRA Custom Drawn controls, how to use Pascal Script and BGRABitmap, bgrascript or how to create your own scripting solution with BGRABitmap.\n\n### Tests Extra\n\nThese are extra tests like how to use fading effect, an fpGUI theme, games like maze and puzzle, how we created the material design animation, pix2svg or how to convert a small picture to svg using hexagons, rectangles and ellipses, plugins or how to load .dlls and use into a TBGRAVirtualScreen, rain effect, shadow effect, 9-slice-scaling with Custom Drawn or how to theme with bitmaps an application to look like Windows themes and 9-slice-scaling with charts.\n\n# Another units\n\nThese units come with BGRA Controls and contains more functionality that is sometimes used with the controls, sometimes not but are usefull in some way. Some are listed here, others you can see linked directly with any control like bcrtti, bcstylesform, bctools, bctypes.\n\nAuthor: Dibo.\n\n### BCEffect\n\nFading effect with BGRABitmap.\n\nAuthor: Lainz, Circular.\n\n### BCFilters\n\nA set of pixel filters to use with BGRABitmap.\n\nAuthor: Lainz.\n\n### BGRAScript\n\nScripting with BGRABitmap, see test project.\n\nAuthor: Lainz.\n",179,graphics,Pascal,5,Pascal,C#,Java,Batchfile,Makefile,,,,,,,,,,,,,,,,,,,,,,,,87,3,84,0,2,11,122,29865,30,98,86,12,faa0d281eb2752f14afa643f4fd97c50540fa59b,Merge pull request #186 from bgrabitmap/dev-bgracontrols,2024-07-04T14:45:23Z,Leandro Oscar Ezequiel Diaz,lainz@users.noreply.github.com,lainz,BGRAControls v9.0.1.5,## What's Changed\r\n* Changed ktSector Operation to compute divisions for sectors.  by @sganz in https://github.com/bgrabitmap/bgracontrols/pull/177\r\n* Dev bgracontrols by @lainz in https://github.com/bgrabitmap/bgracontrols/pull/178\r\n* Update bgraknob.pas removed needless procs by @sganz in https://github.com/bgrabitmap/bgracontrols/pull/179\r\n* Dev bgracontrols v9.0.1.5 by @lainz in https://github.com/bgrabitmap/bgracontrols/pull/180\r\n\r\n\r\n**Full Changelog**: https://github.com/bgrabitmap/bgracontrols/compare/v9.0.1.4...v9.0.1.5,v9.0.1.5,Leandro Oscar Ezequiel Diaz,,lainz,,bgracontrols,bgrabitmap,57,bgra-controls,lazarus,graphics,graphics-library,pascal,bgrabitmap,ui,,,,,,,,,,,,,,/bgrabitmap/bgracontrols,58,29,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/bebop/poly,https://github.com/bebop/poly,0,just a package,,0,1,0,0,0,0,1,1,0,0,0,A Go package for engineering organisms.,"# (Poly)merase <img align=""right"" src=""https://raw.githubusercontent.com/bebop/presskit/main/gopher.png"" width=""100"">\n\n[![PkgGoDev](https://pkg.go.dev/badge/github.com/bebop/poly)](https://pkg.go.dev/github.com/bebop/poly)\n[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/bebop/poly/blob/main/LICENSE) \n![Tests](https://github.com/bebop/poly/workflows/Test/badge.svg)\n![Test Coverage](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/TimothyStiles/e58f265655ac0acacdd1a38376ccd32a/raw/coverage.json)\n\nPoly is a Go package for engineering organisms.\n\n* **Fast:** Poly is fast and scalable.\n\n* **Modern:** Poly tackles issues that other libraries and utilities just don't. From general codon optimization and primer design to circular sequence hashing. All written in a language that was designed to be fast, scalable, and easy to develop in and maintain. Did we say it was fast?\n\n* **Reproducible:** Poly is well tested and designed to be used in industrial, academic, and hobbyist settings. No more copy and pasting strings into random websites to process the data you need.\n\n* **Ambitious:** Poly's goal is to be the most complete, open, and well used collection of computational synthetic biology tools ever assembled. If you like our dream and want to support us please star this repo, request a feature, open a pull request, or [sponsor the project](https://github.com/sponsors/TimothyStiles).\n\n\n## Install\n\n`go get github.com/bebop/poly@latest`\n\n## Documentation\n\n\n* **[Library](https://pkg.go.dev/github.com/bebop/poly#pkg-examples)**\n\n* **[Tutorials](https://github.com/bebop/poly/tree/main/tutorials)**\n\n* **[Learning Synbio](https://github.com/TimothyStiles/how-to-synbio)**\n\n## Community\n\n* **[Discord](https://discord.gg/Hc8Ncwt):** Chat about Poly and join us for game nights on our discord server!\n\n## Contributing\n\n* **[Code of conduct](CODE_OF_CONDUCT.md):** Please read the full text so you can understand what we're all about and remember to be excellent to each other!\n\n* **[Contributor's guide](CONTRIBUTING.md):** Please read through it before you start hacking away and pushing contributions to this fine codebase.\n\n## Sponsor\n\n* **[Sponsor](https://github.com/sponsors/TimothyStiles):** 🤘 Thanks for your support 🤘\n\n## License\n\n* [MIT](LICENSE)\n\n* Copyright (c) 2024 Timothy Stiles\n",661,bioinformatics,Go,1,Go,,,,,,,,,,,,,,,,,,,,,,,,,,,,268,65,198,5,48,32,137,13571,69,165,138,27,e33972664f2b0358dfd366ba1074c578d4381943,Update copyright year,2024-04-30T01:25:00Z,Tim,TimothyStiles@users.noreply.github.com,TimothyStiles,,,,,,,MIT License,poly,bebop,54,synthetic-biology,synbio,genetic-engineering,molecular-biology,bioengineering,computational-biology,bioinformatics,dna,fasta,genbank,codon-optimizer,sequence-hashing,plasmids,primer-design,dna-barcode,dna-barcoding,golden-gate,mash,alignment,go,/bebop/poly,58,13,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/bdaiinstitute/spatialmath-python,https://github.com/bdaiinstitute/spatialmath-python,0,,,0,0,0,0,0,0,1,0,0,0,0,"Create,  manipulate and convert representations of position and orientation in 2D or 3D using Python","# Spatial Maths for Python\n\n[![A Python Robotics Package](https://raw.githubusercontent.com/petercorke/robotics-toolbox-python/master/.github/svg/py_collection.min.svg)](https://github.com/petercorke/robotics-toolbox-python)\n[![QUT Centre for Robotics Open Source](https://github.com/qcr/qcr.github.io/raw/master/misc/badge.svg)](https://qcr.github.io)\n\n[![PyPI version](https://badge.fury.io/py/spatialmath-python.svg)](https://badge.fury.io/py/spatialmath-python)\n[![Anaconda version](https://anaconda.org/conda-forge/spatialmath-python/badges/version.svg)](https://anaconda.org/conda-forge/spatialmath-python)\n![Python Version](https://img.shields.io/pypi/pyversions/spatialmath-python.svg)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n[![Build Status](https://github.com/bdaiinstitute/spatialmath-python/actions/workflows/master.yml/badge.svg?branch=master)](https://github.com/bdaiinstitute/spatialmath-python/actions/workflows/master.yml?query=workflow%3Abuild+branch%3Amaster)\n[![Coverage](https://codecov.io/github/bdaiinstitute/spatialmath-python/graph/badge.svg?token=W15FGBA059)](https://codecov.io/github/bdaiinstitute/spatialmath-python)\n[![PyPI - Downloads](https://img.shields.io/pypi/dw/spatialmath-python)](https://pypistats.org/packages/spatialmath-python)\n[![GitHub stars](https://img.shields.io/github/stars/bdaiinstitute/spatialmath-python.svg?style=social&label=Star)](https://GitHub.com/bdaiinstitute/spatialmath-python/stargazers/)\n\n\n\n<table style=""border:0px"">\n<tr style=""border:0px"">\n<td style=""border:0px"">\n<img src=""https://github.com/bdaiinstitute/spatialmath-python/raw/master/docs/figs/CartesianSnakes_LogoW.png"" width=""200""></td>\n<td style=""border:0px"">\nA Python implementation of the <a href=""https://github.com/petercorke/spatial-math"">Spatial Math Toolbox for MATLAB<sup>&reg;</sup></a>\n<ul>\n<li><a href=""https://github.com/bdaiinstitute/spatialmath-python"">GitHub repository </a></li>\n<li><a href=""https://bdaiinstitute.github.io/spatialmath-python"">Documentation</a></li>\n<li><a href=https://github.com/bdaiinstitute/spatialmath-python/discussions/categories/changes>Recent changes</a>\n<li><a href=""https://github.com/bdaiinstitute/spatialmath-python/wiki"">Wiki (examples and details)</a></li>\n<li><a href=""installation#"">Installation</a></li>\n</ul>\n</td>\n</tr>\n</table>\n\nSpatial mathematics capability underpins all of robotics and robotic vision where we need to describe the position, orientation or pose of objects in 2D or 3D spaces.\n\n\n\n# What it does\n\nThe package provides classes to represent pose and orientation in 3D and 2D\nspace:\n\n| Represents   | in 3D            |   in 2D  |\n| ------------ | ---------------- | -------- |\n| pose         | ``SE3`` ``Twist3`` ``UnitDualQuaternion``   |   ``SE2`` ``Twist2`` |\n| orientation  | ``SO3`` ``UnitQuaternion`` |            ``SO2``  |\n                \n                \nMore specifically:\n\n * `SE3` matrices belonging to the group $\mathbf{SE}(3)$ for position and orientation (pose) in 3-dimensions\n * `SO3` matrices belonging to the group $\mathbf{SO}(3)$ for orientation in 3-dimensions\n *  `UnitQuaternion` belonging to the group $\mathbf{S}^3$ for orientation in 3-dimensions\n * `Twist3` vectors belonging to the group $\mathbf{se}(3)$ for pose in 3-dimensions\n * `UnitDualQuaternion` maps to the group $\mathbf{SE}(3)$ for position and orientation (pose) in 3-dimensions\n * `SE2` matrices belonging to the group $\mathbf{SE}(2)$ for position and orientation (pose) in 2-dimensions\n * `SO2` matrices belonging to the group $\mathbf{SO}(2)$ for orientation in 2-dimensions\n * `Twist2` vectors belonging to the group $\mathbf{se}(2)$ for pose in 2-dimensions\n\n\nThese classes provide convenience and type safety, as well as methods and overloaded operators to support:\n\n * composition, using the `*` operator\n * point transformation, using the `*` operator\n * exponent, using the `**` operator\n * normalization\n * inversion\n * connection to the Lie algebra via matrix exponential and logarithm operations\n * conversion of orientation to/from Euler angles, roll-pitch-yaw angles and angle-axis forms.\n * list operations such as append, insert and get\n\nThese are layered over a set of base functions that perform many of the same operations but represent data explicitly in terms of `numpy` arrays.\n\nThe class, method and functions names largely mirror those of the MATLAB toolboxes, and the semantics are quite similar.\n\n![trplot](https://github.com/bdaiinstitute/spatialmath-python/raw/master/docs/figs/fig1.png)\n\n![animation video](./docs/figs/animate.gif)\n\n# Citing\n\nCheck out our ICRA 2021 paper on [IEEE Xplore](https://ieeexplore.ieee.org/document/9561366) or get the PDF from [Peter's website](https://bit.ly/icra_rtb).  This describes the [Robotics Toolbox for Python](https://github.com/petercorke/robotics-toolbox-python) as well Spatial Maths.\n\nIf the toolbox helped you in your research, please cite\n\n```\n@inproceedings{rtb,\n  title={Not your grandmother’s toolbox--the Robotics Toolbox reinvented for Python},\n  author={Corke, Peter and Haviland, Jesse},\n  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)},\n  pages={11357--11363},\n  year={2021},\n  organization={IEEE}\n}\n```\n\n<br>\n\n<a id='6'></a>\n\n## Using the Toolbox in your Open Source Code?\n\nIf you are using the Toolbox in your open source code, feel free to add our badge to your readme!\n\n[![Powered by the Spatial Math Toolbox](https://github.com/bdaiinstitute/spatialmath-python/raw/master/.github/svg/sm_powered.min.svg)](https://github.com/bdaiinstitute/spatialmath-python)\n\nSimply copy the following\n\n```\n[![Powered by the Spatial Math Toolbox](https://github.com/bdaiinstitute/spatialmath-python/raw/master/.github/svg/sm_powered.min.svg)](https://github.com/bdaiinstitute/spatialmath-python)\n```\n\n\n# Installation\n\n## Using pip\n\nInstall a snapshot from PyPI\n\n```\npip install spatialmath-python\n```\n\n## From GitHub\n\nInstall the current code base from GitHub and pip install a link to that cloned copy\n\n```\ngit clone https://github.com/bdaiinstitute/spatialmath-python.git\ncd spatialmath-python\npip install -e .\n# Optional: if you would like to contribute and commit code changes to the repository,\n# pre-commit install\n```\n\n## Dependencies\n\n`numpy`, `scipy`, `matplotlib`, `ffmpeg` (if rendering animations as a movie)\n\n# Examples\n\n\n## High-level classes\n\nThese classes abstract the low-level numpy arrays into objects that obey the rules associated with the mathematical groups SO(2), SE(2), SO(3), SE(3) as well as twists and quaternions.\n\nUsing classes ensures type safety, for example it stops us mixing a 2D homogeneous transformation with a 3D rotation matrix -- both of which are 3x3 matrices.  It also ensures that the internal matrix representation is always a valid member of the relevant group.\n\nFor example, to create an object representing a rotation of 0.3 radians about the x-axis is simply\n\n```python\n>>> from spatialmath import SO3, SE3\n>>> R1 = SO3.Rx(0.3)\n>>> R1\n   1         0         0          \n   0         0.955336 -0.29552    \n   0         0.29552   0.955336         \n```\nwhile a rotation of 30 deg about the z-axis is\n\n```python\n>>> R2 = SO3.Rz(30, 'deg')\n>>> R2\n   0.866025 -0.5       0          \n   0.5       0.866025  0          \n   0         0         1    \n```\nand the composition of these two rotations is \n\n```python\n>>> R = R1 * R2\n   0.866025 -0.5       0          \n   0.433013  0.75     -0.5        \n   0.25      0.433013  0.866025 \n```\n\nWe can find the corresponding Euler angles (in radians)\n\n```python\n>> R.eul()\narray([-1.57079633,  0.52359878,  2.0943951 ])\n```\n\nFrequently in robotics we want a sequence, a trajectory, of rotation matrices or poses. These pose classes inherit capability from the `list` class\n\n```python\n>>> R = SO3()   # the null rotation or identity matrix\n>>> R.append(R1)\n>>> R.append(R2)\n>>> len(R)\n 3\n>>> R[1]\n   1         0         0          \n   0         0.955336 -0.29552    \n   0         0.29552   0.955336             \n```\nand this can be used in `for` loops and list comprehensions.\n\nAn alternative way of constructing this would be (`R1`, `R2` defined above)\n\n```python\n>>> R = SO3( [ SO3(), R1, R2 ] )       \n>>> len(R)\n 3\n```\n\nMany of the constructors such as `.Rx`, `.Ry` and `.Rz` support vectorization\n\n```python\n>>> R = SO3.Rx( np.arange(0, 2*np.pi, 0.2))\n>>> len(R)\n 32\n```\nwhich has created, in a single line, a list of rotation matrices.\n\nVectorization also applies to the operators, for instance\n\n```python\n>>> A = R * SO3.Ry(0.5)\n>>> len(R)\n 32\n```\nwill produce a result where each element is the product of each element of the left-hand side with the right-hand side, ie. `R[i] * SO3.Ry(0.5)`.\n\nSimilarly\n\n```python\n>>> A = SO3.Ry(0.5) * R \n>>> len(R)\n 32\n```\nwill produce a result where each element is the product of the left-hand side with each element of the right-hand side , ie. `SO3.Ry(0.5) * R[i] `.\n\nFinally\n\n```python\n>>> A = R * R \n>>> len(R)\n 32\n```\nwill produce a result where each element is the product of each element of the left-hand side with each element of the right-hand side , ie. `R[i] * R[i] `.\n\nThe underlying representation of these classes is a numpy matrix, but the class ensures that the structure of that matrix is valid for the particular group represented: SO(2), SE(2), SO(3), SE(3).  Any operation that is not valid for the group will return a matrix rather than a pose class, for example\n\n```python\n>>> SO3.Rx(0.3) * 2\narray([[ 2.        ,  0.        ,  0.        ],\n       [ 0.        ,  1.91067298, -0.59104041],\n       [ 0.        ,  0.59104041,  1.91067298]])\n\n>>> SO3.Rx(0.3) - 1\narray([[ 0.        , -1.        , -1.        ],\n       [-1.        , -0.04466351, -1.29552021],\n       [-1.        , -0.70447979, -0.04466351]])\n```\n\nWe can print and plot these objects as well\n\n```\n>>> T = SE3(1,2,3) * SE3.Rx(30, 'deg')\n>>> T.print()\n   1         0         0         1          \n   0         0.866025 -0.5       2          \n   0         0.5       0.866025  3          \n   0         0         0         1          \n\n>>> T.printline()\nt =        1,        2,        3; rpy/zyx =       30,        0,        0 deg\n\n>>> T.plot()\n```\n\n![trplot](https://github.com/bdaiinstitute/spatialmath-python/raw/master/docs/figs/fig1.png)\n\n`printline` is a compact single line format for tabular listing, whereas `print` shows the underlying matrix and for consoles that support it, it is colorised, with rotational elements in red and translational elements in blue.\n\nFor more detail checkout the shipped Python notebooks:\n\n* [gentle introduction](https://github.com/bdaiinstitute/spatialmath-python/blob/master/notebooks/gentle-introduction.ipynb)\n* [deeper introduction](https://github.com/bdaiinstitute/spatialmath-python/blob/master/notebooks/introduction.ipynb)\n\n\nYou can browse it statically through the links above, or clone the toolbox and run them interactively using [Jupyter](https://jupyter.org) or [JupyterLab](https://jupyter.org).\n\n\n## Low-level spatial math\n\n\nImport the low-level transform functions\n\n```\n>>> from spatialmath.base import *\n```\n\nWe can create a 3D rotation matrix\n\n```\n>>> rotx(0.3)\narray([[ 1.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.95533649, -0.29552021],\n       [ 0.        ,  0.29552021,  0.95533649]])\n\n>>> rotx(30, unit='deg')\narray([[ 1.       ,  0.       ,  0.       ],\n       [ 0.       ,  0.8660254, -0.5      ],\n       [ 0.       ,  0.5      ,  0.8660254]])\n```\nThe results are `numpy` arrays so to perform matrix multiplication you need to use the `@` operator, for example\n\n```\nrotx(0.3) @ roty(0.2)\n```\n\nWe also support multiple ways of passing vector information to functions that require it:\n\n* as separate positional arguments\n\n```\ntransl2(1, 2)\narray([[1., 0., 1.],\n       [0., 1., 2.],\n       [0., 0., 1.]])\n```\n\n* as a list or a tuple\n\n```\ntransl2( [1,2] )\narray([[1., 0., 1.],\n       [0., 1., 2.],\n       [0., 0., 1.]])\n\ntransl2( (1,2) )\nOut[444]: \narray([[1., 0., 1.],\n       [0., 1., 2.],\n       [0., 0., 1.]])\n```\n\n* or as a `numpy` array\n\n```\ntransl2( np.array([1,2]) )\nOut[445]: \narray([[1., 0., 1.],\n       [0., 1., 2.],\n       [0., 0., 1.]])\n```\n\n\nThere is a single module that deals with quaternions, unit or not, and the representation is a `numpy` array of four elements.  As above, functions can accept the `numpy` array, a list, dict or `numpy` row or column vectors.\n\n```\n>>> from spatialmath.base.quaternion import *\n>>> q = qqmul([1,2,3,4], [5,6,7,8])\n>>> q\narray([-60,  12,  30,  24])\n>>> qprint(q)\n-60.000000 < 12.000000, 30.000000, 24.000000 >\n>>> qnorm(q)\n72.24956747275377\n```\n\n## Graphics\n\n![trplot](https://github.com/bdaiinstitute/spatialmath-python/raw/master/docs/figs/transforms3d.png)\n\nThe functions support various plotting styles\n\n```\ntrplot( transl(1,2,3), frame='A', rviz=True, width=1, dims=[0, 10, 0, 10, 0, 10])\ntrplot( transl(3,1, 2), color='red', width=3, frame='B')\ntrplot( transl(4, 3, 1)@trotx(math.pi/3), color='green', frame='c', dims=[0,4,0,4,0,4])\n```\n\nAnimation is straightforward\n\n```\ntranimate(transl(4, 3, 4)@trotx(2)@troty(-2), frame='A', arrow=False, dims=[0, 5], nframes=200)\n```\n\nand it can be saved to a file by\n\n```\ntranimate(transl(4, 3, 4)@trotx(2)@troty(-2), frame='A', arrow=False, dims=[0, 5], nframes=200, movie='out.mp4')\n```\n\n![animation video](./docs/figs/animate.gif)\n\nAt the moment we can only save as an MP4, but the following incantation will covert that to an animated GIF for embedding in web pages\n\n```\nffmpeg -i out -r 20 -vf ""fps=10,scale=640:-1:flags=lanczos,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse"" out.gif\n```\n\nFor use in a Jupyter notebook, or on Colab, you can display an animation by\n```\nfrom IPython.core.display import HTML\nHTML(tranimate(transl(4, 3, 4)@trotx(2)@troty(-2), frame='A', arrow=False, dims=[0, 5], nframes=200, movie=True))\n```\nThe `movie=True` option causes `tranimate` to output an HTML5 fragment which\nis displayed inline by the `HTML` function.\n\n## Symbolic support\n\nSome functions have support for symbolic variables, for example\n\n```\nimport sympy\n\ntheta = sym.symbols('theta')\nprint(rotx(theta))\n[[1 0 0]\n [0 cos(theta) -sin(theta)]\n [0 sin(theta) cos(theta)]]\n```\n\nThe resulting `numpy` array is an array of symbolic objects not numbers &ndash; the constants are also symbolic objects.  You can read the elements of the matrix\n\n```\na = T[0,0]\n\na\nOut[258]: 1\n\ntype(a)\nOut[259]: int\n\na = T[1,1]\na\nOut[256]: \ncos(theta)\ntype(a)\nOut[255]: cos\n```\nWe see that the symbolic constants are converted back to Python numeric types on read.\n\nSimilarly when we assign an element or slice of the symbolic matrix to a numeric value, they are converted to symbolic constants on the way in.\n\n## History & Contributors\n\nThis package was originally created by [Peter Corke](https://github.com/petercorke) and [Jesse Haviland](https://github.com/jhavl) and was inspired by the [Spatial Math Toolbox for MATLAB](https://github.com/petercorke/spatialmath-matlab).  It supports the textbook [Robotics, Vision & Control in Python 3e](https://github.com/petercorke/RVC3-python).\n\nThe package is now a collaboration with [Boston Dynamics AI Institute](https://theaiinstitute.com/).\n",482,graphics,Python,1,Python,,,,,,,,,,,,,,,,,,,,,,,,,,,,67,11,50,6,9,22,195,31235,81,46,39,7,6074eb7d0a66981b51386a13441d3077a40bcfb2,Add BSplineSE3 class. (#128),2024-07-09T18:59:47Z,Mark Yeatman,129521731+myeatman-bdai@users.noreply.github.com,myeatman-bdai,01.01.2010,## What's Changed\r\n* Fixed fall through error in binary operation by @bokorn-bdaii in https://github.com/bdaiinstitute/spatialmath-python/pull/117\r\n* Fix UnitQuaternion constructor with v keyword. by @myeatman-bdai in https://github.com/bdaiinstitute/spatialmath-python/pull/118\r\n* Fixed code block bugs and clarified a sentence in intro.rst by @tjdwill in https://github.com/bdaiinstitute/spatialmath-python/pull/120\r\n* Created internal quaternion conversion function for SO3 by @bokorn-bdaii in https://github.com/bdaiinstitute/spatialmath-python/pull/119\r\n* Fixed [Issue-122] for interp by @jcao-bdai in https://github.com/bdaiinstitute/spatialmath-python/pull/123\r\n* Add python 3.12 to list of supported versions; exclude macos-latest+python3.7 from testing by @jcao-bdai in https://github.com/bdaiinstitute/spatialmath-python/pull/124\r\n\r\n## New Contributors\r\n* @tjdwill made their first contribution in https://github.com/bdaiinstitute/spatialmath-python/pull/120\r\n\r\n**Full Changelog**: https://github.com/bdaiinstitute/spatialmath-python/compare/1.1.9...1.1.10,01.01.2010,Jien Cao,,jcao-bdai,MIT License,spatialmath-python,bdaiinstitute,4,robot,math,2d,3d,transform,quaternion,graphics,animation,euler-angles,roll-pitch-yaw-angles,python,rotation-matrix,so3,,,,,,,,/bdaiinstitute/spatialmath-python,14,15,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/bcgsc/abyss,https://github.com/bcgsc/abyss,1,,,1,1,1,1,0,0,0,0,0,0,1,:microscope: Assemble large genomes using short reads,"[![Release](https://img.shields.io/github/release/bcgsc/abyss.svg)](https://github.com/bcgsc/abyss/releases)\n[![Downloads](https://img.shields.io/github/downloads/bcgsc/abyss/total?logo=github)](https://github.com/bcgsc/abyss/releases/download/2.2.3/abyss-2.2.3.tar.gz)\n[![Conda](https://img.shields.io/conda/dn/bioconda/abyss?label=Conda)](https://anaconda.org/bioconda/abyss)\n[![Issues](https://img.shields.io/github/issues/bcgsc/abyss.svg)](https://github.com/bcgsc/abyss/issues)\n\nABySS\n================================================================================\n\nABySS is a *de novo* sequence assembler intended for short paired-end reads and genomes of all sizes.\n\nPlease [cite our papers](#citation).\n\nContents\n================================================================================\n\n* [Installation](#installation)\n	* [Install ABySS using Conda](#install-abyss-using-conda-recommended)\n	* [Install ABySS using Homebrew](#install-abyss-using-homebrew)\n	* [Install ABySS on Windows](#install-abyss-on-windows)\n* [Dependencies](#dependencies)\n	* [Dependencies for linked reads](#dependencies-for-linked-reads)\n	* [Optional dependencies](#optional-dependencies)\n* [Compiling ABySS from source](#compiling-abyss-from-source)\n* [Before starting an assembly](#before-starting-an-assembly)\n* [Modes](#modes)\n	* [Bloom filter mode](#bloom-filter-mode)\n	* [MPI mode (legacy)](#mpi-mode-legacy)\n* [Examples](#examples)\n	* [Assemble a small synthetic data set](#assemble-a-small-synthetic-data-set)\n	* [Assembling a paired-end library](#assembling-a-paired-end-library)\n	* [Assembling multiple libraries](#assembling-multiple-libraries)\n	* [Scaffolding](#scaffolding)\n	* [Scaffolding with linked reads](#scaffolding-with-linked-reads)\n	* [Rescaffolding with long sequences](#rescaffolding-with-long-sequences)\n	* [Assembling using a paired de Bruijn graph](#assembling-using-a-paired-de-bruijn-graph)\n	* [Assembling a strand-specific RNA-Seq library](#assembling-a-strand-specific-rna-seq-library)\n* [Optimizing the parameters k and kc](#optimizing-the-parameters-k-and-kc)\n* [Running ABySS on a cluster](#running-abyss-on-a-cluster)\n* [Using the DIDA alignment framework](#using-the-dida-alignment-framework)\n* [Assembly Parameters](#assembly-parameters)\n* [ABySS programs](#abyss-programs)\n* [Export to SQLite Database](#export-to-sqlite-database)\n	* [Database parameters](#database-parameters)\n	* [Helper programs](#helper-programs)\n* [Citation](#citation)\n* [Related Publications](#related-publications)\n* [Support](#support)\n* [Authors](#authors)\n\nInstallation\n================================================================================\n\n## Install ABySS using Conda (recommended)\n\nIf you have the [Conda](https://docs.conda.io/en/latest/) package manager (Linux, MacOS) installed, run:\n\n	conda install -c bioconda -c conda-forge abyss\n\nOr you can install ABySS in a dedicated environment:\n\n    conda create -n abyss-env\n    conda activate abyss-env\n    conda install -c bioconda -c conda-forge abyss\n\n## Install ABySS using Homebrew\n\nIf you have the [Homebrew](https://brew.sh) package manager (Linux, MacOS) installed, run:\n\n	brew install abyss\n\n## Install ABySS on Windows\n\nInstall [Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/) from which you can run Conda or Homebrew installation.\n\nDependencies\n============\n\n## Dependencies for linked reads\n\n- [ARCS](https://github.com/bcgsc/arcs) for scaffolding.\n- [Tigmint](https://github.com/bcgsc/tigmint) for correcting assembly errors.\n\nThese can be installed through Conda:\n\n	conda install -c bioconda arcs tigmint\n\nOr Homebrew:\n\n	brew install brewsci/bio/arcs brewsci/bio/links-scaffolder\n\n## Optional dependencies\n\n- [pigz](https://zlib.net/pigz/) for parallel gzip.\n- [samtools](https://samtools.github.io) for reading BAM files.\n- [zsh](https://sourceforge.net/projects/zsh/) for reporting time and memory usage.\n\nConda:\n\n	conda install -c bioconda samtools\n	conda install -c conda-forge pigz zsh\n\nHomebrew:\n\n	brew install pigz samtools zsh\n\nCompiling ABySS from source\n================================================================================\n\nWhen compiling ABySS from source the following tools are\nrequired:\n\n* [Autoconf](http://www.gnu.org/software/autoconf)\n* [Automake](http://www.gnu.org/software/automake)\n\nABySS requires a C++ compiler that supports\n[OpenMP](http://www.openmp.org) such as [GCC](http://gcc.gnu.org).\n\nThe following libraries are required:\n\n* [Boost](http://www.boost.org/)\n* [Open MPI](http://www.open-mpi.org)\n* [sparsehash](https://code.google.com/p/sparsehash/)\n* [btllib](https://github.com/bcgsc/btllib)\n\nConda:\n\n	conda install -c conda-forge boost openmpi\n	conda install -c bioconda google-sparsehash btllib\n\nIt is also helpful to install the compilers Conda package that automatically passes the correct compiler flags to use the available Conda packages:\n\n	conda install -c conda-forge compilers\n\nHomebrew:\n\n	brew install boost open-mpi google-sparsehash\n\nABySS will receive an error when compiling with Boost 1.51.0 or 1.52.0\nsince they contain a bug. Later versions of Boost compile without error.\n\nTo compile, run the following:\n\n	./autogen.sh\n	mkdir build\n	cd build\n	../configure --prefix=/path/to/abyss\n	make\n	make install\n\nYou may also pass the following flags to `configure` script:\n\n	--with-boost=PATH\n	--with-mpi=PATH\n	--with-sqlite=PATH\n	--with-sparsehash=PATH\n	--with-btllib=PATH\n\nWhere PATH is the path to the directory containing the corresponding dependencies. This should only be necessary if `configure` doesn't find the dependencies by default. If you are using Conda, PATH would be the path to the Conda installation. SQLite and MPI are optional dependencies.\n\nThe above steps install ABySS at the provided path, in this case `/path/to/abyss`.\nNot specifying `--prefix` would install in `/usr/local`, which requires\nsudo privileges when running `make install`.\n\nABySS requires a modern compiler such as GCC 6 or greater. If you have multiple\nversions of GCC installed, you can specify a different compiler:\n\n	../configure CC=gcc-10 CXX=g++-10\n\nWhile OpenMPI is assumed by default you can switch to LAM/MPI or MPICH\nusing:\n\n        ../configure --enable-lammpi\n        ../configure --enable-mpich\n\nThe default maximum k-mer size is 192 and may be decreased to reduce\nmemory usage or increased at compile time. This value must be a\nmultiple of 32 (i.e. 32, 64, 96, 128, etc):\n\n	../configure --enable-maxk=160\n\nIf you encounter compiler warnings that are not critical, you can allow the compilation to continue:\n\n	../configure --disable-werror\n\nTo run ABySS, its executables should be found in your `PATH` environment variable. If you\ninstalled ABySS in `/opt/abyss`, add `/opt/abyss/bin` to your `PATH`:\n\n	PATH=/opt/abyss/bin:$PATH\n\nBefore starting an assembly\n================================================================================\n\nABySS stores temporary files in `TMPDIR`, which is `/tmp` by default on most systems. If your default temporary disk volume is too small, set `TMPDIR` to a larger volume, such as `/var/tmp` or your home directory.\n\n	export TMPDIR=/var/tmp\n\nModes\n================================================================================\n\n## Bloom filter mode\n\nThe recommended mode of running ABySS is the Bloom filter mode. Specifying\nthe Bloom filter memory budget with the `B` parameter enables this mode, which can\nreduce memory consumption by ten-fold compared to the MPI mode. `B` may be specified\nwith unit suffixes 'k' (kilobytes), 'M' (megabytes), 'G' (gigabytes). If no units\nare specified bytes are assumed. Internally, the Bloom filter assembler allocates\nthe entire memory budget (`B * 8/9`) to a Counting Bloom filter, and an additional\n(`B/9`) memory to another Bloom filter that is used to track k-mers that have previously\nbeen included in contigs.\n\nA good value for `B` depends on a number of factors, but primarily on the\ngenome being assembled. A general guideline is:\n\nP. glauca (~20Gbp): `B=500G`\nH. sapiens (~3.1Gbp): `B=50G`\nC. elegans (~101Mbp): `B=2G`\n\nFor other genome sizes, the value for `B` can be interpolated. Note that\nthere is no downside to using larger than necessary `B` value, except for\nthe memory required. To make sure you have selected a correct `B` value,\ninspect the standard error log of the assembly process and ensure that the\nreported FPR value under `Counting Bloom filter stats` is 5% or less. This\nrequires using verbosity level 1 with `v=-v` option.\n\n## MPI mode (legacy)\n\nThis mode is legacy and we do not recommend running ABySS with it.\nTo run ABySS in the MPI mode, you need to specify the `np` parameter,\nwhich specifies the number of processes to use for the parallel MPI job.\nWithout any MPI configuration, this will allow you to use multiple cores\non a single machine. To use multiple machines for assembly, you must create\na `hostfile` for `mpirun`, which is described in the `mpirun` man page.\n\n*Do not* run `mpirun -np 8 abyss-pe`. To run ABySS with 8 threads, use\n`abyss-pe np=8`. The `abyss-pe` driver script will start the MPI\nprocess, like so: `mpirun -np 8 ABYSS-P`.\n\nThe paired-end assembly stage is multithreaded, but must run on a\nsingle machine. The number of threads to use may be specified with the\nparameter `j`. The default value for `j` is the value of `np`.\n\nExamples\n================================================================================\n\n## Assemble a small synthetic data set\n\n	wget http://www.bcgsc.ca/platform/bioinfo/software/abyss/releases/1.3.4/test-data.tar.gz\n	tar xzvf test-data.tar.gz\n	abyss-pe k=25 name=test B=1G \\n		in='test-data/reads1.fastq test-data/reads2.fastq'\n\nCalculate assembly contiguity statistics:\n\n	abyss-fac test-unitigs.fa test-contigs.fa test-scaffolds.fa\n\n## Assembling a paired-end library\n\nTo assemble paired reads in two files named `reads1.fa` and\n`reads2.fa` into contigs in a file named `ecoli-contigs.fa`, run the\ncommand:\n\n	abyss-pe name=ecoli k=96 B=2G in='reads1.fa reads2.fa'\n\nThe parameter `in` specifies the input files to read, which may be in\nFASTA, FASTQ, qseq, export, SRA, SAM or BAM format and compressed with\ngz, bz2 or xz and may be tarred. The assembled contigs will be stored\nin `${name}-contigs.fa` and the scaffolds will be stored in `${name}-scaffolds.fa`.\n\nA pair of reads must be named with the suffixes `/1` and `/2` to\nidentify the first and second read, or the reads may be named\nidentically. The paired reads may be in separate files or interleaved\nin a single file.\n\nReads without mates should be placed in a file specified by the\nparameter `se` (single-end). Reads without mates in the paired-end\nfiles will slow down the paired-end assembler considerably during the\n`abyss-fixmate` stage.\n\n## Assembling multiple libraries\n\nThe distribution of fragment sizes of each library is calculated\nempirically by aligning paired reads to the contigs produced by the\nsingle-end assembler, and the distribution is stored in a file with\nthe extension `.hist`, such as `ecoli-3.hist`. The N50 of the\nsingle-end assembly must be well over the fragment-size to obtain an\naccurate empirical distribution.\n\nHere's an example scenario of assembling a data set with two different\nfragment libraries and single-end reads. Note that the names of the libraries\n(`pea` and `peb`) are arbitrary.\n\n * Library `pea` has reads in two files,\n   `pea_1.fa` and `pea_2.fa`.\n * Library `peb` has reads in two files,\n   `peb_1.fa` and `peb_2.fa`.\n * Single-end reads are stored in two files, `se1.fa` and `se2.fa`.\n\nThe command line to assemble this example data set is:\n\n	abyss-pe k=96 B=2G name=ecoli lib='pea peb' \\n		pea='pea_1.fa pea_2.fa' peb='peb_1.fa peb_2.fa' \\n		se='se1.fa se2.fa'\n\nThe empirical distribution of fragment sizes will be stored in two\nfiles named `pea-3.hist` and `peb-3.hist`. These files may be\nplotted to check that the empirical distribution agrees with the\nexpected distribution. The assembled contigs will be stored in\n`${name}-contigs.fa` and the scaffolds will be stored in `${name}-scaffolds.fa`.\n\n## Scaffolding\n\nLong-distance mate-pair libraries may be used to scaffold an assembly.\nSpecify the names of the mate-pair libraries using the parameter `mp`.\nThe scaffolds will be stored in the file `${name}-scaffolds.fa`.\nHere's an example of assembling a data set with two paired-end\nlibraries and two mate-pair libraries. Note that the names of the libraries\n(`pea`, `peb`, `mpa`, `mpb`) are arbitrary.\n\n	abyss-pe k=96 B=2G name=ecoli lib='pea peb' mp='mpc mpd' \\n		pea='pea_1.fa pea_2.fa' peb='peb_1.fa peb_2.fa' \\n		mpc='mpc_1.fa mpc_2.fa' mpd='mpd_1.fa mpd_2.fa'\n\nThe mate-pair libraries are used only for scaffolding and do not\ncontribute towards the consensus sequence.\n\n## Scaffolding with linked reads\n\nABySS can scaffold using linked reads from 10x Genomics Chromium. The barcodes must first be extracted from the read sequences and added to the `BX:Z` tag of the FASTQ header, typically using the `longranger basic` command of [Long Ranger](https://support.10xgenomics.com/genome-exome/software/overview/welcome) or [EMA preproc](https://github.com/arshajii/ema#readme). The linked reads are used to correct assembly errors, which requires that [Tigmint](https://github.com/bcgsc/tigmint). The linked reads are also used for scaffolding, which requires [ARCS](https://github.com/bcgsc/arcs). See [Dependencies](#dependencies) for installation instructions.\n\nABySS can combine paired-end, mate-pair, and linked-read libraries. The `pe` and `lr` libraries will be used to build the de Bruijn graph. The `mp` libraries will be used for paired-end/mate-pair scaffolding. The `lr` libraries will be used for misassembly correction using Tigmint and scaffolding using ARCS.\n\n	abyss-pe k=96 B=2G name=hsapiens \\n		pe='pea' pea='lra.fastq.gz' \\n		mp='mpa' mpa='lra.fastq.gz' \\n		lr='lra' lra='lra.fastq.gz'\n\nABySS performs better with a mixture of paired-end, mate-pair, and linked reads, but it is possible to assemble only linked reads using ABySS, though this mode of operation is experimental.\n\n	abyss-pe k=96 name=hsapiens lr='lra' lra='lra.fastq.gz'\n\n## Rescaffolding with long sequences\n\nLong sequences such as RNA-Seq contigs can be used to rescaffold an\nassembly. Sequences are aligned using BWA-MEM to the assembled\nscaffolds. Additional scaffolds are then formed between scaffolds that\ncan be linked unambiguously when considering all BWA-MEM alignments.\n\nSimilar to scaffolding, the names of the datasets can be specified with\nthe `long` parameter. These scaffolds will be stored in the file\n`${name}-long-scaffs.fa`. The following is an example of an assembly with PET, MPET and an RNA-Seq assembly. Note that the names of the libraries are arbitrary.\n\n	abyss-pe k=96 B=2G name=ecoli lib='pe1 pe2' mp='mp1 mp2' long='longa' \\n		pe1='pe1_1.fa pe1_2.fa' pe2='pe2_1.fa pe2_2.fa' \\n		mp1='mp1_1.fa mp1_2.fa' mp2='mp2_1.fa mp2_2.fa' \\n		longa='longa.fa'\n\n## Assembling using a paired de Bruijn graph\n\nAssemblies may be performed using a _paired de Bruijn graph_ instead\nof a standard de Bruijn graph. In paired de Bruijn graph mode, ABySS\nuses _k-mer pairs_ in place of k-mers, where each k-mer pair consists of\ntwo equal-size k-mers separated by a fixed distance. A k-mer pair\nis functionally similar to a large k-mer spanning the breadth of the k-mer\npair, but uses less memory because the sequence in the gap is not stored.\nTo assemble using paired de Bruijn graph mode, specify both individual\nk-mer size (`K`) and k-mer pair span (`k`). For example, to assemble E.\ncoli with a individual k-mer size of 16 and a k-mer pair span of 96:\n\n	abyss-pe name=ecoli K=16 k=96 in='reads1.fa reads2.fa'\n\nIn this example, the size of the intervening gap between k-mer pairs is\n64 bp (96 - 2\*16). Note that the `k` parameter takes on a new meaning\nin paired de Bruijn graph mode. `k` indicates kmer pair span in\npaired de Bruijn graph mode (when `K` is set), whereas `k` indicates\nk-mer size in standard de Bruijn graph mode (when `K` is not set).\n\n## Assembling a strand-specific RNA-Seq library\n\nStrand-specific RNA-Seq libraries can be assembled such that the\nresulting unitigs, contigs and scaffolds are oriented correctly with\nrespect to the original transcripts that were sequenced. In order to\nrun ABySS in strand-specific mode, the `SS` parameter must be used as\nin the following example:\n\n	abyss-pe name=SS-RNA B=2G k=96 in='reads1.fa reads2.fa' SS=--SS\n\nThe expected orientation for the read sequences with respect to the\noriginal RNA is RF. i.e. the first read in a read pair is always in\nreverse orientation.\n\nOptimizing the parameters k and kc\n================================================================================\n\nIt is standard practice when running ABySS to run multiple assemblies\nto find the optimal values for the `k` and `kc` parameters. `k` determines\nthe k-mer size in the de Bruijn Graph, and `kc` is the k-mer minimum coverage\nmultiplicity cutoff, which filters out erroneous k-mers. The range in which `k`\nshould be tested depends on the read size and read coverage.\n\nA rough indicator is, for 2x150bp reads and 40x coverage, the right `k` value is often around 70 to 90. For 2x250bp reads and 40x coverage, the right value might be around 110 to 140.\n\nFor `kc`, 2 is most often a good value, but can go as high as 4.\n\nThe following shell snippet will assemble for `k` values 2 and 3, and every eighth value of `k` from 50 to 90. In the end, we calculate the contiguity statistics, as a proxy for identifying the optimal assembly. Other metrics can be used, as needed.\n\n	for kc in 2 3; do\n		for k in `seq 50 8 90`; do\n			mkdir k${k}-kc${kc}\n			abyss-pe -C k${k}-kc${kc} name=ecoli B=2G k=$k kc=$kc in=../reads.fa\n		done\n	done\n	abyss-fac k*/ecoli-scaffolds.fa\n\nThe default maximum value for `k` is 192. This limit may be changed at\ncompile time using the `--enable-maxk` option of configure. It may be\ndecreased to 32 to decrease memory usage or increased to larger values.\n\nRunning ABySS on a cluster\n================================================================================\n\nABySS integrates well with cluster job schedulers, such as:\n\n * SGE (Sun Grid Engine)\n * Portable Batch System (PBS)\n * Load Sharing Facility (LSF)\n * IBM LoadLeveler\n\nFor example, to submit an array of jobs to assemble every eighth value of\n`k` between 50 and 90 using 64 processes for each job:\n\n	qsub -N ecoli -pe openmpi 64 -t 50-90:8 \\n		<<<'mkdir k$SGE_TASK_ID && abyss-pe -C k$SGE_TASK_ID in=/data/reads.fa'\n\nUsing the DIDA alignment framework\n================================================================================\n\nABySS supports the use of DIDA (Distributed Indexing Dispatched Alignment),\nan MPI-based framework for computing sequence alignments in parallel across\nmultiple machines. The DIDA software must be separately downloaded and\ninstalled from http://www.bcgsc.ca/platform/bioinfo/software/dida. In\ncomparison to the standard ABySS alignment stages which are constrained\nto a single machine, DIDA offers improved performance and the ability to\nscale to larger targets. Please see the DIDA section of the abyss-pe man\npage (in the `doc` subdirectory) for details on usage.\n\nAssembly Parameters\n================================================================================\n\nParameters of the driver script, `abyss-pe`\n\n * `a`: maximum number of branches of a bubble [`2`]\n * `b`: maximum length of a bubble (bp) [`""""`]\n * `B`: Bloom filter size (e.g. ""100M"")\n * `c`: minimum mean k-mer coverage of a unitig [`sqrt(median)`]\n * `d`: allowable error of a distance estimate (bp) [`6`]\n * `e`: minimum erosion k-mer coverage [`round(sqrt(median))`]\n * `E`: minimum erosion k-mer coverage per strand [1 if `sqrt(median) > 2` else 0]\n * `G`: genome size, used to calculate NG50\n * `H`: number of Bloom filter hash functions [`4`]\n * `j`: number of threads [`2`]\n * `k`: size of k-mer (when `K` is not set) or the span of a k-mer pair (when `K` is set)\n * `kc`: minimum k-mer count threshold for Bloom filter assembly [`2`]\n * `K`: the length of a single k-mer in a k-mer pair (bp)\n * `l`: minimum alignment length of a read (bp) [`40`]\n * `m`: minimum overlap of two unitigs (bp) [`0` (interpreted as `k - 1`) if `mp` is provided or if `k<=50`, otherwise `50`]\n * `n`: minimum number of pairs required for building contigs [`10`]\n * `N`: minimum number of pairs required for building scaffolds [`15-20`]\n * `np`: number of MPI processes [`1`]\n * `p`: minimum sequence identity of a bubble [`0.9`]\n * `q`: minimum base quality [`3`]\n * `s`: minimum unitig size required for building contigs (bp) [`1000`]\n * `S`: minimum contig size required for building scaffolds (bp) [`100-5000`]\n * `t`: maximum length of blunt contigs to trim [`k`]\n * `v`: use `v=-v` for verbose logging, `v=-vv` for extra verbose\n * `x`: spaced seed (Bloom filter assembly only)\n * `lr_s`: minimum contig size required for building scaffolds with linked reads (bp) [`S`]\n * `lr_n`: minimum number of barcodes required for building scaffolds with linked reads [`10`]\n\nEnvironment variables\n================================================================================\n\n`abyss-pe` configuration variables may be set on the command line or from the environment, for example with `export k=96`. It can happen that `abyss-pe` picks up such variables from your environment that you had not intended, and that can cause trouble. To troubleshoot that situation, use the `abyss-pe env` command to print the values of all the `abyss-pe` configuration variables:\n\n	abyss-pe env [options]\n\nABySS programs\n================================================================================\n\n`abyss-pe` is a driver script implemented as a Makefile. Any option of\n`make` may be used with `abyss-pe`. Particularly useful options are:\n\n * `-C dir`, `--directory=dir`\n   Change to the directory `dir` and store the results there.\n * `-n`, `--dry-run`\n   Print the commands that would be executed, but do not execute\n   them.\n\n`abyss-pe` uses the following programs, which must be found in your\n`PATH`:\n\n * `ABYSS`: de Bruijn graph assembler\n * `ABYSS-P`: parallel (MPI) de Bruijn graph assembler\n * `AdjList`: find overlapping sequences\n * `DistanceEst`: estimate the distance between sequences\n * `MergeContigs`: merge sequences\n * `MergePaths`: merge overlapping paths\n * `Overlap`: find overlapping sequences using paired-end reads\n * `PathConsensus`: find a consensus sequence of ambiguous paths\n * `PathOverlap`: find overlapping paths\n * `PopBubbles`: remove bubbles from the sequence overlap graph\n * `SimpleGraph`: find paths through the overlap graph\n * `abyss-fac`: calculate assembly contiguity statistics\n * `abyss-filtergraph`: remove shim contigs from the overlap graph\n * `abyss-fixmate`: fill the paired-end fields of SAM alignments\n * `abyss-map`: map reads to a reference sequence\n * `abyss-scaffold`: scaffold contigs using distance estimates\n * `abyss-todot`: convert graph formats and merge graphs\n * `abyss-rresolver`: resolve repeats using short reads\n\nThis [flowchart](https://github.com/bcgsc/abyss/blob/master/doc/flowchart.pdf) shows the ABySS assembly pipeline and its intermediate files.\n\nExport to SQLite Database\n================================================================================\n\nABySS has a built-in support for SQLite database to export log values into a SQLite file and/or `.csv` files at runtime.\n\n## Database parameters\nOf `abyss-pe`:\n * `db`: path to SQLite repository file [`$(name).sqlite`]\n * `species`: name of species to archive [ ]\n * `strain`: name of strain to archive [ ]\n * `library`: name of library to archive [ ]\n\nFor example, to export data of species 'Ecoli', strain 'O121' and library 'pea' into your SQLite database repository named '/abyss/test.sqlite':\n\n	abyss-pe db=/abyss/test.sqlite species=Ecoli strain=O121 library=pea [other options]\n\n## Helper programs\n\nFound in your `path`:\n\n * `abyss-db-txt`: create a flat file showing entire repository at a glance\n * `abyss-db-csv`: create `.csv` table(s) from the repository\n\nUsage:\n\n    abyss-db-txt /your/repository\n    abyss-db-csv /your/repository program(s)\n\nFor example,\n\n	abyss-db-txt repo.sqlite\n	abyss-db-csv repo.sqlite DistanceEst\n	abyss-db-csv repo.sqlite DistanceEst abyss-scaffold\n	abyss-db-csv repo.sqlite --all\n\nCitation\n================================================================================\n\n## [ABySS 2.0](http://doi.org/10.1101/gr.214346.116)\n\nShaun D Jackman, Benjamin P Vandervalk, Hamid Mohamadi, Justin Chu, Sarah Yeo, S Austin Hammond, Golnaz Jahesh, Hamza Khan, Lauren Coombe, René L Warren, and Inanc Birol (2017).\n**ABySS 2.0: Resource-efficient assembly of large genomes using a Bloom filter**.\n*Genome research*, 27(5), 768-777.\n[doi:10.1101/gr.214346.116](http://doi.org/10.1101/gr.214346.116)\n\n## [ABySS](http://genome.cshlp.org/content/19/6/1117)\n\nSimpson, Jared T., Kim Wong, Shaun D. Jackman, Jacqueline E. Schein,\nSteven JM Jones, and Inanc Birol (2009).\n**ABySS: a parallel assembler for short read sequence data**.\n*Genome research*, 19(6), 1117-1123.\n[doi:10.1101/gr.089532.108](http://dx.doi.org/10.1101/gr.089532.108)\n\nRelated Publications\n================================================================================\n\n## [RResolver](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-022-04790-z)\n\nVladimir Nikolić, Amirhossein Afshinfard, Justin Chu, Johnathan Wong, Lauren Coombe, Ka Ming Nip, René L. Warren & Inanç Birol (2022).\n**RResolver: efficient short-read repeat resolution within ABySS**.\n*BMC Bioinformatics* 23, Article number: 246 (2022).\n[doi:10.1186/s12859-022-04790-z](https://doi.org/10.1186/s12859-022-04790-z)\n\n## [Trans-ABySS](http://www.nature.com/nmeth/journal/v7/n11/abs/nmeth.1517.html)\n\nRobertson, Gordon, Jacqueline Schein, Readman Chiu, Richard Corbett,\nMatthew Field, Shaun D. Jackman, Karen Mungall, et al (2010).\n**De novo assembly and analysis of RNA-seq data**.\n*Nature methods*, 7(11), 909-912.\n[doi:10.1038/10.1038/nmeth.1517](http://dx.doi.org/10.1038/nmeth.1517)\n\n## [ABySS-Explorer](http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5290690)\n\nNielsen, Cydney B., Shaun D. Jackman, Inanc Birol, and Steven JM Jones (2009).\n**ABySS-Explorer: visualizing genome sequence assemblies**.\n*IEEE Transactions on Visualization and Computer Graphics*, 15(6), 881-888.\n[doi:10.1109/TVCG.2009.116](http://dx.doi.org/10.1109/TVCG.2009.116)\n\nSupport\n================================================================================\n\n[Create a new issue on GitHub.](https://github.com/bcgsc/abyss/issues)\n\n[Ask a question on Biostars.](https://www.biostars.org/tag/abyss/)\n\nSubscribe to the [ABySS mailing list](http://groups.google.com/group/abyss-users), <abyss-users@googlegroups.com>.\n\nFor questions related to transcriptome assembly, contact the [Trans-ABySS mailing list](http://groups.google.com/group/trans-abyss), <trans-abyss@googlegroups.com>.\n\nAuthors\n================================================================================\n\n+ **[Shaun Jackman](http://sjackman.ca)** - [GitHub/sjackman](https://github.com/sjackman) - [@sjackman](https://twitter.com/sjackman)\n+ **Tony Raymond** - [GitHub/traymond](https://github.com/traymond)\n+ **Ben Vandervalk** - [GitHub/benvvalk ](https://github.com/benvvalk)\n+ **Jared Simpson** - [GitHub/jts](https://github.com/jts)\n+ **Johnathan Wong** - [GitHub/jowong4](https://github.com/jowong4)\n+ **Vladimir Nikolić** - [GitHub/vlad0x00](https://github.com/vlad0x00)\n\nSupervised by [**Dr. Inanc Birol**](http://www.bcgsc.ca/faculty/inanc-birol).\n\nCopyright 2016-present Canada's Michael Smith Genome Sciences Centre\n",304,bioinformatics,C++,8,C++,C,Haskell,Makefile,Shell,Perl,M4,Dockerfile,,,,,,,,,,,,,,,,,,,,,97,29,67,1,32,46,166,63881,103,324,320,4,4906b9967854112619bbaad955aaf3b937526d3a,Type changes required for LTO (#479),2024-07-18T01:54:45Z,JW,34543031+jwcodee@users.noreply.github.com,jwcodee,02.03.2007,"* Release version 2.3.7\r\n\r\n    General:\r\n	* Fix build on macOS-12 and macOS-13 (Thanks @parham-k!)",02.03.2007,Lauren Coombe,,lcoombe,Other,abyss,bcgsc,32,bloom-filter,c-plus-plus,science,bioinformatics,assembler,scaffold,genome,openmp,mpi,,,,,,,,,,,,/bcgsc/abyss,69,25,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/BaranziniLab/KG_RAG,https://github.com/BaranziniLab/KG_RAG,0,,,0,0,0,0,0,0,1,0,0,0,0,Empower Large Language Models (LLM) using Knowledge Graph based Retrieval-Augmented Generation (KG-RAG) for knowledge intensive tasks,"<p align=""center"">\n  <img src=""https://github.com/BaranziniLab/KG_RAG/assets/42702311/0b2f5b42-761e-4d5b-8d6f-77c8b965f017"" width=""450"">\n</p>\n\n\n\n\n## Table of Contents\n[What is KG-RAG](https://github.com/BaranziniLab/KG_RAG#what-is-kg-rag)\n\n[Example use case of KG-RAG](https://github.com/BaranziniLab/KG_RAG#example-use-case-of-kg-rag)\n - [Prompting GPT without KG-RAG](https://github.com/BaranziniLab/KG_RAG#without-kg-rag)  \n - [Prompting GPT with KG-RAG](https://github.com/BaranziniLab/KG_RAG#with-kg-rag)\n - [Example notebook for KG-RAG with GPT](https://github.com/BaranziniLab/KG_RAG/blob/main/notebooks/kg_rag_based_gpt_prompts.ipynb)\n\n[How to run KG-RAG](https://github.com/BaranziniLab/KG_RAG#how-to-run-kg-rag)\n - [Step 1: Clone the repo](https://github.com/BaranziniLab/KG_RAG#step-1-clone-the-repo)\n - [Step 2: Create a virtual environment](https://github.com/BaranziniLab/KG_RAG#step-2-create-a-virtual-environment)\n - [Step 3: Install dependencies](https://github.com/BaranziniLab/KG_RAG#step-3-install-dependencies)\n - [Step 4: Update config.yaml](https://github.com/BaranziniLab/KG_RAG#step-4-update-configyaml)\n - [Step 5: Run the setup script](https://github.com/BaranziniLab/KG_RAG#step-5-run-the-setup-script)\n - [Step 6: Run KG-RAG from your terminal](https://github.com/BaranziniLab/KG_RAG#step-6-run-kg-rag-from-your-terminal)\n    - [Using GPT](https://github.com/BaranziniLab/KG_RAG#using-gpt)\n    - [Using GPT interactive mode](https://github.com/BaranziniLab/KG_RAG/blob/main/README.md#using-gpt-interactive-mode)\n    - [Using Llama](https://github.com/BaranziniLab/KG_RAG#using-llama)\n    - [Using Llama interactive mode](https://github.com/BaranziniLab/KG_RAG/blob/main/README.md#using-llama-interactive-mode)\n  - [Command line arguments for KG-RAG](https://github.com/BaranziniLab/KG_RAG?tab=readme-ov-file#command-line-arguments-for-kg-rag)\n  \n[Citation](https://github.com/BaranziniLab/KG_RAG/blob/main/README.md#citation)\n\n## What is KG-RAG?\n\nKG-RAG stands for Knowledge Graph-based Retrieval Augmented Generation.\n\n### Start by watching the video of KG-RAG\n\n<video src=""https://github.com/BaranziniLab/KG_RAG/assets/42702311/86e5b8a3-eb58-4648-95a4-271e9c69b4ed"" controls=""controls"" style=""max-width: 730px;"">\n</video>\n\nIt is a task agnostic framework that combines the explicit knowledge of a Knowledge Graph (KG) with the implicit knowledge of a Large Language Model (LLM). Here is the [arXiv preprint](https://arxiv.org/abs/2311.17330) of the work.\n\nHere, we utilize a massive biomedical KG called [SPOKE](https://spoke.ucsf.edu/) as the provider for the biomedical context. SPOKE has incorporated over 40 biomedical knowledge repositories from diverse domains, each focusing on biomedical concept like genes, proteins, drugs, compounds, diseases, and their established connections. SPOKE consists of more than 27 million nodes of 21 different types and 53 million edges of 55 types [[Ref](https://doi.org/10.1093/bioinformatics/btad080)]\n\n\nThe main feature of KG-RAG is that it extracts ""prompt-aware context"" from SPOKE KG, which is defined as: \n\n**the minimal context sufficient enough to respond to the user prompt.** \n\nHence, this framework empowers a general-purpose LLM by incorporating an optimized domain-specific 'prompt-aware context' from a biomedical KG.\n\n## Example use case of KG-RAG\nFollowing snippet shows the news from FDA [website](https://www.fda.gov/drugs/news-events-human-drugs/fda-approves-treatment-weight-management-patients-bardet-biedl-syndrome-aged-6-or-older) about the drug **""setmelanotide""** approved by FDA for weight management in patients with *Bardet-Biedl Syndrome*\n\n<img src=""https://github.com/BaranziniLab/KG_RAG/assets/42702311/fc4d0b8d-0edb-461d-86c5-9d0d191bd97d"" width=""600"" height=""350"">\n\n### Ask GPT-4 about the above drug:\n\n### WITHOUT KG-RAG\n\n*Note: This example was run using KG-RAG v0.3.0. We are prompting GPT from the terminal, NOT from the chatGPT browser. Temperature parameter is set to 0 for all the analysis. Refer [this](https://github.com/BaranziniLab/KG_RAG/blob/main/config.yaml) yaml file for parameter setting*\n\n<video src=""https://github.com/BaranziniLab/KG_RAG/assets/42702311/dbabb812-2a8a-48b6-9785-55b983cb61a4"" controls=""controls"" style=""max-width: 730px;"">\n</video>\n\n### WITH KG-RAG\n\n*Note: This example was run using KG-RAG v0.3.0. Temperature parameter is set to 0 for all the analysis. Refer [this](https://github.com/BaranziniLab/KG_RAG/blob/main/config.yaml) yaml file for parameter setting*\n\n<video src=""https://github.com/BaranziniLab/KG_RAG/assets/42702311/acd08954-a496-4a61-a3b1-8fc4e647b2aa"" controls=""controls"" style=""max-width: 730px;"">\n</video>\n\nYou can see that, KG-RAG was able to give the correct information about the FDA approved [drug](https://www.fda.gov/drugs/news-events-human-drugs/fda-approves-treatment-weight-management-patients-bardet-biedl-syndrome-aged-6-or-older).\n\n## How to run KG-RAG\n\n**Note: At the moment, KG-RAG is specifically designed for running prompts related to Diseases. We are actively working on improving its versatility.**\n\n### Step 1: Clone the repo\n\nClone this repository. All Biomedical data used in the paper are uploaded to this repository, hence you don't have to download that separately.\n\n### Step 2: Create a virtual environment\nNote: Scripts in this repository were run using python 3.10.9\n```\nconda create -n kg_rag python=3.10.9\nconda activate kg_rag\ncd KG_RAG\n```\n\n### Step 3: Install dependencies\n\n```\npip install -r requirements.txt\n```\n\n### Step 4: Update config.yaml \n\n[config.yaml](https://github.com/BaranziniLab/KG_RAG/blob/main/config.yaml) holds all the necessary information required to run the scripts in your machine. Make sure to populate [this](https://github.com/BaranziniLab/KG_RAG/blob/main/config.yaml) yaml file accordingly.\n\nNote: There is another yaml file called [system_prompts.yaml](https://github.com/BaranziniLab/KG_RAG/blob/main/system_prompts.yaml). This is already populated and it holds all the system prompts used in the KG-RAG framework.\n\n### Step 5: Run the setup script\nNote: Make sure you are in KG_RAG folder\n\nSetup script runs in an interactive fashion.\n\nRunning the setup script will: \n\n- create disease vector database for KG-RAG\n- download Llama model in your machine (optional, you can skip this and that is totally fine)\n\n```\npython -m kg_rag.run_setup\n```\n\n### Step 6: Run KG-RAG from your terminal\nNote: Make sure you are in KG_RAG folder\n\nYou can run KG-RAG using GPT and Llama model. \n\n#### Using GPT\n\n```\n# GPT_API_TYPE='azure'\npython -m kg_rag.rag_based_generation.GPT.text_generation -g <your favorite gpt model - ""gpt-4"" or ""gpt-35-turbo"">\n# GPT_API_TYPE='openai'\npython -m kg_rag.rag_based_generation.GPT.text_generation -g <your favorite gpt model - ""gpt-4"" or ""gpt-3.5-turbo"">\n```\n\nExample:\n\nNote: The following example was run on AWS p3.8xlarge EC2 instance and using KG-RAG v0.3.0.\n\n<video src=""https://github.com/BaranziniLab/KG_RAG/assets/42702311/defcbff7-e777-4db6-b028-10f54c76b234"" controls=""controls"" style=""max-width: 730px;"">\n</video>\n\n#### Using GPT interactive mode\n\nThis allows the user to go over each step of the process in an interactive fashion\n\n```\n# GPT_API_TYPE='azure'\npython -m kg_rag.rag_based_generation.GPT.text_generation -i True -g <your favorite gpt model - ""gpt-4"" or ""gpt-35-turbo"">\n# GPT_API_TYPE='openai'\npython -m kg_rag.rag_based_generation.GPT.text_generation -i True -g <your favorite gpt model - ""gpt-4"" or ""gpt-3.5-turbo"">\n```\n\n#### Using Llama\nNote: If you haven't downloaded Llama during [setup](https://github.com/BaranziniLab/KG_RAG#step-5-run-the-setup-script) step, then when you run the following, it may take sometime since it will download the model first.\n\n```\npython -m kg_rag.rag_based_generation.Llama.text_generation -m <method-1 or method2, if nothing is mentioned it will take 'method-1'>\n```\n\nExample:\n\nNote: The following example was run on AWS p3.8xlarge EC2 instance and using KG-RAG v0.3.0.\n\n<video src=""https://github.com/BaranziniLab/KG_RAG/assets/42702311/94bda923-dafb-451a-943a-1d7c65f3ffd4"" controls=""controls"" style=""max-width: 730px;"">\n</video>\n\n#### Using Llama interactive mode\n\nThis allows the user to go over each step of the process in an interactive fashion\n\n```\npython -m kg_rag.rag_based_generation.Llama.text_generation -i True -m <method-1 or method2, if nothing is mentioned it will take 'method-1'>\n```\n\n### Command line arguments for KG-RAG\n\n| Argument | Default Value         | Definition                                               | Allowed Options                     | Notes                                                            |\n|----------|-----------------|----------------------------------------------------------|------------------------------------|------------------------------------------------------------------|\n| -g       | gpt-35-turbo    | GPT model selection                                      | gpt models provided by OpenAI     | Use only for GPT models                                          |\n| -i       | False           | Flag for interactive mode (shows step-by-step)           | True or False                      | Can be used for both GPT and Llama models                        |\n| -e       | False           | Flag for showing evidence of association from the graph | True or False                      | Can be used for both GPT and Llama models                        |\n| -m       | method-1        | Which tokenizer method to use                            | method-1 or method-2. method-1 uses 'AutoTokenizer' and method-2 uses 'LlamaTokenizer' and with an additional 'legacy' flag set to False while initiating the tokenizer              | Use only for Llama models|\n\n\n\n## Citation\n\n```\n@article{soman2023biomedical,\n  title={Biomedical knowledge graph-enhanced prompt generation for large language models},\n  author={Soman, Karthik and Rose, Peter W and Morris, John H and Akbas, Rabia E and Smith, Brett and Peetoom, Braian and Villouta-Reyes, Catalina and Cerono, Gabriel and Shi, Yongmei and Rizk-Jackson, Angela and others},\n  journal={arXiv preprint arXiv:2311.17330},\n  year={2023}\n}\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",536,bioinformatics,Jupyter Notebook,2,Python,Jupyter Notebook,,,,,,,,,,,,,,,,,,,,,,,,,,,12,1,10,1,1,4,0,10332,72,15,13,2,fc0c4bca37b047e2d0f619739f7d669a77e5d2c5,amended utility file to accumulate context from node_hits,2024-07-12T04:32:01Z,Karthik Soman,karthi.soman@gmail.com,karthiksoman,Now run KG-RAG with flexible command line args!,This release has following features:\r\n\r\n1. Added the provision to install Llama models with 'LlamaTokenizer' and 'legacy'=False option. We name this as 'method-2' in this repo.\r\n\r\n2. Run KG-RAG using command line args in a flexible fashion.\r\n(A) To run in interactive mode: -i True \r\n       Default value : False\r\n(B) To select gpt-models : -g gpt-4\r\n       Default value : gpt-35-turbo\r\n(C) To select method-2 to run Llama : -m method-2\r\n       Default value : method-1\r\n\r\n3. Demo videos of README is updated using these updated command line args,v0.3.0,karthik-soman,,karthiksoman,Apache License 2.0,KG_RAG,BaranziniLab,3,gpt,knowledge-graph,large-language-models,llama2,llm,rag,retrieval-augmented-generation,biomedical-applications,biomedical-informatics,bioinformatics,gpt35turbo,gpt4,llama,prompt-engineering,bert-models,bioinformatics-algorithms,context-aware,knowledge-base,sentence-transformers,prompt-tuning,/BaranziniLab/KG_RAG,3,15,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/axondeepseg/axondeepseg,https://github.com/axondeepseg/axondeepseg,0,AI tool,,0,1,0,0,0,0,1,0,0,0,0,Axon/Myelin segmentation using Deep Learning,"<img src=""https://github.com/neuropoly/axondeepseg/blob/master/docs/source/_static/logo_ads-alpha.png"" width=""385"">\n\n[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/neuropoly/axondeepseg/master?filepath=notebooks%2Fgetting_started.ipynb)\n[![Build Status](https://github.com/neuropoly/axondeepseg/actions/workflows/run_tests.yaml/badge.svg)](https://github.com/neuropoly/axondeepseg/actions/workflows/run_tests.yaml)\n[![Documentation Status](https://readthedocs.org/projects/axondeepseg/badge/?version=stable)](http://axondeepseg.readthedocs.io/en/latest/?badge=latest)\n[![Coverage Status](https://coveralls.io/repos/github/neuropoly/axondeepseg/badge.svg?branch=master)](https://coveralls.io/github/neuropoly/axondeepseg?branch=master)\n[![Twitter Follow](https://img.shields.io/twitter/follow/axondeepseg.svg?style=social&label=Follow)](https://twitter.com/axondeepseg)\n\nSegment axon and myelin from microscopy data using deep learning. Written in Python. Using the TensorFlow framework.\nBased on a convolutional neural network architecture. Pixels are classified as either axon, myelin or background.\n\nFor more information, see the [documentation website](http://axondeepseg.readthedocs.io/).\n\n![alt tag](https://raw.githubusercontent.com/axondeepseg/doc-figures/main/index/fig0.png)\n\n\n\n## Help\n\nWhether you are a newcomer or an experienced user, we will do our best to help and reply to you as soon as possible. Of course, please be considerate and respectful of all people participating in our community interactions.\n\n* If you encounter difficulties during installation and/or while using AxonDeepSeg, or have general questions about the project, you can start a new discussion on the [AxonDeepSeg GitHub Discussions forum](https://github.com/neuropoly/axondeepseg/discussions). We also encourage you, once you've familiarized yourself with the software, to continue participating in the forum by helping answer future questions from fellow users!\n* If you encounter bugs during installation and/or use of AxonDeepSeg, you can open a new issue ticket on the [AxonDeepSeg GitHub issues webpage](https://github.com/neuropoly/axondeepseg/issues).\n\n\n\n\n### Napari plugin\n\nA tutorial demonstrating the basic features of our plugin for Napari is hosted on YouTube, and can be viewed by clicking [this link](https://www.youtube.com/watch?v=zibDbpko6ko).\n\n## References\n\n**AxonDeepSeg**\n\n* [Lubrano et al. *Deep Active Leaning for Myelin Segmentation on Histology Data.* Montreal Artificial Intelligence and Neuroscience 2019](https://arxiv.org/abs/1907.05143) - \[[**source code**](https://github.com/neuropoly/deep-active-learning)\]\n* [Zaimi et al. *AxonDeepSeg: automatic axon and myelin segmentation from microscopy data using convolutional neural networks.* Scientific Reports 2018](https://www.nature.com/articles/s41598-018-22181-4)\n\n**Applications**\n\n* [Tabarin et al. *Deep learning segmentation (AxonDeepSeg) to generate axonal-property map from ex vivo human optic chiasm using light microscopy.* ISMRM 2019](https://www.ismrm.org/19/program_files/DP23.htm) - \[[**source code**](https://github.com/thibaulttabarin/UnAxSeg)\]\n* [Lousada et al. *Characterization of cortico-striatal myelination in the context of pathological Repetitive Behaviors.*  International Basal Ganglia Society (IBAGS) 2019](http://www.ibags2019.com/key4register/images/client/863/files/Abstractbook1405.pdf)\n* [Duval et al. *Axons morphometry in the human spinal cord.* NeuroImage 2019](https://www.sciencedirect.com/science/article/pii/S1053811918320044)\n* [Yu et al. *Model-informed machine learning for multi-component T2 relaxometry.* Medical Image Analysis 2021](https://www.sciencedirect.com/science/article/pii/S1361841520303042) - \[[**source code**](https://github.com/thomas-yu-epfl/Model_Informed_Machine_Learning)\]\n\n**Reviews**\n\n* [Riordon et al. *Deep learning with microfluidics for biotechnology.* Trends in Biotechnology 2019](https://www.sciencedirect.com/science/article/pii/S0167779918302452)\n\n## Citation\n\nIf you use this work in your research, please cite it as follows:\n\nZaimi, A., Wabartha, M., Herman, V., Antonsanti, P.-L., Perone, C. S., & Cohen-Adad, J. (2018). AxonDeepSeg: automatic axon and myelin segmentation from microscopy data using convolutional neural networks. Scientific Reports, 8(1), 3816. Link to paper: https://doi.org/10.1038/s41598-018-22181-4.\n\nCopyright (c) 2018 NeuroPoly (Polytechnique Montreal)\n\n## Licence\n\nThe MIT License (MIT)\n\nCopyright (c) 2018 NeuroPoly, École Polytechnique, Université de Montréal\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n## Contributors\n\nPierre-Louis Antonsanti, Stoyan Asenov, Mathieu Boudreau, Oumayma Bounou, Marie-Hélène Bourget, Julien Cohen-Adad, Victor Herman, Melanie Lubrano, Antoine Moevus, Christian Perone, Vasudev Sharma, Thibault Tabarin, Maxime Wabartha, Aldo Zaimi.\n",113,histology,Python,3,Python,Jupyter Notebook,Dockerfile,,,,,,,,,,,,,,,,,,,,,,,,,,263,30,225,8,36,21,196,428940,32,442,286,156,2f9ba11b2d810d79264f8adb399fdac5d372ca79,Update sphinx to fix doc build (#816),2024-07-16T15:17:53Z,Armand Collin,83031821+hermancollin@users.noreply.github.com,hermancollin,AxonDeepSeg v4.1.0,[Release notes and Changelog](https://github.com/neuropoly/axondeepseg/blob/master/CHANGELOG.md),v4.1.0,"Mathieu Boudreau, PhD",,mathieuboudreau,MIT License,axondeepseg,axondeepseg,14,deep-learning,machine-learning,myelin,axon,neuropoly,spinalcord,segmentation,electron-microscopy,microscopy,histology,convolutional-neural-networks,,,,,,,,,,/axondeepseg/axondeepseg,17,13,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/ATOMScience-org/AMPL,https://github.com/ATOMScience-org/AMPL,0.5,Based on machine learning?,1,1,1,1,1,0,0,0,0,0,0,1,"The ATOM Modeling PipeLine (AMPL) is an open-source, modular, extensible software pipeline for building and sharing models to advance in silico drug discovery.","[![License](http://img.shields.io/:license-mit-blue.svg)](https://github.com/ATOMScience-org/AMPL/blob/master/LICENSE) [![LinkedIn](https://img.shields.io/badge/linkedin-%230077B5.svg?logo=linkedin&logoColor=white)](https://www.linkedin.com/company/atomscience) \n\n| [Install](#install) | [Docker](#install-with-docker) | [Tutorials](#ampl-tutorials) |  [Features](#ampl-features) | [Pipeline parameters](atomsci/ddm/docs/PARAMETERS.md) | [Docs](https://ampl.readthedocs.io/en/latest/) |\n\n# ATOM Modeling PipeLine (AMPL) for Drug Discovery\nAn open-source, end-to-end software pipeline for data curation, model building, and molecular property prediction to advance in silico drug discovery.\n\n*Created by the [Accelerating Therapeutics for Opportunities in Medicine (ATOM) Consortium](https://atomscience.org)*\n\n<img src=""atomsci/ddm/docs/ATOM_cymatics_black_wordmark.jpg"" width=""370"" height=""100"" class=""center""></img>\n\n\n![Static Badge](https://img.shields.io/badge/Announcement-1.6.1-blue)![Static Badge](https://img.shields.io/badge/Announcement-1.6.1-blue)![Static Badge](https://img.shields.io/badge/Announcement-1.6.1-blue)![Static Badge](https://img.shields.io/badge/Announcement-1.6.1-blue)![Static Badge](https://img.shields.io/badge/Announcement-1.6.1-blue)![Static Badge](https://img.shields.io/badge/Announcement-1.6.1-blue)\n\n## Check out our new tutorial series that walks through AMPL's end-to-end modeling pipeline to build a machine learning model! View them in our [docs](https://ampl.readthedocs.io/en/latest/) or as Jupyter notebooks in our [repo](https://github.com/ATOMScience-org/AMPL/tree/master/atomsci/ddm/examples/tutorials).\n\nThe ATOM Modeling PipeLine (AMPL) extends the functionality of DeepChem and supports an array of machine learning and molecular featurization tools to predict key potency, safety and pharmacokinetic-relevant parameters. AMPL has been benchmarked on a large collection of pharmaceutical datasets covering a wide range of parameters. This is a living software project with active development. Check back for continued updates. Feedback is welcomed and appreciated, and the project is open to contributions! An [article describing the AMPL project](https://pubs.acs.org/doi/abs/10.1021/acs.jcim.9b01053) was published in JCIM. For those without access to JCIM, a preprint of the article is available on [ArXiv](http://arxiv.org/abs/1911.05211). [Documentation is available here.](https://ampl.readthedocs.io/en/latest/pipeline.html)\n\n\n---\n## Table of contents\n- [Install](#install)\n   - [Quick Install](#installation-quick-summary)\n   - [Jupyter kernel](#create-jupyter-notebook-kernel-optional)\n   - [Docker](#install-with-docker)\n   - [Uninstall](#uninstall)\n- [AMPL Features](#ampl-features)\n- [Running AMPL](#running-ampl)\n- [Tests](#tests)\n- [Advanced AMPL usage](#advanced-ampl-usage)\n- [Advanced testing](#advanced-testing)\n- [Tutorials](#ampl-tutorials)\n- [Development](#development)\n- [Project information](#project-information)  \n- [Suggestions or Report Issues](#support-suggestions-or-report-issues)\n\n## Useful links\n- [Pipeline parameters (options)](atomsci/ddm/docs/PARAMETERS.md)\n- [Library documentation](https://ampl.readthedocs.io/en/latest/index.html)  \n---\n## Install\nAMPL 1.6 supports Python 3.9 CPU or CUDA-enabled machines using CUDA 11.8 on Linux. All other systems are experimental. For a quick install summary, see [here](#install-summary). We do not support other CUDA versions because there are multiple ML package dependency conflicts that can occur. For more information you can look at [DeepChem](https://deepchem.readthedocs.io/en/latest/get_started/installation.html), [TensorFlow](https://www.tensorflow.org/install/pip), [PyTorch](https://pytorch.org/get-started/locally/), [DGL](https://www.dgl.ai/pages/start.html) or [Jax](https://github.com/google/jax#installation).\n\n### Create pip environment\n\n#### 1. Create a virtual env with Python 3.9 \nMake sure to create your virtual env in a convenient directory that has at least 12Gb space.\n\nGo to the directory where the new environment directory be installed in. Define an environment variable - ""ENVROOT"".\n\n```bash\nexport ENVROOT=~/workspace # for LLNL LC users, use your workspace\nor\nexport ENVROOT=~ # or the directory as your environment root\ncd $ENVROOT\n```\n\n> *We use ""workspace"" and ""atomsci-env"" as an example here.*\n\n```bash\n# LLNL only: \n# module load python/3.9.12\ncd $ENVROOT\npython3.9 -m venv atomsci-env\n```\n\n#### 2. Activate the environment\n```bash\nsource $ENVROOT/atomsci-env/bin/activate\n```\n\n#### 3. Update pip\n```bash\npip install pip --upgrade\n```\n\n#### 4. Clone AMPL repository\n```bash\ngit clone https://github.com/ATOMScience-org/AMPL.git \n```\n\n#### 5. Install pip requirements\nDepending on system performance, creating the environment can take some time.\n> ***Note:*** *Based on which environment (CPU or CUDA) to run on, only run one of the following:*\n\n- CPU-only installation:\n```bash\ncd AMPL/pip\npip install -r cpu_requirements.txt\n```\n\n- CUDA installation:\n\nFirst load the CUDA module. Then run cuda specific package install.\n\n```bash\ncd AMPL/pip\n# LLNL only: \n# module load cuda/11.8\npip install -r cuda_requirements.txt\n```\nIf you get `out of memory` errors, try setting these environment variables:\n```\nexport LD_LIBRARY_PATH=<your_env>/lib:$LD_LIBRARY_PATH\nexport PYTHONUSERBASE=<your_env>\nexport OPENBLAS_NUM_THREADS=1\nexport OMP_NUM_THREADS=48\nexport PYTORCH_HIP_ALLOC_CONF=gargage_collection_threshold:0.9,max_split_size_mb:128\nexport TF_FORCE_GPU_ALLOW_GROWTH=true\n```\n\n#### 6. *(Optional) LLNL LC only*: if you use [model_tracker](https://ampl.readthedocs.io/en/latest/pipeline.html#module-pipeline.model_tracker), install atomsci.clients\n```bash\n# LLNL only: required for ATOM model_tracker\npip install -r clients_requirements.txt\n```\n\n### Install AMPL\nRun the following to build the ""atomsci"" modules. This is required.\n\n```bash\n# return to AMPL parent directory\ncd ..\n./build.sh\npip install -e .\n```\n---\n## Installation Quick Summary\n```bash\nexport ENVROOT=~/workspace           # set ENVROOT example\ncd $ENVROOT                          # go to a convenient home directory\n# LLNL only:\n# module load python/3.9.12\n\npython3.9 -m venv atomsci-env        # create environment with Python 3.9\nsource $ENVROOT/atomsci-env/bin/activate \npip install pip --upgrade               \n\ngit clone https://github.com/ATOMScience-org/AMPL.git # clone AMPL\ncd AMPL/pip    \n# LLNL only:\n# If use CUDA: \n# module load cuda/11.8                         \npip install -r cpu_requirements.txt    # install cpu_requirements.txt OR cuda_requirements.txt  \n\n# LLNL only: required for ATOM model_tracker\n# pip install -r clients_requirements.txt\n\ncd ..                                   \n./build.sh                            \npip install -e .                        \n```\n---\n## Create jupyter notebook kernel (optional)\nTo run AMPL from Jupyter Notebook, with your environment activated. To setup a new kernel:\n\n```\npython -m ipykernel install --user --name atomsci-env\n```\n---\n## Install with Docker\n- Download and install Docker Desktop.\n  - https://www.docker.com/get-started\n- Create a workspace folder to mount with Docker environment and transfer files. \n- Get the Docker image and run it.\n  ```\n  docker pull atomsci/atomsci-ampl\n  docker run -it -p 8888:8888 -v </local_workspace_folder>:</directory_in_docker> atomsci/atomsci-ampl\n  #inside docker environment\n  jupyter-notebook --ip=0.0.0.0 --allow-root --port=8888 &\n  # -OR-\n  jupyter-lab --ip=0.0.0.0 --allow-root --port=8888 &\n  ```\n- Visit the provided URL in your browser, ie\n  - http://d33b0faf6bc9:8888/?token=656b8597498b18db2213b1ec9a00e9d738dfe112bbe7566d\n  - Replace the ""d33b0faf6bc9"" with ""localhost""\n  - If this doesn't work, exit the container and change port from 8888 to some other number such as 7777 or 8899 (in all 3 places it's written), then rerun both commands\n- From the notebook, you may need to set the kernel that atomsci is installed (""atomsci-venv"") in order to acccess the `atomsci` package.\n\n> ***Note***: *Be sure to save any work you want to be permanent in your workspace folder. If the container is shut down, you'll lose anything not in that folder.*  \n---\n\n## Uninstall\nTo remove AMPL from a pip environment use:\n```bash\npip uninstall atomsci-ampl\n```\n\nTo remove an entire virtual environment named ""atomsci-env"":\n```bash\nrm -rf $ENVROOT/atomsci-env\n```\n\nTo remove cached packages and clear space:\n```bash\npip cache purge\n```\n\n---\n## AMPL Features\n<details><summary>AMPL enables tasks for modeling and prediction from data ingestion to data analysis and can be broken down into the following stages:</summary>\n\n### 1. Data curation\n- Generation of RDKit molecular SMILES structures\n- Processing of qualified or censored data processing\n- Curation of activity and property values\n\n### 2. Featurization\n- Extended connectivity fingerprints (ECFP)\n- Graph convolution latent vectors from DeepChem\n- Chemical descriptors from Mordred package\n- Descriptors generated by MOE (requires MOE license)  \n\n### 3. Model training and tuning\n- Test set selection\n- Cross-validation\n- Uncertainty quantification\n- Hyperparameter optimization  \n\n### 4. Supported models\n- scikit-learn random forest models\n- XGBoost models\n- Fully connected neural networks\n- Graph convolution models  \n\n### 5. Visualization and analysis\n- Visualization and analysis tools  \n</details>\nDetails of running specific features are within the [parameter (options) documentation](#Pipeline-parameters). More detailed documentation is in the [library documentation](#Library-documentation).  \n\n---\n## Running AMPL\nAMPL can be run from the command line or by importing into Python scripts and Jupyter notebooks.  \n\n### Python scripts and Jupyter notebooks\nAMPL can be used to fit and predict molecular activities and properties by importing the appropriate modules. See the [examples](atomsci/ddm/examples/tutorials/14_BSEP_modeling.ipynb) for more descriptions on how to fit and make predictions using AMPL.\n\n### Pipeline parameters\nAMPL includes many parameters to run various model fitting and prediction tasks.\n- Pipeline options (parameters) can be set within JSON files containing a parameter list.\n- The parameter list with detailed explanations of each option can be found at [atomsci/ddm/docs/PARAMETERS.md](atomsci/ddm/docs/PARAMETERS.md).\n- Example pipeline JSON files can be found in the tests directory and the example directory.  \n\n### Library documentation\nAMPL includes detailed docstrings and comments to explain the modules. Full HTML documentation of the Python library is available with the package at [https://ampl.readthedocs.io/en/latest/](https://ampl.readthedocs.io/en/latest/).\n\n### More information on AMPL usage\n- More information on AMPL usage can be found in [Advanced AMPL usage](#advanced-ampl-usage)   \n\n---\n## Tests\nAMPL includes a suite of software tests. This section explains how to run a very simple test that is fast to run. The Python test fits a random forest model using Mordred descriptors on a set of compounds from Delaney, *et al* with solubility data. A molecular scaffold-based split is used to create the training and test sets. In addition, an external holdout set is used to demonstrate how to make predictions on new compounds.\n\nTo run the Delaney Python script that curates a dataset, fits a model, and makes predictions, run the following commands:\n```\nsource $ENVROOT/atomsci-env/bin/activate # activate your pip environment.\ncd atomsci/ddm/test/integrative/delaney_RF\npytest\n```\n> ***Note***: *This test generally takes a few minutes on a modern system*  \n\nThe important files for this test are listed below:\n\n- `test_delany_RF.py`: This script loads and curates the dataset, generates a model pipeline object, and fits a model. The model is reloaded from the filesystem and then used to predict solubilities for a new dataset.\n- `config_delaney_fit_RF.json`: Basic parameter file for fitting\n- `config_delaney_predict_RF.json`: Basic parameter file for predicting\n\n### More example and test information\n- More details on examples and tests can be found in [Advanced testing](#advanced-testing).\n\n---\n## Advanced AMPL usage\n\n### Command line\nAMPL can **fit** models from the command line with:\n```bash\npython model_pipeline.py --config_file filename.json # [filename].json is the name of the config file\n```  \n\nTo get more info on an AMPL config file, please refer to:\n\n  - [AMPL Features](https://github.com/ATOMScience-org/AMPL#ampl-features)\n  - [Running AMPL](https://github.com/ATOMScience-org/AMPL#running-ampl)\n  - [AMPL Tutorials](atomsci/ddm/examples/tutorials)\n\n### Hyperparameter optimization\n<details><summary>Hyperparameter optimization for AMPL model fitting is available to run on SLURM clusters or with [HyperOpt](https://hyperopt.github.io/hyperopt/) (Bayesian Optimization). To run Bayesian Optimization, the following steps can be followed.</summary>\n\n1. (Optional) Install HyperOpt with ""pip install hyperopt""\n2. Pre-split your dataset with computed_descriptors if you want to use Mordred/MOE/RDKit descriptors.\n3. In the config JSON file, set the following parameters.\n   \n   - ""hyperparam"": ""True""\n   - ""search_type"": ""hyperopt""\n   - ""descriptor_type"": ""mordred_filtered,rdkit_raw"" (use comma to separate multiple values)\n   - ""model_type"": ""RF|20"" (the number after | is the number of evaluations of Bayesian Optimization)\n   - ""featurizer"": ""ecfp,computed_descriptors"" (use comma if you want to try multiple featurizers, note the RF and graphconv are not compatible)\n   - ""result_dir"": ""/path/to/save/the/final/results,/temp/path/to/save/models/during/optimization"" (Two paths separated by a comma)\n  \n   RF model specific parameters:\n   - ""rfe"": ""uniformint|8,512"", (RF number of estimators)\n   - ""rfd"": ""uniformint|8,512"", (RF max depth of the decision tree)\n   - ""rff"": ""uniformint|8,200"", (RF max number of features)\n  \n    Use the following schemes to define the searching domains\n    \n    method|parameter1,parameter2...\n    \n    method: supported searching schemes in HyperOpt include: choice, uniform, loguniform, uniformint, see https://github.com/hyperopt/hyperopt/wiki/FMin for details.\n    \n    parameters:\n      - choice: all values to search from, separated by comma, e.g. choice|0.0001,0.0005,0.0002,0.001\n      - uniform: low and high bound of the interval to serach, e.g. uniform|0.00001,0.001\n      - loguniform: low and high bound (in natural log) of the interval to serach, e.g. uniform|-13.8,-6.9\n      - uniformint: low and high bound of the interval as integers, e.g. uniforming|8,256\n  \n    NN model specific parameters:\n     - ""lr"": ""loguniform|-13.8,-6.9"", (learning rate)\n     - ""ls"": ""uniformint|3|8,512"", (layer_sizes)\n        - The number between two bars (|) is the number of layers, namely 3 layers, each one with 8~512 nodes\n        - Note that the number of layers (number between two |) can not be changed during optimization, if you want to try different number of layers, just run several optimizations. \n     - ""dp"": ""uniform|3|0,0.4"", (dropouts)\n        - 3 layers, each one has a dropout range from 0 to 0.4\n        - Note that the number of layers (number between two |) can not be changed during optimization, if you want to try different number of layers, just run several optimizations. \n    \n    XGBoost model specific parameters:\n     - ""xgbg"": ""uniform|0,0.4"", (xgb_gamma, Minimum loss reduction required to make a further partition on a leaf node of the tree)\n     - ""xgbl"": ""loguniform|-6.9,-2.3"", (xgb_learning_rate, Boosting learning rate (xgboost's ""eta""))\n\n4. Run hyperparameter search in batch mode or submit a slurm job.\n\n    ```\n    python hyperparam_search_wrapper.py --config_file filename.json\n    ```\n    \n5. Save a checkpoint to continue it later.\n    \n    To save a checkpoint file of the hyperparameter search job, you want to set the following two parameters.\n    - ""hp_checkpoint_save"": ""/path/to/the/checkpoint/file.pkl""\n    - ""hp_checkpoint_load"": ""/path/to/the/checkpoint/file.pkl""\n    \n    If the ""hp_checkpoint_load"" is provided, the hyperparameter search will continue from the checkpoint. \n</details>\n\n---\n\n## Advanced testing\n### Running all tests\nTo run the full set of tests, use Pytest from the test directory:\n```bash\nsource $ENVROOT/atomsci-env/bin/activate # activate your pip environment. ""atomsci"" is an example here.\ncd atomsci/ddm/test\npytest\n```\n\n### Running SLURM tests\n<details><summary>Several of the tests take some time to fit. These tests can be submitted to a SLURM cluster as a batch job.</summary> Example general SLURM submit scripts are included as `pytest_slurm.sh`.\n\n```bash\nsource $ENVROOT/atomsci-env/bin/activate # activate your pip environment. ""atomsci-env"" is an example here.\ncd atomsci/ddm/test/integrative/delaney_NN\nsbatch pytest_slurm.sh\ncd ../../../..\ncd atomsci/ddm/test/integrative/wenzel_NN\nsbatch pytest_slurm.sh\n```\n</details>\n\n### Running tests without internet access\n<details><summary>AMPL works without internet access. Curation, fitting, and prediction do not require internet access.</summary>\n\nHowever, the public datasets used in tests and examples are not included in the repo due to licensing concerns. These are automatically downloaded when the tests are run. \n\nIf a system does not have internet access, the datasets will need to be downloaded before running the tests and examples. From a system with internet access, run the following shell script to download the public datasets. Then, copy the AMPL directory to the offline system.\n\n```\ncd atomsci/ddm/test\nbash download_datset.sh\ncd ../../..\n# Copy AMPL directory to offline system\n```\n</details>\n\n---\n## AMPL tutorials\nPlease follow link, [""atomsci/ddm/examples/tutorials""](https://github.com/ATOMScience-org/AMPL/tree/master/atomsci/ddm/examples/tutorials), to access a collection of AMPL tutorial notebooks. The tutorial notebooks give an exhaustive coverage of AMPL features. The AMPL team has prepared the tutorials to help beginners understand the basics to advanced AMPL features, and a reference for advanced AMPL users. \n\n---\n## Development\n### Installing the AMPL for development\nUsing ""pip install -e ."" will create a namespace package in your environment directory that points back to your git working directory, so every time you reimport a module you'll be in sync with your working code. Since site-packages is already in your sys.path, you won't have to fuss with PYTHONPATH or setting sys.path in your notebooks.  \n\n### Code Push Policy\nIt's recommended to use a development branch to do the work. After each release, there will be a branch opened for development.\n\nThe policy is \n\n1. Create a branch based off a development (""1.6.0 ""for example) or ""master"" branch\n2. Create a pull request. Assign a reviewer to approve the code changes \n\n> ***Note***:\n> Step 2 is required for pushing directly to ""master"". For a development branch, this step is recommended but not required.\n\n### Docstring format\nThe [""Google docstring""](https://github.com/google/styleguide/blob/gh-pages/pyguide.md#38-comments-and-docstrings) format is used in the AMPL code. When writing new code, please use the same Docstring style. Refer [here](https://www.sphinx-doc.org/en/master/usage/extensions/example_google.html#example-google) and [here](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html) for examples.\n\n### Versioning\nVersions are managed through GitHub tags on this repository.  \n\n### Built with\n- [DeepChem](https://github.com/deepchem/deepchem): A rich repository of chemistry-specific model types and utilities\n- [RDKit](https://github.com/rdkit/rdkit): Molecular informatics library\n- [Mordred](https://github.com/mordred-descriptor/mordred): Chemical descriptors\n- Other Python package dependencies\n\n---\n## Project information\n### Authors\n**[The Accelerating Therapeutics for Opportunities in Medicine (ATOM) Consortium](https://atomscience.org)**\n\n- Amanda J. Minnich <sub>(1)</sub>\n- Kevin McLoughlin <sub>(1)</sub>\n- Margaret Tse <sub>(2)</sub>\n- Jason Deng <sub>(2)</sub>\n- Andrew Weber <sub>(2)</sub>\n- Neha Murad <sub>(2)</sub>\n- Benjamin D. Madej <sub>(3)</sub>\n- Bharath Ramsundar <sub>(4)</sub>\n- Tom Rush <sub>(2)</sub>\n- Stacie Calad-Thomson <sub>(2)</sub>\n- Jim Brase <sub>(1)</sub>\n- Jonathan E. Allen <sub>(1)</sub>\n&nbsp;  \n\n### Contributors\n- [Amanda Paulson](https://github.com/paulsonak) <sub>(5)</sub>\n- Stewart He <sub>(1)</sub>\n- Da Shi <sub>(6)</sub>\n- Ravichandran Sarangan <sub>(7)</sub>\n- Jessica Mauvais <sub>(1)</sub>\n\n<sub>1. [Lawrence Livermore National Laboratory](https://www.llnl.gov/)</sub>\\n<sub>2. [GlaxoSmithKline Inc.](https://www.gsk.com/en-gb)</sub>\\n<sub>3. [Frederick National Laboratory for Cancer Research](https://frederick.cancer.gov)</sub>\\n<sub>4. Computable</sub>\\n<sub>5. [University of California, San Francisco](https://www.ucsf.edu/)</sub>\\n<sub>6. [Schrodinger](https://www.schrodinger.com/)</sub>\\n<sub>7. [Leidos](https://www.leidos.com)</sub>\n&nbsp;  \n\n### Support, Suggestions or Report Issues\n- If you have suggestions or like to report issues, please click [here](https://github.com/ATOMScience-org/AMPL/issues).\n&nbsp;\n\n### Contributing\nThank you for contributing to AMPL!\n\n- Contributions must be submitted through pull requests.\n- All new contributions must adhere to the MIT license.  \n&nbsp;  \n\n### Release\nAMPL is distributed under the terms of the MIT license. All new contributions must be made under this license.\n\nSee [MIT license](LICENSE) and [NOTICE](NOTICE) for more details.\n\n- LLNL-CODE-795635\n- CRADA TC02264\n",133,chemistry,Jupyter Notebook,8,Python,Makefile,Shell,Jupyter Notebook,HTML,CSS,JavaScript,Dockerfile,,,,,,,,,,,,,,,,,,,,,289,78,207,4,60,19,0,251375,64,43,33,10,9ee8d0ee14eced504b2070b89f6505740d2ccbcf,Merge pull request #329 from ATOMScience-org/remove_test_dbg,2024-07-11T15:19:19Z,mauvais2,76968305+mauvais2@users.noreply.github.com,mauvais2,1.6.1 Release,"# Highlights\r\n\r\n- Created a core tutorial series that represents the end-to-end modeling pipeline to build a machine learning model\r\n- Numerous improvements to visualizations in perf_plots module:\r\n   - Modified all plots to use color vision deficiency (CVD) friendly colors\r\n   - Added functions to visualize confusion matrices and model performance metrics\r\n   - Improved layout of plots produced by plot_perf_vs_epoch and plot_pred_vs_actual and added parameter to control plot size\r\n   - Reimplemented plot_prec_recall_curve to produce smoother curves.\r\n- Enhancements to multitask scaffold splitter: faster performance and optimization for response value distribution matching\r\n- Redesigned the AMPL readthedocs for easier end-user navigation. \r\n\r\n# Enhancements\r\n\r\n- Added ability to optimize multitask scaffold split for similarity of response value distributions across split subsets, using Wasserstein distance as dissimilarity metric; controlled by new parameter mtss_response_distr_weight. Improved performance of MTSS code to be much faster.\r\n- Added perf_plots functions plot_confusion_matrices, plot_model_metrics, get_metrics_from_model_pipeline and get_metrics_from_model_file to visualize and provide access to model performance metrics.\r\n- Modified plot_pred_vs_actual_from_file to make the output more consistent with plot_pred_from_actual; changed plot_pred_from_actual so that it accepts either a ModelPipeline or a model file path as its argument.\r\n- Reimplemented plot_prec_recall_curve with sklearn PrecisionRecallDisplay, with better handling of multitask models.\r\n\r\n# Bug Fixes\r\n\r\n- Fixed bug when number of scaffolds < number of superscaffolds requested\r\n- Fixed plot_pred_vs_actual_from_file so that it works on models trained with k-fold CV.\r\n- Fixed to exclude NaNs from % active calculation.",01.06.2001,,,mauvais2,MIT License,AMPL,ATOMScience-org,14,nci-doe-collaboration-capability,python,machine-learning,math-physics,neural-network,chemistry-discovery,chemistry,cheminformatics,,,,,,,,,,,,,/ATOMScience-org/AMPL,14,13,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/aseprite/aseprite,https://github.com/aseprite/aseprite,0,,0,0,0,0,0,0,0,1,1,0,0,0,"Animated sprite editor & pixel art tool (Windows, macOS, Linux)","# Aseprite\r\n\r\n[![build](https://github.com/aseprite/aseprite/actions/workflows/build.yml/badge.svg)](https://github.com/aseprite/aseprite/actions/workflows/build.yml)\r\n[![Translation Status](https://hosted.weblate.org/widget/aseprite/aseprite/svg-badge.svg)](https://hosted.weblate.org/engage/aseprite/)\r\n[![Discourse Community](https://img.shields.io/badge/discourse-community-brightgreen.svg?style=flat)](https://community.aseprite.org/)\r\n[![Discord Server](https://discordapp.com/api/guilds/324979738533822464/embed.png)](https://discord.gg/Yb2CeX8)\r\n\r\n## Introduction\r\n\r\n**Aseprite** is a program to create animated sprites. Its main features are:\r\n\r\n* Sprites are composed of [layers &amp; frames](https://www.aseprite.org/docs/timeline/) as separated concepts.\r\n* Support for [color profiles](https://www.aseprite.org/docs/color-profile/) and different [color modes](https://www.aseprite.org/docs/color-mode/): RGBA, Indexed (palettes up to 256 colors), Grayscale.\r\n* [Animation facilities](https://www.aseprite.org/docs/animation/), with real-time [preview](https://www.aseprite.org/docs/preview-window/) and [onion skinning](https://www.aseprite.org/docs/onion-skinning/).\r\n* [Export/import](https://www.aseprite.org/docs/exporting/) animations to/from [sprite sheets](https://www.aseprite.org/docs/sprite-sheet/), GIF files, or sequence of PNG files (and FLC, FLI, JPG, BMP, PCX, TGA).\r\n* [Multiple editors](https://www.aseprite.org/docs/workspace/#drag-and-drop-tabs) support.\r\n* [Layer groups](https://imgur.com/x3OKkGj) for organizing your work, and [reference layers](https://twitter.com/aseprite/status/806889204601016325) for rotoscoping.\r\n* Pixel-art specific tools like [Pixel Perfect freehand mode](https://imgur.com/0fdlNau), [Shading ink](https://www.aseprite.org/docs/shading/), [Custom Brushes](https://twitter.com/aseprite/status/1196883990080344067), [Outlines](https://twitter.com/aseprite/status/1126548469865431041), [Wide Pixels](https://imgur.com/1yZKUcs), etc.\r\n* Other special drawing tools like [Pressure sensitivity](https://twitter.com/aseprite/status/1253770784708886533), [Symmetry Tool](https://twitter.com/aseprite/status/659709226747625472), [Stroke and Fill](https://imgur.com/7JZQ81o) selection, [Gradients](https://twitter.com/aseprite/status/1126549217856622597).\r\n* [Tiled mode](https://youtu.be/G_JeWBaxQIg) useful to draw patterns and textures.\r\n* [Transform multiple frames/layers](https://twitter.com/aseprite/status/1170007034651172866) at the same time.\r\n* [Lua scripting capabilities](https://www.aseprite.org/docs/scripting/).\r\n* [CLI - Command Line Interface](https://www.aseprite.org/docs/cli/) to automatize tasks.\r\n* [Quick Reference / Cheat Sheet](https://www.aseprite.org/quickref/) keyboard shortcuts ([customizable keys](https://imgur.com/rvAUxyF) and [mouse wheel](https://imgur.com/oNqFqVb)).\r\n* [Reopen closed files](https://twitter.com/aseprite/status/1202641475256881153) and [recover data](https://www.aseprite.org/docs/data-recovery/) in case of crash.\r\n* Undo/Redo for every operation and support for [non-linear undo](https://imgur.com/9I42fZK).\r\n* [More features &amp; tips](https://twitter.com/aseprite/status/1124442198651678720)\r\n\r\n## Issues\r\n\r\nThere is a list of\r\n[Known Issues](https://github.com/aseprite/aseprite/issues) (things\r\nto be fixed or that aren't yet implemented).\r\n\r\nIf you found a bug or have a new idea/feature for the program,\r\n[you can report them](https://github.com/aseprite/aseprite/issues/new).\r\n\r\n## Support\r\n\r\nYou can ask for help in:\r\n\r\n* [Aseprite Community](https://community.aseprite.org/)\r\n* [Aseprite Discord Server](https://discord.gg/Yb2CeX8)\r\n* Official support: [support@aseprite.org](mailto:support@aseprite.org)\r\n* Social networks and community-driven places:\r\n  [Twitter](https://twitter.com/aseprite/),\r\n  [Facebook](https://facebook.com/aseprite/),\r\n  [YouTube](https://www.youtube.com/user/aseprite),\r\n  [Instagram](https://www.instagram.com/aseprite/).\r\n\r\n## Authors\r\n\r\nAseprite is being developed by [Igara Studio](https://igara.com/):\r\n\r\n* [David Capello](https://davidcapello.com/)\r\n* [Gaspar Capello](https://github.com/Gasparoken)\r\n* [Martín Capello](https://github.com/martincapello)\r\n\r\n## Credits\r\n\r\nThe default Aseprite theme was introduced in v0.8, created by:\r\n\r\n* [Ilija Melentijevic](https://ilkke.net/)\r\n\r\nA modified dark version of this theme introduced in v1.3-beta1 was created by:\r\n\r\n* [Nicolas Desilets](https://twitter.com/MapleGecko)\r\n* [David Capello](https://twitter.com/davidcapello)\r\n\r\nAseprite includes color palettes created by:\r\n\r\n* [Richard ""DawnBringer"" Fhager](http://pixeljoint.com/p/23821.htm), [16 colors](http://pixeljoint.com/forum/forum_posts.asp?TID=12795),  [32 colors](http://pixeljoint.com/forum/forum_posts.asp?TID=16247).\r\n* [Arne Niklas Jansson](http://androidarts.com/), [16 colors](http://androidarts.com/palette/16pal.htm), [32 colors](http://wayofthepixel.net/index.php?topic=15824.msg144494).\r\n* [ENDESGA Studios](https://twitter.com/ENDESGA), [EDG16 and EDG32](https://forums.tigsource.com/index.php?topic=46126.msg1279124#msg1279124), and [other palettes](https://twitter.com/ENDESGA/status/865812366931353600).\r\n* [Hyohnoo Games](https://twitter.com/Hyohnoo), [mail24](https://twitter.com/Hyohnoo/status/797472587974639616) palette.\r\n* [Davit Masia](https://twitter.com/DavitMasia), [matriax8c](https://twitter.com/DavitMasia/status/834862452164612096) palette.\r\n* [Javier Guerrero](https://twitter.com/Xavier_Gd), [nyx8](https://twitter.com/Xavier_Gd/status/868519467864686594) palette.\r\n* [Adigun A. Polack](https://twitter.com/adigunpolack), [AAP-64](http://pixeljoint.com/pixelart/119466.htm), [AAP-Splendor128](http://pixeljoint.com/pixelart/120714.htm), [SimpleJPC-16](http://pixeljoint.com/pixelart/119844.htm), and [AAP-Micro12](http://pixeljoint.com/pixelart/121151.htm) palette.\r\n* [PineTreePizza](https://twitter.com/PineTreePizza), [Rosy-42](https://twitter.com/PineTreePizza/status/1006536191955623938) palette.\r\n\r\nIt tries to replicate some pixel-art algorithms:\r\n\r\n* [RotSprite](http://forums.sonicretro.org/index.php?showtopic=8848&st=15&p=159754&#entry159754) by Xenowhirl.\r\n* [Pixel perfect drawing algorithm](https://deepnight.net/blog/tools/pixel-perfect-drawing/) by [Sébastien Bénard](https://twitter.com/deepnightfr) and [Carduus](https://twitter.com/CarduusHimself/status/420554200737935361).\r\n\r\nThanks to [third-party open source projects](docs/LICENSES.md), to\r\n[contributors](https://www.aseprite.org/contributors/), and all the\r\npeople who have contributed ideas, patches, bugs report, feature\r\nrequests, donations, and help us to develop Aseprite.\r\n\r\n## License\r\n\r\nThis program is distributed under three different licenses:\r\n\r\n1. Source code and official releases/binaries are distributed under\r\n   our [End-User License Agreement for Aseprite (EULA)](EULA.txt). Please check\r\n   that there are [modules/libraries in the source code](src/README.md) that\r\n   are distributed under the MIT license\r\n   (e.g. [laf](https://github.com/aseprite/laf),\r\n   [clip](https://github.com/aseprite/clip),\r\n   [undo](https://github.com/aseprite/undo),\r\n   [observable](https://github.com/aseprite/observable),\r\n   [ui](src/ui), etc.).\r\n2. You can request a special\r\n   [educational license](https://www.aseprite.org/faq/#is-there-an-educational-license)\r\n   in case you are a teacher in an educational institution and want to\r\n   use Aseprite in your classroom (in-situ).\r\n3. Steam releases are distributed under the terms of the\r\n   [Steam Subscriber Agreement](http://store.steampowered.com/subscriber_agreement/).\r\n\r\nYou can get more information about Aseprite license in the\r\n[FAQ](https://www.aseprite.org/faq/#licensing-&-commercial).\r\n",27784,graphics,C++,6,CMake,C,C++,Objective-C++,Shell,Lua,,,,,,,,,,,,,,,,,,,,,,,715,417,257,41,4,108,0,40107,5153,3771,2213,1558,3c4483183150f4adeb898013ae0a45a33f8058ac,We can use = {} for default arg values in function declarations,2024-07-19T20:10:39Z,David Capello,david@igara.com,dacap,Aseprite v1.3.7,## Release notes\r\n\r\n* Allow backslash (`\`) in filenames on Linux and macOS [#3936](https://github.com/aseprite/aseprite/issues/3936)\r\n* Fixed nested ping-pong tags [#4271](https://github.com/aseprite/aseprite/issues/4271)\r\n* Fixed wrong alignment between mouse and sensor threshold position [#4428](https://github.com/aseprite/aseprite/issues/4428)\r\n* Fixed painting with right-click with custom brush paints with background color [#4013](https://github.com/aseprite/aseprite/issues/4013)\r\n* Lua API:\r\n  * New [app.os object](https://www.aseprite.org/api/app_os#appos)\r\n  * Added `recent` parameter to [SaveFile](https://www.aseprite.org/api/command/SaveFile#savefile)/[ExportSpriteSheet](https://www.aseprite.org/api/command/ExportSpriteSheet#exportspritesheet) commands to avoid adding the file to the list of recent files\r\n  * Disable progress bar in commands that can receive `ui=false` [#4165](https://github.com/aseprite/aseprite/issues/4165)\r\n  * Fixed bugs handling errors inside `app.transaction()` [#3276](https://github.com/aseprite/aseprite/issues/3276) [#4431](https://github.com/aseprite/aseprite/pull/4431)\r\n  * Fixed return values of `os.execute()`\r\n  * Ask permissions for `io.popen()`/`lines()`/`input()`/`output()` functions\r\n\r\n\r\n**Full Changelog**: https://github.com/aseprite/aseprite/compare/v1.3.6...v1.3.7,v1.3.7,David Capello,,dacap,,aseprite,aseprite,102,animation,pixel-art,graphics,animated-sprites,spritesheet,gif,aseprite,c-plus-plus,sprites,draw,pixel-editor,tile-editor,tilemap-editor,sprite-editor,cpp,,,,,,/aseprite/aseprite,172,440,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/AryanpurTech/BlueEngine,https://github.com/AryanpurTech/BlueEngine,0,,,0,0,0,0,0,0,1,1,0,0,0,Blue Engine is a general-purpose and easy-to-use graphics engine written in rust.,"<img src=""https://raw.githubusercontent.com/AryanpurTech/BlueEngineDocs/master/resources/logo_3d.gif"" loop=infinite width=""100%"" />\r\n\r\n[![Rust Linux](https://github.com/AryanpurTech/BlueEngine/actions/workflows/rust-linux.yml/badge.svg)](https://github.com/AryanpurTech/BlueEngine/actions/workflows/rust-linux.yml)\r\n[![Rust Windows](https://github.com/AryanpurTech/BlueEngine/actions/workflows/rust-win.yml/badge.svg)](https://github.com/AryanpurTech/BlueEngine/actions/workflows/rust-win.yml)\r\n[![Rust MacOS](https://github.com/AryanpurTech/BlueEngine/actions/workflows/rust-osx.yml/badge.svg)](https://github.com/AryanpurTech/BlueEngine/actions/workflows/rust-osx.yml)\r\n[![rust-clippy analyze](https://github.com/AryanpurTech/BlueEngine/actions/workflows/rust-clippy.yml/badge.svg)](https://github.com/AryanpurTech/BlueEngine/actions/workflows/rust-clippy.yml)\r\n\r\nMake sure to use latest Rust version, as the engine is always kept up to date.\r\n\r\n## About\r\n\r\nBlue Engine is a general-purpose, easy-to-use, extendable, and portable graphics engine written in rust. The engine can run on many popular back-end APIs including Vulkan, D3D-12, GL-ES 3, and Metal as well as Windows, Linux, Mobile, and OSX to ensure cross-platform compatibility.\r\n\r\nHello World:\r\n\r\n```rust\r\nuse blue_engine::{\r\n    header::{ Engine, ObjectSettings },\r\n    primitive_shapes::triangle\r\n};\r\n\r\nfn main() {\r\n    // initialize the engine\r\n    let mut engine = Engine::new().expect(""engine couldn't be initialized"");\r\n\r\n    // create a triangle\r\n    triangle(""my triangle"", ObjectSettings::default(), &mut engine.renderer, &mut engine.objects).unwrap();\r\n\r\n    // run the engine\r\n    engine\r\n        .update_loop(move |_, _, _, _, _, _| {})\r\n        .expect(""Error during update loop"");\r\n}\r\n```\r\n\r\n* [Join our discord server](https://discord.gg/s7xsj9q)\r\n\r\n* [WIP] [Guide](https://aryanpurtech.github.io/BlueEngineDocs/)\r\n\r\n* Check out the [workflow](https://github.com/orgs/AryanpurTech/projects/2) for roadmap, status, ...\r\n\r\n* Check out the [examples](https://github.com/AryanpurTech/BlueEngine/tree/master/examples) folder to get a sense of how things are done\r\n\r\n* Check out the [utilities library](https://github.com/AryanpurTech/BlueEngineUtilities) for extra functionality with the engine\r\n\r\n* Check out the [editor](https://github.com/rustylabs/blue_flame)\r\n\r\n*the credits to the image on top: NotPB*\r\n\r\n\r\n*the development might seem slow sometimes, its due to multiple repositories being handled and due to my education taking a large chunk of my time. The project isn't dead, just slow.*\r\n",318,graphics,Rust,4,Rust,Just,WGSL,Dockerfile,,,,,,,,,,,,,,,,,,,,,,,,,23,7,16,0,1,6,0,33316,14,38,35,3,74214791584833e27b008ba56995a7aebbfe439e,fix: #62 and cleaned up bloat,2024-07-13T16:13:57Z,Elham Aryanpur,elhamaryanpur5@gmail.com,ElhamAryanpur,new update! 0.5.14,"## Actually fixed surface creation issue\r\n\r\nthe surface was being created twice, hence the errors on the surface already existing but attempting to recreate. This is due to the winit's `Resumed` event being a requirement for some platforms and in those platforms the surface isn't created until then. So now it checks if a surface exist at all and then create one if there is none.\r\n\r\n## Debloat\r\n\r\nThere was a lot of fields and methods that was not used or duplicated. They're all cleansed now. An example was setting color had two duplicate fields `set_color` and `set_uniform_color` which now theres only `set_color`. Also removed the `scale` field from the objects as it was not used anywhere.\r\n\r\n## Camera. lots of them.\r\n\r\nNow the engine supports multiple cameras. They all exist in the CameraContainer that replaced the Camera field. Your code should not be broken as all operations are now given to `""main""` camera. You can create new cameras and assign them to different objects to manipulate their displays.\r\n\r\n## Cutting edge dependencies\r\n\r\nEverything on Blue Engine and the utilities crate have been updated to the most recent version! (except winit. winit have been naughty).\r\n\r\nEnjoy!",0.5.14,Elham Aryanpur,,ElhamAryanpur,Apache License 2.0,BlueEngine,AryanpurTech,13,game-development,game-engine,graphics,rust,,,,,,,,,,,,,,,,,/AryanpurTech/BlueEngine,13,10,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/apple/swift-numerics,https://github.com/apple/swift-numerics,0,,,0,0,0,0,0,0,1,0,0,0,0,Advanced mathematical types and functions for Swift,"# Swift Numerics\n  \n## Introduction\n\nSwift Numerics provides a set of modules that support numerical computing in Swift.\nThese modules fall broadly into two categories:\n\n- API that is too specialized to go into the standard library, but which is sufficiently general to be centralized in a single common package.\n- API that is under active development toward possible future inclusion in the standard library.\n\nThere is some overlap between these two categories, and an API that begins in the first category may migrate into the second as it matures and new uses are discovered.\n\nSwift Numerics modules are fine-grained.\nFor example, if you need support for Complex numbers, you can import ComplexModule[^1] as a standalone module:\n\n```swift\nimport ComplexModule\n\nlet z = Complex<Double>.i\n```\n\nThere is also a top-level `Numerics` module that re-exports the complete public interface of Swift Numerics:\n\n```swift\nimport Numerics\n\n// The entire Swift Numerics API is now available\n```\n\nSwift Numerics modules have minimal dependencies on other projects.\n\nThe current modules assume only the availability of the Swift and C standard libraries and the runtime support provided by compiler-rt.\n\nFuture expansion may assume the availability of other standard interfaces, such as [BLAS (Basic Linear Algebra Subprograms)](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms) and [LAPACK (Linear Algebra Package)](https://en.wikipedia.org/wiki/LAPACK), but modules with more specialized dependencies (or dependencies that are not available on all platforms supported by Swift) belong in a separate package.\n\nBecause we intend to make it possible to adopt Swift Numerics modules in the standard library at some future point, Swift Numerics uses the same license and contribution guidelines as the Swift project.\n\n## Using Swift Numerics in your project\n\nTo use Swift Numerics in a SwiftPM project:\n\n1. Add the following line to the dependencies in your `Package.swift` file:\n\n```swift\n.package(url: ""https://github.com/apple/swift-numerics"", from: ""1.0.0""),\n```\n\n2. Add `Numerics` as a dependency for your target:\n\n```swift\n.target(name: ""MyTarget"", dependencies: [\n  .product(name: ""Numerics"", package: ""swift-numerics""),\n  ""AnotherModule""\n]),\n```\n\n3. Add `import Numerics` in your source code.\n\n## Source stability\n\nThe Swift Numerics package is source stable; version numbers follow [Semantic Versioning](https://semver.org).\nThe public API of the `swift-numerics` package consists of non-underscored declarations that are marked either `public` or `usableFromInline` in modules re-exported by the top-level `Numerics` module.\nInterfaces that aren't part of the public API may continue to change in any release, including patch releases. \n\nNote that contents of the `_NumericsShims` and `_TestSupport` modules, as well as contents of the `Tests` directory, explicitly are not public API.\nThe definitions therein may therefore change at whim, and the entire module may be removed in any new release.\nIf you have a use case that requires underscored operations, please raise an issue to request that they be made public API.\n\nFuture minor versions of the package may introduce changes to these rules as needed.\n\nWe'd like this package to quickly embrace Swift language and toolchain improvements that are relevant to its mandate.\nAccordingly, from time to time, we expect that new versions of this package will require clients to upgrade to a more recent Swift toolchain release.\nRequiring a new Swift release will only require a minor version bump.\n\n## Contributing to Swift Numerics\n\nSwift Numerics is a standalone library that is separate from the core Swift project, but it will sometimes act as a staging ground for APIs that will later be incorporated into the Swift Standard Library.\nWhen that happens, such changes will be proposed to the Swift Standard Library using the established evolution process of the Swift project.\n\nSwift Numerics uses GitHub issues to track bugs and features. We use pull requests for development.\n\n### How to propose a new module\n\n1. Raise an issue with the [new module] tag.\n2. Raise a PR with an implementation sketch.\n3. Once you have some consensus, ask an admin to create a feature branch against which PRs can be raised.\n4. When the design has stabilized and is functional enough to be useful, raise a PR to merge the new module to master.\n\n### How to propose a new feature for an existing module\n\n1. Raise an issue with the [enhancement] tag.\n2. Raise a PR with your implementation, and discuss the implementation there.\n3. Once there is a consensus that the new feature is desirable and the design is suitable, it can be merged.\n\n### How to fix a bug, or make smaller improvements\n\n1. Raise a PR with your change. \n2. Make sure to add test coverage for whatever changes you are making.\n\n### Forums\n\nQuestions about how to use Swift Numerics modules, or issues that are not clearly bugs can be discussed in the [""Swift Numerics"" section of the Swift forums](https://forums.swift.org/c/related-projects/swift-numerics).\n\n## Modules\n\n1. [`RealModule`](Sources/RealModule/README.md)\n2. [`ComplexModule`](Sources/ComplexModule/README.md)\n3. [`IntegerUtilities`](Sources/IntegerUtilities/README.md) (on main only, not yet present in a released tag)\n\n## Future expansion\n\n1. [Large Fixed-Width Integers](https://github.com/apple/swift-numerics/issues/4)\n2. [Arbitrary-Precision Integers](https://github.com/apple/swift-numerics/issues/5)\n3. [Shaped Arrays](https://github.com/apple/swift-numerics/issues/6)\n4. [Decimal Floating-point](https://github.com/apple/swift-numerics/issues/7)\n\n[^1]: The module is named `ComplexModule` instead of `Complex` because Swift is currently unable to use the fully-qualified name for types when a type and module have the same name (discussion here: https://forums.swift.org/t/pitch-fully-qualified-name-syntax/28482).\n    This would prevent users of Swift Numerics who don't need generic types from doing things such as:\n\n    ```swift\n    import Complex\n    // I know I only ever want Complex<Double>, so I shouldn't need the generic parameter.\n    typealias Complex = Complex.Complex<Double> // This doesn't work, because name lookup fails.\n    ```\n    \n    For this reason, modules that would have this ambiguity are suffixed with `Module` within Swift Numerics:\n    \n    ```swift\n    import ComplexModule\n    // I know I only ever want Complex<Double>, so I shouldn't need the generic parameter.\n    typealias Complex = ComplexModule.Complex<Double>\n    // But I can still refer to the generic type by qualifying the name if I need it occasionally:\n    let a = ComplexModule.Complex<Float>\n    ```\n\n    The `Real` module does not contain a `Real` type, but does contain a `Real` protocol.\n    Users may want to define their own `Real` type (and possibly re-export the `Real` module)--that is why the suffix is also applied there.\n    New modules have to evaluate this decision carefully, but can err on the side of adding the suffix.\n    It's expected that most users will simply `import Numerics`, so this isn't an issue for them.\n",1657,mathematics,Swift,2,Swift,CMake,,,,,,,,,,,,,,,,,,,,,,,,,,,204,20,159,25,4,385,0,751,140,92,53,39,53afff107ad77e7853558f4679d66370072392b5,Merge pull request #295 from stephentyrone/double-width-improvements,2024-07-12T18:00:55Z,Stephen Canon,scanon@apple.com,stephentyrone,CMake build support for arm64/macOS,This release adds support for the CMake build in macOS on arm64. Thanks to @DanboDuan for the patch.,1.0.2,Stephen Canon,,stephentyrone,Apache License 2.0,swift-numerics,apple,13,math,complex,mathematics,maths,real,trig,,,,,,,,,,,,,,,/apple/swift-numerics,13,53,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/antvis/G2Plot,https://github.com/antvis/G2Plot,0,,,0,0,0,0,0,0,1,1,0,0,0,:dango:  An interactive and responsive charting library based on G2.,"<img src=""https://gw.alipayobjects.com/zos/antfincdn/R8sN%24GNdh6/language.svg"" width=""18""> [English](./README.en-US.md) | 简体中文\n\n<h1 align=""center"">G2Plot</h1>\n\n<div align=""center"">\n\n基于 [G2 4.x](https://github.com/antvis/G2) 版本二次封装的图表库。\n\n[![Version](https://badgen.net/npm/v/@antv/g2plot)](https://npmjs.com/@antv/g2plot)\n[![NPM downloads](https://img.shields.io/npm/dm/@antv/g2plot.svg)](https://npmjs.com/@antv/g2plot)\n![Latest commit](https://badgen.net/github/last-commit/antvis/G2Plot)\n[![build Status](https://github.com/antvis/G2Plot/workflows/build/badge.svg?branch=master)](https://github.com/antvis/G2Plot/actions?query=workflow%3Abuild)\n[![coverage](https://img.shields.io/coveralls/antvis/G2Plot/master.svg)](https://coveralls.io/github/antvis/G2Plot)\n[![Percentage of issues still open](http://isitmaintained.com/badge/open/antvis/g2plot.svg)](http://isitmaintained.com/project/antvis/g2plot ""Percentage of issues still open"")\n[![Average time to resolve an issue](http://isitmaintained.com/badge/resolution/antvis/g2plot.svg)](http://isitmaintained.com/project/antvis/g2plot ""Average time to resolve an issue"")\n\n> 📢 新版本 [G2 v5](https://github.com/antvis/G2) 已经发布，未来不会基于 G2 v5 封装 G2Plot v3 版本，但是可以使用 [Ant Design Charts](https://github.com/ant-design/ant-design-charts) 代替。\n\n<p align=""center"">\n  <a href=""https://g2plot.antv.vision/en"">网站</a> •\n  <a href=""https://g2plot.antv.vision/en/docs/manual/getting-started"">快速开始</a> •\n  <a href=""https://www.yuque.com/antv/g2plot"">博客</a> •\n  <a href=""https://github.com/antvis/theme-set"">AntV ThemeSet</a>\n</p>\n\n</div>\n\n一套简单、易用、并具备一定扩展能力和组合能力的统计图表库，基于图形语法理论搭建而成，『G2Plot』中的 G2 即意指图形语法 (the Grammar of Graphics)，同时也致敬了 [ggplot2](https://ggplot2.tidyverse.org/)。我们想做的事有三件：\n\n1. 使用户不用成为可视化专家也能够轻松制作出优雅美观的图表。\n2. 保证图表能够经受得起业务的检验，在真实的场景中易用、好用。\n3. 探索统计图表的更多可能性，使统计图表变得更好玩、更酷。\n\n<div align=""center"">\n  <img src=""https://gw.alipayobjects.com/mdn/rms_d314dd/afts/img/A*sXqrRrEwFRQAAAAAAAAAAABkARQnAQ"" width=""800"">\n</div>\n\n## ✨ 特性\n\n### 📦 开箱即用、体验优雅的高质量统计图表\n\nG2Plot 呈现给用户的是一套提炼自企业级产品的视觉语言和设计规范。不仅对图表的整体视觉样式进行了优化，并且针对每一个图表自身的特点，沉淀出一套最佳配置，保证用户能够通过最少的配置制作出优雅、标准的图表。\n\n<div align=""center"">\n<img src=""https://gw.alipayobjects.com/mdn/rms_d314dd/afts/img/A*rqI2Qqt0pTwAAAAAAAAAAABkARQnAQ"" width=""600"" />\n</div>\n\n### 📊 响应式：让图表更聪明\n\n在现实的图表应用场景中，一个棘手的难题是图表的展示空间往往并不足够显示图表的数据量，造成极值情况下文本的重叠遮挡、内容无法自适应、内容裁剪等问题。G2Plot 借鉴宽容性设计的思想，在图表的信息密度过高时，对图表辅助信息进行抽稀，保证图表主要信息的展示和基本可读性。\n\n<div align=""center"">\n  <img src=""https://gw.alipayobjects.com/mdn/rms_d314dd/afts/img/A*ifK1TLi_4WoAAAAAAAAAAABkARQnAQ"" width=""600"" />\n</div>\n\n### 🔳 向前一步：会讲故事的图表\n\n在 G2Plot 体系下，图表不仅仅只是各不相关的实例，图层概念的引入提供了多图表组合、叠加、联动，共同讲述一个数据故事的可能性。未来，我们还将探索统计图表转化信息图的可能性，丰富统计图表的表现能力。\n\n<div align=""center"">\n  <img src=""https://gw.alipayobjects.com/mdn/rms_d314dd/afts/img/A*gd00QaD9110AAAAAAAAAAABkARQnAQ"" width=""600"" />\n</div>\n\n## 📦 安装\n\n```bash\n$ npm install @antv/g2plot\n```\n\n## 🔨 使用\n\n<div align=""center"">\n<img src=""https://gw.alipayobjects.com/mdn/rms_d314dd/afts/img/A*37siRJftYDIAAAAAAAAAAABkARQnAQ"" width=""450"" />\n</div>\n\n```html\n<div id=""container""></div>\n```\n\n```ts\nimport { Bar } from '@antv/g2plot';\n\nconst data = [\n  { year: '1951 年', sales: 38 },\n  { year: '1952 年', sales: 52 },\n  { year: '1956 年', sales: 61 },\n  { year: '1957 年', sales: 145 },\n  { year: '1958 年', sales: 48 },\n];\n\nconst bar = new Bar('container', {\n  data,\n  xField: 'sales',\n  yField: 'year',\n  seriesField: 'year',\n});\n\nbar.render();\n```\n\n## 🤝 参与贡献\n\n我们非常欢迎你的贡献！无论是 issue 还是 PR。\n\n反馈问题请先阅读 [issues](https://github.com/antvis/g2plot/issues)。\n\n提交代码请遵循 [贡献指引](https://github.com/antvis/g2plot/blob/master/CONTRIBUTING.md)。\n\n感谢下面这些贡献者 ([emoji key](https://allcontributors.org/docs/en/emoji-key)):\n\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n<table>\n  <tr>\n    <td align=""center""><a href=""https://github.com/visiky""><img src=""https://avatars.githubusercontent.com/u/15646325?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>Visiky</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=visiky"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://hust.cc/""><img src=""https://avatars.githubusercontent.com/u/7856674?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>hustcc</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=hustcc"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://github.com/lxfu1""><img src=""https://avatars.githubusercontent.com/u/31396322?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>Joel Alan</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=lxfu1"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://github.com/liuzhenying""><img src=""https://avatars.githubusercontent.com/u/11748654?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>刘珍莹</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=liuzhenying"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://github.com/zqlu""><img src=""https://avatars.githubusercontent.com/u/1142242?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>zqlu</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=zqlu"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://github.com/arcsin1""><img src=""https://avatars.githubusercontent.com/u/13724222?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>arcsin1</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=arcsin1"" title=""Code"">💻</a></td>\n  </tr>\n  <tr>\n    <td align=""center""><a href=""https://github.com/zhangzhonghe""><img src=""https://avatars.githubusercontent.com/u/38434641?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>被雨水过滤的空气</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=zhangzhonghe"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://github.com/yp0413150120""><img src=""https://avatars.githubusercontent.com/u/24318174?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>banli</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=yp0413150120"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://github.com/BBSQQ""><img src=""https://avatars.githubusercontent.com/u/35586469?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>xi li</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=BBSQQ"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://blog.csdn.net/weixin_42628594""><img src=""https://avatars.githubusercontent.com/u/42288791?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>DarrenPei</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=DarrenPei"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://github.com/pearmini""><img src=""https://avatars.githubusercontent.com/u/49330279?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>MiniPear</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=pearmini"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://github.com/connono""><img src=""https://avatars.githubusercontent.com/u/36756846?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>connono</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=connono"" title=""Code"">💻</a></td>\n  </tr>\n  <tr>\n    <td align=""center""><a href=""https://github.com/yujs""><img src=""https://avatars.githubusercontent.com/u/16610138?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>于向前</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=yujs"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://twitter.com/afc163""><img src=""https://avatars.githubusercontent.com/u/507615?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>afc163</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=afc163"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""http://www.mjul.com/""><img src=""https://avatars.githubusercontent.com/u/142868?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>Martin Jul</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=mjul"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://github.com/jinhuiWong""><img src=""https://avatars.githubusercontent.com/u/23117130?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>jhwong</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=jinhuiWong"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://kingsongao.com/""><img src=""https://avatars.githubusercontent.com/u/6930280?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>Jingsong Gao</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=kagawagao"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://github.com/MrSmallLiu""><img src=""https://avatars.githubusercontent.com/u/26038018?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>Mr小刘</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=MrSmallLiu"" title=""Code"">💻</a></td>\n  </tr>\n  <tr>\n    <td align=""center""><a href=""https://github.com/ntscshen""><img src=""https://avatars.githubusercontent.com/u/21041458?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>ntscshen</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=ntscshen"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://juejin.cn/user/3491704660305111""><img src=""https://avatars.githubusercontent.com/u/12762626?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>yiminanci</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=guonanci"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://github.com/ai-qing-hai""><img src=""https://avatars.githubusercontent.com/u/65594180?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>ai-qing-hai</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=ai-qing-hai"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://github.com/xrkffgg""><img src=""https://avatars.githubusercontent.com/u/29775873?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>xrkffgg</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=xrkffgg"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://github.com/DawnLck""><img src=""https://avatars.githubusercontent.com/u/12195307?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>Dawnlck</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=DawnLck"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://github.com/CarisL""><img src=""https://avatars.githubusercontent.com/u/13416424?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>Karis</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=CarisL"" title=""Code"">💻</a></td>\n  </tr>\n  <tr>\n    <td align=""center""><a href=""https://gine.me/""><img src=""https://avatars.githubusercontent.com/u/6588202?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>Mayne</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=mayneyao"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://github.com/Plortinus""><img src=""https://avatars.githubusercontent.com/u/20693993?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>Plortinus</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=Plortinus"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://github.com/csjkevin""><img src=""https://avatars.githubusercontent.com/u/17211870?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>Shanjie Chen</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=csjkevin"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://doocs.github.io/""><img src=""https://avatars.githubusercontent.com/u/21008209?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>Yang Libin</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=yanglbme"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://github.com/beewolf233""><img src=""https://avatars.githubusercontent.com/u/24711525?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>beewolf233</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=beewolf233"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://github.com/lqzhgood""><img src=""https://avatars.githubusercontent.com/u/9134671?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>lqzhgood</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=lqzhgood"" title=""Code"">💻</a></td>\n  </tr>\n  <tr>\n    <td align=""center""><a href=""https://jiazhe.wang/""><img src=""https://avatars.githubusercontent.com/u/6898060?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>neoddish</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=neoddish"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://github.com/stack-stark""><img src=""https://avatars.githubusercontent.com/u/46991054?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>stack-stark</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=stack-stark"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://github.com/NewByVector""><img src=""https://avatars.githubusercontent.com/u/20186737?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>vector</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=NewByVector"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""http://www.wanyingxing.vip/""><img src=""https://avatars.githubusercontent.com/u/10885578?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>嘤嘤嘤</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=xingwanying"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://wineso.me/""><img src=""https://avatars.githubusercontent.com/u/2106987?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>琚致远</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=juzhiyuan"" title=""Code"">💻</a></td>\n    <td align=""center""><a href=""https://github.com/YiSiWang""><img src=""https://avatars.githubusercontent.com/u/20316342?v=4?s=32"" width=""32px;"" alt=""""/><br /><sub><b>14</b></sub></a><br /><a href=""https://github.com/antvis/G2Plot/commits?author=YiSiWang"" title=""Code"">💻</a></td>\n  </tr>\n</table>\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n\n## 🔗 相关链接\n\n<img src=""https://gw.alipayobjects.com/zos/antfincdn/1yMwFkBvyV/chartcube-logo-cube.svg"" width=""18""> [ChartCube](https://chartcube.alipay.com/) - 基于 G2Plot 的在线图表制作工具，交互简单，一键导出图表代码！\n\n## 许可证\n\nMIT\n",2535,graphics,TypeScript,3,JavaScript,TypeScript,CSS,,,,,,,,,,,,,,,,,,,,,,,,,,1827,218,1606,3,14,114,428,21479,604,1873,1438,435,bdc1342e3cc8a71d8cb1284acec9299b64648774,area plot: the default value of isPercent option should be false (#3725),2024-03-28T05:26:42Z,glfeng,glfeng318@gmail.com,glfeng318,02.04.1931,<!-- Release notes generated using configuration in .github/release.yml at 2.4.31 -->\r\n\r\n\r\n## New Contributors\r\n* @sersishen made their first contribution in https://github.com/antvis/G2Plot/pull/3544\r\n\r\n**Full Changelog**: https://github.com/antvis/G2Plot/compare/2.4.29...2.4.31,02.04.1931,,,sersishen,MIT License,G2Plot,antvis,139,antv,visualization,graphics,g2,charts,plot,g2plot,,,,,,,,,,,,,,/antvis/G2Plot,158,63,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/antvis/G2,https://github.com/antvis/G2,0,,,0,0,0,0,0,0,1,1,0,0,0,📊 The concise and progressive visualization grammar.,"<img src=""https://gw.alipayobjects.com/zos/antfincdn/R8sN%24GNdh6/language.svg"" width=""18""> English | [简体中文](./README.zh-CN.md)\n\n<h1 align=""center"">\n<b>G2 5.0</b>\n</h1>\n\n<div align=""center"">\n\nG2 is a visualization grammar for dashboard building, data exploration and storytelling.\n\n[![Build Status](https://github.com/antvis/g2/workflows/build/badge.svg?branch=v5)](https://github.com/antvis//actions)\n[![Coverage Status](https://img.shields.io/coveralls/github/antvis/g2/v5.svg)](https://coveralls.io/github/antvis/g2?branch=v5)\n[![npm Version](https://img.shields.io/npm/v/@antv/g2.svg)](https://www.npmjs.com/package/@antv/g2)\n[![npm Download](https://img.shields.io/npm/dm/@antv/g2.svg)](https://www.npmjs.com/package/@antv/g2)\n[![npm License](https://img.shields.io/npm/l/@antv/g2.svg)](https://www.npmjs.com/package/@antv/g2)\n\n![examples](https://mdn.alipayobjects.com/huamei_qa8qxu/afts/img/A*_GfqQoRCqQkAAAAAAAAAAAAADmJ7AQ/fmt.webp)\n\n</div>\n\nG2 is named after Leland Wilkinson’s book _The Grammar of Graphics_ and was profoundly inspired by it in the very beginning. Here are some resources you can begin with:\n\n- [Introduction](https://g2.antv.antgroup.com/manual/introduction/what-is-g2) - a brief overview and G2's motivations\n- [Examples](https://g2.antv.antgroup.com/examples) - a large number of demos to learn from and copy-paste\n- [Tutorials](https://g2.antv.antgroup.com/manual/introduction/getting-started) - interactive case-driven guides of G2's core concepts\n- [API Reference](https://g2.antv.antgroup.com/api/overview) - complete documentation for all visualization components\n- [Editor](https://editor.antv.antgroup.com/) - an intelligent generation tool based on AntV. It utilizes AI to reduce the development cost of data visualization, and can quickly generate visual charts through natural language.\n\n## ✨ Features\n\n- **Progressive Usage** - The main objective of G2 is to help you get meaningful visualizations quickly with concise declarations and it infers the rest. But you can configure much more for complex and advanced situations.\n- **Declarative API** - We employs a functional declarative API to specify chart options in a programmatic fashion, which contributes to better logic reuse and more flexible code organization.\n- **High Extensibility** - To satisfy specific needs, G2 provides a convenient and consistent mechanism to extend everything you can imagine, whether a scale, a transform, a mark, etc,. You can even customize a brand new visualization tool based on this mechanism.\n- **Comprehensive Grammar** - G2 rejects a chart typology in favor of marks, transforms, scales, coordinates, and compositions. In addition to static visual representations, it's possible to declare data-driven animations and apply well-designed action-based interactions to plots as well.\n- **Powerful Renderer** - There is a powerful renderer [G](https://github.com/antvis/G) under G2 to generate web-based visualizations using Canvas, SVG or WebGL. The plenty of plugins it has benefit G2 from rendering charts with novel styles such as hand-drawn and fully embracing the ecosystem of [D3](https://github.com/d3/d3).\n\n## 🔨 Getting Started\n\nG2 is usually installed via a package manager such as npm or Yarn.\n\n```bash\n$ npm install @antv/g2\n```\n\n```bash\n$ yarn add @antv/g2\n```\n\nThe Chart object then can be imported from G2.\n\n```html\n<div id=""container""></div>\n```\n\n```js\nimport { Chart } from '@antv/g2';\n\n// A tabular data to be visualized.\nconst data = [\n  { genre: 'Sports', sold: 275 },\n  { genre: 'Strategy', sold: 115 },\n  { genre: 'Action', sold: 120 },\n  { genre: 'Shooter', sold: 350 },\n  { genre: 'Other', sold: 150 },\n];\n\n// Instantiate a new chart.\nconst chart = new Chart({\n  container: 'container',\n});\n\n// Specify visualization.\nchart\n  .interval() // Create an interval mark and add it to the chart.\n  .data(data) // Bind data for this mark.\n  .encode('x', 'genre') // Assign genre column to x position channel.\n  .encode('y', 'sold') // Assign sold column to y position channel.\n  .encode('color', 'genre'); // Assign genre column to color channel.\n\n// Render visualization.\nchart.render();\n```\n\nIf all goes well, you can get the following lovely bar chart!\n\n<img src=""https://mdn.alipayobjects.com/huamei_qa8qxu/afts/img/A*XqCnTbkpAkQAAAAAAAAAAAAADmJ7AQ/fmt.webp"" width=""640"" alt=""example"">\n\n## 🌍 Ecosystem\n\n- [g2-react](https://github.com/pearmini/g2-react) - The lightweight React component for G2, without extra features.\n- [g2-extensions](https://github.com/antvis/g2-extensions) - The one-stop shop for official G2 extensions, such as 3d visualization, automated visual analytics, etc.\n- [ant-design-charts](https://github.com/ant-design/ant-design-charts) - The React chart library, based on [G2](https://github.com/antvis/G2), [G6](https://github.com/antvis/G6), [X6](https://github.com/antvis/X6), [L7Plot](https://github.com/antvis/L7Plot).\n- [More...](https://github.com/antvis/G2/discussions/5772)\n\n## 📮 Contributing\n\n- [Issues](https://github.com/antvis/g2/issues) - report bugs or request features\n- [Contributing Guide](https://github.com/antvis/g2/blob/v5/CONTRIBUTING.md) - help build G2\n- [Discussions](https://github.com/antvis/G2/discussions) - discuss on GitHub or in DingTalk group(30233731, 35686967, 44788198)\n\n<img src=""https://gw.alipayobjects.com/zos/antfincdn/hTzzaqgHgQ/Antv%252520G2%252520%26%252520G2Plot.png"" width=""200"" height=""266"" alt=""code""/>\n\n## 📄 License\n\nMIT@[AntV](https://github.com/antvis).\n",12046,graphics,TypeScript,6,JavaScript,TypeScript,Shell,HTML,CSS,Less,,,,,,,,,,,,,,,,,,,,,,,2478,189,2286,3,12,202,808,332309,1580,3520,3369,151,eadad96ede90e8032aef66f10434856b8c11cbdb,fix(facet): legend color do not match pie color (#6369),2024-07-18T11:09:34Z,Bairui Su,subairui@icloud.com,pearmini,05.02.2001,## What's Changed\r\n\r\n* Fix NaN in liquid by @the-lemonboy in https://github.com/antvis/G2/pull/6336\r\n* fix(dodgeX): do not update series channel if groupBy is invalid by @pearmini in https://github.com/antvis/G2/pull/6344\r\n* fix(legend-filter): filter facet pie by @pearmini in https://github.com/antvis/G2/pull/6346\r\n* Update README.md by @lxfu1 in https://github.com/antvis/G2/pull/6352\r\n* feat(wordcloud): accept custom canvas by @pearmini in https://github.com/antvis/G2/pull/6354\r\n* chore: update to 5.2.1 by @pearmini in https://github.com/antvis/G2/pull/6355\r\n\r\n\r\n**Full Changelog**: https://github.com/antvis/G2/compare/5.2.0...5.2.1,05.02.2001,Bairui Su,,pearmini,MIT License,G2,antvis,129,visualization,grammar,graphics,canvas,svg,interaction,animation,webgl,chart,,,,,,,,,,,,/antvis/G2,250,251,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/antigenomics/vdjdb-db,https://github.com/antigenomics/vdjdb-db,0,,,0,1,1,0,0,0,0,0,0,0,0,🗂️ [vdjdb.cdr3.net is up and running] Git-based TCR database storage & management. Submissions welcome!,"# VDJDB: A curated database of T-cell receptor sequences of known antigen specificity\n\n![Splash](images/vdjdb-splash.png)\n\nThe primary goal of VDJdb is to facilitate access to existing information on T-cell receptor antigen specificities, i.e. the ability to recognize certain epitopes in certain MHC contexts.\n\nOur mission is to both aggregate the scarce TCR specificity information available so far and to create a curated repository to store such data.\n\nIn addition to routine database updates providing the most up-to-date information, we make our best to ensure data consistency and fight irregularities in TCR specificity reporting with a complex database validation scheme:\n\n* We take into account all available information on experimental setup used to identify antigen-specific TCR sequences and assign a single confidence score to highligh most reliable records at the database generation stage.\n* Each database record is also automatically checked against a database of V/J segment germline sequences to ensure standardized and consistent reporting of V-J junctions and CDR3 sequences that define T-cell clones.\n\nThis repository hosts the submissions to database and scripts to check, fix and build the database itself.\n\nTo build database directly from submissions, go to ``src`` directory and run ``groovy -cp . BuildDatabase.groovy`` script (requires [Groovy](http://www.groovy-lang.org/)).\n\nTo query the database for your immune repertoire sample(s) use the [vdjmatch](https://github.com/antigenomics/vdjmatch) software.\n\nA web-based GUI for the database can be found in [VDJdb-web](https://github.com/antigenomics/vdjdb-web) repository.\n\n## Citing\n\nPlease cite the database using the **most recent** paper ``Mikhail Goncharov, Dmitry Bagaev, Dmitrii Shcherbinin, Ivan Zvyagin, Dmitry Bolotin, Paul G. Thomas, Anastasia A. Minervina, Mikhail V. Pogorelyy, Kristin Ladell, James E. McLaren, David A. Price, Thi H. O. Nguyen, Louise C. Rowntree, E. Bridie Clemens, Katherine Kedzierska, Garry Dolton, Cristina Rafael Rius, Andrew Sewell, Jerome Samir, Fabio Luciani, Ksenia V. Zornikova, Alexandra A. Khmelevskaya, Saveliy A. Sheetikov, Grigory A. Efimov, Dmitry Chudakov & Mikhail Shugay. VDJdb in the pandemic era: a compendium of T cell receptors specific for SARS-CoV-2. Nature Methods 2022.`` [doi:10.1038/s41592-022-01578-0](https://doi.org/10.1038/s41592-022-01578-0).\n\n## Submission guide\n\nTo submit previously published sequence follow the steps below:\n\n* Create an issue(s) labeled as ``paper`` and named by the paper pubmed id, ``PMID:XXXXXXX``. Note that if paper is a meta-study, you can mark it as ``meta-paper`` and link issues for its references in a reply to this issue. Also note that in case submitting unpublished sequences, choose any appropriate issue name with details on submitter (name, organization, etc) in issue comments.\n\n* Create new branch and add chunk(s) for corresponding papers named as ``PMID_XXXXXXX``. Don't forget to close/reference corresponding issues in the commit message.\n\n* Create a pull request for the branch and check if it passes the CI build. If there are any issues, modify them by fixing/removing entries as necessary.\n\nThe structure of submission chunk is provided below, but first a couple of notes:\n\n> **STYLE** Try avoiding spaces (e.g. ``TRBV7,TRBV5``, not ``TRBV7, TRBV5``) and leave fields that have no information as blank (don't use any placeholder). Stick to listed field values at all cost! In case a critical part of your submission doesn't fit in current specification: 1) Create an issue in the issues section (and tag it as ``maintainance``), 2) provide us with an example (e.g. open a pull request). Do not insert critical information into the comment field.\n\n> **FORMAT** Please ensure that Variable/Joining and MHC names in your submission come from IMGT nomenclature (this does not apply to donor MHC typing fields).\n\nThe ``BuildDatabase`` routine will be executed during CI tests upon each submission and prior to every database release implements table format checks, CDR3 sequence checks and fixes (if possible), and VDJdb confidence score assignment (see below).\n\nTo view the list of papers that were not yet processed follow [here](https://github.com/antigenomics/vdjdb-db/labels/paper).\n\nAn XLS template is available [here](https://raw.githubusercontent.com/antigenomics/vdjdb-db/master/template.xls).\n\n> **CAUTION** make sure that nothing is messed up (``x/X`` frequencies are transformed to dates, bad encoding, etc) when importing from XLS template. The format of all fields is pre-set to *text* to prevent this case.\n\n## Database specification\n\nEach database submission in ``chunks/`` folder should have the following header and columns:\n\n### Complex information columns (required)\n\nThese columns convey full information about TCR:peptide:MHC complex and are mandatory for any submission.\n\ncolumn name     | description\n----------------|-------------\ncdr3.alpha | TCR alpha CDR3 amino acid sequence. Complete sequence starting with C and ending with F/W should be provided if possible. Trimmed sequences will be fixed at database building stage in case sufficient V/J germline parts are present\nv.alpha | TCR alpha Variable (V) segment id, up to best resolution possible (``TRAVX*XX``, e.g. ``TRAV7``, ``TRAV7*01``, ``TRAV7*02``...). Strictly IMGT nomenclature. Can be left blank if unknown.\nj.alpha | TCR alpha Joining (J) segment id\ncdr3.beta | TCR beta CDR3 amino acid sequence\nv.beta | TCR beta V segment id\nj.beta | TCR beta J segment id\nspecies | TCR parent species (``HomoSapiens``, ``MusMusculus``,...)\nmhc.a | First MHC chain allele, to best resolution possible, ``HLA-X*XX:XX``, e.g. ``HLA-A*02:01``\nmhc.b | Second MHC chain allele (``B2M`` for MHCI)\nmhc.class | ``MHCI`` or ``MHCII``\nantigen.epitope | Amino acid sequence of the epitope\nantigen.gene | Parent gene of the epitope sequence (e.g. ``pp24``)\nantigen.species | Parent species of the antigen, to the best clade resolution possible (e.g. ``HIV-1``, ``HIV-1*HXB2``)\nreference.id | Pubmed id, doi, etc\nsubmitter | Name of submitting person/organization\n\n> **Notes:**\n\n> In case given record represents a clonotype with either TCR alpha or beta sequence unknown, missing CDR3/V/(D)/J fields should be left blank.\n\n> V/(D)/J fields can be left blank, however this will abrogate CDR3 fixing/verification procedure for a given record.\n\n> Any record should have at least one of CDR3 alpha/beta fields that are not blank.\n\n### Method information columns (optional)\n\nOptional columns (i.e. it is not required to fill them, but they **should** be present in table header) that ensure correct confidence ranking of a given entry. Used to calculate a single confidence score based on various factors, e.g. fraction of a given TCRab sequence among tetramer+ clones sequenced and verification experiments performed.\n\ncolumn name     | description\n----------------|-------------\nmethod.identification | ``tetramer-sort``, ``dextramer-sort``, ``pelimer-sort``, ``pentamer-sort``, etc for sorting-based identification. For molecular assays use: ``antigen-loaded-targets`` (if T cells specificity was analysed against cells incubatetd with antigenic peptide), ``antigen-expressing-targets`` (if T cells specificity was analysed against cells tranformed with antigenic organism, protein or peptide, e.g. BCL transformed with EBV). For magnetic cell separation use ``beads`` keyword. Add ``cultured-T-cells`` or ``limiting-dilution-cloning`` if T cells were cultured before sequencing as in this case ``method.frequency`` will have completely different meaning. Use comma to separate phrases. For cases that use UMI-tagged multimers use ``tetramer-umi``, etc.\nmethod.frequency | Frequency in isolated antigen-specific population, reported as ``X/X`` if possible, e.g. ``7/30`` if a given V/D/J/CDR3 is encountered in 7 out of 30 tetramer+ clones. Formats ``X%``, ``X.X%`` and ``X.X`` are also supported.\nmethod.singlecell | ``yes`` if single cell sequencing was performed, blank otherwise\nmethod.sequencing | Sequencing method: ``sanger``, ``rna-seq`` or ``amplicon-seq``\nmethod.verification | ``tetramer-stain``, ``dextramer-stain``, ``pelimer-stain``, ``pentamer-stain``, etc for methods that include TCR cloning and re-staining with multimers. For magnetic cell separation use ``beads`` keyword. ``restimulation``, ``co-culture``, ``antigen-loaded-targets``, ``antigen-expressing-targets`` for molecular assays that validate specificity of **cloned** T-cell receptors. ``direct`` in case the affinity of TCRs of specific T-cells to the pMHC is quantified directly in some way. Several comma-separated verification methods can be specified.\n\n> **Notes:**\n\n> In case ``method.identification`` is left blank, the record is automatically assigned with a lowest confidence score possible.\n\n> For special cases such as CD8-null tetramers that utilize HLA with mutated residues that abrogate CD8 binding, specify ``cd8null-tetramer`` in ``method.identification`` field rather than using ``mhc.a`` field.\n\nDuring database build phase, the information from columns mentioned above is collapsed to a JSON string and stored in a single ``method`` column, e.g.:\n```json\n{\n   ""identification"":""tetramer-sort"",\n   ""frequency"":""5/13"",\n   ""sequencing"":""sanger"",\n   ""verification"":""antigen-loaded-targets""\n}\n```\n\n### Meta-information columns (optional)\n\ncolumn name     | description\n----------------|-------------\nmeta.study.id | Internal study id\nmeta.cell.subset | T-cell subset, free style, e.g. ``CD8+``, ``CD4+CD25+``\nmeta.subset.frequency | Frequency of a given TCR sequence in specified cell subset, e.g. ``5%`` means that the TCR sequence represents an expanded clone occupying 5% of CD8+ cells\nmeta.subject.cohort | Subject cohort, free style, e.g. ``healthy`` or ``HIV+``. If possible, specify to what extent a healthy donor is healthy, e.g. ``CMV-seronegative``.\nmeta.subject.id | Subject id (e.g. ``donor1``, ``donor2``,...)\nmeta.replica.id | Replicate sample coming from the same donor, also applies for different time points, etc (e.g. ``5mo``)\nmeta.clone.id | T-cell clone id\nmeta.epitope.id | Epitope id (e.g. ``FL10``)\nmeta.tissue | Tissue used to isolate T-cells: ``PBMC``, ``spleen``, etc. or ``TCL`` (T-cell culture) if isolated from re-stimulated T-cells\nmeta.donor.MHC | Donor MHC list if available, blank otherwise. IMGT nomenclature (e.g. HLA-A*02:01) is preferable. Allele group names (e.g. ``A02``, ``B18``) is also acceptable (don't use asterisk in such cases). Use comma to separate alleles.\nmeta.donor.MHC.method | Donor MHC typing method if available, blank otherwise\nmeta.structure.id | PDB structure ID if exists, or blank. Records having a structural data associated with them will automatically get the highest confidence score.\ncomment | Plain text comment, maximum 140 characters\n\n> **Note:**\n\n> While these columns are optional, subject identifier, replica identifier, etc are used when scanning submission for duplicates. Normally duplicate records (with identical **complex information** columns) are not allowed, but they will not be considered as duplicates in case they have distinct id fields mentioned above.\n\nDuring database build phase, the information from columns mentioned above is collapsed to a JSON string and stored in a single ``meta`` column, e.g.:\n```json\n{\n   ""cell.subset"":""CD8+"",\n   ""subject.cohort"":""HSV-2+"",\n   ""subject.id"":12,\n   ""clone.id"":46,\n   ""tissue"":""PBMC""\n}\n```\n\n### Condition association columns (for extended database, TBA)\n\nCondition metadata:\n\ncolumn name    | description\n---------------|------------\ncondition.name | natural language terms like ``T1D``, ``pollen allergy``, ``BRCA`` or ``YF vaccination``\ncondition.id   | ``ICD-11:5A10`` for ``T1D`` in [ICD-11](https://icd.who.int/browse11/l-m/en) or ``OMIM:114480`` for ``breast cancer`` in [OMIM](https://www.omim.org/entry/114480)\ncondition.type | ``infection``, ``vaccination``, ``cancer``, ``allergy`` or ``autoimmune``\ncondition.subtype | natural language terms like ``acute`` or ``poor prognosis`` or ``grade II``\n\nAssociation metadata:\n\ncolumn name    | description\n---------------|------------\ncondition.freq | fraction of samples matching the entry\ncondition.count | number of samples matching the entry (can be blank)\npopulation.freq | fraction of controls matching the entry, or Pgen computed by [OLGA/IgOR](https://github.com/statbiophys/OLGA/tree/master/olga)\npopulation.count | number of controls matching the entry (can be blank)\nassociation.pvalue | Association P-value, e.g. enrichment P-value for Fisher's exact test\nassociation.test | ``Fisher``, ``TCRNET``, ``ALICE`` or another statistical method\n\n### Ambiguous antigens (for extended database, TBA)\n\nPeptide pools, long peptides for T-cell culture expansion, non-peptide ligands\n\ncolumn name    | description\n---------------|------------\nantigen.epitope.long | encompassing protein sequence containing the epitope\nantigen.peptide.pool | e.g. ``MIRA COVID19`` TBD\nantigen.nonpeptide | ``α-GalCer`` or ``KRN7000`` TBD\n\n### Non TRAB columns (for extended database, TBA)\n\nInformation for non alpha-beta T-cells, CAR-T, etc\n\ncolumn name    | description\n---------------|------------\nv.delta | ID of Variable segment in delta chain\ncdr3.delta | CDR3 of delta chain\n... | ...\nv.heavy.shm | CIGAR string of hypermutations in the heavy chain Variable segment\n... | ...\n\n## Database processing\n\n### CDR3 sequence fixing\n\nAt this stage, a series of checks is performed for CDR3 sequence and reported V/J segments:\n\n* In case of *canonical* (starting with conserved ``C`` and ending with ``F/W``) CDR3 sequences: checks if 5' and 3' germline parts match corresponding V/J segment sequences.\n* In case of truncated CDR3 sequences: adds conserved ``C/F/W`` residues. Can add more missing residues in case a relatively large contiguous V/J germline match is present.\n* In case excessive germline part is reported (e.g. ``FGXG`` instead of simply ``F`` at CDR3 3' part), excessive residues are removed.\n* Can correct mismatches in V/J germline regions in case a reliable non-contiguous V/J match is found.\n\nThe main reason behind that is that current immune repertoire sequencing (RepSeq) data processing software reports *canonical* clonotype sequences, high number antigen-specific TCR sequences present in literature are reported inconsistently. The latter greatly complicates annotation of RepSeq data using known antigen-specific TCR sequences.\n\nIn case of good V/J germline matching and errors in CDR3 sequence, the final CDR3 sequence in the database is replaced by its fixed version. The following report of CDR3 fixer is placed under ``cdr3fix.alpha`` and ``cdr3fix.beta`` columns, e.g.\n\n```json\n{\n	""fixNeeded"":true,\n	""good"":false,\n	""cdr3"":""CASSQDVGTGGVFALYF"",\n	""cdr3_old"":""CASSQDVGTGGVFALY"",\n	""jFixType"":""FixAdd"",\n	""jId"":""TRBJ1-6*01"",\n	""jCanonical"":true,\n	""jStart"":14,\n	""vFixType"":""FailedBadSegment"",\n	""vId"":null,\n	""vCanonical"":true,\n	""vEnd"":-1\n	}\n```\n\nand\n\n```json\n{\n	""fixNeeded"":true,\n	""good"":true,\n	""cdr3"":""CASSLSRGGNQPQYF"",\n	""cdr3_old"":""CASSLSRGGNQPQY"",\n	""jFixType"":""FixAdd"",\n	""jId"":""TRBJ1-5*01"",\n	""jCanonical"":true,\n	""jStart"":9,\n	""vFixType"":""NoFixNeeded"",\n	""vId"":""TRBV14*01"",\n	""vCanonical"":true,\n	""vEnd"":4\n}\n```\n\nField descriptions:\n\nfield | description\n------|-------------\n``fixNeeded`` | ``true`` if corrected CDR3 sequence differs from the original one, ``false`` otherwise\n``good`` | ``true`` if the fix can be applied, ``false`` if the fix cannot be applied due to bad V/J entry or no V/J matching\n``cdr3`` | Fixed CDR3 sequence\n``cdr3_old`` | Original CDR3 sequence\n``jFixType`` | Type of fix applied to CDR3 J germline part\n``jCanonical`` | ``true`` if CDR3 ends with ``F`` or ``W``, ``false`` otherwise\n``jId``  | J segment identifier\n``jStart``  | A 0-based index of first CDR3 amino acid that belongs to J segment\n``vFixType`` | Type of fix applied to CDR3 V germline part\n``vCanonical`` | ``true`` if CDR3 starts with ``C``, ``false`` otherwise\n``vId`` | V segment identifier\n``vEnd``  | A 0-based index of the last CDR3 amino acid of V segment plus one\n\n> **Note:**\n\n> Possible V and J fix types: ``NoFixNeeded``, ``FixAdd``, ``FixReplace``, ``FixTrim``, ``FailedReplace`` (too many mismatches), ``FailedBadSegment`` (bad segment entry), ``FailedNoAlignment`` (no alignment at all)\n\n### VDJdb scoring\n\nAt the final stage of database processing, TCR:peptide:MHC complexes are assigned with confidence scores. Scores are computed according to reported **method** entries.\n\nVDJdb scoring is performed by evaluating TCR sequence, identification and verification confidence based on the following criteria:\n\n1. Ensuring TCR sequence is correctly identified according to ``method.sequencing`` and ``method.singlecell`` (1-3 points)\n    * sanger - several cells sequenced (2+ cells sequenced according to ``method.frequency``) - 2 points, otherwise 1\n    * amplicon-seq - frequency is higher than ``0.01`` - 2 points, otherwise 0\n    * single-cell - 3 points if performed\n2. Initial identification of TCR:pMHC is correct according to ``method.identification`` (0-1 point)\n    * sort-based - frequency is higher than ``0.1`` according to ``method.frequency``)\n    * culture-based - frequency is higher than ``0.5``\n    * limiting dilution/culture prior to sequencing - the ``method.frequency`` becomes somewhat ambigous, check if it is higher than ``0.5``\n3. Verification T-cell specificity (0-3 points)\n    * direct method - 3 points, e.g. has PDB id (``meta.structure.id`` is not empty) or some other method that directly evaluates TCR:pMHC binding\n    * target stimulation-based - 2 points\n    * staining-based - 1 points\n    * If verification is performed, then the TCR sequence is assumed to be known, so score from ``1.`` is set to 3\n\nThe final score is then calculated as minimal between score from part ``1.`` and sum of scores from part ``2.`` and part ``3.``.\n\nMaximal score is then selected among different records (independent submissions, replicas, etc) pointing to the same unique complex entry (i.e. set of unique **complex** fields).\n\nscore | description\n------|----------------------\n0     | Low confidence/no information - a critical aspect of sequencing/specificity validation is missing\n1     | Moderate confidence - no verification / poor TCR sequence confidence\n2     | High confidence - has some specificity verification, good TCR sequence confidence\n3     | Very high confidence - has extensive verification or structural data\n\n## Database build contents\n\nThe final database assembly can be found in the ``database/`` folder upon execution of ``BuildDatabase.groovy`` script:\n\n* ``vdjdb_full.txt`` - combined chunks with TCRalpha/beta records, antigen information, etc. All method and meta information are collapsed into two columns with corresponding names. VDJdb scores and CDR3 fixing information for TCR alpha and beta are given in separate columns. This is the raw version of VDJdb.\n* ``vdjdb.txt`` - a collapsed version of database used for annotation of single-chain TCR sequencing data by VDJdb-standalone software. Each line corresponds to either TCR alpha or TCR beta record as specified by the ``gene`` column. TCR records coming from the same alpha-beta pair have the same index in ``complex.id`` column. In case ``complex.id`` is equal to ``0`` a record doesn't have either TCRalpha or TCRbeta chain information. This table is used by VDJdb-standalone and VDJdb-server.\n* ``vdjdb.meta.txt`` - metadata for ``vdjdb.txt`` table, used by VDJdb-standalone and VDJdb-server.\n* ``vdjdb.slim.txt`` - a slim database used for annotation of single-chain TCR sequencing data by VDJdb-standalone software. This is a collapsed version of ``vdjdb.txt`` containing unique records for each CDR3:antigen pair and comma-separated lists of values for other columns (``*.segm``,``mhc.*``, ``complex.id`` and ``reference.id``). This table can be easily parsed with R and Python/Pandas, it is intended for end users exploring VDJdb.\n* ``vdjdb.slim.meta.txt`` - metadata for ``vdjdb.slim.txt`` table.\n* ``motif_pwms.txt`` and ``cluster_members.txt`` - position-weight matrices for antigen-specific TCR motifs and representative sets of TCR sequences that constitute them. These tables are computed separately using code from [vdjdb-motifs](https://github.com/antigenomics/vdjdb-motifs) repository.\n\nNote that some statistics can be generated by running R markdown templates in ``summary/`` folder.\n\n## Building database release and generating summary figures\n\nFirst make sure that you clone both [vdjdb-db repo](https://github.com/antigenomics/vdjdb-db) and [vdjdb-motifs repo](https://github.com/antigenomics/vdjdb-motifs) to the same folder, say ``~/vcs``.\n\nThen navigate to ``vdjdb-db`` and run ``bash release.sh``. You can then find the output in ``~/vcs/vdjdb-db/database``, ``~/vcs/vdjdb-db/summary`` and ``~/vcs/vdjdb-motifs`` folders. Note that you have to check ``.Rmd`` files that will be executed and manually install missing R packages, as well as get [VDJtools binary](https://github.com/mikessh/vdjtools) and place it in the path specified in ``~/vcs/vdjdb-motifs/compute_vdjdb_motifs.Rmd``.\n\n## Database build process with Docker\n\nThe repository contains `Dockerfile` to simplify the database building process. `Dockerfile` instantiates the correct environment needed to build the database.\nIf you have [Docker Desktop](https://www.docker.com/products/docker-desktop) installed and running on your machine use the following command to build local Docker image:\n\n```bash\ndocker build -t vdjdbdb .\n```\n\n**NOTE** You may need `sudo` to run docker.\n\nIn order to build the database using the newly created local Docker image create some folder (e.g. `/tmp/output`) and use it as a external volume when running Docker image. Docker image always puts the result in `/root/output` folder within docker container.\n\n**NOTE**: Host path, e.g. `/tmp/output`, should be absolute.\n\n**NOTE**: Database building process requires at least 64GB of RAM.\n\n```bash\nmkdir -p /tmp/output\ndocker run -v /tmp/output:/root/output vdjdbdb\n```\n\nPre-built images can be found at [DockerHub](https://hub.docker.com/r/mikessh/vdjdb), N.B. replace `vdjdb` with `mikessh/vdjdb:legacy` if running this image.",126,bioinformatics,Groovy,4,Groovy,Python,Shell,Dockerfile,,,,,,,,,,,,,,,,,,,,,,,,,117,8,109,0,82,18,0,15737,27,257,192,65,cdd74ddd03b997ac4b982c794f85a4b317474ab5,Delete tcremp_pipeline.png,2024-07-14T01:06:19Z,yuliajk,74723905+yuliajk@users.noreply.github.com,yuliajk,Hotfix#1 for 2024 summer release,"Fixing some typos, adding some papers",13.06.2024,Mikhail Shugay,,mikessh,Other,vdjdb-db,antigenomics,33,bioinformatics,immunology,database,antigen,t-cell,,,,,,,,,,,,,,,,/antigenomics/vdjdb-db,37,20,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/ANHIG/IMGTHLA,https://github.com/ANHIG/IMGTHLA,0,,,0,0,0,0,0,0,0,0,0,1,0,Github for files currently published in the IPD-IMGT/HLA FTP Directory hosted at the European Bioinformatics Institute,"--------------------------------------------------------------------------------\n IPD-IMGT/HLA Database\n--------------------------------------------------------------------------------\n\nThis directory contains data for the IPD-IMGT/HLA database. The IPD-IMGT/HLA database is a specialist sequence database for sequences of the human histocompatibility complex. This directory contains the IPD-IMGT/HLA flat files and documentation. \n\n### Cloning the Repository\n\n#### From April 2024, Release 3.56.0\n\nAs of Release 3.56.0, due April 2024, all large files (>100MB) will be provided as compressed files rather than utilise Git LFS, which was previously required. This currently includes the hla.dat, xml/hla.xml, xml/hla_ambigs.xml and hla_gen.fasta. This has been done to simplify the cloning process and also due to escalating and unpredictable costs in providing the files using Git LFS from a public repository. All compressed files will use the [ZIP format](https://en.wikipedia.org/wiki/ZIP_(file_format)). This formatting change will be applied to all branches.\n\n#### Up to April 2024\n\nPreviously the repository has required the use of the Git LFS tools (https://git-lfs.github.com) to handle files over 100MB in size. Whilst all hla.dat files are now provided as a zipped file, any pulls from previous commits for Release 3.55.0 and earlier will still require Git LFS. Please use this when cloning the repository to ensure the larger files are downloaded correctly. If Git LFS is not used then large files will contain pointers to the Git LFS location rather than the data required.\n\n--------------------------------------------------------------------------------\nFile Formats \n--------------------------------------------------------------------------------\n\nThe directory also contains the HLA sequences in a number of formats. Within the following folders, the various format types are explained briefly here:\n\n### Alignments folder\n\nFiles designated “X_prot.txt”, where X is a locus or gene, contain protein sequences. Please note that alleles that contain non-coding variations may be identical at the protein level. \n\nFiles designated “X_nuc.txt”, where X is a locus or gene, contain the nucleotide coding sequences (CDS). Please note that alleles that contain non-coding variations may be identical at the CDS level.\n\nFiles designated “X_gen.txt”, where X is a locus or gene, contain genomic DNA sequences. Please note that for alleles that do not possess genomic sequences there will be no entry in the file, or where there is only a single genomic sequence at the locus, a file will not be produced.  \n\nFor further information on the construction of these text files, please refer to the description available here: https://www.ebi.ac.uk/ipd/imgt/hla/alignment/help/. To provide consistency in both formatting and to record versioning information, as of version 3.32.0, the header is designated by hash tags at the start of the line. \n\nA zip compressed archive of all the text-format alignment files is available from the top-level directory. \n\n### FASTA folder\n\nAll files in this folder are provided in the FASTA sequence format. Please note the FASTA format contains no alignment information. Due to large file sizes (>100MB) some fasta files will be provided as compressed files. This currently includes the hla_gen.fasta.\n\nFiles designated “X_prot.fasta”, where X is a locus or gene, contain protein sequences. Please note that alleles that contain non-coding variations may be identical at the protein level. \n\nFiles designated “X_nuc.fasta”, where X is a locus or gene, contain the nucleotide coding sequences (CDS). Please note that alleles that contain non-coding variations may be identical at the CDS level.\n\nFiles designated “X_gen.fasta”, where X is a locus or gene, contain genomic DNA sequences. Please note for alleles that do not possess genomic sequences, there will be no entry in the file.\n\n### MSF Folder\n\nAll files in this folder are provided in the MSF sequence format. \n\nFiles designated “X_prot.msf”, where X is a locus or gene, contain protein sequences. Please note that alleles that contain non-coding variations may be identical at the protein level. \n\nFiles designated “X_nuc.msf”, where X is a locus or gene, contain the nucleotide coding sequences (CDS). Please note that alleles that contain non-coding variations may be identical at the CDS level.\n\nFiles designated “X_gen.msf”, where X is a locus or gene, contain genomic DNA sequences. Please note for alleles that do not possess genomic sequences, there will be no entry in the file.\n\n### OID Folder\n\nFurther information on the OID files can be found in the dedicated README file in the oid directory. As of version 3.32.0, all list files have been converted to csv format, and contain a header. The header is donated by hash tags at the start of the line.  \nhttps://github.com/ANHIG/IMGTHLA/blob/Latest/oid/README.md\n\n### PIR Folder\n\nAll files in this folder are provided in the PIR sequence format. \n\nFiles designated “X_prot.pir”, where X is a locus or gene, contain protein sequences. Please note that alleles that contain non-coding variations may be identical at the protein level. \n\nFiles designated “X_nuc.pir”, where X is a locus or gene, contain the nucleotide coding sequences (CDS). Please note that alleles that contain non-coding variations may be identical at the CDS level.\n\nFiles designated “X_gen.pir”, where X is a locus or gene, contain genomic DNA sequences. Please note for alleles that do not possess genomic sequences, there will be no entry in the file.\n\n### TCE Folder\n\nThe files in this folder provide a listing of the T-Cell Epitope Group Assignments for DPB1 proteins. The assignments are taken from the algorithms used for the online tools at https://www.ebi.ac.uk/ipd/imgt/hla/matching/. The file formart is as follows;\n\n* DPB1 allele, DPB1 protein, Version 1 Assignment, Version 2 Assignment, Comments\n\nAlleles which have yet to be assigned a TCE group using either version are left blank.  \n\n### WMDA Folder\n\nFurther information on the WMDA files can be found in the dedicated README file in the wmda directory. \nhttps://github.com/ANHIG/IMGTHLA/blob/Latest/wmda/README.md\n\n### XML Folder\n\nPlease refer to the relevant XSD file for information regarding the XML files, which can be found here: https://github.com/ANHIG/IMGTHLA/blob/Latest/xml/hla_ambigs.xsd\n\nPlease note in release 3.43.0, there are three XML files for the release, hla.xml, hla_ciwd.xml and hla_ambigs.xml. The hla_ciwd.xml file is an updated version of the hla.xml file and includes the addition of new information from the Common, intermediate and well‐documented HLA alleles in world populations: CIWD version 3.0.0 (https://doi.org/10.1111/tan.13811). This is as new elements have been required to incorporate this data, and the CWD version 2.0.0 data has been recoded to the same structure. In release 3.44.0 and onwards, hla_ciwd.xml will replace hla.xml, and the older format archived.\n\nPlease note in release 3.53.0, there was a change made to the hla.xml. The releaseversions tag attribute releasestatus has been changed to a binary flag containing either ""Public"" or ""Deleted"" to allow for easier filtering of deleted alleles. In addition a releasecomments attribute has been added containing information about changes to this allele with this verison of the database, this contains the information previously stored in the releasestatus attribute.\n\nPlease note in release 3.55.0, there are three XML files for the release, hla.xml, hla_new.xml and hla_ambigs.xml. The hla_new.xml is an updated version of the hla.xml and includes a new release tag containing version and date information for the release. In release 3.56.0 and onwards, hla_new.xml will replace hla.xml, and the older format archived.\n\n### Allele List Folder\n\nLists of alleles for different versions of the database are now included in this single folder due to the large number of files.\n\nThese filenames take the format Allelelist.XXXX.txt with the XXXX in the file denotes a particular release. These files are a csv format detailing for each allele the official name used in each release of the database.\n\n### Other Files\n\nThe top-level directory contains the following files; \n\n* Alignments_Rel_XXXX.zip - a compressed archive of the alignments folder, where the XXXX in the file denotes a particular release.\n* LICENSE.md - a file detailing the licensing of data included in the IPD-IMGT/HLA Database.\n* Nomenclature_2009.txt - a file detailing pre-2010 allele nomenclature\n* README.md - This README file\n* hla.dat.zip - An EMBL-ENA style format file containing data from the IPD-IMGT/HLA Database, see (https://github.com/ANHIG/IMGTHLA/blob/Latest/Manual.md) for further details. \n* hla_gen.fasta - a copy of the file in the fasta directory, includes the DNA sequence for all alleles, which have genomic sequences available. \n* hla_nuc.fasta - a copy of the file in the fasta directory, includes the DNA sequence for the CDS sequence of all alleles. \n* hla_prot.fasta - a copy of the file in the fasta directory, includes the amino acid sequence for all alleles. \n* md5checksum.txt - a file detailing md5 checksums for all files in the top-level directory\n\nThe top-level directory contains the following lists, in order to provide consistency in both formatting and to record versioning information, as of version 3.32.0, all list files have been converted to csv format, and contain a header. The header is designated by hash tags at the start of the line.  \n\n* Allele_status.txt - a csv file detailing for each allele how many times it has been submitted, from how many cells, the unconfirmed/confirmed status of the allele, if the CDS is fully sequenced and if the allele is cDNa or gDNA sequence.\n* Allelelist.txt  - a csv file listing all alleles named at the time of the latest release.\n* Allelelist_history.txt - a csv file detailing for each allele the official name used in each release of the database. \n* Deleted_alleles.txt - a csv file detailing all deleted allele names, with reasons for the deletion. This list also includes details of any suffix changes. \n* release_version.txt - a plain text file which denotes the current release version.\n* sversion_history.txt - a csv file detailing for each allele the Sequence Version used in each release of the database.\n\n### Versioning\n\nThe database version number, IPD-IMGT/HLA 3.44.0 2021-04-20 b9d9ef7, can be interpreted as;\n\n* Database Name\n* Major release number (nomenclature version, quarterly release, sequence version)\n* Date\n* Latest commit for ANHIG/IMGTHLA/Latest branch\n\nThe major release number contains three key fields, the first is the nomenclature version, which is currently 3. The second is the quarterly release number, which is incremented by 1 every January, April, July and October with each subsequent release. The final third number represents the sequence version. A '0' is used for the primary quarterly release, and only incremented if any subsequent interim path or update contains a change to a valid base (not a * or a .) in either the nucleotide (both cDNA and gDNA) or protein sequence. Changes to the positioning of indels, or unsequenced bases are not included if the raw sequence remains unchanged.\n\n--------------------------------------------------------------------------------\n CONTACTS\n--------------------------------------------------------------------------------\n\nFor information on the IPD-IMGT/HLA Database please see the website at:\nhttp://www.ebi.ac.uk/ipd/imgt/hla\n\nAdditional information on sequence file formats is available from:\nhttp://www.ebi.ac.uk/ipd/imgt/hla/download/\n\nFor any other information please contact hla@alleles.org.\n\n--------------------------------------------------------------------------------\n COPYRIGHT NOTICE\n--------------------------------------------------------------------------------\n\nWe have chosen to apply the Creative Commons Attribution-NoDerivs License to all\ncopyrightable parts of our databases, which includes the sequence alignments.\nThis means that you are free to copy, distribute, display and make commercial\nuse of the databases in all legislations, provided you give us credit by citing\nthe following;\n\nBarker D, Maccari G, Georgiou X, Cooper M, Flicek P, Robinson J, Marsh SGE\nThe IPD-IMGT/HLA Database\nNucleic Acids Research(2023), 51(D1): D948-D955\n\nRobinson J, Barker D, Marsh SGE\n25 years of the IPD-IMGT/HLA Database.\nHLA(2024),103(6): e15549\n\nRobinson J, Malik A, Parham P, Bodmer JG, Marsh SGE:\nIMGT/HLA - a sequence database for the human major histocompatibility complex\nTissue Antigens (2000), 55:280-287\n\nWe are strongly opposed to the mirroring of the data contained on our sites, both\nhla.alleles.org and the IPD-IMGT/HLA Database, and would ask that rather than mirror\nthe information, appropriate links are provided where applicable.\n\nIf you intend to distribute a modified version of our data, you must ask us for\npermission first, please contact hla [at] alleles [dot] org for further details\nof how modified data can be reproduced.\n\n--------------------------------------------------------------------------------\n FUNDING\n--------------------------------------------------------------------------------\n\nThe development of the IPD-IMGT/HLA Database was funded by an EU BIOTECH grant. The\nwork of maintaining and updating the database has been supported in the past by\nthe Imperial Cancer Research Fund, the National Institute of Health, the\nNational Marrow Donor Program (NMDP) and more recently by the Anthony Nolan\nTrust. The continual maintenace and any further development of the database\nrelies on alternate sources of financial support, which are actively been sought\nfor the continued maintenance of the database. The Sequence.org initiative at\nthe NMDP has solicited funds from institutions and companies who produce HLA\ntyping reagents, typing systems, and instrumentation or that otherwise utilise\nthese databases in critical components of their business. To learn more about\nhow your business can support the IPD-IMGT/HLA Database, please contact:\nAnna Bedard, (Email: abedard [at] nmdp [dot] org), Be The Match Foundation.\n\nIf you intend to use any of the data found on our sites for commercial use, we\nwould ask you to consider funding the database and the work we do. Without\ncontinued funding the database cannot be maintained.\n\n--------------------------------------------------------------------------------\n DISCLAIMER\n--------------------------------------------------------------------------------\n\nWhere discrepancies have arisen between reported sequences and those stored in\nthe database, the original authors have been contacted where possible, and\nnecessary amendments to published sequences have been incorporated. Future\nsequencing may identify errors and the WHO Nomenclature Committee would welcome\nany evidence that helps to maintain the accuracy of the database. We therefore\nmake no warranties regarding the correctness of the data, and disclaim liability\nfor damages resulting from its use. We cannot provide unrestricted permission\nregarding the use of the data, as some data may be covered by patents or other\nrights. Any medical or genetic information is provided for research, educational\nand informational purposes only. It is not in any way intended to be used as a\nsubstitute for professional medical advice, diagnosis, treatment or care.\n\nWe reserve the right to use information about visitors (IP addresses), date/time\nvisited, page visited, referring website, etc. for site usage statistics and to\nimprove our services.\n",197,bioinformatics,Parrot,1,Parrot,,,,,,,,,,,,,,,,,,,,,,,,,,,,229,5,224,0,60,5,0,2883334,59,151,151,0,d3ab1821a0ea7c5c5fec01d255397baa83691ff5,Merge pull request #380 from ANHIG/3570,2024-07-17T13:37:36Z,dominicbarkerAN,37367778+dominicbarkerAN@users.noreply.github.com,dominicbarkerAN,IPD-IMGT/HLA Release 3.57.0,## What's Changed\r\n* 3560 by @dominicbarkerAN in https://github.com/ANHIG/IMGTHLA/pull/372\r\n* Correction of alignmentreference alleleid in value in hla.xml.zip by @dominicbarkerAN in https://github.com/ANHIG/IMGTHLA/pull/376\r\n* Removal of AA track from R_nuc.txt file by @dominicbarkerAN in https://github.com/ANHIG/IMGTHLA/pull/378\r\n* Correction to hla.xml.zip to include sequence features missing in error by @dominicbarkerAN in https://github.com/ANHIG/IMGTHLA/pull/379\r\n\r\n\r\n**Full Changelog**: https://github.com/ANHIG/IMGTHLA/compare/v3.56.0-alpha...v3.57.0-alpha,v3.57.0-alpha,,,dominicbarkerAN,Other,IMGTHLA,ANHIG,60,bioinformatics,alleles,hla,hla-database,nomenclature,,,,,,,,,,,,,,,,/ANHIG/IMGTHLA,70,46,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/amusecode/amuse,https://github.com/amusecode/amuse,1,,,1,1,1,1,0,0,0,0,0,0,1,Astrophysical Multipurpose Software Environment. This is the main repository  for AMUSE,"# AMUSE: The Astrophysical Multipurpose Software Environment\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1435860.svg)](https://doi.org/10.5281/zenodo.1435860)\n[![PyPI version](https://badge.fury.io/py/amuse.svg)](https://badge.fury.io/py/amuse)\n\nThis repository contains the AMUSE software. With AMUSE you can write\nscripts to simulate astrophysical problems in different domains.\n\nThe project website is:\n\n* www.amusecode.org\n\nand the documentation can be found at:\n\n* https://amuse.readthedocs.io\n\nGetting Started\n===============\n\nIn short, most probably\n\n```bash\npip install amuse\n```\nshould get you going if you have a linux or Mac were you compile \ncodes on (HDF5 and an MPI libraries must be installed). \n\nBelow are some hints for a quick install, if these fail please \nlook for options at the detailed descriptions of the installation \nprocedure in the documents in the 'doc/install' directory.\n\nCompilers\n=========\n\nTo build AMUSE from source you need to have a working build\nenvironment. The AMUSE build system needs C/C++ and fortan 90\ncompilers, we recommend a recent version of GCC. \n\nIn Ubuntu you can setup the environment with (as root):\n\n```bash\napt-get install build-essential curl g++ gfortran gettext zlib1g-dev\n```\n\nOther distributions have similar package or package groups available.\n\nIn macOS you can use the homebrew or macports package manager (both\nrequire the Apple Developer Tools and Xcode to be installed).\n\nFor a Windows 10 machine, AMUSE can be installed in the Windows Subsystem\nfor linux (WSL), and installing e.g. Ubuntu from the Microsoft store. \nIts recommended to use WSL 2. For further installation instructions, see the \nLinux install instructions.\n\nPython\n======\n\nAMUSE needs Python 3 version >=3.7 installed preferably with pip and \nvirtualenv. It may be necessary to update pip to a recent version.\nIf you cannot use Python 3, legacy support for Python 2 is available in the \nAMUSE 12 release and the python2 branch.\n\nInstalling Prerequisites\n========================\n\nThe following libraries need to be installed:\n\n* HDF (version 1.6.5 - 1.12.x)\n* MPI (OpenMPI or MPICH)\n\nThe following are needed for some codes:\n* FFTW (version >= 3.0)\n* GSL\n* CMake (version >= 2.4)\n* GMP (version >= 4.2.1)\n* MPFR (version >= 2.3.1)\n\nInstalling+building AMUSE\n=========================\n\nAMUSE can be installed through pip:\n\n```bash\npip install [--user] amuse\n```\n\nThis will build and install AMUSE with an extensive set of codes.\nIf necessary this will also install some required Python packages:\n\n* Numpy (version >= 1.3.0)\n* h5py (version >= 1.2.2)\n* mpi4py (version >= 1.1.0)\n* pytest (version >= 5.0)\n* docutils (version >= 0.6)\n\nIf you are not using pip these must be installed by hand.\n\nIt is possible to install the minimal framework by:\n\n```bash\npip install [--user] amuse-framework\n```\n\nThis does not include any codes. These can be added\n```bash\npip install [--user] amuse-<code name>\n```\n\nAMUSE Development \n=================\n\nAn AMUSE development install can also be handled through pip by executing (in the root of a clone of the \nrepository)\n\n```bash\npip install -e .\n```\n\nafter this the codes need to be build:\n\n```bash\npython setup.py develop_build\n```\n\nRunning the tests\n=================\nAMUSE comes with a large set of tests, most can be run automatically.\nTo run these tests start the py.test command from the main\namuse directory (directory this README file lives in).\n\nTo run these tests do:\n\n1. install the tests\n\n```bash\npip install [--user] amuse-tests\n```\n(this will install all tests whether or not you have installed the full amuse package)\n\n2. Run the automatic tests\n\n```bash\npytest --pyargs -v amuse.test.suite\n```\nyou can also just run the tests for the specific packages you have installed e.g.\n```bash\npytest --pyargs amuse.test.suite.codes_tests.test_huayno\n```\nyou may have to prefix ```mpiexec -n 1 --oversubscribe``` to the pytest command.\n",151,astronomy,AMPL,28,Shell,Fortran,Python,TeX,C++,C,Java,IDL,MATLAB,Perl,Makefile,AMPL,Mathematica,Cuda,GLSL,Gnuplot,CMake,Forth,xBase,Batchfile,HTML,QMake,M4,Roff,SWIG,SourcePawn,NASL,Cython,458,67,372,19,8,44,84,297564,99,574,452,122,cf040fecfa0e9d9824dc7fc44c5fc1f402928056,Update community-seba.yml (#1062),2024-07-02T14:18:44Z,Steven Rieder,steven@rieder.nl,rieder,v2024.6.0,## What's Changed\r\n* Create .editorconfig by @rieder in https://github.com/amusecode/amuse/pull/1049\r\n* Refactoring apr2024 by @rieder in https://github.com/amusecode/amuse/pull/1051\r\n* Update ci.yml by @rieder in https://github.com/amusecode/amuse/pull/1060\r\n* Adds hermite_grx package\r\n\r\n\r\n**Full Changelog**: https://github.com/amusecode/amuse/compare/v2024.4.0...v2024.6.0,v2024.6.0,Steven Rieder,,rieder,Apache License 2.0,amuse,amusecode,36,astrophysics,astronomy,simulations,python,,,,,,,,,,,,,,,,,/amusecode/amuse,52,17,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/AMReX-Astro/Castro,https://github.com/AMReX-Astro/Castro,1,,,1,1,1,1,0,0,0,0,0,0,1,"Castro (Compressible Astrophysics): An adaptive mesh, astrophysical compressible (radiation-, magneto-)  hydrodynamics simulation code for massively parallel CPU and GPU architectures.","[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.2301848.svg)](https://doi.org/10.5281/zenodo.2301848)\n[![DOI](https://joss.theoj.org/papers/10.21105/joss.02513/status.svg)](https://doi.org/10.21105/joss.02513)\n[![AMReX](https://amrex-codes.github.io/badges/powered%20by-AMReX-red.svg)](https://amrex-codes.github.io)\n[![yt-project](https://img.shields.io/static/v1?label=""works%20with""&message=""yt""&color=""blueviolet"")](https://yt-project.org)\n[![github pages](https://github.com/AMReX-Astro/Castro/workflows/github%20pages/badge.svg)](https://github.com/AMReX-Astro/Castro/actions?query=workflow%3A%22github+pages%22)\n[![coverity](https://scan.coverity.com/projects/29689/badge.svg)](https://scan.coverity.com/projects/amrex-astro-castro)\n\n![Castro](https://github.com/AMReX-Astro/Castro/blob/development/Util/logo/castro_logo_hot_200.png)\n\n*an adaptive mesh, astrophysical radiation hydrodynamics simulation code*\n\n`Castro` is an adaptive-mesh compressible radiation / MHD / hydrodynamics\ncode for astrophysical flows.  `Castro` supports a general equation of\nstate, full Poisson gravity, and reactive flows, and is parallelized\nwith MPI + OpenMP for CPUs and MPI + CUDA for NVIDIA GPUs and MPI + HIP for\nAMD GPUs.\n\nMore information on Castro can be found here:\n\nhttp://amrex-astro.github.io/Castro/\n\n\n## Getting Started\n\nThe ""Getting Started"" section of the User's Guide walks you\nthrough running your first problem:\n\nhttps://amrex-astro.github.io/Castro/docs/getting_started.html\n\nThis will have you clone Castro and its dependencies (AMReX and\nStarKiller Microphysics),\n\nThe User's Guide in written in re-structured text using Sphinx, with\nthe source in `Castro/Docs/`, and is built automatically\nfrom the `development` branch.\n\n## Running at Supercomputer Centers\n\nDocumentation for running the AMReX Astrophysics codes at popular\nsupercomputing centers can be found at:\nhttps://amrex-astro.github.io/workflow/\n\n## Development Model:\n\nDevelopment generally follows the following ideas:\n\n  * New features are committed to the `development` branch.\n\n    Nightly regression testing is used to ensure that no answers\n    change (or if they do, that the changes were expected).\n\n    If a change is critical, we can cherry-pick the commit from\n    `development` to `main`.\n\n  * Contributions are welcomed from anyone in the form of a pull\n    request from your fork of Castro, targeting the `development`\n    branch. (If you mistakenly target `main`, we can change it\n    for you.)\n\n    Please add a line to `CHANGES.md` summarizing your change if it\n    is a bug fix or new feature.  Reference the PR or issue as\n    appropriate. Additionally, if your change fixes a bug (or if\n    you find a bug but do not fix it), and there is no current\n    issue describing the bug, please file a separate issue describing\n    the bug, regardless of how significant the bug is. If possible,\n    in both the `CHANGES.md` file and the issue, please cite the pull\n    request numbers or git commit hashes where the problem was\n    introduced and fixed, respectively.\n\n    We will squash commits upon merge to have a clean history.\n    *Please ensure that your PR title and and the PR summary field are\n    descriptive, since these will be used for a squashed commit message.*\n\n  * On the first workday of each month, we perform a merge of\n    `development` into `main`, in coordination with `AMReX`,\n    `Maestro`, and `Microphysics`.  For this merge to take place, we\n    need to be passing the regression tests.\n\n    To accommodate this need, we close the merge window into\n    `development` a few days before the merge day.  While the merge\n    window is closed, only bug fixes should be pushed into\n    `development`.  Once the merge from `development` -> `main` is\n    done, the merge window reopens.\n\n\n## Core Developers\n\nPeople who make a number of substantive contributions will be named\n""core developers"" of Castro.  The criteria for becoming a core\ndeveloper are flexible, but generally involve one of the following:\n\n  * 10 non-merge commits to `Castro/Source/` or `Castro/Docs/`\n    or one of the problems that is not your own science problem *or*\n\n  * addition of a new algorithm / module  *or*\n\n  * substantial input into the code design process or testing\n\nCore developers will be recognized in the following ways:\n\n  * invited to the group's slack team\n\n  * listed in the User's Guide and website as a core developer\n\n  * listed in the author list on the Zenodo DOI for the project\n    (as given in the .zenodo.json file)\n\n  * invited to co-author general code papers / proceedings describing\n    Castro, its performance, etc.  (Note: science papers will always\n    be left to the science leads to determine authorship).\n\nIf a core developer is inactive for 3 years, we may reassess their\nstatus as a core developer.\n\n\n\n## Getting help\n\nWe use Github discussions for asking general questions about the code:\n\nhttps://github.com/AMReX-Astro/Castro/discussions\n",293,astrophysics,C++,8,C++,Makefile,Fortran,Gnuplot,Python,Shell,Lua,Roff,,,,,,,,,,,,,,,,,,,,,2240,140,2072,28,25,46,982,173112,99,673,571,102,8521f818df844d8c2d559475c4dc90ad655ccee7,Merge branch 'development' of github.com:amrex-astro/Castro into deve…,2024-07-03T12:25:03Z,Michael Zingale,michael.zingale@stonybrook.edu,zingale,Release 24.07,,24.07,,,github-actions[bot],Other,Castro,AMReX-Astro,75,hydrodynamics,cfd,pde,gravity,radiation,castro,amr,adaptive-mesh-refinement,reactions,astrophysical-simulation,astrophysics,,,,,,,,,,/AMReX-Astro/Castro,122,18,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/AllskyTeam/allsky,https://github.com/AllskyTeam/allsky,0,,,0,0,0,0,0,0,1,0,0,0,0,A Raspberry Pi operated Wireless Allsky Camera,"# Allsky Camera ![Release](https://img.shields.io/badge/Version-v2023.05.01_04-green.svg) [![Donate](https://img.shields.io/badge/Donate-PayPal-green.svg)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=MEBU2KN75G2NG&source=url)\n\nThis is the source code for the Allsky Camera project described [on Instructables](http://www.instructables.com/id/Wireless-All-Sky-Camera/).\n&nbsp;  \n<p align=""center"">\n<img src=""https://github.com/thomasjacquin/allsky/blob/master/assets/allsky_camera.png"" width=""50%"" title=""Example of an allsky camera"">\n</p>\n\n> **This README and the [Allsky documentation](https://htmlpreview.github.io/?https://raw.githubusercontent.com/thomasjacquin/allsky/master/html/documentation/index.html) will help get your allsky camera up and running.**\n\n&nbsp;  \n\n<!-- =============================================================================== --> \n## Requirements\n\nYou will need the following:\n\n * A Raspberry Pi (Zero 2, 2, 3, 4) running Pi OS.\n * A camera (Raspberry Pi HQ, Module 3, or RPi compatible, or ZWO ASI)\n\n\n&nbsp;  \n> **NOTES:**\n>	- Only the Raspberry Pi OS is supported (Buster, Bullseye, or Bookworm).  Other operating systems like Ubuntu are NOT supported.\n> 	- **NOTE**: support for Buster is going away so please upgrade to Bookworm.\n>	- The ZWO ASI120-series cameras are not recommended due to somewhat poor quality and tendency to produce timeout errors. See [Troubleshooting --> ZWO Cameras](https://htmlpreview.github.io/?https://raw.githubusercontent.com/thomasjacquin/allsky/master/html/documentation/troubleshooting/ZWOCameras.html) for notes on the ASI120-series and related T7 / T7C cameras.\n>	- The Pi Zero with its limited memory and _very_ limited CPU power (single CPU core), is **not** recommended.  You will most likely not be able to create keograms, startrails, or timelapse videos.\n>	- The Pi Zero 2 with its limited memory and somewhat limited CPU power, is not recommended unless cost is the only concern.  Creating keograms, startrails, and timelapse videos may or may not be possible.\n>	- The Le Potato is the only ""Pi-compatible"" board that we've found to actually be compatible, so buyer beware.\n---\n\n\n&nbsp;\n<!-- =============================================================================== --> \n## Software Installation\n\nPatriotAstro created a great [video](https://www.youtube.com/watch?v=7TGpGz5SeVI) describing the installation steps below.\n**We highly suggest viewing it before installing the software.**\n\nDetailed installation instructions can be found at [Installing / Upgrading --> Allsky](https://htmlpreview.github.io/?https://raw.githubusercontent.com/thomasjacquin/allsky/master/html/documentation/installations/Allsky.html).\n\n---\n\n\n&nbsp;\n<!-- =============================================================================== --> \n## Web User Interface (WebUI)\n\n<p align=""center"">\n<img src=""https://github.com/thomasjacquin/allsky/blob/master/html/documentation/settings/AllskySettingsPage.png"" style=""border: 1px solid black"">\n</p>\n\nThe WebUI is now installed as part of Allsky and is used to administer Allsky, and to a lesser extent, your Pi. It can also be used to view the current image as well as all saved images, keograms, startrails, and timelapse videos.\n\nA public page is also available in order to view the current image without having to log into the WebUI and without being able to do any administrative tasks. This can be useful for people who don't have a Allsky Website but still want to share a view of their sky:\n\n```\nhttp://your_raspberry_IP/public.php\n```\n\nMake sure this page is publically viewable.\nIf it is behind a firewall consult the documentation for your network equipment for information on allowing inbound connections.\n\n---\n\n&nbsp;\n<!-- =============================================================================== --> \n## Allsky Website\n\nBy installling the optional Allsky Website you can display your files on a website on the Pi, on another machine, or on both.\n\nSee [Installation / Upgrading --> Website](https://htmlpreview.github.io/?https://raw.githubusercontent.com/thomasjacquin/allsky/master/html/documentation/installations/AllskyWebsite.html) for information on how to install and configure an Allsky Website.\n\n---\n\n\n&nbsp;\n<!-- =============================================================================== --> \n## Post-capture processing\n\nCaptured images can be resized, cropped, and stretched, and bad images (i.e., too light or too dark) can be removed automatically.\n\nAllsky supports running ""modules"" after each picture is taken to change the image (e.g., add an overlay) or perform other tasks (e.g., count the number of stars in the image).  You can determine what modules to run and in what order.  Modules can pass data to other modules, for example, the Start Count Module can pass the star count to the Overlay Module to be added to the overlay.\n\nThe Overlay Editor lets you easily specify what text and images you want in your overlay, and place them using drag-and-drop.  Each field can be formatted however you want (font, color, size, position, rotation, etc.).  The only limit is your imagination!!\n\nSee [Explanations / How To -> Overlays](https://htmlpreview.github.io/?https://raw.githubusercontent.com/thomasjacquin/allsky/master/html/documentation/overlays/overlays.html) and [Explanations / How To -> Modules](https://htmlpreview.github.io/?https://raw.githubusercontent.com/thomasjacquin/allsky/master/html/documentation/modules/modules.html) for more information.\n\n---\n\n\n&nbsp;\n<!-- =============================================================================== --> \n## Dark frame subtraction\n\nDark frame subtraction removes hot pixels from images by taking images at different temperatures with a cover on your camera lens and subtracting those images from nighttime images.\n\nSee [Explanations / How To -> Dark frames](https://htmlpreview.github.io/?https://raw.githubusercontent.com/thomasjacquin/allsky/master/html/documentation/explanations/darkFrames.html) for more information.\n\n---\n\n\n&nbsp;\n<!-- =============================================================================== --> \n## Timelapse and mini timelapse\n\nBy default, a timelapse video is generated at the end of nighttime from all of the images captured in the last 24 hours.\n\n""Mini"" timelapse videos can also be created every few images, and contain the last several images.  They are useful to see what the sky was recently like.\n\n---\n\n\n&nbsp;\n<!-- =============================================================================== --> \n## Keograms\n\n<p align=""center"">\n<img src=""https://github.com/thomasjacquin/allsky/blob/master/assets/Keogram.png"" width=""75%"">\n</p>\n\nA **Keogram** is an image giving a quick view of the day's activity.\nFor each image a central vertical column 1 pixel wide is extracted. All these columns are then stitched together from left to right. This results in a timeline that reads from dawn to the end of nighttime (the image above only shows nighttime data since daytime images were turned off).\n\nSee [Explanations / How To --> Keograms](https://htmlpreview.github.io/?https://raw.githubusercontent.com/thomasjacquin/allsky/master/html/documentation/explanations/keograms.html).\n\n\n---\n\n\n&nbsp;\n<!-- =============================================================================== --> \n## Startrails\n\n<p align=""center"">\n<img src=""https://github.com/thomasjacquin/allsky/blob/master/assets/Startrails.png"" width=""50%"">\n</p>\n\n**Startrails** are generated by stacking all the images from a night on top of each other.\nIn the image above, Polaris is centered about one-fourth the way from the top.\n\nSee [Explanations / How To --> Startrails](https://htmlpreview.github.io/?https://raw.githubusercontent.com/thomasjacquin/allsky/master/html/documentation/explanations/startrails.html).\n	\n\n---\n\n\n&nbsp;\n<!-- =============================================================================== --> \n## Automatic deletion of old data\n\nYou can specify how many days worth of images to keep in order to keep the Raspberry Pi SD card from filling up.  If you have the Allsky Website installed on your Pi, you can specify how many days worth of its imags to keep.\n\n\nSee the **DAYS_TO_KEEP** and **WEB_DAYS_TO_KEEP** settings in [Settings --> Allsky](https://htmlpreview.github.io/?https://raw.githubusercontent.com/thomasjacquin/allsky/master/html/documentation/settings/allsky.html).\n\n---\n\n\n\n&nbsp;\n<!-- =============================================================================== --> \n## Share your sky\n\n\nIf you want your allsky camera added to the [Allsky map](http://www.thomasjacquin.com/allsky-map), see [Put your camera on Allsky Map](https://htmlpreview.github.io/?https://raw.githubusercontent.com/thomasjacquin/allsky/master/html/documentation/miscellaneous/AllskyMap.html).\n\nIf you know anyone in Greenland or Antartica, send them a camera!!\n\n<p align=""center"">\n<a href=""https://www.thomasjacquin.com/allsky-map/"" title=""Allsky map example - click to see real map"">\n<img src=""https://github.com/thomasjacquin/allsky/blob/master/html/documentation/miscellaneous/allsky-map-with-pins.png"">\n</a>\n</p>\n\n---\n\n\n&nbsp;\n<!-- =============================================================================== --> \n## Release changes\n\nSee the\n[Allsky Version Change Log](https://htmlpreview.github.io/?https://raw.githubusercontent.com/thomasjacquin/allsky/master/html/documentation/changeLog.html)\nfor a list of changes in this release and all prior releases.\n\n---\n\n\n&nbsp;\n<!-- =============================================================================== --> \n## Donation\nIf you found this project useful, here's a link to send Thomas a cup of coffee :)\n\n[![](https://www.paypalobjects.com/en_US/i/btn/btn_donate_SM.gif)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=MEBU2KN75G2NG&source=url)\n",1120,astronomy,JavaScript,9,Shell,Makefile,C,C++,Python,CSS,PHP,JavaScript,HTML,,,,,,,,,,,,,,,,,,,,1878,71,1804,3,18,25,0,135879,175,875,836,39,478f3e67591d4e8b94ba9e2a9f72181e9c2e8092,Merge pull request #3715 from AllskyTeam/3714-revert-bit-depth-change,2024-06-30T23:52:11Z,Eric Claeys,83164203+EricClaeys@users.noreply.github.com,EricClaeys,v2023.05.01_04,"Point Release 4 for base release v2023.05.01.\r\nSee the Change Log in the documentation for an overview of the changes.\r\n\r\n## What's Changed\r\n* Hotfix to remove colour error by @Alex-developer in https://github.com/thomasjacquin/allsky/pull/2962\r\n* Fix for Python dependencies by @Alex-developer in https://github.com/thomasjacquin/allsky/pull/2999\r\n* Bump grunt from 1.4.1 to 1.5.3 in /html/js/jquery-ui-1.13.1.custom by @dependabot in https://github.com/thomasjacquin/allsky/pull/3001\r\n* Eliminate flicker in firefox brower by @minichate in https://github.com/thomasjacquin/allsky/pull/3024\r\n* Update install.sh: Temp. check for Bookworm by @EricClaeys in https://github.com/thomasjacquin/allsky/pull/3026\r\n* Point release 4 by @Alex-developer in https://github.com/thomasjacquin/allsky/pull/3027\r\n* Merges from master by @EricClaeys in https://github.com/thomasjacquin/allsky/pull/3037\r\n* Fixes by @Alex-developer in https://github.com/thomasjacquin/allsky/pull/3118\r\n* Added support for ArduCam IMX462 by @will-rigby in https://github.com/thomasjacquin/allsky/pull/3123\r\n* Update ASI SDK to 1.31 by @Varnius in https://github.com/thomasjacquin/allsky/pull/3138\r\n* Update AllskyWebsite.html by @Dhovin in https://github.com/thomasjacquin/allsky/pull/3160\r\n* Additional error handling by @Alex-developer in https://github.com/thomasjacquin/allsky/pull/3163\r\n* Add missing modules by @Alex-developer in https://github.com/thomasjacquin/allsky/pull/3164\r\n* Fixes for Bookworm and pi 5 by @Alex-developer in https://github.com/thomasjacquin/allsky/pull/3174\r\n* Remove experimental display option by @Alex-developer in https://github.com/thomasjacquin/allsky/pull/3181\r\n* Upgrade to ZWO SDK 1.32 by @EricClaeys in https://github.com/thomasjacquin/allsky/pull/3186\r\n* Remove advanced options by @EricClaeys in https://github.com/thomasjacquin/allsky/pull/3188\r\n* Remove unused python modules by @Alex-developer in https://github.com/thomasjacquin/allsky/pull/3192\r\n* Update FAQ.html: Better explain how to focus by @EricClaeys in https://github.com/thomasjacquin/allsky/pull/3201\r\n* Update startrails.html: minor improvements by @EricClaeys in https://github.com/thomasjacquin/allsky/pull/3204\r\n* #3185 Fixes for locale dp separator by @Alex-developer in https://github.com/thomasjacquin/allsky/pull/3211\r\n* Revert ""#3185 Fixes for locale dp separator"" by @Alex-developer in https://github.com/thomasjacquin/allsky/pull/3212\r\n* #3185 Fix for locale by @Alex-developer in https://github.com/thomasjacquin/allsky/pull/3213\r\n* Fix for missing module by @Alex-developer in https://github.com/thomasjacquin/allsky/pull/3214\r\n\r\n**Full Changelog**: https://github.com/thomasjacquin/allsky/compare/v2023.05.01_03...v2023.05.01_04",v2023.05.01_04,Eric Claeys,,EricClaeys,MIT License,allsky,AllskyTeam,10,allsky-camera,raspberry-pi,astronomy,open-source,,,,,,,,,,,,,,,,,/AllskyTeam/allsky,15,65,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/allenai/scispacy,https://github.com/allenai/scispacy,1,,,1,1,1,1,0,0,0,0,0,0,1,A full spaCy pipeline and models for scientific/biomedical documents.,"<p align=""center""><img width=""50%"" src=""docs/scispacy-logo.png"" /></p>\n\n\nThis repository contains custom pipes and models related to using spaCy for scientific documents.\n\nIn particular, there is a custom tokenizer that adds tokenization rules on top of spaCy's\nrule-based tokenizer, a POS tagger and syntactic parser trained on biomedical data and\nan entity span detection model. Separately, there are also NER models for more specific tasks.\n\n**Just looking to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org)** (Note: this demo is running an older version of scispaCy and may produce different results than the latest version).\n\n\n## Installation\nInstalling scispacy requires two steps: installing the library and intalling the models. To install the library, run:\n```bash\npip install scispacy\n```\n\nto install a model (see our full selection of available models below), run a command like the following:\n\n```bash\npip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz\n```\n\nNote: We strongly recommend that you use an isolated Python environment (such as virtualenv or conda) to install scispacy.\nTake a look below in the ""Setting up a virtual environment"" section if you need some help with this.\nAdditionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**.\n\n\n\n#### Setting up a virtual environment\n\n[Conda](https://conda.io/) can be used set up a virtual environment with the\nversion of Python required for scispaCy.  If you already have a Python\nenvironment you want to use, you can skip to the 'installing via pip' section.\n\n1.  [Follow the installation instructions for Conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html?highlight=conda#regular-installation).\n\n2.  Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):\n\n    ```bash\n    conda create -n scispacy python=3.9\n    ```\n\n3.  Activate the Conda environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy.\n\n    ```bash\n    source activate scispacy\n    ```\n\nNow you can install `scispacy` and one of the models using the steps above.\n\n\nOnce you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:\n```python\nimport spacy\nnlp = spacy.load(""en_core_sci_sm"")\ndoc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals."")\n```\n\n#### Note on upgrading\nIf you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have.\n\n## Available Models\n\nTo install a model, click on the link below to download the model, and then run \n\n```python\npip install </path/to/download>\n```\n\nAlternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running \n```python\npip install CMD-V(to paste the copied URL)\n```\n\n| Model          | Description       | Install URL\n|:---------------|:------------------|:----------|\n| en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|\n| en_core_sci_md |  A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|\n| en_core_sci_lg |  A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|\n| en_core_sci_scibert |  A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|\n| en_ner_craft_md|  A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|\n| en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|\n| en_ner_bc5cdr_md |  A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|\n| en_ner_bionlp13cg_md |  A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|\n\n\n## Additional Pipeline Components\n\n\n### AbbreviationDetector\nThe AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm\n    for identifying abbreviation definitions in biomedical text."", (Schwartz & Hearst, 2003).\n\nYou can access the list of abbreviations via the `doc._.abbreviations` attribute and for a given abbreviation,\nyou can access it's long form (which is a `spacy.tokens.Span`) using `span._.long_form`, which will point to\nanother span in the document.\n\n\n#### Example Usage\n```python\nimport spacy\n\nfrom scispacy.abbreviation import AbbreviationDetector\n\nnlp = spacy.load(""en_core_sci_sm"")\n\n# Add the abbreviation pipe to the spacy pipeline.\nnlp.add_pipe(""abbreviation_detector"")\n\ndoc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \\n           inherited motor neuron disease caused by the expansion \\n           of a polyglutamine tract within the androgen receptor (AR). \\n           SBMA can be caused by this easily."")\n\nprint(""Abbreviation"", ""\t"", ""Definition"")\nfor abrv in doc._.abbreviations:\n	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}"")\n\n>>> Abbreviation	 Span	    Definition\n>>> SBMA 		 (33, 34)   Spinal and bulbar muscular atrophy\n>>> SBMA 	   	 (6, 7)     Spinal and bulbar muscular atrophy\n>>> AR   		 (29, 30)   androgen receptor\n```\n\n> **Note**\n> If you want to be able to [serialize your `doc` objects](https://spacy.io/usage/saving-loading), load the abbreviation detector with `make_serializable=True`, e.g. `nlp.add_pipe(""abbreviation_detector"", config={""make_serializable"": True})`\n\n### EntityLinker\n\nThe `EntityLinker` is a SpaCy component which performs linking to a knowledge base. The linker simply performs\na string overlap - based search (char-3grams) on named entities, comparing them with the concepts in a knowledge base\nusing an approximate nearest neighbours search.\n\nCurrently (v2.5.0), there are 5 supported linkers:\n\n- `umls`: Links to the [Unified Medical Language System](https://www.nlm.nih.gov/research/umls/index.html), levels 0,1,2 and 9. This has ~3M concepts.\n- `mesh`: Links to the [Medical Subject Headings](https://www.nlm.nih.gov/mesh/meshhome.html). This contains a smaller set of higher quality entities, which are used for indexing in Pubmed. MeSH contains ~30k entities. NOTE: The MeSH KB is derived directly from MeSH itself, and as such uses different unique identifiers than the other KBs.\n- `rxnorm`: Links to the [RxNorm](https://www.nlm.nih.gov/research/umls/rxnorm/index.html) ontology. RxNorm contains ~100k concepts focused on normalized names for clinical drugs. It is comprised of several other drug vocabularies commonly used in pharmacy management and drug interaction, including First Databank, Micromedex, and the Gold Standard Drug Database.\n- `go`: Links to the [Gene Ontology](http://geneontology.org/). The Gene Ontology contains ~67k concepts focused on the functions of genes.\n- `hpo`: Links to the [Human Phenotype Ontology](https://hpo.jax.org/app/). The Human Phenotype Ontology contains 16k concepts focused on phenotypic abnormalities encountered in human disease.\n\nYou may want to play around with some of the parameters\nbelow to adapt to your use case (higher precision, higher recall etc).\n\n- `resolve_abbreviations : bool = True, optional (default = False)`\n    Whether to resolve abbreviations identified in the Doc before performing linking.\n    This parameter has no effect if there is no `AbbreviationDetector` in the spacy\n    pipeline.\n- `k : int, optional, (default = 30)`\n    The number of nearest neighbours to look up from the candidate generator per mention.\n- `threshold : float, optional, (default = 0.7)`\n    The threshold that a mention candidate must reach to be added to the mention in the Doc\n    as a mention candidate.\n-   `no_definition_threshold : float, optional, (default = 0.95)`\n        The threshold that a entity candidate must reach to be added to the mention in the Doc\n        as a mention candidate if the entity candidate does not have a definition.\n- `filter_for_definitions: bool, default = True`\n    Whether to filter entities that can be returned to only include those with definitions\n    in the knowledge base.\n- `max_entities_per_mention : int, optional, default = 5`\n    The maximum number of entities which will be returned for a given mention, regardless of\n    how many are nearest neighbours are found.\n\nThis class sets the `._.kb_ents` attribute on spacy Spans, which consists of a\nList[Tuple[str, float]] corresponding to the KB concept_id and the associated score\nfor a list of `max_entities_per_mention` number of entities.\n\nYou can look up more information for a given id using the kb attribute of this class:\n```\nprint(linker.kb.cui_to_entity[concept_id])\n```\n\n#### Example Usage\n```python\nimport spacy\nimport scispacy\n\nfrom scispacy.linking import EntityLinker\n\nnlp = spacy.load(""en_core_sci_sm"")\n\n# This line takes a while, because we have to download ~1GB of data\n# and load a large JSON file (the knowledge base). Be patient!\n# Thankfully it should be faster after the first time you use it, because\n# the downloads are cached.\n# NOTE: The resolve_abbreviations parameter is optional, and requires that\n# the AbbreviationDetector pipe has already been added to the pipeline. Adding\n# the AbbreviationDetector pipe and setting resolve_abbreviations to True means\n# that linking will only be performed on the long form of abbreviations.\nnlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""})\n\ndoc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \\n           inherited motor neuron disease caused by the expansion \\n           of a polyglutamine tract within the androgen receptor (AR). \\n           SBMA can be caused by this easily."")\n\n# Let's look at a random entity!\nentity = doc.ents[1]\n\nprint(""Name: "", entity)\n>>> Name: bulbar muscular atrophy\n\n# Each entity is linked to UMLS with a score\n# (currently just char-3gram matching).\nlinker = nlp.get_pipe(""scispacy_linker"")\nfor umls_ent in entity._.kb_ents:\n	print(linker.kb.cui_to_entity[umls_ent[0]])\n\n\n>>> CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked\n>>> Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the\n  				gene encoding the ANDROGEN RECEPTOR.\n>>> TUI(s): T047\n>>> Aliases (abbreviated, total: 50):\n         Bulbo-Spinal Atrophy, X-Linked, Bulbo-Spinal Atrophy, X-Linked, ....\n\n>>> CUI: C0541794, Name: Skeletal muscle atrophy\n>>> Definition: A process, occurring in skeletal muscle, that is characterized by a decrease in protein content,\n                fiber diameter, force production and fatigue resistance in response to ...\n>>> TUI(s): T046\n>>> Aliases: (total: 9):\n         Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, ....\n\n>>> CUI: C1447749, Name: AR protein, human\n>>> Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene.\n                This protein plays a role in the modulation of steroid-dependent gene transcription.\n>>> TUI(s): T116, T192\n>>> Aliases (abbreviated, total: 16):\n         AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, ...\n```\n\n### Hearst Patterns (v0.3.0 and up)\n\nThis component implements [Automatic Aquisition of Hyponyms from Large Text Corpora](https://www.aclweb.org/anthology/C92-2082.pdf) using the SpaCy Matcher component.\n\nPassing `extended=True` to the `HyponymDetector` will use the extended set of hearst patterns, which include higher recall but lower precision hyponymy relations (e.g X compared to Y, X similar to Y, etc).\n\nThis component produces a doc level attribute on the spacy doc: `doc._.hearst_patterns`, which is a list containing tuples of extracted hyponym pairs. The tuples contain:\n\n- The relation rule used to extract the hyponym (type: `str`)\n- The more general concept  (type: `spacy.Span`)\n- The more specific concept (type: `spacy.Span`)\n\n\n#### Usage:\n\n```python\nimport spacy\nfrom scispacy.hyponym_detector import HyponymDetector\n\nnlp = spacy.load(""en_core_sci_sm"")\nnlp.add_pipe(""hyponym_detector"", last=True, config={""extended"": False})\n\ndoc = nlp(""Keystone plant species such as fig trees are good for the soil."")\n\nprint(doc._.hearst_patterns)\n>>> [('such_as', Keystone plant species, fig trees)]\n```\n\n\n## Citing\n\nIf you use ScispaCy in your research, please cite [ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing](https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Additionally, please indicate which version and model of ScispaCy you used so that your research can be reproduced.\n```\n@inproceedings{neumann-etal-2019-scispacy,\n    title = ""{S}cispa{C}y: {F}ast and {R}obust {M}odels for {B}iomedical {N}atural {L}anguage {P}rocessing"",\n    author = ""Neumann, Mark  and\n      King, Daniel  and\n      Beltagy, Iz  and\n      Ammar, Waleed"",\n    booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",\n    month = aug,\n    year = ""2019"",\n    address = ""Florence, Italy"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""https://www.aclweb.org/anthology/W19-5034"",\n    doi = ""10.18653/v1/W19-5034"",\n    pages = ""319--327"",\n    eprint = {arXiv:1902.07669},\n    abstract = ""Despite recent advances in natural language processing, many statistical models for processing text perform extremely poorly under domain shift. Processing biomedical and clinical text is a critically important application area of natural language processing, for which there are few robust, practical, publicly available models. This paper describes scispaCy, a new Python library and models for practical biomedical/scientific text processing, which heavily leverages the spaCy library. We detail the performance of two packages of models released in scispaCy and demonstrate their robustness on several tasks and datasets. Models and code are available at https://allenai.github.io/scispacy/."",\n}\n```\n\nScispaCy is an open-source project developed by [the Allen Institute for Artificial Intelligence (AI2)](http://www.allenai.org).\nAI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering.\n\n",1651,bioinformatics,Python,3,Dockerfile,Python,Shell,,,,,,,,,,,,,,,,,,,,,,,,,,204,28,174,2,11,43,157,248857,223,315,285,30,021fe76d69b20523d3f94a08b447c27e1a46597e,add non_suppressed to main func (#511),2024-03-30T17:39:23Z,Ethan,ethanhkim97@gmail.com,ethanhkim,v0.5.4,Update for spacy 3.7.x\r\n\r\n## What's Changed\r\n* Fixes #485 Project Page URL in setup.py by @sajedjalil in https://github.com/allenai/scispacy/pull/495\r\n* add progress bar to http_get by @WeixiongLin in https://github.com/allenai/scispacy/pull/499\r\n* Update for spacy 3.7 compatibility by @dakinggg in https://github.com/allenai/scispacy/pull/507\r\n* Update publish workflow to trusted publisher by @dakinggg in https://github.com/allenai/scispacy/pull/508\r\n\r\n## New Contributors\r\n* @sajedjalil made their first contribution in https://github.com/allenai/scispacy/pull/495\r\n* @WeixiongLin made their first contribution in https://github.com/allenai/scispacy/pull/499\r\n\r\n**Full Changelog**: https://github.com/allenai/scispacy/compare/v0.5.3...v0.5.4,v0.5.4,Daniel King,,dakinggg,Apache License 2.0,scispacy,allenai,11,scientific-documents,spacy,custom-pipes,nlp,biomedical,bioinformatics,,,,,,,,,,,,,,,/allenai/scispacy,14,52,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/AlexandrovLab/SigProfilerExtractor,https://github.com/AlexandrovLab/SigProfilerExtractor,1,,,1,1,1,1,0,0,1,0,0,0,0,"SigProfilerExtractor allows de novo extraction of mutational signatures from data generated in a matrix format. The tool identifies the number of operative mutational signatures, their activities in each sample, and the probability for each signature to cause a specific mutation type in a cancer sample. The tool makes use of SigProfilerMatrixGenerator and SigProfilerPlotting. ","[![Docs](https://img.shields.io/badge/docs-latest-blue.svg)](https://osf.io/t6j7u/wiki/home/) \n[![License](https://img.shields.io/badge/License-BSD\%202--Clause-orange.svg)](https://opensource.org/licenses/BSD-2-Clause)\n[![Build Status](https://travis-ci.com/AlexandrovLab/SigProfilerExtractor.svg?branch=master)](https://app.travis-ci.com/AlexandrovLab/SigProfilerExtractor)\n\n# SigProfilerExtractor\nSigProfilerExtractor allows de novo extraction of mutational signatures from data generated in a matrix format. \nThe tool identifies the number of operative mutational signatures, their activities in each sample, and the probability \nfor each signature to cause a specific mutation type in a cancer sample. The tool makes use of SigProfilerMatrixGenerator \nand SigProfilerPlotting. Detailed documentation can be found at: https://osf.io/t6j7u/wiki/home/\n\n# Table of contents\n- [Installation](#installation)\n- [Functions](#functions)\n  - [importdata](#importdata)\n  - [sigProfilerExtractor](#sigProfilerExtractor)\n  - [estimate_solution](#estimate_solution)\n  - [decompose](#decompose)\n  - [PlotActivity.py](#plotActivity)\n- [Video Tutorials](#video_tutorials)\n- [Citation](#citation)\n- [Copyright](#copyright)\n- [Contact Information](#contact)\n\n\n## <a name=""installation""></a> Installation\n\nTo install the current version of this Github repo, git clone this repo or download the zip file.\nUnzip the contents of SigProfilerExtractor-master.zip or the zip file of a corresponding branch.\n\nIn the command line, please run the following:\n```bash\n$ cd SigProfilerExtractor-master\n$ pip install .\n```\n\nFor most recent stable pypi version of this tool,\nIn the command line, please run the following:\n```bash\n$ pip install SigProfilerExtractor\n```\n\nInstall your desired reference genome from the command line/terminal as follows (available reference genomes are: GRCh37, GRCh38, mm9, and mm10):\n```python\n$ python\nfrom SigProfilerMatrixGenerator import install as genInstall\ngenInstall.install('GRCh37')\n```\n\nThis will install the human 37 assembly as a reference genome. You may install as many genomes as you wish.\n\nNext, open a python interpreter and import the SigProfilerExtractor module. Please see the examples of the functions. \n\n## <a name=""functions""></a> Functions\nThe list of available functions are:\n- importdata\n- sigProfilerExtractor\n- estimate_solution\n- decompose\n\nAnd an additional script:\n- plotActivity.py\n\n### <a name=""importdata""></a> importdata\nImports the path of example data.\n\n```python\nimportdata(datatype=""matrix"")\n```\n\n#### importdata Example\n\n```python \nfrom SigProfilerExtractor import sigpro as sig\npath_to_example_table = sig.importdata(""matrix"")\ndata = path_to_example_table \n# This ""data"" variable can be used as a parameter of the ""project"" argument of the sigProfilerExtractor function.\n\n# To get help on the parameters and outputs of the ""importdata"" function, please use the following:\nhelp(sig.importdata)\n```\n\n### <a name=""sigProfilerExtractor""></a> sigProfilerExtractor\n    \nExtracts mutational signatures from an array of samples.\n\n```python \nsigProfilerExtractor(input_type, out_put, input_data, reference_genome=""GRCh37"", opportunity_genome = ""GRCh37"", context_type = ""default"", exome = False, \n                         minimum_signatures=1, maximum_signatures=10, nmf_replicates=100, resample = True, batch_size=1, cpu=-1, gpu=False, \n                         nmf_init=""random"", precision= ""single"", matrix_normalization= ""gmm"", seeds= ""random"", \n                         min_nmf_iterations= 10000, max_nmf_iterations=1000000, nmf_test_conv= 10000, nmf_tolerance= 1e-15, get_all_signature_matrices= False)\n```\n\n| Category | Parameter | Variable Type | Parameter Description |\n| --------- | --------------------- | -------- |-------- |\n| **Input Data** |  |  | |\n|  | **input_type** | String | The type of input:<br><ul><li>""vcf"": used for vcf format inputs.</li><li>""matrix"": used for table format inputs using a tab separated file.</li><li>""bedpe"": used for bedpe files with each SV annotated with its type, size bin, and clustered/non-clustered status. Please check the required format at https://github.com/AlexandrovLab/SigProfilerMatrixGenerator#structural-variant-matrix-generation.</li><li>""seg:TYPE"": used for a multi-sample segmentation file for copy number analysis. Please check the required format at https://github.com/AlexandrovLab/SigProfilerMatrixGenerator#copy-number-matrix-generation. The accepted callers for TYPE are the following {""ASCAT"", ""ASCAT_NGS"", ""SEQUENZA"", ""ABSOLUTE"", ""BATTENBERG"", ""FACETS"", ""PURPLE"", ""TCGA""}. For example, when using segmentation file from BATTENBERG then set input_type to ""seg:BATTENBERG"".</li></ul> |\n|  | **output** | String | The name of the output folder. The output folder will be generated in the current working directory.  |\n|  | **input_data** | String | <br>Path to input folder for input_type:<ul><li>vcf</li><li>bedpe</li></ul>Path to file for input_type:<ul><li>matrix</li><li>seg:TYPE</li></ul> |\n|  | **reference_genome** | String | The name of the reference genome. The default reference genome is ""GRCh37"". This parameter is applicable only if the input_type is ""vcf"". | \n|  | **opportunity_genome** | String | The build or version of the reference genome for the reference signatures. The default opportunity genome is GRCh37. If the input_type is ""vcf"", the opportunity_genome automatically matches the input reference genome value. Only the genomes available in COSMIC are supported (GRCh37, GRCh38, mm9, mm10 and rn6). If a different opportunity genome is selected, the default genome GRCh37 will be used. | \n|  | **context_type** | String | A string of mutaion context name/names separated by comma ("",""). The items in the list defines the mutational contexts to be considered to extract the signatures. The default value is ""96,DINUC,ID"", where ""96"" is the SBS96 context, ""DINUC"" is the DINUCLEOTIDE context and ID is INDEL context. | \n|  | **exome** | Boolean | Defines if the exomes will be extracted. The default value is ""False"".  | \n| **NMF Replicates** |  |  |  | \n|  | **minimum_signatures** | Positive Integer | The minimum number of signatures to be extracted. The default value is 1. | \n|  | **maximum_signatures** | Positive Integer | The maximum number of signatures to be extracted. The default value is 25. | \n|  | **nmf_replicates** | Positive Integer | The number of iteration to be performed to extract each number signature. The default value is 100. | \n|  | **resample** | Boolean | Default is True. If True, add poisson noise to samples by resampling. | \n|  | **seeds** | String | It can be used to get reproducible resamples for the NMF replicates. A path of a tab separated .txt file containing the replicated id and preset seeds in a two columns dataframe can be passed through this parameter. The Seeds.txt file in the results folder from a previous analysis can be used for the seeds parameter in a new analysis. The Default value for this parameter is ""random"". When ""random"", the seeds for resampling will be random for different analysis. | \n| **NMF Engines** |  |  |  | \n|  | **matrix_normalization** | String | Method of normalizing the genome matrix before it is analyzed by NMF. Default is value is ""gmm"". Other options are, ""log2"", ""custom"" or ""none"". | \n|  | **nmf_init** | String | The initialization algorithm for W and H matrix of NMF. Options are 'random', 'nndsvd', 'nndsvda', 'nndsvdar' and 'nndsvd_min'. Default is 'random'. | \n|  | **precision** | String | Values should be single or double. Default is single. | \n|  | **min_nmf_iterations** | Integer | Value defines the minimum number of iterations to be completed before NMF converges. Default is 10000. | \n|  | **max_nmf_iterations** | Integer | Value defines the maximum number of iterations to be completed before NMF converges. Default is 1000000. | \n|  | **nmf_test_conv** | Integer | Value defines the number number of iterations to done between checking next convergence. Default is 10000. | \n|  | **nmf_tolerance** | Float | Value defines the tolerance to achieve to converge. Default is 1e-15. | \n| **Execution** |  |  |  | \n|  | **cpu** | Integer | The number of processors to be used to extract the signatures. The default value is -1 which will use all available processors. | \n|  | **gpu** | Boolean | Defines if the GPU resource will used if available. Default is False. If True, the GPU resources will be used in the computation. *Note: All available CPU processors are used by default, which may cause a memory error. This error can be resolved by reducing the number of CPU processes through the **cpu** parameter.*|\n|  | **batch_size** | Integer | Will be effective only if the GPU is used. Defines the number of NMF replicates to be performed by each CPU during the parallel processing. Default is 1. | \n| **Solution Estimation Thresholds** |  |  |  | \n|  | **stability** | Float | Default is 0.8. The cutoff thresh-hold of the average stability. Solutions with average stabilities below this thresh-hold will not be considered. | \n|  | **min_stability** | Float | Default is 0.2. The cutoff thresh-hold of the minimum stability. Solutions with minimum stabilities below this thresh-hold will not be considered.  | \n|  | **combined_stability** | Float | Default is 1.0. The cutoff thresh-hold of the combined stability (sum of average and minimum stability). Solutions with combined stabilities below this thresh-hold will not be considered. | \n|  | **allow_stability_drop** | Boolean | Default is False. Defines if solutions with a drop in stability with respect to the highest stable number of signatures will be considered. | \n| **Decomposition** |  |  |  | \n|  | **cosmic_version** | Float | Takes a positive float among 1, 2, 3, 3.1, 3.2, 3.3, and 3.4. Default is 3.4. Defines the version of the COSMIC reference signatures. | \n|  | **make_decomposition_plots** | Boolean | Defualt is True. If True, Denovo to Cosmic sigantures decompostion plots will be created as a part the results. | \n|  | **collapse_to_SBS96** | Boolean | Defualt is True. If True, SBS288 and SBS1536 Denovo signatures will be mapped to SBS96 reference signatures. If False, those will be mapped to reference signatures of the same context. \n| **Others** |  |  |  | \n|  | **get_all_signature_matrices** | Boolean | If True, the Ws and Hs from all the NMF iterations are generated in the output. | \n|  | **export_probabilities** | Boolean | Defualt is True. If False, then doesn't create the probability matrix. | \n    \n#### sigProfilerExtractor Example\nVCF Files as Input\n```python    \nfrom SigProfilerExtractor import sigpro as sig\ndef main_function():\n    # to get input from vcf files\n    path_to_example_folder_containing_vcf_files = sig.importdata(""vcf"")\n    # you can put the path to your folder containing the vcf samples\n    data = path_to_example_folder_containing_vcf_files\n    sig.sigProfilerExtractor(""vcf"", ""example_output"", data, minimum_signatures=1, maximum_signatures=3)\nif __name__==""__main__"":\n   main_function()\n# Wait until the excecution is finished. The process may a couple of hours based on the size of the data.\n# Check the current working directory for the ""example_output"" folder.\n```\nMatrix File as Input\n```python\nfrom SigProfilerExtractor import sigpro as sig\ndef main_function():    \n   # to get input from table format (mutation catalog matrix)\n   path_to_example_table = sig.importdata(""matrix"")\n   data = path_to_example_table # you can put the path to your tab delimited file containing the mutational catalog matrix/table\n   sig.sigProfilerExtractor(""matrix"", ""example_output"", data, opportunity_genome=""GRCh38"", minimum_signatures=1, maximum_signatures=3)\nif __name__==""__main__"":\n   main_function()\n```\n\n#### sigProfilerExtractor Output\nTo learn about the output, please visit https://osf.io/t6j7u/wiki/home/\n  \n\n### <a name=""estimate_solution""></a> Estimation of the Optimum Solution\nEstimate the optimum solution (rank) among different number of solutions (ranks). \n\n```python\nestimate_solution(base_csvfile=""All_solutions_stat.csv"", \n          All_solution=""All_Solutions"", \n          genomes=""Samples.txt"", \n          output=""results"", \n          title=""Selection_Plot"",\n          stability=0.8, \n          min_stability=0.2, \n          combined_stability=1.0,\n          allow_stability_drop=False,\n          exome=False)\n```  \n    \n| Parameter | Variable Type | Parameter Description |\n| --------------------- | -------- |-------- |\n| **base_csvfile** | String | Default is ""All_solutions_stat.csv"". Path to a  csv file that contains the statistics of all solutions. |\n| **All_solution** | String | Default is ""All_Solutions"". Path to a folder that contains the results of all solutions. |\n| **genomes** | String | Default is Samples.txt. Path to a tab delimilted file that contains the mutation counts for all genomes given to different mutation types. |\n| **output** | String | Default is ""results"". Path to the output folder. |\n| **title** | String | Default is ""Selection_Plot"". This sets the title of the selection_plot.pdf |\n| **stability** | Float | Default is 0.8. The cutoff thresh-hold of the average stability. Solutions with average stabilities below this thresh-hold will not be considered. |\n| **min_stability** | Float | Default is 0.2. The cutoff thresh-hold of the minimum stability. Solutions with minimum stabilities below this thresh-hold will not be considered. |\n| **combined_stability** | Float | Default is 1.0. The cutoff thresh-hold of the combined stability (sum of average and minimum stability). Solutions with combined stabilities below this thresh-hold will not be considered. |\n| **allow_stability_drop** | Boolean | Default is False. Defines if solutions with a drop in stability with respect to the highest stable number of signatures will be considered. | \n| **exome** | Boolean | Default is ""False"". Defines if exomes samples are used. | \n\n        \n#### Estimation of the Optimum Solution Example\n```python \nfrom SigProfilerExtractor import estimate_best_solution as ebs\nebs.estimate_solution(base_csvfile=""All_solutions_stat.csv"", \n          All_solution=""All_Solutions"", \n          genomes=""Samples.txt"", \n          output=""results"", \n          title=""Selection_Plot"",\n          stability=0.8, \n          min_stability=0.2, \n          combined_stability=1.0,\n          allow_stability_drop=False,\n          exome=False)\n```                \n\n#### Estimation of the Optimum Solution Output\nThe files below will be generated in the output folder:\n| File Name | Description |\n| ----- | ----- |\n| **All_solutions_stat.csv** | A csv file that contains the statistics of all solutions. |\n| **selection_plot.pdf** | A plot that depict the Stability and Mean Sample Cosine Distance for different solutions. |\n\n### <a name=""decompose""></a> Decompose\n\nFor decomposition of denovo signatures please use [SigProfilerAssignment](https://github.com/AlexandrovLab/SigProfilerAssignment)\n        \n### <a name=""plotActivity""></a> Activity Stacked Bar Plot\nGenerates a stacked bar plot showing activities in individuals\n\n```python \nplotActivity(activity_file, output_file = ""Activity_in_samples.pdf"", bin_size = 50, log = False)\n``` \n\n| Parameter | Variable Type | Parameter Description |\n| --------------------- | -------- |-------- |\n| **activity_file** | String | The standard output activity file showing the number of, or percentage of mutations attributed to each sample. The row names should be samples while the column names should be signatures. |\n| **output_file** | String | The path and full name of the output pdf file, including "".pdf"" |\n| **bin_size** | Integer | Number of samples plotted per page, recommended: 50 |\n        \n#### Activity Stacked Bar Plot Example\n```bash \n$ python plotActivity.py 50 sig_attribution_sample.txt test_out.pdf\n``` \n\n## <a name=""video_tutorials""></a> Video Tutorials\nTake a look at our video tutorials for step-by-step instructions on how to install and run SigProfilerExtractor on Amazon Web Services.\n\n### Tutorial #1: Installing SigProfilerExtractor on Amazon Web Services ###\n\n[![Video Tutorial #3](https://img.youtube.com/vi/30JmjvJ-DtI/0.jpg)](https://www.youtube.com/watch?v=30JmjvJ-DtI/)\n\n### Tutorial #2: Running the Quick Start Example Program ###\n\n[![Video Tutorial #3](https://img.youtube.com/vi/BiBYZz_khIY/0.jpg)](https://www.youtube.com/watch?v=BiBYZz_khIY/)\n\n### Tutorial #3: Reviewing the output from SigProfilerExtractor ###\n\n[![Video Tutorial #3](https://img.youtube.com/vi/BchtNeaQlv0/0.jpg)](https://www.youtube.com/watch?v=BchtNeaQlv0/)\n\n### GPU support\n\nIf CUDA out of memory exceptions occur, it will be necessary to reduce the number of CPU processes used (the `cpu` parameter).\n\n#### For more information, help, and examples, please visit: https://osf.io/t6j7u/wiki/home/\n\n## <a name=""citation""></a> Citation\nIslam SMA, Díaz-Gay M, Wu Y, Barnes M, Vangara R, Bergstrom EN, He Y, Vella M, Wang J, Teague JW, Clapham P, Moody S, Senkin S, Li YR, Riva L, Zhang T, Gruber AJ, Steele CD, Otlu B, Khandekar A, Abbasi A, Humphreys L, Syulyukina N, Brady SW, Alexandrov BS, Pillay N, Zhang J, Adams DJ, Martincorena I, Wedge DC, Landi MT, Brennan P, Stratton MR, Rozen SG, and Alexandrov LB (2022) Uncovering novel mutational signatures by _de novo_ extraction with SigProfilerExtractor. __Cell Genomics__. doi: [10.1016/j.xgen.2022.100179](https://doi.org/10.1016/j.xgen.2022.100179).\n\n\n## <a name=""copyright""></a> Copyright\nThis software and its documentation are copyright 2018 as a part of the sigProfiler project. The SigProfilerExtractor framework is free software and is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\n\n## <a name=""contact""></a> Contact Information\nPlease address any queries or bug reports to Mark Barnes at mdbarnes@ucsd.edu\n",148,somatic-variants,Python,1,Python,,,,,,,,,,,,,,,,,,,,,,,,,,,,50,1,48,1,3,29,0,174322,50,203,199,4,e649027bdc642d8f6264cfefa3fdb35acee3b6d8,Merge pull request #245 from AlexandrovLab/u65,2024-05-10T17:50:22Z,mdbarnesUCSD,mdbarnes@ucsd.edu,mdbarnesUCSD,v1.1.24,"### Changes Made:\r\n- Applied Black automatic formatting.\r\n- Utilized SigProfilerPlotting's `process_input` function to process input data.\r\n- Updated requirements:\r\n  - **SigProfilerMatrixGenerator:** \r\n    - Upgraded from version 1.2.17 to 1.2.25.\r\n  - **SigProfilerPlotting:** \r\n    - Upgraded from version 1.3.16 to 1.3.22.\r\n  - **SigProfilerAssignment:** \r\n    - Upgraded from version 0.1.0 to 0.1.4.\r\n\r\nThese updates aim to enhance code readability, maintainability, and ensure compatibility with the latest versions of the required dependencies.",v1.1.24,,,mdbarnesUCSD,"BSD 2-Clause ""Simplified"" License",SigProfilerExtractor,AlexandrovLab,4,bioinformatics,somatic-variants,cancer-genomics,mutational-signatures,mutation-analysis,,,,,,,,,,,,,,,,/AlexandrovLab/SigProfilerExtractor,25,24,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/AgnostiqHQ/covalent,https://github.com/AgnostiqHQ/covalent,0.5,Machine learning?,0,0,1,1,1,0,0,0,0,0,0,1,Pythonic tool for orchestrating machine-learning/high performance/quantum-computing workflows in heterogeneous compute environments.,"<div align=""center"">\n  <img src=""./doc/source/_static/banner_executor.gif"" alt=""hero"" />\n</div>\n</br>\n<div align=""center"">\n\n[![version](https://img.shields.io/github/v/tag/AgnostiqHQ/covalent?color=%235552FF&include_prereleases&label=version&sort=semver)](https://github.com/AgnostiqHQ/covalent/blob/develop/CHANGELOG.md)\n[![Static Badge](https://img.shields.io/badge/python-3.8_%7C_3.9_%7C_3.10-%235552FF)](#)\n[![Static Badge](https://img.shields.io/badge/tests-passing-%235552FF?logo=github)](https://github.com/AgnostiqHQ/covalent/actions/workflows/tests.yml)\n[![Static Badge](https://img.shields.io/badge/docs-passing-%235552FF)](https://docs.covalent.xyz/docs/)\n[![Static Badge](https://img.shields.io/badge/codecov-88%25-%235552FF?logo=codecov)](https://codecov.io/gh/AgnostiqHQ/covalent)\n[![apache](https://img.shields.io/badge/License-Apache_License_2.0-blue?color=%235552FF)](https://www.apache.org/licenses/LICENSE-2.0)</div>\n\n<div align=""center""><b>Run AI, ML, and Scientific Research Code on Any Cloud or On-Prem Cluster with a Single Line</b></div>\n</br>\n <div align=""center"">\n<a href=""https://docs.covalent.xyz/docs/get-started/quick-start""><img src=""./doc/source/_static/getting_started.svg"" width=""150"" alt=""divider""></a>\n&nbsp&nbsp\n<a href=""https://docs.covalent.xyz/docs/""><img src=""./doc/source/_static/documentation.svg"" alt=""divider"" width=""150""></a>\n&nbsp&nbsp\n<a href=""https://docs.covalent.xyz/docs/user-documentation/tutorials/""><img src=""./doc/source/_static/examples.svg"" alt=""divider"" width=""105""></a>\n&nbsp&nbsp\n<a href=""https://covalentworkflows.slack.com/join/shared_invite/zt-1ew7f2rfk-dKSXVQmRniu5mQW4Z_eQuw#/shared-invite/email""><img src=""./doc/source/_static/slack.svg"" alt=""divider"" width=""70""></a>\n</div>\n</br>\n\n\n```bash\npip install covalent --upgrade\n```\nCheck our [Quick Start Guide](https://docs.covalent.xyz/docs/get-started/quick-start/) for setup instructions or dive into your [First Experiment](https://docs.covalent.xyz/docs/get-started/first-experiment/). Learn more on the [Concepts](https://docs.covalent.xyz/docs/user-documentation/concepts/concepts-index/).\n\n### What is Covalent?\n\nCovalent is a Python library for AI/ML engineers, developers, and researchers. It provides a straightforward approach to running compute jobs, like LLMs, generative AI, and scientific research, on various cloud platforms or on-prem clusters.\n\n\n<details>\n<summary><i><b>Run Code Anywhere:</b> Execute Python functions in any cloud or on-prem cluster by changing just a single line of code.</i></summary>\n<br>\n\nIt is as simple as swapping the decorator with our executor plugins. Choose from [existing plugins](https://docs.covalent.xyz/docs/plugin) or [create custom ones](https://github.com/AgnostiqHQ/covalent-executor-template) for tailored interactions with any infrastructure.\n<div align=""center""><img src=""./doc/source/_static/executors_ship.png""  width=""700""></div>\n</details>\n\n<details>\n<summary><i><b>Abstraction of Infrastructure Management:</b> Abstract the complexities of cloud consoles, terraform, or IaC in the background.</i> </summary>\n<br>\n<div align=""center""><img src=""./doc/source/_static/abstract_infra.png""  width=""700""></div>\n</details>\n\n<details>\n<summary><i><b>Serverless Infrastructure:</b> Automatically converts any infrastructure, including on-prem SLURM clusters or cloud compute, into a serverless setup.</i></summary>\n<br>\n<div align=""center""><img src=""./doc/source/_static/serverless-illustration.png""  width=""700""></div>\n</details>\n\n\nIf you find Covalent useful or interesting, feel free to give us a ⭐ on GitHub! Your support helps us to continue developing and improving this framework.\n\n\n</br>\n<table border=""0"">\n <tr>\n    <td><b style=""font-size:18px; padding-right: 20px;"">For AI/ML Practitioners and Developers</b></td>\n    <td><b style=""font-size:18px; padding-left: 20px;"">For Researchers</b></td>\n </tr>\n <tr>\n    <td valign=""top"">\n        <ul style=""font-size:16px; list-style-type: circle; padding-right: 20px;"">\n            <li><b>Robust Compute Backend:</b> Ideal as a backend compute framework for AI/ML applications, Large Language Models (LLMs), Generative AI, and more.</li>\n            <li><b>Cloud-Agnostic Execution:</b> Execute high-compute tasks seamlessly across different cloud environments.</li>\n            <li><b>Infrastructure Abstraction:</b> Directly use computing resources while keeping your business code independent from the infrastructure/resource definitions.</li>\n        </ul>\n    </td>\n    <td valign=""top"">\n        <ul style=""font-size:16px; list-style-type: circle; padding-left: 20px;"">\n            <li><b>Local-Like Access:</b> Effortlessly connect to compute resources from your laptop, eliminating the need for SSH or complex scripts.</li>\n            <li><b>Unified Interface Across Environments:</b> Consistent experience with on-prem HPC clusters and cloud platforms like SLURM, PBS, LSF, AWS, GCP, Azure.</li>\n            <li><b>Real-Time Monitoring Monitoring:</b> <a href=""http://demo.covalent.xyz/""> User-friendly UI </a> for real-time monitoring, enabling cost-effective and iterative R&D.</li>\n        </ul>\n    </td>\n </tr>\n</table>\n\n### Out-of-box observability - [Try out the demo](http://demo.covalent.xyz/)\n\nIf you find Covalent useful or interesting, feel free to give us a ⭐ on GitHub! Your support helps us to continue developing and improving this framework.\n\n<!-- https://github.com/AgnostiqHQ/covalent/assets/116076079/87268cc8-4d53-4053-b739-1d03f2eafa7c -->\n<div align=""center"">\n  <img src=""./doc/source/_static/readmeVid-gif.gif"" alt=""video"" />\n</div>\n\n\n\n### Explore Covalent Through Examples\n\nJump right into practical examples to see Covalent in action. These tutorials cover a range of applications, giving you a hands-on experience:\n\n<div align=""center"">\n  <a href=""https://docs.covalent.xyz/docs/user-documentation/tutorials/generativeai/"">\n    <img src=""./doc/source/_static/ai_tutorial.svg"" alt=""AI Tutorial"">\n  </a>\n  <a href=""https://docs.covalent.xyz/docs/user-documentation/tutorials/mnist/"">\n    <img src=""./doc/source/_static/mnist_tutorial.svg"" alt=""MNIST Tutorial"">\n  </a>\n  <a href=""https://docs.covalent.xyz/docs/user-documentation/tutorials/quantumchemistry/"">\n    <img src=""./doc/source/_static/quantum_tutorial.svg"" alt=""Quantum Tutorial"">\n  </a>\n</div>\n\n### Explore Our Extensive Plugin Ecosystem\n\nCovalent integrates seamlessly with a variety of platforms. Discover our range of plugins to enhance your Covalent experience:\n\n</br>\n  <div align=""center"">\n<a href=""https://docs.covalent.xyz/docs/user-documentation/api-reference/executors/aws-plugins/""><img src=""./doc/source/_static/aws.svg"" alt=""divider""></a>\n<a href=""https://docs.covalent.xyz/docs/user-documentation/api-reference/executors/azurebatch/""><img src=""./doc/source/_static/azure.svg"" alt=""divider""></a>\n<a href=""https://docs.covalent.xyz/docs/user-documentation/api-reference/executors/gcp/""><img src=""./doc/source/_static/google.svg"" alt=""divider""></a>\n<a href=""https://docs.covalent.xyz/docs""><img src=""./doc/source/_static/kubernetes.svg"" alt=""divider""></a>\n</div>\n<div align=""center""><a href=""https://docs.covalent.xyz/docs/user-documentation/api-reference/executors/slurm/""><img src=""./doc/source/_static/slurm.svg"" alt=""divider""></a>\n<a href=""https://docs.covalent.xyz/docs/user-documentation/api-reference/executors/dask/""><img src=""./doc/source/_static/dask.svg"" alt=""divider""></a>\n<a href=""https://docs.covalent.xyz/docs/user-documentation/api-reference/executors/ibmq/""><img src=""./doc/source/_static/ibmq.svg"" alt=""divider""></a>\n<a href=""https://docs.covalent.xyz/docs/plugin""><img src=""./doc/source/_static/many_more.svg"" alt=""divider""></a></div>\n\n### Key Features at a Glance\n\nGet a quick overview of what Covalent offers. Our infographic summarizes the main features, providing you with a snapshot of our capabilities:\n\n</br>\n  <div align=""center"">\n  <a href=""https://docs.covalent.xyz/docs/""><img src=""./doc/source/_static/development.svg""  alt=""development""></img></a>\n </div>\n\n</br>\n\n---\n\n### Know More About Covalent\n\nFor a more in-depth description of Covalent's features and how they work, see the [Concepts](https://docs.covalent.xyz/docs/user-documentation/concepts/concepts-index/) page in the documentation.\n\n</br>\n\n<div align=""center"">\n<a href=""https://www.covalent.xyz/what-is-covalent/""><img src=""./doc/source/_static/what_is_covalent.svg"" alt=""divider""></a>\n<a href=""https://www.covalent.xyz/navigating-the-modern-hpc-landscape/""><img src=""./doc/source/_static/cloud_hpc.svg"" alt=""divider""></a>\n<a href=""https://docs.covalent.xyz/docs/user-documentation/concepts/covalent-basics/""><img src=""./doc/source/_static/concepts_of_covalent.svg"" alt=""divider""></a>\n<a href=""https://docs.covalent.xyz/docs/user-documentation/concepts/covalent-arch/covalent-architecture""><img src=""./doc/source/_static/covalent_work.svg"" alt=""divider""></a>\n</div>\n\n<div >\n\n### Installation\n\nCovalent is developed using Python on Linux and macOS. The easiest way to install Covalent is by using the PyPI package manager.\n\n```\npip install covalent --upgrade\n```\n\nFor other methods of installation, please [check the docs.](https://docs.covalent.xyz/docs/get-started/install/)\n\n**Deployments**\n\n<div>\nCovalent offers flexible deployment options, from Docker image/AMIs for self-hosting to pip package for local installations, accommodating various use cases\n</div>\n\n</br>\n\n<div align=""center"">\n\n<a href=""https://docs.covalent.xyz/docs/user-documentation/server-deployment""><img src=""./doc/source/_static/local-laptop.svg"" alt=""divider""></a>\n<a href=""https://docs.covalent.xyz/docs/user-documentation/sd-docker""><img src=""./doc/source/_static/onprem_hosted.svg"" alt=""divider""></a>\n<a href=""https://docs.covalent.xyz/docs/user-documentation/sd-systemd""><img src=""./doc/source/_static/cloud_hosted.svg"" alt=""divider""></a>\n\n</div>\n\n</br>\n</div>\n\n### Contributing\n\n <!-- <div><img src=""./contributing_heading.svg"" alt=""divider""></div> -->\n\nTo contribute to Covalent, refer to the [Contribution Guidelines](https://github.com/AgnostiqHQ/covalent/blob/master/CONTRIBUTING.md). We use GitHub's [issue tracking](https://github.com/AgnostiqHQ/covalent/issues) to manage known issues, bugs, and pull requests. Get started by forking the `develop` branch and submitting a pull request with your contributions. Improvements to the documentation, including tutorials and how-to guides, are also welcome from the community. For more information on adding tutorials, check the [Tutorial Guidelines](https://github.com/AgnostiqHQ/covalent/blob/master/doc/TUTORIAL_GUIDELINES.md). Participation in the Covalent community is governed by the [Code of Conduct](https://github.com/AgnostiqHQ/covalent/blob/master/CODE_OF_CONDUCT.md).\n\n### Citation\n\nPlease use the following citation in any publications.\n\n[https://doi.org/10.5281/zenodo.5903364](https://zenodo.org/records/8369670)\n\n### License\n\nCovalent is licensed under the Apache 2.0 License. See the [LICENSE](https://github.com/AgnostiqHQ/covalent/blob/master/LICENSE) file or contact the [support team](mailto:support@aqnostic.ai) for more details.\n\nFor a detailed history of changes and new features, see the [Changelog](https://github.com/AgnostiqHQ/covalent/blob/master/CHANGELOG.md).\n",732,hpc-applications,Python,8,Python,Dockerfile,HTML,CSS,JavaScript,Jupyter Notebook,Mako,Shell,,,,,,,,,,,,,,,,,,,,,1006,152,842,12,211,63,0,389130,88,813,747,66,215d8d35243c9276624b56d8c0520a8fa02fd6b2,[pre-commit.ci] pre-commit autoupdate (#1807),2024-06-10T19:45:44Z,pre-commit-ci[bot],66853113+pre-commit-ci[bot]@users.noreply.github.com,pre-commit-ci[bot],v0.232.0.post1,## [0.232.0.post1] - 2024-01-23\n\n### Authors\n\n### Added\n\n### Changed\n\n### Removed\n\n### Fixed\n\n### Tests\n\n### Docs\n\n### Operations,v0.232.0.post1,Will Cunningham,,wjcunningham7,Apache License 2.0,covalent,AgnostiqHQ,224,hpc,workflow,workflow-automation,workflow-management,quantum-computing,hpc-applications,python,machine-learning,pipelines,covalent,orchestration,parallelization,machinelearning,machinelearning-python,quantum,quantum-machine-learning,data-science,data-pipeline,deep-learning,hacktoberfest,/AgnostiqHQ/covalent,296,24,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/aehrc/VariantSpark,https://github.com/aehrc/VariantSpark,0,ML,,0,1,1,1,0,0,0,0,0,0,1,machine learning for genomic variants,"# Variant Spark\n\n[![Build](https://github.com/aehrc/VariantSpark/workflows/Java%20and%20Python%20CI%20with%20Maven/badge.svg)](https://github.com/aehrc/VariantSpark/actions?query=workflow%3CI)\n[![Documentation Status](https://readthedocs.org/projects/variantspark/badge/?version=latest)](http://variantspark.readthedocs.io/en/latest/?badge=latest)\n\n_variant-spark_ is a scalable toolkit for genome-wide association studies optimized for GWAS-like datasets.\n\nMachine learning methods and, in particular, random forests (RFs) are promising alternatives to standard single SNP analyses in genome-wide association studies (GWAS). RFs provide variable importance measures to rank SNPs according to their predictive power.\nAlthough there are several existing random forest implementations available, some even parallel or distributed such as Random Jungle, ranger, or SparkML, most of them are not optimized to deal with GWAS datasets, which usually come with thousands of samples and millions of variables.\n\n_variant-spark_ currently provides the basic functionality of building a random forest model and estimating variable importance with the mean decrease gini method. The tool can operate on VCF and CSV files. Future extensions will include support for other importance measures, variable selection methods, and data formats.\n\n_variant-spark_ utilizes a novel approach of building random forests from data in transposed representation, which allows it to efficiently deal with even extremely wide GWAS datasets. Moreover, since the most common genomics variant calls file format, i.e. VCF, which uses the transposed representation, variant-spark can work directly with the VCF data, without the costly pre-processing required by other tools.\n\n_variant-spark_ is built on top of Apache Spark – a modern distributed framework for big data processing, which gives variant-spark the ability to scale horizontally on both bespoke cluster and public clouds.\n\nThe potential users include:\n\n- Medical researchers seeking to perform GWAS-like analysis on large cohort data of genome-wide sequencing data or imputed SNP array data.\n- Medical researchers or clinicians seeking to perform clustering on genomic profiles to stratify large-cohort genomic data.\n- General researchers with classification or clustering needs of datasets with millions of features.\n\n### Community\n\nPlease feel free to add issues and/or upvote issues you care about. Also, join the [Gitter chat](https://gitter.im/VariantSpark/Lobby).\nWe also started [ReadTheDocs](https://variantspark.readthedocs.io/en/latest/) and there is always this repo's issues page for you to add requests. Thanks for your support.\n\n### Learn More\n\nTo learn more watch this video from HUGO Conference 2020.\n\n[![variant-spark YOW! Brisbane 2017](/images/YOW__Conference_2017_Lynn_Langit___Denis_Bauer_-_Cloud_Data_Pipelines_-_YouTube.png?raw=true)](https://www.youtube.com/watch?v=7bVoPmPVzKQ)\n\n### Building\n\n_variant-spark_ requires java jdk 1.8+ and maven 3+\n\nIn order to build the binaries use:\n\n    mvn clean install\n\nFor Python _variant-spark_ requires Python 3.6+ with pip.\nThe other packages required for development are listed in `dev/dev-requirements.txt` and can be installed with:\n\n    pip install -r dev/dev-requirements.txt\n\nor with:\n\n./dev/py-setup.sh\n\nThe complete build including all checks can be run with:\n\n    ./dev/build.sh\n\n### Running\n\nvariant-spark requires an existing spark 3.1+ installation (either a local one or a cluster one).\n\nTo run variant-spark use:\n\n    ./variant-spark [(--spark|--local) <spark-options>* --] [<command>] <command-options>*\n\nTo obtain the list of the available commands use:\n\n    ./variant-spark -h\n\nTo obtain help for a specific command (for example `importance`) use:\n\n    ./variant-spark importance -h\n\nYou can use `--spark` marker before the command to pass `spark-submit` options to variant-spark. The list of spark options needs to be terminated with `--`, e.g:\n\n    ./variant-spark --spark --master yarn-client --num-executors 32 -- importance ....\n\nPlease, note that `--spark` needs to be the first argument of `variant-spark`\n\nYou can also run variant-spark in the `--local` mode. In this mode, variant-spark will ignore any Hadoop or Spark configuration files and run in the local mode for both Hadoop and Spark. In particular, in this mode, all file paths are interpreted as local file system paths. Also, any parameters passed after `--local` and before `--` are ignored. For example:\n\n    ./bin/variant-spark --local -- importance  -if data/chr22_1000.vcf -ff data/chr22-labels.csv -fc 22_16051249 -v -rn 500 -rbs 20 -ro\n\nNote:\n\nThe difference between running in `--local` mode and in `--spark` with `local` master is that in the latter case, Spark uses the Hadoop filesystem configuration and the input files need to be copied to this filesystem (e.g. HDFS)\nAlso, the output will be written to the location determined by the Hadoop filesystem settings. In particular paths without schema e.g. 'output.csv' will be resolved with the Hadoop default filesystem (usually HDFS)\nTo change this behavior you can set the default filesystem in the command line using `spark.hadoop.fs.default.name` option. For example to use local filesystem as the default use:\n\n    ./bin/variant-spark --spark ... --conf ""spark.hadoop.fs.default.name=file:///"" ... -- importance  ... -of output.csv\n\nYou can also use the full URI with the schema to address any filesystem for both input and output files e.g.:\n\n    ./bin/variant-spark --spark ... --conf ""spark.hadoop.fs.default.name=file:///"" ... -- importance  -if hdfs:///user/data/input.csv ... -of output.csv\n\n### Running examples\n\nThere are multiple methods for running variant-spark examples\n\n#### Manual Examples\n\nvariant-spark comes with a few example scripts in the `scripts` directory that demonstrate how to run its commands on sample data.\n\nThere are a few small data sets in the `data` directory suitable for running on a single machine. For example:\n\n    ./examples/command-line/local_run-importance-ch22.sh\n\nruns variable importance command on a small sample of the chromosome 22 VCF file (from 1000 Genomes Project)\n\nThe full-size examples require a cluster environment (the scripts are configured to work with Spark on YARN).\n\nThe data required for the examples can be obtained from the data folder [https://github.com/aehrc/VariantSpark/tree/master/data](https://github.com/aehrc/VariantSpark/tree/master/data)\n\nThis repository uses the git Large File Support extension, which needs to be installed first (see: [https://git-lfs.github.com/](https://git-lfs.github.com/))\n\nClone the `variant-spark-data` repository and then install the test data into your Hadoop filesystem using:\n\n    ./install-data\n\nBy default, the sample data will installed into the `variant-spark-data\input` sub-directory of your HDFS home directory.\n\nYou can choose a different location by setting the `VS_DATA_DIR` environment variable.\n\nAfter the test data has been successfully copied to HDFS you can run examples scripts, e.g.:\n\n    ./examples/command-line/yarn_run-importance-ch22.sh\n\nNote: if you installed the data to a non-default location the `VS_DATA_DIR` needs to be set accordingly when running the examples\n\n### VariantSpark on the cloud\n\nVariantSpark can easily be used in AWS and Azure. For more examples and information, check the [cloud folder](https://github.com/aehrc/VariantSpark/tree/master/cloud). For a quick start, check the few pointers below.\n\n#### AWS Marketplace\n\nVariantSpark is now available on [AWS Marketplace](https://aws.amazon.com/marketplace/pp/AEHRC-VariantSpark-Notebook/B07YVND4TD). Please read the [Guidlines](contributions/AwsMarketplace/README.md) for specifications and step-by-step instructions.\n\n#### Azure Databricks\n\nVariantSpark can be easily deployed in Azure Databricks through the button below. Please read the [VariantSpark Azure manual](https://github.com/aehrc/VariantSpark-Azure-deployment) for specifications and step-by-step instructions.\n\n[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Faehrc%2FVariantSpark-Azure-deployment%2Fmaster%2Fazuredeploy.json)\n\n### Contributions\n\n#### JsonRfAnalyser\n\n[JsonRfAnalyser](contributions/JsonRfAnalyser) is a Python program that looks into the JSON RandomForest model and lists variables on each tree and branch. Please read [README](contributions/JsonRfAnalyser/README.md) to see the complete list of functionalities.\n\n#### WebVisualiser\n\n[rfview.html](contributions/WebVisualiser/rfview.html) is a web program (run locally on your machine) where you can upload the JSON model produced by variant-spark and it visualizes trees in the model. You can identify which tree to be visualized. Node color and node labels could be set to different parameters such as the number of samples in the node or the node impurity. It uses [vis.js](https://visjs.org/) for tree Visualisation.\n",138,bioinformatics,JavaScript,10,Shell,Java,Scala,R,Python,Makefile,Jupyter Notebook,CSS,HTML,JavaScript,,,,,,,,,,,,,,,,,,,114,11,94,9,39,17,83,78167,45,124,102,22,2fb26428d0d37d5f7d012dfa588b3608f6407067,Merge pull request #238 from aehrc/load_vcf_bgz,2024-04-15T04:17:41Z,bhosking,brendan.hosking@gmail.com,bhosking,v0.5.2,Release v0.5.2 includes:\r\n\r\n- Hail init enhancement,v0.5.2,,,github-actions[bot],Other,VariantSpark,aehrc,8,variant-spark,gwas,random-forest,genome,association-studies,vcf,variantspark,notebook,databricks,bioinformatics,aws,emr,,,,,,,,,/aehrc/VariantSpark,14,18,TRUE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
/ablab/IsoQuant,https://github.com/ablab/IsoQuant,1,,,1,1,1,1,0,0,0,0,0,0,1,Transcript discovery and quantification with long RNA reads (Nanopores and PacBio),"[![BioConda Install](https://img.shields.io/conda/dn/bioconda/isoquant.svg?style=flag&label=BioConda%20install)](https://anaconda.org/bioconda/isoquant)\n[![Python version](https://img.shields.io/badge/python-3.8-blue)](https://www.python.org/downloads/)\n[![License](https://img.shields.io/badge/licence-GPLv2-blue)](https://www.gnu.org/licenses/old-licenses/gpl-2.0)\n[![GitHub release (latest by date)](https://img.shields.io/github/v/release/ablab/IsoQuant)](https://github.com/ablab/IsoQuant/releases/)\n[![GitHub Downloads](https://img.shields.io/github/downloads/ablab/IsoQuant/total.svg?style=social&logo=github&label=Download)](https://github.com/ablab/IsoQuant/releases)\n[![UnitTests](https://github.com/ablab/IsoQuant/actions/workflows/Unit_tests.yml/badge.svg)](https://github.com/ablab/IsoQuant/actions/workflows/Unit_tests.yml)\n\n\n\n# IsoQuant 3.4 manual\n\n1. [About IsoQuant](#sec1) </br>\n1.1. [Supported data types](#sec1.1)</br>\n1.2. [Supported reference data](#sec1.2)</br>\n2. [Installation](#sec2)</br>\n2.1. [Installing from conda](#sec2.1)</br>\n2.2. [Manual installation and requirements](#sec2.2)</br>\n2.3. [Verifying your installation](#sec2.3)</br>\n3. [Running IsoQuant](#sec3)</br>\n3.1. [IsoQuant input](#sec3.1)</br>\n3.2. [Command line options](#sec3.2)</br>\n3.3. [IsoQuant output](#sec3.3)</br>\n4. [Citation](#sec4)</br>\n5. [Feedback and bug reports](#sec5)</br>\n\n**Quick start:**  \n\n*   IsoQuant can be downloaded from [https://github.com/ablab/IsoQuant](https://github.com/ablab/IsoQuant) or installed via conda:\n\n        conda create -c conda-forge -c bioconda -n isoquant python=3.8 isoquant\n\n*   If installing manually, you will need Python3 (3.8 or higher), [gffutils](https://pythonhosted.org/gffutils/installation.html), [pysam](https://pysam.readthedocs.io/en/latest/index.html), [pybedtools](https://daler.github.io/pybedtools/), [biopython](https://biopython.org/) and some other common Python libraries to be installed. See `requirements.txt` for details. You will also need to have [minimap2](https://github.com/lh3/minimap2) and [samtools](http://www.htslib.org/download/) to be in your `$PATH` variable.\n\n*   Verify your installation by running:\n\n        isoquant.py --test\n\n*   To run IsoQuant on raw FASTQ/FASTA files use the following command\n\n        isoquant.py --reference /PATH/TO/reference_genome.fasta \\n        --genedb /PATH/TO/gene_annotation.gtf \\n        --fastq /PATH/TO/sample1.fastq.gz /PATH/TO/sample2.fastq.gz \\n        --data_type (assembly|pacbio_ccs|nanopore) -o OUTPUT_FOLDER\n\n    For example, using the toy data provided within this repository,\n\n        ./isoquant.py --reference tests/toy_data/MAPT.Mouse.reference.fasta \\n        --genedb tests/toy_data/MAPT.Mouse.genedb.gtf \\n        --fastq tests/toy_data/MAPT.Mouse.ONT.simulated.fastq \\n        --data_type nanopore -o toy_data_out\n\n\n* To run IsoQuant on aligned reads (make sure your BAM is sorted and indexed) use the following command:\n\n        isoquant.py --reference /PATH/TO/reference_genome.fasta \\n        --genedb /PATH/TO/gene_annotation.gtf \\n        --bam /PATH/TO/sample1.sorted.bam /PATH/TO/sample2.sorted.bam \\n        --data_type (assembly|pacbio_ccs|nanopore) -o OUTPUT_FOLDER\n\n    For example, using the toy data provided within this repository,\n\n        ./isoquant.py --reference tests/toy_data/MAPT.Mouse.reference.fasta \\n        --genedb tests/toy_data/MAPT.Mouse.genedb.gtf \\n        --fastq tests/toy_data/MAPT.Mouse.ONT.simulated.fastq \\n        --data_type nanopore -o toy_data_out\n\n* If using official annotations containing `gene` and `transcript` features use `--complete_genedb` to save time.\n\n* Using reference annotation is optional since version 3.0, you may preform de novo transcript discovery without providing `--genedb` option':\n\n        isoquant.py --reference /PATH/TO/reference_genome.fasta \\n        --fastq /PATH/TO/sample1.fastq.gz /PATH/TO/sample2.fastq.gz \\n        --data_type (assembly|pacbio|nanopore) -o OUTPUT_FOLDER\n\n* If multiple files are provided, IsoQuant will create a single output annotation and a single set of gene/transcript expression tables.\n\n<a name=""sec1""></a>\n# About IsoQuant\n\nIsoQuant is a tool for the genome-based analysis of long RNA reads, such as PacBio or\nOxford Nanopores. IsoQuant allows to reconstruct and quantify transcript models with\nhigh precision and decent recall. If the reference annotation is given, IsoQuant also\nassigns reads to the annotated isoforms based on their intron and exon structure.\nIsoQuant further performs annotated gene, isoform, exon and intron quantification.\nIf reads are grouped (e.g. according to cell type), counts are reported according to the provided grouping.\n\nIsoQuant consists of two stages, which generate its own output:\n1. Reference-based analysis. Runs only if reference annotation is provided. Performs read-to-isoform assignment,\nsplice site correction and abundance quantification for reference genes/transcripts.\n2. Transcript discovery. Reconstructs transcript models and performs abundance quantification for discovered isoforms.\n\nLatest IsoQuant version can be downloaded from [https://github.com/ablab/IsoQuant/releases/latest](https://github.com/ablab/IsoQuant/releases/latest).\n\n#### IsoQuant pipeline\n![Pipeline](figs/isoquant_pipeline.png)\n\n<a name=""sec1.1""></a>\n## Supported data types\n\nIsoQuant support all kinds of long RNA data:\n* PacBio CCS\n* ONT dRNA / ONT cDNA\n* Assembled / corrected transcript sequences\n\nReads must be provided in FASTQ or FASTA format (can be gzipped). If you have already aligned your reads to the reference genome, simply provide sorted and indexed BAM files.\n\nIsoQuant expect reads to contain polyA tails. For more reliable transcript model construction do not trim polyA tails.\n\nIsoQuant can also take aligned Illumina reads to correct long-read spliced alignments. However, short reads are _not_\nused to discover transcript models or compute abundances.\n\n<a name=""sec1.2""></a>\n## Supported reference data\n\nReference genome should be provided in multi-FASTA format (can be gzipped).\nReference genome is mandatory even when BAM files are provided.\n\nReference gene annotation is not mandatory, but is likely to increase precision and recall.\nIt can be provided in GFF/GTF format (can be gzipped).\nIn this case it will be converted to [gffutils](https://pythonhosted.org/gffutils/installation.html) database. Information on converted databases will be stored in your `~/.config/IsoQuant/db_config.json` to increase speed of future runs. You can also provide gffutils database manually. Make sure that chromosome/scaffold names are identical in FASTA file and gene annotation.\nNote, that gffutils databases may not work correctly on NFS shares. It is possible to set a designated folder for \nthe database with `--genedb_output` (different from the output directory).\n\nPre-constructed aligner index can also be provided to increase mapping time.\n\n<a name=""sec2""></a>\n# Installation\nIsoQuant requires a 64-bit Linux system or Mac OS and Python (3.8 and higher) to be pre-installed on it.\nYou will also need\n* [gffutils](https://pythonhosted.org/gffutils/installation.html)\n* [pysam](https://pysam.readthedocs.io/en/latest/index.html)\n* [biopython](https://biopython.org/)\n* [pybedtools](https://daler.github.io/pybedtools/)\n* [pyfaidx](https://pypi.org/project/pyfaidx/)\n* [pandas](https://pandas.pydata.org/)\n* [pyyaml](https://pypi.org/project/PyYAML/)\n* [minimap2](https://github.com/lh3/minimap2)\n* [samtools](http://www.htslib.org/download/)\n* [STAR](https://github.com/alexdobin/STAR) (optional)\n\n<a name=""sec2.1""></a>\n## Installing from conda\nIsoQuant can be installed with conda:\n```bash\nconda install -c bioconda isoquant\n```\n\n<a name=""sec2.2""></a>\n## Manual installation and requirements\nTo obtain IsoQuant you can download repository and install requirements.  \nClone IsoQuant repository and switch to the latest release:\n```bash\ngit clone https://github.com/ablab/IsoQuant.git\ncd IsoQuant\ngit checkout latest\n```\nInstall requirements:\n```bash\npip install -r requirements.txt\n```\n\nYou also need [samtools](http://www.htslib.org/download/) and [minimap2](https://github.com/lh3/minimap2) to be in the `$PATH` variable.\n\n<a name=""sec2.3""></a>\n## Verifying your installation\nTo verify IsoQuant installation type\n```bash\nisoquant.py --test\n```\nto run on toy dataset.  \nIf the installation is successful, you will find the following information at the end of the log:\n```bash\n=== IsoQuant pipeline finished ===\n=== TEST PASSED CORRECTLY ===\n```\n\n<a name=""sec3""></a>\n# Running IsoQuant\n<a name=""sec3.1""></a>\n## IsoQuant input\nTo run IsoQuant, you should provide:\n* Long RNA reads (PacBio or Oxford Nanopore) in one of the following formats:\n  * FASTA/FASTQ (can be gzipped);\n  * Sorted and indexed BAM;\n* Reference sequence in FASTA format (can be gzipped);\n* _Optionally_, you may provide a reference gene annotation in gffutils database or GTF/GFF format (can be gzipped).\n\nIsoQuant is also capable of using short Illumina reads to correct long-read alignments.\n\nIsoQuant can handle data from multiple _experiments_ simultaneously. Each experiment may contain multiple _samples_ (or _replicas_).\nEach experiment is processed individually. Running IsoQuant on several experiments simultaneously\nis equivalent to several separate IsoQuant runs.\n\nThe output files for each experiment will be placed into a separate folder.\nFiles from the same _experiment_ are used to construct a single GTF and aggregated abundance tables.\nIf a single experiment contains multiple samples/replicas, per sample abundance tables are also generated.\n\nThe ways of providing input files are described below.\n\n\n### Specifying input data via command line\n\nTwo main options are `--fastq` and `--bam` (see description below). Both options accept one or multiple files separated by space.\nAll provided files are treated as a single experiment, which means a single combined GTF will\nbe generated. If multiple files are provided, IsoQuant will compute tables with each column\ncorresponding to an individual file (per-sample counts).\nTo set a specific label for each sample use the `--label` option. Number of labels must be equal to the number of files.\nTo a set a prefix for the output files use the `--prefix` option.\n\nThis pipeline is typical for the cases when a user is\ninterested in comparing expression between different replicas/conditions within the same experiment.\n\n#### Short reads for alignment correction\n\nA BAM file with Illumina reads can be provided via `--illumina_bam`. It cannot be the only input, but may only be used with either `--bam` or `--fastq`.\nThe option accepts one or multiple bam files separated by space. All files will be combined and used to correct offsets between introns in long and short reads as well as skipped exons.\n\n\n### Specifying input data via yaml file\n\nTo provide all input files in a single description file, you can use a [YAML](https://www.redhat.com/en/topics/automation/what-is-yaml) file via `--yaml` (see description below).\nYou can provide multiple experiments in a single YAML file with each experiment containing an arbitrary number of smaples/replicas.\nA distinct output folder with individual GTFs and abundance tables will be generated for each experiment.\nIn this option, BAM files with short reads for correction can be provided for each experiment.\n\nThe YAML file contains a list of experiments (e.g. in square brackets).\nThe first entry in the list should be the type of files the experiments contain, written as `data format: `\nfollowed by the type in quotation marks. The type can be either `fastq` or `bam`.\n\nEach experiment is represented as set of parameters (e.g. in curly brackets).\nEach experiment must have a name and a list of long-read files in the specified format.\nAdditionally, it may contain one or multiple BAM files with short reads.\nThe name is provided as `name: ` followed by the experiment name in quotation marks.\nBoth short and long read files are provided as a list of file paths in quotation marks,\nfollowing `long read files: ` and `illumina bam: ` respectively.\nLabels for the files can also be set with `labels: `.\nThe number of labels needs to be the same as the number of files with long reads.\nAll paths should be either absolute or relative to the YAML file.\n\nFor example:\n\n```\n[\n  data format: ""fastq"",\n  {\n    name: ""Experiment1"",\n    long read files: [\n      ""/PATH/TO/FILE1.fastq"",\n      ""/PATH/TO/FILE2.fastq""\n    ],\n    labels: [\n      ""Sample1"",\n      ""Sample2""\n    ],\n    illumina bam: [""PATH/TO/ILLUMINA1.bam""]\n  },\n  {\n    name: ""Experiment2"",\n    long read files: [\n      ""/PATH/TO/FILE3.fastq""\n    ],\n    illumina bam: [""PATH/TO/ILLUMINA2.bam""]\n  }\n]\n\n```\n\n\nOutput sub-folders will be named `Experiment1` and `Experiment2`.\nBoth sub-folders will contain predicted transcript models and abundance tables.\nAbundance table for `Experiment2` with have columns ""Sample1"" and ""Sample2"".\n\nNote, that  `--bam`, `--fastq` and `--label` options are not compatible with `--yaml`.\nSee more in [examples](#examples).\n\n\n### Specifying input data via dataset description file (deprecated)\n\nThis option is deprecated since version 3.4 and will be removed later. To process multiple experiments, please use `--yaml` instead.\n\nA dataset description file can be provided via `--fastq_list` or `--bam_list` (see description below).\nA distinct output folder with individual GTFs and abundance tables will be generated for each experiment.\n\nInput files should be provided one per line. Experiments should be separated by blank lines or experiment names\nstarting with #. You can also set a specific label for each listed file using colon. For example:\n\n```\n#EXPERIMENT1\n/PATH/TO/FILE1A.fastq:SAMPLE_A\n/PATH/TO/FILE2A.fastq:SAMPLE_A\n/PATH/TO/FILE1B.fastq:SAMPLE_B\n/PATH/TO/FILE2B.fastq:SAMPLE_B\n#EXPERIMENT2\n/PATH/TO/FILE3.fastq:SAMPLE_C1\n/PATH/TO/FILE4.fastq:SAMPLE_C2\n```\n\nOutput sub-folders will be named `EXPERIMENT1` and `EXPERIMENT2`.\nAbundance tables will have specified labels as column names.\nIf you want to group multiple files as a single sample within the experiment, use identical labels.\n\n\nNote, that  `--label` option has no effect in this case.\nSee more in [examples](#examples).\n\n\n<a name=""sec3.2""></a>\n## IsoQuant command line options\n\n\n### Basic options\n`--output` (or `-o`)\n    Output folder, will be created automatically.\n\nNote: if your output folder is located on a shared disk, use `--genedb_output` for storing\nreference annotation database.\n\n`--help` (or `-h`)\n    Prints help message.\n\n`--full_help`\n    Prints all available options (including hidden ones).\n\n`--test`\n    Runs IsoQuant on the toy data set.   \n\n\n### Input options\n`--data_type` or `-d`\n    Type of data to process, supported values are:  `pacbio_ccs` (same as `pacbio`), `nanopore` (same as `ont`)\nand  `assembly` (same as `transcripts`). This option affects the algorithm parameters.\n\nNote, that for novel mono-exonic transcripts are not reported for ONT data by default, use `--report_novel_unspliced true`.\n\n`--reference` or `-r`\n    Reference genome in FASTA format (can be gzipped), required even when BAM files are provided.\n\n`--index`\n    Reference genome index for the specified aligner (`minimap2` by default),\ncan be provided only when raw reads are used as an input (constructed automatically if not set).\n\n`--genedb` or `-g`\n    Gene database in gffutils database format or GTF/GFF format (can be gzipped).\nIf you use official gene annotations we recommend to set `--complete_genedb` option.\n\n`--complete_genedb`\n    Set this flag if gene annotation contains transcript and gene meta-features.\nUse this flag when providing official annotations, e.g. GENCODE.\nThis option will set `disable_infer_transcripts` and `disable_infer_genes` gffutils options,\nwhich dramatically speeds up gene database conversion (see more [here](https://daler.github.io/gffutils/autodocs/gffutils.create.create_db.html)).\n\n#### Providing input reads via command line option:\n\n`--fastq`\n    Input FASTQ/FASTA file(s), can be gzipped;  a single GTF will be generated for all files. If multiple files are provided,\nexpression tables with ""per-file"" columns will be computed. See more about [input data](#sec3.1).\n\n\n`--bam`\n    Sorted and indexed BAM file(s); a single GTF will be generated for all files. If multiple files are provided,\nexpression tables with ""per-file"" columns will be computed. See more about [input data](#sec3.1).\n\n\n#### Providing input reads via YAML configuration file:\n\n`--yaml`\n    Path to dataset description file in [YAML](https://www.redhat.com/en/topics/automation/what-is-yaml) format. The file should contain a list with `data format` property,\nwhich can be `fastq` or `bam` and an individual entry for experiment.\nEach experiment is represented as set of parameters (e.g. in curly brackets):\n- `name` - experiment name, string (optional);\n- `long read files` - a list of paths to long read files matching the specified format;\n- `lables` - a list labels for long read files for expression table (optional, must be equal to the number of long read files)\n- `illumina bam` - a list of paths to short read BAM files for splice site correction (optional).\n\nAll paths should be either absolute or relative to the YAML file.\nSee more in [examples](#examples).\n\n#### Providing input reads via dataset description file (deprecated since 3.4)\n\n`--bam_list` (_deprecated since 3.4_)\n    Text file with list of BAM files, one file per line. Each file must be sorted and indexed.\nLeave empty line or experiment name starting with # between the experiments.\nFor each experiment IsoQuant will generate a individual GTF and count tables.\nYou may also give a label for each file specifying it after a colon (e.g. `/PATH/TO/file.bam:replicate1`).\n\n`--fastq_list` (_deprecated since 3.4_)\n    Text file with list of FASTQ/FASTA files (can be gzipped),  one file per line.\nLeave empty line or experiment name starting with # between the experiments.\nFor each experiment IsoQuant will generate a individual GTF and count tables.\nYou may also give a label for each file specifying it after a colon (e.g. `/PATH/TO/file.fastq:replicate1`).\n\n#### Other input options:\n`--stranded`\n    Reads strandness type, supported values are: `forward`, `reverse`, `none`.\n\n`--fl_data`\n    Input sequences represent full-length transcripts; both ends of the sequence are considered to be reliable.\n\n`--prefix` or `-p`\n    Prefix for all output files and sub-folder name. `OUT` if not set.\n\n`--labels` or `-l`\n    Sets space-separated sample names. Make sure that the number of labels is equal to the number of files.\nInput file names are used as labels if not set.\n\n`--read_group`\n Sets a way to group feature counts (e.g. by cell type). Available options are:\n * `file_name`: groups reads by their original file names (or file name labels) within an experiment.\nThis option makes sense when multiple files are provided.\nThis option is designed for obtaining expression tables with a separate column for each file.\nIf multiple BAM/FASTQ files are provided and `--read_group` option is not set, IsoQuant will set `--read_group:file_name`\nby default.\n * `tag`: groups reads by BAM file read tag: set `tag:TAG`, where `TAG` is the desired tag name\n(e.g. `tag:RG` with use `RG` values as groups, `RG` will be used if unset);\n * `read_id`: groups reads by read name suffix: set `read_id:DELIM` where `DELIM` is the\nsymbol/string by which the read id will be split\n(e.g. if `DELIM` is `_`, for read `m54158_180727_042959_59310706_ccs_NEU` the group will set as `NEU`);\n * `file`: uses additional file with group information for every read: `file:FILE:READ_COL:GROUP_COL:DELIM`,\nwhere `FILE` is the file name, `READ_COL` is column with read ids (0 if not set),\n`GROUP_COL` is column with group ids (1 if not set),\n`DELIM` is separator symbol (tab if not set). File can be gzipped.\n\n\n### Output options\n\n`--sqanti_output`\n    Produce comparison between novel and known transcripts in SQANTI-like format.\n    Will take effect only when reference annotation is provided.\n\n`--check_canonical`\n    Report whether read or constructed transcript model contains non-canonical splice junction (requires more time).\n\n`--count_exons`\n    Perform exon and intron counting in addition to gene and transcript counting.\n    Will take effect only when reference annotation is provided.\n\n`--bam_tags`\n    Comma separated list of BAM tags that will be imported into `read_assignments.tsv`.\n\n### Pipeline options\n\n`--resume`\n    Resume a previously unfinished run. Output folder with previous run must be specified.\n    Allowed options are `--threads` and `--debug`, other options cannot be changed.\n    IsoQuant will run from the beginning if the output folder does not contain the previous run.\n\n`--force`\n    force to overwrite the folder with previous run.\n\n`--threads` or `-t`\n    Number of threads to use, 16 by default.\n\n`--clean_start`\n    Do not use previously generated gene database, genome indices or BAM files, run pipeline from the very beginning (will take more time).\n\n`--no_model_construction`\n    Do not report transcript models, run read assignment and quantification of reference features only.\n\n`--run_aligner_only`\n    Align reads to the reference without running IsoQuant itself.\n\n\n### Algorithm parameters\n<a name=""params""></a>\n\n#### Quantification\n\n`--transcript_quantification` Transcript quantification strategy;\n`--gene_quantification` Gene quantification strategy;\n\nAvailable options for quantification:\n\n* `unique_only` - use only reads that are uniquely assigned and consistent with a transcript/gene\n(i.e. flagged as unique/unique_minor_difference), default fot transcript quantification;\n* `with_ambiguous` - in addition to unique reads, ambiguously assigned consistent reads are split between features with equal weights \n(e.g. 1/2 when a read is assigned to 2 features simultaneously);\n* `unique_splicing_consistent` - uses uniquely assigned reads that do not contradict annotated splice sites\n(i.e. flagged as unique/unique_minor_difference or inconsistent_non_intronic), default for gene quantification;\n* `unique_inconsistent` - uses uniquely assigned reads allowing any kind of inconsistency;\n* `all` - all of the above.\n\n\n#### Read to isoform matching:\n\n`--matching_strategy` A preset of parameters for read-to-isoform matching algorithm, should be one of:\n\n* `exact` - delta = 0, all minor errors are treated as inconsistencies;  \n* `precise` - delta = 4, only minor alignment errors are allowed, default for PacBio data;  \n* `default` - delta = 6, alignment errors typical for Nanopore reads are allowed, short novel introns are treated as deletions;   \n* `loose` - delta = 12, even more serious inconsistencies are ignored, ambiguity is resolved based on nucleotide similarity.\n\nMatching strategy is chosen automatically based on specified data type.\nHowever, the parameters will be overridden if the matching strategy is set manually.\n\n#### Read alignment correction:\n\n`--splice_correction_strategy` A preset of parameters for read alignment correction algorithms, should be one of:\n\n* `none` - no correction is applied;  \n* `default_pacbio` - optimal settings for PacBio CCS reads;\n* `default_ont` - optimal settings for ONT reads;\n* `conservative_ont` - conservative settings for ONT reads, only incorrect splice junction and skipped exons are fixed;\n* `assembly` - optimal settings for a transcriptome assembly;    \n* `all` - correct all discovered minor inconsistencies, may result in overcorrection.\n\nThis option is chosen automatically based on specified data type, but will be overridden if set manually.\n\n#### Transcript model construction:\n`--model_construction_strategy` A preset of parameters for transcript model construction algorithm, should be one of\n\n* `reliable` - only the most abundant and reliable transcripts are reported, precise, but not sensitive;  \n* `default_pacbio` - optimal settings for PacBio CCS reads;\n* `sensitive_pacbio` - sensitive settings for PacBio CCS reads, more transcripts are reported possibly at a cost of precision;\n* `fl_pacbio` - optimal settings for full-length PacBio CCS reads, will be used if `--data_type pacbio_ccs` and `--fl_data` options are set;\n* `default_ont` - optimal settings for ONT reads, novel mono-exonic transcripts are not reported (use `--report_novel_unspliced true`);\n* `sensitive_ont` - sensitive settings for ONT reads, more transcripts are reported possibly at a cost of precision (including novel mono-exonic isoforms);\n* `assembly` - optimal settings for a transcriptome assembly: input sequences are considered to be reliable and each transcript to be represented only once, so abundance is not considered;    \n* `all` - reports almost all novel transcripts, loses precision in favor to recall.\n\nThis option is chosen automatically based on specified data type, but will be overridden if set manually.\n\n\n`--report_novel_unspliced` Report novel mono-exonic transcripts (set `true` or `false`).\nThe default value is `false` for Nanopore data and `true` for other data types.\nThe main explanation that some aligners report a lot of false unspliced alignments\nfor ONT reads.\n\n\n`--report_canonical`\n    Strategy for reporting novel transcripts based on canonical splice sites, should be one of:\n\n* `auto` - automatic selection based on the data type and model construction strategy (default); \n* `only_canonical` - report novel transcripts, which contain only canonical splice sites;\n* `only_stranded` - report novel transcripts, for which the strand can be unambiguously derived using splice sites and \npresence of a polyA tail, allowing some splice sites to be non-canonical;\n* `all` -- report all transcript model regardless of their splice sites.\n\n\n`--polya_requirement` Strategy for using polyA tails during transcript model construction, should be one of:\n\n* `auto` - default behaviour: polyA tails are required if at least 70% of the reads have polyA tail; \npolyA tails are always required for 1/2-exon transcripts when using ONT data (this is caused by elevated number of false 1/2-exonic alignments reported by minimap2); \n* `never` - polyA tails are never required; use this option **at your own risk** as it may noticeably increase false discovery rate, especially for ONT data;\n* `always` - reported transcripts are always required to have polyA support in the reads.\n\nNote, that polyA tails are always required for reporting novel unspliced isoforms. \n\n\n\n### Hidden options\n<a name=""hidden""></a>\nOptions below are shown only with `--full_help` option.\nWe recommend _not_ to modify these options unless you are clearly aware of their effect.\n\n`--no_gzip`\n    Do not compress large output files.\n\n`--no_gtf_check`\n    Do not perform input GTF checks.\n\n`--no_secondary`\n    Ignore secondary alignments.\n\n`--aligner`\n    Force to use this alignment method, can be `starlong` or `minimap2`; `minimap2` is currently used as default. Make sure the specified aligner is in the `$PATH` variable.\n\n`--no_junc_bed`\n    Do not use gene annotation for read mapping.\n\n`--junc_bed_file`\n    Annotation in BED12 format produced by `paftools.js gff2bed` (can be found in `minimap2`), will be created automatically if not given.\n\n`--delta`\n    Delta for inexact splice junction comparison, chosen automatically based on data type (e.g. 4bp for PacBio, 6pb for ONT).\n\n`--genedb_output`\n    If your output folder is located on a shared storage (e.g. NFS share), use this option to set another path\n    for storing the annotation database, because SQLite database cannot be created on a shared disks.\n    The folder will be created automatically.\n\n`--high_memory`\n    Cache read alignments instead for making several passes over a BAM file, noticeably increases RAM usage, \nbut may improve running time when disk I/O is relatively slow.\n\n`--min_mapq`\n    Filers out all alignments with MAPQ less than this value (will also filter all secondary alignments, as they typically have MAPQ = 0).\n\n`--inconsistent_mapq_cutoff`\n    Filers out inconsistent alignments with MAPQ less than this value (works when the reference annotation is provided, default is 5).\n\n`--simple_alignments_mapq_cutoff`\n    Filers out alignments with 1 or 2 exons and MAPQ less than this value (works only in annotation-free mode, default is 1).\n\n`--normalization_method`\n    Method for normalizing non-grouped counts into TPMs:\n* `simple` - standard method, scale factor equals to 1 million divided by the counts sum (default);\n* `usable_reads` - includes all reads assigned to a feature including the ones that were filtered out\nduring quantification (i.e. inconsistent or ambiguous);\nscale factor equals to 1 million divided by the number of all assigned reads.\nIn this case the sum of all gene/transcript TPMs may not add up to 1 million.\nExperiments with simulated data show that this method could give more accurate estimations.\nHowever, normalization method does not affect correlation/relative proportions.\n\n\n### Examples\n<a name=""examples""></a>\n\n* Mapped PacBio CCS reads in BAM format; pre-converted gene annotation:\n\n```bash\nisoquant.py -d pacbio_ccs --bam mapped_reads.bam \\n --genedb annotation.db --output output_dir\n```\n\n* Nanopore dRNA stranded reads; official annotation in GTF format, use custon prefix for output:\n```bash\nisoquant.py -d nanopore --stranded forward --fastq ONT.raw.fastq.gz \\n --reference reference.fasta --genedb annotation.gtf --complete_genedb \\n --output output_dir --prefix My_ONT\n```\n\n* Nanopore cDNA reads; no reference annotation:\n```bash\nisoquant.py -d nanopore --fastq ONT.cDNA.raw.fastq.gz \\n --reference reference.fasta --output output_dir --prefix My_ONT_cDNA\n```\n\n* PacBio FL reads; custom annotation in GTF format, which contains only exon features:\n```bash\nisoquant.py -d pacbio_ccs --fl_data --fastq CCS.fastq \\n --reference reference.fasta --genedb genes.gtf --output output_dir\n```\n\n* Nanopore cDNA reads, multiple samples/replicas within a single experiment; official annotation in GTF format:\n```bash\nisoquant.py -d nanopore --bam ONT.cDNA_1.bam ONT.cDNA_2.bam ONT.cDNA_3.bam \\n --reference reference.fasta --genedb annotation.gtf --complete_genedb --output output_dir\n --predix ONT_3samples --labels A1 A2 A3\n```\n\n* ONT cDNA reads; 2 experiments with 3 replicates; official annotation in GTF format:\n```bash\nisoquant.py -d nanopore --yaml dataset.yaml  \\n --complete_genedb --genedb genes.gtf \\n --reference reference.fasta --output output_dir\n```\n\ndataset.yaml file :\n\n```\n[\n  data format: ""fastq"",\n  {\n    name: ""Experiment1"",\n    long read files: [\n      ""/PATH/TO/SAMPLE1/file1.fastq"",\n      ""/PATH/TO/SAMPLE1/file2.fastq"",\n      ""/PATH/TO/SAMPLE1/file3.fastq""\n    ],\n    labels: [\n      ""Replicate1"",\n      ""Replicate2"",\n      ""Replicate3""\n    ]\n  },\n  {\n    name: ""Experiment1"",\n    long read files: [\n      ""/PATH/TO/SAMPLE2/file1.fastq"",\n      ""/PATH/TO/SAMPLE2/file2.fastq"",\n      ""/PATH/TO/SAMPLE2/file3.fastq""\n    ],\n    labels: [\n      ""Replicate1"",\n      ""Replicate2"",\n      ""Replicate3""\n    ]\n  }\n]\n\n```\n\n\nIsoQuant will produce 2 sets of resulting files (including annotations and expression tables), one for each experiment.\nOutput sub-folder will be named `Experiment1` and `Experiment2`.\nExpression tables will have columns ""Replicate1"", ""Replicate2"" and ""Replicate3"".\n\n\n* ONT cDNA reads; 1 experiment with 2 replicates, each replicate has 2 files; official annotation in GTF format:\n```bash\nisoquant.py -d nanopore --yaml dataset.yaml  \\n  --complete_genedb --genedb genes.gtf \\n --reference reference.fasta --prefix MY_SAMPLE \\n --output output_dir  \n```\n\ndataset.yaml file :\n\n\n```\n[\n  data format: ""fastq"",\n  {\n    name: ""Experiment1"",\n    long read files: [\n      ""/PATH/TO/SAMPLE1/file1.fastq"",\n      ""/PATH/TO/SAMPLE1/file2.fastq"",\n      ""/PATH/TO/SAMPLE1/file3.fastq"",\n      ""/PATH/TO/SAMPLE1/file3.fastq""\n    ],\n    labels: [\n      ""Replicate1"",\n      ""Replicate1"",\n      ""Replicate2"",\n      ""Replicate2""\n    ]\n  }\n]\n\n```\n\n\nIsoQuant will produce one output sub-folder `Experiment1`.\nExpressio",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
