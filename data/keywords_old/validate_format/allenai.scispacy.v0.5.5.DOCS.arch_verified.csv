quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,filename,wiki,url,total_similar,target_keywords,target_matched_words,format_prompt,to_eliminate,reason
Availability,"	1607	1609	CF	T038	UMLS:C0010674; 25763772	1627	1630	CPA	T038	UMLS:C0854135; 25763772	1643	1661	class II mutations	T038	UMLS:C0026882; 25763772	1706	1711	DCTN4	T103	UMLS:C4308010; 25763772	1721	1729	variants	T103	UMLS:C0597298; 25763772	1742	1753	p.Tyr263Cys	T103	UMLS:C0597298; 25763772	1778	1790	pathogenesis	T038	UMLS:C0699748; 25763772	1794	1797	CPA	T038	UMLS:C0854135; 25763772	1806	1808	CF	T038	UMLS:C0010674. 25847295|t|Nonylphenol diethoxylate inhibits apoptosis induced in PC12 cells; 25847295|a|Nonylphenol and short-chain nonylphenol ethoxylates such as NP2 EO are present in aquatic environment as wastewater contaminants, and their toxic effects on aquatic species have been reported. Apoptosis has been shown to be induced by serum deprivation or copper treatment. To understand the toxicity of nonylphenol diethoxylate, we investigated the effects of NP2 EO on apoptosis induced by serum deprivation and copper by using PC12 cell system. Nonylphenol diethoxylate itself showed no toxicity and recovered cell viability from apoptosis. In addition, nonylphenol diethoxylate decreased DNA fragmentation caused by apoptosis in PC12 cells. This phenomenon was confirmed after treating apoptotic PC12 cells with nonylphenol diethoxylate, whereas the cytochrome c release into the cytosol decreased as compared to that in apoptotic cells not treated with nonylphenol diethoxylate s. Furthermore, Bax contents in apoptotic cells were reduced after exposure to nonylphenol diethoxylate. Thus, nonylphenol diethoxylate has the opposite effect on apoptosis in PC12 cells compared to nonylphenol, which enhances apoptosis induced by serum deprivation. The difference in structure of the two compounds is hypothesized to be responsible for this phenomenon. These results indicated that nonylphenol diethoxylate has capability to affect cell differentiation and development and has potentially harmful effect on organisms because of its unexpected impact on apoptosis. ¬© 2015 Wiley Periodicals, Inc.",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/tests/fixtures/med_mentions.txt:5631,recover,recovered,5631,tests/fixtures/med_mentions.txt,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/tests/fixtures/med_mentions.txt,1,['recover'],['recovered'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: 	1607	1609	CF	T038	UMLS:C0010674; 25763772	1627	1630	CPA	T038	UMLS:C0854135; 25763772	1643	1661	class II mutations	T038	UMLS:C0026882; 25763772	1706	1711	DCTN4	T103	UMLS:C4308010; 25763772	1721	1729	variants	T103	UMLS:C0597298; 25763772	1742	1753	p.Tyr263Cys	T103	UMLS:C0597298; 25763772	1778	1790	pathogenesis	T038	UMLS:C0699748; 25763772	1794	1797	CPA	T038	UMLS:C0854135; 25763772	1806	1808	CF	T038	UMLS:C0010674. 25847295|t|Nonylphenol diethoxylate inhibits apoptosis induced in PC12 cells; 25847295|a|Nonylphenol and short-chain nonylphenol ethoxylates such as NP2 EO are present in aquatic environment as wastewater contaminants, and their toxic effects on aquatic species have been reported. Apoptosis has been shown to be induced by serum deprivation or copper treatment. To understand the toxicity of nonylphenol diethoxylate, we investigated the effects of NP2 EO on apoptosis induced by serum deprivation and copper by using PC12 cell system. Nonylphenol diethoxylate itself showed no toxicity and recovered cell viability from apoptosis. In addition, nonylphenol diethoxylate decreased DNA fragmentation caused by apoptosis in PC12 cells. This phenomenon was confirmed after treating apoptotic PC12 cells with nonylphenol diethoxylate, whereas the cytochrome c release into the cytosol decreased as compared to that in apoptotic cells not treated with nonylphenol diethoxylate s. Furthermore, Bax contents in apoptotic cells were reduced after exposure to nonylphenol diethoxylate. Thus, nonylphenol diethoxylate has the opposite effect on apoptosis in PC12 cells compared to nonylphenol, which enhances apoptosis induced by serum deprivation. The difference in structure of the two compounds is hypothesized to be responsible for this phenomenon. These results indicated that nonylphenol diethoxylate has capability to affect cell differentiation and development and has potentially harmful effect on organisms because of its unexpected impact on apoptosis. ¬© 2015 Wiley Periodicals, Inc.
",False,"The text includes a detailed study on the effects of nonylphenol diethoxylate on apoptosis in PC12 cells, including methods, observations, and conclusions. It is a descriptive scientific text with complete sentences discussing research findings."
Availability,"-installation.html). 2. Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:4338,down,download,4338,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['down'],['download'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: -installation.html). 2. Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors
",False,
Availability,"<p align=""center""><img width=""50%"" src=""docs/scispacy-logo.png"" /></p>. This repository contains custom pipes and models related to using spaCy for scientific documents. In particular, there is a custom tokenizer that adds tokenization rules on top of spaCy's; rule-based tokenizer, a POS tagger and syntactic parser trained on biomedical data and; an entity span detection model. Separately, there are also NER models for more specific tasks. **Just looking to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org)** (Note: this demo is running an older version of scispaCy and may produce different results than the latest version). ## Installation; Installing scispacy requires two steps: installing the library and intalling the models. To install the library, run:; ```bash; pip install scispacy; ```. to install a model (see our full selection of available models below), run a command like the following:. ```bash; pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz; ```. Note: We strongly recommend that you use an isolated Python environment (such as virtualenv or conda) to install scispacy.; Take a look below in the ""Setting up a virtual environment"" section if you need some help with this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:889,avail,available,889,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['avail'],['available'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: <p align=""center""><img width=""50%"" src=""docs/scispacy-logo.png"" /></p>. This repository contains custom pipes and models related to using spaCy for scientific documents. In particular, there is a custom tokenizer that adds tokenization rules on top of spaCy's; rule-based tokenizer, a POS tagger and syntactic parser trained on biomedical data and; an entity span detection model. Separately, there are also NER models for more specific tasks. **Just looking to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org)** (Note: this demo is running an older version of scispaCy and may produce different results than the latest version). ## Installation; Installing scispacy requires two steps: installing the library and intalling the models. To install the library, run:; ```bash; pip install scispacy; ```. to install a model (see our full selection of available models below), run a command like the following:. ```bash; pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz; ```. Note: We strongly recommend that you use an isolated Python environment (such as virtualenv or conda) to install scispacy.; Take a look below in the ""Setting up a virtual environment"" section if you need some help with this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------
",True,"The text contains installation instructions and technical details which are more programmatic in nature. While it does include some descriptive content, the primary focus is on providing code snippets and setup procedures, which fit into the category of material that should be eliminated."
Availability,"all --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html). 2. Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sc",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:3823,down,downloaded,3823,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['down'],['downloaded'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: all --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html). 2. Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sc
",False,
Availability,"atterns); >>> [('such_as', Keystone plant species, fig trees)]; ```. ## Citing. If you use ScispaCy in your research, please cite [ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing](https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Additionally, please indicate which version and model of ScispaCy you used so that your research can be reproduced.; ```; @inproceedings{neumann-etal-2019-scispacy,; title = ""{S}cispa{C}y: {F}ast and {R}obust {M}odels for {B}iomedical {N}atural {L}anguage {P}rocessing"",; author = ""Neumann, Mark and; King, Daniel and; Beltagy, Iz and; Ammar, Waleed"",; booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",; month = aug,; year = ""2019"",; address = ""Florence, Italy"",; publisher = ""Association for Computational Linguistics"",; url = ""https://www.aclweb.org/anthology/W19-5034"",; doi = ""10.18653/v1/W19-5034"",; pages = ""319--327"",; eprint = {arXiv:1902.07669},; abstract = ""Despite recent advances in natural language processing, many statistical models for processing text perform extremely poorly under domain shift. Processing biomedical and clinical text is a critically important application area of natural language processing, for which there are few robust, practical, publicly available models. This paper describes scispaCy, a new Python library and models for practical biomedical/scientific text processing, which heavily leverages the spaCy library. We detail the performance of two packages of models released in scispaCy and demonstrate their robustness on several tasks and datasets. Models and code are available at https://allenai.github.io/scispacy/."",; }; ```. ScispaCy is an open-source project developed by [the Allen Institute for Artificial Intelligence (AI2)](http://www.allenai.org).; AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering. ",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:16023,robust,robust,16023,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,4,"['avail', 'robust']","['available', 'robust', 'robustness']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: atterns); >>> [('such_as', Keystone plant species, fig trees)]; ```. ## Citing. If you use ScispaCy in your research, please cite [ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing](https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Additionally, please indicate which version and model of ScispaCy you used so that your research can be reproduced.; ```; @inproceedings{neumann-etal-2019-scispacy,; title = ""{S}cispa{C}y: {F}ast and {R}obust {M}odels for {B}iomedical {N}atural {L}anguage {P}rocessing"",; author = ""Neumann, Mark and; King, Daniel and; Beltagy, Iz and; Ammar, Waleed"",; booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",; month = aug,; year = ""2019"",; address = ""Florence, Italy"",; publisher = ""Association for Computational Linguistics"",; url = ""https://www.aclweb.org/anthology/W19-5034"",; doi = ""10.18653/v1/W19-5034"",; pages = ""319--327"",; eprint = {arXiv:1902.07669},; abstract = ""Despite recent advances in natural language processing, many statistical models for processing text perform extremely poorly under domain shift. Processing biomedical and clinical text is a critically important application area of natural language processing, for which there are few robust, practical, publicly available models. This paper describes scispaCy, a new Python library and models for practical biomedical/scientific text processing, which heavily leverages the spaCy library. We detail the performance of two packages of models released in scispaCy and demonstrate their robustness on several tasks and datasets. Models and code are available at https://allenai.github.io/scispacy/."",; }; ```. ScispaCy is an open-source project developed by [the Allen Institute for Artificial Intelligence (AI2)](http://www.allenai.org).; AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering. 
",True,"The content contains an API documentation or specifications such as parameter lists, return types, and links to external resources which are typically associated with programmatic descriptions."
Availability,"c; as a mention candidate if the entity candidate does not have a definition.; - `filter_for_definitions: bool, default = True`; Whether to filter entities that can be returned to only include those with definitions; in the knowledge base.; - `max_entities_per_mention : int, optional, default = 5`; The maximum number of entities which will be returned for a given mention, regardless of; how many are nearest neighbours are found. This class sets the `._.kb_ents` attribute on spacy Spans, which consists of a; List[Tuple[str, float]] corresponding to the KB concept_id and the associated score; for a list of `max_entities_per_mention` number of entities. You can look up more information for a given id using the kb attribute of this class:; ```; print(linker.kb.cui_to_entity[concept_id]); ```. #### Example Usage; ```python; import spacy; import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_sm""). # This line takes a while, because we have to download ~1GB of data; # and load a large JSON file (the knowledge base). Be patient!; # Thankfully it should be faster after the first time you use it, because; # the downloads are cached.; # NOTE: The resolve_abbreviations parameter is optional, and requires that; # the AbbreviationDetector pipe has already been added to the pipeline. Adding; # the AbbreviationDetector pipe and setting resolve_abbreviations to True means; # that linking will only be performed on the long form of abbreviations.; nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). # Let's look at a random entity!; entity = doc.ents[1]. print(""Name: "", entity); >>> Name: bulbar muscular atrophy. # Each entity is linked to UMLS with a score; # (currently just char-3gram mat",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:11443,down,download,11443,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['down'],['download'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: c; as a mention candidate if the entity candidate does not have a definition.; - `filter_for_definitions: bool, default = True`; Whether to filter entities that can be returned to only include those with definitions; in the knowledge base.; - `max_entities_per_mention : int, optional, default = 5`; The maximum number of entities which will be returned for a given mention, regardless of; how many are nearest neighbours are found. This class sets the `._.kb_ents` attribute on spacy Spans, which consists of a; List[Tuple[str, float]] corresponding to the KB concept_id and the associated score; for a list of `max_entities_per_mention` number of entities. You can look up more information for a given id using the kb attribute of this class:; ```; print(linker.kb.cui_to_entity[concept_id]); ```. #### Example Usage; ```python; import spacy; import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_sm""). # This line takes a while, because we have to download ~1GB of data; # and load a large JSON file (the knowledge base). Be patient!; # Thankfully it should be faster after the first time you use it, because; # the downloads are cached.; # NOTE: The resolve_abbreviations parameter is optional, and requires that; # the AbbreviationDetector pipe has already been added to the pipeline. Adding; # the AbbreviationDetector pipe and setting resolve_abbreviations to True means; # that linking will only be performed on the long form of abbreviations.; nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). # Let's look at a random entity!; entity = doc.ents[1]. print(""Name: "", entity); >>> Name: bulbar muscular atrophy. # Each entity is linked to UMLS with a score; # (currently just char-3gram mat
",False,"The text contains complete, meaningful sentences in natural language discussing the concept of Spinal and Bulbar Muscular Atrophy (SBMA) and its causes. It includes descriptive content about the disease and its relation to polyglutamine tracts in the androgen receptor."
Availability,"ith different types of cancer, including hepatocellular ; carcinoma (HCC).; """"""; doc = nlp(text). print(list(doc.sents)); >>> [""Myeloid derived suppressor cells (MDSC) are immature myeloid cells with immunosuppressive activity."", ; ""They accumulate in tumor-bearing mice and humans with different types of cancer, including hepatocellular carcinoma (HCC).""]. # Examine the entities extracted by the mention detector.; # Note that they don't have types like in SpaCy, and they; # are more general (e.g including verbs) - these are any; # spans which might be an entity in UMLS, a large; # biomedical database.; print(doc.ents); >>> (Myeloid derived suppressor cells,; MDSC,; immature,; myeloid cells,; immunosuppressive activity,; accumulate,; tumor-bearing mice,; humans,; cancer,; hepatocellular carcinoma,; HCC). # We can also visualise dependency parses; # (This renders automatically inside a jupyter notebook!):; from spacy import displacy; displacy.render(next(doc.sents), style='dep', jupyter=True). # See below for the generated SVG.; # Zoom your browser in a bit!. ```. ![Branching](./example.svg). ### Data Sources. scispaCy models are trained on data from a variety of sources. In particular,; we use:. * **[The GENIA 1.0 Treebank](https://nlp.stanford.edu/~mcclosky/biomedical.html)**, converted to basic Universal Dependencies using the [Stanford Dependency Converter](https://nlp.stanford.edu/software/stanford-dependencies.shtml).; We have made this [dataset available along with the original raw data](https://github.com/allenai/genia-dependency-trees).; * **[word2vec word vectors](http://bio.nlplab.org/#word-vectors)** trained on the Pubmed Central Open Access Subset.; * **[The MedMentions Entity Linking dataset](https://github.com/chanzuckerberg/MedMentions)**, used for training a mention detector.; * **[Ontonotes 5.0](https://catalog.ldc.upenn.edu/LDC2013T19)** to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publicly available.; ",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md:4898,avail,available,4898,docs/index.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md,3,"['avail', 'robust']","['available', 'robust']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ith different types of cancer, including hepatocellular ; carcinoma (HCC).; """"""; doc = nlp(text). print(list(doc.sents)); >>> [""Myeloid derived suppressor cells (MDSC) are immature myeloid cells with immunosuppressive activity."", ; ""They accumulate in tumor-bearing mice and humans with different types of cancer, including hepatocellular carcinoma (HCC).""]. # Examine the entities extracted by the mention detector.; # Note that they don't have types like in SpaCy, and they; # are more general (e.g including verbs) - these are any; # spans which might be an entity in UMLS, a large; # biomedical database.; print(doc.ents); >>> (Myeloid derived suppressor cells,; MDSC,; immature,; myeloid cells,; immunosuppressive activity,; accumulate,; tumor-bearing mice,; humans,; cancer,; hepatocellular carcinoma,; HCC). # We can also visualise dependency parses; # (This renders automatically inside a jupyter notebook!):; from spacy import displacy; displacy.render(next(doc.sents), style='dep', jupyter=True). # See below for the generated SVG.; # Zoom your browser in a bit!. ```. ![Branching](./example.svg). ### Data Sources. scispaCy models are trained on data from a variety of sources. In particular,; we use:. * **[The GENIA 1.0 Treebank](https://nlp.stanford.edu/~mcclosky/biomedical.html)**, converted to basic Universal Dependencies using the [Stanford Dependency Converter](https://nlp.stanford.edu/software/stanford-dependencies.shtml).; We have made this [dataset available along with the original raw data](https://github.com/allenai/genia-dependency-trees).; * **[word2vec word vectors](http://bio.nlplab.org/#word-vectors)** trained on the Pubmed Central Open Access Subset.; * **[The MedMentions Entity Linking dataset](https://github.com/chanzuckerberg/MedMentions)**, used for training a mention detector.; * **[Ontonotes 5.0](https://catalog.ldc.upenn.edu/LDC2013T19)** to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publicly available.; 
",False,
Availability,"nd; an entity span detection model. Separately, there are also NER models for more specific tasks. **Just looking to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org)** (Note: this demo is running an older version of scispaCy and may produce different results than the latest version). ## Installation; Installing scispacy requires two steps: installing the library and intalling the models. To install the library, run:; ```bash; pip install scispacy; ```. to install a model (see our full selection of available models below), run a command like the following:. ```bash; pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz; ```. Note: We strongly recommend that you use an isolated Python environment (such as virtualenv or conda) to install scispacy.; Take a look below in the ""Setting up a virtual environment"" section if you need some help with this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Python 3.11 | ‚ùåüêç | ‚ùåüêç | ‚ùì | ‚ùì | ‚ùì | ‚ùå |; | Python 3.12 | ‚ùåüêç | ‚ùåüêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with ",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:1371,avail,available,1371,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['avail'],['available'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: nd; an entity span detection model. Separately, there are also NER models for more specific tasks. **Just looking to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org)** (Note: this demo is running an older version of scispaCy and may produce different results than the latest version). ## Installation; Installing scispacy requires two steps: installing the library and intalling the models. To install the library, run:; ```bash; pip install scispacy; ```. to install a model (see our full selection of available models below), run a command like the following:. ```bash; pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz; ```. Note: We strongly recommend that you use an isolated Python environment (such as virtualenv or conda) to install scispacy.; Take a look below in the ""Setting up a virtual environment"" section if you need some help with this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Python 3.11 | ‚ùåüêç | ‚ùåüêç | ‚ùì | ‚ùì | ‚ùì | ‚ùå |; | Python 3.12 | ‚ùåüêç | ‚ùåüêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with 
",False,"The text contains explanatory content about installing scispaCy and its models, including instructions for installation steps, which are meaningful sentences in natural language."
Availability,"rk):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; |",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:4478,down,download,4478,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['down'],['download'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: rk):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; |
",False,"The text contains instructions for installing and activating a Conda environment, steps for using the scispaCy model, examples of loading spaCy models, descriptions of available models with their descriptions and install URLs."
Availability,"t = True`; Whether to filter entities that can be returned to only include those with definitions; in the knowledge base.; - `max_entities_per_mention : int, optional, default = 5`; The maximum number of entities which will be returned for a given mention, regardless of; how many are nearest neighbours are found. This class sets the `._.kb_ents` attribute on spacy Spans, which consists of a; List[Tuple[str, float]] corresponding to the KB concept_id and the associated score; for a list of `max_entities_per_mention` number of entities. You can look up more information for a given id using the kb attribute of this class:; ```; print(linker.kb.cui_to_entity[concept_id]); ```. #### Example Usage; ```python; import spacy; import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_sm""). # This line takes a while, because we have to download ~1GB of data; # and load a large JSON file (the knowledge base). Be patient!; # Thankfully it should be faster after the first time you use it, because; # the downloads are cached.; # NOTE: The resolve_abbreviations parameter is optional, and requires that; # the AbbreviationDetector pipe has already been added to the pipeline. Adding; # the AbbreviationDetector pipe and setting resolve_abbreviations to True means; # that linking will only be performed on the long form of abbreviations.; nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). # Let's look at a random entity!; entity = doc.ents[1]. print(""Name: "", entity); >>> Name: bulbar muscular atrophy. # Each entity is linked to UMLS with a score; # (currently just char-3gram matching).; linker = nlp.get_pipe(""scispacy_linker""); for umls_ent in entity._.kb_ents:; 	print(linker.kb.cui_to_entity[u",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:11611,down,downloads,11611,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['down'],['downloads'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: t = True`; Whether to filter entities that can be returned to only include those with definitions; in the knowledge base.; - `max_entities_per_mention : int, optional, default = 5`; The maximum number of entities which will be returned for a given mention, regardless of; how many are nearest neighbours are found. This class sets the `._.kb_ents` attribute on spacy Spans, which consists of a; List[Tuple[str, float]] corresponding to the KB concept_id and the associated score; for a list of `max_entities_per_mention` number of entities. You can look up more information for a given id using the kb attribute of this class:; ```; print(linker.kb.cui_to_entity[concept_id]); ```. #### Example Usage; ```python; import spacy; import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_sm""). # This line takes a while, because we have to download ~1GB of data; # and load a large JSON file (the knowledge base). Be patient!; # Thankfully it should be faster after the first time you use it, because; # the downloads are cached.; # NOTE: The resolve_abbreviations parameter is optional, and requires that; # the AbbreviationDetector pipe has already been added to the pipeline. Adding; # the AbbreviationDetector pipe and setting resolve_abbreviations to True means; # that linking will only be performed on the long form of abbreviations.; nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). # Let's look at a random entity!; entity = doc.ents[1]. print(""Name: "", entity); >>> Name: bulbar muscular atrophy. # Each entity is linked to UMLS with a score; # (currently just char-3gram matching).; linker = nlp.get_pipe(""scispacy_linker""); for umls_ent in entity._.kb_ents:; 	print(linker.kb.cui_to_entity[u
",False,"The text contains a mixture of natural language sentences and code snippets, but primarily consists of explanatory content about the use and functionality of an EntityLinker in Spacy. The majority of the content is descriptive and meant to guide the user through the process of using the linking component, rather than consisting solely of code or logs."
Availability,"the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html). 2. Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:4197,down,download,4197,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['down'],['download'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html). 2. Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/
",False,"The text contains complete, meaningful sentences in natural language, explaining installation steps for a software package."
Availability,"vate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data w",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:4546,down,download,4546,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['down'],['download'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: vate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data w
",False,
Deployability," [RxNorm](https://www.nlm.nih.gov/research/umls/rxnorm/index.html) ontology. RxNorm contains ~100k concepts focused on normalized names for clinical drugs. It is comprised of several other drug vocabularies commonly used in pharmacy management and drug interaction, including First Databank, Micromedex, and the Gold Standard Drug Database.; - `go`: Links to the [Gene Ontology](http://geneontology.org/). The Gene Ontology contains ~67k concepts focused on the functions of genes.; - `hpo`: Links to the [Human Phenotype Ontology](https://hpo.jax.org/app/). The Human Phenotype Ontology contains 16k concepts focused on phenotypic abnormalities encountered in human disease. You may want to play around with some of the parameters; below to adapt to your use case (higher precision, higher recall etc). - `resolve_abbreviations : bool = True, optional (default = False)`; Whether to resolve abbreviations identified in the Doc before performing linking.; This parameter has no effect if there is no `AbbreviationDetector` in the spacy; pipeline.; - `k : int, optional, (default = 30)`; The number of nearest neighbours to look up from the candidate generator per mention.; - `threshold : float, optional, (default = 0.7)`; The threshold that a mention candidate must reach to be added to the mention in the Doc; as a mention candidate.; - `no_definition_threshold : float, optional, (default = 0.95)`; The threshold that a entity candidate must reach to be added to the mention in the Doc; as a mention candidate if the entity candidate does not have a definition.; - `filter_for_definitions: bool, default = True`; Whether to filter entities that can be returned to only include those with definitions; in the knowledge base.; - `max_entities_per_mention : int, optional, default = 5`; The maximum number of entities which will be returned for a given mention, regardless of; how many are nearest neighbours are found. This class sets the `._.kb_ents` attribute on spacy Spans, which consists of a;",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:10005,pipeline,pipeline,10005,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  [RxNorm](https://www.nlm.nih.gov/research/umls/rxnorm/index.html) ontology. RxNorm contains ~100k concepts focused on normalized names for clinical drugs. It is comprised of several other drug vocabularies commonly used in pharmacy management and drug interaction, including First Databank, Micromedex, and the Gold Standard Drug Database.; - `go`: Links to the [Gene Ontology](http://geneontology.org/). The Gene Ontology contains ~67k concepts focused on the functions of genes.; - `hpo`: Links to the [Human Phenotype Ontology](https://hpo.jax.org/app/). The Human Phenotype Ontology contains 16k concepts focused on phenotypic abnormalities encountered in human disease. You may want to play around with some of the parameters; below to adapt to your use case (higher precision, higher recall etc). - `resolve_abbreviations : bool = True, optional (default = False)`; Whether to resolve abbreviations identified in the Doc before performing linking.; This parameter has no effect if there is no `AbbreviationDetector` in the spacy; pipeline.; - `k : int, optional, (default = 30)`; The number of nearest neighbours to look up from the candidate generator per mention.; - `threshold : float, optional, (default = 0.7)`; The threshold that a mention candidate must reach to be added to the mention in the Doc; as a mention candidate.; - `no_definition_threshold : float, optional, (default = 0.95)`; The threshold that a entity candidate must reach to be added to the mention in the Doc; as a mention candidate if the entity candidate does not have a definition.; - `filter_for_definitions: bool, default = True`; Whether to filter entities that can be returned to only include those with definitions; in the knowledge base.; - `max_entities_per_mention : int, optional, default = 5`; The maximum number of entities which will be returned for a given mention, regardless of; how many are nearest neighbours are found. This class sets the `._.kb_ents` attribute on spacy Spans, which consists of a;
",False,"The text includes both descriptive paragraphs about different ontologies and their sizes, as well as some technical parameters for an entity linking tool. While there are code-like elements such as parameter descriptions, the main content is explanatory and provides context about the tools used in a project."
Deployability," to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org)** (Note: this demo is running an older version of scispaCy and may produce different results than the latest version). ## Installation; Installing scispacy requires two steps: installing the library and intalling the models. To install the library, run:; ```bash; pip install scispacy; ```. to install a model (see our full selection of available models below), run a command like the following:. ```bash; pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz; ```. Note: We strongly recommend that you use an isolated Python environment (such as virtualenv or conda) to install scispacy.; Take a look below in the ""Setting up a virtual environment"" section if you need some help with this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Python 3.11 | ‚ùåüêç | ‚ùåüêç | ‚ùì | ‚ùì | ‚ùì | ‚ùå |; | Python 3.12 | ‚ùåüêç | ‚ùåüêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be ",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:1459,install,installing,1459,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['install'],['installing'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org)** (Note: this demo is running an older version of scispaCy and may produce different results than the latest version). ## Installation; Installing scispacy requires two steps: installing the library and intalling the models. To install the library, run:; ```bash; pip install scispacy; ```. to install a model (see our full selection of available models below), run a command like the following:. ```bash; pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz; ```. Note: We strongly recommend that you use an isolated Python environment (such as virtualenv or conda) to install scispacy.; Take a look below in the ""Setting up a virtual environment"" section if you need some help with this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Python 3.11 | ‚ùåüêç | ‚ùåüêç | ‚ùì | ‚ùì | ‚ùì | ‚ùå |; | Python 3.12 | ‚ùåüêç | ‚ùåüêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be 
",False,"The text contains instructions for installing software (scispacy and nmslib) which are in natural language. Even though there are code snippets with bash commands, the overall content is explanatory and helpful to users."
Deployability," using spaCy for scientific documents. In particular, there is a custom tokenizer that adds tokenization rules on top of spaCy's; rule-based tokenizer, a POS tagger and syntactic parser trained on biomedical data and; an entity span detection model. Separately, there are also NER models for more specific tasks. **Just looking to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org)** (Note: this demo is running an older version of scispaCy and may produce different results than the latest version). ## Installation; Installing scispacy requires two steps: installing the library and intalling the models. To install the library, run:; ```bash; pip install scispacy; ```. to install a model (see our full selection of available models below), run a command like the following:. ```bash; pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz; ```. Note: We strongly recommend that you use an isolated Python environment (such as virtualenv or conda) to install scispacy.; Take a look below in the ""Setting up a virtual environment"" section if you need some help with this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Pyt",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:1176,install,install,1176,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  using spaCy for scientific documents. In particular, there is a custom tokenizer that adds tokenization rules on top of spaCy's; rule-based tokenizer, a POS tagger and syntactic parser trained on biomedical data and; an entity span detection model. Separately, there are also NER models for more specific tasks. **Just looking to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org)** (Note: this demo is running an older version of scispaCy and may produce different results than the latest version). ## Installation; Installing scispacy requires two steps: installing the library and intalling the models. To install the library, run:; ```bash; pip install scispacy; ```. to install a model (see our full selection of available models below), run a command like the following:. ```bash; pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz; ```. Note: We strongly recommend that you use an isolated Python environment (such as virtualenv or conda) to install scispacy.; Take a look below in the ""Setting up a virtual environment"" section if you need some help with this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Pyt
",False,"The text contains a mix of setup instructions, installation guides, and product descriptions which are meaningful for users. It includes natural language explanations and specific steps for installing software packages."
Deployability, | en_core_sci_md | A full spaCy pipeline for biomedical data with a larger vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a larger vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ### Performance. Our models achieve performance within 3% of published state of the art dependency parsers and within 0.4% accuracy of state of the art biomedical POS taggers. | model | UAS | LAS | POS | Mentions (F1) | Web UAS | ; |:---------------|:----|:------|:------|:---|:---|; | en_core_sci_sm | 89.18| 87.15 | 98.18 | 67.89 | 87.36 |; | en_core_sci_md | 90.08| 88.16 | 98.46 | 68.86 | 88.04 |; | en_core_sci_lg | 89.97| 88.18 | 98.51 | 68.98 | 87.89 |; | en_core_sci_scibert | 92.12| 90.58 | 98.18 | 67.70 | 92.58 |. | model | F1 | Entity Types|; |:--------------,MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md:1649,release,releases,1649,docs/index.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md,1,['release'],['releases'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  | en_core_sci_md | A full spaCy pipeline for biomedical data with a larger vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a larger vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ### Performance. Our models achieve performance within 3% of published state of the art dependency parsers and within 0.4% accuracy of state of the art biomedical POS taggers. | model | UAS | LAS | POS | Mentions (F1) | Web UAS | ; |:---------------|:----|:------|:------|:---|:---|; | en_core_sci_sm | 89.18| 87.15 | 98.18 | 67.89 | 87.36 |; | en_core_sci_md | 90.08| 88.16 | 98.46 | 68.86 | 88.04 |; | en_core_sci_lg | 89.97| 88.18 | 98.51 | 68.98 | 87.89 |; | en_core_sci_scibert | 92.12| 90.58 | 98.18 | 67.70 | 92.58 |. | model | F1 | Entity Types|; |:--------------
",False,"The text contains a mix of informative sentences discussing the performance of various spaCy models, including their UAS and LAS scores, POS tagging accuracy, and mentions. It also lists different models with their respective performance metrics. While it includes some technical details, these are presented in a way that is understandable for humans and not purely code snippets or logs."
Deployability,------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a larger vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a larger vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ### Performance. Our models achieve performance within 3% of published state of the art dependency parsers and within 0.4% accuracy of state of the art biomedical POS taggers. | model | UAS | LAS | POS | Mentions (F1) | Web UAS | ; |:---------------|:----|:------|:------|:---|:---|; | en_core_sci_sm | 89.18| 87.15 | 98.18 | 67.89 | 87.36 |; | en_core_sci_md | 90.08| 88.16 | 98.46 | 68.86 ,MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md:1469,release,releases,1469,docs/index.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md,1,['release'],['releases'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a larger vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a larger vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ### Performance. Our models achieve performance within 3% of published state of the art dependency parsers and within 0.4% accuracy of state of the art biomedical POS taggers. | model | UAS | LAS | POS | Mentions (F1) | Web UAS | ; |:---------------|:----|:------|:------|:---|:---|; | en_core_sci_sm | 89.18| 87.15 | 98.18 | 67.89 | 87.36 |; | en_core_sci_md | 90.08| 88.16 | 98.46 | 68.86 
",False,"The provided text contains sentences that are complete and meaningful, discussing the performance of spaCy models in biomedical data processing."
Deployability,"---; layout: default; ---. **scispaCy is a Python package containing [spaCy](https://spacy.io/) models for processing _biomedical_, _scientific_ or _clinical_ text.**. ## Interactive Demo; Just looking to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org). ## Installing; ```python; pip install scispacy; pip install <Model URL>; ```; ## Models. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a larger vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a larger vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. | [Download](https://s3-us-west-2.amazonaws.",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md:326,install,install,326,docs/index.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md,7,"['install', 'pipeline', 'release']","['install', 'pipeline', 'releases']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ---; layout: default; ---. **scispaCy is a Python package containing [spaCy](https://spacy.io/) models for processing _biomedical_, _scientific_ or _clinical_ text.**. ## Interactive Demo; Just looking to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org). ## Installing; ```python; pip install scispacy; pip install <Model URL>; ```; ## Models. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a larger vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a larger vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. | [Download](https://s3-us-west-2.amazonaws.
",False,
Deployability,"0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a larger vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ### Performance. Our models achieve performance within 3% of published state of the art dependency parsers and within 0.4% accuracy of state of the art biomedical POS taggers. | model | UAS | LAS | POS | Mentions (F1) | Web UAS | ; |:---------------|:----|:------|:------|:---|:---|; | en_core_sci_sm | 89.18| 87.15 | 98.18 | 67.89 | 87.36 |; | en_core_sci_md | 90.08| 88.16 | 98.46 | 68.86 | 88.04 |; | en_core_sci_lg | 89.97| 88.18 | 98.51 | 68.98 | 87.89 |; | en_core_sci_scibert | 92.12| 90.58 | 98.18 | 67.70 | 92.58 |. | model | F1 | Entity Types|; |:---------------|:-----|:--------|; | en_ner_craft_md | 78.01|GGP, SO, TAXON, CHEBI, GO, CL|; | en_ner_jnlpba_md | 72.06| DNA, CELL_TYPE, CELL_LINE, RNA, PROTEIN |; | en_ner_bc5cdr_md | 84.28| DISE",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md:1831,release,releases,1831,docs/index.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md,1,['release'],['releases'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: 0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a larger vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ### Performance. Our models achieve performance within 3% of published state of the art dependency parsers and within 0.4% accuracy of state of the art biomedical POS taggers. | model | UAS | LAS | POS | Mentions (F1) | Web UAS | ; |:---------------|:----|:------|:------|:---|:---|; | en_core_sci_sm | 89.18| 87.15 | 98.18 | 67.89 | 87.36 |; | en_core_sci_md | 90.08| 88.16 | 98.46 | 68.86 | 88.04 |; | en_core_sci_lg | 89.97| 88.18 | 98.51 | 68.98 | 87.89 |; | en_core_sci_scibert | 92.12| 90.58 | 98.18 | 67.70 | 92.58 |. | model | F1 | Entity Types|; |:---------------|:-----|:--------|; | en_ner_craft_md | 78.01|GGP, SO, TAXON, CHEBI, GO, CL|; | en_ner_jnlpba_md | 72.06| DNA, CELL_TYPE, CELL_LINE, RNA, PROTEIN |; | en_ner_bc5cdr_md | 84.28| DISE
",False,
Deployability,"60k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Additional Pipeline Components. ### AbbreviationDetector; The AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm; for identifying abbreviation definitions in biomedical text."", (Schwartz & Hearst, 2003). You can access the list of abbreviations via the `doc._.abbreviations` attribute and for a given abbreviation,; you can access it's long form (which is a `spacy.tokens.Span`) using `span._.long_form`, which will point to; another span in the document. #### Example Usage; ```python; import spacy. ",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:6109,release,releases,6109,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['release'],['releases'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: 60k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Additional Pipeline Components. ### AbbreviationDetector; The AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm; for identifying abbreviation definitions in biomedical text."", (Schwartz & Hearst, 2003). You can access the list of abbreviations via the `doc._.abbreviations` attribute and for a given abbreviation,; you can access it's long form (which is a `spacy.tokens.Span`) using `span._.long_form`, which will point to; another span in the document. #### Example Usage; ```python; import spacy. 
",False,
Deployability,"://github.com/nmslib/nmslib.git/#subdirectory=python_bindings""`; - `pip install --upgrade pybind11` + `pip install --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html). 2. Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied U",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:3715,install,install,3715,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ://github.com/nmslib/nmslib.git/#subdirectory=python_bindings""`; - `pip install --upgrade pybind11` + `pip install --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html). 2. Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied U
",False,
Deployability,"; ### Creating a release. Scispacy has two components:. - The scispacy pip package; - The scispacy models. The scispacy pip package is published automatically using the `.github/actions/publish.yml` github action. It happens whenever a release is published (with an associated tag) in the github releases UI. In order to create a new release, the following should happen:. #### Updating `scispacy/version.py`; Update the version in version.py. #### Training new models. The entire pipeline can be run using `spacy project run all`. This will train and package all the models. The packages should then be uploaded to the `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/{VERSION}` S3 bucket, and references to previous models (e.g in the readme and in the docs) should be updated. You can find all these places using `git grep <previous version>`. The scripts `install_local_packages.py`, `install_remote_packages.py`, `print_out_metrics.py`, `smoke_test.py`, and `uninstall_local_packages.py` are useful for testing at each step of the process. Before uploading, `install_local_packages.py` and `smoke_test.py` can be used to make sure the packages are installable and do a quick check of output. `print_out_metrics.py` can then be used to easily get the metrics that need to be updated in the README. Once the packages have been uploaded, `uninstall_local_packages.py`, `install_remote_packages.py`, and `smoke_test.py` can be used to ensure everything was uploaded correctly. #### Merge a PR with the above changes; Merge a PR with the above changes, and publish a release with a tag corresponding to the commit from the merged PR. This should trigger the publish github action, which will create the `scispacy` package and publish it to pypi. ",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/RELEASE.md:17,release,release,17,RELEASE.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/RELEASE.md,10,"['install', 'pipeline', 'release', 'update']","['installable', 'pipeline', 'release', 'releases', 'updated']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ; ### Creating a release. Scispacy has two components:. - The scispacy pip package; - The scispacy models. The scispacy pip package is published automatically using the `.github/actions/publish.yml` github action. It happens whenever a release is published (with an associated tag) in the github releases UI. In order to create a new release, the following should happen:. #### Updating `scispacy/version.py`; Update the version in version.py. #### Training new models. The entire pipeline can be run using `spacy project run all`. This will train and package all the models. The packages should then be uploaded to the `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/{VERSION}` S3 bucket, and references to previous models (e.g in the readme and in the docs) should be updated. You can find all these places using `git grep <previous version>`. The scripts `install_local_packages.py`, `install_remote_packages.py`, `print_out_metrics.py`, `smoke_test.py`, and `uninstall_local_packages.py` are useful for testing at each step of the process. Before uploading, `install_local_packages.py` and `smoke_test.py` can be used to make sure the packages are installable and do a quick check of output. `print_out_metrics.py` can then be used to easily get the metrics that need to be updated in the README. Once the packages have been uploaded, `uninstall_local_packages.py`, `install_remote_packages.py`, and `smoke_test.py` can be used to ensure everything was uploaded correctly. #### Merge a PR with the above changes; Merge a PR with the above changes, and publish a release with a tag corresponding to the commit from the merged PR. This should trigger the publish github action, which will create the `scispacy` package and publish it to pypi. 
",False,
Deployability,"<p align=""center""><img width=""50%"" src=""docs/scispacy-logo.png"" /></p>. This repository contains custom pipes and models related to using spaCy for scientific documents. In particular, there is a custom tokenizer that adds tokenization rules on top of spaCy's; rule-based tokenizer, a POS tagger and syntactic parser trained on biomedical data and; an entity span detection model. Separately, there are also NER models for more specific tasks. **Just looking to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org)** (Note: this demo is running an older version of scispaCy and may produce different results than the latest version). ## Installation; Installing scispacy requires two steps: installing the library and intalling the models. To install the library, run:; ```bash; pip install scispacy; ```. to install a model (see our full selection of available models below), run a command like the following:. ```bash; pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz; ```. Note: We strongly recommend that you use an isolated Python environment (such as virtualenv or conda) to install scispacy.; Take a look below in the ""Setting up a virtual environment"" section if you need some help with this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:728,install,installing,728,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,5,['install'],"['install', 'installing']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: <p align=""center""><img width=""50%"" src=""docs/scispacy-logo.png"" /></p>. This repository contains custom pipes and models related to using spaCy for scientific documents. In particular, there is a custom tokenizer that adds tokenization rules on top of spaCy's; rule-based tokenizer, a POS tagger and syntactic parser trained on biomedical data and; an entity span detection model. Separately, there are also NER models for more specific tasks. **Just looking to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org)** (Note: this demo is running an older version of scispaCy and may produce different results than the latest version). ## Installation; Installing scispacy requires two steps: installing the library and intalling the models. To install the library, run:; ```bash; pip install scispacy; ```. to install a model (see our full selection of available models below), run a command like the following:. ```bash; pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz; ```. Note: We strongly recommend that you use an isolated Python environment (such as virtualenv or conda) to install scispacy.; Take a look below in the ""Setting up a virtual environment"" section if you need some help with this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------
",False,"The text contains a mixture of explanatory content about the project and installation instructions. It includes complete sentences, descriptive information, and setup guides which are meaningful and human-readable."
Deployability,"><img width=""50%"" src=""docs/scispacy-logo.png"" /></p>. This repository contains custom pipes and models related to using spaCy for scientific documents. In particular, there is a custom tokenizer that adds tokenization rules on top of spaCy's; rule-based tokenizer, a POS tagger and syntactic parser trained on biomedical data and; an entity span detection model. Separately, there are also NER models for more specific tasks. **Just looking to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org)** (Note: this demo is running an older version of scispaCy and may produce different results than the latest version). ## Installation; Installing scispacy requires two steps: installing the library and intalling the models. To install the library, run:; ```bash; pip install scispacy; ```. to install a model (see our full selection of available models below), run a command like the following:. ```bash; pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz; ```. Note: We strongly recommend that you use an isolated Python environment (such as virtualenv or conda) to install scispacy.; Take a look below in the ""Setting up a virtual environment"" section if you need some help with this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 ",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:1021,release,releases,1021,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['release'],['releases'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ><img width=""50%"" src=""docs/scispacy-logo.png"" /></p>. This repository contains custom pipes and models related to using spaCy for scientific documents. In particular, there is a custom tokenizer that adds tokenization rules on top of spaCy's; rule-based tokenizer, a POS tagger and syntactic parser trained on biomedical data and; an entity span detection model. Separately, there are also NER models for more specific tasks. **Just looking to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org)** (Note: this demo is running an older version of scispaCy and may produce different results than the latest version). ## Installation; Installing scispacy requires two steps: installing the library and intalling the models. To install the library, run:; ```bash; pip install scispacy; ```. to install a model (see our full selection of available models below), run a command like the following:. ```bash; pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz; ```. Note: We strongly recommend that you use an isolated Python environment (such as virtualenv or conda) to install scispacy.; Take a look below in the ""Setting up a virtual environment"" section if you need some help with this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 
",False,
Deployability,"C0597298; 25763772	1778	1790	pathogenesis	T038	UMLS:C0699748; 25763772	1794	1797	CPA	T038	UMLS:C0854135; 25763772	1806	1808	CF	T038	UMLS:C0010674. 25847295|t|Nonylphenol diethoxylate inhibits apoptosis induced in PC12 cells; 25847295|a|Nonylphenol and short-chain nonylphenol ethoxylates such as NP2 EO are present in aquatic environment as wastewater contaminants, and their toxic effects on aquatic species have been reported. Apoptosis has been shown to be induced by serum deprivation or copper treatment. To understand the toxicity of nonylphenol diethoxylate, we investigated the effects of NP2 EO on apoptosis induced by serum deprivation and copper by using PC12 cell system. Nonylphenol diethoxylate itself showed no toxicity and recovered cell viability from apoptosis. In addition, nonylphenol diethoxylate decreased DNA fragmentation caused by apoptosis in PC12 cells. This phenomenon was confirmed after treating apoptotic PC12 cells with nonylphenol diethoxylate, whereas the cytochrome c release into the cytosol decreased as compared to that in apoptotic cells not treated with nonylphenol diethoxylate s. Furthermore, Bax contents in apoptotic cells were reduced after exposure to nonylphenol diethoxylate. Thus, nonylphenol diethoxylate has the opposite effect on apoptosis in PC12 cells compared to nonylphenol, which enhances apoptosis induced by serum deprivation. The difference in structure of the two compounds is hypothesized to be responsible for this phenomenon. These results indicated that nonylphenol diethoxylate has capability to affect cell differentiation and development and has potentially harmful effect on organisms because of its unexpected impact on apoptosis. ¬© 2015 Wiley Periodicals, Inc. Environ Toxicol 31: 1389-1398, 2016.; 25847295	34	43	apoptosis	T038	UMLS:C0162638; 25847295	55	65	PC12 cells	T017	UMLS:C0085262; 25847295	137	144	present	T033	UMLS:C0150312; 25847295	206	219	toxic effects	T037	UMLS:C0600688; 25847295	259	268	Apoptosis	T038	UMLS:C016263",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/tests/fixtures/med_mentions.txt:5895,release,release,5895,tests/fixtures/med_mentions.txt,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/tests/fixtures/med_mentions.txt,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: C0597298; 25763772	1778	1790	pathogenesis	T038	UMLS:C0699748; 25763772	1794	1797	CPA	T038	UMLS:C0854135; 25763772	1806	1808	CF	T038	UMLS:C0010674. 25847295|t|Nonylphenol diethoxylate inhibits apoptosis induced in PC12 cells; 25847295|a|Nonylphenol and short-chain nonylphenol ethoxylates such as NP2 EO are present in aquatic environment as wastewater contaminants, and their toxic effects on aquatic species have been reported. Apoptosis has been shown to be induced by serum deprivation or copper treatment. To understand the toxicity of nonylphenol diethoxylate, we investigated the effects of NP2 EO on apoptosis induced by serum deprivation and copper by using PC12 cell system. Nonylphenol diethoxylate itself showed no toxicity and recovered cell viability from apoptosis. In addition, nonylphenol diethoxylate decreased DNA fragmentation caused by apoptosis in PC12 cells. This phenomenon was confirmed after treating apoptotic PC12 cells with nonylphenol diethoxylate, whereas the cytochrome c release into the cytosol decreased as compared to that in apoptotic cells not treated with nonylphenol diethoxylate s. Furthermore, Bax contents in apoptotic cells were reduced after exposure to nonylphenol diethoxylate. Thus, nonylphenol diethoxylate has the opposite effect on apoptosis in PC12 cells compared to nonylphenol, which enhances apoptosis induced by serum deprivation. The difference in structure of the two compounds is hypothesized to be responsible for this phenomenon. These results indicated that nonylphenol diethoxylate has capability to affect cell differentiation and development and has potentially harmful effect on organisms because of its unexpected impact on apoptosis. ¬© 2015 Wiley Periodicals, Inc. Environ Toxicol 31: 1389-1398, 2016.; 25847295	34	43	apoptosis	T038	UMLS:C0162638; 25847295	55	65	PC12 cells	T017	UMLS:C0085262; 25847295	137	144	present	T033	UMLS:C0150312; 25847295	206	219	toxic effects	T037	UMLS:C0600688; 25847295	259	268	Apoptosis	T038	UMLS:C016263
",False,"The text consists of a research study discussing apoptosis in PC12 cells induced by serum deprivation and copper treatment, and the effects of nonylphenol diethoxylate on this process. It includes detailed experimental methods and observations, but it is presented in a way that's structured more like a report than raw data or code. The content is explanatory and discusses concepts related to toxicology and cell biology."
Deployability,"Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Python 3.11 | ‚ùåüêç | ‚ùåüêç | ‚ùì | ‚ùì | ‚ùì | ‚ùå |; | Python 3.12 | ‚ùåüêç | ‚ùåüêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`; - `pip install --no-binary :all: nmslib`; - `pip install ""nmslib @ git+https://github.com/nmslib/nmslib.git/#subdirectory=python_bindings""`; - `pip install --upgrade pybind11` + `pip install --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/insta",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:2331,install,installed,2331,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,2,['install'],"['install', 'installed']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Python 3.11 | ‚ùåüêç | ‚ùåüêç | ‚ùì | ‚ùì | ‚ùì | ‚ùå |; | Python 3.12 | ‚ùåüêç | ‚ùåüêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`; - `pip install --no-binary :all: nmslib`; - `pip install ""nmslib @ git+https://github.com/nmslib/nmslib.git/#subdirectory=python_bindings""`; - `pip install --upgrade pybind11` + `pip install --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/insta
",False,
Deployability,"Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Python 3.11 | ‚ùåüêç | ‚ùåüêç | ‚ùì | ‚ùì | ‚ùì | ‚ùå |; | Python 3.12 | ‚ùåüêç | ‚ùåüêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`; - `pip install --no-binary :all: nmslib`; - `pip install ""nmslib @ git+https://github.com/nmslib/nmslib.git/#subdirectory=python_bindings""`; - `pip install --upgrade pybind11` + `pip install --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html). 2. Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and download",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:2812,install,install,2812,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,3,"['install', 'upgrade']","['install', 'upgrade']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Python 3.11 | ‚ùåüêç | ‚ùåüêç | ‚ùì | ‚ùì | ‚ùì | ‚ùå |; | Python 3.12 | ‚ùåüêç | ‚ùåüêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`; - `pip install --no-binary :all: nmslib`; - `pip install ""nmslib @ git+https://github.com/nmslib/nmslib.git/#subdirectory=python_bindings""`; - `pip install --upgrade pybind11` + `pip install --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html). 2. Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and download
",False,
Deployability,"](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a larger vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ### Performance. Our models achieve performance within 3% of published state of the art dependency parsers and within 0.4% accuracy of state of the art biomedical POS taggers. | model | UAS | LAS | POS | Mentions (F1) | Web UAS | ; |:---------------|:----|:------|:------|:---|:---|; | en_core_sci_sm | 89.18| 87.15 | 98.18 | 67.89 | 87.36 |; | en_core_sci_md | 90.08| 88.16 | 98.46 | 68.86 | 88.04 |; | en_core_sci_lg | 89.97| 88.18 | 98.51 | 68.98 | 87.89 |; | en_core_sci_scibert | 92.12| 90.58 | 98.18 | 67.70 | 92.58 |. | model | F1 | Entity Types|; |:---------------|:-----|:--------|; | en_ner_craft_md | 78.01|GGP, SO, TAXON, CHEBI, GO, CL|; | en_ner_jnlpba_md | 72.06| DNA, CELL_TYPE, CELL_LINE, RNA, PROTEIN |; | en_ner_bc5cdr_md | 84.28| DISEASE, CHEMICAL|; | en_ner_bionlp13cg_md | 77.84| AMINO_ACID, ANATOMICAL_SYSTEM, CANCER, CELL, CELLULAR_COMPONENT, DEVELOPING_ANATOMICAL_STRUCTURE, GENE_OR_GENE_PRODUCT, IMMATERIAL_ANATOMICAL_",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md:2021,release,releases,2021,docs/index.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md,1,['release'],['releases'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a larger vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ### Performance. Our models achieve performance within 3% of published state of the art dependency parsers and within 0.4% accuracy of state of the art biomedical POS taggers. | model | UAS | LAS | POS | Mentions (F1) | Web UAS | ; |:---------------|:----|:------|:------|:---|:---|; | en_core_sci_sm | 89.18| 87.15 | 98.18 | 67.89 | 87.36 |; | en_core_sci_md | 90.08| 88.16 | 98.46 | 68.86 | 88.04 |; | en_core_sci_lg | 89.97| 88.18 | 98.51 | 68.98 | 87.89 |; | en_core_sci_scibert | 92.12| 90.58 | 98.18 | 67.70 | 92.58 |. | model | F1 | Entity Types|; |:---------------|:-----|:--------|; | en_ner_craft_md | 78.01|GGP, SO, TAXON, CHEBI, GO, CL|; | en_ner_jnlpba_md | 72.06| DNA, CELL_TYPE, CELL_LINE, RNA, PROTEIN |; | en_ner_bc5cdr_md | 84.28| DISEASE, CHEMICAL|; | en_ner_bionlp13cg_md | 77.84| AMINO_ACID, ANATOMICAL_SYSTEM, CANCER, CELL, CELLULAR_COMPONENT, DEVELOPING_ANATOMICAL_STRUCTURE, GENE_OR_GENE_PRODUCT, IMMATERIAL_ANATOMICAL_
",True,The text consists primarily of programmatic data such as performance metrics and entity type statistics which are typically found in API documentation or model specifications. This includes tables and numbers that describe the performance of various models.
Deployability,"_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ne",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:4988,release,releases,4988,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['release'],['releases'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: _core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ne
",False,"The content contains complete, meaningful sentences in natural language discussing the installation of models and their descriptions."
Deployability,"```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Additional Pipeline Components. ### AbbreviationDetector; The AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm; for identifying abbreviatio",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:5749,release,releases,5749,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['release'],['releases'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Additional Pipeline Components. ### AbbreviationDetector; The AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm; for identifying abbreviatio
",False,
Deployability,"amba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`; - `pip install --no-binary :all: nmslib`; - `pip install ""nmslib @ git+https://github.com/nmslib/nmslib.git/#subdirectory=python_bindings""`; - `pip install --upgrade pybind11` + `pip install --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html). 2. Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download sh",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:3339,install,installation,3339,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,2,['install'],['installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: amba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`; - `pip install --no-binary :all: nmslib`; - `pip install ""nmslib @ git+https://github.com/nmslib/nmslib.git/#subdirectory=python_bindings""`; - `pip install --upgrade pybind11` + `pip install --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html). 2. Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download sh
",False,"The text contains detailed installation instructions using bash commands and describes how to set up a virtual environment with Mamba for scispaCy, including code snippets for creating and activating environments. It also includes an example of loading a spaCy model. While there are some code-like elements (bash commands), the majority of the content is explanatory prose discussing how to install and use the software."
Deployability,"atterns); >>> [('such_as', Keystone plant species, fig trees)]; ```. ## Citing. If you use ScispaCy in your research, please cite [ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing](https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Additionally, please indicate which version and model of ScispaCy you used so that your research can be reproduced.; ```; @inproceedings{neumann-etal-2019-scispacy,; title = ""{S}cispa{C}y: {F}ast and {R}obust {M}odels for {B}iomedical {N}atural {L}anguage {P}rocessing"",; author = ""Neumann, Mark and; King, Daniel and; Beltagy, Iz and; Ammar, Waleed"",; booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",; month = aug,; year = ""2019"",; address = ""Florence, Italy"",; publisher = ""Association for Computational Linguistics"",; url = ""https://www.aclweb.org/anthology/W19-5034"",; doi = ""10.18653/v1/W19-5034"",; pages = ""319--327"",; eprint = {arXiv:1902.07669},; abstract = ""Despite recent advances in natural language processing, many statistical models for processing text perform extremely poorly under domain shift. Processing biomedical and clinical text is a critically important application area of natural language processing, for which there are few robust, practical, publicly available models. This paper describes scispaCy, a new Python library and models for practical biomedical/scientific text processing, which heavily leverages the spaCy library. We detail the performance of two packages of models released in scispaCy and demonstrate their robustness on several tasks and datasets. Models and code are available at https://allenai.github.io/scispacy/."",; }; ```. ScispaCy is an open-source project developed by [the Allen Institute for Artificial Intelligence (AI2)](http://www.allenai.org).; AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering. ",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:16280,release,released,16280,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['release'],['released'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: atterns); >>> [('such_as', Keystone plant species, fig trees)]; ```. ## Citing. If you use ScispaCy in your research, please cite [ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing](https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Additionally, please indicate which version and model of ScispaCy you used so that your research can be reproduced.; ```; @inproceedings{neumann-etal-2019-scispacy,; title = ""{S}cispa{C}y: {F}ast and {R}obust {M}odels for {B}iomedical {N}atural {L}anguage {P}rocessing"",; author = ""Neumann, Mark and; King, Daniel and; Beltagy, Iz and; Ammar, Waleed"",; booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",; month = aug,; year = ""2019"",; address = ""Florence, Italy"",; publisher = ""Association for Computational Linguistics"",; url = ""https://www.aclweb.org/anthology/W19-5034"",; doi = ""10.18653/v1/W19-5034"",; pages = ""319--327"",; eprint = {arXiv:1902.07669},; abstract = ""Despite recent advances in natural language processing, many statistical models for processing text perform extremely poorly under domain shift. Processing biomedical and clinical text is a critically important application area of natural language processing, for which there are few robust, practical, publicly available models. This paper describes scispaCy, a new Python library and models for practical biomedical/scientific text processing, which heavily leverages the spaCy library. We detail the performance of two packages of models released in scispaCy and demonstrate their robustness on several tasks and datasets. Models and code are available at https://allenai.github.io/scispacy/."",; }; ```. ScispaCy is an open-source project developed by [the Allen Institute for Artificial Intelligence (AI2)](http://www.allenai.org).; AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering. 
",True,"The text consists primarily of API documentation or specifications, including parameter lists and return types, as well as citations from academic papers."
Deployability,"ave. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:5284,pipeline,pipeline,5284,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ave. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.
",True,"The text consists primarily of API documentation or specifications, including model descriptions and installation instructions. It contains links to download URLs and code snippets for installation."
Deployability,"b has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Python 3.11 | ‚ùåüêç | ‚ùåüêç | ‚ùì | ‚ùì | ‚ùì | ‚ùå |; | Python 3.12 | ‚ùåüêç | ‚ùåüêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`; - `pip install --no-binary :all: nmslib`; - `pip install ""nmslib @ git+https://github.com/nmslib/nmslib.git/#subdirectory=python_bindings""`; - `pip install --upgrade pybind11` + `pip install --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html). 2. Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):.",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:2458,install,installed,2458,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,2,['install'],"['install', 'installed']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: b has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Python 3.11 | ‚ùåüêç | ‚ùåüêç | ‚ùì | ‚ùì | ‚ùì | ‚ùå |; | Python 3.12 | ‚ùåüêç | ‚ùåüêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`; - `pip install --no-binary :all: nmslib`; - `pip install ""nmslib @ git+https://github.com/nmslib/nmslib.git/#subdirectory=python_bindings""`; - `pip install --upgrade pybind11` + `pip install --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html). 2. Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):.
",False,
Deployability,"ble Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:5423,release,releases,5423,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['release'],['releases'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ble Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[
",False,
Deployability,"cispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:4585,install,install,4585,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,2,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: cispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.
",False,"The text contains complete, meaningful sentences in natural language discussing how to install scispacy and its models. It includes step-by-step instructions and explanatory content."
Deployability,"e narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazon",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:5067,pipeline,pipeline,5067,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: e narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazon
",True,The content is API documentation or specifications containing parameter lists and installation instructions which are primarily code-oriented rather than meaningful human-readable text.
Deployability,"e of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:4871,pipeline,pipeline,4871,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: e of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model
",False,
Deployability,"eline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Additional Pipeline Components. ### AbbreviationDetector; The AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm; for identifying abbreviation definitions in biomedical text."", (Schwartz & Hearst, 2003). You can access the list of abbreviations via the `doc._.abbreviations` attribute and for a given abbreviation,; you can access it's long form (which is a `spacy.tokens.Span`) using `span._.long_form`, which will point to; another span in the document. #### Example Usage; ```python; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector"").",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:6291,release,releases,6291,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['release'],['releases'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: eline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Additional Pipeline Components. ### AbbreviationDetector; The AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm; for identifying abbreviation definitions in biomedical text."", (Schwartz & Hearst, 2003). You can access the list of abbreviations via the `doc._.abbreviations` attribute and for a given abbreviation,; you can access it's long form (which is a `spacy.tokens.Span`) using `span._.long_form`, which will point to; another span in the document. #### Example Usage; ```python; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector"").
",False,"The content contains a mix of natural language text and technical details about spaCy components. While there are code snippets and links, the primary content includes explanations of models and their usage, which is meaningful for humans."
Deployability,"load the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trai",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:5205,release,releases,5205,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['release'],['releases'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: load the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trai
",True,"The content consists primarily of code snippets and installation instructions, which should be filtered out."
Deployability,"ng [spaCy](https://spacy.io/) models for processing _biomedical_, _scientific_ or _clinical_ text.**. ## Interactive Demo; Just looking to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org). ## Installing; ```python; pip install scispacy; pip install <Model URL>; ```; ## Models. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a larger vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a larger vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.ta",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md:1070,release,releases,1070,docs/index.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md,1,['release'],['releases'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ng [spaCy](https://spacy.io/) models for processing _biomedical_, _scientific_ or _clinical_ text.**. ## Interactive Demo; Just looking to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org). ## Installing; ```python; pip install scispacy; pip install <Model URL>; ```; ## Models. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a larger vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a larger vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.ta
",False,
Deployability,"ore_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Additional Pipeline Components. ### AbbreviationDetector; The AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm; for identifying abbreviation definitions in biomedical text."", (Schwartz & Hearst, 2003). You can access the list of abbreviations via the `doc._.abbreviations` attribute and for a given abbreviation,; you can access it's long form (which is a `spacy.tokens.Span`) using `span._.long_form`, which will point to; another span in the document. #### Example Usage; ```python; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA ",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:6480,release,releases,6480,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['release'],['releases'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ore_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Additional Pipeline Components. ### AbbreviationDetector; The AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm; for identifying abbreviation definitions in biomedical text."", (Schwartz & Hearst, 2003). You can access the list of abbreviations via the `doc._.abbreviations` attribute and for a given abbreviation,; you can access it's long form (which is a `spacy.tokens.Span`) using `span._.long_form`, which will point to; another span in the document. #### Example Usage; ```python; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA 
",False,
Deployability,org). ## Installing; ```python; pip install scispacy; pip install <Model URL>; ```; ## Models. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a larger vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a larger vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ### Performance. Our models achieve performance within 3% of published state of the art dependency parsers and within 0.4% accuracy of state of the art biomedical POS taggers. | model | UAS | LAS | POS | Mentions (F1,MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md:1294,release,releases,1294,docs/index.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md,1,['release'],['releases'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: org). ## Installing; ```python; pip install scispacy; pip install <Model URL>; ```; ## Models. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a larger vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a larger vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ### Performance. Our models achieve performance within 3% of published state of the art dependency parsers and within 0.4% accuracy of state of the art biomedical POS taggers. | model | UAS | LAS | POS | Mentions (F1
",False,
Deployability,"ormally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`; - `pip install --no-binary :all: nmslib`; - `pip install ""nmslib @ git+https://github.com/nmslib/nmslib.git/#subdirectory=python_bindings""`; - `pip install --upgrade pybind11` + `pip install --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html). 2. Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:3263,install,installation,3263,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['install'],['installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ormally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`; - `pip install --no-binary :all: nmslib`; - `pip install ""nmslib @ git+https://github.com/nmslib/nmslib.git/#subdirectory=python_bindings""`; - `pip install --upgrade pybind11` + `pip install --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html). 2. Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `
",False,
Deployability,"path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Ad",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:5507,pipeline,pipeline,5507,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Ad
",False,"The text contains several spaCy model descriptions, including their purposes and download links. These are explanatory sentences discussing the features and availability of each model."
Deployability,"r**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Python 3.11 | ‚ùåüêç | ‚ùåüêç | ‚ùì | ‚ùì | ‚ùì | ‚ùå |; | Python 3.12 | ‚ùåüêç | ‚ùåüêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`; - `pip install --no-binary :all: nmslib`; - `pip install ""nmslib @ git+https://github.com/nmslib/nmslib.git/#subdirectory=python_bindings""`; - `pip install --upgrade pybind11` + `pip install --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html). 2. Create a Conda environment ",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:2381,install,installed,2381,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,2,['install'],"['install', 'installed']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: r**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Python 3.11 | ‚ùåüêç | ‚ùåüêç | ‚ùì | ‚ùì | ‚ùì | ‚ùå |; | Python 3.12 | ‚ùåüêç | ‚ùåüêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`; - `pip install --no-binary :all: nmslib`; - `pip install ""nmslib @ git+https://github.com/nmslib/nmslib.git/#subdirectory=python_bindings""`; - `pip install --upgrade pybind11` + `pip install --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html). 2. Create a Conda environment 
",False,
Deployability,"results than the latest version). ## Installation; Installing scispacy requires two steps: installing the library and intalling the models. To install the library, run:; ```bash; pip install scispacy; ```. to install a model (see our full selection of available models below), run a command like the following:. ```bash; pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz; ```. Note: We strongly recommend that you use an isolated Python environment (such as virtualenv or conda) to install scispacy.; Take a look below in the ""Setting up a virtual environment"" section if you need some help with this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Python 3.11 | ‚ùåüêç | ‚ùåüêç | ‚ùì | ‚ùì | ‚ùì | ‚ùå |; | Python 3.12 | ‚ùåüêç | ‚ùåüêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:1633,install,install,1633,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: results than the latest version). ## Installation; Installing scispacy requires two steps: installing the library and intalling the models. To install the library, run:; ```bash; pip install scispacy; ```. to install a model (see our full selection of available models below), run a command like the following:. ```bash; pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz; ```. Note: We strongly recommend that you use an isolated Python environment (such as virtualenv or conda) to install scispacy.; Take a look below in the ""Setting up a virtual environment"" section if you need some help with this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Python 3.11 | ‚ùåüêç | ‚ùåüêç | ‚ùì | ‚ùì | ‚ùì | ‚ùå |; | Python 3.12 | ‚ùåüêç | ‚ùåüêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)
",False,
Deployability,"rk):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; |",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:4434,install,install,4434,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: rk):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; |
",False,"The text contains complete, meaningful sentences in natural language explaining how to install and use the scispaCy package. There are instructions for creating a Conda environment, activating it, installing the package, and loading models with example code provided. The content is explanatory and written by humans discussing concepts related to using scispaCy."
Deployability,"rpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Additional Pipeline Components. ### AbbreviationDetector; The AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm; for identifying abbreviation definitions in biomedical text."", (Schwartz & Hearst, 2003). You can access the list of abbreviations via the `doc._.abbreviations` attribute and for a given abbreviation,; you can access it's long form (which is a `spacy.tokens.Span`) using `span._.long_form`, which will point to; another span in the document. #### Example Usage; ```python; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 Span	 Definition; >>> SBMA 		 (33, 34) Spinal and bulbar muscular atrophy; >>> SBMA 	 	 (6, 7) Spinal and bulbar muscular atrophy; >>> AR 		 (29, 30) androgen receptor; ```. > **Note**; > If you want to be able to [serialize your `doc` objects](https://spacy.io/usage/saving-loading), load the abbreviation detector with `make_serializable=True`, e.g. `nlp.add_pipe(""abbreviation_detector"", config={""make_serializable"": True})`. ### EntityLinker. The `EntityLinker` is a SpaCy component which performs linking to a knowledge base. The linker simply p",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:7238,pipeline,pipeline,7238,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: rpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Additional Pipeline Components. ### AbbreviationDetector; The AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm; for identifying abbreviation definitions in biomedical text."", (Schwartz & Hearst, 2003). You can access the list of abbreviations via the `doc._.abbreviations` attribute and for a given abbreviation,; you can access it's long form (which is a `spacy.tokens.Span`) using `span._.long_form`, which will point to; another span in the document. #### Example Usage; ```python; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 Span	 Definition; >>> SBMA 		 (33, 34) Spinal and bulbar muscular atrophy; >>> SBMA 	 	 (6, 7) Spinal and bulbar muscular atrophy; >>> AR 		 (29, 30) androgen receptor; ```. > **Note**; > If you want to be able to [serialize your `doc` objects](https://spacy.io/usage/saving-loading), load the abbreviation detector with `make_serializable=True`, e.g. `nlp.add_pipe(""abbreviation_detector"", config={""make_serializable"": True})`. ### EntityLinker. The `EntityLinker` is a SpaCy component which performs linking to a knowledge base. The linker simply p
",False,"The text contains complete sentences and descriptive content about the AbbreviationDetector component in spaCy, including examples and usage instructions."
Deployability,"tall nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Python 3.11 | ‚ùåüêç | ‚ùåüêç | ‚ùì | ‚ùì | ‚ùì | ‚ùå |; | Python 3.12 | ‚ùåüêç | ‚ùåüêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`; - `pip install --no-binary :all: nmslib`; - `pip install ""nmslib @ git+https://github.com/nmslib/nmslib.git/#subdirectory=python_bindings""`; - `pip install --upgrade pybind11` + `pip install --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html). 2. Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in whic",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:2647,install,install,2647,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,3,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: tall nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Python 3.11 | ‚ùåüêç | ‚ùåüêç | ‚ùì | ‚ùì | ‚ùì | ‚ùå |; | Python 3.12 | ‚ùåüêç | ‚ùåüêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`; - `pip install --no-binary :all: nmslib`; - `pip install ""nmslib @ git+https://github.com/nmslib/nmslib.git/#subdirectory=python_bindings""`; - `pip install --upgrade pybind11` + `pip install --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html). 2. Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in whic
",False,"The text contains a mix of setup instructions for installing Python packages and configuration guidance, which is more descriptive and explanatory rather than code or logs."
Deployability,teractive Demo; Just looking to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org). ## Installing; ```python; pip install scispacy; pip install <Model URL>; ```; ## Models. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a larger vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a larger vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ### Performance. Our models achieve performance within 3% of published state of the art dependency ,MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md:1154,pipeline,pipeline,1154,docs/index.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: teractive Demo; Just looking to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org). ## Installing; ```python; pip install scispacy; pip install <Model URL>; ```; ## Models. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a larger vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a larger vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ### Performance. Our models achieve performance within 3% of published state of the art dependency 
",True,"The text consists primarily of code snippets and installation instructions, which are more programmatic in nature. Additionally, there are model descriptions that list download URLs, which can be considered as API documentation. The presence of markdown formatting like headers and tables also contributes to the programmatic nature of the content. There are no complete, meaningful human-readable sentences discussing concepts or ideas beyond the technical details of installation and model availability."
Deployability,"this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Python 3.11 | ‚ùåüêç | ‚ùåüêç | ‚ùì | ‚ùì | ‚ùì | ‚ùå |; | Python 3.12 | ‚ùåüêç | ‚ùåüêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`; - `pip install --no-binary :all: nmslib`; - `pip install ""nmslib @ git+https://github.com/nmslib/nmslib.git/#subdirectory=python_bindings""`; - `pip install --upgrade pybind11` + `pip install --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions fo",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:2299,install,install,2299,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Python 3.11 | ‚ùåüêç | ‚ùåüêç | ‚ùì | ‚ùì | ‚ùì | ‚ùå |; | Python 3.12 | ‚ùåüêç | ‚ùåüêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`; - `pip install --no-binary :all: nmslib`; - `pip install ""nmslib @ git+https://github.com/nmslib/nmslib.git/#subdirectory=python_bindings""`; - `pip install --upgrade pybind11` + `pip install --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions fo
",False,
Deployability,"tities_per_mention : int, optional, default = 5`; The maximum number of entities which will be returned for a given mention, regardless of; how many are nearest neighbours are found. This class sets the `._.kb_ents` attribute on spacy Spans, which consists of a; List[Tuple[str, float]] corresponding to the KB concept_id and the associated score; for a list of `max_entities_per_mention` number of entities. You can look up more information for a given id using the kb attribute of this class:; ```; print(linker.kb.cui_to_entity[concept_id]); ```. #### Example Usage; ```python; import spacy; import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_sm""). # This line takes a while, because we have to download ~1GB of data; # and load a large JSON file (the knowledge base). Be patient!; # Thankfully it should be faster after the first time you use it, because; # the downloads are cached.; # NOTE: The resolve_abbreviations parameter is optional, and requires that; # the AbbreviationDetector pipe has already been added to the pipeline. Adding; # the AbbreviationDetector pipe and setting resolve_abbreviations to True means; # that linking will only be performed on the long form of abbreviations.; nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). # Let's look at a random entity!; entity = doc.ents[1]. print(""Name: "", entity); >>> Name: bulbar muscular atrophy. # Each entity is linked to UMLS with a score; # (currently just char-3gram matching).; linker = nlp.get_pipe(""scispacy_linker""); for umls_ent in entity._.kb_ents:; 	print(linker.kb.cui_to_entity[umls_ent[0]]). >>> CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked; >>> Definition: An X-linked recessive form of spinal muscular ",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:11772,pipeline,pipeline,11772,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: tities_per_mention : int, optional, default = 5`; The maximum number of entities which will be returned for a given mention, regardless of; how many are nearest neighbours are found. This class sets the `._.kb_ents` attribute on spacy Spans, which consists of a; List[Tuple[str, float]] corresponding to the KB concept_id and the associated score; for a list of `max_entities_per_mention` number of entities. You can look up more information for a given id using the kb attribute of this class:; ```; print(linker.kb.cui_to_entity[concept_id]); ```. #### Example Usage; ```python; import spacy; import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_sm""). # This line takes a while, because we have to download ~1GB of data; # and load a large JSON file (the knowledge base). Be patient!; # Thankfully it should be faster after the first time you use it, because; # the downloads are cached.; # NOTE: The resolve_abbreviations parameter is optional, and requires that; # the AbbreviationDetector pipe has already been added to the pipeline. Adding; # the AbbreviationDetector pipe and setting resolve_abbreviations to True means; # that linking will only be performed on the long form of abbreviations.; nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). # Let's look at a random entity!; entity = doc.ents[1]. print(""Name: "", entity); >>> Name: bulbar muscular atrophy. # Each entity is linked to UMLS with a score; # (currently just char-3gram matching).; linker = nlp.get_pipe(""scispacy_linker""); for umls_ent in entity._.kb_ents:; 	print(linker.kb.cui_to_entity[umls_ent[0]]). >>> CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked; >>> Definition: An X-linked recessive form of spinal muscular 
",False,
Deployability,"tual environment"" section if you need some help with this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Python 3.11 | ‚ùåüêç | ‚ùåüêç | ‚ùì | ‚ùì | ‚ùì | ‚ùå |; | Python 3.12 | ‚ùåüêç | ‚ùåüêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`; - `pip install --no-binary :all: nmslib`; - `pip install ""nmslib @ git+https://github.com/nmslib/nmslib.git/#subdirectory=python_bindings""`; - `pip install --upgrade pybind11` + `pip install --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip'",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:2242,install,install,2242,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: tual environment"" section if you need some help with this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Python 3.11 | ‚ùåüêç | ‚ùåüêç | ‚ùì | ‚ùì | ‚ùì | ‚ùå |; | Python 3.12 | ‚ùåüêç | ‚ùåüêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`; - `pip install --no-binary :all: nmslib`; - `pip install ""nmslib @ git+https://github.com/nmslib/nmslib.git/#subdirectory=python_bindings""`; - `pip install --upgrade pybind11` + `pip install --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip'
",False,
Deployability,"vate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data w",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:4528,install,install,4528,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: vate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data w
",True,"The content consists primarily of code snippets, installation instructions, and model descriptions which are more programmatic in nature."
Deployability,"|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Additional Pipeline Components. ### AbbreviationDetector; The AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm; for identifying abbreviation definitions in biomedical text."", (Schwartz & Hearst, 2003). You can access the list of abbreviations via the `doc._.abbreviations` attribute and for a given abbreviation,; you c",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:5929,release,releases,5929,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['release'],['releases'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a ~360k vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. You may want to [use a GPU](https://spacy.io/usage#gpu) with this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Additional Pipeline Components. ### AbbreviationDetector; The AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm; for identifying abbreviation definitions in biomedical text."", (Schwartz & Hearst, 2003). You can access the list of abbreviations via the `doc._.abbreviations` attribute and for a given abbreviation,; you c
",False,"The content contains multiple links to downloadable resources, which are spaCy models. It also includes descriptions of each model's features, such as vocabulary size and vector dimensions. Additionally, it lists components like AbbreviationDetector with brief explanations on their functionality. This text is informative and provides human-readable information about the software components available for download, making it suitable to be retained."
Deployability,"üêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`; - `pip install --no-binary :all: nmslib`; - `pip install ""nmslib @ git+https://github.com/nmslib/nmslib.git/#subdirectory=python_bindings""`; - `pip install --upgrade pybind11` + `pip install --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html). 2. Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:3219,install,installing,3219,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['install'],['installing'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: üêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`; - `pip install --no-binary :all: nmslib`; - `pip install ""nmslib @ git+https://github.com/nmslib/nmslib.git/#subdirectory=python_bindings""`; - `pip install --upgrade pybind11` + `pip install --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html). 2. Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need
",False,"The text contains a mix of installation instructions, configuration steps, and usage examples. It includes bash commands for setting up environments with Mamba and Conda, as well as code snippets showing how to load spaCy models. The content is a combination of explanatory prose discussing how to install scispaCy and its usage, along with some code. While there are code elements, the primary focus is on setup instructions rather than purely code or logs."
Energy Efficiency,"25763772|t|DCTN4 as a modifier of chronic Pseudomonas aeruginosa infection in cystic fibrosis; 25763772|a|Pseudomonas aeruginosa (Pa) infection in cystic fibrosis (CF) patients is associated with worse long-term pulmonary disease and shorter survival, and chronic Pa infection (CPA) is associated with reduced lung function, faster rate of lung decline, increased rates of exacerbations and shorter survival. By using exome sequencing and extreme phenotype design, it was recently shown that isoforms of dynactin 4 (DCTN4) may influence Pa infection in CF, leading to worse respiratory disease. The purpose of this study was to investigate the role of DCTN4 missense variants on Pa infection incidence, age at first Pa infection and chronic Pa infection incidence in a cohort of adult CF patients from a single centre. Polymerase chain reaction and direct sequencing were used to screen DNA samples for DCTN4 variants. A total of 121 adult CF patients from the Cochin Hospital CF centre have been included, all of them carrying two CFTR defects: 103 developed at least 1 pulmonary infection with Pa, and 68 patients of them had CPA. DCTN4 variants were identified in 24% (29/121) CF patients with Pa infection and in only 17% (3/18) CF patients with no Pa infection. Of the patients with CPA, 29% (20/68) had DCTN4 missense variants vs 23% (8/35) in patients without CPA. Interestingly, p.Tyr263Cys tend to be more frequently observed in CF patients with CPA than in patients without CPA (4/68 vs 0/35), and DCTN4 missense variants tend to be more frequent in male CF patients with CPA bearing two class II mutations than in male CF patients without CPA bearing two class II mutations (P = 0.06). Our observations reinforce that DCTN4 missense variants, especially p.Tyr263Cys, may be involved in the pathogenesis of CPA in male CF.; 25763772	0	5	DCTN4	T103	UMLS:C4308010; 25763772	23	63	chronic Pseudomonas aeruginosa infection	T038	UMLS:C0854135; 25763772	67	82	cystic fibrosis	T038	UMLS:C0010674; 2",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/tests/fixtures/med_mentions.txt:302,reduce,reduced,302,tests/fixtures/med_mentions.txt,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/tests/fixtures/med_mentions.txt,1,['reduce'],['reduced'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: 25763772|t|DCTN4 as a modifier of chronic Pseudomonas aeruginosa infection in cystic fibrosis; 25763772|a|Pseudomonas aeruginosa (Pa) infection in cystic fibrosis (CF) patients is associated with worse long-term pulmonary disease and shorter survival, and chronic Pa infection (CPA) is associated with reduced lung function, faster rate of lung decline, increased rates of exacerbations and shorter survival. By using exome sequencing and extreme phenotype design, it was recently shown that isoforms of dynactin 4 (DCTN4) may influence Pa infection in CF, leading to worse respiratory disease. The purpose of this study was to investigate the role of DCTN4 missense variants on Pa infection incidence, age at first Pa infection and chronic Pa infection incidence in a cohort of adult CF patients from a single centre. Polymerase chain reaction and direct sequencing were used to screen DNA samples for DCTN4 variants. A total of 121 adult CF patients from the Cochin Hospital CF centre have been included, all of them carrying two CFTR defects: 103 developed at least 1 pulmonary infection with Pa, and 68 patients of them had CPA. DCTN4 variants were identified in 24% (29/121) CF patients with Pa infection and in only 17% (3/18) CF patients with no Pa infection. Of the patients with CPA, 29% (20/68) had DCTN4 missense variants vs 23% (8/35) in patients without CPA. Interestingly, p.Tyr263Cys tend to be more frequently observed in CF patients with CPA than in patients without CPA (4/68 vs 0/35), and DCTN4 missense variants tend to be more frequent in male CF patients with CPA bearing two class II mutations than in male CF patients without CPA bearing two class II mutations (P = 0.06). Our observations reinforce that DCTN4 missense variants, especially p.Tyr263Cys, may be involved in the pathogenesis of CPA in male CF.; 25763772	0	5	DCTN4	T103	UMLS:C4308010; 25763772	23	63	chronic Pseudomonas aeruginosa infection	T038	UMLS:C0854135; 25763772	67	82	cystic fibrosis	T038	UMLS:C0010674; 2
",True,"The provided text consists primarily of data and results from a study, including numerical data such as patient numbers, statistical observations, and specific biological information. It lacks meaningful human-readable sentences discussing concepts or ideas in natural language."
Energy Efficiency,"Defective survival and activation of thymocytes in transgenic mice expressing a catalytically inactive form of Ca2+/calmodulin-dependent protein kinase IV.; We have generated transgenic mice that express a catalytically inactive form of Ca2+/calmodulin-dependent protein kinase IV (CaMKIV) specifically in thymic T cells.; The presence of this protein results in a markedly reduced thymic cellularity, although the distribution of the remaining cells is normal based on evaluation of the CD4 and CD8 cell surface antigens that are used to gauge T cell development.; Isolated thymic T cells from the transgenic mice also show a dramatically decreased survival rate when evaluated in culture under conditions that do not favor activation.; When challenged with an activating stimulus such as alpha-CD3 or a combination of phorbol ester plus ionophore, the cells are severely compromised in their ability to produce the cytokine interleukin-2 (IL-2).; Reduction of IL-2 production is secondary to the inability to phosphorylate the cAMP response element binding protein, CREB, and induce expression of the immediate early genes such as Fos B that are required to transactivate the IL-2 promoter.; Because transgene expression was regulated by the proximal promoter of the murine lck gene and this promoter is inactivated in T cells that exit the thymus, the mutant hCaMKIV is not present in peripheral T cells.; Consequently, T lymphocytes present in the spleen can be activated normally in response to either stimulus mentioned above, demonstrating that the effects of the inactive CaMKIV on activation are reversible.; Our results suggest that CaMKIV may represent a physiologically relevant CREB kinase in T cells and that the enzyme is also required to ensure normal expansion of T cells in the thymus.; Whereas the pathway responsible for this latter role is yet to be elucidated, it is unlikely to include CREB phosphorylation.; ",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/tests/custom_tests/data_fixtures/raw/9171236.txt:374,reduce,reduced,374,tests/custom_tests/data_fixtures/raw/9171236.txt,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/tests/custom_tests/data_fixtures/raw/9171236.txt,1,['reduce'],['reduced'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Defective survival and activation of thymocytes in transgenic mice expressing a catalytically inactive form of Ca2+/calmodulin-dependent protein kinase IV.; We have generated transgenic mice that express a catalytically inactive form of Ca2+/calmodulin-dependent protein kinase IV (CaMKIV) specifically in thymic T cells.; The presence of this protein results in a markedly reduced thymic cellularity, although the distribution of the remaining cells is normal based on evaluation of the CD4 and CD8 cell surface antigens that are used to gauge T cell development.; Isolated thymic T cells from the transgenic mice also show a dramatically decreased survival rate when evaluated in culture under conditions that do not favor activation.; When challenged with an activating stimulus such as alpha-CD3 or a combination of phorbol ester plus ionophore, the cells are severely compromised in their ability to produce the cytokine interleukin-2 (IL-2).; Reduction of IL-2 production is secondary to the inability to phosphorylate the cAMP response element binding protein, CREB, and induce expression of the immediate early genes such as Fos B that are required to transactivate the IL-2 promoter.; Because transgene expression was regulated by the proximal promoter of the murine lck gene and this promoter is inactivated in T cells that exit the thymus, the mutant hCaMKIV is not present in peripheral T cells.; Consequently, T lymphocytes present in the spleen can be activated normally in response to either stimulus mentioned above, demonstrating that the effects of the inactive CaMKIV on activation are reversible.; Our results suggest that CaMKIV may represent a physiologically relevant CREB kinase in T cells and that the enzyme is also required to ensure normal expansion of T cells in the thymus.; Whereas the pathway responsible for this latter role is yet to be elucidated, it is unlikely to include CREB phosphorylation.; 
",False,"The text contains multiple complex sentences discussing scientific research on transgenic mice and the effects of a catalytically inactive form of CaMKIV. It includes explanations about experimental results, observations, and implications. The content is explanatory and descriptive, written in natural language without code or logs."
Energy Efficiency,"Induction of cytokine expression in leukocytes by binding of thrombin-stimulated platelets.; BACKGROUND: Activated platelets tether and activate myeloid leukocytes.; To investigate the potential relevance of this mechanism in acute myocardial infarction (AMI), we examined cytokine induction by leukocyte-platelet adhesion and the occurrence of leukocyte-platelet conjugates in patients with AMI.; METHODS AND RESULTS: We obtained peripheral venous blood samples in 20 patients with AMI before and daily for 5 days after direct percutaneous transluminal coronary angioplasty (PTCA) and in 20 patients undergoing elective PTCA.; Throughout the study period, CD41 immunofluorescence of leukocytes (flow cytometry) revealed increased leukocyte-platelet adhesion in patients with AMI compared with control patients (mean +/- SE of fluorescence [channels] before PTCA: 77 +/- 16 versus 35 +/- 9; P = .003).; In vitro, thrombin-stimulated fixed platelets bound to neutrophils and monocytes.; Within 2 hours, this resulted in increased mRNA for interleukin (IL),1 beta, IL-8, and monocyte chemoattractant protein (MCP)-1 in unfractionated leukocytes.; After 4 hours, IL-1 beta and IL-8 concentration of the cell-free supernatant had increased by 268 +/- 36% and 210 +/- 7%, respectively, and cellular MCP-1 content had increased by 170 +/- 8%.; Addition of activated platelets to adherent monocytes had a similar effect and was associated with nuclear factor-kappa B activation.; Inhibition of binding by anti-P selectin antibodies reduced the effect of activated platelets on cytokine production.; CONCLUSIONS: In patients with AMI, leukocyte-platelet adhesion is increased.; Binding of activated platelets induces IL-1 beta, IL-8, and MCP-1 in leukocytes.; Our findings suggest that leukocyte-platelet adhesion contributes to the regulation of inflammatory responses in AMI.; ",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/tests/custom_tests/data_fixtures/raw/9170401.txt:1525,reduce,reduced,1525,tests/custom_tests/data_fixtures/raw/9170401.txt,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/tests/custom_tests/data_fixtures/raw/9170401.txt,1,['reduce'],['reduced'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Induction of cytokine expression in leukocytes by binding of thrombin-stimulated platelets.; BACKGROUND: Activated platelets tether and activate myeloid leukocytes.; To investigate the potential relevance of this mechanism in acute myocardial infarction (AMI), we examined cytokine induction by leukocyte-platelet adhesion and the occurrence of leukocyte-platelet conjugates in patients with AMI.; METHODS AND RESULTS: We obtained peripheral venous blood samples in 20 patients with AMI before and daily for 5 days after direct percutaneous transluminal coronary angioplasty (PTCA) and in 20 patients undergoing elective PTCA.; Throughout the study period, CD41 immunofluorescence of leukocytes (flow cytometry) revealed increased leukocyte-platelet adhesion in patients with AMI compared with control patients (mean +/- SE of fluorescence [channels] before PTCA: 77 +/- 16 versus 35 +/- 9; P = .003).; In vitro, thrombin-stimulated fixed platelets bound to neutrophils and monocytes.; Within 2 hours, this resulted in increased mRNA for interleukin (IL),1 beta, IL-8, and monocyte chemoattractant protein (MCP)-1 in unfractionated leukocytes.; After 4 hours, IL-1 beta and IL-8 concentration of the cell-free supernatant had increased by 268 +/- 36% and 210 +/- 7%, respectively, and cellular MCP-1 content had increased by 170 +/- 8%.; Addition of activated platelets to adherent monocytes had a similar effect and was associated with nuclear factor-kappa B activation.; Inhibition of binding by anti-P selectin antibodies reduced the effect of activated platelets on cytokine production.; CONCLUSIONS: In patients with AMI, leukocyte-platelet adhesion is increased.; Binding of activated platelets induces IL-1 beta, IL-8, and MCP-1 in leukocytes.; Our findings suggest that leukocyte-platelet adhesion contributes to the regulation of inflammatory responses in AMI.; 
",False,
Energy Efficiency,"T017	UMLS:C0085262; 25847295	1176	1185	apoptosis	T038	UMLS:C0162638; 25847295	1197	1202	serum	T031	UMLS:C0229671; 25847295	1234	1243	structure	T082	UMLS:C0678594; 25847295	1255	1264	compounds	T103	UMLS:C0220806; 25847295	1326	1333	results	T033	UMLS:C2825142; 25847295	1399	1419	cell differentiation	T038	UMLS:C0007589; 25847295	1424	1435	development	T038	UMLS:C0243107; 25847295	1456	1470	harmful effect	T037	UMLS:C0600688; 25847295	1520	1529	apoptosis	T038	UMLS:C0162638. 26316050|t|Prevascularized silicon membranes for the enhancement of transport to implanted medical devices; 26316050|a|Recent advances in drug delivery and sensing devices for in situ applications are limited by the diffusion -limiting foreign body response of fibrous encapsulation. In this study, we fabricated prevascularized synthetic device ports to help mitigate this limitation. Membranes with rectilinear arrays of square pores with widths ranging from 40 to 200 Œºm were created using materials (50 Œºm thick double-sided polished silicon) and processes (photolithography and directed reactive ion etching) common in the manufacturing of microfabricated sensors. Vascular endothelial cells responded to membrane geometry by either forming vascular tubes that extended through the pore or completely filling membrane pores after 4 days in culture. Although tube formation began to predominate overgrowth around 75 Œºm and continued to increase at even larger pore sizes, tubes formed at these large pore sizes were not completely round and had relatively thin walls. Thus, the optimum range of pore size for prevascularization of these membranes was estimated to be 75-100 Œºm. This study lays the foundation for creating a prevascularized port that can be used to reduce fibrous encapsulation and thus enhance diffusion to implanted medical devices and sensors. ¬© 2015 Wiley Periodicals, Inc. J Biomed Mater Res Part B: Appl Biomater, 104B: 1602-1609, 2016.; 26316050	16	23	silicon	T103	UMLS:C0037114; 26316050	70	95	impla",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/tests/fixtures/med_mentions.txt:8923,sensor,sensors,8923,tests/fixtures/med_mentions.txt,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/tests/fixtures/med_mentions.txt,1,['sensor'],['sensors'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: T017	UMLS:C0085262; 25847295	1176	1185	apoptosis	T038	UMLS:C0162638; 25847295	1197	1202	serum	T031	UMLS:C0229671; 25847295	1234	1243	structure	T082	UMLS:C0678594; 25847295	1255	1264	compounds	T103	UMLS:C0220806; 25847295	1326	1333	results	T033	UMLS:C2825142; 25847295	1399	1419	cell differentiation	T038	UMLS:C0007589; 25847295	1424	1435	development	T038	UMLS:C0243107; 25847295	1456	1470	harmful effect	T037	UMLS:C0600688; 25847295	1520	1529	apoptosis	T038	UMLS:C0162638. 26316050|t|Prevascularized silicon membranes for the enhancement of transport to implanted medical devices; 26316050|a|Recent advances in drug delivery and sensing devices for in situ applications are limited by the diffusion -limiting foreign body response of fibrous encapsulation. In this study, we fabricated prevascularized synthetic device ports to help mitigate this limitation. Membranes with rectilinear arrays of square pores with widths ranging from 40 to 200 Œºm were created using materials (50 Œºm thick double-sided polished silicon) and processes (photolithography and directed reactive ion etching) common in the manufacturing of microfabricated sensors. Vascular endothelial cells responded to membrane geometry by either forming vascular tubes that extended through the pore or completely filling membrane pores after 4 days in culture. Although tube formation began to predominate overgrowth around 75 Œºm and continued to increase at even larger pore sizes, tubes formed at these large pore sizes were not completely round and had relatively thin walls. Thus, the optimum range of pore size for prevascularization of these membranes was estimated to be 75-100 Œºm. This study lays the foundation for creating a prevascularized port that can be used to reduce fibrous encapsulation and thus enhance diffusion to implanted medical devices and sensors. ¬© 2015 Wiley Periodicals, Inc. J Biomed Mater Res Part B: Appl Biomater, 104B: 1602-1609, 2016.; 26316050	16	23	silicon	T103	UMLS:C0037114; 26316050	70	95	impla
",False,"The text contains sentences that are explanatory and descriptive. It discusses the role of prevascularized silicon membranes in enhancing transport for medical devices, detailing fabrication methods and cellular responses to membrane geometry. The content is written in human-readable prose and provides a clear explanation of the study's findings."
Energy Efficiency,"ethoxylate inhibits apoptosis induced in PC12 cells; 25847295|a|Nonylphenol and short-chain nonylphenol ethoxylates such as NP2 EO are present in aquatic environment as wastewater contaminants, and their toxic effects on aquatic species have been reported. Apoptosis has been shown to be induced by serum deprivation or copper treatment. To understand the toxicity of nonylphenol diethoxylate, we investigated the effects of NP2 EO on apoptosis induced by serum deprivation and copper by using PC12 cell system. Nonylphenol diethoxylate itself showed no toxicity and recovered cell viability from apoptosis. In addition, nonylphenol diethoxylate decreased DNA fragmentation caused by apoptosis in PC12 cells. This phenomenon was confirmed after treating apoptotic PC12 cells with nonylphenol diethoxylate, whereas the cytochrome c release into the cytosol decreased as compared to that in apoptotic cells not treated with nonylphenol diethoxylate s. Furthermore, Bax contents in apoptotic cells were reduced after exposure to nonylphenol diethoxylate. Thus, nonylphenol diethoxylate has the opposite effect on apoptosis in PC12 cells compared to nonylphenol, which enhances apoptosis induced by serum deprivation. The difference in structure of the two compounds is hypothesized to be responsible for this phenomenon. These results indicated that nonylphenol diethoxylate has capability to affect cell differentiation and development and has potentially harmful effect on organisms because of its unexpected impact on apoptosis. ¬© 2015 Wiley Periodicals, Inc. Environ Toxicol 31: 1389-1398, 2016.; 25847295	34	43	apoptosis	T038	UMLS:C0162638; 25847295	55	65	PC12 cells	T017	UMLS:C0085262; 25847295	137	144	present	T033	UMLS:C0150312; 25847295	206	219	toxic effects	T037	UMLS:C0600688; 25847295	259	268	Apoptosis	T038	UMLS:C0162638; 25847295	301	306	serum	T031	UMLS:C0229671; 25847295	322	328	copper	T103	UMLS:C0009968; 25847295	437	446	apoptosis	T038	UMLS:C0162638; 25847295	458	463	serum	T031	UMLS:C",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/tests/fixtures/med_mentions.txt:6064,reduce,reduced,6064,tests/fixtures/med_mentions.txt,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/tests/fixtures/med_mentions.txt,1,['reduce'],['reduced'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ethoxylate inhibits apoptosis induced in PC12 cells; 25847295|a|Nonylphenol and short-chain nonylphenol ethoxylates such as NP2 EO are present in aquatic environment as wastewater contaminants, and their toxic effects on aquatic species have been reported. Apoptosis has been shown to be induced by serum deprivation or copper treatment. To understand the toxicity of nonylphenol diethoxylate, we investigated the effects of NP2 EO on apoptosis induced by serum deprivation and copper by using PC12 cell system. Nonylphenol diethoxylate itself showed no toxicity and recovered cell viability from apoptosis. In addition, nonylphenol diethoxylate decreased DNA fragmentation caused by apoptosis in PC12 cells. This phenomenon was confirmed after treating apoptotic PC12 cells with nonylphenol diethoxylate, whereas the cytochrome c release into the cytosol decreased as compared to that in apoptotic cells not treated with nonylphenol diethoxylate s. Furthermore, Bax contents in apoptotic cells were reduced after exposure to nonylphenol diethoxylate. Thus, nonylphenol diethoxylate has the opposite effect on apoptosis in PC12 cells compared to nonylphenol, which enhances apoptosis induced by serum deprivation. The difference in structure of the two compounds is hypothesized to be responsible for this phenomenon. These results indicated that nonylphenol diethoxylate has capability to affect cell differentiation and development and has potentially harmful effect on organisms because of its unexpected impact on apoptosis. ¬© 2015 Wiley Periodicals, Inc. Environ Toxicol 31: 1389-1398, 2016.; 25847295	34	43	apoptosis	T038	UMLS:C0162638; 25847295	55	65	PC12 cells	T017	UMLS:C0085262; 25847295	137	144	present	T033	UMLS:C0150312; 25847295	206	219	toxic effects	T037	UMLS:C0600688; 25847295	259	268	Apoptosis	T038	UMLS:C0162638; 25847295	301	306	serum	T031	UMLS:C0229671; 25847295	322	328	copper	T103	UMLS:C0009968; 25847295	437	446	apoptosis	T038	UMLS:C0162638; 25847295	458	463	serum	T031	UMLS:C
",False,
Energy Efficiency,"ins a smaller set of higher quality entities, which are used for indexing in Pubmed. MeSH contains ~30k entities. NOTE: The MeSH KB is derived directly from MeSH itself, and as such uses different unique identifiers than the other KBs.; - `rxnorm`: Links to the [RxNorm](https://www.nlm.nih.gov/research/umls/rxnorm/index.html) ontology. RxNorm contains ~100k concepts focused on normalized names for clinical drugs. It is comprised of several other drug vocabularies commonly used in pharmacy management and drug interaction, including First Databank, Micromedex, and the Gold Standard Drug Database.; - `go`: Links to the [Gene Ontology](http://geneontology.org/). The Gene Ontology contains ~67k concepts focused on the functions of genes.; - `hpo`: Links to the [Human Phenotype Ontology](https://hpo.jax.org/app/). The Human Phenotype Ontology contains 16k concepts focused on phenotypic abnormalities encountered in human disease. You may want to play around with some of the parameters; below to adapt to your use case (higher precision, higher recall etc). - `resolve_abbreviations : bool = True, optional (default = False)`; Whether to resolve abbreviations identified in the Doc before performing linking.; This parameter has no effect if there is no `AbbreviationDetector` in the spacy; pipeline.; - `k : int, optional, (default = 30)`; The number of nearest neighbours to look up from the candidate generator per mention.; - `threshold : float, optional, (default = 0.7)`; The threshold that a mention candidate must reach to be added to the mention in the Doc; as a mention candidate.; - `no_definition_threshold : float, optional, (default = 0.95)`; The threshold that a entity candidate must reach to be added to the mention in the Doc; as a mention candidate if the entity candidate does not have a definition.; - `filter_for_definitions: bool, default = True`; Whether to filter entities that can be returned to only include those with definitions; in the knowledge base.; - `max_ent",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:9710,adapt,adapt,9710,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['adapt'],['adapt'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ins a smaller set of higher quality entities, which are used for indexing in Pubmed. MeSH contains ~30k entities. NOTE: The MeSH KB is derived directly from MeSH itself, and as such uses different unique identifiers than the other KBs.; - `rxnorm`: Links to the [RxNorm](https://www.nlm.nih.gov/research/umls/rxnorm/index.html) ontology. RxNorm contains ~100k concepts focused on normalized names for clinical drugs. It is comprised of several other drug vocabularies commonly used in pharmacy management and drug interaction, including First Databank, Micromedex, and the Gold Standard Drug Database.; - `go`: Links to the [Gene Ontology](http://geneontology.org/). The Gene Ontology contains ~67k concepts focused on the functions of genes.; - `hpo`: Links to the [Human Phenotype Ontology](https://hpo.jax.org/app/). The Human Phenotype Ontology contains 16k concepts focused on phenotypic abnormalities encountered in human disease. You may want to play around with some of the parameters; below to adapt to your use case (higher precision, higher recall etc). - `resolve_abbreviations : bool = True, optional (default = False)`; Whether to resolve abbreviations identified in the Doc before performing linking.; This parameter has no effect if there is no `AbbreviationDetector` in the spacy; pipeline.; - `k : int, optional, (default = 30)`; The number of nearest neighbours to look up from the candidate generator per mention.; - `threshold : float, optional, (default = 0.7)`; The threshold that a mention candidate must reach to be added to the mention in the Doc; as a mention candidate.; - `no_definition_threshold : float, optional, (default = 0.95)`; The threshold that a entity candidate must reach to be added to the mention in the Doc; as a mention candidate if the entity candidate does not have a definition.; - `filter_for_definitions: bool, default = True`; Whether to filter entities that can be returned to only include those with definitions; in the knowledge base.; - `max_ent
",False,"The text contains natural language sentences discussing concepts and relationships between different ontologies, such as MeSH, RxNorm, GO, and HPO. It includes parameter descriptions which are part of the documentation but also provides context for their usage, making it readable by humans."
Energy Efficiency,"psulation. In this study, we fabricated prevascularized synthetic device ports to help mitigate this limitation. Membranes with rectilinear arrays of square pores with widths ranging from 40 to 200 Œºm were created using materials (50 Œºm thick double-sided polished silicon) and processes (photolithography and directed reactive ion etching) common in the manufacturing of microfabricated sensors. Vascular endothelial cells responded to membrane geometry by either forming vascular tubes that extended through the pore or completely filling membrane pores after 4 days in culture. Although tube formation began to predominate overgrowth around 75 Œºm and continued to increase at even larger pore sizes, tubes formed at these large pore sizes were not completely round and had relatively thin walls. Thus, the optimum range of pore size for prevascularization of these membranes was estimated to be 75-100 Œºm. This study lays the foundation for creating a prevascularized port that can be used to reduce fibrous encapsulation and thus enhance diffusion to implanted medical devices and sensors. ¬© 2015 Wiley Periodicals, Inc. J Biomed Mater Res Part B: Appl Biomater, 104B: 1602-1609, 2016.; 26316050	16	23	silicon	T103	UMLS:C0037114; 26316050	70	95	implanted medical devices	T033	UMLS:C2828363; 26316050	115	128	drug delivery	T074	UMLS:C0085104; 26316050	153	160	in situ	T082	UMLS:C0444498; 26316050	161	173	applications	T058	UMLS:C0185125; 26316050	213	234	foreign body response	T033	UMLS:C1708386; 26316050	400	406	square	T082	UMLS:C0205120; 26316050	506	522	polished silicon	T103	UMLS:C0037114; 26316050	647	673	Vascular endothelial cells	T017	UMLS:C1257792; 26316050	723	737	vascular tubes	T017	UMLS:C0005847; 26316050	743	751	extended	T082	UMLS:C0231449; 26316050	876	886	overgrowth	T033	UMLS:C1849265; 26316050	1012	1017	round	T082	UMLS:C0332490; 26316050	1042	1047	walls	T082	UMLS:C0442069; 26316050	1164	1169	study	T062	UMLS:C2603343; 26316050	1305	1330	implanted medical devices	T033	UMLS:C28",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/tests/fixtures/med_mentions.txt:9531,reduce,reduce,9531,tests/fixtures/med_mentions.txt,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/tests/fixtures/med_mentions.txt,2,"['reduce', 'sensor']","['reduce', 'sensors']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: psulation. In this study, we fabricated prevascularized synthetic device ports to help mitigate this limitation. Membranes with rectilinear arrays of square pores with widths ranging from 40 to 200 Œºm were created using materials (50 Œºm thick double-sided polished silicon) and processes (photolithography and directed reactive ion etching) common in the manufacturing of microfabricated sensors. Vascular endothelial cells responded to membrane geometry by either forming vascular tubes that extended through the pore or completely filling membrane pores after 4 days in culture. Although tube formation began to predominate overgrowth around 75 Œºm and continued to increase at even larger pore sizes, tubes formed at these large pore sizes were not completely round and had relatively thin walls. Thus, the optimum range of pore size for prevascularization of these membranes was estimated to be 75-100 Œºm. This study lays the foundation for creating a prevascularized port that can be used to reduce fibrous encapsulation and thus enhance diffusion to implanted medical devices and sensors. ¬© 2015 Wiley Periodicals, Inc. J Biomed Mater Res Part B: Appl Biomater, 104B: 1602-1609, 2016.; 26316050	16	23	silicon	T103	UMLS:C0037114; 26316050	70	95	implanted medical devices	T033	UMLS:C2828363; 26316050	115	128	drug delivery	T074	UMLS:C0085104; 26316050	153	160	in situ	T082	UMLS:C0444498; 26316050	161	173	applications	T058	UMLS:C0185125; 26316050	213	234	foreign body response	T033	UMLS:C1708386; 26316050	400	406	square	T082	UMLS:C0205120; 26316050	506	522	polished silicon	T103	UMLS:C0037114; 26316050	647	673	Vascular endothelial cells	T017	UMLS:C1257792; 26316050	723	737	vascular tubes	T017	UMLS:C0005847; 26316050	743	751	extended	T082	UMLS:C0231449; 26316050	876	886	overgrowth	T033	UMLS:C1849265; 26316050	1012	1017	round	T082	UMLS:C0332490; 26316050	1042	1047	walls	T082	UMLS:C0442069; 26316050	1164	1169	study	T062	UMLS:C2603343; 26316050	1305	1330	implanted medical devices	T033	UMLS:C28
",False,"The text consists of a research study discussing fabricated synthetic device ports, membrane creation using specific materials and processes, cell responses to pore geometry, and implications for implanted medical devices. It includes detailed methodology and results, but does not primarily consist of code, logs, API docs, or other programmatic content."
Integrability," full spaCy pipeline for biomedical data with a larger vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ### Performance. Our models achieve performance within 3% of published state of the art dependency parsers and within 0.4% accuracy of state of the art biomedical POS taggers. | model | UAS | LAS | POS | Mentions (F1) | Web UAS | ; |:---------------|:----|:------|:------|:---|:---|; | en_core_sci_sm | 89.18| 87.15 | 98.18 | 67.89 | 87.36 |; | en_core_sci_md | 90.08| 88.16 | 98.46 | 68.86 | 88.04 |; | en_core_sci_lg | 89.97| 88.18 | 98.51 | 68.98 | 87.89 |; | en_core_sci_scibert | 92.12| 90.58 | 98.18 | 67.70 | 92.58 |. | model | F1 | Entity Types|; |:---------------|:-----|:--------|; | en_ner_craft_md | 78.01|GGP, SO, TAXON, CHEBI, GO, CL|; | en_ner_jnlpba_md | 72.06| DNA, CELL_TYPE, CELL_LINE, RNA, PROTEIN |; | en_ner_bc5cdr_md | 84.28| DISEASE, CHEMICAL|; | en_ner_bionlp13cg_md | 77.84| AMINO_ACID, ANATOMICAL_SYSTEM, CANCER, CELL, CELLULAR_COMPONENT, DEVELOPING_ANATOMICAL_STRUCTURE, GENE_OR_GENE_PRODUCT, IMMATERIAL_ANATOMICAL_ENTITY, MULTI-TISSUE_STRUCTURE, ORGAN, ORGANISM, ORGANISM_SUBDIVISION, ORGANISM_SUBSTANCE, PATHOLOGICAL_FORMATION, SIMPLE_CHE",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md:2162,depend,dependency,2162,docs/index.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md,1,['depend'],['dependency'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  full spaCy pipeline for biomedical data with a larger vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ### Performance. Our models achieve performance within 3% of published state of the art dependency parsers and within 0.4% accuracy of state of the art biomedical POS taggers. | model | UAS | LAS | POS | Mentions (F1) | Web UAS | ; |:---------------|:----|:------|:------|:---|:---|; | en_core_sci_sm | 89.18| 87.15 | 98.18 | 67.89 | 87.36 |; | en_core_sci_md | 90.08| 88.16 | 98.46 | 68.86 | 88.04 |; | en_core_sci_lg | 89.97| 88.18 | 98.51 | 68.98 | 87.89 |; | en_core_sci_scibert | 92.12| 90.58 | 98.18 | 67.70 | 92.58 |. | model | F1 | Entity Types|; |:---------------|:-----|:--------|; | en_ner_craft_md | 78.01|GGP, SO, TAXON, CHEBI, GO, CL|; | en_ner_jnlpba_md | 72.06| DNA, CELL_TYPE, CELL_LINE, RNA, PROTEIN |; | en_ner_bc5cdr_md | 84.28| DISEASE, CHEMICAL|; | en_ner_bionlp13cg_md | 77.84| AMINO_ACID, ANATOMICAL_SYSTEM, CANCER, CELL, CELLULAR_COMPONENT, DEVELOPING_ANATOMICAL_STRUCTURE, GENE_OR_GENE_PRODUCT, IMMATERIAL_ANATOMICAL_ENTITY, MULTI-TISSUE_STRUCTURE, ORGAN, ORGANISM, ORGANISM_SUBDIVISION, ORGANISM_SUBSTANCE, PATHOLOGICAL_FORMATION, SIMPLE_CHE
",False,
Integrability,"Defective survival and activation of thymocytes in transgenic mice expressing a catalytically inactive form of Ca2+/calmodulin-dependent protein kinase IV.; We have generated transgenic mice that express a catalytically inactive form of Ca2+/calmodulin-dependent protein kinase IV (CaMKIV) specifically in thymic T cells.; The presence of this protein results in a markedly reduced thymic cellularity, although the distribution of the remaining cells is normal based on evaluation of the CD4 and CD8 cell surface antigens that are used to gauge T cell development.; Isolated thymic T cells from the transgenic mice also show a dramatically decreased survival rate when evaluated in culture under conditions that do not favor activation.; When challenged with an activating stimulus such as alpha-CD3 or a combination of phorbol ester plus ionophore, the cells are severely compromised in their ability to produce the cytokine interleukin-2 (IL-2).; Reduction of IL-2 production is secondary to the inability to phosphorylate the cAMP response element binding protein, CREB, and induce expression of the immediate early genes such as Fos B that are required to transactivate the IL-2 promoter.; Because transgene expression was regulated by the proximal promoter of the murine lck gene and this promoter is inactivated in T cells that exit the thymus, the mutant hCaMKIV is not present in peripheral T cells.; Consequently, T lymphocytes present in the spleen can be activated normally in response to either stimulus mentioned above, demonstrating that the effects of the inactive CaMKIV on activation are reversible.; Our results suggest that CaMKIV may represent a physiologically relevant CREB kinase in T cells and that the enzyme is also required to ensure normal expansion of T cells in the thymus.; Whereas the pathway responsible for this latter role is yet to be elucidated, it is unlikely to include CREB phosphorylation.; ",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/tests/custom_tests/data_fixtures/raw/9171236.txt:127,depend,dependent,127,tests/custom_tests/data_fixtures/raw/9171236.txt,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/tests/custom_tests/data_fixtures/raw/9171236.txt,2,['depend'],['dependent'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Defective survival and activation of thymocytes in transgenic mice expressing a catalytically inactive form of Ca2+/calmodulin-dependent protein kinase IV.; We have generated transgenic mice that express a catalytically inactive form of Ca2+/calmodulin-dependent protein kinase IV (CaMKIV) specifically in thymic T cells.; The presence of this protein results in a markedly reduced thymic cellularity, although the distribution of the remaining cells is normal based on evaluation of the CD4 and CD8 cell surface antigens that are used to gauge T cell development.; Isolated thymic T cells from the transgenic mice also show a dramatically decreased survival rate when evaluated in culture under conditions that do not favor activation.; When challenged with an activating stimulus such as alpha-CD3 or a combination of phorbol ester plus ionophore, the cells are severely compromised in their ability to produce the cytokine interleukin-2 (IL-2).; Reduction of IL-2 production is secondary to the inability to phosphorylate the cAMP response element binding protein, CREB, and induce expression of the immediate early genes such as Fos B that are required to transactivate the IL-2 promoter.; Because transgene expression was regulated by the proximal promoter of the murine lck gene and this promoter is inactivated in T cells that exit the thymus, the mutant hCaMKIV is not present in peripheral T cells.; Consequently, T lymphocytes present in the spleen can be activated normally in response to either stimulus mentioned above, demonstrating that the effects of the inactive CaMKIV on activation are reversible.; Our results suggest that CaMKIV may represent a physiologically relevant CREB kinase in T cells and that the enzyme is also required to ensure normal expansion of T cells in the thymus.; Whereas the pathway responsible for this latter role is yet to be elucidated, it is unlikely to include CREB phosphorylation.; 
",False,"The text contains complete sentences discussing research findings and implications, including experimental methods and results. It does not consist primarily of code, logs, or programmatic content."
Integrability,"are immature ; myeloid cells with immunosuppressive activity. ; They accumulate in tumor-bearing mice and humans ; with different types of cancer, including hepatocellular ; carcinoma (HCC).; """"""; doc = nlp(text). print(list(doc.sents)); >>> [""Myeloid derived suppressor cells (MDSC) are immature myeloid cells with immunosuppressive activity."", ; ""They accumulate in tumor-bearing mice and humans with different types of cancer, including hepatocellular carcinoma (HCC).""]. # Examine the entities extracted by the mention detector.; # Note that they don't have types like in SpaCy, and they; # are more general (e.g including verbs) - these are any; # spans which might be an entity in UMLS, a large; # biomedical database.; print(doc.ents); >>> (Myeloid derived suppressor cells,; MDSC,; immature,; myeloid cells,; immunosuppressive activity,; accumulate,; tumor-bearing mice,; humans,; cancer,; hepatocellular carcinoma,; HCC). # We can also visualise dependency parses; # (This renders automatically inside a jupyter notebook!):; from spacy import displacy; displacy.render(next(doc.sents), style='dep', jupyter=True). # See below for the generated SVG.; # Zoom your browser in a bit!. ```. ![Branching](./example.svg). ### Data Sources. scispaCy models are trained on data from a variety of sources. In particular,; we use:. * **[The GENIA 1.0 Treebank](https://nlp.stanford.edu/~mcclosky/biomedical.html)**, converted to basic Universal Dependencies using the [Stanford Dependency Converter](https://nlp.stanford.edu/software/stanford-dependencies.shtml).; We have made this [dataset available along with the original raw data](https://github.com/allenai/genia-dependency-trees).; * **[word2vec word vectors](http://bio.nlplab.org/#word-vectors)** trained on the Pubmed Central Open Access Subset.; * **[The MedMentions Entity Linking dataset](https://github.com/chanzuckerberg/MedMentions)**, used for training a mention detector.; * **[Ontonotes 5.0](https://catalog.ldc.upenn.edu/LDC2013T19)*",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md:4263,depend,dependency,4263,docs/index.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md,1,['depend'],['dependency'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: are immature ; myeloid cells with immunosuppressive activity. ; They accumulate in tumor-bearing mice and humans ; with different types of cancer, including hepatocellular ; carcinoma (HCC).; """"""; doc = nlp(text). print(list(doc.sents)); >>> [""Myeloid derived suppressor cells (MDSC) are immature myeloid cells with immunosuppressive activity."", ; ""They accumulate in tumor-bearing mice and humans with different types of cancer, including hepatocellular carcinoma (HCC).""]. # Examine the entities extracted by the mention detector.; # Note that they don't have types like in SpaCy, and they; # are more general (e.g including verbs) - these are any; # spans which might be an entity in UMLS, a large; # biomedical database.; print(doc.ents); >>> (Myeloid derived suppressor cells,; MDSC,; immature,; myeloid cells,; immunosuppressive activity,; accumulate,; tumor-bearing mice,; humans,; cancer,; hepatocellular carcinoma,; HCC). # We can also visualise dependency parses; # (This renders automatically inside a jupyter notebook!):; from spacy import displacy; displacy.render(next(doc.sents), style='dep', jupyter=True). # See below for the generated SVG.; # Zoom your browser in a bit!. ```. ![Branching](./example.svg). ### Data Sources. scispaCy models are trained on data from a variety of sources. In particular,; we use:. * **[The GENIA 1.0 Treebank](https://nlp.stanford.edu/~mcclosky/biomedical.html)**, converted to basic Universal Dependencies using the [Stanford Dependency Converter](https://nlp.stanford.edu/software/stanford-dependencies.shtml).; We have made this [dataset available along with the original raw data](https://github.com/allenai/genia-dependency-trees).; * **[word2vec word vectors](http://bio.nlplab.org/#word-vectors)** trained on the Pubmed Central Open Access Subset.; * **[The MedMentions Entity Linking dataset](https://github.com/chanzuckerberg/MedMentions)**, used for training a mention detector.; * **[Ontonotes 5.0](https://catalog.ldc.upenn.edu/LDC2013T19)*
",False,"The text contains complete sentences discussing biological concepts and processes, such as the role of myeloid derived suppressor cells in cancer. It includes explanations about data sources used for model training, which are relevant to understanding the context but do not fall under code, logs, or API documentation. The mention detector extracts entities like 'myeloid derived suppressor cells' and 'hepatocellular carcinoma,' indicating meaningful biological terms. There are no syntax elements typical of code samples or programmatic descriptions."
Integrability,"ith different types of cancer, including hepatocellular ; carcinoma (HCC).; """"""; doc = nlp(text). print(list(doc.sents)); >>> [""Myeloid derived suppressor cells (MDSC) are immature myeloid cells with immunosuppressive activity."", ; ""They accumulate in tumor-bearing mice and humans with different types of cancer, including hepatocellular carcinoma (HCC).""]. # Examine the entities extracted by the mention detector.; # Note that they don't have types like in SpaCy, and they; # are more general (e.g including verbs) - these are any; # spans which might be an entity in UMLS, a large; # biomedical database.; print(doc.ents); >>> (Myeloid derived suppressor cells,; MDSC,; immature,; myeloid cells,; immunosuppressive activity,; accumulate,; tumor-bearing mice,; humans,; cancer,; hepatocellular carcinoma,; HCC). # We can also visualise dependency parses; # (This renders automatically inside a jupyter notebook!):; from spacy import displacy; displacy.render(next(doc.sents), style='dep', jupyter=True). # See below for the generated SVG.; # Zoom your browser in a bit!. ```. ![Branching](./example.svg). ### Data Sources. scispaCy models are trained on data from a variety of sources. In particular,; we use:. * **[The GENIA 1.0 Treebank](https://nlp.stanford.edu/~mcclosky/biomedical.html)**, converted to basic Universal Dependencies using the [Stanford Dependency Converter](https://nlp.stanford.edu/software/stanford-dependencies.shtml).; We have made this [dataset available along with the original raw data](https://github.com/allenai/genia-dependency-trees).; * **[word2vec word vectors](http://bio.nlplab.org/#word-vectors)** trained on the Pubmed Central Open Access Subset.; * **[The MedMentions Entity Linking dataset](https://github.com/chanzuckerberg/MedMentions)**, used for training a mention detector.; * **[Ontonotes 5.0](https://catalog.ldc.upenn.edu/LDC2013T19)** to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publicly available.; ",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md:4849,depend,dependencies,4849,docs/index.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md,2,['depend'],"['dependencies', 'dependency-trees']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ith different types of cancer, including hepatocellular ; carcinoma (HCC).; """"""; doc = nlp(text). print(list(doc.sents)); >>> [""Myeloid derived suppressor cells (MDSC) are immature myeloid cells with immunosuppressive activity."", ; ""They accumulate in tumor-bearing mice and humans with different types of cancer, including hepatocellular carcinoma (HCC).""]. # Examine the entities extracted by the mention detector.; # Note that they don't have types like in SpaCy, and they; # are more general (e.g including verbs) - these are any; # spans which might be an entity in UMLS, a large; # biomedical database.; print(doc.ents); >>> (Myeloid derived suppressor cells,; MDSC,; immature,; myeloid cells,; immunosuppressive activity,; accumulate,; tumor-bearing mice,; humans,; cancer,; hepatocellular carcinoma,; HCC). # We can also visualise dependency parses; # (This renders automatically inside a jupyter notebook!):; from spacy import displacy; displacy.render(next(doc.sents), style='dep', jupyter=True). # See below for the generated SVG.; # Zoom your browser in a bit!. ```. ![Branching](./example.svg). ### Data Sources. scispaCy models are trained on data from a variety of sources. In particular,; we use:. * **[The GENIA 1.0 Treebank](https://nlp.stanford.edu/~mcclosky/biomedical.html)**, converted to basic Universal Dependencies using the [Stanford Dependency Converter](https://nlp.stanford.edu/software/stanford-dependencies.shtml).; We have made this [dataset available along with the original raw data](https://github.com/allenai/genia-dependency-trees).; * **[word2vec word vectors](http://bio.nlplab.org/#word-vectors)** trained on the Pubmed Central Open Access Subset.; * **[The MedMentions Entity Linking dataset](https://github.com/chanzuckerberg/MedMentions)**, used for training a mention detector.; * **[Ontonotes 5.0](https://catalog.ldc.upenn.edu/LDC2013T19)** to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publicly available.; 
",False,
Integrability,"m matching).; linker = nlp.get_pipe(""scispacy_linker""); for umls_ent in entity._.kb_ents:; 	print(linker.kb.cui_to_entity[umls_ent[0]]). >>> CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked; >>> Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the; 				gene encoding the ANDROGEN RECEPTOR.; >>> TUI(s): T047; >>> Aliases (abbreviated, total: 50):; Bulbo-Spinal Atrophy, X-Linked, Bulbo-Spinal Atrophy, X-Linked, .... >>> CUI: C0541794, Name: Skeletal muscle atrophy; >>> Definition: A process, occurring in skeletal muscle, that is characterized by a decrease in protein content,; fiber diameter, force production and fatigue resistance in response to ...; >>> TUI(s): T046; >>> Aliases: (total: 9):; Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, .... >>> CUI: C1447749, Name: AR protein, human; >>> Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene.; This protein plays a role in the modulation of steroid-dependent gene transcription.; >>> TUI(s): T116, T192; >>> Aliases (abbreviated, total: 16):; AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, ...; ```. ### Hearst Patterns (v0.3.0 and up). This component implements [Automatic Aquisition of Hyponyms from Large Text Corpora](https://www.aclweb.org/anthology/C92-2082.pdf) using the SpaCy Matcher component. Passing `extended=True` to the `HyponymDetector` will use the extended set of hearst patterns, which include higher recall but lower precision hyponymy relations (e.g X compared to Y, X similar to Y, etc). This component produces a doc level attribute on the spacy doc: `doc._.hearst_patterns`, which is a list containing tuples of extracted hyponym pairs. The tuples contain:. - The relation rule used to extract the hyponym (type: `str`); - The more general concept (type: `spacy.Span`); - The more specific concept (type: `spacy.Span`). #### Usage:. ```python; import spacy; from scispacy.h",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:13465,depend,dependent,13465,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['depend'],['dependent'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: m matching).; linker = nlp.get_pipe(""scispacy_linker""); for umls_ent in entity._.kb_ents:; 	print(linker.kb.cui_to_entity[umls_ent[0]]). >>> CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked; >>> Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the; 				gene encoding the ANDROGEN RECEPTOR.; >>> TUI(s): T047; >>> Aliases (abbreviated, total: 50):; Bulbo-Spinal Atrophy, X-Linked, Bulbo-Spinal Atrophy, X-Linked, .... >>> CUI: C0541794, Name: Skeletal muscle atrophy; >>> Definition: A process, occurring in skeletal muscle, that is characterized by a decrease in protein content,; fiber diameter, force production and fatigue resistance in response to ...; >>> TUI(s): T046; >>> Aliases: (total: 9):; Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, .... >>> CUI: C1447749, Name: AR protein, human; >>> Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene.; This protein plays a role in the modulation of steroid-dependent gene transcription.; >>> TUI(s): T116, T192; >>> Aliases (abbreviated, total: 16):; AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, ...; ```. ### Hearst Patterns (v0.3.0 and up). This component implements [Automatic Aquisition of Hyponyms from Large Text Corpora](https://www.aclweb.org/anthology/C92-2082.pdf) using the SpaCy Matcher component. Passing `extended=True` to the `HyponymDetector` will use the extended set of hearst patterns, which include higher recall but lower precision hyponymy relations (e.g X compared to Y, X similar to Y, etc). This component produces a doc level attribute on the spacy doc: `doc._.hearst_patterns`, which is a list containing tuples of extracted hyponym pairs. The tuples contain:. - The relation rule used to extract the hyponym (type: `str`); - The more general concept (type: `spacy.Span`); - The more specific concept (type: `spacy.Span`). #### Usage:. ```python; import spacy; from scispacy.h
",False,"The text contains sentences in natural language discussing concepts and providing explanations about entities like 'Bulbo-Spinal Atrophy, X-Linked' and 'AR protein'. It includes definitions, aliases, and relationships, which are meaningful and explanatory content. Additionally, it references SpaCy components related to hearst patterns, which are part of the NLP pipeline configuration but do not indicate code or logs. The text is more descriptive and explanatory rather than being code snippets or program outputs."
Modifiability," Example Usage; ```python; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 Span	 Definition; >>> SBMA 		 (33, 34) Spinal and bulbar muscular atrophy; >>> SBMA 	 	 (6, 7) Spinal and bulbar muscular atrophy; >>> AR 		 (29, 30) androgen receptor; ```. > **Note**; > If you want to be able to [serialize your `doc` objects](https://spacy.io/usage/saving-loading), load the abbreviation detector with `make_serializable=True`, e.g. `nlp.add_pipe(""abbreviation_detector"", config={""make_serializable"": True})`. ### EntityLinker. The `EntityLinker` is a SpaCy component which performs linking to a knowledge base. The linker simply performs; a string overlap - based search (char-3grams) on named entities, comparing them with the concepts in a knowledge base; using an approximate nearest neighbours search. Currently (v2.5.0), there are 5 supported linkers:. - `umls`: Links to the [Unified Medical Language System](https://www.nlm.nih.gov/research/umls/index.html), levels 0,1,2 and 9. This has ~3M concepts.; - `mesh`: Links to the [Medical Subject Headings](https://www.nlm.nih.gov/mesh/meshhome.html). This contains a smaller set of higher quality entities, which are used for indexing in Pubmed. MeSH contains ~30k entities. NOTE: The MeSH KB is derived directly from MeSH itself, and as such uses different unique identifiers than the other KBs.; - `rxnorm`: Links to the [RxNorm](https://www.nlm.nih.gov/research/umls/rxnorm/index.html) ontology. RxNorm contains ~100",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:8063,config,config,8063,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['config'],['config'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  Example Usage; ```python; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 Span	 Definition; >>> SBMA 		 (33, 34) Spinal and bulbar muscular atrophy; >>> SBMA 	 	 (6, 7) Spinal and bulbar muscular atrophy; >>> AR 		 (29, 30) androgen receptor; ```. > **Note**; > If you want to be able to [serialize your `doc` objects](https://spacy.io/usage/saving-loading), load the abbreviation detector with `make_serializable=True`, e.g. `nlp.add_pipe(""abbreviation_detector"", config={""make_serializable"": True})`. ### EntityLinker. The `EntityLinker` is a SpaCy component which performs linking to a knowledge base. The linker simply performs; a string overlap - based search (char-3grams) on named entities, comparing them with the concepts in a knowledge base; using an approximate nearest neighbours search. Currently (v2.5.0), there are 5 supported linkers:. - `umls`: Links to the [Unified Medical Language System](https://www.nlm.nih.gov/research/umls/index.html), levels 0,1,2 and 9. This has ~3M concepts.; - `mesh`: Links to the [Medical Subject Headings](https://www.nlm.nih.gov/mesh/meshhome.html). This contains a smaller set of higher quality entities, which are used for indexing in Pubmed. MeSH contains ~30k entities. NOTE: The MeSH KB is derived directly from MeSH itself, and as such uses different unique identifiers than the other KBs.; - `rxnorm`: Links to the [RxNorm](https://www.nlm.nih.gov/research/umls/rxnorm/index.html) ontology. RxNorm contains ~100
",False,"The provided text contains complete sentences and explanatory content discussing the usage of SpaCy components like EntityLinker and its linkers such as umls and mesh. It includes instructions on how to add pipes and configurations, which are part of the programmatic setup, but also contains human-readable explanations about the functionality of these components. The text is not solely code or logs; it includes both configuration steps and descriptive information, making it meaningful for human readers interested in SpaCy's functionalities."
Modifiability," Skeletal muscle atrophy; >>> Definition: A process, occurring in skeletal muscle, that is characterized by a decrease in protein content,; fiber diameter, force production and fatigue resistance in response to ...; >>> TUI(s): T046; >>> Aliases: (total: 9):; Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, .... >>> CUI: C1447749, Name: AR protein, human; >>> Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene.; This protein plays a role in the modulation of steroid-dependent gene transcription.; >>> TUI(s): T116, T192; >>> Aliases (abbreviated, total: 16):; AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, ...; ```. ### Hearst Patterns (v0.3.0 and up). This component implements [Automatic Aquisition of Hyponyms from Large Text Corpora](https://www.aclweb.org/anthology/C92-2082.pdf) using the SpaCy Matcher component. Passing `extended=True` to the `HyponymDetector` will use the extended set of hearst patterns, which include higher recall but lower precision hyponymy relations (e.g X compared to Y, X similar to Y, etc). This component produces a doc level attribute on the spacy doc: `doc._.hearst_patterns`, which is a list containing tuples of extracted hyponym pairs. The tuples contain:. - The relation rule used to extract the hyponym (type: `str`); - The more general concept (type: `spacy.Span`); - The more specific concept (type: `spacy.Span`). #### Usage:. ```python; import spacy; from scispacy.hyponym_detector import HyponymDetector. nlp = spacy.load(""en_core_sci_sm""); nlp.add_pipe(""hyponym_detector"", last=True, config={""extended"": False}). doc = nlp(""Keystone plant species such as fig trees are good for the soil.""). print(doc._.hearst_patterns); >>> [('such_as', Keystone plant species, fig trees)]; ```. ## Citing. If you use ScispaCy in your research, please cite [ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing](https://www.semanticscholar.org/pa",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:13867,extend,extended,13867,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,2,['extend'],['extended'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  Skeletal muscle atrophy; >>> Definition: A process, occurring in skeletal muscle, that is characterized by a decrease in protein content,; fiber diameter, force production and fatigue resistance in response to ...; >>> TUI(s): T046; >>> Aliases: (total: 9):; Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, .... >>> CUI: C1447749, Name: AR protein, human; >>> Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene.; This protein plays a role in the modulation of steroid-dependent gene transcription.; >>> TUI(s): T116, T192; >>> Aliases (abbreviated, total: 16):; AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, ...; ```. ### Hearst Patterns (v0.3.0 and up). This component implements [Automatic Aquisition of Hyponyms from Large Text Corpora](https://www.aclweb.org/anthology/C92-2082.pdf) using the SpaCy Matcher component. Passing `extended=True` to the `HyponymDetector` will use the extended set of hearst patterns, which include higher recall but lower precision hyponymy relations (e.g X compared to Y, X similar to Y, etc). This component produces a doc level attribute on the spacy doc: `doc._.hearst_patterns`, which is a list containing tuples of extracted hyponym pairs. The tuples contain:. - The relation rule used to extract the hyponym (type: `str`); - The more general concept (type: `spacy.Span`); - The more specific concept (type: `spacy.Span`). #### Usage:. ```python; import spacy; from scispacy.hyponym_detector import HyponymDetector. nlp = spacy.load(""en_core_sci_sm""); nlp.add_pipe(""hyponym_detector"", last=True, config={""extended"": False}). doc = nlp(""Keystone plant species such as fig trees are good for the soil.""). print(doc._.hearst_patterns); >>> [('such_as', Keystone plant species, fig trees)]; ```. ## Citing. If you use ScispaCy in your research, please cite [ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing](https://www.semanticscholar.org/pa
",False,"The content includes sentences that are complete and meaningful, discussing biological concepts like skeletal muscle atrophy and the androgen receptor. It also contains usage examples for a specific NLP component, which provides explanatory information suitable for human readers."
Modifiability," more information for a given id using the kb attribute of this class:; ```; print(linker.kb.cui_to_entity[concept_id]); ```. #### Example Usage; ```python; import spacy; import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_sm""). # This line takes a while, because we have to download ~1GB of data; # and load a large JSON file (the knowledge base). Be patient!; # Thankfully it should be faster after the first time you use it, because; # the downloads are cached.; # NOTE: The resolve_abbreviations parameter is optional, and requires that; # the AbbreviationDetector pipe has already been added to the pipeline. Adding; # the AbbreviationDetector pipe and setting resolve_abbreviations to True means; # that linking will only be performed on the long form of abbreviations.; nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). # Let's look at a random entity!; entity = doc.ents[1]. print(""Name: "", entity); >>> Name: bulbar muscular atrophy. # Each entity is linked to UMLS with a score; # (currently just char-3gram matching).; linker = nlp.get_pipe(""scispacy_linker""); for umls_ent in entity._.kb_ents:; 	print(linker.kb.cui_to_entity[umls_ent[0]]). >>> CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked; >>> Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the; 				gene encoding the ANDROGEN RECEPTOR.; >>> TUI(s): T047; >>> Aliases (abbreviated, total: 50):; Bulbo-Spinal Atrophy, X-Linked, Bulbo-Spinal Atrophy, X-Linked, .... >>> CUI: C0541794, Name: Skeletal muscle atrophy; >>> Definition: A process, occurring in skeletal muscle, that is characterized by a decrease in protein content,; fiber diameter, force production and fatigue resis",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:12103,inherit,inherited,12103,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['inherit'],['inherited'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  more information for a given id using the kb attribute of this class:; ```; print(linker.kb.cui_to_entity[concept_id]); ```. #### Example Usage; ```python; import spacy; import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_sm""). # This line takes a while, because we have to download ~1GB of data; # and load a large JSON file (the knowledge base). Be patient!; # Thankfully it should be faster after the first time you use it, because; # the downloads are cached.; # NOTE: The resolve_abbreviations parameter is optional, and requires that; # the AbbreviationDetector pipe has already been added to the pipeline. Adding; # the AbbreviationDetector pipe and setting resolve_abbreviations to True means; # that linking will only be performed on the long form of abbreviations.; nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). # Let's look at a random entity!; entity = doc.ents[1]. print(""Name: "", entity); >>> Name: bulbar muscular atrophy. # Each entity is linked to UMLS with a score; # (currently just char-3gram matching).; linker = nlp.get_pipe(""scispacy_linker""); for umls_ent in entity._.kb_ents:; 	print(linker.kb.cui_to_entity[umls_ent[0]]). >>> CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked; >>> Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the; 				gene encoding the ANDROGEN RECEPTOR.; >>> TUI(s): T047; >>> Aliases (abbreviated, total: 50):; Bulbo-Spinal Atrophy, X-Linked, Bulbo-Spinal Atrophy, X-Linked, .... >>> CUI: C0541794, Name: Skeletal muscle atrophy; >>> Definition: A process, occurring in skeletal muscle, that is characterized by a decrease in protein content,; fiber diameter, force production and fatigue resis
",False,"The text contains complete sentences discussing concepts like spinal muscular atrophy, its causes, and related entities. It also includes code examples for entity linking which could be considered programmatic but also provides explanatory text about the usage of the code."
Modifiability,"7	UMLS:C0085262; 25847295	766	776	PC12 cells	T017	UMLS:C0085262; 25847295	820	832	cytochrome c	T103	UMLS:C0010749; 25847295	850	857	cytosol	T017	UMLS:C1383501; 25847295	891	906	apoptotic cells	T017	UMLS:C0007634; 25847295	965	968	Bax	T103	UMLS:C0219474; 25847295	981	996	apoptotic cells	T017	UMLS:C0007634; 25847295	1112	1121	apoptosis	T038	UMLS:C0162638; 25847295	1125	1135	PC12 cells	T017	UMLS:C0085262; 25847295	1176	1185	apoptosis	T038	UMLS:C0162638; 25847295	1197	1202	serum	T031	UMLS:C0229671; 25847295	1234	1243	structure	T082	UMLS:C0678594; 25847295	1255	1264	compounds	T103	UMLS:C0220806; 25847295	1326	1333	results	T033	UMLS:C2825142; 25847295	1399	1419	cell differentiation	T038	UMLS:C0007589; 25847295	1424	1435	development	T038	UMLS:C0243107; 25847295	1456	1470	harmful effect	T037	UMLS:C0600688; 25847295	1520	1529	apoptosis	T038	UMLS:C0162638. 26316050|t|Prevascularized silicon membranes for the enhancement of transport to implanted medical devices; 26316050|a|Recent advances in drug delivery and sensing devices for in situ applications are limited by the diffusion -limiting foreign body response of fibrous encapsulation. In this study, we fabricated prevascularized synthetic device ports to help mitigate this limitation. Membranes with rectilinear arrays of square pores with widths ranging from 40 to 200 Œºm were created using materials (50 Œºm thick double-sided polished silicon) and processes (photolithography and directed reactive ion etching) common in the manufacturing of microfabricated sensors. Vascular endothelial cells responded to membrane geometry by either forming vascular tubes that extended through the pore or completely filling membrane pores after 4 days in culture. Although tube formation began to predominate overgrowth around 75 Œºm and continued to increase at even larger pore sizes, tubes formed at these large pore sizes were not completely round and had relatively thin walls. Thus, the optimum range of pore size for prevascularization of these ",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/tests/fixtures/med_mentions.txt:8315,enhance,enhancement,8315,tests/fixtures/med_mentions.txt,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/tests/fixtures/med_mentions.txt,1,['enhance'],['enhancement'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: 7	UMLS:C0085262; 25847295	766	776	PC12 cells	T017	UMLS:C0085262; 25847295	820	832	cytochrome c	T103	UMLS:C0010749; 25847295	850	857	cytosol	T017	UMLS:C1383501; 25847295	891	906	apoptotic cells	T017	UMLS:C0007634; 25847295	965	968	Bax	T103	UMLS:C0219474; 25847295	981	996	apoptotic cells	T017	UMLS:C0007634; 25847295	1112	1121	apoptosis	T038	UMLS:C0162638; 25847295	1125	1135	PC12 cells	T017	UMLS:C0085262; 25847295	1176	1185	apoptosis	T038	UMLS:C0162638; 25847295	1197	1202	serum	T031	UMLS:C0229671; 25847295	1234	1243	structure	T082	UMLS:C0678594; 25847295	1255	1264	compounds	T103	UMLS:C0220806; 25847295	1326	1333	results	T033	UMLS:C2825142; 25847295	1399	1419	cell differentiation	T038	UMLS:C0007589; 25847295	1424	1435	development	T038	UMLS:C0243107; 25847295	1456	1470	harmful effect	T037	UMLS:C0600688; 25847295	1520	1529	apoptosis	T038	UMLS:C0162638. 26316050|t|Prevascularized silicon membranes for the enhancement of transport to implanted medical devices; 26316050|a|Recent advances in drug delivery and sensing devices for in situ applications are limited by the diffusion -limiting foreign body response of fibrous encapsulation. In this study, we fabricated prevascularized synthetic device ports to help mitigate this limitation. Membranes with rectilinear arrays of square pores with widths ranging from 40 to 200 Œºm were created using materials (50 Œºm thick double-sided polished silicon) and processes (photolithography and directed reactive ion etching) common in the manufacturing of microfabricated sensors. Vascular endothelial cells responded to membrane geometry by either forming vascular tubes that extended through the pore or completely filling membrane pores after 4 days in culture. Although tube formation began to predominate overgrowth around 75 Œºm and continued to increase at even larger pore sizes, tubes formed at these large pore sizes were not completely round and had relatively thin walls. Thus, the optimum range of pore size for prevascularization of these 
",False,"The text includes a detailed description of a study on prevascularized silicon membranes, including methods used in their fabrication and cell responses, which are meaningful human-readable sentences."
Modifiability,"ins a smaller set of higher quality entities, which are used for indexing in Pubmed. MeSH contains ~30k entities. NOTE: The MeSH KB is derived directly from MeSH itself, and as such uses different unique identifiers than the other KBs.; - `rxnorm`: Links to the [RxNorm](https://www.nlm.nih.gov/research/umls/rxnorm/index.html) ontology. RxNorm contains ~100k concepts focused on normalized names for clinical drugs. It is comprised of several other drug vocabularies commonly used in pharmacy management and drug interaction, including First Databank, Micromedex, and the Gold Standard Drug Database.; - `go`: Links to the [Gene Ontology](http://geneontology.org/). The Gene Ontology contains ~67k concepts focused on the functions of genes.; - `hpo`: Links to the [Human Phenotype Ontology](https://hpo.jax.org/app/). The Human Phenotype Ontology contains 16k concepts focused on phenotypic abnormalities encountered in human disease. You may want to play around with some of the parameters; below to adapt to your use case (higher precision, higher recall etc). - `resolve_abbreviations : bool = True, optional (default = False)`; Whether to resolve abbreviations identified in the Doc before performing linking.; This parameter has no effect if there is no `AbbreviationDetector` in the spacy; pipeline.; - `k : int, optional, (default = 30)`; The number of nearest neighbours to look up from the candidate generator per mention.; - `threshold : float, optional, (default = 0.7)`; The threshold that a mention candidate must reach to be added to the mention in the Doc; as a mention candidate.; - `no_definition_threshold : float, optional, (default = 0.95)`; The threshold that a entity candidate must reach to be added to the mention in the Doc; as a mention candidate if the entity candidate does not have a definition.; - `filter_for_definitions: bool, default = True`; Whether to filter entities that can be returned to only include those with definitions; in the knowledge base.; - `max_ent",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:9710,adapt,adapt,9710,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['adapt'],['adapt'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ins a smaller set of higher quality entities, which are used for indexing in Pubmed. MeSH contains ~30k entities. NOTE: The MeSH KB is derived directly from MeSH itself, and as such uses different unique identifiers than the other KBs.; - `rxnorm`: Links to the [RxNorm](https://www.nlm.nih.gov/research/umls/rxnorm/index.html) ontology. RxNorm contains ~100k concepts focused on normalized names for clinical drugs. It is comprised of several other drug vocabularies commonly used in pharmacy management and drug interaction, including First Databank, Micromedex, and the Gold Standard Drug Database.; - `go`: Links to the [Gene Ontology](http://geneontology.org/). The Gene Ontology contains ~67k concepts focused on the functions of genes.; - `hpo`: Links to the [Human Phenotype Ontology](https://hpo.jax.org/app/). The Human Phenotype Ontology contains 16k concepts focused on phenotypic abnormalities encountered in human disease. You may want to play around with some of the parameters; below to adapt to your use case (higher precision, higher recall etc). - `resolve_abbreviations : bool = True, optional (default = False)`; Whether to resolve abbreviations identified in the Doc before performing linking.; This parameter has no effect if there is no `AbbreviationDetector` in the spacy; pipeline.; - `k : int, optional, (default = 30)`; The number of nearest neighbours to look up from the candidate generator per mention.; - `threshold : float, optional, (default = 0.7)`; The threshold that a mention candidate must reach to be added to the mention in the Doc; as a mention candidate.; - `no_definition_threshold : float, optional, (default = 0.95)`; The threshold that a entity candidate must reach to be added to the mention in the Doc; as a mention candidate if the entity candidate does not have a definition.; - `filter_for_definitions: bool, default = True`; Whether to filter entities that can be returned to only include those with definitions; in the knowledge base.; - `max_ent
",False,"The content contains complete sentences discussing the characteristics of different ontologies and their concepts, which are meaningful and not code or logs."
Modifiability,"model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Additional Pipeline Components. ### AbbreviationDetector; The AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm; for identifying abbreviation definitions in biomedical text."", (Schwartz & Hearst, 2003). You can access the list of abbreviations via the `doc._.abbreviations` attribute and for a given abbreviation,; you can access it's long form (which is a `spacy.tokens.Span`) using `span._.long_form`, which will point to; another span in the document. #### Example Usage; ```python; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 Span	 Definition; >>> SBMA 		 (33, 34) Spinal and bulbar muscular atrophy; >>> SBMA 	 	 (6, 7) Spinal and bulbar muscular atrophy; >>> AR 		 (29, 30) androgen receptor; ```. > **Note**; > If you want to be able to [serialize your `doc` objects](https://spacy.io/usage/saving-loading), load the abbreviation detector with `make_serializable=True`, e.g. `nlp.add_pipe(""abbreviation_detector"", config={""make_serializable"": True})`. ### EntityLinker. The `EntityLinker` is a SpaCy component which performs linking to a knowledge base. The linker simply performs; a string overlap - based search (char-3grams) on named entities, comparing them with the concepts in a knowledge base; using an approximate neares",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:7350,inherit,inherited,7350,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['inherit'],['inherited'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Additional Pipeline Components. ### AbbreviationDetector; The AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm; for identifying abbreviation definitions in biomedical text."", (Schwartz & Hearst, 2003). You can access the list of abbreviations via the `doc._.abbreviations` attribute and for a given abbreviation,; you can access it's long form (which is a `spacy.tokens.Span`) using `span._.long_form`, which will point to; another span in the document. #### Example Usage; ```python; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 Span	 Definition; >>> SBMA 		 (33, 34) Spinal and bulbar muscular atrophy; >>> SBMA 	 	 (6, 7) Spinal and bulbar muscular atrophy; >>> AR 		 (29, 30) androgen receptor; ```. > **Note**; > If you want to be able to [serialize your `doc` objects](https://spacy.io/usage/saving-loading), load the abbreviation detector with `make_serializable=True`, e.g. `nlp.add_pipe(""abbreviation_detector"", config={""make_serializable"": True})`. ### EntityLinker. The `EntityLinker` is a SpaCy component which performs linking to a knowledge base. The linker simply performs; a string overlap - based search (char-3grams) on named entities, comparing them with the concepts in a knowledge base; using an approximate neares
",True,"The text consists of code examples and API documentation, including syntax-specific language like 'python' imports, pipeline configurations, and entity linker details."
Modifiability,"n. In this study, we fabricated prevascularized synthetic device ports to help mitigate this limitation. Membranes with rectilinear arrays of square pores with widths ranging from 40 to 200 Œºm were created using materials (50 Œºm thick double-sided polished silicon) and processes (photolithography and directed reactive ion etching) common in the manufacturing of microfabricated sensors. Vascular endothelial cells responded to membrane geometry by either forming vascular tubes that extended through the pore or completely filling membrane pores after 4 days in culture. Although tube formation began to predominate overgrowth around 75 Œºm and continued to increase at even larger pore sizes, tubes formed at these large pore sizes were not completely round and had relatively thin walls. Thus, the optimum range of pore size for prevascularization of these membranes was estimated to be 75-100 Œºm. This study lays the foundation for creating a prevascularized port that can be used to reduce fibrous encapsulation and thus enhance diffusion to implanted medical devices and sensors. ¬© 2015 Wiley Periodicals, Inc. J Biomed Mater Res Part B: Appl Biomater, 104B: 1602-1609, 2016.; 26316050	16	23	silicon	T103	UMLS:C0037114; 26316050	70	95	implanted medical devices	T033	UMLS:C2828363; 26316050	115	128	drug delivery	T074	UMLS:C0085104; 26316050	153	160	in situ	T082	UMLS:C0444498; 26316050	161	173	applications	T058	UMLS:C0185125; 26316050	213	234	foreign body response	T033	UMLS:C1708386; 26316050	400	406	square	T082	UMLS:C0205120; 26316050	506	522	polished silicon	T103	UMLS:C0037114; 26316050	647	673	Vascular endothelial cells	T017	UMLS:C1257792; 26316050	723	737	vascular tubes	T017	UMLS:C0005847; 26316050	743	751	extended	T082	UMLS:C0231449; 26316050	876	886	overgrowth	T033	UMLS:C1849265; 26316050	1012	1017	round	T082	UMLS:C0332490; 26316050	1042	1047	walls	T082	UMLS:C0442069; 26316050	1164	1169	study	T062	UMLS:C2603343; 26316050	1305	1330	implanted medical devices	T033	UMLS:C2828363; ",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/tests/fixtures/med_mentions.txt:10266,extend,extended,10266,tests/fixtures/med_mentions.txt,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/tests/fixtures/med_mentions.txt,1,['extend'],['extended'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: n. In this study, we fabricated prevascularized synthetic device ports to help mitigate this limitation. Membranes with rectilinear arrays of square pores with widths ranging from 40 to 200 Œºm were created using materials (50 Œºm thick double-sided polished silicon) and processes (photolithography and directed reactive ion etching) common in the manufacturing of microfabricated sensors. Vascular endothelial cells responded to membrane geometry by either forming vascular tubes that extended through the pore or completely filling membrane pores after 4 days in culture. Although tube formation began to predominate overgrowth around 75 Œºm and continued to increase at even larger pore sizes, tubes formed at these large pore sizes were not completely round and had relatively thin walls. Thus, the optimum range of pore size for prevascularization of these membranes was estimated to be 75-100 Œºm. This study lays the foundation for creating a prevascularized port that can be used to reduce fibrous encapsulation and thus enhance diffusion to implanted medical devices and sensors. ¬© 2015 Wiley Periodicals, Inc. J Biomed Mater Res Part B: Appl Biomater, 104B: 1602-1609, 2016.; 26316050	16	23	silicon	T103	UMLS:C0037114; 26316050	70	95	implanted medical devices	T033	UMLS:C2828363; 26316050	115	128	drug delivery	T074	UMLS:C0085104; 26316050	153	160	in situ	T082	UMLS:C0444498; 26316050	161	173	applications	T058	UMLS:C0185125; 26316050	213	234	foreign body response	T033	UMLS:C1708386; 26316050	400	406	square	T082	UMLS:C0205120; 26316050	506	522	polished silicon	T103	UMLS:C0037114; 26316050	647	673	Vascular endothelial cells	T017	UMLS:C1257792; 26316050	723	737	vascular tubes	T017	UMLS:C0005847; 26316050	743	751	extended	T082	UMLS:C0231449; 26316050	876	886	overgrowth	T033	UMLS:C1849265; 26316050	1012	1017	round	T082	UMLS:C0332490; 26316050	1042	1047	walls	T082	UMLS:C0442069; 26316050	1164	1169	study	T062	UMLS:C2603343; 26316050	1305	1330	implanted medical devices	T033	UMLS:C2828363; 
",True,"The provided text is a research paper abstract. It contains detailed technical descriptions of the methodology, results, and conclusions related to the creation of synthetic device ports using microfabrication techniques. While this content is highly technical, it does not meet the criteria for elimination as it is written in natural language and provides meaningful explanations of the research process. However, if it were presented as code snippets, logs, API documentation, or other programmatic content, it would be considered for elimination."
Modifiability,"orresponding to the KB concept_id and the associated score; for a list of `max_entities_per_mention` number of entities. You can look up more information for a given id using the kb attribute of this class:; ```; print(linker.kb.cui_to_entity[concept_id]); ```. #### Example Usage; ```python; import spacy; import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_sm""). # This line takes a while, because we have to download ~1GB of data; # and load a large JSON file (the knowledge base). Be patient!; # Thankfully it should be faster after the first time you use it, because; # the downloads are cached.; # NOTE: The resolve_abbreviations parameter is optional, and requires that; # the AbbreviationDetector pipe has already been added to the pipeline. Adding; # the AbbreviationDetector pipe and setting resolve_abbreviations to True means; # that linking will only be performed on the long form of abbreviations.; nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). # Let's look at a random entity!; entity = doc.ents[1]. print(""Name: "", entity); >>> Name: bulbar muscular atrophy. # Each entity is linked to UMLS with a score; # (currently just char-3gram matching).; linker = nlp.get_pipe(""scispacy_linker""); for umls_ent in entity._.kb_ents:; 	print(linker.kb.cui_to_entity[umls_ent[0]]). >>> CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked; >>> Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the; 				gene encoding the ANDROGEN RECEPTOR.; >>> TUI(s): T047; >>> Aliases (abbreviated, total: 50):; Bulbo-Spinal Atrophy, X-Linked, Bulbo-Spinal Atrophy, X-Linked, .... >>> CUI: C0541794, Name: Skeletal muscle atrophy; >>> Definition: A process, oc",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:11977,config,config,11977,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['config'],['config'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: orresponding to the KB concept_id and the associated score; for a list of `max_entities_per_mention` number of entities. You can look up more information for a given id using the kb attribute of this class:; ```; print(linker.kb.cui_to_entity[concept_id]); ```. #### Example Usage; ```python; import spacy; import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_sm""). # This line takes a while, because we have to download ~1GB of data; # and load a large JSON file (the knowledge base). Be patient!; # Thankfully it should be faster after the first time you use it, because; # the downloads are cached.; # NOTE: The resolve_abbreviations parameter is optional, and requires that; # the AbbreviationDetector pipe has already been added to the pipeline. Adding; # the AbbreviationDetector pipe and setting resolve_abbreviations to True means; # that linking will only be performed on the long form of abbreviations.; nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). # Let's look at a random entity!; entity = doc.ents[1]. print(""Name: "", entity); >>> Name: bulbar muscular atrophy. # Each entity is linked to UMLS with a score; # (currently just char-3gram matching).; linker = nlp.get_pipe(""scispacy_linker""); for umls_ent in entity._.kb_ents:; 	print(linker.kb.cui_to_entity[umls_ent[0]]). >>> CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked; >>> Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the; 				gene encoding the ANDROGEN RECEPTOR.; >>> TUI(s): T047; >>> Aliases (abbreviated, total: 50):; Bulbo-Spinal Atrophy, X-Linked, Bulbo-Spinal Atrophy, X-Linked, .... >>> CUI: C0541794, Name: Skeletal muscle atrophy; >>> Definition: A process, oc
",False,The text contains complete sentences in natural language discussing the properties of entities like spinal atrophy and includes descriptive content.
Modifiability,"psulation. In this study, we fabricated prevascularized synthetic device ports to help mitigate this limitation. Membranes with rectilinear arrays of square pores with widths ranging from 40 to 200 Œºm were created using materials (50 Œºm thick double-sided polished silicon) and processes (photolithography and directed reactive ion etching) common in the manufacturing of microfabricated sensors. Vascular endothelial cells responded to membrane geometry by either forming vascular tubes that extended through the pore or completely filling membrane pores after 4 days in culture. Although tube formation began to predominate overgrowth around 75 Œºm and continued to increase at even larger pore sizes, tubes formed at these large pore sizes were not completely round and had relatively thin walls. Thus, the optimum range of pore size for prevascularization of these membranes was estimated to be 75-100 Œºm. This study lays the foundation for creating a prevascularized port that can be used to reduce fibrous encapsulation and thus enhance diffusion to implanted medical devices and sensors. ¬© 2015 Wiley Periodicals, Inc. J Biomed Mater Res Part B: Appl Biomater, 104B: 1602-1609, 2016.; 26316050	16	23	silicon	T103	UMLS:C0037114; 26316050	70	95	implanted medical devices	T033	UMLS:C2828363; 26316050	115	128	drug delivery	T074	UMLS:C0085104; 26316050	153	160	in situ	T082	UMLS:C0444498; 26316050	161	173	applications	T058	UMLS:C0185125; 26316050	213	234	foreign body response	T033	UMLS:C1708386; 26316050	400	406	square	T082	UMLS:C0205120; 26316050	506	522	polished silicon	T103	UMLS:C0037114; 26316050	647	673	Vascular endothelial cells	T017	UMLS:C1257792; 26316050	723	737	vascular tubes	T017	UMLS:C0005847; 26316050	743	751	extended	T082	UMLS:C0231449; 26316050	876	886	overgrowth	T033	UMLS:C1849265; 26316050	1012	1017	round	T082	UMLS:C0332490; 26316050	1042	1047	walls	T082	UMLS:C0442069; 26316050	1164	1169	study	T062	UMLS:C2603343; 26316050	1305	1330	implanted medical devices	T033	UMLS:C28",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/tests/fixtures/med_mentions.txt:9569,enhance,enhance,9569,tests/fixtures/med_mentions.txt,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/tests/fixtures/med_mentions.txt,1,['enhance'],['enhance'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: psulation. In this study, we fabricated prevascularized synthetic device ports to help mitigate this limitation. Membranes with rectilinear arrays of square pores with widths ranging from 40 to 200 Œºm were created using materials (50 Œºm thick double-sided polished silicon) and processes (photolithography and directed reactive ion etching) common in the manufacturing of microfabricated sensors. Vascular endothelial cells responded to membrane geometry by either forming vascular tubes that extended through the pore or completely filling membrane pores after 4 days in culture. Although tube formation began to predominate overgrowth around 75 Œºm and continued to increase at even larger pore sizes, tubes formed at these large pore sizes were not completely round and had relatively thin walls. Thus, the optimum range of pore size for prevascularization of these membranes was estimated to be 75-100 Œºm. This study lays the foundation for creating a prevascularized port that can be used to reduce fibrous encapsulation and thus enhance diffusion to implanted medical devices and sensors. ¬© 2015 Wiley Periodicals, Inc. J Biomed Mater Res Part B: Appl Biomater, 104B: 1602-1609, 2016.; 26316050	16	23	silicon	T103	UMLS:C0037114; 26316050	70	95	implanted medical devices	T033	UMLS:C2828363; 26316050	115	128	drug delivery	T074	UMLS:C0085104; 26316050	153	160	in situ	T082	UMLS:C0444498; 26316050	161	173	applications	T058	UMLS:C0185125; 26316050	213	234	foreign body response	T033	UMLS:C1708386; 26316050	400	406	square	T082	UMLS:C0205120; 26316050	506	522	polished silicon	T103	UMLS:C0037114; 26316050	647	673	Vascular endothelial cells	T017	UMLS:C1257792; 26316050	723	737	vascular tubes	T017	UMLS:C0005847; 26316050	743	751	extended	T082	UMLS:C0231449; 26316050	876	886	overgrowth	T033	UMLS:C1849265; 26316050	1012	1017	round	T082	UMLS:C0332490; 26316050	1042	1047	walls	T082	UMLS:C0442069; 26316050	1164	1169	study	T062	UMLS:C2603343; 26316050	1305	1330	implanted medical devices	T033	UMLS:C28
",False,"The text is a detailed, scientific study describing the creation and effects of synthetic device ports, including methods used (photolithography and directed reactive ion etching), cellular responses, and implications for medical applications. It contains complete, meaningful sentences in natural language discussing the process and outcomes. Therefore, it should not be eliminated."
Modifiability,"re present in aquatic environment as wastewater contaminants, and their toxic effects on aquatic species have been reported. Apoptosis has been shown to be induced by serum deprivation or copper treatment. To understand the toxicity of nonylphenol diethoxylate, we investigated the effects of NP2 EO on apoptosis induced by serum deprivation and copper by using PC12 cell system. Nonylphenol diethoxylate itself showed no toxicity and recovered cell viability from apoptosis. In addition, nonylphenol diethoxylate decreased DNA fragmentation caused by apoptosis in PC12 cells. This phenomenon was confirmed after treating apoptotic PC12 cells with nonylphenol diethoxylate, whereas the cytochrome c release into the cytosol decreased as compared to that in apoptotic cells not treated with nonylphenol diethoxylate s. Furthermore, Bax contents in apoptotic cells were reduced after exposure to nonylphenol diethoxylate. Thus, nonylphenol diethoxylate has the opposite effect on apoptosis in PC12 cells compared to nonylphenol, which enhances apoptosis induced by serum deprivation. The difference in structure of the two compounds is hypothesized to be responsible for this phenomenon. These results indicated that nonylphenol diethoxylate has capability to affect cell differentiation and development and has potentially harmful effect on organisms because of its unexpected impact on apoptosis. ¬© 2015 Wiley Periodicals, Inc. Environ Toxicol 31: 1389-1398, 2016.; 25847295	34	43	apoptosis	T038	UMLS:C0162638; 25847295	55	65	PC12 cells	T017	UMLS:C0085262; 25847295	137	144	present	T033	UMLS:C0150312; 25847295	206	219	toxic effects	T037	UMLS:C0600688; 25847295	259	268	Apoptosis	T038	UMLS:C0162638; 25847295	301	306	serum	T031	UMLS:C0229671; 25847295	322	328	copper	T103	UMLS:C0009968; 25847295	437	446	apoptosis	T038	UMLS:C0162638; 25847295	458	463	serum	T031	UMLS:C0229671; 25847295	480	486	copper	T103	UMLS:C0009968; 25847295	496	512	PC12 cell system	T017	UMLS:C0085262; 25847295	579	593	cell via",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/tests/fixtures/med_mentions.txt:6229,enhance,enhances,6229,tests/fixtures/med_mentions.txt,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/tests/fixtures/med_mentions.txt,1,['enhance'],['enhances'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: re present in aquatic environment as wastewater contaminants, and their toxic effects on aquatic species have been reported. Apoptosis has been shown to be induced by serum deprivation or copper treatment. To understand the toxicity of nonylphenol diethoxylate, we investigated the effects of NP2 EO on apoptosis induced by serum deprivation and copper by using PC12 cell system. Nonylphenol diethoxylate itself showed no toxicity and recovered cell viability from apoptosis. In addition, nonylphenol diethoxylate decreased DNA fragmentation caused by apoptosis in PC12 cells. This phenomenon was confirmed after treating apoptotic PC12 cells with nonylphenol diethoxylate, whereas the cytochrome c release into the cytosol decreased as compared to that in apoptotic cells not treated with nonylphenol diethoxylate s. Furthermore, Bax contents in apoptotic cells were reduced after exposure to nonylphenol diethoxylate. Thus, nonylphenol diethoxylate has the opposite effect on apoptosis in PC12 cells compared to nonylphenol, which enhances apoptosis induced by serum deprivation. The difference in structure of the two compounds is hypothesized to be responsible for this phenomenon. These results indicated that nonylphenol diethoxylate has capability to affect cell differentiation and development and has potentially harmful effect on organisms because of its unexpected impact on apoptosis. ¬© 2015 Wiley Periodicals, Inc. Environ Toxicol 31: 1389-1398, 2016.; 25847295	34	43	apoptosis	T038	UMLS:C0162638; 25847295	55	65	PC12 cells	T017	UMLS:C0085262; 25847295	137	144	present	T033	UMLS:C0150312; 25847295	206	219	toxic effects	T037	UMLS:C0600688; 25847295	259	268	Apoptosis	T038	UMLS:C0162638; 25847295	301	306	serum	T031	UMLS:C0229671; 25847295	322	328	copper	T103	UMLS:C0009968; 25847295	437	446	apoptosis	T038	UMLS:C0162638; 25847295	458	463	serum	T031	UMLS:C0229671; 25847295	480	486	copper	T103	UMLS:C0009968; 25847295	496	512	PC12 cell system	T017	UMLS:C0085262; 25847295	579	593	cell via
",False,"The text contains detailed descriptions of scientific studies, including methods and results related to apoptosis in PC12 cells induced by various factors such as serum deprivation and copper treatment. It also discusses the effects of nonylphenol diethoxylate on these processes. The content is written in natural language, explaining concepts in a human-readable format."
Modifiability,"tein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, ...; ```. ### Hearst Patterns (v0.3.0 and up). This component implements [Automatic Aquisition of Hyponyms from Large Text Corpora](https://www.aclweb.org/anthology/C92-2082.pdf) using the SpaCy Matcher component. Passing `extended=True` to the `HyponymDetector` will use the extended set of hearst patterns, which include higher recall but lower precision hyponymy relations (e.g X compared to Y, X similar to Y, etc). This component produces a doc level attribute on the spacy doc: `doc._.hearst_patterns`, which is a list containing tuples of extracted hyponym pairs. The tuples contain:. - The relation rule used to extract the hyponym (type: `str`); - The more general concept (type: `spacy.Span`); - The more specific concept (type: `spacy.Span`). #### Usage:. ```python; import spacy; from scispacy.hyponym_detector import HyponymDetector. nlp = spacy.load(""en_core_sci_sm""); nlp.add_pipe(""hyponym_detector"", last=True, config={""extended"": False}). doc = nlp(""Keystone plant species such as fig trees are good for the soil.""). print(doc._.hearst_patterns); >>> [('such_as', Keystone plant species, fig trees)]; ```. ## Citing. If you use ScispaCy in your research, please cite [ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing](https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Additionally, please indicate which version and model of ScispaCy you used so that your research can be reproduced.; ```; @inproceedings{neumann-etal-2019-scispacy,; title = ""{S}cispa{C}y: {F}ast and {R}obust {M}odels for {B}iomedical {N}atural {L}anguage {P}rocessing"",; author = ""Neumann, Mark and; King, Daniel and; Beltagy, Iz and; Ammar, Waleed"",; booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",; month = aug,; year = ""2019"",; address = ""Florence, Italy"",; publisher = ""Association for C",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:14571,config,config,14571,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,2,"['config', 'extend']","['config', 'extended']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: tein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, ...; ```. ### Hearst Patterns (v0.3.0 and up). This component implements [Automatic Aquisition of Hyponyms from Large Text Corpora](https://www.aclweb.org/anthology/C92-2082.pdf) using the SpaCy Matcher component. Passing `extended=True` to the `HyponymDetector` will use the extended set of hearst patterns, which include higher recall but lower precision hyponymy relations (e.g X compared to Y, X similar to Y, etc). This component produces a doc level attribute on the spacy doc: `doc._.hearst_patterns`, which is a list containing tuples of extracted hyponym pairs. The tuples contain:. - The relation rule used to extract the hyponym (type: `str`); - The more general concept (type: `spacy.Span`); - The more specific concept (type: `spacy.Span`). #### Usage:. ```python; import spacy; from scispacy.hyponym_detector import HyponymDetector. nlp = spacy.load(""en_core_sci_sm""); nlp.add_pipe(""hyponym_detector"", last=True, config={""extended"": False}). doc = nlp(""Keystone plant species such as fig trees are good for the soil.""). print(doc._.hearst_patterns); >>> [('such_as', Keystone plant species, fig trees)]; ```. ## Citing. If you use ScispaCy in your research, please cite [ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing](https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Additionally, please indicate which version and model of ScispaCy you used so that your research can be reproduced.; ```; @inproceedings{neumann-etal-2019-scispacy,; title = ""{S}cispa{C}y: {F}ast and {R}obust {M}odels for {B}iomedical {N}atural {L}anguage {P}rocessing"",; author = ""Neumann, Mark and; King, Daniel and; Beltagy, Iz and; Ammar, Waleed"",; booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",; month = aug,; year = ""2019"",; address = ""Florence, Italy"",; publisher = ""Association for C
",False,"The content contains both technical descriptions and usage examples which are meaningful human-readable sentences. While it includes some code snippets, the majority of the text is explanatory and discusses concepts in a natural language manner."
Modifiability,"ults	T033	UMLS:C2825142; 25847295	1399	1419	cell differentiation	T038	UMLS:C0007589; 25847295	1424	1435	development	T038	UMLS:C0243107; 25847295	1456	1470	harmful effect	T037	UMLS:C0600688; 25847295	1520	1529	apoptosis	T038	UMLS:C0162638. 26316050|t|Prevascularized silicon membranes for the enhancement of transport to implanted medical devices; 26316050|a|Recent advances in drug delivery and sensing devices for in situ applications are limited by the diffusion -limiting foreign body response of fibrous encapsulation. In this study, we fabricated prevascularized synthetic device ports to help mitigate this limitation. Membranes with rectilinear arrays of square pores with widths ranging from 40 to 200 Œºm were created using materials (50 Œºm thick double-sided polished silicon) and processes (photolithography and directed reactive ion etching) common in the manufacturing of microfabricated sensors. Vascular endothelial cells responded to membrane geometry by either forming vascular tubes that extended through the pore or completely filling membrane pores after 4 days in culture. Although tube formation began to predominate overgrowth around 75 Œºm and continued to increase at even larger pore sizes, tubes formed at these large pore sizes were not completely round and had relatively thin walls. Thus, the optimum range of pore size for prevascularization of these membranes was estimated to be 75-100 Œºm. This study lays the foundation for creating a prevascularized port that can be used to reduce fibrous encapsulation and thus enhance diffusion to implanted medical devices and sensors. ¬© 2015 Wiley Periodicals, Inc. J Biomed Mater Res Part B: Appl Biomater, 104B: 1602-1609, 2016.; 26316050	16	23	silicon	T103	UMLS:C0037114; 26316050	70	95	implanted medical devices	T033	UMLS:C2828363; 26316050	115	128	drug delivery	T074	UMLS:C0085104; 26316050	153	160	in situ	T082	UMLS:C0444498; 26316050	161	173	applications	T058	UMLS:C0185125; 26316050	213	234	foreign body response	T033	UML",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/tests/fixtures/med_mentions.txt:9028,extend,extended,9028,tests/fixtures/med_mentions.txt,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/tests/fixtures/med_mentions.txt,1,['extend'],['extended'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ults	T033	UMLS:C2825142; 25847295	1399	1419	cell differentiation	T038	UMLS:C0007589; 25847295	1424	1435	development	T038	UMLS:C0243107; 25847295	1456	1470	harmful effect	T037	UMLS:C0600688; 25847295	1520	1529	apoptosis	T038	UMLS:C0162638. 26316050|t|Prevascularized silicon membranes for the enhancement of transport to implanted medical devices; 26316050|a|Recent advances in drug delivery and sensing devices for in situ applications are limited by the diffusion -limiting foreign body response of fibrous encapsulation. In this study, we fabricated prevascularized synthetic device ports to help mitigate this limitation. Membranes with rectilinear arrays of square pores with widths ranging from 40 to 200 Œºm were created using materials (50 Œºm thick double-sided polished silicon) and processes (photolithography and directed reactive ion etching) common in the manufacturing of microfabricated sensors. Vascular endothelial cells responded to membrane geometry by either forming vascular tubes that extended through the pore or completely filling membrane pores after 4 days in culture. Although tube formation began to predominate overgrowth around 75 Œºm and continued to increase at even larger pore sizes, tubes formed at these large pore sizes were not completely round and had relatively thin walls. Thus, the optimum range of pore size for prevascularization of these membranes was estimated to be 75-100 Œºm. This study lays the foundation for creating a prevascularized port that can be used to reduce fibrous encapsulation and thus enhance diffusion to implanted medical devices and sensors. ¬© 2015 Wiley Periodicals, Inc. J Biomed Mater Res Part B: Appl Biomater, 104B: 1602-1609, 2016.; 26316050	16	23	silicon	T103	UMLS:C0037114; 26316050	70	95	implanted medical devices	T033	UMLS:C2828363; 26316050	115	128	drug delivery	T074	UMLS:C0085104; 26316050	153	160	in situ	T082	UMLS:C0444498; 26316050	161	173	applications	T058	UMLS:C0185125; 26316050	213	234	foreign body response	T033	UML
",True,"The text consists primarily of programmatic data and UMLS identifiers, which are not meaningful human-readable sentences."
Performance," full spaCy pipeline for biomedical data with a larger vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ### Performance. Our models achieve performance within 3% of published state of the art dependency parsers and within 0.4% accuracy of state of the art biomedical POS taggers. | model | UAS | LAS | POS | Mentions (F1) | Web UAS | ; |:---------------|:----|:------|:------|:---|:---|; | en_core_sci_sm | 89.18| 87.15 | 98.18 | 67.89 | 87.36 |; | en_core_sci_md | 90.08| 88.16 | 98.46 | 68.86 | 88.04 |; | en_core_sci_lg | 89.97| 88.18 | 98.51 | 68.98 | 87.89 |; | en_core_sci_scibert | 92.12| 90.58 | 98.18 | 67.70 | 92.58 |. | model | F1 | Entity Types|; |:---------------|:-----|:--------|; | en_ner_craft_md | 78.01|GGP, SO, TAXON, CHEBI, GO, CL|; | en_ner_jnlpba_md | 72.06| DNA, CELL_TYPE, CELL_LINE, RNA, PROTEIN |; | en_ner_bc5cdr_md | 84.28| DISEASE, CHEMICAL|; | en_ner_bionlp13cg_md | 77.84| AMINO_ACID, ANATOMICAL_SYSTEM, CANCER, CELL, CELLULAR_COMPONENT, DEVELOPING_ANATOMICAL_STRUCTURE, GENE_OR_GENE_PRODUCT, IMMATERIAL_ANATOMICAL_ENTITY, MULTI-TISSUE_STRUCTURE, ORGAN, ORGANISM, ORGANISM_SUBDIVISION, ORGANISM_SUBSTANCE, PATHOLOGICAL_FORMATION, SIMPLE_CHE",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md:2110,perform,performance,2110,docs/index.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md,1,['perform'],['performance'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  full spaCy pipeline for biomedical data with a larger vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ### Performance. Our models achieve performance within 3% of published state of the art dependency parsers and within 0.4% accuracy of state of the art biomedical POS taggers. | model | UAS | LAS | POS | Mentions (F1) | Web UAS | ; |:---------------|:----|:------|:------|:---|:---|; | en_core_sci_sm | 89.18| 87.15 | 98.18 | 67.89 | 87.36 |; | en_core_sci_md | 90.08| 88.16 | 98.46 | 68.86 | 88.04 |; | en_core_sci_lg | 89.97| 88.18 | 98.51 | 68.98 | 87.89 |; | en_core_sci_scibert | 92.12| 90.58 | 98.18 | 67.70 | 92.58 |. | model | F1 | Entity Types|; |:---------------|:-----|:--------|; | en_ner_craft_md | 78.01|GGP, SO, TAXON, CHEBI, GO, CL|; | en_ner_jnlpba_md | 72.06| DNA, CELL_TYPE, CELL_LINE, RNA, PROTEIN |; | en_ner_bc5cdr_md | 84.28| DISEASE, CHEMICAL|; | en_ner_bionlp13cg_md | 77.84| AMINO_ACID, ANATOMICAL_SYSTEM, CANCER, CELL, CELLULAR_COMPONENT, DEVELOPING_ANATOMICAL_STRUCTURE, GENE_OR_GENE_PRODUCT, IMMATERIAL_ANATOMICAL_ENTITY, MULTI-TISSUE_STRUCTURE, ORGAN, ORGANISM, ORGANISM_SUBDIVISION, ORGANISM_SUBSTANCE, PATHOLOGICAL_FORMATION, SIMPLE_CHE
",False,
Performance," nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 Span	 Definition; >>> SBMA 		 (33, 34) Spinal and bulbar muscular atrophy; >>> SBMA 	 	 (6, 7) Spinal and bulbar muscular atrophy; >>> AR 		 (29, 30) androgen receptor; ```. > **Note**; > If you want to be able to [serialize your `doc` objects](https://spacy.io/usage/saving-loading), load the abbreviation detector with `make_serializable=True`, e.g. `nlp.add_pipe(""abbreviation_detector"", config={""make_serializable"": True})`. ### EntityLinker. The `EntityLinker` is a SpaCy component which performs linking to a knowledge base. The linker simply performs; a string overlap - based search (char-3grams) on named entities, comparing them with the concepts in a knowledge base; using an approximate nearest neighbours search. Currently (v2.5.0), there are 5 supported linkers:. - `umls`: Links to the [Unified Medical Language System](https://www.nlm.nih.gov/research/umls/index.html), levels 0,1,2 and 9. This has ~3M concepts.; - `mesh`: Links to the [Medical Subject Headings](https://www.nlm.nih.gov/mesh/meshhome.html). This contains a smaller set of higher quality entities, which are used for indexing in Pubmed. MeSH contains ~30k entities. NOTE: The MeSH KB is derived directly from MeSH itself, and as such uses different unique identifiers than the other KBs.; - `rxnorm`: Links to the [RxNorm](https://www.nlm.nih.gov/research/umls/rxnorm/index.html) ontology. RxNorm contains ~100k concepts focused on normalized names for clinical drugs. It is comprised of several other dru",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:8165,perform,performs,8165,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['perform'],['performs'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 Span	 Definition; >>> SBMA 		 (33, 34) Spinal and bulbar muscular atrophy; >>> SBMA 	 	 (6, 7) Spinal and bulbar muscular atrophy; >>> AR 		 (29, 30) androgen receptor; ```. > **Note**; > If you want to be able to [serialize your `doc` objects](https://spacy.io/usage/saving-loading), load the abbreviation detector with `make_serializable=True`, e.g. `nlp.add_pipe(""abbreviation_detector"", config={""make_serializable"": True})`. ### EntityLinker. The `EntityLinker` is a SpaCy component which performs linking to a knowledge base. The linker simply performs; a string overlap - based search (char-3grams) on named entities, comparing them with the concepts in a knowledge base; using an approximate nearest neighbours search. Currently (v2.5.0), there are 5 supported linkers:. - `umls`: Links to the [Unified Medical Language System](https://www.nlm.nih.gov/research/umls/index.html), levels 0,1,2 and 9. This has ~3M concepts.; - `mesh`: Links to the [Medical Subject Headings](https://www.nlm.nih.gov/mesh/meshhome.html). This contains a smaller set of higher quality entities, which are used for indexing in Pubmed. MeSH contains ~30k entities. NOTE: The MeSH KB is derived directly from MeSH itself, and as such uses different unique identifiers than the other KBs.; - `rxnorm`: Links to the [RxNorm](https://www.nlm.nih.gov/research/umls/rxnorm/index.html) ontology. RxNorm contains ~100k concepts focused on normalized names for clinical drugs. It is comprised of several other dru
",False,"The text includes explanations about the EntityLinker component in SpaCy, which is part of a larger discussion about NLP tools and their functionalities. This content is descriptive and serves to inform users about how the tool works, making it meaningful and relevant for human readers."
Performance," that a entity candidate must reach to be added to the mention in the Doc; as a mention candidate if the entity candidate does not have a definition.; - `filter_for_definitions: bool, default = True`; Whether to filter entities that can be returned to only include those with definitions; in the knowledge base.; - `max_entities_per_mention : int, optional, default = 5`; The maximum number of entities which will be returned for a given mention, regardless of; how many are nearest neighbours are found. This class sets the `._.kb_ents` attribute on spacy Spans, which consists of a; List[Tuple[str, float]] corresponding to the KB concept_id and the associated score; for a list of `max_entities_per_mention` number of entities. You can look up more information for a given id using the kb attribute of this class:; ```; print(linker.kb.cui_to_entity[concept_id]); ```. #### Example Usage; ```python; import spacy; import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_sm""). # This line takes a while, because we have to download ~1GB of data; # and load a large JSON file (the knowledge base). Be patient!; # Thankfully it should be faster after the first time you use it, because; # the downloads are cached.; # NOTE: The resolve_abbreviations parameter is optional, and requires that; # the AbbreviationDetector pipe has already been added to the pipeline. Adding; # the AbbreviationDetector pipe and setting resolve_abbreviations to True means; # that linking will only be performed on the long form of abbreviations.; nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). # Let's look at a random entity!; entity = doc.ents[1]. print(""Name: "", entity); >>> Name: bulbar muscular atrophy. # Each ",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:11373,load,load,11373,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['load'],['load'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  that a entity candidate must reach to be added to the mention in the Doc; as a mention candidate if the entity candidate does not have a definition.; - `filter_for_definitions: bool, default = True`; Whether to filter entities that can be returned to only include those with definitions; in the knowledge base.; - `max_entities_per_mention : int, optional, default = 5`; The maximum number of entities which will be returned for a given mention, regardless of; how many are nearest neighbours are found. This class sets the `._.kb_ents` attribute on spacy Spans, which consists of a; List[Tuple[str, float]] corresponding to the KB concept_id and the associated score; for a list of `max_entities_per_mention` number of entities. You can look up more information for a given id using the kb attribute of this class:; ```; print(linker.kb.cui_to_entity[concept_id]); ```. #### Example Usage; ```python; import spacy; import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_sm""). # This line takes a while, because we have to download ~1GB of data; # and load a large JSON file (the knowledge base). Be patient!; # Thankfully it should be faster after the first time you use it, because; # the downloads are cached.; # NOTE: The resolve_abbreviations parameter is optional, and requires that; # the AbbreviationDetector pipe has already been added to the pipeline. Adding; # the AbbreviationDetector pipe and setting resolve_abbreviations to True means; # that linking will only be performed on the long form of abbreviations.; nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). # Let's look at a random entity!; entity = doc.ents[1]. print(""Name: "", entity); >>> Name: bulbar muscular atrophy. # Each 
",False,"The text contains complete, meaningful sentences in natural language discussing concepts and ideas about spinal atrophy and its causes."
Performance,") can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html). 2. Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:3976,load,load,3976,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['load'],['load'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html). 2. Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data with a ~100k vocabulary. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_
",False,
Performance,"2; >>> Aliases (abbreviated, total: 16):; AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, ...; ```. ### Hearst Patterns (v0.3.0 and up). This component implements [Automatic Aquisition of Hyponyms from Large Text Corpora](https://www.aclweb.org/anthology/C92-2082.pdf) using the SpaCy Matcher component. Passing `extended=True` to the `HyponymDetector` will use the extended set of hearst patterns, which include higher recall but lower precision hyponymy relations (e.g X compared to Y, X similar to Y, etc). This component produces a doc level attribute on the spacy doc: `doc._.hearst_patterns`, which is a list containing tuples of extracted hyponym pairs. The tuples contain:. - The relation rule used to extract the hyponym (type: `str`); - The more general concept (type: `spacy.Span`); - The more specific concept (type: `spacy.Span`). #### Usage:. ```python; import spacy; from scispacy.hyponym_detector import HyponymDetector. nlp = spacy.load(""en_core_sci_sm""); nlp.add_pipe(""hyponym_detector"", last=True, config={""extended"": False}). doc = nlp(""Keystone plant species such as fig trees are good for the soil.""). print(doc._.hearst_patterns); >>> [('such_as', Keystone plant species, fig trees)]; ```. ## Citing. If you use ScispaCy in your research, please cite [ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing](https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Additionally, please indicate which version and model of ScispaCy you used so that your research can be reproduced.; ```; @inproceedings{neumann-etal-2019-scispacy,; title = ""{S}cispa{C}y: {F}ast and {R}obust {M}odels for {B}iomedical {N}atural {L}anguage {P}rocessing"",; author = ""Neumann, Mark and; King, Daniel and; Beltagy, Iz and; Ammar, Waleed"",; booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",; month = aug,; year = ""2019"",; address = ""F",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:14503,load,load,14503,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['load'],['load'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: 2; >>> Aliases (abbreviated, total: 16):; AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, ...; ```. ### Hearst Patterns (v0.3.0 and up). This component implements [Automatic Aquisition of Hyponyms from Large Text Corpora](https://www.aclweb.org/anthology/C92-2082.pdf) using the SpaCy Matcher component. Passing `extended=True` to the `HyponymDetector` will use the extended set of hearst patterns, which include higher recall but lower precision hyponymy relations (e.g X compared to Y, X similar to Y, etc). This component produces a doc level attribute on the spacy doc: `doc._.hearst_patterns`, which is a list containing tuples of extracted hyponym pairs. The tuples contain:. - The relation rule used to extract the hyponym (type: `str`); - The more general concept (type: `spacy.Span`); - The more specific concept (type: `spacy.Span`). #### Usage:. ```python; import spacy; from scispacy.hyponym_detector import HyponymDetector. nlp = spacy.load(""en_core_sci_sm""); nlp.add_pipe(""hyponym_detector"", last=True, config={""extended"": False}). doc = nlp(""Keystone plant species such as fig trees are good for the soil.""). print(doc._.hearst_patterns); >>> [('such_as', Keystone plant species, fig trees)]; ```. ## Citing. If you use ScispaCy in your research, please cite [ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing](https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Additionally, please indicate which version and model of ScispaCy you used so that your research can be reproduced.; ```; @inproceedings{neumann-etal-2019-scispacy,; title = ""{S}cispa{C}y: {F}ast and {R}obust {M}odels for {B}iomedical {N}atural {L}anguage {P}rocessing"",; author = ""Neumann, Mark and; King, Daniel and; Beltagy, Iz and; Ammar, Waleed"",; booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",; month = aug,; year = ""2019"",; address = ""F
",False,"The text contains explanatory content about a component in ScispaCy that implements an algorithm for acquiring hyponyms from text corpora. It explains the usage of this component with code snippets and examples, but also includes human-readable sentences discussing the purpose and function."
Performance,"Cy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Additional Pipeline Components. ### AbbreviationDetector; The AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm; for identifying abbreviation definitions in biomedical text."", (Schwartz & Hearst, 2003). You can access the list of abbreviations via the `doc._.abbreviations` attribute and for a given abbreviation,; you can access it's long form (which is a `spacy.tokens.Span`) using `span._.long_form`, which will point to; another span in the document. #### Example Usage; ```python; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 Span	 Definition; >>> SBMA 		 (33, 34) Spinal and bulbar muscular atrophy; >>> SBMA 	 	 (6, 7) Spinal and bulbar muscular atrophy; >>> AR 		 (29, 30) androgen receptor; ```. > **Note**; > If you want to be able to [serialize your `doc` objects](https://spacy.io/usage/saving-loading), load the abbreviation detector with `make_serializable=True`, e.g. `nlp.add_pipe(""abbreviation_detector"", config={""make_serializable"": True})`. ### EntityLinker. The `EntityLinker` is a SpaCy component which performs linking to ",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:7173,load,load,7173,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['load'],['load'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Cy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Additional Pipeline Components. ### AbbreviationDetector; The AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm; for identifying abbreviation definitions in biomedical text."", (Schwartz & Hearst, 2003). You can access the list of abbreviations via the `doc._.abbreviations` attribute and for a given abbreviation,; you can access it's long form (which is a `spacy.tokens.Span`) using `span._.long_form`, which will point to; another span in the document. #### Example Usage; ```python; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 Span	 Definition; >>> SBMA 		 (33, 34) Spinal and bulbar muscular atrophy; >>> SBMA 	 	 (6, 7) Spinal and bulbar muscular atrophy; >>> AR 		 (29, 30) androgen receptor; ```. > **Note**; > If you want to be able to [serialize your `doc` objects](https://spacy.io/usage/saving-loading), load the abbreviation detector with `make_serializable=True`, e.g. `nlp.add_pipe(""abbreviation_detector"", config={""make_serializable"": True})`. ### EntityLinker. The `EntityLinker` is a SpaCy component which performs linking to 
",False,
Performance,"S | ; |:---------------|:----|:------|:------|:---|:---|; | en_core_sci_sm | 89.18| 87.15 | 98.18 | 67.89 | 87.36 |; | en_core_sci_md | 90.08| 88.16 | 98.46 | 68.86 | 88.04 |; | en_core_sci_lg | 89.97| 88.18 | 98.51 | 68.98 | 87.89 |; | en_core_sci_scibert | 92.12| 90.58 | 98.18 | 67.70 | 92.58 |. | model | F1 | Entity Types|; |:---------------|:-----|:--------|; | en_ner_craft_md | 78.01|GGP, SO, TAXON, CHEBI, GO, CL|; | en_ner_jnlpba_md | 72.06| DNA, CELL_TYPE, CELL_LINE, RNA, PROTEIN |; | en_ner_bc5cdr_md | 84.28| DISEASE, CHEMICAL|; | en_ner_bionlp13cg_md | 77.84| AMINO_ACID, ANATOMICAL_SYSTEM, CANCER, CELL, CELLULAR_COMPONENT, DEVELOPING_ANATOMICAL_STRUCTURE, GENE_OR_GENE_PRODUCT, IMMATERIAL_ANATOMICAL_ENTITY, MULTI-TISSUE_STRUCTURE, ORGAN, ORGANISM, ORGANISM_SUBDIVISION, ORGANISM_SUBSTANCE, PATHOLOGICAL_FORMATION, SIMPLE_CHEMICAL, TISSUE |. ### Example Usage. ```python; import scispacy; import spacy. nlp = spacy.load(""en_core_sci_sm""); text = """"""; Myeloid derived suppressor cells (MDSC) are immature ; myeloid cells with immunosuppressive activity. ; They accumulate in tumor-bearing mice and humans ; with different types of cancer, including hepatocellular ; carcinoma (HCC).; """"""; doc = nlp(text). print(list(doc.sents)); >>> [""Myeloid derived suppressor cells (MDSC) are immature myeloid cells with immunosuppressive activity."", ; ""They accumulate in tumor-bearing mice and humans with different types of cancer, including hepatocellular carcinoma (HCC).""]. # Examine the entities extracted by the mention detector.; # Note that they don't have types like in SpaCy, and they; # are more general (e.g including verbs) - these are any; # spans which might be an entity in UMLS, a large; # biomedical database.; print(doc.ents); >>> (Myeloid derived suppressor cells,; MDSC,; immature,; myeloid cells,; immunosuppressive activity,; accumulate,; tumor-bearing mice,; humans,; cancer,; hepatocellular carcinoma,; HCC). # We can also visualise dependency parses; # (This renders aut",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md:3232,load,load,3232,docs/index.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md,1,['load'],['load'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: S | ; |:---------------|:----|:------|:------|:---|:---|; | en_core_sci_sm | 89.18| 87.15 | 98.18 | 67.89 | 87.36 |; | en_core_sci_md | 90.08| 88.16 | 98.46 | 68.86 | 88.04 |; | en_core_sci_lg | 89.97| 88.18 | 98.51 | 68.98 | 87.89 |; | en_core_sci_scibert | 92.12| 90.58 | 98.18 | 67.70 | 92.58 |. | model | F1 | Entity Types|; |:---------------|:-----|:--------|; | en_ner_craft_md | 78.01|GGP, SO, TAXON, CHEBI, GO, CL|; | en_ner_jnlpba_md | 72.06| DNA, CELL_TYPE, CELL_LINE, RNA, PROTEIN |; | en_ner_bc5cdr_md | 84.28| DISEASE, CHEMICAL|; | en_ner_bionlp13cg_md | 77.84| AMINO_ACID, ANATOMICAL_SYSTEM, CANCER, CELL, CELLULAR_COMPONENT, DEVELOPING_ANATOMICAL_STRUCTURE, GENE_OR_GENE_PRODUCT, IMMATERIAL_ANATOMICAL_ENTITY, MULTI-TISSUE_STRUCTURE, ORGAN, ORGANISM, ORGANISM_SUBDIVISION, ORGANISM_SUBSTANCE, PATHOLOGICAL_FORMATION, SIMPLE_CHEMICAL, TISSUE |. ### Example Usage. ```python; import scispacy; import spacy. nlp = spacy.load(""en_core_sci_sm""); text = """"""; Myeloid derived suppressor cells (MDSC) are immature ; myeloid cells with immunosuppressive activity. ; They accumulate in tumor-bearing mice and humans ; with different types of cancer, including hepatocellular ; carcinoma (HCC).; """"""; doc = nlp(text). print(list(doc.sents)); >>> [""Myeloid derived suppressor cells (MDSC) are immature myeloid cells with immunosuppressive activity."", ; ""They accumulate in tumor-bearing mice and humans with different types of cancer, including hepatocellular carcinoma (HCC).""]. # Examine the entities extracted by the mention detector.; # Note that they don't have types like in SpaCy, and they; # are more general (e.g including verbs) - these are any; # spans which might be an entity in UMLS, a large; # biomedical database.; print(doc.ents); >>> (Myeloid derived suppressor cells,; MDSC,; immature,; myeloid cells,; immunosuppressive activity,; accumulate,; tumor-bearing mice,; humans,; cancer,; hepatocellular carcinoma,; HCC). # We can also visualise dependency parses; # (This renders aut
",False,"The content includes human-readable sentences discussing biological cells and their functions in oncology, which are meaningful and not code or logs."
Performance,"Span`) using `span._.long_form`, which will point to; another span in the document. #### Example Usage; ```python; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 Span	 Definition; >>> SBMA 		 (33, 34) Spinal and bulbar muscular atrophy; >>> SBMA 	 	 (6, 7) Spinal and bulbar muscular atrophy; >>> AR 		 (29, 30) androgen receptor; ```. > **Note**; > If you want to be able to [serialize your `doc` objects](https://spacy.io/usage/saving-loading), load the abbreviation detector with `make_serializable=True`, e.g. `nlp.add_pipe(""abbreviation_detector"", config={""make_serializable"": True})`. ### EntityLinker. The `EntityLinker` is a SpaCy component which performs linking to a knowledge base. The linker simply performs; a string overlap - based search (char-3grams) on named entities, comparing them with the concepts in a knowledge base; using an approximate nearest neighbours search. Currently (v2.5.0), there are 5 supported linkers:. - `umls`: Links to the [Unified Medical Language System](https://www.nlm.nih.gov/research/umls/index.html), levels 0,1,2 and 9. This has ~3M concepts.; - `mesh`: Links to the [Medical Subject Headings](https://www.nlm.nih.gov/mesh/meshhome.html). This contains a smaller set of higher quality entities, which are used for indexing in Pubmed. MeSH contains ~30k entities. NOTE: The MeSH KB is derived directly from MeSH itself, and as such uses different unique identifiers than the other KBs.; - `rxnorm`: Links to the [RxNorm",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:7947,load,loading,7947,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,2,['load'],"['load', 'loading']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Span`) using `span._.long_form`, which will point to; another span in the document. #### Example Usage; ```python; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 Span	 Definition; >>> SBMA 		 (33, 34) Spinal and bulbar muscular atrophy; >>> SBMA 	 	 (6, 7) Spinal and bulbar muscular atrophy; >>> AR 		 (29, 30) androgen receptor; ```. > **Note**; > If you want to be able to [serialize your `doc` objects](https://spacy.io/usage/saving-loading), load the abbreviation detector with `make_serializable=True`, e.g. `nlp.add_pipe(""abbreviation_detector"", config={""make_serializable"": True})`. ### EntityLinker. The `EntityLinker` is a SpaCy component which performs linking to a knowledge base. The linker simply performs; a string overlap - based search (char-3grams) on named entities, comparing them with the concepts in a knowledge base; using an approximate nearest neighbours search. Currently (v2.5.0), there are 5 supported linkers:. - `umls`: Links to the [Unified Medical Language System](https://www.nlm.nih.gov/research/umls/index.html), levels 0,1,2 and 9. This has ~3M concepts.; - `mesh`: Links to the [Medical Subject Headings](https://www.nlm.nih.gov/mesh/meshhome.html). This contains a smaller set of higher quality entities, which are used for indexing in Pubmed. MeSH contains ~30k entities. NOTE: The MeSH KB is derived directly from MeSH itself, and as such uses different unique identifiers than the other KBs.; - `rxnorm`: Links to the [RxNorm
",False,"The text includes code snippets (e.g., Python syntax) but also contains meaningful sentences discussing the functionality of SpaCy components like EntityLinker. The presence of both types of content makes it unclear if the primary purpose is to convey information or code."
Performance,"Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 Span	 Definition; >>> SBMA 		 (33, 34) Spinal and bulbar muscular atrophy; >>> SBMA 	 	 (6, 7) Spinal and bulbar muscular atrophy; >>> AR 		 (29, 30) androgen receptor; ```. > **Note**; > If you want to be able to [serialize your `doc` objects](https://spacy.io/usage/saving-loading), load the abbreviation detector with `make_serializable=True`, e.g. `nlp.add_pipe(""abbreviation_detector"", config={""make_serializable"": True})`. ### EntityLinker. The `EntityLinker` is a SpaCy component which performs linking to a knowledge base. The linker simply performs; a string overlap - based search (char-3grams) on named entities, comparing them with the concepts in a knowledge base; using an approximate nearest neighbours search. Currently (v2.5.0), there are 5 supported linkers:. - `umls`: Links to the [Unified Medical Language System](https://www.nlm.nih.gov/research/umls/index.html), levels 0,1,2 and 9. This has ~3M concepts.; - `mesh`: Links to the [Medical Subject Headings](https://www.nlm.nih.gov/mesh/meshhome.html). This contains a smaller set of higher quality entities, which are used for indexing in Pubmed. MeSH contains ~30k entities. NOTE: The MeSH KB is derived directly from MeSH itself, and as such uses different unique identifiers than the other KBs.; - `rxnorm`: Links to the [RxNorm](https://www.nlm.nih.gov/research/umls/rxnorm/index.html) ontology. RxNorm contains ~100k concepts focused on normalized names for clinical drugs. It is comprised of several other drug vocabularies commonly used in pharmacy management and drug interaction, including First Databank, Micromedex, and the Gold Standard Drug D",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:8221,perform,performs,8221,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['perform'],['performs'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 Span	 Definition; >>> SBMA 		 (33, 34) Spinal and bulbar muscular atrophy; >>> SBMA 	 	 (6, 7) Spinal and bulbar muscular atrophy; >>> AR 		 (29, 30) androgen receptor; ```. > **Note**; > If you want to be able to [serialize your `doc` objects](https://spacy.io/usage/saving-loading), load the abbreviation detector with `make_serializable=True`, e.g. `nlp.add_pipe(""abbreviation_detector"", config={""make_serializable"": True})`. ### EntityLinker. The `EntityLinker` is a SpaCy component which performs linking to a knowledge base. The linker simply performs; a string overlap - based search (char-3grams) on named entities, comparing them with the concepts in a knowledge base; using an approximate nearest neighbours search. Currently (v2.5.0), there are 5 supported linkers:. - `umls`: Links to the [Unified Medical Language System](https://www.nlm.nih.gov/research/umls/index.html), levels 0,1,2 and 9. This has ~3M concepts.; - `mesh`: Links to the [Medical Subject Headings](https://www.nlm.nih.gov/mesh/meshhome.html). This contains a smaller set of higher quality entities, which are used for indexing in Pubmed. MeSH contains ~30k entities. NOTE: The MeSH KB is derived directly from MeSH itself, and as such uses different unique identifiers than the other KBs.; - `rxnorm`: Links to the [RxNorm](https://www.nlm.nih.gov/research/umls/rxnorm/index.html) ontology. RxNorm contains ~100k concepts focused on normalized names for clinical drugs. It is comprised of several other drug vocabularies commonly used in pharmacy management and drug interaction, including First Databank, Micromedex, and the Gold Standard Drug D
",False,
Performance,"all --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html). 2. Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sc",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:3867,load,load,3867,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['load'],['load'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: all --verbose 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'`. #### Setting up a virtual environment. [Mamba](https://mamba.readthedocs.io/en/latest/) can be used set up a virtual environment with the; version of Python required for scispaCy. If you already have a Python; environment you want to use, you can skip to the 'installing via pip' section. 1. [Follow the installation instructions for Mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html). 2. Create a Conda environment called ""scispacy"" with Python 3.9 (any version >= 3.6 should work):. ```bash; mamba create -n scispacy python=3.10; ```. 3. Activate the Mamba environment. You will need to activate the Conda environment in each terminal in which you want to use scispaCy. ```bash; mamba activate scispacy; ```. Now you can install `scispacy` and one of the models using the steps above. Once you have completed the above steps and downloaded one of the models below, you can load a scispaCy model as you would any other spaCy model. For example:; ```python; import spacy; nlp = spacy.load(""en_core_sci_sm""); doc = nlp(""Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in some animals.""); ```. #### Note on upgrading; If you are upgrading `scispacy`, you will need to download the models again, to get the model versions compatible with the version of `scispacy` that you have. The link to the model that you download should contain the version number of `scispacy` that you have. ## Available Models. To install a model, click on the link below to download the model, and then run . ```python; pip install </path/to/download>; ```. Alternatively, you can install directly from the URL by right-clicking on the link, selecting ""Copy Link Address"" and running ; ```python; pip install CMD-V(to paste the copied URL); ```. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sc
",False,
Performance,"atterns); >>> [('such_as', Keystone plant species, fig trees)]; ```. ## Citing. If you use ScispaCy in your research, please cite [ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing](https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Additionally, please indicate which version and model of ScispaCy you used so that your research can be reproduced.; ```; @inproceedings{neumann-etal-2019-scispacy,; title = ""{S}cispa{C}y: {F}ast and {R}obust {M}odels for {B}iomedical {N}atural {L}anguage {P}rocessing"",; author = ""Neumann, Mark and; King, Daniel and; Beltagy, Iz and; Ammar, Waleed"",; booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",; month = aug,; year = ""2019"",; address = ""Florence, Italy"",; publisher = ""Association for Computational Linguistics"",; url = ""https://www.aclweb.org/anthology/W19-5034"",; doi = ""10.18653/v1/W19-5034"",; pages = ""319--327"",; eprint = {arXiv:1902.07669},; abstract = ""Despite recent advances in natural language processing, many statistical models for processing text perform extremely poorly under domain shift. Processing biomedical and clinical text is a critically important application area of natural language processing, for which there are few robust, practical, publicly available models. This paper describes scispaCy, a new Python library and models for practical biomedical/scientific text processing, which heavily leverages the spaCy library. We detail the performance of two packages of models released in scispaCy and demonstrate their robustness on several tasks and datasets. Models and code are available at https://allenai.github.io/scispacy/."",; }; ```. ScispaCy is an open-source project developed by [the Allen Institute for Artificial Intelligence (AI2)](http://www.allenai.org).; AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering. ",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:15839,perform,perform,15839,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,2,['perform'],"['perform', 'performance']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: atterns); >>> [('such_as', Keystone plant species, fig trees)]; ```. ## Citing. If you use ScispaCy in your research, please cite [ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing](https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Additionally, please indicate which version and model of ScispaCy you used so that your research can be reproduced.; ```; @inproceedings{neumann-etal-2019-scispacy,; title = ""{S}cispa{C}y: {F}ast and {R}obust {M}odels for {B}iomedical {N}atural {L}anguage {P}rocessing"",; author = ""Neumann, Mark and; King, Daniel and; Beltagy, Iz and; Ammar, Waleed"",; booktitle = ""Proceedings of the 18th BioNLP Workshop and Shared Task"",; month = aug,; year = ""2019"",; address = ""Florence, Italy"",; publisher = ""Association for Computational Linguistics"",; url = ""https://www.aclweb.org/anthology/W19-5034"",; doi = ""10.18653/v1/W19-5034"",; pages = ""319--327"",; eprint = {arXiv:1902.07669},; abstract = ""Despite recent advances in natural language processing, many statistical models for processing text perform extremely poorly under domain shift. Processing biomedical and clinical text is a critically important application area of natural language processing, for which there are few robust, practical, publicly available models. This paper describes scispaCy, a new Python library and models for practical biomedical/scientific text processing, which heavily leverages the spaCy library. We detail the performance of two packages of models released in scispaCy and demonstrate their robustness on several tasks and datasets. Models and code are available at https://allenai.github.io/scispacy/."",; }; ```. ScispaCy is an open-source project developed by [the Allen Institute for Artificial Intelligence (AI2)](http://www.allenai.org).; AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering. 
",False,The text contains complete sentences and explanatory content describing ScispaCy's purpose and development.
Performance,"c; as a mention candidate if the entity candidate does not have a definition.; - `filter_for_definitions: bool, default = True`; Whether to filter entities that can be returned to only include those with definitions; in the knowledge base.; - `max_entities_per_mention : int, optional, default = 5`; The maximum number of entities which will be returned for a given mention, regardless of; how many are nearest neighbours are found. This class sets the `._.kb_ents` attribute on spacy Spans, which consists of a; List[Tuple[str, float]] corresponding to the KB concept_id and the associated score; for a list of `max_entities_per_mention` number of entities. You can look up more information for a given id using the kb attribute of this class:; ```; print(linker.kb.cui_to_entity[concept_id]); ```. #### Example Usage; ```python; import spacy; import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_sm""). # This line takes a while, because we have to download ~1GB of data; # and load a large JSON file (the knowledge base). Be patient!; # Thankfully it should be faster after the first time you use it, because; # the downloads are cached.; # NOTE: The resolve_abbreviations parameter is optional, and requires that; # the AbbreviationDetector pipe has already been added to the pipeline. Adding; # the AbbreviationDetector pipe and setting resolve_abbreviations to True means; # that linking will only be performed on the long form of abbreviations.; nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). # Let's look at a random entity!; entity = doc.ents[1]. print(""Name: "", entity); >>> Name: bulbar muscular atrophy. # Each entity is linked to UMLS with a score; # (currently just char-3gram mat",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:11472,load,load,11472,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['load'],['load'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: c; as a mention candidate if the entity candidate does not have a definition.; - `filter_for_definitions: bool, default = True`; Whether to filter entities that can be returned to only include those with definitions; in the knowledge base.; - `max_entities_per_mention : int, optional, default = 5`; The maximum number of entities which will be returned for a given mention, regardless of; how many are nearest neighbours are found. This class sets the `._.kb_ents` attribute on spacy Spans, which consists of a; List[Tuple[str, float]] corresponding to the KB concept_id and the associated score; for a list of `max_entities_per_mention` number of entities. You can look up more information for a given id using the kb attribute of this class:; ```; print(linker.kb.cui_to_entity[concept_id]); ```. #### Example Usage; ```python; import spacy; import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_sm""). # This line takes a while, because we have to download ~1GB of data; # and load a large JSON file (the knowledge base). Be patient!; # Thankfully it should be faster after the first time you use it, because; # the downloads are cached.; # NOTE: The resolve_abbreviations parameter is optional, and requires that; # the AbbreviationDetector pipe has already been added to the pipeline. Adding; # the AbbreviationDetector pipe and setting resolve_abbreviations to True means; # that linking will only be performed on the long form of abbreviations.; nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). # Let's look at a random entity!; entity = doc.ents[1]. print(""Name: "", entity); >>> Name: bulbar muscular atrophy. # Each entity is linked to UMLS with a score; # (currently just char-3gram mat
",False,"The text contains natural language sentences explaining concepts and providing context about spinal and bulbar muscular atrophy, making it meaningful for human reading."
Performance,"rest neighbours are found. This class sets the `._.kb_ents` attribute on spacy Spans, which consists of a; List[Tuple[str, float]] corresponding to the KB concept_id and the associated score; for a list of `max_entities_per_mention` number of entities. You can look up more information for a given id using the kb attribute of this class:; ```; print(linker.kb.cui_to_entity[concept_id]); ```. #### Example Usage; ```python; import spacy; import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_sm""). # This line takes a while, because we have to download ~1GB of data; # and load a large JSON file (the knowledge base). Be patient!; # Thankfully it should be faster after the first time you use it, because; # the downloads are cached.; # NOTE: The resolve_abbreviations parameter is optional, and requires that; # the AbbreviationDetector pipe has already been added to the pipeline. Adding; # the AbbreviationDetector pipe and setting resolve_abbreviations to True means; # that linking will only be performed on the long form of abbreviations.; nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). # Let's look at a random entity!; entity = doc.ents[1]. print(""Name: "", entity); >>> Name: bulbar muscular atrophy. # Each entity is linked to UMLS with a score; # (currently just char-3gram matching).; linker = nlp.get_pipe(""scispacy_linker""); for umls_ent in entity._.kb_ents:; 	print(linker.kb.cui_to_entity[umls_ent[0]]). >>> CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked; >>> Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the; 				gene encoding the ANDROGEN RECEPTOR.; >>> TUI(s): T047; >>> Aliases (abbreviated, total: 50):; Bulbo-Spinal At",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:11899,perform,performed,11899,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['perform'],['performed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: rest neighbours are found. This class sets the `._.kb_ents` attribute on spacy Spans, which consists of a; List[Tuple[str, float]] corresponding to the KB concept_id and the associated score; for a list of `max_entities_per_mention` number of entities. You can look up more information for a given id using the kb attribute of this class:; ```; print(linker.kb.cui_to_entity[concept_id]); ```. #### Example Usage; ```python; import spacy; import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_sm""). # This line takes a while, because we have to download ~1GB of data; # and load a large JSON file (the knowledge base). Be patient!; # Thankfully it should be faster after the first time you use it, because; # the downloads are cached.; # NOTE: The resolve_abbreviations parameter is optional, and requires that; # the AbbreviationDetector pipe has already been added to the pipeline. Adding; # the AbbreviationDetector pipe and setting resolve_abbreviations to True means; # that linking will only be performed on the long form of abbreviations.; nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). # Let's look at a random entity!; entity = doc.ents[1]. print(""Name: "", entity); >>> Name: bulbar muscular atrophy. # Each entity is linked to UMLS with a score; # (currently just char-3gram matching).; linker = nlp.get_pipe(""scispacy_linker""); for umls_ent in entity._.kb_ents:; 	print(linker.kb.cui_to_entity[umls_ent[0]]). >>> CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked; >>> Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the; 				gene encoding the ANDROGEN RECEPTOR.; >>> TUI(s): T047; >>> Aliases (abbreviated, total: 50):; Bulbo-Spinal At
",False,
Performance,"t = True`; Whether to filter entities that can be returned to only include those with definitions; in the knowledge base.; - `max_entities_per_mention : int, optional, default = 5`; The maximum number of entities which will be returned for a given mention, regardless of; how many are nearest neighbours are found. This class sets the `._.kb_ents` attribute on spacy Spans, which consists of a; List[Tuple[str, float]] corresponding to the KB concept_id and the associated score; for a list of `max_entities_per_mention` number of entities. You can look up more information for a given id using the kb attribute of this class:; ```; print(linker.kb.cui_to_entity[concept_id]); ```. #### Example Usage; ```python; import spacy; import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_sm""). # This line takes a while, because we have to download ~1GB of data; # and load a large JSON file (the knowledge base). Be patient!; # Thankfully it should be faster after the first time you use it, because; # the downloads are cached.; # NOTE: The resolve_abbreviations parameter is optional, and requires that; # the AbbreviationDetector pipe has already been added to the pipeline. Adding; # the AbbreviationDetector pipe and setting resolve_abbreviations to True means; # that linking will only be performed on the long form of abbreviations.; nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). # Let's look at a random entity!; entity = doc.ents[1]. print(""Name: "", entity); >>> Name: bulbar muscular atrophy. # Each entity is linked to UMLS with a score; # (currently just char-3gram matching).; linker = nlp.get_pipe(""scispacy_linker""); for umls_ent in entity._.kb_ents:; 	print(linker.kb.cui_to_entity[u",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:11625,cache,cached,11625,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['cache'],['cached'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: t = True`; Whether to filter entities that can be returned to only include those with definitions; in the knowledge base.; - `max_entities_per_mention : int, optional, default = 5`; The maximum number of entities which will be returned for a given mention, regardless of; how many are nearest neighbours are found. This class sets the `._.kb_ents` attribute on spacy Spans, which consists of a; List[Tuple[str, float]] corresponding to the KB concept_id and the associated score; for a list of `max_entities_per_mention` number of entities. You can look up more information for a given id using the kb attribute of this class:; ```; print(linker.kb.cui_to_entity[concept_id]); ```. #### Example Usage; ```python; import spacy; import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_sm""). # This line takes a while, because we have to download ~1GB of data; # and load a large JSON file (the knowledge base). Be patient!; # Thankfully it should be faster after the first time you use it, because; # the downloads are cached.; # NOTE: The resolve_abbreviations parameter is optional, and requires that; # the AbbreviationDetector pipe has already been added to the pipeline. Adding; # the AbbreviationDetector pipe and setting resolve_abbreviations to True means; # that linking will only be performed on the long form of abbreviations.; nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). # Let's look at a random entity!; entity = doc.ents[1]. print(""Name: "", entity); >>> Name: bulbar muscular atrophy. # Each entity is linked to UMLS with a score; # (currently just char-3gram matching).; linker = nlp.get_pipe(""scispacy_linker""); for umls_ent in entity._.kb_ents:; 	print(linker.kb.cui_to_entity[u
",False,
Performance,"ved directly from MeSH itself, and as such uses different unique identifiers than the other KBs.; - `rxnorm`: Links to the [RxNorm](https://www.nlm.nih.gov/research/umls/rxnorm/index.html) ontology. RxNorm contains ~100k concepts focused on normalized names for clinical drugs. It is comprised of several other drug vocabularies commonly used in pharmacy management and drug interaction, including First Databank, Micromedex, and the Gold Standard Drug Database.; - `go`: Links to the [Gene Ontology](http://geneontology.org/). The Gene Ontology contains ~67k concepts focused on the functions of genes.; - `hpo`: Links to the [Human Phenotype Ontology](https://hpo.jax.org/app/). The Human Phenotype Ontology contains 16k concepts focused on phenotypic abnormalities encountered in human disease. You may want to play around with some of the parameters; below to adapt to your use case (higher precision, higher recall etc). - `resolve_abbreviations : bool = True, optional (default = False)`; Whether to resolve abbreviations identified in the Doc before performing linking.; This parameter has no effect if there is no `AbbreviationDetector` in the spacy; pipeline.; - `k : int, optional, (default = 30)`; The number of nearest neighbours to look up from the candidate generator per mention.; - `threshold : float, optional, (default = 0.7)`; The threshold that a mention candidate must reach to be added to the mention in the Doc; as a mention candidate.; - `no_definition_threshold : float, optional, (default = 0.95)`; The threshold that a entity candidate must reach to be added to the mention in the Doc; as a mention candidate if the entity candidate does not have a definition.; - `filter_for_definitions: bool, default = True`; Whether to filter entities that can be returned to only include those with definitions; in the knowledge base.; - `max_entities_per_mention : int, optional, default = 5`; The maximum number of entities which will be returned for a given mention, regardless of; h",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:9903,perform,performing,9903,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['perform'],['performing'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ved directly from MeSH itself, and as such uses different unique identifiers than the other KBs.; - `rxnorm`: Links to the [RxNorm](https://www.nlm.nih.gov/research/umls/rxnorm/index.html) ontology. RxNorm contains ~100k concepts focused on normalized names for clinical drugs. It is comprised of several other drug vocabularies commonly used in pharmacy management and drug interaction, including First Databank, Micromedex, and the Gold Standard Drug Database.; - `go`: Links to the [Gene Ontology](http://geneontology.org/). The Gene Ontology contains ~67k concepts focused on the functions of genes.; - `hpo`: Links to the [Human Phenotype Ontology](https://hpo.jax.org/app/). The Human Phenotype Ontology contains 16k concepts focused on phenotypic abnormalities encountered in human disease. You may want to play around with some of the parameters; below to adapt to your use case (higher precision, higher recall etc). - `resolve_abbreviations : bool = True, optional (default = False)`; Whether to resolve abbreviations identified in the Doc before performing linking.; This parameter has no effect if there is no `AbbreviationDetector` in the spacy; pipeline.; - `k : int, optional, (default = 30)`; The number of nearest neighbours to look up from the candidate generator per mention.; - `threshold : float, optional, (default = 0.7)`; The threshold that a mention candidate must reach to be added to the mention in the Doc; as a mention candidate.; - `no_definition_threshold : float, optional, (default = 0.95)`; The threshold that a entity candidate must reach to be added to the mention in the Doc; as a mention candidate if the entity candidate does not have a definition.; - `filter_for_definitions: bool, default = True`; Whether to filter entities that can be returned to only include those with definitions; in the knowledge base.; - `max_entities_per_mention : int, optional, default = 5`; The maximum number of entities which will be returned for a given mention, regardless of; h
",True,"The text contains API documentation or specifications such as parameter lists and return types. It includes descriptions of different knowledge bases (RxNorm, Gene Ontology, Human Phenotype Ontology) with details on their concepts and parameters used in the system."
Safety,"	1607	1609	CF	T038	UMLS:C0010674; 25763772	1627	1630	CPA	T038	UMLS:C0854135; 25763772	1643	1661	class II mutations	T038	UMLS:C0026882; 25763772	1706	1711	DCTN4	T103	UMLS:C4308010; 25763772	1721	1729	variants	T103	UMLS:C0597298; 25763772	1742	1753	p.Tyr263Cys	T103	UMLS:C0597298; 25763772	1778	1790	pathogenesis	T038	UMLS:C0699748; 25763772	1794	1797	CPA	T038	UMLS:C0854135; 25763772	1806	1808	CF	T038	UMLS:C0010674. 25847295|t|Nonylphenol diethoxylate inhibits apoptosis induced in PC12 cells; 25847295|a|Nonylphenol and short-chain nonylphenol ethoxylates such as NP2 EO are present in aquatic environment as wastewater contaminants, and their toxic effects on aquatic species have been reported. Apoptosis has been shown to be induced by serum deprivation or copper treatment. To understand the toxicity of nonylphenol diethoxylate, we investigated the effects of NP2 EO on apoptosis induced by serum deprivation and copper by using PC12 cell system. Nonylphenol diethoxylate itself showed no toxicity and recovered cell viability from apoptosis. In addition, nonylphenol diethoxylate decreased DNA fragmentation caused by apoptosis in PC12 cells. This phenomenon was confirmed after treating apoptotic PC12 cells with nonylphenol diethoxylate, whereas the cytochrome c release into the cytosol decreased as compared to that in apoptotic cells not treated with nonylphenol diethoxylate s. Furthermore, Bax contents in apoptotic cells were reduced after exposure to nonylphenol diethoxylate. Thus, nonylphenol diethoxylate has the opposite effect on apoptosis in PC12 cells compared to nonylphenol, which enhances apoptosis induced by serum deprivation. The difference in structure of the two compounds is hypothesized to be responsible for this phenomenon. These results indicated that nonylphenol diethoxylate has capability to affect cell differentiation and development and has potentially harmful effect on organisms because of its unexpected impact on apoptosis. ¬© 2015 Wiley Periodicals, Inc.",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/tests/fixtures/med_mentions.txt:5631,recover,recovered,5631,tests/fixtures/med_mentions.txt,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/tests/fixtures/med_mentions.txt,1,['recover'],['recovered'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: 	1607	1609	CF	T038	UMLS:C0010674; 25763772	1627	1630	CPA	T038	UMLS:C0854135; 25763772	1643	1661	class II mutations	T038	UMLS:C0026882; 25763772	1706	1711	DCTN4	T103	UMLS:C4308010; 25763772	1721	1729	variants	T103	UMLS:C0597298; 25763772	1742	1753	p.Tyr263Cys	T103	UMLS:C0597298; 25763772	1778	1790	pathogenesis	T038	UMLS:C0699748; 25763772	1794	1797	CPA	T038	UMLS:C0854135; 25763772	1806	1808	CF	T038	UMLS:C0010674. 25847295|t|Nonylphenol diethoxylate inhibits apoptosis induced in PC12 cells; 25847295|a|Nonylphenol and short-chain nonylphenol ethoxylates such as NP2 EO are present in aquatic environment as wastewater contaminants, and their toxic effects on aquatic species have been reported. Apoptosis has been shown to be induced by serum deprivation or copper treatment. To understand the toxicity of nonylphenol diethoxylate, we investigated the effects of NP2 EO on apoptosis induced by serum deprivation and copper by using PC12 cell system. Nonylphenol diethoxylate itself showed no toxicity and recovered cell viability from apoptosis. In addition, nonylphenol diethoxylate decreased DNA fragmentation caused by apoptosis in PC12 cells. This phenomenon was confirmed after treating apoptotic PC12 cells with nonylphenol diethoxylate, whereas the cytochrome c release into the cytosol decreased as compared to that in apoptotic cells not treated with nonylphenol diethoxylate s. Furthermore, Bax contents in apoptotic cells were reduced after exposure to nonylphenol diethoxylate. Thus, nonylphenol diethoxylate has the opposite effect on apoptosis in PC12 cells compared to nonylphenol, which enhances apoptosis induced by serum deprivation. The difference in structure of the two compounds is hypothesized to be responsible for this phenomenon. These results indicated that nonylphenol diethoxylate has capability to affect cell differentiation and development and has potentially harmful effect on organisms because of its unexpected impact on apoptosis. ¬© 2015 Wiley Periodicals, Inc.
",False,"The text contains complete sentences discussing the effects of nonylphenol diethoxylate on apoptosis in PC12 cells, providing explanatory and descriptive content."
Safety,"<p align=""center""><img width=""50%"" src=""docs/scispacy-logo.png"" /></p>. This repository contains custom pipes and models related to using spaCy for scientific documents. In particular, there is a custom tokenizer that adds tokenization rules on top of spaCy's; rule-based tokenizer, a POS tagger and syntactic parser trained on biomedical data and; an entity span detection model. Separately, there are also NER models for more specific tasks. **Just looking to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org)** (Note: this demo is running an older version of scispaCy and may produce different results than the latest version). ## Installation; Installing scispacy requires two steps: installing the library and intalling the models. To install the library, run:; ```bash; pip install scispacy; ```. to install a model (see our full selection of available models below), run a command like the following:. ```bash; pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz; ```. Note: We strongly recommend that you use an isolated Python environment (such as virtualenv or conda) to install scispacy.; Take a look below in the ""Setting up a virtual environment"" section if you need some help with this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:364,detect,detection,364,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['detect'],['detection'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: <p align=""center""><img width=""50%"" src=""docs/scispacy-logo.png"" /></p>. This repository contains custom pipes and models related to using spaCy for scientific documents. In particular, there is a custom tokenizer that adds tokenization rules on top of spaCy's; rule-based tokenizer, a POS tagger and syntactic parser trained on biomedical data and; an entity span detection model. Separately, there are also NER models for more specific tasks. **Just looking to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org)** (Note: this demo is running an older version of scispaCy and may produce different results than the latest version). ## Installation; Installing scispacy requires two steps: installing the library and intalling the models. To install the library, run:; ```bash; pip install scispacy; ```. to install a model (see our full selection of available models below), run a command like the following:. ```bash; pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz; ```. Note: We strongly recommend that you use an isolated Python environment (such as virtualenv or conda) to install scispacy.; Take a look below in the ""Setting up a virtual environment"" section if you need some help with this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------
",False,
Safety,"Span`) using `span._.long_form`, which will point to; another span in the document. #### Example Usage; ```python; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 Span	 Definition; >>> SBMA 		 (33, 34) Spinal and bulbar muscular atrophy; >>> SBMA 	 	 (6, 7) Spinal and bulbar muscular atrophy; >>> AR 		 (29, 30) androgen receptor; ```. > **Note**; > If you want to be able to [serialize your `doc` objects](https://spacy.io/usage/saving-loading), load the abbreviation detector with `make_serializable=True`, e.g. `nlp.add_pipe(""abbreviation_detector"", config={""make_serializable"": True})`. ### EntityLinker. The `EntityLinker` is a SpaCy component which performs linking to a knowledge base. The linker simply performs; a string overlap - based search (char-3grams) on named entities, comparing them with the concepts in a knowledge base; using an approximate nearest neighbours search. Currently (v2.5.0), there are 5 supported linkers:. - `umls`: Links to the [Unified Medical Language System](https://www.nlm.nih.gov/research/umls/index.html), levels 0,1,2 and 9. This has ~3M concepts.; - `mesh`: Links to the [Medical Subject Headings](https://www.nlm.nih.gov/mesh/meshhome.html). This contains a smaller set of higher quality entities, which are used for indexing in Pubmed. MeSH contains ~30k entities. NOTE: The MeSH KB is derived directly from MeSH itself, and as such uses different unique identifiers than the other KBs.; - `rxnorm`: Links to the [RxNorm",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:7979,detect,detector,7979,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['detect'],['detector'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Span`) using `span._.long_form`, which will point to; another span in the document. #### Example Usage; ```python; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 Span	 Definition; >>> SBMA 		 (33, 34) Spinal and bulbar muscular atrophy; >>> SBMA 	 	 (6, 7) Spinal and bulbar muscular atrophy; >>> AR 		 (29, 30) androgen receptor; ```. > **Note**; > If you want to be able to [serialize your `doc` objects](https://spacy.io/usage/saving-loading), load the abbreviation detector with `make_serializable=True`, e.g. `nlp.add_pipe(""abbreviation_detector"", config={""make_serializable"": True})`. ### EntityLinker. The `EntityLinker` is a SpaCy component which performs linking to a knowledge base. The linker simply performs; a string overlap - based search (char-3grams) on named entities, comparing them with the concepts in a knowledge base; using an approximate nearest neighbours search. Currently (v2.5.0), there are 5 supported linkers:. - `umls`: Links to the [Unified Medical Language System](https://www.nlm.nih.gov/research/umls/index.html), levels 0,1,2 and 9. This has ~3M concepts.; - `mesh`: Links to the [Medical Subject Headings](https://www.nlm.nih.gov/mesh/meshhome.html). This contains a smaller set of higher quality entities, which are used for indexing in Pubmed. MeSH contains ~30k entities. NOTE: The MeSH KB is derived directly from MeSH itself, and as such uses different unique identifiers than the other KBs.; - `rxnorm`: Links to the [RxNorm
",True,"The content consists primarily of code examples, syntax, and API documentation."
Safety,"h this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Additional Pipeline Components. ### AbbreviationDetector; The AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm; for identifying abbreviation definitions in biomedical text."", (Schwartz & Hearst, 2003). You can access the list of abbreviations via the `doc._.abbreviations` attribute and for a given abbreviation,; you can access it's long form (which is a `spacy.tokens.Span`) using `span._.long_form`, which will point to; another span in the document. #### Example Usage; ```python; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 S",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:6674,detect,detection,6674,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['detect'],['detection'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: h this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Additional Pipeline Components. ### AbbreviationDetector; The AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm; for identifying abbreviation definitions in biomedical text."", (Schwartz & Hearst, 2003). You can access the list of abbreviations via the `doc._.abbreviations` attribute and for a given abbreviation,; you can access it's long form (which is a `spacy.tokens.Span`) using `span._.long_form`, which will point to; another span in the document. #### Example Usage; ```python; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 S
",False,"The text contains complete, meaningful sentences in natural language, such as describing the purpose and functionality of components like the AbbreviationDetector. It also includes example usage code with explanations, which is human-readable prose discussing concepts or ideas. There are no signs of code snippets, logs, API documentation, or other programmatic content."
Safety,"ith different types of cancer, including hepatocellular ; carcinoma (HCC).; """"""; doc = nlp(text). print(list(doc.sents)); >>> [""Myeloid derived suppressor cells (MDSC) are immature myeloid cells with immunosuppressive activity."", ; ""They accumulate in tumor-bearing mice and humans with different types of cancer, including hepatocellular carcinoma (HCC).""]. # Examine the entities extracted by the mention detector.; # Note that they don't have types like in SpaCy, and they; # are more general (e.g including verbs) - these are any; # spans which might be an entity in UMLS, a large; # biomedical database.; print(doc.ents); >>> (Myeloid derived suppressor cells,; MDSC,; immature,; myeloid cells,; immunosuppressive activity,; accumulate,; tumor-bearing mice,; humans,; cancer,; hepatocellular carcinoma,; HCC). # We can also visualise dependency parses; # (This renders automatically inside a jupyter notebook!):; from spacy import displacy; displacy.render(next(doc.sents), style='dep', jupyter=True). # See below for the generated SVG.; # Zoom your browser in a bit!. ```. ![Branching](./example.svg). ### Data Sources. scispaCy models are trained on data from a variety of sources. In particular,; we use:. * **[The GENIA 1.0 Treebank](https://nlp.stanford.edu/~mcclosky/biomedical.html)**, converted to basic Universal Dependencies using the [Stanford Dependency Converter](https://nlp.stanford.edu/software/stanford-dependencies.shtml).; We have made this [dataset available along with the original raw data](https://github.com/allenai/genia-dependency-trees).; * **[word2vec word vectors](http://bio.nlplab.org/#word-vectors)** trained on the Pubmed Central Open Access Subset.; * **[The MedMentions Entity Linking dataset](https://github.com/chanzuckerberg/MedMentions)**, used for training a mention detector.; * **[Ontonotes 5.0](https://catalog.ldc.upenn.edu/LDC2013T19)** to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publicly available.; ",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md:5236,detect,detector,5236,docs/index.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md,1,['detect'],['detector'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ith different types of cancer, including hepatocellular ; carcinoma (HCC).; """"""; doc = nlp(text). print(list(doc.sents)); >>> [""Myeloid derived suppressor cells (MDSC) are immature myeloid cells with immunosuppressive activity."", ; ""They accumulate in tumor-bearing mice and humans with different types of cancer, including hepatocellular carcinoma (HCC).""]. # Examine the entities extracted by the mention detector.; # Note that they don't have types like in SpaCy, and they; # are more general (e.g including verbs) - these are any; # spans which might be an entity in UMLS, a large; # biomedical database.; print(doc.ents); >>> (Myeloid derived suppressor cells,; MDSC,; immature,; myeloid cells,; immunosuppressive activity,; accumulate,; tumor-bearing mice,; humans,; cancer,; hepatocellular carcinoma,; HCC). # We can also visualise dependency parses; # (This renders automatically inside a jupyter notebook!):; from spacy import displacy; displacy.render(next(doc.sents), style='dep', jupyter=True). # See below for the generated SVG.; # Zoom your browser in a bit!. ```. ![Branching](./example.svg). ### Data Sources. scispaCy models are trained on data from a variety of sources. In particular,; we use:. * **[The GENIA 1.0 Treebank](https://nlp.stanford.edu/~mcclosky/biomedical.html)**, converted to basic Universal Dependencies using the [Stanford Dependency Converter](https://nlp.stanford.edu/software/stanford-dependencies.shtml).; We have made this [dataset available along with the original raw data](https://github.com/allenai/genia-dependency-trees).; * **[word2vec word vectors](http://bio.nlplab.org/#word-vectors)** trained on the Pubmed Central Open Access Subset.; * **[The MedMentions Entity Linking dataset](https://github.com/chanzuckerberg/MedMentions)**, used for training a mention detector.; * **[Ontonotes 5.0](https://catalog.ldc.upenn.edu/LDC2013T19)** to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publicly available.; 
",False,
Safety,"md | 84.28| DISEASE, CHEMICAL|; | en_ner_bionlp13cg_md | 77.84| AMINO_ACID, ANATOMICAL_SYSTEM, CANCER, CELL, CELLULAR_COMPONENT, DEVELOPING_ANATOMICAL_STRUCTURE, GENE_OR_GENE_PRODUCT, IMMATERIAL_ANATOMICAL_ENTITY, MULTI-TISSUE_STRUCTURE, ORGAN, ORGANISM, ORGANISM_SUBDIVISION, ORGANISM_SUBSTANCE, PATHOLOGICAL_FORMATION, SIMPLE_CHEMICAL, TISSUE |. ### Example Usage. ```python; import scispacy; import spacy. nlp = spacy.load(""en_core_sci_sm""); text = """"""; Myeloid derived suppressor cells (MDSC) are immature ; myeloid cells with immunosuppressive activity. ; They accumulate in tumor-bearing mice and humans ; with different types of cancer, including hepatocellular ; carcinoma (HCC).; """"""; doc = nlp(text). print(list(doc.sents)); >>> [""Myeloid derived suppressor cells (MDSC) are immature myeloid cells with immunosuppressive activity."", ; ""They accumulate in tumor-bearing mice and humans with different types of cancer, including hepatocellular carcinoma (HCC).""]. # Examine the entities extracted by the mention detector.; # Note that they don't have types like in SpaCy, and they; # are more general (e.g including verbs) - these are any; # spans which might be an entity in UMLS, a large; # biomedical database.; print(doc.ents); >>> (Myeloid derived suppressor cells,; MDSC,; immature,; myeloid cells,; immunosuppressive activity,; accumulate,; tumor-bearing mice,; humans,; cancer,; hepatocellular carcinoma,; HCC). # We can also visualise dependency parses; # (This renders automatically inside a jupyter notebook!):; from spacy import displacy; displacy.render(next(doc.sents), style='dep', jupyter=True). # See below for the generated SVG.; # Zoom your browser in a bit!. ```. ![Branching](./example.svg). ### Data Sources. scispaCy models are trained on data from a variety of sources. In particular,; we use:. * **[The GENIA 1.0 Treebank](https://nlp.stanford.edu/~mcclosky/biomedical.html)**, converted to basic Universal Dependencies using the [Stanford Dependency Converter](https",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md:3831,detect,detector,3831,docs/index.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md,1,['detect'],['detector'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: md | 84.28| DISEASE, CHEMICAL|; | en_ner_bionlp13cg_md | 77.84| AMINO_ACID, ANATOMICAL_SYSTEM, CANCER, CELL, CELLULAR_COMPONENT, DEVELOPING_ANATOMICAL_STRUCTURE, GENE_OR_GENE_PRODUCT, IMMATERIAL_ANATOMICAL_ENTITY, MULTI-TISSUE_STRUCTURE, ORGAN, ORGANISM, ORGANISM_SUBDIVISION, ORGANISM_SUBSTANCE, PATHOLOGICAL_FORMATION, SIMPLE_CHEMICAL, TISSUE |. ### Example Usage. ```python; import scispacy; import spacy. nlp = spacy.load(""en_core_sci_sm""); text = """"""; Myeloid derived suppressor cells (MDSC) are immature ; myeloid cells with immunosuppressive activity. ; They accumulate in tumor-bearing mice and humans ; with different types of cancer, including hepatocellular ; carcinoma (HCC).; """"""; doc = nlp(text). print(list(doc.sents)); >>> [""Myeloid derived suppressor cells (MDSC) are immature myeloid cells with immunosuppressive activity."", ; ""They accumulate in tumor-bearing mice and humans with different types of cancer, including hepatocellular carcinoma (HCC).""]. # Examine the entities extracted by the mention detector.; # Note that they don't have types like in SpaCy, and they; # are more general (e.g including verbs) - these are any; # spans which might be an entity in UMLS, a large; # biomedical database.; print(doc.ents); >>> (Myeloid derived suppressor cells,; MDSC,; immature,; myeloid cells,; immunosuppressive activity,; accumulate,; tumor-bearing mice,; humans,; cancer,; hepatocellular carcinoma,; HCC). # We can also visualise dependency parses; # (This renders automatically inside a jupyter notebook!):; from spacy import displacy; displacy.render(next(doc.sents), style='dep', jupyter=True). # See below for the generated SVG.; # Zoom your browser in a bit!. ```. ![Branching](./example.svg). ### Data Sources. scispaCy models are trained on data from a variety of sources. In particular,; we use:. * **[The GENIA 1.0 Treebank](https://nlp.stanford.edu/~mcclosky/biomedical.html)**, converted to basic Universal Dependencies using the [Stanford Dependency Converter](https
",True,"The provided text is a code example with API documentation and command outputs, including import statements, variable initializations, and usage examples that are more programmatic in nature. It also contains log-like information such as print statements and error messages. These elements make the content primarily logs or code snippets."
Security,"-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Additional Pipeline Components. ### AbbreviationDetector; The AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm; for identifying abbreviation definitions in biomedical text."", (Schwartz & Hearst, 2003). You can access the list of abbreviations via the `doc._.abbreviations` attribute and for a given abbreviation,; you can access it's long form (which is a `spacy.tokens.Span`) using `span._.long_form`, which will point to; another span in the document. #### Example Usage; ```python; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 Span	 Definition; >>> SBMA 		 (33, 34) Spinal and bulbar muscular atrophy; >>> SBMA 	 	 (6, 7) Spinal and bulbar muscular atrophy; >>> AR 		 (29, 30) androgen receptor; ```. > **Note**; > If you want to be able to [serialize your `doc` objects](",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:6928,access,access,6928,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['access'],['access'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: -s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Additional Pipeline Components. ### AbbreviationDetector; The AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm; for identifying abbreviation definitions in biomedical text."", (Schwartz & Hearst, 2003). You can access the list of abbreviations via the `doc._.abbreviations` attribute and for a given abbreviation,; you can access it's long form (which is a `spacy.tokens.Span`) using `span._.long_form`, which will point to; another span in the document. #### Example Usage; ```python; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 Span	 Definition; >>> SBMA 		 (33, 34) Spinal and bulbar muscular atrophy; >>> SBMA 	 	 (6, 7) Spinal and bulbar muscular atrophy; >>> AR 		 (29, 30) androgen receptor; ```. > **Note**; > If you want to be able to [serialize your `doc` objects](
",False,
Security,"ary and intalling the models. To install the library, run:; ```bash; pip install scispacy; ```. to install a model (see our full selection of available models below), run a command like the following:. ```bash; pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz; ```. Note: We strongly recommend that you use an isolated Python environment (such as virtualenv or conda) to install scispacy.; Take a look below in the ""Setting up a virtual environment"" section if you need some help with this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Python 3.11 | ‚ùåüêç | ‚ùåüêç | ‚ùì | ‚ùì | ‚ùì | ‚ùå |; | Python 3.12 | ‚ùåüêç | ‚ùåüêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`; - `pip install --no-binary :all: nmslib`; - `pip install ""nmslib @ git+https://githu",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:1696,access,access,1696,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['access'],['access'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ary and intalling the models. To install the library, run:; ```bash; pip install scispacy; ```. to install a model (see our full selection of available models below), run a command like the following:. ```bash; pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz; ```. Note: We strongly recommend that you use an isolated Python environment (such as virtualenv or conda) to install scispacy.; Take a look below in the ""Setting up a virtual environment"" section if you need some help with this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Python 3.11 | ‚ùåüêç | ‚ùåüêç | ‚ùì | ‚ùì | ‚ùì | ‚ùå |; | Python 3.12 | ‚ùåüêç | ‚ùåüêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`; - `pip install --no-binary :all: nmslib`; - `pip install ""nmslib @ git+https://githu
",False,
Security,"del trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Additional Pipeline Components. ### AbbreviationDetector; The AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm; for identifying abbreviation definitions in biomedical text."", (Schwartz & Hearst, 2003). You can access the list of abbreviations via the `doc._.abbreviations` attribute and for a given abbreviation,; you can access it's long form (which is a `spacy.tokens.Span`) using `span._.long_form`, which will point to; another span in the document. #### Example Usage; ```python; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 Span	 Definition; >>> SBMA 		 (33, 34) Spinal and bulbar muscular atrophy; >>> SBMA 	 	 (6, 7) Spinal and bulbar muscular atrophy; >>> AR 		 (29, 30) androgen rece",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:6816,access,access,6816,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['access'],['access'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: del trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Additional Pipeline Components. ### AbbreviationDetector; The AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm; for identifying abbreviation definitions in biomedical text."", (Schwartz & Hearst, 2003). You can access the list of abbreviations via the `doc._.abbreviations` attribute and for a given abbreviation,; you can access it's long form (which is a `spacy.tokens.Span`) using `span._.long_form`, which will point to; another span in the document. #### Example Usage; ```python; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 Span	 Definition; >>> SBMA 		 (33, 34) Spinal and bulbar muscular atrophy; >>> SBMA 	 	 (6, 7) Spinal and bulbar muscular atrophy; >>> AR 		 (29, 30) androgen rece
",False,"The text contains sentences discussing the functionality of an abbreviation detector component in a spaCy model. It includes examples of abbreviations, their definitions, and usage instructions, which are meaningful and explanatory."
Testability,"---; layout: default; ---. **scispaCy is a Python package containing [spaCy](https://spacy.io/) models for processing _biomedical_, _scientific_ or _clinical_ text.**. ## Interactive Demo; Just looking to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org). ## Installing; ```python; pip install scispacy; pip install <Model URL>; ```; ## Models. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a larger vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a larger vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. | [Download](https://s3-us-west-2.amazonaws.",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md:205,test,test,205,docs/index.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/docs/index.md,1,['test'],['test'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ---; layout: default; ---. **scispaCy is a Python package containing [spaCy](https://spacy.io/) models for processing _biomedical_, _scientific_ or _clinical_ text.**. ## Interactive Demo; Just looking to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org). ## Installing; ```python; pip install scispacy; pip install <Model URL>; ```; ## Models. | Model | Description | Install URL; |:---------------|:------------------|:----------|; | en_core_sci_sm | A full spaCy pipeline for biomedical data. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz)|; | en_core_sci_md | A full spaCy pipeline for biomedical data with a larger vocabulary and 50k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz)|; | en_core_sci_scibert | A full spaCy pipeline for biomedical data with a ~785k vocabulary and `allenai/scibert-base` as the transformer model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_core_sci_lg | A full spaCy pipeline for biomedical data with a larger vocabulary and 600k word vectors. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. | [Download](https://s3-us-west-2.amazonaws.
",False,"The content contains installation instructions, model descriptions, and links to demos and downloads. While there is some code syntax (e.g., `pip install`), the majority of the text consists of explanatory and descriptive sentences that discuss the purpose and usage of the package."
Testability,"; ### Creating a release. Scispacy has two components:. - The scispacy pip package; - The scispacy models. The scispacy pip package is published automatically using the `.github/actions/publish.yml` github action. It happens whenever a release is published (with an associated tag) in the github releases UI. In order to create a new release, the following should happen:. #### Updating `scispacy/version.py`; Update the version in version.py. #### Training new models. The entire pipeline can be run using `spacy project run all`. This will train and package all the models. The packages should then be uploaded to the `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/{VERSION}` S3 bucket, and references to previous models (e.g in the readme and in the docs) should be updated. You can find all these places using `git grep <previous version>`. The scripts `install_local_packages.py`, `install_remote_packages.py`, `print_out_metrics.py`, `smoke_test.py`, and `uninstall_local_packages.py` are useful for testing at each step of the process. Before uploading, `install_local_packages.py` and `smoke_test.py` can be used to make sure the packages are installable and do a quick check of output. `print_out_metrics.py` can then be used to easily get the metrics that need to be updated in the README. Once the packages have been uploaded, `uninstall_local_packages.py`, `install_remote_packages.py`, and `smoke_test.py` can be used to ensure everything was uploaded correctly. #### Merge a PR with the above changes; Merge a PR with the above changes, and publish a release with a tag corresponding to the commit from the merged PR. This should trigger the publish github action, which will create the `scispacy` package and publish it to pypi. ",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/RELEASE.md:1020,test,testing,1020,RELEASE.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/RELEASE.md,1,['test'],['testing'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ; ### Creating a release. Scispacy has two components:. - The scispacy pip package; - The scispacy models. The scispacy pip package is published automatically using the `.github/actions/publish.yml` github action. It happens whenever a release is published (with an associated tag) in the github releases UI. In order to create a new release, the following should happen:. #### Updating `scispacy/version.py`; Update the version in version.py. #### Training new models. The entire pipeline can be run using `spacy project run all`. This will train and package all the models. The packages should then be uploaded to the `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/{VERSION}` S3 bucket, and references to previous models (e.g in the readme and in the docs) should be updated. You can find all these places using `git grep <previous version>`. The scripts `install_local_packages.py`, `install_remote_packages.py`, `print_out_metrics.py`, `smoke_test.py`, and `uninstall_local_packages.py` are useful for testing at each step of the process. Before uploading, `install_local_packages.py` and `smoke_test.py` can be used to make sure the packages are installable and do a quick check of output. `print_out_metrics.py` can then be used to easily get the metrics that need to be updated in the README. Once the packages have been uploaded, `uninstall_local_packages.py`, `install_remote_packages.py`, and `smoke_test.py` can be used to ensure everything was uploaded correctly. #### Merge a PR with the above changes; Merge a PR with the above changes, and publish a release with a tag corresponding to the commit from the merged PR. This should trigger the publish github action, which will create the `scispacy` package and publish it to pypi. 
",False,"The provided text consists of detailed instructions for creating a release in a software development context. It includes step-by-step procedures, commands to execute, and explanations of each process. While the content may be technical in nature, it is written in a way that is understandable to humans, detailing how actions should be taken rather than providing code snippets or logs."
Testability,"<p align=""center""><img width=""50%"" src=""docs/scispacy-logo.png"" /></p>. This repository contains custom pipes and models related to using spaCy for scientific documents. In particular, there is a custom tokenizer that adds tokenization rules on top of spaCy's; rule-based tokenizer, a POS tagger and syntactic parser trained on biomedical data and; an entity span detection model. Separately, there are also NER models for more specific tasks. **Just looking to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org)** (Note: this demo is running an older version of scispaCy and may produce different results than the latest version). ## Installation; Installing scispacy requires two steps: installing the library and intalling the models. To install the library, run:; ```bash; pip install scispacy; ```. to install a model (see our full selection of available models below), run a command like the following:. ```bash; pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz; ```. Note: We strongly recommend that you use an isolated Python environment (such as virtualenv or conda) to install scispacy.; Take a look below in the ""Setting up a virtual environment"" section if you need some help with this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:54,log,logo,54,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,2,"['log', 'test']","['logo', 'test']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: <p align=""center""><img width=""50%"" src=""docs/scispacy-logo.png"" /></p>. This repository contains custom pipes and models related to using spaCy for scientific documents. In particular, there is a custom tokenizer that adds tokenization rules on top of spaCy's; rule-based tokenizer, a POS tagger and syntactic parser trained on biomedical data and; an entity span detection model. Separately, there are also NER models for more specific tasks. **Just looking to test out the models on your data? Check out our [demo](https://scispacy.apps.allenai.org)** (Note: this demo is running an older version of scispaCy and may produce different results than the latest version). ## Installation; Installing scispacy requires two steps: installing the library and intalling the models. To install the library, run:; ```bash; pip install scispacy; ```. to install a model (see our full selection of available models below), run a command like the following:. ```bash; pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz; ```. Note: We strongly recommend that you use an isolated Python environment (such as virtualenv or conda) to install scispacy.; Take a look below in the ""Setting up a virtual environment"" section if you need some help with this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------
",False,The content consists of installation instructions and documentation which are meaningful and in human-readable format.
Testability,"ary and intalling the models. To install the library, run:; ```bash; pip install scispacy; ```. to install a model (see our full selection of available models below), run a command like the following:. ```bash; pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz; ```. Note: We strongly recommend that you use an isolated Python environment (such as virtualenv or conda) to install scispacy.; Take a look below in the ""Setting up a virtual environment"" section if you need some help with this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Python 3.11 | ‚ùåüêç | ‚ùåüêç | ‚ùì | ‚ùì | ‚ùì | ‚ùå |; | Python 3.12 | ‚ùåüêç | ‚ùåüêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`; - `pip install --no-binary :all: nmslib`; - `pip install ""nmslib @ git+https://githu",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:1755,test,test,1755,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['test'],['test'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ary and intalling the models. To install the library, run:; ```bash; pip install scispacy; ```. to install a model (see our full selection of available models below), run a command like the following:. ```bash; pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz; ```. Note: We strongly recommend that you use an isolated Python environment (such as virtualenv or conda) to install scispacy.; Take a look below in the ""Setting up a virtual environment"" section if you need some help with this.; Additionally, scispacy uses modern features of Python and as such is only available for **Python 3.6 or greater**. ### Installation note: nmslib; Over the years, installing nmslib has becomes quite difficult. There are a number of GitHub issues on scispaCy and the nmslib repo itself about this. This matrix is an attempt to help users install nmslib in whatever environment they have. I don't have access to every type of environment, so if you are able to test something out, please open an issue or pull request!. | | Windows 11 | Windows Subsystem for Linux | Mac M1 | Mac M2 | Mac M3 | Intel Mac |; |---------------|------------|----------------------------|---------|---------|---------|-----------|; | Python 3.8 | ‚úÖ | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.9 | ‚ùåüêç | ‚úÖ | üíª | ‚ùì | ‚ùì | ‚ùì |; | Python 3.10 | ‚ùåüêç | ‚úÖ | ‚ùì | ‚ùì | ‚ùì | ‚úÖ |; | Python 3.11 | ‚ùåüêç | ‚ùåüêç | ‚ùì | ‚ùì | ‚ùì | ‚ùå |; | Python 3.12 | ‚ùåüêç | ‚ùåüêçüß† | ‚ùì | ‚ùì | ‚ùì | ‚ùì |. ‚úÖ = works normally with pip install of scispacy. ‚ùå = does not work normally with pip install of scispacy. üêç = can be installed with `mamba install nmslib`. üíª = can be installed with `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. üß† = can be installed with `pip install nmslib-metabrainz`. ‚ùì = unconfirmed. Other methods mentioned in GitHub issues, but unconfirmed what versions they work for:; - `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`; - `pip install --no-binary :all: nmslib`; - `pip install ""nmslib @ git+https://githu
",False,"The text contains detailed installation instructions for scispacy and nmslib. It includes step-by-step guides on how to install the library using pip commands, notes on virtual environments, system requirements, and specific compatibility information for different operating systems and Python versions. While there are code snippets like 'pip install' and references to version control ('git+https://github...'), the overall content is explanatory and provides meaningful guidance for users."
Usability,"Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 Span	 Definition; >>> SBMA 		 (33, 34) Spinal and bulbar muscular atrophy; >>> SBMA 	 	 (6, 7) Spinal and bulbar muscular atrophy; >>> AR 		 (29, 30) androgen receptor; ```. > **Note**; > If you want to be able to [serialize your `doc` objects](https://spacy.io/usage/saving-loading), load the abbreviation detector with `make_serializable=True`, e.g. `nlp.add_pipe(""abbreviation_detector"", config={""make_serializable"": True})`. ### EntityLinker. The `EntityLinker` is a SpaCy component which performs linking to a knowledge base. The linker simply performs; a string overlap - based search (char-3grams) on named entities, comparing them with the concepts in a knowledge base; using an approximate nearest neighbours search. Currently (v2.5.0), there are 5 supported linkers:. - `umls`: Links to the [Unified Medical Language System](https://www.nlm.nih.gov/research/umls/index.html), levels 0,1,2 and 9. This has ~3M concepts.; - `mesh`: Links to the [Medical Subject Headings](https://www.nlm.nih.gov/mesh/meshhome.html). This contains a smaller set of higher quality entities, which are used for indexing in Pubmed. MeSH contains ~30k entities. NOTE: The MeSH KB is derived directly from MeSH itself, and as such uses different unique identifiers than the other KBs.; - `rxnorm`: Links to the [RxNorm](https://www.nlm.nih.gov/research/umls/rxnorm/index.html) ontology. RxNorm contains ~100k concepts focused on normalized names for clinical drugs. It is comprised of several other drug vocabularies commonly used in pharmacy management and drug interaction, including First Databank, Micromedex, and the Gold Standard Drug D",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:8214,simpl,simply,8214,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['simpl'],['simply'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 Span	 Definition; >>> SBMA 		 (33, 34) Spinal and bulbar muscular atrophy; >>> SBMA 	 	 (6, 7) Spinal and bulbar muscular atrophy; >>> AR 		 (29, 30) androgen receptor; ```. > **Note**; > If you want to be able to [serialize your `doc` objects](https://spacy.io/usage/saving-loading), load the abbreviation detector with `make_serializable=True`, e.g. `nlp.add_pipe(""abbreviation_detector"", config={""make_serializable"": True})`. ### EntityLinker. The `EntityLinker` is a SpaCy component which performs linking to a knowledge base. The linker simply performs; a string overlap - based search (char-3grams) on named entities, comparing them with the concepts in a knowledge base; using an approximate nearest neighbours search. Currently (v2.5.0), there are 5 supported linkers:. - `umls`: Links to the [Unified Medical Language System](https://www.nlm.nih.gov/research/umls/index.html), levels 0,1,2 and 9. This has ~3M concepts.; - `mesh`: Links to the [Medical Subject Headings](https://www.nlm.nih.gov/mesh/meshhome.html). This contains a smaller set of higher quality entities, which are used for indexing in Pubmed. MeSH contains ~30k entities. NOTE: The MeSH KB is derived directly from MeSH itself, and as such uses different unique identifiers than the other KBs.; - `rxnorm`: Links to the [RxNorm](https://www.nlm.nih.gov/research/umls/rxnorm/index.html) ontology. RxNorm contains ~100k concepts focused on normalized names for clinical drugs. It is comprised of several other drug vocabularies commonly used in pharmacy management and drug interaction, including First Databank, Micromedex, and the Gold Standard Drug D
",True,The content consists primarily of code snippets or program logs.
Usability,"h this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Additional Pipeline Components. ### AbbreviationDetector; The AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm; for identifying abbreviation definitions in biomedical text."", (Schwartz & Hearst, 2003). You can access the list of abbreviations via the `doc._.abbreviations` attribute and for a given abbreviation,; you can access it's long form (which is a `spacy.tokens.Span`) using `span._.long_form`, which will point to; another span in the document. #### Example Usage; ```python; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 S",MatchSource.DOCS,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/tree/v0.5.5/README.md:6700,simpl,simple,6700,README.md,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/tree/v0.5.5/README.md,1,['simpl'],['simple'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: h this model. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz)|; | en_ner_craft_md| A spaCy NER model trained on the CRAFT corpus.|[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz)|; | en_ner_jnlpba_md | A spaCy NER model trained on the JNLPBA corpus.| [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz)|; | en_ner_bc5cdr_md | A spaCy NER model trained on the BC5CDR corpus. | [Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz)|; | en_ner_bionlp13cg_md | A spaCy NER model trained on the BIONLP13CG corpus. |[Download](https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz)|. ## Additional Pipeline Components. ### AbbreviationDetector; The AbbreviationDetector is a Spacy component which implements the abbreviation detection algorithm in ""A simple algorithm; for identifying abbreviation definitions in biomedical text."", (Schwartz & Hearst, 2003). You can access the list of abbreviations via the `doc._.abbreviations` attribute and for a given abbreviation,; you can access it's long form (which is a `spacy.tokens.Span`) using `span._.long_form`, which will point to; another span in the document. #### Example Usage; ```python; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 S
",False,"The text contains complete, meaningful sentences in natural language discussing the functionality and usage of an abbreviation detector component."
