quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,filename,wiki,url,total_similar,target_keywords,target_matched_words,format_prompt,to_eliminate,reason
Availability," PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Downloa",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:1319,down,download,1319,docs/deepvariant-pacbio-model-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md,1,['down'],['download'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Downloa
",False,
Availability," curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepVariant with one command. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can now run DeepVariant with one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvarian",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:1655,avail,available,1655,docs/deepvariant-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md,1,['avail'],['available'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepVariant with one command. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can now run DeepVariant with one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvarian
",True,"The text contains code snippets (curl commands, bash scripts), logs or API descriptions, which are primarily used for programmatic operations and not for human-readable purposes."
Availability," email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure; ```. NOTE: To create an instance *without GPU*, simply omit the last line from the; command. Check that the instance has been created and started:. ```shell; gcloud compute instances list; ```. which should produce output like:. ```; NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS; [USER]-deepvariant-quickstart us-west1-b n1-standard-8 10.138.0.4 35.185.203.59 RUNNING; ```. Then connect to your instance via SSH:. ```shell; gcloud compute ssh --zone us-west1-b ""${USER}-deepvariant-quickstart""; ```. You should land at a shell prompt in your new instance!. NOTE: All of these steps can also be completed from the Cloud Console, if you; prefer. Consult [this; guide](https://cloud.google.com/compute/docs/quickstart-linux), but be sure to; choose Ubuntu 20.04 as your image, as DeepVariant has",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:4123,mainten,maintenance-policy,4123,docs/deepvariant-gcp-info.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md,2,"['failure', 'mainten']","['failure', 'maintenance-policy']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure; ```. NOTE: To create an instance *without GPU*, simply omit the last line from the; command. Check that the instance has been created and started:. ```shell; gcloud compute instances list; ```. which should produce output like:. ```; NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS; [USER]-deepvariant-quickstart us-west1-b n1-standard-8 10.138.0.4 35.185.203.59 RUNNING; ```. Then connect to your instance via SSH:. ```shell; gcloud compute ssh --zone us-west1-b ""${USER}-deepvariant-quickstart""; ```. You should land at a shell prompt in your new instance!. NOTE: All of these steps can also be completed from the Cloud Console, if you; prefer. Consult [this; guide](https://cloud.google.com/compute/docs/quickstart-linux), but be sure to; choose Ubuntu 20.04 as your image, as DeepVariant has
",False,
Availability," free to change.**; --logging_dir=/output/logs \ **Optional. This saves the log output for each stage separately.; --haploid_contigs=""chrX,chrY"" \ **Optional. Heterozygous variants in these contigs will be re-genotyped as the most likely of reference or homozygous alternates. For a sample with karyotype XY, it should be set to ""chrX,chrY"" for GRCh38 and ""X,Y"" for GRCh37. For a sample with karyotype XX, this should not be used.; --par_regions_bed=""/input/GRCh3X_par.bed"" \ **Optional. If --haploid_contigs is set, then this can be used to provide PAR regions to be excluded from genotype adjustment. Download links to this files are available in this page.; --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.; ```. For details on X,Y support, please see; [DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case; study in; [DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You; can download the PAR bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). To see all flags you can use, run: `docker run; google/deepvariant:""${BIN_VERSION}""`. If you're using GPUs, or want to use Singularity instead, see; [Quick Start](docs/deepvariant-quick-start.md) for more details or see all the; [setup options](#deepvariant_setup) available. For more information, also see:. * [Full documentation list](docs/README.md); * [Detailed usage guide](docs/deepvariant-details.md) with more information on; the input and output file formats and how to work with them.; * [Best practices for multi-sample variant calling with DeepVariant](docs/trio-merge-case-study.md); * [(Advanced) Training tutorial](docs/deepvariant-training-case-study.md); * [DeepVariant's Frequently Asked Questions, FAQ](docs/FAQ.md). ## How to cite. If you're using DeepVariant in your",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:4807,down,download,4807,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['down'],['download'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  free to change.**; --logging_dir=/output/logs \ **Optional. This saves the log output for each stage separately.; --haploid_contigs=""chrX,chrY"" \ **Optional. Heterozygous variants in these contigs will be re-genotyped as the most likely of reference or homozygous alternates. For a sample with karyotype XY, it should be set to ""chrX,chrY"" for GRCh38 and ""X,Y"" for GRCh37. For a sample with karyotype XX, this should not be used.; --par_regions_bed=""/input/GRCh3X_par.bed"" \ **Optional. If --haploid_contigs is set, then this can be used to provide PAR regions to be excluded from genotype adjustment. Download links to this files are available in this page.; --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.; ```. For details on X,Y support, please see; [DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case; study in; [DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You; can download the PAR bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). To see all flags you can use, run: `docker run; google/deepvariant:""${BIN_VERSION}""`. If you're using GPUs, or want to use Singularity instead, see; [Quick Start](docs/deepvariant-quick-start.md) for more details or see all the; [setup options](#deepvariant_setup) available. For more information, also see:. * [Full documentation list](docs/README.md); * [Detailed usage guide](docs/deepvariant-details.md) with more information on; the input and output file formats and how to work with them.; * [Best practices for multi-sample variant calling with DeepVariant](docs/trio-merge-case-study.md); * [(Advanced) Training tutorial](docs/deepvariant-training-case-study.md); * [DeepVariant's Frequently Asked Questions, FAQ](docs/FAQ.md). ## How to cite. If you're using DeepVariant in your
",False,"The text contains complete, meaningful sentences in natural language that discuss concepts and provide guidance on using the tool, such as explaining how flags are used, what steps to take for X,Y calling support, and references to documentation. There are no code snippets, logs, API docs, or other programmatic content present. Instead, it's explanatory prose about the software's features and usage."
Availability," or other similar arguments these should; refer to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and; `confident_regions` arguments should point to VCF and BED files containing the; true variants and regions where we are confident in our calls (i.e., calls; within these regions and not in the truth_vcf are considered false positives).; These should be bgzipped and tabix indexed and be on a reference consistent with; the one provided with the `--ref` argument. ### call_variants. `call_variants` consumes TFRecord file(s) of tf.Examples protos created; by `make_examples` and a deep learning model checkpoint and evaluates the model; on each example in the input TFRecord. The output here is a TFRecord of; CallVariantsOutput protos. `call_variants` doesn't directly support sharding its; outputs, but accepts a glob or shard-pattern for its inputs. `call_variants` uses around 4 GB per process and uses TensorFlow for evaluation.; When evaluating a model in CPU mode, TensorFlow can make use of multiple cores,; but scaling is sub-linear. In other words, `call_variants` on a 64 core machine; is less than 8x faster than running on a 8 core machine. When using a GPU, `call_variants` is both faster, more efficient, and needs; fewer CPUs. Based on a small number of experiments, currently the most efficient; configuration for `call_variants` on a GPU instance is 4-8 CPUs and 1 GPU.; Compared to our setting in the [whole genome case study], we noticed a 2.5x; speedup on the call_variants step using a single P100 GPU and 8 CPUs. Note that; currently `call_variants` can only use one GPU at most. ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:4883,checkpoint,checkpoint,4883,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['checkpoint'],['checkpoint'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  or other similar arguments these should; refer to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and; `confident_regions` arguments should point to VCF and BED files containing the; true variants and regions where we are confident in our calls (i.e., calls; within these regions and not in the truth_vcf are considered false positives).; These should be bgzipped and tabix indexed and be on a reference consistent with; the one provided with the `--ref` argument. ### call_variants. `call_variants` consumes TFRecord file(s) of tf.Examples protos created; by `make_examples` and a deep learning model checkpoint and evaluates the model; on each example in the input TFRecord. The output here is a TFRecord of; CallVariantsOutput protos. `call_variants` doesn't directly support sharding its; outputs, but accepts a glob or shard-pattern for its inputs. `call_variants` uses around 4 GB per process and uses TensorFlow for evaluation.; When evaluating a model in CPU mode, TensorFlow can make use of multiple cores,; but scaling is sub-linear. In other words, `call_variants` on a 64 core machine; is less than 8x faster than running on a 8 core machine. When using a GPU, `call_variants` is both faster, more efficient, and needs; fewer CPUs. Based on a small number of experiments, currently the most efficient; configuration for `call_variants` on a GPU instance is 4-8 CPUs and 1 GPU.; Compared to our setting in the [whole genome case study], we noticed a 2.5x; speedup on the call_variants step using a single P100 GPU and 8 CPUs. Note that; currently `call_variants` can only use one GPU at most. 
",False,
Availability," the `make_examples` program has a flag; `--gvcf_gq_binsize <int>`. This flag allows the merging of adjacent records that; all have GQ values within a bin of the given size, and for each record emits the; minimum GQ value seen within the bin. For example, setting `--gvcf_gq_binsize 5` has the effect that adjacent records; with GQ=0; GQ in [1, 5]; GQ in [6, 10]; GQ in [11, 15]; etc. are binned; together. A concrete example shown below has non-variant sites at each of positions 1-9 on; a hypothetical chromosome:. ```bash; Example input records:; Genome position | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |; GQ of position | 8 | 10 | 9 | 27 | 47 | 50 | 50 | 45 | 33 |; ```. They would create five resultant gVCF record values with `--gvcf_gq_binsize 5`,; with relevant values of:. ```bash; start | INFO | GQ; ------------------; 1 | END=3 | 8; 4 | END=4 | 27; 5 | END=7 | 47; 8 | END=8 | 45; 9 | END=9 | 33; ```. By synthetically downsampling a 50x coverage whole genome and applying different; GQ binning strategies, we see how the size of the resultant data varies as the; two factors change. The below figure shows the size of output (measured as the; number of records generated relative to the baseline of a 50x whole genome with; `--gvcf_gq_binsize 1`) at different coverage levels, for GQ bins of size 1, 3,; 5, and 10. The value of each bar is written in blue font above it for clarity. ![gVCF size](images/DeepVariant-gvcf-sizes-figure.png?raw=true ""DeepVariant gVCF sizes""). ### Runtime. Despite the creation of many additional records, the running time of; `make_examples` increases minimally when gVCF support is enabled. The; single-threaded `postprocess_variants` program is more adversely affected, with; observed runtimes increasing on the [WGS case study] from ~25 minutes to 5-7; hours depending on genome coverage. ### New option to include MED_DP. Starting in v1.2.0, we added a flag to enable adding MED_DP (median read; coverage seen in the block) in addition to the default MIN_DP",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:5571,down,downsampling,5571,docs/deepvariant-gvcf-support.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md,1,['down'],['downsampling'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  the `make_examples` program has a flag; `--gvcf_gq_binsize <int>`. This flag allows the merging of adjacent records that; all have GQ values within a bin of the given size, and for each record emits the; minimum GQ value seen within the bin. For example, setting `--gvcf_gq_binsize 5` has the effect that adjacent records; with GQ=0; GQ in [1, 5]; GQ in [6, 10]; GQ in [11, 15]; etc. are binned; together. A concrete example shown below has non-variant sites at each of positions 1-9 on; a hypothetical chromosome:. ```bash; Example input records:; Genome position | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |; GQ of position | 8 | 10 | 9 | 27 | 47 | 50 | 50 | 45 | 33 |; ```. They would create five resultant gVCF record values with `--gvcf_gq_binsize 5`,; with relevant values of:. ```bash; start | INFO | GQ; ------------------; 1 | END=3 | 8; 4 | END=4 | 27; 5 | END=7 | 47; 8 | END=8 | 45; 9 | END=9 | 33; ```. By synthetically downsampling a 50x coverage whole genome and applying different; GQ binning strategies, we see how the size of the resultant data varies as the; two factors change. The below figure shows the size of output (measured as the; number of records generated relative to the baseline of a 50x whole genome with; `--gvcf_gq_binsize 1`) at different coverage levels, for GQ bins of size 1, 3,; 5, and 10. The value of each bar is written in blue font above it for clarity. ![gVCF size](images/DeepVariant-gvcf-sizes-figure.png?raw=true ""DeepVariant gVCF sizes""). ### Runtime. Despite the creation of many additional records, the running time of; `make_examples` increases minimally when gVCF support is enabled. The; single-threaded `postprocess_variants` program is more adversely affected, with; observed runtimes increasing on the [WGS case study] from ~25 minutes to 5-7; hours depending on genome coverage. ### New option to include MED_DP. Starting in v1.2.0, we added a flag to enable adding MED_DP (median read; coverage seen in the block) in addition to the default MIN_DP
",False,
Availability," the cheapest possible configuration. The resulting model also does not represent; the greatest achievable accuracy for BGISEQ-500 data. ## High level summary of result. We demonstrated that by training on 1 replicate of BGISEQ-500 whole genome data; (everything except for chromosome 20-22), we can significantly improve the; accuracy comparing to the WGS model as a baseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; following command:. ```bash; gcloud compute ssh ${host} --zone ${zone}; ```. Once you have logged in, set the variables:. ```bash; YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT; OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant""; VERSION=""1.6.1""; DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard""; GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study""; DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input""; BIN_DIR=""${INPUT_DIR}/bin""",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:1635,mainten,maintenance-policy,1635,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['mainten'],['maintenance-policy'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  the cheapest possible configuration. The resulting model also does not represent; the greatest achievable accuracy for BGISEQ-500 data. ## High level summary of result. We demonstrated that by training on 1 replicate of BGISEQ-500 whole genome data; (everything except for chromosome 20-22), we can significantly improve the; accuracy comparing to the WGS model as a baseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; following command:. ```bash; gcloud compute ssh ${host} --zone ${zone}; ```. Once you have logged in, set the variables:. ```bash; YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT; OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant""; VERSION=""1.6.1""; DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard""; GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study""; DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input""; BIN_DIR=""${INPUT_DIR}/bin""
",False,
Availability," we describe applying DeepVariant to Oxford Nanopore R10.4.1; duplex reads. Then we assess the quality of the DeepVariant variant calls with; `hap.py`. To make it faster to go over this case study, we run only on chromosome 20. The dataset used in this case-study has following attributes:. ```bash; Sample: HG002; Region: Chr20; Chemistry: ONT R10.4.1 Duplex; Basecaller: Dorado v0.1.1; Coverage: 80x; ```. **Model note:**. * The model is trained with Guppy 6+ ""SUP"" Simplex and Dorado v0.1.1 Duplex; reads. * The model is trained on both Ultra-long and sheared reads with varying read; N50 and coverage. ## Prepare environment. In this case-study, we will use [Docker](https://docs.docker.com/get-docker/) to; run DeepVariant for variant calling and; [hap.py](https://github.com/illumina/hap.py) for benchmarking. If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/ont-case-study-duplex"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input/data""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG002 Duplex chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam > ${INPUT_DIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam; curl ${HTTPDI",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md:1095,down,download,1095,docs/deepvariant-ont-r104-duplex-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md,1,['down'],['download'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  we describe applying DeepVariant to Oxford Nanopore R10.4.1; duplex reads. Then we assess the quality of the DeepVariant variant calls with; `hap.py`. To make it faster to go over this case study, we run only on chromosome 20. The dataset used in this case-study has following attributes:. ```bash; Sample: HG002; Region: Chr20; Chemistry: ONT R10.4.1 Duplex; Basecaller: Dorado v0.1.1; Coverage: 80x; ```. **Model note:**. * The model is trained with Guppy 6+ ""SUP"" Simplex and Dorado v0.1.1 Duplex; reads. * The model is trained on both Ultra-long and sheared reads with varying read; N50 and coverage. ## Prepare environment. In this case-study, we will use [Docker](https://docs.docker.com/get-docker/) to; run DeepVariant for variant calling and; [hap.py](https://github.com/illumina/hap.py) for benchmarking. If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/ont-case-study-duplex"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input/data""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG002 Duplex chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam > ${INPUT_DIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam; curl ${HTTPDI
",True,"The content primarily consists of code snippets and commands, which are marked by bash syntax and code blocks. There are detailed steps for setting up directories and downloading files using curl commands, along with references to specific tools like Docker and hap.py. The text lacks meaningful human-readable sentences discussing concepts or ideas beyond the procedural instructions."
Availability,"# Calling variants in non-autosomal contigs. For details about the support for haploid contigs, please read; [DeepVariant haploid support](deepvariant-haploid-support.md). In this case study, we describe how to call variants in non-autosomal regions; like X, Y chromosomes. Then we assess the quality of the DeepVariant variant; calls with `hap.py`. The dataset used in this case-study has following attributes:. ```bash; Sample: HG002; Region: ChrX, ChrY; Platform: PacBio; Sample Karyotype: X, Y; ```. ## Prepare environment. In this case study, we will use [Docker](https://docs.docker.com/get-docker/) to; run DeepVariant for variant calling and; [hap.py](https://github.com/illumina/hap.py) for benchmarking. If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/XY-walkthrough"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}/data"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/xy-case-study-testdata; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai. HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata; curl ${HTTPDIR}",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md:917,down,download,917,docs/deepvariant-xy-calling-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md,1,['down'],['download'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # Calling variants in non-autosomal contigs. For details about the support for haploid contigs, please read; [DeepVariant haploid support](deepvariant-haploid-support.md). In this case study, we describe how to call variants in non-autosomal regions; like X, Y chromosomes. Then we assess the quality of the DeepVariant variant; calls with `hap.py`. The dataset used in this case-study has following attributes:. ```bash; Sample: HG002; Region: ChrX, ChrY; Platform: PacBio; Sample Karyotype: X, Y; ```. ## Prepare environment. In this case study, we will use [Docker](https://docs.docker.com/get-docker/) to; run DeepVariant for variant calling and; [hap.py](https://github.com/illumina/hap.py) for benchmarking. If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/XY-walkthrough"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}/data"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/xy-case-study-testdata; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai. HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata; curl ${HTTPDIR}
",False,"The text contains detailed, step-by-step instructions for downloading and setting up data necessary for variant calling. It includes commands for creating directories, downloading files via FTP and HTTP, which are programming steps rather than general explanatory content."
Availability,"# DeepTrio quick start. This document explains how to quickly start using; [DeepTrio](deeptrio-details.md) to generate variant calls for trio samples. This; tutorial does not cover all possible settings of DeepTrio. It is intended to be; a starting point for using DeepTrio. ## Background. To get started, we've provided a Docker image, and some test data in a bucket on; Google Cloud Storage. The instructions below show how to download the data; through the corresponding public URLs. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepTrio in one command. Although DeepTrio can be built from a source, we provide a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test b",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:429,down,download,429,docs/deeptrio-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md,1,['down'],['download'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # DeepTrio quick start. This document explains how to quickly start using; [DeepTrio](deeptrio-details.md) to generate variant calls for trio samples. This; tutorial does not cover all possible settings of DeepTrio. It is intended to be; a starting point for using DeepTrio. ## Background. To get started, we've provided a Docker image, and some test data in a bucket on; Google Cloud Storage. The instructions below show how to download the data; through the corresponding public URLs. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepTrio in one command. Although DeepTrio can be built from a source, we provide a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test b
",False,"The text contains complete, meaningful sentences discussing how to quickly start using DeepTrio, including setup instructions and prerequisites. It includes explanatory content on downloading data and Docker usage, which are human-readable and not code or logs."
Availability,"# DeepVariant RNA-seq Case Study. This case study will demonstrate how to run DeepVariant using the RNA-seq model,; and evaluate the result using `hap.py`. ## Overview. ### Tools. We will use the following tools:. * [Docker](https://docs.docker.com/get-docker/) - Used to run DeepVariant.; * [mosdepth](https://github.com/brentp/mosdepth) - For calculating coverage.; * [bedtools](https://bedtools.readthedocs.io) - Used to intersect bedfiles.; * [hap.py](https://github.com/illumina/hap.py) - Used to evaluate the results.; We will use Docker to run `hap.py`. ### Data. We will use these data in our analysis. Files will be downloaded in subsequent; steps. * HG005 RNA-seq BAM; * Model Checkpoint Files; * GRCh38 Reference + Index; * CDS bedfile (chr20 only); * GIAB benchmark data. ## Prepare Data. ### Setup directories. Lets first create directories to organize files. ```bash; mkdir -p data benchmark reference model output happy; ```. ### Download the GRCh38 Reference. We will be using GRCh38 for this case study. ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:625,down,downloaded,625,docs/deepvariant-rnaseq-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md,1,['down'],['downloaded'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # DeepVariant RNA-seq Case Study. This case study will demonstrate how to run DeepVariant using the RNA-seq model,; and evaluate the result using `hap.py`. ## Overview. ### Tools. We will use the following tools:. * [Docker](https://docs.docker.com/get-docker/) - Used to run DeepVariant.; * [mosdepth](https://github.com/brentp/mosdepth) - For calculating coverage.; * [bedtools](https://bedtools.readthedocs.io) - Used to intersect bedfiles.; * [hap.py](https://github.com/illumina/hap.py) - Used to evaluate the results.; We will use Docker to run `hap.py`. ### Data. We will use these data in our analysis. Files will be downloaded in subsequent; steps. * HG005 RNA-seq BAM; * Model Checkpoint Files; * GRCh38 Reference + Index; * CDS bedfile (chr20 only); * GIAB benchmark data. ## Prepare Data. ### Setup directories. Lets first create directories to organize files. ```bash; mkdir -p data benchmark reference model output happy; ```. ### Download the GRCh38 Reference. We will be using GRCh38 for this case study. ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}
",False,"The content includes detailed, step-by-step instructions on how to prepare and run DeepVariant for an RNA-seq case study. It uses bash commands for directory creation and file downloads, but the overall text is explanatory and instructional, which falls under human-readable prose."
Availability,"# DeepVariant quick start. This is an explanation of how to use DeepVariant. ## Background. To get started, you'll need the DeepVariant programs (and some packages they; depend on), some test data, and of course a place to run them. We've provided a Docker image, and some test data in a bucket on Google Cloud; Storage. The instructions below show how to download the data through the; corresponding public URLs from these data. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepVariant in one command. Starting from the 0.8 release, we introduced one convenient command that will; run through all 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:356,down,download,356,docs/deepvariant-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md,1,['down'],['download'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # DeepVariant quick start. This is an explanation of how to use DeepVariant. ## Background. To get started, you'll need the DeepVariant programs (and some packages they; depend on), some test data, and of course a place to run them. We've provided a Docker image, and some test data in a bucket on Google Cloud; Storage. The instructions below show how to download the data through the; corresponding public URLs from these data. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepVariant in one command. Starting from the 0.8 release, we introduced one convenient command that will; run through all 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; 
",False,"The text contains several paragraphs explaining how to use DeepVariant, including steps like downloading Docker images, installing dependencies, and setting up data files. It uses natural language without code syntax or logs."
Availability,"# Using DeepVariant for small variant calling from PacBio HiFi reads. #### Author: William Rowell <wrowell@pacificbiosciences.com>. In this case study we describe applying DeepVariant to PacBio HiFi reads to call; variants. We will call small variants from a publicly available whole genome; HiFi dataset from PacBio. Starting in v1.4.0, PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchma",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:268,avail,available,268,docs/deepvariant-pacbio-model-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md,1,['avail'],['available'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # Using DeepVariant for small variant calling from PacBio HiFi reads. #### Author: William Rowell <wrowell@pacificbiosciences.com>. In this case study we describe applying DeepVariant to PacBio HiFi reads to call; variants. We will call small variants from a publicly available whole genome; HiFi dataset from PacBio. Starting in v1.4.0, PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchma
",False,"The text contains complete, meaningful sentences in natural language discussing the application of DeepVariant for variant calling from PacBio HiFi reads, including methodology and steps taken, such as preparing environments, downloading references, and benchmarking. There is no code or logs present, and it is written in a human-readable prose format."
Availability,"# Using graph genomes: VG Giraffe + DeepVariant case study; ---. This is an example to run `vg giraffe`, so we can go from FASTQs --> BAM. For simplicity and consistency, we run the following with a; [Google Cloud instance with 64 cores](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). I added more disks because 300G is not enough for the example below. I changed; it to `--boot-disk-size ""1000""`. ## Install softwares that will be used later. ```bash; sudo apt update -y; sudo apt-get -y install aria2 docker.io samtools; ```. ## Download input FASTQ files. ```bash; DATA_DIR=${PWD}/data; mkdir -p ${DATA_DIR}; gcloud storage cp gs://brain-genomics-public/research/sequencing/fastq/novaseq/wgs_pcr_free/35x/HG003.novaseq.pcr-free.35x.R?.fastq.gz ${DATA_DIR}/; ```. ## Download VG files. Get binaries `vg` 1.51.0 and `kmc`:. ```bash; wget https://github.com/refresh-bio/KMC/releases/download/v3.2.2/KMC3.2.2.linux.x64.tar.gz; tar zxf KMC3.2.2.linux.x64.tar.gz bin/kmc; mv bin/kmc ${DATA_DIR}/; wget https://github.com/vgteam/vg/releases/download/v1.51.0/vg -O ${DATA_DIR}/vg; chmod +x ${DATA_DIR}/vg ${DATA_DIR}/kmc; ```. Get the graph (.gbz) and haplotype index (.hapl).; I used `aria2c` to download these files. You can use other approaches as well. ```bash; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.gbz; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.hapl; ```. ## Run `vg giraffe` with one command to get from FASTQs to BAM. Put the paths name into a file named HG003.fq.paths:. ```bash; cat > HG003.fq.paths <<- EOM; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R1.fastq.gz; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R2.fastq.gz; EOM; ```. Run `kmc`` on this file. I used -t$(nproc) to use all cores, and $TMPDIR ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:917,down,download,917,docs/deepvariant-vg-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md,1,['down'],['download'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # Using graph genomes: VG Giraffe + DeepVariant case study; ---. This is an example to run `vg giraffe`, so we can go from FASTQs --> BAM. For simplicity and consistency, we run the following with a; [Google Cloud instance with 64 cores](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). I added more disks because 300G is not enough for the example below. I changed; it to `--boot-disk-size ""1000""`. ## Install softwares that will be used later. ```bash; sudo apt update -y; sudo apt-get -y install aria2 docker.io samtools; ```. ## Download input FASTQ files. ```bash; DATA_DIR=${PWD}/data; mkdir -p ${DATA_DIR}; gcloud storage cp gs://brain-genomics-public/research/sequencing/fastq/novaseq/wgs_pcr_free/35x/HG003.novaseq.pcr-free.35x.R?.fastq.gz ${DATA_DIR}/; ```. ## Download VG files. Get binaries `vg` 1.51.0 and `kmc`:. ```bash; wget https://github.com/refresh-bio/KMC/releases/download/v3.2.2/KMC3.2.2.linux.x64.tar.gz; tar zxf KMC3.2.2.linux.x64.tar.gz bin/kmc; mv bin/kmc ${DATA_DIR}/; wget https://github.com/vgteam/vg/releases/download/v1.51.0/vg -O ${DATA_DIR}/vg; chmod +x ${DATA_DIR}/vg ${DATA_DIR}/kmc; ```. Get the graph (.gbz) and haplotype index (.hapl).; I used `aria2c` to download these files. You can use other approaches as well. ```bash; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.gbz; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.hapl; ```. ## Run `vg giraffe` with one command to get from FASTQs to BAM. Put the paths name into a file named HG003.fq.paths:. ```bash; cat > HG003.fq.paths <<- EOM; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R1.fastq.gz; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R2.fastq.gz; EOM; ```. Run `kmc`` on this file. I used -t$(nproc) to use all cores, and $TMPDIR 
",False,
Availability,"# show_examples: Saving human-readable images from DeepVariant examples. This is a short guide to using the show_examples tool to view the pileup images; used within DeepVariant and save them as PNG image files. This tool is; particularly useful when you want to try to understand how a candidate variant; of interest was represented when it was passed into the neural network. ![An example pileup image](images/example_1.4.0.png). This example was generated with the data from the; [quick start guide](deepvariant-quick-start.md) and the example commands below. For more information on the pileup images and how to read them, please see the; [""Looking through DeepVariant's Eyes"" blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). The `show_examples` tool is introduced in DeepVariant 1.0.0, so it is not; available in older versions, but it will work with make_examples output files; from older versions of DeepVariant. ## Finding the make_examples output tfrecord files. First, find the make_examples.tfrecord.gz files output by DeepVariant during the; make_examples (first) stage. If you followed along with the [quick start guide](deepvariant-quick-start.md); and case studies that used the Docker version, then these files are usually; hidden inside the Docker container. But you can get them exported into the same; output directory where the VCF file appears by adding the following setting in; the `run_deepvariant` command. ```bash; # Add the following to your run_deepvariant command.; --intermediate_results_dir=/output/; ```. Then the make_examples file should appear in the directory docker mounted as; `/output/`. For example, if you followed the; [quick-start documentation](deepvariant-quick-start.md), it looks like this:; `${OUTPUT_DIR}/make_examples.tfrecord-00000-of-00001.gz`. ## Running show_examples. Once you have a make_examples output tfrecord file, then you can run; `show_examples` to see the pileup images inside:. ```ba",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md:858,avail,available,858,docs/show-examples.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md,1,['avail'],['available'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # show_examples: Saving human-readable images from DeepVariant examples. This is a short guide to using the show_examples tool to view the pileup images; used within DeepVariant and save them as PNG image files. This tool is; particularly useful when you want to try to understand how a candidate variant; of interest was represented when it was passed into the neural network. ![An example pileup image](images/example_1.4.0.png). This example was generated with the data from the; [quick start guide](deepvariant-quick-start.md) and the example commands below. For more information on the pileup images and how to read them, please see the; [""Looking through DeepVariant's Eyes"" blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). The `show_examples` tool is introduced in DeepVariant 1.0.0, so it is not; available in older versions, but it will work with make_examples output files; from older versions of DeepVariant. ## Finding the make_examples output tfrecord files. First, find the make_examples.tfrecord.gz files output by DeepVariant during the; make_examples (first) stage. If you followed along with the [quick start guide](deepvariant-quick-start.md); and case studies that used the Docker version, then these files are usually; hidden inside the Docker container. But you can get them exported into the same; output directory where the VCF file appears by adding the following setting in; the `run_deepvariant` command. ```bash; # Add the following to your run_deepvariant command.; --intermediate_results_dir=/output/; ```. Then the make_examples file should appear in the directory docker mounted as; `/output/`. For example, if you followed the; [quick-start documentation](deepvariant-quick-start.md), it looks like this:; `${OUTPUT_DIR}/make_examples.tfrecord-00000-of-00001.gz`. ## Running show_examples. Once you have a make_examples output tfrecord file, then you can run; `show_examples` to see the pileup images inside:. ```ba
",False,"The text contains complete sentences discussing how to use the show_examples tool in DeepVariant. It includes instructions for saving images and using the tool with examples, along with references to documentation and a blog post. These are meaningful, explanatory, and human-readable content."
Availability,"## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md:390,down,downloads,390,docs/deepvariant-build-test.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md,2,['down'],['downloads'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; 
",False,"The text describes step-by-step instructions for building and configuring DeepVariant on Ubuntu, including commands to install prerequisites and build scripts. While it includes some shell commands and technical details, it's written in a human-readable format that explains how to perform tasks, rather than being purely code or logs."
Availability,"). ## Hybrid (Illumina + PacBio HiFi). ### Runtime. Runtime is on HG003 (all chromosomes). Stage | Time (minutes); -------------------------------- | -------------------; make_examples | ~172m; call_variants | ~211m; postprocess_variants (with gVCF) | ~24m; total | ~407m = ~6.78 hours. ### Accuracy. Evaluating on HG003 (all chromosomes, using NIST v4.2.1 truth), which was held; out while training the hybrid model. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 503014 | 1487 | 2767 | 0.997053 | 0.994781 | 0.995916 |; | SNP | 3323624 | 3871 | 2273 | 0.998837 | 0.999317 | 0.999077 |. [See VCF stats report.](https://storage.googleapis.com/deepvariant/visual_reports/DeepVariant/1.6.1/HYBRID/deepvariant.output.visual_report.html). ## Inspect outputs that produced the metrics above. The DeepVariant VCFs, gVCFs, and hap.py evaluation outputs are available at:. ```; gs://deepvariant/case-study-outputs; ```. You can also inspect them in a web browser here:; https://42basepairs.com/browse/gs/deepvariant/case-study-outputs. ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; # Get the script.; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deepvariant.sh. # WGS; bash inference_deepvariant.sh --model_preset WGS. # WES; bash inference_deepvariant.sh --model_preset WES. # PacBio; bash inference_deepvariant.sh --model_preset PACBIO. # ONT_R104; bash inference_deepvariant.sh --model_preset ONT_R104. # Hybrid; bash inference_deepvariant.sh --model_preset HY",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md:4553,avail,available,4553,docs/metrics.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md,1,['avail'],['available'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ). ## Hybrid (Illumina + PacBio HiFi). ### Runtime. Runtime is on HG003 (all chromosomes). Stage | Time (minutes); -------------------------------- | -------------------; make_examples | ~172m; call_variants | ~211m; postprocess_variants (with gVCF) | ~24m; total | ~407m = ~6.78 hours. ### Accuracy. Evaluating on HG003 (all chromosomes, using NIST v4.2.1 truth), which was held; out while training the hybrid model. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 503014 | 1487 | 2767 | 0.997053 | 0.994781 | 0.995916 |; | SNP | 3323624 | 3871 | 2273 | 0.998837 | 0.999317 | 0.999077 |. [See VCF stats report.](https://storage.googleapis.com/deepvariant/visual_reports/DeepVariant/1.6.1/HYBRID/deepvariant.output.visual_report.html). ## Inspect outputs that produced the metrics above. The DeepVariant VCFs, gVCFs, and hap.py evaluation outputs are available at:. ```; gs://deepvariant/case-study-outputs; ```. You can also inspect them in a web browser here:; https://42basepairs.com/browse/gs/deepvariant/case-study-outputs. ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; # Get the script.; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deepvariant.sh. # WGS; bash inference_deepvariant.sh --model_preset WGS. # WES; bash inference_deepvariant.sh --model_preset WES. # PacBio; bash inference_deepvariant.sh --model_preset PACBIO. # ONT_R104; bash inference_deepvariant.sh --model_preset ONT_R104. # Hybrid; bash inference_deepvariant.sh --model_preset HY
",False,
Availability,"-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). The steps in this document can be extended to merge larger cohorts as well. See this workflow:. ![workflow](images/cohort-workflow.png?raw=true ""DeepVariant+GLnexus cohort workflow""). A few things to note before we start:. * It is recommended to use BAM files with original quality scores. In the case; that BAM files went through recalibration, optional DV flags can be used in; order to use original scores: `--parse_sam_aux_fields`,; `--use_original_quality_scores`.; * DeepVariant optionally allows gVCF output. This option is required for; further GLnexus analysis in this document. ## Dataset. The Whole Exome Sequencing (WES) dataset we're using is from:. [ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/). * HG002_NA24385_son; * HG003_NA24149_father; * HG004_NA24143_mother. ### Commands for downloading the input BAMs. Just for convenience, we use aria2 to download our data. You can change it to; whatever other tools (wget, curl) that you prefer. To install aria2, you can run: `sudo apt-get -y install aria2`. ```; DIR=""${PWD}/trio""; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam -o HG002.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai -o HG002.bai; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bam -o HG003.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncb",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:1888,down,download,1888,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['down'],['download'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: -details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). The steps in this document can be extended to merge larger cohorts as well. See this workflow:. ![workflow](images/cohort-workflow.png?raw=true ""DeepVariant+GLnexus cohort workflow""). A few things to note before we start:. * It is recommended to use BAM files with original quality scores. In the case; that BAM files went through recalibration, optional DV flags can be used in; order to use original scores: `--parse_sam_aux_fields`,; `--use_original_quality_scores`.; * DeepVariant optionally allows gVCF output. This option is required for; further GLnexus analysis in this document. ## Dataset. The Whole Exome Sequencing (WES) dataset we're using is from:. [ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/). * HG002_NA24385_son; * HG003_NA24149_father; * HG004_NA24143_mother. ### Commands for downloading the input BAMs. Just for convenience, we use aria2 to download our data. You can change it to; whatever other tools (wget, curl) that you prefer. To install aria2, you can run: `sudo apt-get -y install aria2`. ```; DIR=""${PWD}/trio""; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam -o HG002.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai -o HG002.bai; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bam -o HG003.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncb
",True,"The text contains code snippets or command lines, which are primarily for program execution. The use of commands like `aria2c` and code syntax such as `${DIR}` suggests it is related to build or download processes, fitting under the criteria for elimination."
Availability,". curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. # Extract chr20 CDS regions and convert to bed file.; gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \; awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \; awk '!dup[$0]++' > data/chr20_CDS.bed; ```. ### Download HG005 BAM. We'll use HG005 poly-A selected Illumina RNA-seq reads that are publicly; available. ```bash; HTTPDIR=https://storage.googleapis.com/brain-genomics-public/research/sequencing/grch38/bam/rna/illumina/mrna. curl -L ${HTTPDIR}/hg005_gm26107.mrna.grch38.bam > data/hg005_gm26107.mrna.grch38.bam; curl -L ${HTTPDIR}/hg005_gm26107.mrna.grch38.bam.bai > data/hg005_gm26107.mrna.grch38.bam.bai; ```. ### Generate a 3x coverage file. RNA-seq data is only observed in regions that are expressed in a given sample.; Therefore, we will restrict our evaluation to regions of the BAM file that reach; a minimum threshold of 3x in our truth dataset intersected with the confident; GIAB regions. This allows us to better evaluate the accuracy of the model when; it is feasible for a variant to be called from RNA-seq data. ```bash; # Generate a coverage file, and filter for 3x.; sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; -it quay.io/biocontainers/mosdepth:0.3.1--h4dc83fb_1 \; mosdepth \; --threads $(nproc) \; data/hg005_coverage \; data/hg005_",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:2906,avail,available,2906,docs/deepvariant-rnaseq-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md,1,['avail'],['available'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: . curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. # Extract chr20 CDS regions and convert to bed file.; gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \; awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \; awk '!dup[$0]++' > data/chr20_CDS.bed; ```. ### Download HG005 BAM. We'll use HG005 poly-A selected Illumina RNA-seq reads that are publicly; available. ```bash; HTTPDIR=https://storage.googleapis.com/brain-genomics-public/research/sequencing/grch38/bam/rna/illumina/mrna. curl -L ${HTTPDIR}/hg005_gm26107.mrna.grch38.bam > data/hg005_gm26107.mrna.grch38.bam; curl -L ${HTTPDIR}/hg005_gm26107.mrna.grch38.bam.bai > data/hg005_gm26107.mrna.grch38.bam.bai; ```. ### Generate a 3x coverage file. RNA-seq data is only observed in regions that are expressed in a given sample.; Therefore, we will restrict our evaluation to regions of the BAM file that reach; a minimum threshold of 3x in our truth dataset intersected with the confident; GIAB regions. This allows us to better evaluate the accuracy of the model when; it is feasible for a variant to be called from RNA-seq data. ```bash; # Generate a coverage file, and filter for 3x.; sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; -it quay.io/biocontainers/mosdepth:0.3.1--h4dc83fb_1 \; mosdepth \; --threads $(nproc) \; data/hg005_coverage \; data/hg005_
",True,"The text consists of shell commands and script snippets, which are primarily code. It includes command lines with syntax like curl, bash, and use of variables. These are indicative of programmatic instructions rather than meaningful human-readable sentences."
Availability,".init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cl",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14612,checkpoint,checkpoint,14612,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['checkpoint'],['checkpoint'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: .init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cl
",False,"The text contains multiple sentences discussing the training process, dataset statistics, validation frequency, checkpoint storage conditions, and runtime information for different GPU configurations. These are all human-readable and meaningful explanations that provide guidance on how to conduct the training effectively. The content includes natural language descriptions of parameters, configurations, and best practices, which should not be eliminated."
Availability,"/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. # Extract chr20 CDS regions and convert to bed file.; gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \; awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \; awk '!dup[$0]++' > data/chr20_CDS.bed; ```. ### Download HG005 BAM. We'll use HG005 poly-A selected Illumina RNA-seq reads that are publicly; available. ```bash; HTTPDIR=https://storage.googleapis.com/brain-genomics-public/research/sequencing/grch38/bam/rna/illumina/mrna. curl -L ${HTTPDIR}/hg005_gm26107.mrna.grch38.bam > data/hg005_gm26107.mrna.grch38.bam; curl -L ${HTTPDIR}/hg005_gm26107.mrna.grch38.bam.bai > data/hg005_gm26107.mrna.grch38.bam.bai; ```. ### Generate a 3x coverage file. RNA-seq data is only observed in ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:2281,down,download,2281,docs/deepvariant-rnaseq-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md,1,['down'],['download'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: /GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. # Extract chr20 CDS regions and convert to bed file.; gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \; awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \; awk '!dup[$0]++' > data/chr20_CDS.bed; ```. ### Download HG005 BAM. We'll use HG005 poly-A selected Illumina RNA-seq reads that are publicly; available. ```bash; HTTPDIR=https://storage.googleapis.com/brain-genomics-public/research/sequencing/grch38/bam/rna/illumina/mrna. curl -L ${HTTPDIR}/hg005_gm26107.mrna.grch38.bam > data/hg005_gm26107.mrna.grch38.bam; curl -L ${HTTPDIR}/hg005_gm26107.mrna.grch38.bam.bai > data/hg005_gm26107.mrna.grch38.bam.bai; ```. ### Generate a 3x coverage file. RNA-seq data is only observed in 
",False,
Availability,"38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai; ```. ## Run DeepVariant on chromosome 20 alignments. ```bash; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.6.1""; mkdir -p deepvariant_output. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant_output/output.vcf.gz \; --num_shards $(nproc) \; --regio",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:2411,avail,available,2411,docs/deepvariant-pacbio-model-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md,1,['avail'],['available'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: 38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai; ```. ## Run DeepVariant on chromosome 20 alignments. ```bash; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.6.1""; mkdir -p deepvariant_output. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant_output/output.vcf.gz \; --num_shards $(nproc) \; --regio
",False,
Availability,"; Key changes and improvements include:. * Rearchitected with open source release in mind; * Built on [TensorFlow]; * Increased variant calling accuracy, especially for indels; * Vastly faster with reduced memory usage. We have made a number of improvements to the methodology as well. The biggest; change was to move away from RGB-encoded (3-channel) pileup images and instead; represent the aligned read data using a multi-channel tensor data layout. We; currently represent the data as a 6-channel raw tensor in which we encode:. * The read base (A, C, G, T); * The base's quality score; * The read's mapping quality score; * The read's strand (positive or negative); * Does the read support the allele being evaluated?; * Does the base match the reference genome at this position?. These are all readily derived from the information found in the BAM file; encoding of each read. Additional modeling changes were to move to the inception-v3 architecture and to; train on many more independent sequencing replicates of the ground truth; training samples, including 50% downsampled versions of each of those read sets.; In our testing this allowed the model to better generalize to other data types. In the end these changes reduced our error rate by more than 50% on the held out; evaluation sample (NA24385 / HG002) as compared to our results in the; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results/):. DeepVariant April 2016 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 4175 | 2839 | 0.987882 | 0.991728 | 0.989802; SNP | 1689 | 832 | 0.999447 | 0.999728 | 0.999587. DeepVariant December 2017 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 2384 | 1811 | 0.993081 | 0.994954 | 0.994017; SNP | 735 | 363 | 0.999759 | 0.999881 | 0.999820. See the [whole genome case study]",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:7871,down,downsampled,7871,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['down'],['downsampled'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ; Key changes and improvements include:. * Rearchitected with open source release in mind; * Built on [TensorFlow]; * Increased variant calling accuracy, especially for indels; * Vastly faster with reduced memory usage. We have made a number of improvements to the methodology as well. The biggest; change was to move away from RGB-encoded (3-channel) pileup images and instead; represent the aligned read data using a multi-channel tensor data layout. We; currently represent the data as a 6-channel raw tensor in which we encode:. * The read base (A, C, G, T); * The base's quality score; * The read's mapping quality score; * The read's strand (positive or negative); * Does the read support the allele being evaluated?; * Does the base match the reference genome at this position?. These are all readily derived from the information found in the BAM file; encoding of each read. Additional modeling changes were to move to the inception-v3 architecture and to; train on many more independent sequencing replicates of the ground truth; training samples, including 50% downsampled versions of each of those read sets.; In our testing this allowed the model to better generalize to other data types. In the end these changes reduced our error rate by more than 50% on the held out; evaluation sample (NA24385 / HG002) as compared to our results in the; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results/):. DeepVariant April 2016 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 4175 | 2839 | 0.987882 | 0.991728 | 0.989802; SNP | 1689 | 832 | 0.999447 | 0.999728 | 0.999587. DeepVariant December 2017 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 2384 | 1811 | 0.993081 | 0.994954 | 0.994017; SNP | 735 | 363 | 0.999759 | 0.999881 | 0.999820. See the [whole genome case study]
",True,The text consists of technical details and specifications that are more appropriate for a programmatic API description or technical documentation rather than human-readable prose.
Availability,"; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTP",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14952,checkpoint,checkpoints,14952,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['checkpoint'],['checkpoints'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTP
",False,
Availability,"DeepVariant RNA-seq model and produce an output; VCF (`output/out.vcf.gz`). ```bash; BIN_VERSION=""1.4.0"". sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; google/deepvariant:""${BIN_VERSION}"" \; run_deepvariant \; --model_type=WES \; --customized_model=model/model.ckpt \; --ref=reference/GRCh38_no_alt_analysis_set.fasta \; --reads=data/hg005_gm26107.mrna.grch38.bam \; --output_vcf=output/HG005.output.vcf.gz \; --num_shards=$(nproc) \; --regions=data/chr20_CDS_3x.bed \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir output/intermediate_results_dir; ```. **Flag summary**. * `--model_type` - Sets the model and options, but we will override the model; with `--customized model`.; * `--customized_model` - Points to a model trained using RNA-seq data.; * `--ref` - Specifies the reference sequence.; * `--reads` - Specifies the input bam file.; * `--output_vcf` - Specifies the output variant file.; * `--num_shards` - Sets the number of shards to the number of available; processors (`$(nproc)`). This is used to perform parallelization.; * `--regions` - Restricts analysis to 3x chr20 CDS regions only.; * `--make_examples_extra_args=` - Passes additional arguments to; make_examples.; * `split_skip_reads=true` - *Important!* This flag is critical for RNA-seq; variant calling to work properly. It enables RNA-seq data to be; processed efficiently.; * `channels=''` - Resets the channel list to be appropriate for the; RNA-seq model.; * `--intermediate_results_dir` - Outputs results to an intermediate directory. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; sudo docker run \; -v $(pwd):$(pwd) \; -w $(pwd) \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; output/HG005.output.vcf.gz \; -f benchmark/chr20_CDS_3x.benchmark_regions.bed \; -r reference/GRCh38_no_alt_analysis_set.fas",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:8169,avail,available,8169,docs/deepvariant-rnaseq-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md,1,['avail'],['available'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: DeepVariant RNA-seq model and produce an output; VCF (`output/out.vcf.gz`). ```bash; BIN_VERSION=""1.4.0"". sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; google/deepvariant:""${BIN_VERSION}"" \; run_deepvariant \; --model_type=WES \; --customized_model=model/model.ckpt \; --ref=reference/GRCh38_no_alt_analysis_set.fasta \; --reads=data/hg005_gm26107.mrna.grch38.bam \; --output_vcf=output/HG005.output.vcf.gz \; --num_shards=$(nproc) \; --regions=data/chr20_CDS_3x.bed \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir output/intermediate_results_dir; ```. **Flag summary**. * `--model_type` - Sets the model and options, but we will override the model; with `--customized model`.; * `--customized_model` - Points to a model trained using RNA-seq data.; * `--ref` - Specifies the reference sequence.; * `--reads` - Specifies the input bam file.; * `--output_vcf` - Specifies the output variant file.; * `--num_shards` - Sets the number of shards to the number of available; processors (`$(nproc)`). This is used to perform parallelization.; * `--regions` - Restricts analysis to 3x chr20 CDS regions only.; * `--make_examples_extra_args=` - Passes additional arguments to; make_examples.; * `split_skip_reads=true` - *Important!* This flag is critical for RNA-seq; variant calling to work properly. It enables RNA-seq data to be; processed efficiently.; * `channels=''` - Resets the channel list to be appropriate for the; RNA-seq model.; * `--intermediate_results_dir` - Outputs results to an intermediate directory. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; sudo docker run \; -v $(pwd):$(pwd) \; -w $(pwd) \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; output/HG005.output.vcf.gz \; -f benchmark/chr20_CDS_3x.benchmark_regions.bed \; -r reference/GRCh38_no_alt_analysis_set.fas
",False,"The text contains detailed bash commands and pipeline descriptions which are part of a command line tool's usage. However, it also includes explanations that provide context about each flag's purpose. There is no clear code or syntax present in the text other than command calls and parameter lists, but the overall content is still descriptive rather than purely programmatic."
Availability,"IC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 9811 | 212 | 155 | 0.978849 | 0.985044 | 0.981937 |; | SNP | 66180 | 57 | 70 | 0.999139 | 0.998944 | 0.999042 |. The baseline we're comparing to is to directly use the WGS model to make the; calls, using this command:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE}-gpu \; run_deepvariant \; --model_type WGS \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/baseline.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. Baseline:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 9620 | 403 | 823 | 0.959792 | 0.924112 | 0.941615 |; | SNP | 66159 | 78 | 83 | 0.998822 | 0.998748 | 0.998785 |. ### Additional things to try. #### Parameters to tune. Starting from the default setting of this tutorial is a good starting point, but; this training case study is by no means the best setting. Training is both a; science and an art. There are many knobs that we could potentially tune. Users; might be able to use different parameters to train a more accurate model even; with the same data, such as `batch_size`, `learning_rate`,; `learning_rate_decay_factor` in modeling.py. #### Downsampling the BAM file to generate more training examples. When generating the training set, we can make some adjustment to create more; training data. For example, when we train the released WGS model for; DeepVariant, for each BAM file, we created an extra set of training examples; using `--downsample_fraction=0.5`, which downsamples the reads and creates; training examples with lower coverage. We found that this makes the trained; model more robust. [GPU machine]: deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:21194,down,downsamples,21194,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,2,"['down', 'robust']","['downsamples', 'robust']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: IC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 9811 | 212 | 155 | 0.978849 | 0.985044 | 0.981937 |; | SNP | 66180 | 57 | 70 | 0.999139 | 0.998944 | 0.999042 |. The baseline we're comparing to is to directly use the WGS model to make the; calls, using this command:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE}-gpu \; run_deepvariant \; --model_type WGS \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/baseline.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. Baseline:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 9620 | 403 | 823 | 0.959792 | 0.924112 | 0.941615 |; | SNP | 66159 | 78 | 83 | 0.998822 | 0.998748 | 0.998785 |. ### Additional things to try. #### Parameters to tune. Starting from the default setting of this tutorial is a good starting point, but; this training case study is by no means the best setting. Training is both a; science and an art. There are many knobs that we could potentially tune. Users; might be able to use different parameters to train a more accurate model even; with the same data, such as `batch_size`, `learning_rate`,; `learning_rate_decay_factor` in modeling.py. #### Downsampling the BAM file to generate more training examples. When generating the training set, we can make some adjustment to create more; training data. For example, when we train the released WGS model for; DeepVariant, for each BAM file, we created an extra set of training examples; using `--downsample_fraction=0.5`, which downsamples the reads and creates; training examples with lower coverage. We found that this makes the trained; model more robust. [GPU machine]: deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform; 
",False,
Availability,"This is an example to run `vg giraffe`, so we can go from FASTQs --> BAM. For simplicity and consistency, we run the following with a; [Google Cloud instance with 64 cores](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). I added more disks because 300G is not enough for the example below. I changed; it to `--boot-disk-size ""1000""`. ## Install softwares that will be used later. ```bash; sudo apt update -y; sudo apt-get -y install aria2 docker.io samtools; ```. ## Download input FASTQ files. ```bash; DATA_DIR=${PWD}/data; mkdir -p ${DATA_DIR}; gcloud storage cp gs://brain-genomics-public/research/sequencing/fastq/novaseq/wgs_pcr_free/35x/HG003.novaseq.pcr-free.35x.R?.fastq.gz ${DATA_DIR}/; ```. ## Download VG files. Get binaries `vg` 1.51.0 and `kmc`:. ```bash; wget https://github.com/refresh-bio/KMC/releases/download/v3.2.2/KMC3.2.2.linux.x64.tar.gz; tar zxf KMC3.2.2.linux.x64.tar.gz bin/kmc; mv bin/kmc ${DATA_DIR}/; wget https://github.com/vgteam/vg/releases/download/v1.51.0/vg -O ${DATA_DIR}/vg; chmod +x ${DATA_DIR}/vg ${DATA_DIR}/kmc; ```. Get the graph (.gbz) and haplotype index (.hapl).; I used `aria2c` to download these files. You can use other approaches as well. ```bash; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.gbz; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.hapl; ```. ## Run `vg giraffe` with one command to get from FASTQs to BAM. Put the paths name into a file named HG003.fq.paths:. ```bash; cat > HG003.fq.paths <<- EOM; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R1.fastq.gz; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R2.fastq.gz; EOM; ```. Run `kmc`` on this file. I used -t$(nproc) to use all cores, and $TMPDIR for a; scratch directory:. ```bash; TMPDIR=$(mktemp -d); time ${D",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:1071,down,download,1071,docs/deepvariant-vg-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md,1,['down'],['download'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: This is an example to run `vg giraffe`, so we can go from FASTQs --> BAM. For simplicity and consistency, we run the following with a; [Google Cloud instance with 64 cores](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). I added more disks because 300G is not enough for the example below. I changed; it to `--boot-disk-size ""1000""`. ## Install softwares that will be used later. ```bash; sudo apt update -y; sudo apt-get -y install aria2 docker.io samtools; ```. ## Download input FASTQ files. ```bash; DATA_DIR=${PWD}/data; mkdir -p ${DATA_DIR}; gcloud storage cp gs://brain-genomics-public/research/sequencing/fastq/novaseq/wgs_pcr_free/35x/HG003.novaseq.pcr-free.35x.R?.fastq.gz ${DATA_DIR}/; ```. ## Download VG files. Get binaries `vg` 1.51.0 and `kmc`:. ```bash; wget https://github.com/refresh-bio/KMC/releases/download/v3.2.2/KMC3.2.2.linux.x64.tar.gz; tar zxf KMC3.2.2.linux.x64.tar.gz bin/kmc; mv bin/kmc ${DATA_DIR}/; wget https://github.com/vgteam/vg/releases/download/v1.51.0/vg -O ${DATA_DIR}/vg; chmod +x ${DATA_DIR}/vg ${DATA_DIR}/kmc; ```. Get the graph (.gbz) and haplotype index (.hapl).; I used `aria2c` to download these files. You can use other approaches as well. ```bash; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.gbz; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.hapl; ```. ## Run `vg giraffe` with one command to get from FASTQs to BAM. Put the paths name into a file named HG003.fq.paths:. ```bash; cat > HG003.fq.paths <<- EOM; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R1.fastq.gz; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R2.fastq.gz; EOM; ```. Run `kmc`` on this file. I used -t$(nproc) to use all cores, and $TMPDIR for a; scratch directory:. ```bash; TMPDIR=$(mktemp -d); time ${D
",False,
Availability,"Variant is a diploid variant caller, it assigns genotypes as {Hom-ref,; Het, Hom-alt} for each candidate allele it observes. For samples with karyotype; XY, the chromosome X and Y are effectively haploid. So, we are introducing two; flags to re-adjust the genotypes in regions that are considered to be haploid; for those samples. You can use `--haploid_contigs` and `--par_regions_bed` parameters to readjust; the genotypes in haploid regions. For samples with XY karyotype, it is expected; that users will set `--haploid_contigs=""chrX,chrY""` for; [GRCh38](https://storage.googleapis.com/deepvariant/case-study-testdata/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa); and `--haploid_contigs=""X,Y""` for; [GRCh37](https://storage.googleapis.com/deepvariant/case-study-testdata/hs37d5.fa).; You can also provide a PAR region bed file with; `--par_regions_bed=""/input/GRCh3X_par.bed""` parameter. The regions in the PAR; bed file will be skipped from genotype readjustment. You can download the PAR; bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). ## How it works. The genotype re-adjustment is implemented in the `postprocess_variants` stage of; DeepVariant. For any variant, that is in the`--haploid_contigs` regions and; **not** in the `--par_regions_bed` regions, the genotype likelihoods of; heterozygous variants are set as 0 and the genotypes are normalized again after; re-adjusting the likelihoods. After that the most-likely genotype is assigned to; the allele which excludes any heterozygous calls. For example, suppose we observe an alternate allele `ALT1` at a position that we; consider to be haploid. So the observed alleles at that position are:; `Candidates: {REF, ALT1}` The neural network generates likelihoods for the; genotypes for this candidate as such:. ```; Homozygous reference: likelihood(REF,REF); Heterozyg",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-haploid-support.md:1246,down,download,1246,docs/deepvariant-haploid-support.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-haploid-support.md,1,['down'],['download'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Variant is a diploid variant caller, it assigns genotypes as {Hom-ref,; Het, Hom-alt} for each candidate allele it observes. For samples with karyotype; XY, the chromosome X and Y are effectively haploid. So, we are introducing two; flags to re-adjust the genotypes in regions that are considered to be haploid; for those samples. You can use `--haploid_contigs` and `--par_regions_bed` parameters to readjust; the genotypes in haploid regions. For samples with XY karyotype, it is expected; that users will set `--haploid_contigs=""chrX,chrY""` for; [GRCh38](https://storage.googleapis.com/deepvariant/case-study-testdata/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa); and `--haploid_contigs=""X,Y""` for; [GRCh37](https://storage.googleapis.com/deepvariant/case-study-testdata/hs37d5.fa).; You can also provide a PAR region bed file with; `--par_regions_bed=""/input/GRCh3X_par.bed""` parameter. The regions in the PAR; bed file will be skipped from genotype readjustment. You can download the PAR; bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). ## How it works. The genotype re-adjustment is implemented in the `postprocess_variants` stage of; DeepVariant. For any variant, that is in the`--haploid_contigs` regions and; **not** in the `--par_regions_bed` regions, the genotype likelihoods of; heterozygous variants are set as 0 and the genotypes are normalized again after; re-adjusting the likelihoods. After that the most-likely genotype is assigned to; the allele which excludes any heterozygous calls. For example, suppose we observe an alternate allele `ALT1` at a position that we; consider to be haploid. So the observed alleles at that position are:; `Candidates: {REF, ALT1}` The neural network generates likelihoods for the; genotypes for this candidate as such:. ```; Homozygous reference: likelihood(REF,REF); Heterozyg
",False,
Availability,"_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have te",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:13687,checkpoint,checkpoints,13687,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['checkpoint'],['checkpoints'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: _label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have te
",False,
Availability,"ange was to move away from RGB-encoded (3-channel) pileup images and instead; represent the aligned read data using a multi-channel tensor data layout. We; currently represent the data as a 6-channel raw tensor in which we encode:. * The read base (A, C, G, T); * The base's quality score; * The read's mapping quality score; * The read's strand (positive or negative); * Does the read support the allele being evaluated?; * Does the base match the reference genome at this position?. These are all readily derived from the information found in the BAM file; encoding of each read. Additional modeling changes were to move to the inception-v3 architecture and to; train on many more independent sequencing replicates of the ground truth; training samples, including 50% downsampled versions of each of those read sets.; In our testing this allowed the model to better generalize to other data types. In the end these changes reduced our error rate by more than 50% on the held out; evaluation sample (NA24385 / HG002) as compared to our results in the; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results/):. DeepVariant April 2016 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 4175 | 2839 | 0.987882 | 0.991728 | 0.989802; SNP | 1689 | 832 | 0.999447 | 0.999728 | 0.999587. DeepVariant December 2017 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 2384 | 1811 | 0.993081 | 0.994954 | 0.994017; SNP | 735 | 363 | 0.999759 | 0.999881 | 0.999820. See the [whole genome case study], which we update with each release of; DeepVariant, for the latest results. You can also see the [Colab example] to see how you can visualize the pileup; images. ## Training data over time. For the models we've released over time, you can find more details about the; training data in; [DeepVariant t",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:8038,error,error,8038,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['error'],['error'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ange was to move away from RGB-encoded (3-channel) pileup images and instead; represent the aligned read data using a multi-channel tensor data layout. We; currently represent the data as a 6-channel raw tensor in which we encode:. * The read base (A, C, G, T); * The base's quality score; * The read's mapping quality score; * The read's strand (positive or negative); * Does the read support the allele being evaluated?; * Does the base match the reference genome at this position?. These are all readily derived from the information found in the BAM file; encoding of each read. Additional modeling changes were to move to the inception-v3 architecture and to; train on many more independent sequencing replicates of the ground truth; training samples, including 50% downsampled versions of each of those read sets.; In our testing this allowed the model to better generalize to other data types. In the end these changes reduced our error rate by more than 50% on the held out; evaluation sample (NA24385 / HG002) as compared to our results in the; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results/):. DeepVariant April 2016 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 4175 | 2839 | 0.987882 | 0.991728 | 0.989802; SNP | 1689 | 832 | 0.999447 | 0.999728 | 0.999587. DeepVariant December 2017 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 2384 | 1811 | 0.993081 | 0.994954 | 0.994017; SNP | 735 | 363 | 0.999759 | 0.999881 | 0.999820. See the [whole genome case study], which we update with each release of; DeepVariant, for the latest results. You can also see the [Colab example] to see how you can visualize the pileup; images. ## Training data over time. For the models we've released over time, you can find more details about the; training data in; [DeepVariant t
",False,
Availability,"current inference code?. No. We have moved from Slim to Keras. All models before `1.6.0` were trained in; Slim platform. So they are not compatible with `1.6.0` anymore. ## Can call_variants be run on multiple GPUs?. No. Although possible, we have not implemented the multi-GPU capability in GPU; inference yet. ## Can model_train be run on multiple GPUs?. No. TensorFlow's Estimator API does provide support for running training on; multiple GPUs through the use of a DistributionStrategy. However,; DistributionStrategy cannot be used with exponential moving average (EMA), which; is present in the DeepVariant codebase. ## What is the realigner and how does it work?. From the; [DeepVariant 2018 manuscript](https://www.nature.com/articles/nbt.4235.epdf?author_access_token=q4ZmzqvvcGBqTuKyKgYrQ9RgN0jAjWel9jnR3ZoTv0NuM3saQzpZk8yexjfPUhdFj4zyaA4Yvq0LWBoCYQ4B9vqPuv8e2HHy4vShDgEs8YxI_hLs9ov6Y1f_4fyS7kGZ):. > Mapped reads are preprocessed using an error-tolerant, local; > De-Bruijn-graph-based read assembly procedure that realigns them according to; > their most likely derived haplotype. Candidate windows across the genome are; > selected for reassembly by looking for any evidence of possible genetic; > variation, such as mismatching or soft clipped bases. The selection criteria; > for a candidate window are very permissive so that true variation is unlikely; > to be missed. All candidate windows across the genome are considered; > independently. De Bruijn graphs are constructed using multiple fixed k-mer; > sizes (from 20 to 75, inclusive, with increments of 5) out of the reference; > genome bases for the candidate window, as well as all overlapping reads. Edges; > are given a weight determined by how many times they are observed in the; > reads. We trim any edges with weight less than three, except that edges found; > in the reference are never trimmed. Candidate haplotypes are generated by; > traversing the assembly graphs and the top two most likely haplotypes are; > select",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:10494,error,error-tolerant,10494,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['error'],['error-tolerant'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: current inference code?. No. We have moved from Slim to Keras. All models before `1.6.0` were trained in; Slim platform. So they are not compatible with `1.6.0` anymore. ## Can call_variants be run on multiple GPUs?. No. Although possible, we have not implemented the multi-GPU capability in GPU; inference yet. ## Can model_train be run on multiple GPUs?. No. TensorFlow's Estimator API does provide support for running training on; multiple GPUs through the use of a DistributionStrategy. However,; DistributionStrategy cannot be used with exponential moving average (EMA), which; is present in the DeepVariant codebase. ## What is the realigner and how does it work?. From the; [DeepVariant 2018 manuscript](https://www.nature.com/articles/nbt.4235.epdf?author_access_token=q4ZmzqvvcGBqTuKyKgYrQ9RgN0jAjWel9jnR3ZoTv0NuM3saQzpZk8yexjfPUhdFj4zyaA4Yvq0LWBoCYQ4B9vqPuv8e2HHy4vShDgEs8YxI_hLs9ov6Y1f_4fyS7kGZ):. > Mapped reads are preprocessed using an error-tolerant, local; > De-Bruijn-graph-based read assembly procedure that realigns them according to; > their most likely derived haplotype. Candidate windows across the genome are; > selected for reassembly by looking for any evidence of possible genetic; > variation, such as mismatching or soft clipped bases. The selection criteria; > for a candidate window are very permissive so that true variation is unlikely; > to be missed. All candidate windows across the genome are considered; > independently. De Bruijn graphs are constructed using multiple fixed k-mer; > sizes (from 20 to 75, inclusive, with increments of 5) out of the reference; > genome bases for the candidate window, as well as all overlapping reads. Edges; > are given a weight determined by how many times they are observed in the; > reads. We trim any edges with weight less than three, except that edges found; > in the reference are never trimmed. Candidate haplotypes are generated by; > traversing the assembly graphs and the top two most likely haplotypes are; > select
",False,
Availability,"dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir""; tensorboard --logdir ${TRAINING_DIR} --port=8080; ```. After it started, I clicked on the “Web Preview” on the top right of the mini; terminal:. ![WebPreview](images/WebPreview.png?raw=true ""Web Preview""). And clicked on ""Prev",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:15199,checkpoint,checkpoints,15199,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['checkpoint'],['checkpoints'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir""; tensorboard --logdir ${TRAINING_DIR} --port=8080; ```. After it started, I clicked on the “Web Preview” on the top right of the mini; terminal:. ![WebPreview](images/WebPreview.png?raw=true ""Web Preview""). And clicked on ""Prev
",True,"The text consists primarily of code snippets and API documentation. There are commands in bash format which are code examples, instructions for using TensorBoard with specific parameters (--logdir, --port), and configuration details such as OUTPUT_BUCKET and TRAINING_DIR. Additionally, there is a mention of the best performing checkpoint and usage of gsutil to list checkpoints. These are all programmatic and related to system operations rather than meaningful human-readable content."
Availability,"e a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:1932,down,downloaded,1932,docs/deeptrio-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md,1,['down'],['downloaded'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: e a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark
",False,
Availability,"e instances at this time, but simply visiting this page will; initialize your compute engine ""service account"" so that we can authorize; it. (As you progress in your use of Google Cloud Platform, you will likely find it; useful to create a [Cloud; Organization](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you c",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:2334,down,downloads,2334,docs/deepvariant-gcp-info.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md,1,['down'],['downloads'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: e instances at this time, but simply visiting this page will; initialize your compute engine ""service account"" so that we can authorize; it. (As you progress in your use of Google Cloud Platform, you will likely find it; useful to create a [Cloud; Organization](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you c
",False,"The text contains descriptive content about Google Cloud Platform usage, installation instructions, and best practices for managing cloud projects. It includes links to detailed documentation, which is a form of explanatory prose. While the content may include some technical terms and URLs, it is written in a way that is understandable by humans and provides value beyond mere code or logs."
Availability,"ed=""/input/GRCh3X_par.bed"" \ **Optional. If --haploid_contigs is set, then this can be used to provide PAR regions to be excluded from genotype adjustment. Download links to this files are available in this page.; --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.; ```. For details on X,Y support, please see; [DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case; study in; [DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You; can download the PAR bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). To see all flags you can use, run: `docker run; google/deepvariant:""${BIN_VERSION}""`. If you're using GPUs, or want to use Singularity instead, see; [Quick Start](docs/deepvariant-quick-start.md) for more details or see all the; [setup options](#deepvariant_setup) available. For more information, also see:. * [Full documentation list](docs/README.md); * [Detailed usage guide](docs/deepvariant-details.md) with more information on; the input and output file formats and how to work with them.; * [Best practices for multi-sample variant calling with DeepVariant](docs/trio-merge-case-study.md); * [(Advanced) Training tutorial](docs/deepvariant-training-case-study.md); * [DeepVariant's Frequently Asked Questions, FAQ](docs/FAQ.md). ## How to cite. If you're using DeepVariant in your work, please cite:. [A universal SNP and small-indel variant caller using deep neural networks. *Nature Biotechnology* 36, 983–987 (2018).](https://rdcu.be/7Dhl) <br/>; Ryan Poplin, Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas Colthurst, Alexander Ku, Dan Newburger, Jojo Dijamco, Nam Nguyen, Pegah T. Afshar, Sam S. Gross, Lizzie Dorfman, Cory Y. McLean, and Mark A. DePristo.<br/>; doi: https://doi.org/10.1038/nbt.4235. Additionally",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:5306,avail,available,5306,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['avail'],['available'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ed=""/input/GRCh3X_par.bed"" \ **Optional. If --haploid_contigs is set, then this can be used to provide PAR regions to be excluded from genotype adjustment. Download links to this files are available in this page.; --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.; ```. For details on X,Y support, please see; [DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case; study in; [DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You; can download the PAR bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). To see all flags you can use, run: `docker run; google/deepvariant:""${BIN_VERSION}""`. If you're using GPUs, or want to use Singularity instead, see; [Quick Start](docs/deepvariant-quick-start.md) for more details or see all the; [setup options](#deepvariant_setup) available. For more information, also see:. * [Full documentation list](docs/README.md); * [Detailed usage guide](docs/deepvariant-details.md) with more information on; the input and output file formats and how to work with them.; * [Best practices for multi-sample variant calling with DeepVariant](docs/trio-merge-case-study.md); * [(Advanced) Training tutorial](docs/deepvariant-training-case-study.md); * [DeepVariant's Frequently Asked Questions, FAQ](docs/FAQ.md). ## How to cite. If you're using DeepVariant in your work, please cite:. [A universal SNP and small-indel variant caller using deep neural networks. *Nature Biotechnology* 36, 983–987 (2018).](https://rdcu.be/7Dhl) <br/>; Ryan Poplin, Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas Colthurst, Alexander Ku, Dan Newburger, Jojo Dijamco, Nam Nguyen, Pegah T. Afshar, Sam S. Gross, Lizzie Dorfman, Cory Y. McLean, and Mark A. DePristo.<br/>; doi: https://doi.org/10.1038/nbt.4235. Additionally
",False,"The text contains meaningful sentences in natural language discussing concepts, usage instructions, and provides context about the tool such as links to documentation and case studies."
Availability,"epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir""; tensorboard --logdir ${TRAINING_DIR} --port=8080; ```. After it started, I clicked on the “Web Preview” on the top right of the",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:15092,checkpoint,checkpoints,15092,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,2,['checkpoint'],"['checkpoint', 'checkpoints']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir""; tensorboard --logdir ${TRAINING_DIR} --port=8080; ```. After it started, I clicked on the “Web Preview” on the top right of the
",True,The text consists primarily of code snippets and API documentation.
Availability,"erence/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai; ```. ## Run DeepVariant on chromosome 20 alignments. ```bash; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.6.1""; mkdir -p deepvariant_output. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant_output/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20; ```. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:2547,down,downloads,2547,docs/deepvariant-pacbio-model-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md,1,['down'],['downloads'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: erence/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai; ```. ## Run DeepVariant on chromosome 20 alignments. ```bash; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.6.1""; mkdir -p deepvariant_output. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant_output/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20; ```. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command
",False,"The text contains a series of commands and instructions for downloading data and running software. It includes bash scripts, curl commands, Docker/Singularity usage, and specific software version references (e.g., DeepVariant v1.6.1). These elements suggest the content is programmatic or related to system operations rather than human-readable prose."
Availability,"f# DeepVariant usage guide. ## Overview. DeepVariant is a set of programs used to transform aligned sequencing reads into; variant calls. At the highest level, a user needs to provide three inputs:. 1. A reference genome in [FASTA](https://en.wikipedia.org/wiki/FASTA_format); format and its corresponding; [.fai index file](http://www.htslib.org/doc/faidx.html) generated using the; `samtools faidx` command. 1. An aligned reads file in [BAM](http://genome.sph.umich.edu/wiki/BAM) format; and its corresponding index file (.bai). The reads must be aligned to the; reference genome described above. 1. A model checkpoint for DeepVariant. The output of DeepVariant is a list of all variant calls in; [VCF](https://samtools.github.io/hts-specs/VCFv4.3.pdf) format. DeepVariant is composed of three programs: `make_examples`, `call_variants`, and; `postprocess_variants`. More details about each program are described in detail; in the [Inputs and outputs](#inputs-and-outputs) section. ## Inputs and outputs. ### General notes. * Sharded files are a single logical collection of files with a common naming; convention. For example, we talk about `filename@10` as a single 10-way; sharded file named `filename`. On most filesystems this actually looks like; 10 distinct files `filename-00000-of-00010`, ..., `filename-00009-of-00010`.; DeepVariant can write sharded files using their `filename@10`-style name and; can read sharded files using both that style as well as the glob form, such; as `filename-*` or `filename-*-of-00010`.; * Files with the `.gz` suffix are interpreted as being compressed with gzip; and are read/written accordingly. ### make_examples. `make_examples` consumes reads and the reference genome to create TensorFlow; examples for evaluation with our deep learning models. The tf.Example protos are; written out in TFRecord format. To learn more about tf.Example and TFRecord, see; the; [Using TFRecords and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord); Co",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:610,checkpoint,checkpoint,610,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['checkpoint'],['checkpoint'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: f# DeepVariant usage guide. ## Overview. DeepVariant is a set of programs used to transform aligned sequencing reads into; variant calls. At the highest level, a user needs to provide three inputs:. 1. A reference genome in [FASTA](https://en.wikipedia.org/wiki/FASTA_format); format and its corresponding; [.fai index file](http://www.htslib.org/doc/faidx.html) generated using the; `samtools faidx` command. 1. An aligned reads file in [BAM](http://genome.sph.umich.edu/wiki/BAM) format; and its corresponding index file (.bai). The reads must be aligned to the; reference genome described above. 1. A model checkpoint for DeepVariant. The output of DeepVariant is a list of all variant calls in; [VCF](https://samtools.github.io/hts-specs/VCFv4.3.pdf) format. DeepVariant is composed of three programs: `make_examples`, `call_variants`, and; `postprocess_variants`. More details about each program are described in detail; in the [Inputs and outputs](#inputs-and-outputs) section. ## Inputs and outputs. ### General notes. * Sharded files are a single logical collection of files with a common naming; convention. For example, we talk about `filename@10` as a single 10-way; sharded file named `filename`. On most filesystems this actually looks like; 10 distinct files `filename-00000-of-00010`, ..., `filename-00009-of-00010`.; DeepVariant can write sharded files using their `filename@10`-style name and; can read sharded files using both that style as well as the glob form, such; as `filename-*` or `filename-*-of-00010`.; * Files with the `.gz` suffix are interpreted as being compressed with gzip; and are read/written accordingly. ### make_examples. `make_examples` consumes reads and the reference genome to create TensorFlow; examples for evaluation with our deep learning models. The tf.Example protos are; written out in TFRecord format. To learn more about tf.Example and TFRecord, see; the; [Using TFRecords and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord); Co
",True,"The text consists primarily of API documentation or specifications, including parameter lists and return types. It details inputs, outputs, and processes, which are typical of programmatic content that should be filtered out."
Availability,"google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with gen",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:2783,avail,availability,2783,docs/deepvariant-gcp-info.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md,1,['avail'],['availability'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with gen
",False,"The text contains complete sentences and explanatory content about installing Google Cloud SDK and starting Compute Engine instances. It includes instructions, notes, and verification steps, all of which are meaningful and human-readable."
Availability,"h 64 cores](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). I added more disks because 300G is not enough for the example below. I changed; it to `--boot-disk-size ""1000""`. ## Install softwares that will be used later. ```bash; sudo apt update -y; sudo apt-get -y install aria2 docker.io samtools; ```. ## Download input FASTQ files. ```bash; DATA_DIR=${PWD}/data; mkdir -p ${DATA_DIR}; gcloud storage cp gs://brain-genomics-public/research/sequencing/fastq/novaseq/wgs_pcr_free/35x/HG003.novaseq.pcr-free.35x.R?.fastq.gz ${DATA_DIR}/; ```. ## Download VG files. Get binaries `vg` 1.51.0 and `kmc`:. ```bash; wget https://github.com/refresh-bio/KMC/releases/download/v3.2.2/KMC3.2.2.linux.x64.tar.gz; tar zxf KMC3.2.2.linux.x64.tar.gz bin/kmc; mv bin/kmc ${DATA_DIR}/; wget https://github.com/vgteam/vg/releases/download/v1.51.0/vg -O ${DATA_DIR}/vg; chmod +x ${DATA_DIR}/vg ${DATA_DIR}/kmc; ```. Get the graph (.gbz) and haplotype index (.hapl).; I used `aria2c` to download these files. You can use other approaches as well. ```bash; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.gbz; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.hapl; ```. ## Run `vg giraffe` with one command to get from FASTQs to BAM. Put the paths name into a file named HG003.fq.paths:. ```bash; cat > HG003.fq.paths <<- EOM; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R1.fastq.gz; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R2.fastq.gz; EOM; ```. Run `kmc`` on this file. I used -t$(nproc) to use all cores, and $TMPDIR for a; scratch directory:. ```bash; TMPDIR=$(mktemp -d); time ${DATA_DIR}/kmc -k29 -m128 -okff -t$(nproc) @HG003.fq.paths ${DATA_DIR}/HG003.fq $TMPDIR; ```. Output on the terminal:. ```; **************************************",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:1226,down,download,1226,docs/deepvariant-vg-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md,1,['down'],['download'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: h 64 cores](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). I added more disks because 300G is not enough for the example below. I changed; it to `--boot-disk-size ""1000""`. ## Install softwares that will be used later. ```bash; sudo apt update -y; sudo apt-get -y install aria2 docker.io samtools; ```. ## Download input FASTQ files. ```bash; DATA_DIR=${PWD}/data; mkdir -p ${DATA_DIR}; gcloud storage cp gs://brain-genomics-public/research/sequencing/fastq/novaseq/wgs_pcr_free/35x/HG003.novaseq.pcr-free.35x.R?.fastq.gz ${DATA_DIR}/; ```. ## Download VG files. Get binaries `vg` 1.51.0 and `kmc`:. ```bash; wget https://github.com/refresh-bio/KMC/releases/download/v3.2.2/KMC3.2.2.linux.x64.tar.gz; tar zxf KMC3.2.2.linux.x64.tar.gz bin/kmc; mv bin/kmc ${DATA_DIR}/; wget https://github.com/vgteam/vg/releases/download/v1.51.0/vg -O ${DATA_DIR}/vg; chmod +x ${DATA_DIR}/vg ${DATA_DIR}/kmc; ```. Get the graph (.gbz) and haplotype index (.hapl).; I used `aria2c` to download these files. You can use other approaches as well. ```bash; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.gbz; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.hapl; ```. ## Run `vg giraffe` with one command to get from FASTQs to BAM. Put the paths name into a file named HG003.fq.paths:. ```bash; cat > HG003.fq.paths <<- EOM; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R1.fastq.gz; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R2.fastq.gz; EOM; ```. Run `kmc`` on this file. I used -t$(nproc) to use all cores, and $TMPDIR for a; scratch directory:. ```bash; TMPDIR=$(mktemp -d); time ${DATA_DIR}/kmc -k29 -m128 -okff -t$(nproc) @HG003.fq.paths ${DATA_DIR}/HG003.fq $TMPDIR; ```. Output on the terminal:. ```; **************************************
",False,
Availability,"hanges to the codebase, we still recommend Docker. You can clone the; DeepVariant repo, modify the source code, and build a Docker image with your; changes using the provided Dockerfile. ## Why can't it find one of the input files? E.g., ""Could not open"". This often happens because the way Docker works, input and output directories; have to be mounted and then files are referred to by their mounted location,; which can be confusing. To check that files are visible inside the Docker; container, you can `ls` inside the container. For example, using the setup shown; in the README and looking inside the `/input` volume:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; ls /input; ```. Mounting directories with Docker can be confusing. One trick to make this; simpler is to set both sides as your `$HOME`, so the paths are the same inside; and outside the Docker container. ```; echo $HOME # see what your home directory is first.; ls $HOME; BIN_VERSION=""1.6.1""; sudo docker run \; -v ""${HOME}"":""${HOME}"" \; google/deepvariant:""${BIN_VERSION}"" \; ls $HOME; ```. ## How do I run multi-sample calling?. Since the DeepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. We; have tested and confirmed that this does",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:8237,echo,echo,8237,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['echo'],['echo'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: hanges to the codebase, we still recommend Docker. You can clone the; DeepVariant repo, modify the source code, and build a Docker image with your; changes using the provided Dockerfile. ## Why can't it find one of the input files? E.g., ""Could not open"". This often happens because the way Docker works, input and output directories; have to be mounted and then files are referred to by their mounted location,; which can be confusing. To check that files are visible inside the Docker; container, you can `ls` inside the container. For example, using the setup shown; in the README and looking inside the `/input` volume:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; ls /input; ```. Mounting directories with Docker can be confusing. One trick to make this; simpler is to set both sides as your `$HOME`, so the paths are the same inside; and outside the Docker container. ```; echo $HOME # see what your home directory is first.; ls $HOME; BIN_VERSION=""1.6.1""; sudo docker run \; -v ""${HOME}"":""${HOME}"" \; google/deepvariant:""${BIN_VERSION}"" \; ls $HOME; ```. ## How do I run multi-sample calling?. Since the DeepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. We; have tested and confirmed that this does
",False,
Availability,"he `--sample_name=XYZ` flag to; `run_deepvariant` to override the sample name written into the gVCF file header. ## Merge the trio samples using GLnexus. ### Run GLnexus to merge 3 gVCFs. And then run GLnexus with this config:. ```; sudo docker pull quay.io/mlin/glnexus:v1.2.7. time sudo docker run \; -v ""${DIR}"":""/data"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWES \; --bed ""/data/${CAPTURE_BED}"" \; /data/HG004.g.vcf.gz /data/HG003.g.vcf.gz /data/HG002.g.vcf.gz \; | sudo docker run -i google/deepvariant:${VERSION} bcftools view - \; | sudo docker run -i google/deepvariant:${VERSION} bgzip -c \; > ${DIR}/deepvariant.cohort.vcf.gz; ```. When we ran on this WES trio, it took only about 13 seconds. For more details on; performance, see; [GLnexus performance guide](https://github.com/dnanexus-rnd/GLnexus/wiki/Performance). For a WGS cohort, we recommend using `--config DeepVariantWGS` instead of; `DeepVariantWES`. Another preset `DeepVariant_unfiltered` is available in; `glnexus:v1.2.7` or later versions for merging DeepVariant gVCFs with no QC; filters or genotype revision (see; [GitHub issue #326](https://github.com/google/deepvariant/issues/326) for a; potential use case). The details of these presets can be found; [here](../deepvariant/cohort_best_practice). ## Annotate the merged VCF with Mendelian discordance information using RTG Tools. Create an SDF template from our reference file:. ```; sudo docker run \; -v ""${DIR}"":""/data"" \; realtimegenomics/rtg-tools format \; -o /data/hs37d5.sdf /data/hs37d5.fa; ```. Create a PED file `$DIR/trio.ped` that looks like this (with the sample name; of the trio):. ```; FILE=""${DIR}/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/ind-id/pat-id/mat-id: 0=unknown; #sex: 1=male; 2=female; 0=unknown; #phenotype: -9=missing, 0=missing; 1=unaffected; 2=affected; #; #fam-id ind-id pat-id mat-id sex phen; 1 Sample_Diag-excap51-HG002-EEogPU Sample_Diag-excap51-HG003-EEogPU Sample_Di",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:8480,avail,available,8480,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['avail'],['available'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: he `--sample_name=XYZ` flag to; `run_deepvariant` to override the sample name written into the gVCF file header. ## Merge the trio samples using GLnexus. ### Run GLnexus to merge 3 gVCFs. And then run GLnexus with this config:. ```; sudo docker pull quay.io/mlin/glnexus:v1.2.7. time sudo docker run \; -v ""${DIR}"":""/data"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWES \; --bed ""/data/${CAPTURE_BED}"" \; /data/HG004.g.vcf.gz /data/HG003.g.vcf.gz /data/HG002.g.vcf.gz \; | sudo docker run -i google/deepvariant:${VERSION} bcftools view - \; | sudo docker run -i google/deepvariant:${VERSION} bgzip -c \; > ${DIR}/deepvariant.cohort.vcf.gz; ```. When we ran on this WES trio, it took only about 13 seconds. For more details on; performance, see; [GLnexus performance guide](https://github.com/dnanexus-rnd/GLnexus/wiki/Performance). For a WGS cohort, we recommend using `--config DeepVariantWGS` instead of; `DeepVariantWES`. Another preset `DeepVariant_unfiltered` is available in; `glnexus:v1.2.7` or later versions for merging DeepVariant gVCFs with no QC; filters or genotype revision (see; [GitHub issue #326](https://github.com/google/deepvariant/issues/326) for a; potential use case). The details of these presets can be found; [here](../deepvariant/cohort_best_practice). ## Annotate the merged VCF with Mendelian discordance information using RTG Tools. Create an SDF template from our reference file:. ```; sudo docker run \; -v ""${DIR}"":""/data"" \; realtimegenomics/rtg-tools format \; -o /data/hs37d5.sdf /data/hs37d5.fa; ```. Create a PED file `$DIR/trio.ped` that looks like this (with the sample name; of the trio):. ```; FILE=""${DIR}/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/ind-id/pat-id/mat-id: 0=unknown; #sex: 1=male; 2=female; 0=unknown; #phenotype: -9=missing, 0=missing; 1=unaffected; 2=affected; #; #fam-id ind-id pat-id mat-id sex phen; 1 Sample_Diag-excap51-HG002-EEogPU Sample_Diag-excap51-HG003-EEogPU Sample_Di
",False,"The text contains meaningful sentences discussing the process of data merging, performance, and configuration, which are human-readable."
Availability,"hkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bam -o HG004.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bai -o HG004.bai; ```. ### Command for downloading the reference file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh3",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:4158,down,downloading,4158,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['down'],['downloading'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: hkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bam -o HG004.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bai -o HG004.bai; ```. ### Command for downloading the reference file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh3
",True,The text consists of command lines for downloading files using aria2c and other tools. These are operational instructions rather than meaningful human-readable sentences.
Availability,"iant/bin/run_deepvariant \; --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**; --ref=/input/YOUR_REF \; --reads=/input/YOUR_BAM \; --output_vcf=/output/YOUR_OUTPUT_VCF \; --output_gvcf=/output/YOUR_OUTPUT_GVCF \; --num_shards=$(nproc) \ **This will use all your cores to run make_examples. Feel free to change.**; --logging_dir=/output/logs \ **Optional. This saves the log output for each stage separately.; --haploid_contigs=""chrX,chrY"" \ **Optional. Heterozygous variants in these contigs will be re-genotyped as the most likely of reference or homozygous alternates. For a sample with karyotype XY, it should be set to ""chrX,chrY"" for GRCh38 and ""X,Y"" for GRCh37. For a sample with karyotype XX, this should not be used.; --par_regions_bed=""/input/GRCh3X_par.bed"" \ **Optional. If --haploid_contigs is set, then this can be used to provide PAR regions to be excluded from genotype adjustment. Download links to this files are available in this page.; --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.; ```. For details on X,Y support, please see; [DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case; study in; [DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You; can download the PAR bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). To see all flags you can use, run: `docker run; google/deepvariant:""${BIN_VERSION}""`. If you're using GPUs, or want to use Singularity instead, see; [Quick Start](docs/deepvariant-quick-start.md) for more details or see all the; [setup options](#deepvariant_setup) available. For more information, also see:. * [Full documentation list](docs/README.md); * [Detailed usage guide](docs/deepvariant-details.md) with more",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:4463,avail,available,4463,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['avail'],['available'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: iant/bin/run_deepvariant \; --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**; --ref=/input/YOUR_REF \; --reads=/input/YOUR_BAM \; --output_vcf=/output/YOUR_OUTPUT_VCF \; --output_gvcf=/output/YOUR_OUTPUT_GVCF \; --num_shards=$(nproc) \ **This will use all your cores to run make_examples. Feel free to change.**; --logging_dir=/output/logs \ **Optional. This saves the log output for each stage separately.; --haploid_contigs=""chrX,chrY"" \ **Optional. Heterozygous variants in these contigs will be re-genotyped as the most likely of reference or homozygous alternates. For a sample with karyotype XY, it should be set to ""chrX,chrY"" for GRCh38 and ""X,Y"" for GRCh37. For a sample with karyotype XX, this should not be used.; --par_regions_bed=""/input/GRCh3X_par.bed"" \ **Optional. If --haploid_contigs is set, then this can be used to provide PAR regions to be excluded from genotype adjustment. Download links to this files are available in this page.; --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.; ```. For details on X,Y support, please see; [DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case; study in; [DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You; can download the PAR bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). To see all flags you can use, run: `docker run; google/deepvariant:""${BIN_VERSION}""`. If you're using GPUs, or want to use Singularity instead, see; [Quick Start](docs/deepvariant-quick-start.md) for more details or see all the; [setup options](#deepvariant_setup) available. For more information, also see:. * [Full documentation list](docs/README.md); * [Detailed usage guide](docs/deepvariant-details.md) with more
",False,"The text contains both commands and explanatory paragraphs discussing the usage of DeepVariant. It includes flags and parameters which are part of a command line interface but also provides links to documentation resources for further information, making it a mix of programmatic setup instructions with human-readable explanations."
Availability,"ide the `/input` volume:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; ls /input; ```. Mounting directories with Docker can be confusing. One trick to make this; simpler is to set both sides as your `$HOME`, so the paths are the same inside; and outside the Docker container. ```; echo $HOME # see what your home directory is first.; ls $HOME; BIN_VERSION=""1.6.1""; sudo docker run \; -v ""${HOME}"":""${HOME}"" \; google/deepvariant:""${BIN_VERSION}"" \; ls $HOME; ```. ## How do I run multi-sample calling?. Since the DeepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. We; have tested and confirmed that this does not affect GPU usage or inference. So; you can continue running DeepVariant without being worried about this message. ## How much GPU memory is needed for the Keras models?. 16GB. In our test, we observe the model occupying 16GB GPU memory. ## Do models from before r1.6.0 work with current inference code?. No. We have moved from Slim to Keras. All models before `1.6.0` were trained in; Slim platform. So they are not compatible with `1.6.0` anymore. ## Can call_variants be run on multiple GPUs?. No. Although possible, we have not implemented the multi-GPU capability in GPU; inference yet. ## ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:8876,error,error,8876,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['error'],['error'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ide the `/input` volume:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; ls /input; ```. Mounting directories with Docker can be confusing. One trick to make this; simpler is to set both sides as your `$HOME`, so the paths are the same inside; and outside the Docker container. ```; echo $HOME # see what your home directory is first.; ls $HOME; BIN_VERSION=""1.6.1""; sudo docker run \; -v ""${HOME}"":""${HOME}"" \; google/deepvariant:""${BIN_VERSION}"" \; ls $HOME; ```. ## How do I run multi-sample calling?. Since the DeepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. We; have tested and confirmed that this does not affect GPU usage or inference. So; you can continue running DeepVariant without being worried about this message. ## How much GPU memory is needed for the Keras models?. 16GB. In our test, we observe the model occupying 16GB GPU memory. ## Do models from before r1.6.0 work with current inference code?. No. We have moved from Slim to Keras. All models before `1.6.0` were trained in; Slim platform. So they are not compatible with `1.6.0` anymore. ## Can call_variants be run on multiple GPUs?. No. Although possible, we have not implemented the multi-GPU capability in GPU; inference yet. ## 
",False,"The text includes several human-readable questions and answers regarding DeepVariant usage, such as how to handle multi-sample calling, why certain errors occur, model compatibility issues, and GPU memory requirements. These are natural language explanations that provide guidance and information for users."
Availability,"implex reads. In this case study, we describe applying DeepVariant to Oxford Nanopore R10.4.1; simplex reads. Then we assess the quality of the DeepVariant variant calls with; `hap.py`. To make it faster to go over this case study, we run only on chromosome 20. The dataset used in this case-study has following attributes:. ```bash; Sample: HG003; Region: Chr20; Chemistry: ONT R10.4.1; Coverage: 80x; ```. **Model note:**. * The model is trained with Guppy 6+ ""SUP"" Simplex and Dorado v0.1.1 Duplex; reads. * The model is trained on both Ultra-long and sheared reads with varying read; N50 and coverage. ## Prepare environment. In this case-study, we will use [Docker](https://docs.docker.com/get-docker/) to; run DeepVariant for variant calling and; [hap.py](https://github.com/illumina/hap.py) for benchmarking. If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/ont-case-study"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input/data""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG003 Ultra-long chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam.bai > ${INPUT_DIR}/HG003_R10",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md:1063,down,download,1063,docs/deepvariant-ont-r104-simplex-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md,1,['down'],['download'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: implex reads. In this case study, we describe applying DeepVariant to Oxford Nanopore R10.4.1; simplex reads. Then we assess the quality of the DeepVariant variant calls with; `hap.py`. To make it faster to go over this case study, we run only on chromosome 20. The dataset used in this case-study has following attributes:. ```bash; Sample: HG003; Region: Chr20; Chemistry: ONT R10.4.1; Coverage: 80x; ```. **Model note:**. * The model is trained with Guppy 6+ ""SUP"" Simplex and Dorado v0.1.1 Duplex; reads. * The model is trained on both Ultra-long and sheared reads with varying read; N50 and coverage. ## Prepare environment. In this case-study, we will use [Docker](https://docs.docker.com/get-docker/) to; run DeepVariant for variant calling and; [hap.py](https://github.com/illumina/hap.py) for benchmarking. If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/ont-case-study"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input/data""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG003 Ultra-long chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam.bai > ${INPUT_DIR}/HG003_R10
",False,"The content contains detailed step-by-step instructions for setting up and running a case study involving DeepVariant and hap.py. It includes bash commands for downloading and preparing data, which are code snippets, but it also provides explanatory text that discusses the purpose of each step and overall workflow."
Availability,"k_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG004.novaseq.pcr-",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:2805,avail,available,2805,docs/deeptrio-wgs-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md,1,['avail'],['available'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: k_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG004.novaseq.pcr-
",True,"The text consists of code snippets and command lines, such as 'curl' commands and file paths, which are indicative of programmatic API descriptions or build steps. These do not contain meaningful human-readable sentences but rather operational instructions."
Availability,"ll 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.ch",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:1915,down,downloaded,1915,docs/deepvariant-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md,1,['down'],['downloaded'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ll 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.ch
",False,"The text contains a series of detailed, human-readable instructions and steps to download necessary files and set up the environment. These are meaningful sentences discussing how to proceed with using the DeepVariant binaries."
Availability,"loUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bai -o HG003.bai; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bam -o HG004.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bai -o HG004.bai; ```. ### Command for downloading the reference file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_fathe",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:3930,down,downloading,3930,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['down'],['downloading'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: loUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bai -o HG003.bai; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bam -o HG004.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bai -o HG004.bai; ```. ### Command for downloading the reference file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_fathe
",False,"The text contains commands and scripts used for downloading data files, which are part of a build process. However, it also includes sentences explaining the purpose of each command and the steps involved in processing the data. These sentences are meaningful and provide explanatory content. Therefore, the text should not be eliminated as it contains both commands and descriptive sentences."
Availability,"n` to run for a while before the plots will appear. ### Test the model. Now that we have performed training, we can test the performance of the new; model using our holdout dataset (chr20). The following one-step command can be used to call DeepVariant and run our newly; trained model:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ""${DOCKER_IMAGE}-gpu"" \; run_deepvariant \; --model_type WGS \; --customized_model ""${BEST_CHECKPOINT}"" \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/test_set.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. In v1.4.0, by using `--model_type WGS`, `run_deepvariant` will automatically add; `insert_size` as an extra channel in the `make_examples` step. So we don't need; to add it in `--make_examples_extra_args`. When the `call_variants` step is run, you might see messages like:. ```; E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. You can use `nvidia-smi` to confirm whether the GPUs are used. If so, you can; ignore the message. Once this is done, we have the final callset in VCF format here:; `${OUTPUT_DIR}/test_set.vcf.gz`. Next step is to run `hap.py` to complete the; evaluation on chromosome 20:. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. time sudo docker run -it \; -v ""${DATA_DIR}:${DATA_DIR}"" \; -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/test_set.vcf.gz"" \; -f ""${TRUTH_BED}"" \; -r ""${REF}"" \; -o ""${OUTPUT_DIR}/chr20-calling.happy.output"" \; -l chr20 \; --engine=vcfeval \; --pass-only; ```. The output of `hap.py` is here:. ```; [I] Total VCF records: 3775119; [I] Non-reference VCF records: 3775119; [W] overlapping records at chr20:60402030 for sample 0; [W] Variants that overlap on the reference allele: 1; [I] Total VCF records: 132914; [I] Non-reference VCF records: 96273;",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:17437,error,error,17437,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['error'],['error'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: n` to run for a while before the plots will appear. ### Test the model. Now that we have performed training, we can test the performance of the new; model using our holdout dataset (chr20). The following one-step command can be used to call DeepVariant and run our newly; trained model:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ""${DOCKER_IMAGE}-gpu"" \; run_deepvariant \; --model_type WGS \; --customized_model ""${BEST_CHECKPOINT}"" \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/test_set.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. In v1.4.0, by using `--model_type WGS`, `run_deepvariant` will automatically add; `insert_size` as an extra channel in the `make_examples` step. So we don't need; to add it in `--make_examples_extra_args`. When the `call_variants` step is run, you might see messages like:. ```; E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. You can use `nvidia-smi` to confirm whether the GPUs are used. If so, you can; ignore the message. Once this is done, we have the final callset in VCF format here:; `${OUTPUT_DIR}/test_set.vcf.gz`. Next step is to run `hap.py` to complete the; evaluation on chromosome 20:. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. time sudo docker run -it \; -v ""${DATA_DIR}:${DATA_DIR}"" \; -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/test_set.vcf.gz"" \; -f ""${TRUTH_BED}"" \; -r ""${REF}"" \; -o ""${OUTPUT_DIR}/chr20-calling.happy.output"" \; -l chr20 \; --engine=vcfeval \; --pass-only; ```. The output of `hap.py` is here:. ```; [I] Total VCF records: 3775119; [I] Non-reference VCF records: 3775119; [W] overlapping records at chr20:60402030 for sample 0; [W] Variants that overlap on the reference allele: 1; [I] Total VCF records: 132914; [I] Non-reference VCF records: 96273;
",False,"The text contains a mix of natural language explanations interspersed with code snippets, but the majority of the content is descriptive and explanatory. The code examples are included for procedural guidance but do not dominate the text."
Availability,"ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bam -o HG003.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bai -o HG003.bai; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bam -o HG004.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bai -o HG004.bai; ```. ### Command for downloading the reference file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:3613,down,downloading,3613,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['down'],['downloading'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bam -o HG003.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bai -o HG003.bai; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bam -o HG004.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bai -o HG004.bai; ```. ### Command for downloading the reference file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -
",False,
Availability,"ng from the quick start linked above:; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". BIN_VERSION=""1.6.1"" # show_examples is available only in version 1.0.0 and later.; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \; --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \; --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \; --output=/output/pileup \; --num_records=20 \; --curate. # And then your images are here:; ls ""${OUTPUT_DIR}""/pileup*.png; ```. ## Try it with these powerful optional parameters. * Filter to regions? Use e.g. `--regions chr20:1-3000000` or paths to BED or; BEDPE files.; * Filter to records from a VCF? Use `--vcf variants.vcf`. This can be a piece; of a VCF, e.g. grepping a hap.py output VCF for false positives. This is a; powerful way to pick out variants of interest and investigate them in more; depth.; * Stop after a certain number of examples, e.g. 10? Use `--num_records 10`.; * Sharded examples? Use for example, `--examples make_examples.tfrecord@64.gz`; to search through them all. This is best paired with `--regions` or `--vcf`; to narrow down to a small number of examples of interest. You can also use; the actual filename of a single make_examples file to only read that one, as; shown in the sample code above.; * Use `--curate` to create a TSV file with concepts for each pileup. Then; filter that TSV in any way you want and read that filtered TSV in using; `--filter_by_tsv` to e.g. get pileup images only for examples with low; mapping quality, many errors, nearby variants, or any other concepts.; Filtering can be done any way you want, `grep` would be an easy option (the; TSV's header is not needed).; * Write out example tfrecords using `--write_tfrecords` after applying any; filtering using the options above.; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md:3328,down,down,3328,docs/show-examples.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md,2,"['down', 'error']","['down', 'errors']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ng from the quick start linked above:; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". BIN_VERSION=""1.6.1"" # show_examples is available only in version 1.0.0 and later.; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \; --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \; --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \; --output=/output/pileup \; --num_records=20 \; --curate. # And then your images are here:; ls ""${OUTPUT_DIR}""/pileup*.png; ```. ## Try it with these powerful optional parameters. * Filter to regions? Use e.g. `--regions chr20:1-3000000` or paths to BED or; BEDPE files.; * Filter to records from a VCF? Use `--vcf variants.vcf`. This can be a piece; of a VCF, e.g. grepping a hap.py output VCF for false positives. This is a; powerful way to pick out variants of interest and investigate them in more; depth.; * Stop after a certain number of examples, e.g. 10? Use `--num_records 10`.; * Sharded examples? Use for example, `--examples make_examples.tfrecord@64.gz`; to search through them all. This is best paired with `--regions` or `--vcf`; to narrow down to a small number of examples of interest. You can also use; the actual filename of a single make_examples file to only read that one, as; shown in the sample code above.; * Use `--curate` to create a TSV file with concepts for each pileup. Then; filter that TSV in any way you want and read that filtered TSV in using; `--filter_by_tsv` to e.g. get pileup images only for examples with low; mapping quality, many errors, nearby variants, or any other concepts.; Filtering can be done any way you want, `grep` would be an easy option (the; TSV's header is not needed).; * Write out example tfrecords using `--write_tfrecords` after applying any; filtering using the options above.; 
",False,"The text contains instructions and explanations about using command-line tools, including details on how to run Docker commands and parameters for generating examples. While there are some technical terms and commands, they are presented in a readable format meant for human interaction, not as code or logs."
Availability,"noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 PacBio HiFi WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10).; These reads have been aligned to the GRCh38_no_alt_analysis reference using; [pbmm2](https://github.com/PacificBiosciences/pbmm2). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/pacbio-case-study-testdata. curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam > input/HG002.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG002.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam > input/HG003.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG003.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam > input/HG004.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG004.pfd",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:2877,avail,available,2877,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,1,['avail'],['available'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 PacBio HiFi WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10).; These reads have been aligned to the GRCh38_no_alt_analysis reference using; [pbmm2](https://github.com/PacificBiosciences/pbmm2). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/pacbio-case-study-testdata. curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam > input/HG002.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG002.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam > input/HG003.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG003.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam > input/HG004.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG004.pfd
",True,"The text consists primarily of code snippets and commands. It contains cURL commands for downloading files, which are typical of programmatic API descriptions or build instructions. Additionally, there are references to file paths and specific filenames, which are more relevant to a programming context rather than meaningful human-readable content."
Availability,"on-human variant calling using species-specific DeepVariant models](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/)”. If these reasons seem applicable, there could be some other reason DeepVariant; determined the position is not variant. You can catalog the variant position and; its support. The way to improve variant calling for these positions is to train; new models, but be aware that training is already a balance between reducing; false negatives and positives, and it may not be possible to call variants like; the one you are seeing without increasing overall false positives by a greater; amount. ## How does DeepVariant use pileup images to call variants?. See this; [blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). ## What happens if I change the pileup_image_height?. If the actual depth in a particular region is greater than the pileup image; height, DeepVariant randomly downsamples reads until the image has been filled; up. For the default DeepVariant models (height 100), an image can accommodate at; most 95 reads in a given region (5 rows are reserved for the reference; sequence). You may be able to successfully run our pretrained models with a different; pileup image height (via `--pileup_image_height` in `make_examples.py`),; depending on the new height, but we generally do not recommend using different; image heights at training and inference time. If you wish to use a different; pileup image height, we recommend retraining a new model with images of that; height. If you are working with extremely high coverage sequencing data for applications; such as somatic sequencing, we recommend using a somatic caller instead of; DeepVariant, which is a germline caller. ## Can I use DeepVariant for somatic (non-germline) calling?. We do not recommend using DeepVariant for somatic calling. We do have a; prototype implementation for s",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:4632,down,downsamples,4632,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['down'],['downsamples'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: on-human variant calling using species-specific DeepVariant models](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/)”. If these reasons seem applicable, there could be some other reason DeepVariant; determined the position is not variant. You can catalog the variant position and; its support. The way to improve variant calling for these positions is to train; new models, but be aware that training is already a balance between reducing; false negatives and positives, and it may not be possible to call variants like; the one you are seeing without increasing overall false positives by a greater; amount. ## How does DeepVariant use pileup images to call variants?. See this; [blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). ## What happens if I change the pileup_image_height?. If the actual depth in a particular region is greater than the pileup image; height, DeepVariant randomly downsamples reads until the image has been filled; up. For the default DeepVariant models (height 100), an image can accommodate at; most 95 reads in a given region (5 rows are reserved for the reference; sequence). You may be able to successfully run our pretrained models with a different; pileup image height (via `--pileup_image_height` in `make_examples.py`),; depending on the new height, but we generally do not recommend using different; image heights at training and inference time. If you wish to use a different; pileup image height, we recommend retraining a new model with images of that; height. If you are working with extremely high coverage sequencing data for applications; such as somatic sequencing, we recommend using a somatic caller instead of; DeepVariant, which is a germline caller. ## Can I use DeepVariant for somatic (non-germline) calling?. We do not recommend using DeepVariant for somatic calling. We do have a; prototype implementation for s
",False,"The text contains complete, meaningful sentences in natural language that discuss variant calling using DeepVariant models and how different parameters affect the process. It also includes explanatory content about when to use specific features or callers for certain sequencing applications."
Availability,"ootnote3"">(3)</a>: In v0.8, we used the; [Platinum Genomes Truthset](https://github.com/Illumina/PlatinumGenomes) to; create more training examples outside the GIAB confident regions. <a name=""vfootnote4"">(4)</a>: Previously, we split train/tune by leaving 3 WES; for tuning. Starting from this release, we leave out chr1 and chr20 from; training, and use chr1 for tuning. <a name=""vfootnote5"">(5)</a>: Starting from this version, we padded (100bps on; both sides) of the capture BED and used that for generating training examples.; We also added more `downsample_fraction`. <a name=""vfootnote6"">(6)</a>: (Before v1.0) PacBio is the only one we currently; uses HG002 in training and tuning. <a name=""vfootnote7"">(7)</a>: In v1.0, we train on HG002-HG004 for WGS as well,; but only using examples from the region of NIST truth confident region v4.2; subtracting v3.3.2. <a name=""vfootnote8"">(8)</a>: In v1.0, PacBio training data contains training; examples with haplotag sorted images and unsorted images. <a name=""vfootnote9"">(9)</a>: In v1.1, we exclude HG003 from training. And we; use all NIST truth confident regions for HG001-HG007 (except for HG003) for; training. We've always excluded chr20-22 from training. <a name=""vfootnote10"">(10)</a>: In v1.2, we include new PacBio training data; from Sequel II, Chemistry 2.2. <a name=""vfootnote11"">(11)</a>: Between v1.1 and v1.2, we fixed an issue where; make_examples can generate fewer class 0 (REF) training examples than before.; This is the reason for more training examples in v1.2 when number of samples; didn't increase. <a name=""vfootnote12"">(12)</a>: In v1.2, we created BAM files with 100bp reads; and 125bp reads by trimming to augment the training data. ## Training data:. See ""[An Extensive Sequence Dataset of Gold-Standard Samples for Benchmarking and Development](https://doi.org/10.1101/2020.12.11.422022)""; for a publicly available set of data we released. Data download information can; be found in the supplementary material.; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details-training-data.md:6717,avail,available,6717,docs/deepvariant-details-training-data.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details-training-data.md,2,"['avail', 'down']","['available', 'download']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ootnote3"">(3)</a>: In v0.8, we used the; [Platinum Genomes Truthset](https://github.com/Illumina/PlatinumGenomes) to; create more training examples outside the GIAB confident regions. <a name=""vfootnote4"">(4)</a>: Previously, we split train/tune by leaving 3 WES; for tuning. Starting from this release, we leave out chr1 and chr20 from; training, and use chr1 for tuning. <a name=""vfootnote5"">(5)</a>: Starting from this version, we padded (100bps on; both sides) of the capture BED and used that for generating training examples.; We also added more `downsample_fraction`. <a name=""vfootnote6"">(6)</a>: (Before v1.0) PacBio is the only one we currently; uses HG002 in training and tuning. <a name=""vfootnote7"">(7)</a>: In v1.0, we train on HG002-HG004 for WGS as well,; but only using examples from the region of NIST truth confident region v4.2; subtracting v3.3.2. <a name=""vfootnote8"">(8)</a>: In v1.0, PacBio training data contains training; examples with haplotag sorted images and unsorted images. <a name=""vfootnote9"">(9)</a>: In v1.1, we exclude HG003 from training. And we; use all NIST truth confident regions for HG001-HG007 (except for HG003) for; training. We've always excluded chr20-22 from training. <a name=""vfootnote10"">(10)</a>: In v1.2, we include new PacBio training data; from Sequel II, Chemistry 2.2. <a name=""vfootnote11"">(11)</a>: Between v1.1 and v1.2, we fixed an issue where; make_examples can generate fewer class 0 (REF) training examples than before.; This is the reason for more training examples in v1.2 when number of samples; didn't increase. <a name=""vfootnote12"">(12)</a>: In v1.2, we created BAM files with 100bp reads; and 125bp reads by trimming to augment the training data. ## Training data:. See ""[An Extensive Sequence Dataset of Gold-Standard Samples for Benchmarking and Development](https://doi.org/10.1101/2020.12.11.422022)""; for a publicly available set of data we released. Data download information can; be found in the supplementary material.; 
",False,"The text consists primarily of version control metadata or comments, including footnotes and references to previous versions (v0.8, v1.0, etc.), which are typically not meant for human reading as they are part of the development process."
Availability,"ost likely haplotypes are; > selected that best explain the read evidence. The likelihood function used to; > score haplotypes is a traditional pair HMM with fixed parameters that do not; > depend on base quality scores. This likelihood function assumes that each read; > is independent. Finally, each read is then realigned to its most likely; > haplotype. This procedure updates both the position and the CIGAR string for; > each read. Local realignment is not performed for long reads (PacBio, and other similar; technologies). The realigner step can optionally be switched off using; `--norealign_reads`. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. This can be done by passing the following parameters:; `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`. Note that this is meant for debugging and produces a bam file for every; candidate variant, which can result in millions of tiny bam files, so when using; this, narrow down the DeepVariant run using `--regions` to just the variants you; want to inspect more closely. ## How are `AD` and `DP` values calculated?. In order to efficiently perform variant calling, DeepVariant partitions the; genome into chunks (set by `--partition_size`), and will read in a max number of; reads into each partition (set by `--max_reads_per_partition`). By default,; `--partition_size` is set to 1000 and `--max_reads_per_partition` is set to; 1500. The `AD` and `DP` values are based on the read depths constrained by; `--max_reads_per_partition`. For example, if you have a depth of 2000x at a given site, DeepVariant will; subsample 1500 reads, and `DP` or `AD` will be capped at 1500. If you want to; calculate the true `AD` and `DP` values at high-depth regions, you can set; `--max_reads_per_partition=0` to calculate `AD` and `DP` using all reads. In; practice, capping reads per partition reduces runtimes with little/no impact on; accuracy. ##",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:12545,down,down,12545,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['down'],['down'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ost likely haplotypes are; > selected that best explain the read evidence. The likelihood function used to; > score haplotypes is a traditional pair HMM with fixed parameters that do not; > depend on base quality scores. This likelihood function assumes that each read; > is independent. Finally, each read is then realigned to its most likely; > haplotype. This procedure updates both the position and the CIGAR string for; > each read. Local realignment is not performed for long reads (PacBio, and other similar; technologies). The realigner step can optionally be switched off using; `--norealign_reads`. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. This can be done by passing the following parameters:; `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`. Note that this is meant for debugging and produces a bam file for every; candidate variant, which can result in millions of tiny bam files, so when using; this, narrow down the DeepVariant run using `--regions` to just the variants you; want to inspect more closely. ## How are `AD` and `DP` values calculated?. In order to efficiently perform variant calling, DeepVariant partitions the; genome into chunks (set by `--partition_size`), and will read in a max number of; reads into each partition (set by `--max_reads_per_partition`). By default,; `--partition_size` is set to 1000 and `--max_reads_per_partition` is set to; 1500. The `AD` and `DP` values are based on the read depths constrained by; `--max_reads_per_partition`. For example, if you have a depth of 2000x at a given site, DeepVariant will; subsample 1500 reads, and `DP` or `AD` will be capped at 1500. If you want to; calculate the true `AD` and `DP` values at high-depth regions, you can set; `--max_reads_per_partition=0` to calculate `AD` and `DP` using all reads. In; practice, capping reads per partition reduces runtimes with little/no impact on; accuracy. ##
",False,"The text contains several meaningful sentences explaining the process of variant calling in DeepVariant, including how AD and DP values are calculated."
Availability,"ows are reserved for the reference; sequence). You may be able to successfully run our pretrained models with a different; pileup image height (via `--pileup_image_height` in `make_examples.py`),; depending on the new height, but we generally do not recommend using different; image heights at training and inference time. If you wish to use a different; pileup image height, we recommend retraining a new model with images of that; height. If you are working with extremely high coverage sequencing data for applications; such as somatic sequencing, we recommend using a somatic caller instead of; DeepVariant, which is a germline caller. ## Can I use DeepVariant for somatic (non-germline) calling?. We do not recommend using DeepVariant for somatic calling. We do have a; prototype implementation for somatic calling, which can take a tumor and normal; BAM and call subclonal variants. However, we don't yet have enough confidence in; the available truth sets, and that they come from a diverse enough sampling of; cancers with mutational profiles, for us to be certain in releasing something of; high quality. We're watching developments in the area of these truth sets and; hope to be able to further develop the somatic caller in the future. ## Can I use DeepVariant on plant genomes?. DeepVariant has previously been applied to plant species. In the case of rice,; there was good evidence of high accuracy. You can see; [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant).; However, these rice genomes were diploid and with a similar variant density of; humans. DeepVariant is currently written to be a diploid variant caller. So if the plant; species you are working with is polyploid, it is not yet clear how DeepVariant; will perform. That is because even with re-training, DeepVariant can only; produce variant calls that are homozygous alternate, heterozygous, or homozygous; reference, which don",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:5743,avail,available,5743,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['avail'],['available'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ows are reserved for the reference; sequence). You may be able to successfully run our pretrained models with a different; pileup image height (via `--pileup_image_height` in `make_examples.py`),; depending on the new height, but we generally do not recommend using different; image heights at training and inference time. If you wish to use a different; pileup image height, we recommend retraining a new model with images of that; height. If you are working with extremely high coverage sequencing data for applications; such as somatic sequencing, we recommend using a somatic caller instead of; DeepVariant, which is a germline caller. ## Can I use DeepVariant for somatic (non-germline) calling?. We do not recommend using DeepVariant for somatic calling. We do have a; prototype implementation for somatic calling, which can take a tumor and normal; BAM and call subclonal variants. However, we don't yet have enough confidence in; the available truth sets, and that they come from a diverse enough sampling of; cancers with mutational profiles, for us to be certain in releasing something of; high quality. We're watching developments in the area of these truth sets and; hope to be able to further develop the somatic caller in the future. ## Can I use DeepVariant on plant genomes?. DeepVariant has previously been applied to plant species. In the case of rice,; there was good evidence of high accuracy. You can see; [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant).; However, these rice genomes were diploid and with a similar variant density of; humans. DeepVariant is currently written to be a diploid variant caller. So if the plant; species you are working with is polyploid, it is not yet clear how DeepVariant; will perform. That is because even with re-training, DeepVariant can only; produce variant calls that are homozygous alternate, heterozygous, or homozygous; reference, which don
",False,"The text contains meaningful sentences discussing the use of DeepVariant for different purposes, such as somatic sequencing and plant genomes. It provides recommendations and explains why certain configurations might not be ideal, including references to specific blog posts. The content is informative and written in a natural, explanatory manner without any code snippets or logs."
Availability,"ples` and; `postprocess_variants` steps. ### `make_examples`. The `make_examples` program is where the gVCF records are computed. One additional flag is required in `make_examples`, the `--gvcf <filename>`; flag. This specifies an additional output, which is a TFRecord file of Variant; protocol buffers. If running with multiple processes, the sharding applied to; this output filename must be the same as that applied to the `--examples`; output. A concrete example call, using variables defined in the [WGS case study]:. ```bash; GVCF_TFRECORDS=""${OUTPUT_DIR}/HG002.gvcf.tfrecord@${N_SHARDS}.gz"". ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`; ```. NOTE: gVCF outputs are only valid when `make_examples` is run in ""calling"" mode;; if attempted to run in ""training"" mode the program will exit and notify the user; of the error. ### `postprocess_variants`. When run in gVCF mode, the `postprocess_variants` program handles the creation; of the final gVCF file that incorporates both the non-variant records and the; true variants discovered by the previous programs. Two additional flags are required in `postprocess_variants`, the input; `--nonvariant_site_tfrecord_path <filename>` which corresponds to the TFRecord; of Variant protocol buffers created in `make_examples`, and the output; `--gvcf_outfile <filename>` which is the final gVCF output. A concrete example call, using variables defined in the [WGS case study] and in; the above `make_examples` example:. ```bash; OUTPUT_GVCF=""${OUTPUT_DIR}/HG002.output.g.vcf.gz"". ( time python ""${BIN_DIR}""/postprocess_variants.zip \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${OUTPUT_VCF}"" \; --nonvariant_site_tfrecord_path ""${GVCF_TFRECORDS}"" \; --gvcf_outfile """,MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:2981,error,error,2981,docs/deepvariant-gvcf-support.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md,1,['error'],['error'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ples` and; `postprocess_variants` steps. ### `make_examples`. The `make_examples` program is where the gVCF records are computed. One additional flag is required in `make_examples`, the `--gvcf <filename>`; flag. This specifies an additional output, which is a TFRecord file of Variant; protocol buffers. If running with multiple processes, the sharding applied to; this output filename must be the same as that applied to the `--examples`; output. A concrete example call, using variables defined in the [WGS case study]:. ```bash; GVCF_TFRECORDS=""${OUTPUT_DIR}/HG002.gvcf.tfrecord@${N_SHARDS}.gz"". ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`; ```. NOTE: gVCF outputs are only valid when `make_examples` is run in ""calling"" mode;; if attempted to run in ""training"" mode the program will exit and notify the user; of the error. ### `postprocess_variants`. When run in gVCF mode, the `postprocess_variants` program handles the creation; of the final gVCF file that incorporates both the non-variant records and the; true variants discovered by the previous programs. Two additional flags are required in `postprocess_variants`, the input; `--nonvariant_site_tfrecord_path <filename>` which corresponds to the TFRecord; of Variant protocol buffers created in `make_examples`, and the output; `--gvcf_outfile <filename>` which is the final gVCF output. A concrete example call, using variables defined in the [WGS case study] and in; the above `make_examples` example:. ```bash; OUTPUT_GVCF=""${OUTPUT_DIR}/HG002.output.g.vcf.gz"". ( time python ""${BIN_DIR}""/postprocess_variants.zip \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${OUTPUT_VCF}"" \; --nonvariant_site_tfrecord_path ""${GVCF_TFRECORDS}"" \; --gvcf_outfile ""
",False,"The text contains descriptive explanations about the use of programs and flags, which are complete sentences in natural language."
Availability,"r training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir""; tensorboard --logdir ${TRAINING_DIR",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14980,checkpoint,checkpoint,14980,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['checkpoint'],['checkpoint'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: r training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir""; tensorboard --logdir ${TRAINING_DIR
",False,"The text contains complete, meaningful sentences in natural language discussing training parameters, evaluation processes, and setup instructions."
Availability,"r20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. This should create a subdirectory in the current directory containing the actual; data files:. ```bash; ls -1 ${INPUT_DIR}; ```. outputting:. ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ### Model location (optional). Starting from r0.8, we put the model files inside the released Docker images.; So there is no need to download model files anymore. If you want to find the; model files of all releases, you can find them in our bucket on the Google Cloud; Storage. You can view them in the browser:; https://console.cloud.google.com/storage/browser/deepvariant/models/DeepVariant. ## Run DeepVariant with one command. DeepVariant consists of 3 main binaries: `make_examples`, `call_variants`, and; `postprocess_variants`. To make it easier to run, we create one entrypoint that; can be directly run as a docker command. If you want to see the details, you can; read through [run_deepvariant.py]. ```bash; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; ```. You can run everything with the following command:. ```bash; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:3592,down,download,3592,docs/deepvariant-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md,1,['down'],['download'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: r20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. This should create a subdirectory in the current directory containing the actual; data files:. ```bash; ls -1 ${INPUT_DIR}; ```. outputting:. ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ### Model location (optional). Starting from r0.8, we put the model files inside the released Docker images.; So there is no need to download model files anymore. If you want to find the; model files of all releases, you can find them in our bucket on the Google Cloud; Storage. You can view them in the browser:; https://console.cloud.google.com/storage/browser/deepvariant/models/DeepVariant. ## Run DeepVariant with one command. DeepVariant consists of 3 main binaries: `make_examples`, `call_variants`, and; `postprocess_variants`. To make it easier to run, we create one entrypoint that; can be directly run as a docker command. If you want to see the details, you can; read through [run_deepvariant.py]. ```bash; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; ```. You can run everything with the following command:. ```bash; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions
",False,"The text contains a mixture of command lines and explanations, including bash commands, file downloads, and instructions for running DeepVariant. However, the content is not purely code or logs but rather a set of steps to perform in a terminal, possibly within a script or document explaining how to use the tool."
Availability,"red \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCK",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14903,checkpoint,checkpoints,14903,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['checkpoint'],['checkpoints'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: red \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCK
",False,
Availability,"ript, supply a `--logging_dir`; directory and set `--runtime_report`. For example, when following the; [quick start](deepvariant-quick-start.md), including `--runtime_report; --logging_dir=/output/logs` would produce the following runtime profiling output; file and visual report:. ```; /output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv; /output/logs/make_examples_runtime_by_region_report.html; ```. ### Running stages individually. If you are using make_examples itself without the run_deepvariant script, supply; make_examples with `--runtime_by_region=` a filename (.tsv). If the examples are; sharded then the runtime profile should be sharded into the same number of; shards, e.g. when using `examples@64` then runtimes could be `runtimes@64.tsv`. Then use the `runtime_by_region_vis` script to create a visual report of the; make_examples runtime by region data. Continuing from the quick start, it looks; like this:. ```bash; BIN_VERSION=""1.6.1"" # Only available in v1.1+.; docker run \; -v ""INPUT_DIR"":""/input"" \; -v ""OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/runtime_by_region_vis \; --input=/output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv \; --output=/output/logs/make_examples_runtime_by_region/make_examples_runtime_vis.html \; --title=""Quick start runtime profiling""; ```. ## Interpreting the runtime report. ### Where the data comes from. `make_examples`, the first stage of DeepVariant, starts from a BAM file and; corresponding reference FASTA file. One 1000 bp region at a time (set by; `--partition_size`), `make_examples` will:. 1. Get reads: Query the bam files for all the reads in the region. Optionally; (`--realign_reads`) do a local assembly of the reads and realign the reads; to the resulting haplotype graph. 2. Find candidates: Catalogue all the putative alternate alleles for all those; reads, and compare the accumulated evidence for each alt against th",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md:2302,avail,available,2302,docs/runtime-by-region.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md,1,['avail'],['available'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ript, supply a `--logging_dir`; directory and set `--runtime_report`. For example, when following the; [quick start](deepvariant-quick-start.md), including `--runtime_report; --logging_dir=/output/logs` would produce the following runtime profiling output; file and visual report:. ```; /output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv; /output/logs/make_examples_runtime_by_region_report.html; ```. ### Running stages individually. If you are using make_examples itself without the run_deepvariant script, supply; make_examples with `--runtime_by_region=` a filename (.tsv). If the examples are; sharded then the runtime profile should be sharded into the same number of; shards, e.g. when using `examples@64` then runtimes could be `runtimes@64.tsv`. Then use the `runtime_by_region_vis` script to create a visual report of the; make_examples runtime by region data. Continuing from the quick start, it looks; like this:. ```bash; BIN_VERSION=""1.6.1"" # Only available in v1.1+.; docker run \; -v ""INPUT_DIR"":""/input"" \; -v ""OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/runtime_by_region_vis \; --input=/output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv \; --output=/output/logs/make_examples_runtime_by_region/make_examples_runtime_vis.html \; --title=""Quick start runtime profiling""; ```. ## Interpreting the runtime report. ### Where the data comes from. `make_examples`, the first stage of DeepVariant, starts from a BAM file and; corresponding reference FASTA file. One 1000 bp region at a time (set by; `--partition_size`), `make_examples` will:. 1. Get reads: Query the bam files for all the reads in the region. Optionally; (`--realign_reads`) do a local assembly of the reads and realign the reads; to the resulting haplotype graph. 2. Find candidates: Catalogue all the putative alternate alleles for all those; reads, and compare the accumulated evidence for each alt against th
",False,"The text contains complete, meaningful sentences in natural language discussing concepts and explaining processes related to DeepVariant's runtime profiling."
Availability,"s. <a name=""myfootnote1"">(1)</a>: Time estimates do not include mapping. ## How DeepVariant works. ![Stages in DeepVariant](docs/images/inference_flow_diagram.svg). For more information on the pileup images and how to read them, please see the; [""Looking through DeepVariant's Eyes"" blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant relies on [Nucleus](https://github.com/google/nucleus), a library of; Python and C++ code for reading and writing data in common genomics file formats; (like SAM and VCF) designed for painless integration with the; [TensorFlow](https://www.tensorflow.org/) machine learning framework. Nucleus; was built with DeepVariant in mind and open-sourced separately so it can be used; by anyone in the genomics research community for other projects. See this blog; post on; [Using Nucleus and TensorFlow for DNA Sequencing Error Correction](https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/). ## DeepVariant Setup. ### Prerequisites. * Unix-like operating system (cannot run on Windows); * Python 3.8. ### Official Solutions. Below are the official solutions provided by the; [Genomics team in Google Health](https://health.google/health-research/). Name | Description; :-------------------------------------------------------------------------------------------------: | -----------; [Docker](docs/deepvariant-quick-start.md) | This is the recommended method.; [Build from source](docs/deepvariant-build-test.md) | DeepVariant comes with scripts to build it on Ubuntu 20.04. To build and run on other Unix-based systems, you will need to modify these scripts.; Prebuilt Binaries | Available at [`gs://deepvariant/`](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you will need a CPU (such as Intel Sandy Bridge) that supports them. You can check the `/proc/cpuinfo` file",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:9921,error,error-correction,9921,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['error'],['error-correction'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: s. <a name=""myfootnote1"">(1)</a>: Time estimates do not include mapping. ## How DeepVariant works. ![Stages in DeepVariant](docs/images/inference_flow_diagram.svg). For more information on the pileup images and how to read them, please see the; [""Looking through DeepVariant's Eyes"" blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant relies on [Nucleus](https://github.com/google/nucleus), a library of; Python and C++ code for reading and writing data in common genomics file formats; (like SAM and VCF) designed for painless integration with the; [TensorFlow](https://www.tensorflow.org/) machine learning framework. Nucleus; was built with DeepVariant in mind and open-sourced separately so it can be used; by anyone in the genomics research community for other projects. See this blog; post on; [Using Nucleus and TensorFlow for DNA Sequencing Error Correction](https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/). ## DeepVariant Setup. ### Prerequisites. * Unix-like operating system (cannot run on Windows); * Python 3.8. ### Official Solutions. Below are the official solutions provided by the; [Genomics team in Google Health](https://health.google/health-research/). Name | Description; :-------------------------------------------------------------------------------------------------: | -----------; [Docker](docs/deepvariant-quick-start.md) | This is the recommended method.; [Build from source](docs/deepvariant-build-test.md) | DeepVariant comes with scripts to build it on Ubuntu 20.04. To build and run on other Unix-based systems, you will need to modify these scripts.; Prebuilt Binaries | Available at [`gs://deepvariant/`](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you will need a CPU (such as Intel Sandy Bridge) that supports them. You can check the `/proc/cpuinfo` file
",False,"The text includes multiple complete sentences discussing how DeepVariant works, its components like Nucleus, and setup instructions which are all explanatory and meaningful for humans."
Availability,"s; [example command to start a machine](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). The steps in this document can be extended to merge larger cohorts as well. See this workflow:. ![workflow](images/cohort-workflow.png?raw=true ""DeepVariant+GLnexus cohort workflow""). A few things to note before we start:. * It is recommended to use BAM files with original quality scores. In the case; that BAM files went through recalibration, optional DV flags can be used in; order to use original scores: `--parse_sam_aux_fields`,; `--use_original_quality_scores`.; * DeepVariant optionally allows gVCF output. This option is required for; further GLnexus analysis in this document. ## Dataset. The Whole Exome Sequencing (WES) dataset we're using is from:. [ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/). * HG002_NA24385_son; * HG003_NA24149_father; * HG004_NA24143_mother. ### Commands for downloading the input BAMs. Just for convenience, we use aria2 to download our data. You can change it to; whatever other tools (wget, curl) that you prefer. To install aria2, you can run: `sudo apt-get -y install aria2`. ```; DIR=""${PWD}/trio""; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam -o HG002.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai -o HG002.bai; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bam -o HG003.bam; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:1822,down,downloading,1822,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['down'],['downloading'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: s; [example command to start a machine](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). The steps in this document can be extended to merge larger cohorts as well. See this workflow:. ![workflow](images/cohort-workflow.png?raw=true ""DeepVariant+GLnexus cohort workflow""). A few things to note before we start:. * It is recommended to use BAM files with original quality scores. In the case; that BAM files went through recalibration, optional DV flags can be used in; order to use original scores: `--parse_sam_aux_fields`,; `--use_original_quality_scores`.; * DeepVariant optionally allows gVCF output. This option is required for; further GLnexus analysis in this document. ## Dataset. The Whole Exome Sequencing (WES) dataset we're using is from:. [ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/). * HG002_NA24385_son; * HG003_NA24149_father; * HG004_NA24143_mother. ### Commands for downloading the input BAMs. Just for convenience, we use aria2 to download our data. You can change it to; whatever other tools (wget, curl) that you prefer. To install aria2, you can run: `sudo apt-get -y install aria2`. ```; DIR=""${PWD}/trio""; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam -o HG002.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai -o HG002.bai; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bam -o HG003.bam; 
",False,
Availability,"so the paths are the same inside; and outside the Docker container. ```; echo $HOME # see what your home directory is first.; ls $HOME; BIN_VERSION=""1.6.1""; sudo docker run \; -v ""${HOME}"":""${HOME}"" \; google/deepvariant:""${BIN_VERSION}"" \; ls $HOME; ```. ## How do I run multi-sample calling?. Since the DeepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. We; have tested and confirmed that this does not affect GPU usage or inference. So; you can continue running DeepVariant without being worried about this message. ## How much GPU memory is needed for the Keras models?. 16GB. In our test, we observe the model occupying 16GB GPU memory. ## Do models from before r1.6.0 work with current inference code?. No. We have moved from Slim to Keras. All models before `1.6.0` were trained in; Slim platform. So they are not compatible with `1.6.0` anymore. ## Can call_variants be run on multiple GPUs?. No. Although possible, we have not implemented the multi-GPU capability in GPU; inference yet. ## Can model_train be run on multiple GPUs?. No. TensorFlow's Estimator API does provide support for running training on; multiple GPUs through the use of a DistributionStrategy. However,; DistributionStrategy cannot be used with exponential moving average (EMA), which; is present in the DeepVariant codebase",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:9204,error,error,9204,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['error'],['error'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: so the paths are the same inside; and outside the Docker container. ```; echo $HOME # see what your home directory is first.; ls $HOME; BIN_VERSION=""1.6.1""; sudo docker run \; -v ""${HOME}"":""${HOME}"" \; google/deepvariant:""${BIN_VERSION}"" \; ls $HOME; ```. ## How do I run multi-sample calling?. Since the DeepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. We; have tested and confirmed that this does not affect GPU usage or inference. So; you can continue running DeepVariant without being worried about this message. ## How much GPU memory is needed for the Keras models?. 16GB. In our test, we observe the model occupying 16GB GPU memory. ## Do models from before r1.6.0 work with current inference code?. No. We have moved from Slim to Keras. All models before `1.6.0` were trained in; Slim platform. So they are not compatible with `1.6.0` anymore. ## Can call_variants be run on multiple GPUs?. No. Although possible, we have not implemented the multi-GPU capability in GPU; inference yet. ## Can model_train be run on multiple GPUs?. No. TensorFlow's Estimator API does provide support for running training on; multiple GPUs through the use of a DistributionStrategy. However,; DistributionStrategy cannot be used with exponential moving average (EMA), which; is present in the DeepVariant codebase
",False,"The text contains complete sentences and explanations about how to run multi-sample calling, error messages regarding CUDA initialization, GPU memory requirements, compatibility of models, and usage of multiple GPUs for training. It also includes recommendations for best practices and links to relevant documentation. These are all meaningful human-readable content that provide guidance and information."
Availability,"tart guide](deepvariant-quick-start.md); and case studies that used the Docker version, then these files are usually; hidden inside the Docker container. But you can get them exported into the same; output directory where the VCF file appears by adding the following setting in; the `run_deepvariant` command. ```bash; # Add the following to your run_deepvariant command.; --intermediate_results_dir=/output/; ```. Then the make_examples file should appear in the directory docker mounted as; `/output/`. For example, if you followed the; [quick-start documentation](deepvariant-quick-start.md), it looks like this:; `${OUTPUT_DIR}/make_examples.tfrecord-00000-of-00001.gz`. ## Running show_examples. Once you have a make_examples output tfrecord file, then you can run; `show_examples` to see the pileup images inside:. ```bash; # Continuing from the quick start linked above:; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". BIN_VERSION=""1.6.1"" # show_examples is available only in version 1.0.0 and later.; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \; --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \; --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \; --output=/output/pileup \; --num_records=20 \; --curate. # And then your images are here:; ls ""${OUTPUT_DIR}""/pileup*.png; ```. ## Try it with these powerful optional parameters. * Filter to regions? Use e.g. `--regions chr20:1-3000000` or paths to BED or; BEDPE files.; * Filter to records from a VCF? Use `--vcf variants.vcf`. This can be a piece; of a VCF, e.g. grepping a hap.py output VCF for false positives. This is a; powerful way to pick out variants of interest and investigate them in more; depth.; * Stop after a certain number of examples, e.g. 10? Use `--num_records 10`.; * Sharded exam",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md:2172,avail,available,2172,docs/show-examples.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md,1,['avail'],['available'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: tart guide](deepvariant-quick-start.md); and case studies that used the Docker version, then these files are usually; hidden inside the Docker container. But you can get them exported into the same; output directory where the VCF file appears by adding the following setting in; the `run_deepvariant` command. ```bash; # Add the following to your run_deepvariant command.; --intermediate_results_dir=/output/; ```. Then the make_examples file should appear in the directory docker mounted as; `/output/`. For example, if you followed the; [quick-start documentation](deepvariant-quick-start.md), it looks like this:; `${OUTPUT_DIR}/make_examples.tfrecord-00000-of-00001.gz`. ## Running show_examples. Once you have a make_examples output tfrecord file, then you can run; `show_examples` to see the pileup images inside:. ```bash; # Continuing from the quick start linked above:; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". BIN_VERSION=""1.6.1"" # show_examples is available only in version 1.0.0 and later.; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \; --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \; --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \; --output=/output/pileup \; --num_records=20 \; --curate. # And then your images are here:; ls ""${OUTPUT_DIR}""/pileup*.png; ```. ## Try it with these powerful optional parameters. * Filter to regions? Use e.g. `--regions chr20:1-3000000` or paths to BED or; BEDPE files.; * Filter to records from a VCF? Use `--vcf variants.vcf`. This can be a piece; of a VCF, e.g. grepping a hap.py output VCF for false positives. This is a; powerful way to pick out variants of interest and investigate them in more; depth.; * Stop after a certain number of examples, e.g. 10? Use `--num_records 10`.; * Sharded exam
",False,"The text contains complete, meaningful sentences discussing how to use the DeepVariant tool, including commands and settings, which are not purely human-readable or explanatory prose but rather step-by-step instructions. However, since these instructions are in natural language and provide value beyond code snippets, they should not be eliminated."
Availability,"thin the bedtools container); min_coverage=3; gzip -dc data/hg005_coverage.per-base.bed.gz | \; egrep -v 'HLA|decoy|random|alt|chrUn|chrEBV' | \; awk -v OFS=""\t"" -v min_coverage=${min_coverage} '$4 >= min_coverage { print }' | \; bedtools merge -d 1 -c 4 -o mean -i - > data/hg005_3x.bed; ```. ### Intersect coverage with CDS regions. Now we will intersect our 3x bedfile with the CDS bed file:. ```bash; # (Run within the bedtools container); bedtools intersect \; -a data/hg005_3x.bed \; -b data/chr20_CDS.bed > data/chr20_CDS_3x.bed. # We will also intersect this file with confident GIAB regions; bedtools intersect \; -a benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed \; -b data/chr20_CDS_3x.bed > benchmark/chr20_CDS_3x.benchmark_regions.bed; ```. We now have a bed file of CDS regions intersected with 3x coverage regions; called `data/chr20_CDS_3x.bed`. You can exit the docker container now. Type; `exit` and hit enter. ### Download the RNA-seq model. Finally, lets download the RNA-seq model that we will use to call variants. ```bash; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta; ```. ### Directory Structure. After you have run the steps above, your directory structure should look like; this:. ```; .; ├── benchmark; │   ├── chr20_CDS_3x.benchmark_regions.bed; │   ├── HG005_GRCh38_1_22_v4.2.1_benchmark.bed;",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:5326,down,download,5326,docs/deepvariant-rnaseq-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md,1,['down'],['download'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: thin the bedtools container); min_coverage=3; gzip -dc data/hg005_coverage.per-base.bed.gz | \; egrep -v 'HLA|decoy|random|alt|chrUn|chrEBV' | \; awk -v OFS=""\t"" -v min_coverage=${min_coverage} '$4 >= min_coverage { print }' | \; bedtools merge -d 1 -c 4 -o mean -i - > data/hg005_3x.bed; ```. ### Intersect coverage with CDS regions. Now we will intersect our 3x bedfile with the CDS bed file:. ```bash; # (Run within the bedtools container); bedtools intersect \; -a data/hg005_3x.bed \; -b data/chr20_CDS.bed > data/chr20_CDS_3x.bed. # We will also intersect this file with confident GIAB regions; bedtools intersect \; -a benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed \; -b data/chr20_CDS_3x.bed > benchmark/chr20_CDS_3x.benchmark_regions.bed; ```. We now have a bed file of CDS regions intersected with 3x coverage regions; called `data/chr20_CDS_3x.bed`. You can exit the docker container now. Type; `exit` and hit enter. ### Download the RNA-seq model. Finally, lets download the RNA-seq model that we will use to call variants. ```bash; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta; ```. ### Directory Structure. After you have run the steps above, your directory structure should look like; this:. ```; .; ├── benchmark; │   ├── chr20_CDS_3x.benchmark_regions.bed; │   ├── HG005_GRCh38_1_22_v4.2.1_benchmark.bed;
",False,
Availability,"tion](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:2655,avail,availability,2655,docs/deepvariant-gcp-info.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md,1,['avail'],['availability'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: tion](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate
",False,
Availability,"u can read more about samtools and bcftools here: http://www.htslib.org/doc/. ## Commands for requesting machines used in case studies. We report runtime in our case studies documentation. In order to make sure the; results we report are reproducible without too much variation, we provide the; commands we used here to show you what kind of machines we ran the case studies; on. This is NOT the fastest or cheapest configuration. ### Command for a CPU-only machine on Google Cloud Platform. We used a 64-core (vCPU) machine with 240GiB of memory and no GPU, on the Google; Cloud Platform. Specifying the CPU platform also allows us to report the runtime; more consistently. ```shell; gcloud compute instances create ""${USER}-cpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-64"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. ### Command for a GPU machine on Google Cloud Platform. ```shell; gcloud compute instances create ""${USER}-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. NOTE: Having an instance up and running could cost you. Remember to delete the; instances you're not using. You can find the instances at:; https://console.cloud.google.com/compute/instances?project=YOUR_PROJECT. [exome case study]: deepvariant-exome-case-study.md; [whole genome case study]: deepvariant-case-study.md; [quick start]: deepvariant-quick-start.md; [Running DeepVariant on Google Cloud Platform]: https://cloud.google.com/life-sciences/docs/tutorials/deepvariant; [TensorFlow]: http://www.tensorflow.org/; [Colab example]: visualizing_examples.ipynb; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:12099,mainten,maintenance-policy,12099,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['mainten'],['maintenance-policy'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: u can read more about samtools and bcftools here: http://www.htslib.org/doc/. ## Commands for requesting machines used in case studies. We report runtime in our case studies documentation. In order to make sure the; results we report are reproducible without too much variation, we provide the; commands we used here to show you what kind of machines we ran the case studies; on. This is NOT the fastest or cheapest configuration. ### Command for a CPU-only machine on Google Cloud Platform. We used a 64-core (vCPU) machine with 240GiB of memory and no GPU, on the Google; Cloud Platform. Specifying the CPU platform also allows us to report the runtime; more consistently. ```shell; gcloud compute instances create ""${USER}-cpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-64"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. ### Command for a GPU machine on Google Cloud Platform. ```shell; gcloud compute instances create ""${USER}-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. NOTE: Having an instance up and running could cost you. Remember to delete the; instances you're not using. You can find the instances at:; https://console.cloud.google.com/compute/instances?project=YOUR_PROJECT. [exome case study]: deepvariant-exome-case-study.md; [whole genome case study]: deepvariant-case-study.md; [quick start]: deepvariant-quick-start.md; [Running DeepVariant on Google Cloud Platform]: https://cloud.google.com/life-sciences/docs/tutorials/deepvariant; [TensorFlow]: http://www.tensorflow.org/; [Colab example]: visualizing_examples.ipynb; 
",False,
Availability,"une_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14526,checkpoint,checkpoint,14526,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,2,['checkpoint'],['checkpoint'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: une_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary
",False,
Deployability," ------------- | ---------------- | --------------- |; | INDEL | 29 | 0 | 0 | 1.0 | 1.0 | 1.0 |; | SNP | 683 | 2 | 0 | 0.99708 | 1.0 | 0.998538 |. #### HG004:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 32 | 1 | 1 | 0.969697 | 0.969697 | 0.969697 |; | SNP | 677 | 2 | 0 | 0.997054 | 1.0 | 0.998525 |. * See VCF stats report (for all chromosomes); - [HG002](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG002.output.visual_report.html); - [HG003](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG003.output.visual_report.html); - [HG004](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG004.output.visual_report.html). ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); For bigger datasets (WGS and PACBIO), we used bigger disk size (900G).; This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deeptrio.sh. # WGS; bash inference_deeptrio.sh --model_preset WGS. # WES; bash inference_deeptrio.sh --model_preset WES. # PacBio; bash inference_deeptrio.sh --model_preset PACBIO. ```. Runtime metrics are taken from the resulting log after each stage of; DeepTrio. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 runs produced the same output. [CPU instance with 64 CPUs]: deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics-deeptrio.md:6121,configurat,configuration,6121,docs/metrics-deeptrio.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics-deeptrio.md,1,['configurat'],['configuration'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  ------------- | ---------------- | --------------- |; | INDEL | 29 | 0 | 0 | 1.0 | 1.0 | 1.0 |; | SNP | 683 | 2 | 0 | 0.99708 | 1.0 | 0.998538 |. #### HG004:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 32 | 1 | 1 | 0.969697 | 0.969697 | 0.969697 |; | SNP | 677 | 2 | 0 | 0.997054 | 1.0 | 0.998525 |. * See VCF stats report (for all chromosomes); - [HG002](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG002.output.visual_report.html); - [HG003](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG003.output.visual_report.html); - [HG004](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG004.output.visual_report.html). ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); For bigger datasets (WGS and PACBIO), we used bigger disk size (900G).; This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deeptrio.sh. # WGS; bash inference_deeptrio.sh --model_preset WGS. # WES; bash inference_deeptrio.sh --model_preset WES. # PacBio; bash inference_deeptrio.sh --model_preset PACBIO. ```. Runtime metrics are taken from the resulting log after each stage of; DeepTrio. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 runs produced the same output. [CPU instance with 64 CPUs]: deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform; 
",True,"The text contains code snippets and API documentation, such as curl commands and script executions, which are indicative of programmatic content."
Deployability," create directories to organize files. ```bash; mkdir -p data benchmark reference model output happy; ```. ### Download the GRCh38 Reference. We will be using GRCh38 for this case study. ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. # Extract chr20 CDS regions and convert to bed file.; gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \; awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \; awk '!dup[$0]++' > data/chr20_CDS.bed; ```. ### Download HG005 BAM. We",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:1818,release,release,1818,docs/deepvariant-rnaseq-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  create directories to organize files. ```bash; mkdir -p data benchmark reference model output happy; ```. ### Download the GRCh38 Reference. We will be using GRCh38 for this case study. ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. # Extract chr20 CDS regions and convert to bed file.; gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \; awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \; awk '!dup[$0]++' > data/chr20_CDS.bed; ```. ### Download HG005 BAM. We
",False,"The text contains detailed instructions for downloading and processing reference files, including shell commands, which are meaningful and part of the process described. It is not merely code or logs but a step-by-step guide for data preparation."
Deployability," that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark.vc",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:2130,release,release,2130,docs/deeptrio-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark.vc
",False,
Deployability,"# Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data. DeepVariant is an analysis pipeline that uses a deep neural network to call; genetic variants from next-generation DNA sequencing (NGS) data. While; DeepVariant is highly accurate for; [many types of NGS data](https://rdcu.be/7Dhl), some users may be interested in; training custom deep learning models that have been optimized for very specific; data. This case study describes one way to train such a custom model using a GPU, in; this case for BGISEQ-500 data. Please note that there is not yet a production-grade training pipeline. This is; just one example of how to train a custom model, and is neither the fastest nor; the cheapest possible configuration. The resulting model also does not represent; the greatest achievable accuracy for BGISEQ-500 data. ## High level summary of result. We demonstrated that by training on 1 replicate of BGISEQ-500 whole genome data; (everything except for chromosome 20-22), we can significantly improve the; accuracy comparing to the WGS model as a baseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; followi",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:125,pipeline,pipeline,125,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,3,"['configurat', 'pipeline']","['configuration', 'pipeline']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data. DeepVariant is an analysis pipeline that uses a deep neural network to call; genetic variants from next-generation DNA sequencing (NGS) data. While; DeepVariant is highly accurate for; [many types of NGS data](https://rdcu.be/7Dhl), some users may be interested in; training custom deep learning models that have been optimized for very specific; data. This case study describes one way to train such a custom model using a GPU, in; this case for BGISEQ-500 data. Please note that there is not yet a production-grade training pipeline. This is; just one example of how to train a custom model, and is neither the fastest nor; the cheapest possible configuration. The resulting model also does not represent; the greatest achievable accuracy for BGISEQ-500 data. ## High level summary of result. We demonstrated that by training on 1 replicate of BGISEQ-500 whole genome data; (everything except for chromosome 20-22), we can significantly improve the; accuracy comparing to the WGS model as a baseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; followi
",False,"The text contains complete, meaningful sentences in natural language discussing concepts and ideas related to training a custom variant caller for BGISEQ-500 data. It includes explanations of the process, results, and instructions on how to request a GPU machine. The content is human-written prose and not code or logs."
Deployability,"# DeepTrio runtime and accuracy metrics for all release models. ## WGS (Illumina). ### Runtime. Runtime is on HG002/HG003/HG004 (all chromosomes). Stage | Wall time (minutes); -------------------------------- | -----------------; make_examples | ~439m; call_variants for HG002 | ~351m; call_variants for HG003 | ~355m; call_variants for HG004 | ~361m; postprocess_variants (parallel) | ~61m; total | ~1567m = ~26.12 hours. ### Accuracy. We report hap.py results on HG002/HG003/HG004 trio (chr20, using NIST v4.2.1; truth), which was held out while training. #### HG002:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 11208 | 48 | 13 | 0.995736 | 0.998884 | 0.997308 |; | SNP | 71087 | 246 | 42 | 0.996551 | 0.99941 | 0.997979 |. #### HG003:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 10584 | 44 | 20 | 0.99586 | 0.998192 | 0.997024 |; | SNP | 69975 | 191 | 55 | 0.997278 | 0.999215 | 0.998246 |. #### HG004:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 10945 | 55 | 27 | 0.995 | 0.997643 | 0.99632 |; | SNP | 71446 | 213 | 52 | 0.997028 | 0.999273 | 0.998149 |. * See VCF stats report (for all chromosomes); - [HG002](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WGS/HG002.output.visual_report.html); - [HG003](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WGS/HG003.output.visual_report.html); - [HG004](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WGS/HG004.output.visual_report.html). ## PacBio (HiFi). In v1.6.1, we introduced read haplo",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics-deeptrio.md:48,release,release,48,docs/metrics-deeptrio.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics-deeptrio.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # DeepTrio runtime and accuracy metrics for all release models. ## WGS (Illumina). ### Runtime. Runtime is on HG002/HG003/HG004 (all chromosomes). Stage | Wall time (minutes); -------------------------------- | -----------------; make_examples | ~439m; call_variants for HG002 | ~351m; call_variants for HG003 | ~355m; call_variants for HG004 | ~361m; postprocess_variants (parallel) | ~61m; total | ~1567m = ~26.12 hours. ### Accuracy. We report hap.py results on HG002/HG003/HG004 trio (chr20, using NIST v4.2.1; truth), which was held out while training. #### HG002:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 11208 | 48 | 13 | 0.995736 | 0.998884 | 0.997308 |; | SNP | 71087 | 246 | 42 | 0.996551 | 0.99941 | 0.997979 |. #### HG003:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 10584 | 44 | 20 | 0.99586 | 0.998192 | 0.997024 |; | SNP | 69975 | 191 | 55 | 0.997278 | 0.999215 | 0.998246 |. #### HG004:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 10945 | 55 | 27 | 0.995 | 0.997643 | 0.99632 |; | SNP | 71446 | 213 | 52 | 0.997028 | 0.999273 | 0.998149 |. * See VCF stats report (for all chromosomes); - [HG002](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WGS/HG002.output.visual_report.html); - [HG003](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WGS/HG003.output.visual_report.html); - [HG004](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WGS/HG004.output.visual_report.html). ## PacBio (HiFi). In v1.6.1, we introduced read haplo
",False,"The text contains detailed descriptions of runtime and accuracy metrics for specific models and datasets, including tables summarizing results. While this information is technical in nature, it represents a form of human-readable content that discusses the performance and outcomes of computational processes. The presence of explanations and numerical data arranged in a structured format indicates that the text serves to inform and describe rather than to provide code or logs."
Deployability,"# DeepVariant Complete Genomics G400 case study. In this case study, we describe applying DeepVariant to a Complete Genomics G400; sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics G400 HG002 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam > input/HG002.complete_g400.V350151728.grch38.chr20.bam. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam.bai > input/HG002.complete_g400.V350151728.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG002. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics G400 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.data-00000-of-00001 > input/weights-60-0.993753.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.index > input/weights-60-0.993753.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/referenc",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md:934,release,release,934,docs/deepvariant-complete-g400-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # DeepVariant Complete Genomics G400 case study. In this case study, we describe applying DeepVariant to a Complete Genomics G400; sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics G400 HG002 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam > input/HG002.complete_g400.V350151728.grch38.chr20.bam. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam.bai > input/HG002.complete_g400.V350151728.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG002. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics G400 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.data-00000-of-00001 > input/weights-60-0.993753.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.index > input/weights-60-0.993753.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/referenc
",False,"The text contains multiple bash script commands for downloading data and running an application, which are code snippets. Additionally, there are step-by-step instructions explaining how to prepare the environment and download necessary files, which may be considered programmatic API documentation or build system output."
Deployability,"# DeepVariant Complete Genomics T7 case study. In this case study, we describe applying DeepVariant to a Complete Genomics T7; sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics T7 HG001 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG001. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics T7 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.data-00000-of-00001 > input/weights-51-0.995354.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.index > input/weights-51-0.995354.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md:940,release,release,940,docs/deepvariant-complete-t7-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # DeepVariant Complete Genomics T7 case study. In this case study, we describe applying DeepVariant to a Complete Genomics T7; sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics T7 HG001 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG001. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics T7 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.data-00000-of-00001 > input/weights-51-0.995354.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.index > input/weights-51-0.995354.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt
",False,"The text contains complete, meaningful sentences in natural language discussing the application of DeepVariant to a genomic case study. It includes explanatory content about preparing environments and downloading necessary data and models. There are no code snippets or logs present."
Deployability,"# DeepVariant Genomic VCF (gVCF) support. Beginning with the 0.5.0 release, DeepVariant supports the creation of Genomic; VCF (gVCF) output. This has the same underlying format specification as the; [VCF format] but also includes additional records that distinguish regions that; have sequence coverage that appears to match the reference genome from regions; without sequence coverage, in which the genotype is unknown. gVCF files are required as input for analyses that create a set of variants in; a cohort of individuals, such as cohort merging or joint genotyping. ## Description of gVCF format. When run with gVCF output enabled, DeepVariant generates both the VCF output; containing only variant calls as well as an additional gVCF output file that; contains both variants and non-variant sites. The gVCF file includes both; variant calls and regions that are confidently called as matching the reference; genome. The non-variant sites compare the reference allele to an ""unspecified; alternate"" allele, represented by `<*>`. To minimize output file size, adjacent; records with equal (or similar, see discussion below) genotype qualities are; merged into a single record. Section 5.5 of the [VCF format] specification gives a description of the gVCF; format and example output, partially reproduced below. The gVCF output of; DeepVariant is syntactically and semantically equivalent to this example. ```bash; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT Sample; 1 4370 . G <*> . . END=4383 GT:GQ 0/0:37; 1 4384 . C <*> . . END=4388 GT:GQ 0/0:41; 1 4389 . T TC,<*> 50 . . GT:GQ 0/1:50; 1 4390 . C <*> . . END=4390 GT:GQ 0/0:3; ```. ## Creating gVCF output with DeepVariant. The exact same three programs (`make_examples`, `call_variants`, and; `postprocess_variants`) are used when creating gVCF output as in the [WGS case; study]. However, additional flags must be passed to the `make_examples` and; `postprocess_variants` steps. ### `make_examples`. The `make_examples` program is where the ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:67,release,release,67,docs/deepvariant-gvcf-support.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # DeepVariant Genomic VCF (gVCF) support. Beginning with the 0.5.0 release, DeepVariant supports the creation of Genomic; VCF (gVCF) output. This has the same underlying format specification as the; [VCF format] but also includes additional records that distinguish regions that; have sequence coverage that appears to match the reference genome from regions; without sequence coverage, in which the genotype is unknown. gVCF files are required as input for analyses that create a set of variants in; a cohort of individuals, such as cohort merging or joint genotyping. ## Description of gVCF format. When run with gVCF output enabled, DeepVariant generates both the VCF output; containing only variant calls as well as an additional gVCF output file that; contains both variants and non-variant sites. The gVCF file includes both; variant calls and regions that are confidently called as matching the reference; genome. The non-variant sites compare the reference allele to an ""unspecified; alternate"" allele, represented by `<*>`. To minimize output file size, adjacent; records with equal (or similar, see discussion below) genotype qualities are; merged into a single record. Section 5.5 of the [VCF format] specification gives a description of the gVCF; format and example output, partially reproduced below. The gVCF output of; DeepVariant is syntactically and semantically equivalent to this example. ```bash; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT Sample; 1 4370 . G <*> . . END=4383 GT:GQ 0/0:37; 1 4384 . C <*> . . END=4388 GT:GQ 0/0:41; 1 4389 . T TC,<*> 50 . . GT:GQ 0/1:50; 1 4390 . C <*> . . END=4390 GT:GQ 0/0:3; ```. ## Creating gVCF output with DeepVariant. The exact same three programs (`make_examples`, `call_variants`, and; `postprocess_variants`) are used when creating gVCF output as in the [WGS case; study]. However, additional flags must be passed to the `make_examples` and; `postprocess_variants` steps. ### `make_examples`. The `make_examples` program is where the 
",False,"The content describes the functionality of DeepVariant's gVCF output, including explanations of the format, usage instructions, and examples. It contains complete sentences discussing how to create gVCF files with DeepVariant, which are meaningful for human readers and not logs or code samples."
Deployability,"# DeepVariant quick start. This is an explanation of how to use DeepVariant. ## Background. To get started, you'll need the DeepVariant programs (and some packages they; depend on), some test data, and of course a place to run them. We've provided a Docker image, and some test data in a bucket on Google Cloud; Storage. The instructions below show how to download the data through the; corresponding public URLs from these data. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepVariant in one command. Starting from the 0.8 release, we introduced one convenient command that will; run through all 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:825,release,release,825,docs/deepvariant-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # DeepVariant quick start. This is an explanation of how to use DeepVariant. ## Background. To get started, you'll need the DeepVariant programs (and some packages they; depend on), some test data, and of course a place to run them. We've provided a Docker image, and some test data in a bucket on Google Cloud; Storage. The instructions below show how to download the data through the; corresponding public URLs from these data. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepVariant in one command. Starting from the 0.8 release, we introduced one convenient command that will; run through all 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; 
",False,"The content includes explanatory and instructional text discussing how to set up and run DeepVariant, including steps with bash commands. However, there are also code snippets (e.g., `BIN_VERSION` assignment, `sudo apt -y update`, `docker pull` command) that may indicate a mix of human-readable text with some programmatic content. While the majority of the text is explanatory, the presence of specific commands and code could be considered as partial automation instructions or configuration steps, which might not strictly meet the criteria for elimination. However, this depends on whether such commands are integrated into a larger narrative or if they are standalone code blocks intended to be executed."
Deployability,# Documentation. * [DeepVariant release notes](https://github.com/google/deepvariant/releases). ## Quick start and Case studies. * [DeepVariant quick start](deepvariant-quick-start.md); * [DeepVariant whole genome case study](deepvariant-case-study.md); * [DeepVariant exome case study](deepvariant-exome-case-study.md); * [DeepVariant PacBio case study](deepvariant-pacbio-model-case-study.md); * [DeepVariant ONT R10.4 simplex case study](deepvariant-ont-r104-simplex-case-study.md); [DeepVariant ONT R10.4 duplex case study](deepvariant-ont-r104-duplex-case-study.md); * [DeepVariant hybrid (PacBio and Illumina) case study](deepvariant-hybrid-case-study.md); * [DeepVariant Complete Genomics T7 case study](deepvariant-complete-t7-case-study.md); * [DeepVariant Complete Genomics G400 case study](deepvariant-complete-g400-case-study.md); * [Runtime and accuracy metrics for all DeepVariant models](metrics.md); * [Best practices for multi-sample variant calling](trio-merge-case-study.md); * [Using graph genomes: VG Giraffe + DeepVariant case study](deepvariant-vg-case-study.md). ## Visualization and analysis. * [show_examples: Saving human-readable images from DeepVariant examples](show-examples.md); * [VCF stats report](deepvariant-vcf-stats-report.md); * [Runtime by region for make_examples](runtime-by-region.md). ### Colab notebooks. * [Colab example: visualizing pileup images/tensors](visualizing_examples.ipynb); * [Can you beat DeepVariant?: A look inside the classification task](cybdv_notebook.ipynb); * [Google Developer Codelab: Variant Calling on a Rice genome with DeepVariant](https://codelabs.developers.google.com/codelabs/genomics-deepvariant). ## (Advanced) Training. * [Advanced Case Study: Train a customized SNP and small indel variant caller; for BGISEQ-500 data](deepvariant-training-case-study.md); * [DeepVariant training data](deepvariant-details-training-data.md). ## More details. * [DeepVariant usage guide](deepvariant-details.md); * [Building and testing De,MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/README.md:32,release,release,32,docs/README.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/README.md,2,['release'],"['release', 'releases']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # Documentation. * [DeepVariant release notes](https://github.com/google/deepvariant/releases). ## Quick start and Case studies. * [DeepVariant quick start](deepvariant-quick-start.md); * [DeepVariant whole genome case study](deepvariant-case-study.md); * [DeepVariant exome case study](deepvariant-exome-case-study.md); * [DeepVariant PacBio case study](deepvariant-pacbio-model-case-study.md); * [DeepVariant ONT R10.4 simplex case study](deepvariant-ont-r104-simplex-case-study.md); [DeepVariant ONT R10.4 duplex case study](deepvariant-ont-r104-duplex-case-study.md); * [DeepVariant hybrid (PacBio and Illumina) case study](deepvariant-hybrid-case-study.md); * [DeepVariant Complete Genomics T7 case study](deepvariant-complete-t7-case-study.md); * [DeepVariant Complete Genomics G400 case study](deepvariant-complete-g400-case-study.md); * [Runtime and accuracy metrics for all DeepVariant models](metrics.md); * [Best practices for multi-sample variant calling](trio-merge-case-study.md); * [Using graph genomes: VG Giraffe + DeepVariant case study](deepvariant-vg-case-study.md). ## Visualization and analysis. * [show_examples: Saving human-readable images from DeepVariant examples](show-examples.md); * [VCF stats report](deepvariant-vcf-stats-report.md); * [Runtime by region for make_examples](runtime-by-region.md). ### Colab notebooks. * [Colab example: visualizing pileup images/tensors](visualizing_examples.ipynb); * [Can you beat DeepVariant?: A look inside the classification task](cybdv_notebook.ipynb); * [Google Developer Codelab: Variant Calling on a Rice genome with DeepVariant](https://codelabs.developers.google.com/codelabs/genomics-deepvariant). ## (Advanced) Training. * [Advanced Case Study: Train a customized SNP and small indel variant caller; for BGISEQ-500 data](deepvariant-training-case-study.md); * [DeepVariant training data](deepvariant-details-training-data.md). ## More details. * [DeepVariant usage guide](deepvariant-details.md); * [Building and testing De
",False,"The text contains documentation links, quick start guides, case studies, visualization tools, and Colab examples. It is descriptive and provides human-readable information without code snippets or logs."
Deployability,"# How to Contribute. We cannot merge external pull requests into the DeepVariant repository at this; time. The source of truth for DeepVariant lives in an internal Google codebase,; and changes must first be made internally. However, we still welcome community contributions! Please feel free to fork the; DeepVariant repository and open a pull request or issue with suggested edits. To; incorporate your contributions, we will make the changes internally and then; push them to GitHub in the subsequent release. We will attribute the changes to; you in the commit description and release notes.; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/CONTRIBUTING.md:504,release,release,504,CONTRIBUTING.md,,https://github.com/google/deepvariant/tree/v1.6.1/CONTRIBUTING.md,2,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # How to Contribute. We cannot merge external pull requests into the DeepVariant repository at this; time. The source of truth for DeepVariant lives in an internal Google codebase,; and changes must first be made internally. However, we still welcome community contributions! Please feel free to fork the; DeepVariant repository and open a pull request or issue with suggested edits. To; incorporate your contributions, we will make the changes internally and then; push them to GitHub in the subsequent release. We will attribute the changes to; you in the commit description and release notes.; 
",False,"The text contains complete, meaningful sentences discussing how to contribute to the project, including information on pull requests, internal processes, and community contributions."
Deployability,"# Runtime and accuracy metrics for all release models. ## WGS (Illumina). ### Runtime. Runtime is on HG003 (all chromosomes). Stage | Time (minutes); -------------------------------- | ------------------; make_examples | ~103m; call_variants | ~196m; postprocess_variants (with gVCF) | ~27m; total | ~326m = ~5.43 hours. ### Accuracy. hap.py results on HG003 (all chromosomes, using NIST v4.2.1 truth), which was; held out while training. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 501683 | 2818 | 1265 | 0.994414 | 0.997586 | 0.995998 |; | SNP | 3306788 | 20708 | 4274 | 0.993777 | 0.99871 | 0.996237 |. [See VCF stats report.](https://storage.googleapis.com/deepvariant/visual_reports/DeepVariant/1.6.1/WGS/deepvariant.output.visual_report.html). ## WES (Illumina). ### Runtime. Runtime is on HG003 (all chromosomes). Stage | Time (minutes); -------------------------------- | -----------------; make_examples | ~6m; call_variants | ~1m; postprocess_variants (with gVCF) | ~1m; total | ~8m. ### Accuracy. hap.py results on HG003 (all chromosomes, using NIST v4.2.1 truth), which was; held out while training. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 1022 | 29 | 13 | 0.972407 | 0.987713 | 0.98 |; | SNP | 24987 | 292 | 59 | 0.988449 | 0.997645 | 0.993025 |. [See VCF stats report.](https://storage.googleapis.com/deepvariant/visual_reports/DeepVariant/1.6.1/WES/deepvariant.output.visual_report.html). ## PacBio (HiFi). ### Runtime. Runtime is on HG003 (all chromosomes). Stage | Time (minutes); -------------------------------- | -------------------; make_examples | ~149m; call_variants | ~217m; postprocess_variants (with gVCF) | ~33m; total | ~399m = ~6.65 hours. ### Accur",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md:39,release,release,39,docs/metrics.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # Runtime and accuracy metrics for all release models. ## WGS (Illumina). ### Runtime. Runtime is on HG003 (all chromosomes). Stage | Time (minutes); -------------------------------- | ------------------; make_examples | ~103m; call_variants | ~196m; postprocess_variants (with gVCF) | ~27m; total | ~326m = ~5.43 hours. ### Accuracy. hap.py results on HG003 (all chromosomes, using NIST v4.2.1 truth), which was; held out while training. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 501683 | 2818 | 1265 | 0.994414 | 0.997586 | 0.995998 |; | SNP | 3306788 | 20708 | 4274 | 0.993777 | 0.99871 | 0.996237 |. [See VCF stats report.](https://storage.googleapis.com/deepvariant/visual_reports/DeepVariant/1.6.1/WGS/deepvariant.output.visual_report.html). ## WES (Illumina). ### Runtime. Runtime is on HG003 (all chromosomes). Stage | Time (minutes); -------------------------------- | -----------------; make_examples | ~6m; call_variants | ~1m; postprocess_variants (with gVCF) | ~1m; total | ~8m. ### Accuracy. hap.py results on HG003 (all chromosomes, using NIST v4.2.1 truth), which was; held out while training. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 1022 | 29 | 13 | 0.972407 | 0.987713 | 0.98 |; | SNP | 24987 | 292 | 59 | 0.988449 | 0.997645 | 0.993025 |. [See VCF stats report.](https://storage.googleapis.com/deepvariant/visual_reports/DeepVariant/1.6.1/WES/deepvariant.output.visual_report.html). ## PacBio (HiFi). ### Runtime. Runtime is on HG003 (all chromosomes). Stage | Time (minutes); -------------------------------- | -------------------; make_examples | ~149m; call_variants | ~217m; postprocess_variants (with gVCF) | ~33m; total | ~399m = ~6.65 hours. ### Accur
",False,"The text contains detailed runtime and accuracy metrics for various testing stages of a model deployment, which is technical in nature but still provides meaningful data for analysis."
Deployability,"# Using DeepVariant for small variant calling from PacBio HiFi reads. #### Author: William Rowell <wrowell@pacificbiosciences.com>. In this case study we describe applying DeepVariant to PacBio HiFi reads to call; variants. We will call small variants from a publicly available whole genome; HiFi dataset from PacBio. Starting in v1.4.0, PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchma",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:816,install,installed,816,docs/deepvariant-pacbio-model-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md,2,"['configurat', 'install']","['configuration', 'installed']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # Using DeepVariant for small variant calling from PacBio HiFi reads. #### Author: William Rowell <wrowell@pacificbiosciences.com>. In this case study we describe applying DeepVariant to PacBio HiFi reads to call; variants. We will call small variants from a publicly available whole genome; HiFi dataset from PacBio. Starting in v1.4.0, PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchma
",False,"The content includes detailed step-by-step instructions for setting up and running a variant calling pipeline using DeepVariant, including code snippets for environment preparation and data downloading. It is a mix of human-readable text explaining the process and some code blocks, but overall, it serves to describe a procedure rather than being solely code or logs."
Deployability,"# Using graph genomes: VG Giraffe + DeepVariant case study; ---. This is an example to run `vg giraffe`, so we can go from FASTQs --> BAM. For simplicity and consistency, we run the following with a; [Google Cloud instance with 64 cores](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). I added more disks because 300G is not enough for the example below. I changed; it to `--boot-disk-size ""1000""`. ## Install softwares that will be used later. ```bash; sudo apt update -y; sudo apt-get -y install aria2 docker.io samtools; ```. ## Download input FASTQ files. ```bash; DATA_DIR=${PWD}/data; mkdir -p ${DATA_DIR}; gcloud storage cp gs://brain-genomics-public/research/sequencing/fastq/novaseq/wgs_pcr_free/35x/HG003.novaseq.pcr-free.35x.R?.fastq.gz ${DATA_DIR}/; ```. ## Download VG files. Get binaries `vg` 1.51.0 and `kmc`:. ```bash; wget https://github.com/refresh-bio/KMC/releases/download/v3.2.2/KMC3.2.2.linux.x64.tar.gz; tar zxf KMC3.2.2.linux.x64.tar.gz bin/kmc; mv bin/kmc ${DATA_DIR}/; wget https://github.com/vgteam/vg/releases/download/v1.51.0/vg -O ${DATA_DIR}/vg; chmod +x ${DATA_DIR}/vg ${DATA_DIR}/kmc; ```. Get the graph (.gbz) and haplotype index (.hapl).; I used `aria2c` to download these files. You can use other approaches as well. ```bash; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.gbz; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.hapl; ```. ## Run `vg giraffe` with one command to get from FASTQs to BAM. Put the paths name into a file named HG003.fq.paths:. ```bash; cat > HG003.fq.paths <<- EOM; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R1.fastq.gz; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R2.fastq.gz; EOM; ```. Run `kmc`` on this file. I used -t$(nproc) to use all cores, and $TMPDIR ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:496,update,update,496,docs/deepvariant-vg-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md,3,"['install', 'release', 'update']","['install', 'releases', 'update']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # Using graph genomes: VG Giraffe + DeepVariant case study; ---. This is an example to run `vg giraffe`, so we can go from FASTQs --> BAM. For simplicity and consistency, we run the following with a; [Google Cloud instance with 64 cores](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). I added more disks because 300G is not enough for the example below. I changed; it to `--boot-disk-size ""1000""`. ## Install softwares that will be used later. ```bash; sudo apt update -y; sudo apt-get -y install aria2 docker.io samtools; ```. ## Download input FASTQ files. ```bash; DATA_DIR=${PWD}/data; mkdir -p ${DATA_DIR}; gcloud storage cp gs://brain-genomics-public/research/sequencing/fastq/novaseq/wgs_pcr_free/35x/HG003.novaseq.pcr-free.35x.R?.fastq.gz ${DATA_DIR}/; ```. ## Download VG files. Get binaries `vg` 1.51.0 and `kmc`:. ```bash; wget https://github.com/refresh-bio/KMC/releases/download/v3.2.2/KMC3.2.2.linux.x64.tar.gz; tar zxf KMC3.2.2.linux.x64.tar.gz bin/kmc; mv bin/kmc ${DATA_DIR}/; wget https://github.com/vgteam/vg/releases/download/v1.51.0/vg -O ${DATA_DIR}/vg; chmod +x ${DATA_DIR}/vg ${DATA_DIR}/kmc; ```. Get the graph (.gbz) and haplotype index (.hapl).; I used `aria2c` to download these files. You can use other approaches as well. ```bash; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.gbz; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.hapl; ```. ## Run `vg giraffe` with one command to get from FASTQs to BAM. Put the paths name into a file named HG003.fq.paths:. ```bash; cat > HG003.fq.paths <<- EOM; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R1.fastq.gz; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R2.fastq.gz; EOM; ```. Run `kmc`` on this file. I used -t$(nproc) to use all cores, and $TMPDIR 
",True,"The text contains code snippets and commands, which are primarily logs or programmatic instructions. It includes bash scripts for installing software, downloading files, and running specific tools like `vg giraffe` and `kmc`. The content is more suited for a user guide or script documentation rather than meaningful human-readable sentences."
Deployability,"## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md:330,install,install,330,docs/deepvariant-build-test.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md,4,['install'],"['install', 'installed']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; 
",False,"The text includes detailed step-by-step instructions for building and running DeepVariant on Ubuntu, including commands and descriptions of necessary software installations and configurations. It also mentions the use of specific shell commands and dependencies like TensorFlow and CLIF. However, since these are part of the setup process and include both configuration steps and executable commands, they could be considered as programmatic API or build documentation rather than purely human-readable prose. But overall, it's not exclusively code or logs."
Deployability,## DeepTrio is under development. Documentation will be released in the next release. We don't provide any support; to DeepTrio codebase right now.; ,MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/deeptrio/README.md:56,release,released,56,deeptrio/README.md,,https://github.com/google/deepvariant/tree/v1.6.1/deeptrio/README.md,2,['release'],"['release', 'released']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ## DeepTrio is under development. Documentation will be released in the next release. We don't provide any support; to DeepTrio codebase right now.; 
",False,
Deployability,",893,180<sup>[(4)](#vfootnote4)</sup> |; | 1.3.0 | 2 HG005/HG006/HG007 trio <br>10 HG002/HG003/HG004 trios | 539,382,124<sup>[(5)](#vfootnote5)</sup> |; | 1.4.0 | (Same model as 1.3.0) | |; | 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |; | Parent model | | |; | 1.1.0 | 1 HG005/HG006/HG007 trio <br> 8 HG002/HG003/HG004 trios | 386,418,918 |; | 1.2.0 | 1 HG005/HG006/HG007 trio <br>8 HG002/HG003/HG004 trios | 392,749,204<sup>[(4)](#vfootnote4)</sup> |; | 1.3.0 | 2 HG005/HG006/HG007 trio <br>10 HG002/HG003/HG004 trios | 533,353,050<sup>[(5)](#vfootnote5)</sup> |; | 1.4.0 | (Same model as 1.3.0) | |; | 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ### ONT models<sup>[(2)](#vfootnote2)</sup><sup>[(3)](#vfootnote3)</sup>; | version | Replicates | #examples |; | ------------ | ---------------------------------- | ----------- |; | 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |; | Parent model | | |; | 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |; <a name=""vfootnote1"">(1)</a>: We include HG002/HG003/HG004 for training WGS; model, but only using examples from the region of NIST truth confident region; v4.2 subtracting v3.3.2. <a name=""vfootnote2"">(2)</a>: We use the entire HG002/HG003/HG004 trio for; PacBio model training. <a name=""vfootnote3"">(3)</a>: PacBio and ONT training data contains training; examples with haplotag sorted images. <a name=""vfootnote4"">(4)</a>: In v1.2.0, we updated the NIST truth versions we; used for training. <a name=""vfootnote5"">(5)</a>: In v1.3.0, we included PacBio Sequel II Chemistry; v2.2 data in the training dataset. And we updated to NIST truth version to; v4.2.1. <a name=""vfootnote6"">(6)</a>: Starting in v1.5.0, for clarity, we report the; number of unique BAM files used. Note that this doesn't mean all the trios were; paired together to produce training data.; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details-training-data.md:4462,update,updated,4462,docs/deeptrio-details-training-data.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details-training-data.md,2,['update'],['updated'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ,893,180<sup>[(4)](#vfootnote4)</sup> |; | 1.3.0 | 2 HG005/HG006/HG007 trio <br>10 HG002/HG003/HG004 trios | 539,382,124<sup>[(5)](#vfootnote5)</sup> |; | 1.4.0 | (Same model as 1.3.0) | |; | 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |; | Parent model | | |; | 1.1.0 | 1 HG005/HG006/HG007 trio <br> 8 HG002/HG003/HG004 trios | 386,418,918 |; | 1.2.0 | 1 HG005/HG006/HG007 trio <br>8 HG002/HG003/HG004 trios | 392,749,204<sup>[(4)](#vfootnote4)</sup> |; | 1.3.0 | 2 HG005/HG006/HG007 trio <br>10 HG002/HG003/HG004 trios | 533,353,050<sup>[(5)](#vfootnote5)</sup> |; | 1.4.0 | (Same model as 1.3.0) | |; | 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ### ONT models<sup>[(2)](#vfootnote2)</sup><sup>[(3)](#vfootnote3)</sup>; | version | Replicates | #examples |; | ------------ | ---------------------------------- | ----------- |; | 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |; | Parent model | | |; | 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |; <a name=""vfootnote1"">(1)</a>: We include HG002/HG003/HG004 for training WGS; model, but only using examples from the region of NIST truth confident region; v4.2 subtracting v3.3.2. <a name=""vfootnote2"">(2)</a>: We use the entire HG002/HG003/HG004 trio for; PacBio model training. <a name=""vfootnote3"">(3)</a>: PacBio and ONT training data contains training; examples with haplotag sorted images. <a name=""vfootnote4"">(4)</a>: In v1.2.0, we updated the NIST truth versions we; used for training. <a name=""vfootnote5"">(5)</a>: In v1.3.0, we included PacBio Sequel II Chemistry; v2.2 data in the training dataset. And we updated to NIST truth version to; v4.2.1. <a name=""vfootnote6"">(6)</a>: Starting in v1.5.0, for clarity, we report the; number of unique BAM files used. Note that this doesn't mean all the trios were; paired together to produce training data.; 
",True,"The text consists primarily of version numbers and technical details about model versions, which are more programmatic and not meaningful human-readable content."
Deployability,"-------- | ------------------------------; v1.6 | 3 HG001<br>1 HG004<br>1 HG005 | 534,302,654. ### HYBRID models. version | Replicates | #examples; ------- | -------------------------------------------------------- | -----------; v1.0 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 193,076,623; v1.1 | Same model as v1.0 |; v1.2 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 214,302,681; v1.3 | Same model as v1.2 |; v1.4 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,863,645; v1.5 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,863,664; v1.6 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,353,081. <a name=""vfootnote1"">(1)</a>: In v0.5, we experimented with adding whole exome; sequencing data into training data. In v0.6, we took it out because it didn't; improve the WGS accuracy. <a name=""vfootnote2"">(2)</a>: The training data are from the same replicates as; v0.5. The number of examples changed because of the update in; [haplotype_labeler](https://github.com/google/deepvariant/tree/r0.6/deepvariant/labeler/haplotype_labeler.py). <a name=""vfootnote3"">(3)</a>: In v0.8, we used the; [Platinum Genomes Truthset](https://github.com/Illumina/PlatinumGenomes) to; create more training examples outside the GIAB confident regions. <a name=""vfootnote4"">(4)</a>: Previously, we split train/tune by leaving 3 WES; for tuning. Starting from this release, we leave out chr1 and chr20 from; training, and use chr1 for tuning. <a name=""vfootnote5"">(5)</a>: Starting from this version, we padded (100bps on; both sides) of the capture BED and used that for generating training examples.; We also added more `downsample_fraction`. <a name=""vfootnote6"">(6)</a>: (Before v1.0) PacBio is the only one we currently; uses HG002 in training and tuning. <a name=""vfootnote7"">(7)</a>: In v1.0, we train on HG002-HG004 for WGS as well,; but only using examples from the region of NIST truth confident region v4.2; subtracting v3.3.2",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details-training-data.md:4691,update,update,4691,docs/deepvariant-details-training-data.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details-training-data.md,1,['update'],['update'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: -------- | ------------------------------; v1.6 | 3 HG001<br>1 HG004<br>1 HG005 | 534,302,654. ### HYBRID models. version | Replicates | #examples; ------- | -------------------------------------------------------- | -----------; v1.0 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 193,076,623; v1.1 | Same model as v1.0 |; v1.2 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 214,302,681; v1.3 | Same model as v1.2 |; v1.4 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,863,645; v1.5 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,863,664; v1.6 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,353,081. <a name=""vfootnote1"">(1)</a>: In v0.5, we experimented with adding whole exome; sequencing data into training data. In v0.6, we took it out because it didn't; improve the WGS accuracy. <a name=""vfootnote2"">(2)</a>: The training data are from the same replicates as; v0.5. The number of examples changed because of the update in; [haplotype_labeler](https://github.com/google/deepvariant/tree/r0.6/deepvariant/labeler/haplotype_labeler.py). <a name=""vfootnote3"">(3)</a>: In v0.8, we used the; [Platinum Genomes Truthset](https://github.com/Illumina/PlatinumGenomes) to; create more training examples outside the GIAB confident regions. <a name=""vfootnote4"">(4)</a>: Previously, we split train/tune by leaving 3 WES; for tuning. Starting from this release, we leave out chr1 and chr20 from; training, and use chr1 for tuning. <a name=""vfootnote5"">(5)</a>: Starting from this version, we padded (100bps on; both sides) of the capture BED and used that for generating training examples.; We also added more `downsample_fraction`. <a name=""vfootnote6"">(6)</a>: (Before v1.0) PacBio is the only one we currently; uses HG002 in training and tuning. <a name=""vfootnote7"">(7)</a>: In v1.0, we train on HG002-HG004 for WGS as well,; but only using examples from the region of NIST truth confident region v4.2; subtracting v3.3.2
",True,"The content consists primarily of code snippets and version updates, including model versions (v1.0 to v1.6), data split changes, and parameter adjustments, which are more programmatic in nature rather than meaningful human-readable sentences."
Deployability,"------------------------------------------------------------------------: | -----------; [Docker](docs/deepvariant-quick-start.md) | This is the recommended method.; [Build from source](docs/deepvariant-build-test.md) | DeepVariant comes with scripts to build it on Ubuntu 20.04. To build and run on other Unix-based systems, you will need to modify these scripts.; Prebuilt Binaries | Available at [`gs://deepvariant/`](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you will need a CPU (such as Intel Sandy Bridge) that supports them. You can check the `/proc/cpuinfo` file on your computer, which lists these features under ""flags"". ## Contribution Guidelines. Please [open a pull request](https://github.com/google/deepvariant/compare) if; you wish to contribute to DeepVariant. Note, we have not set up the; infrastructure to merge pull requests externally. If you agree, we will test and; submit the changes internally and mention your contributions in our; [release notes](https://github.com/google/deepvariant/releases). We apologize; for any inconvenience. If you have any difficulty using DeepVariant, feel free to; [open an issue](https://github.com/google/deepvariant/issues/new). If you have; general questions not specific to DeepVariant, we recommend that you post on a; community discussion forum such as [BioStars](https://www.biostars.org/). ## License. [BSD-3-Clause license](LICENSE). ## Acknowledgements. DeepVariant happily makes use of many open source packages. We would like to; specifically call out a few key ones:. * [Boost Graph Library](http://www.boost.org/doc/libs/1_65_1/libs/graph/doc/index.html); * [abseil-cpp](https://github.com/abseil/abseil-cpp) and; [abseil-py](https://github.com/abseil/abseil-py); * [CLIF](https://github.com/google/clif); * [GNU Parallel](https://www.gnu.org/software/parallel/); * [htslib & samtools](http://www.htslib.org/); * [Nucleus](https://github.com/google/nucl",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:11281,release,release,11281,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ------------------------------------------------------------------------: | -----------; [Docker](docs/deepvariant-quick-start.md) | This is the recommended method.; [Build from source](docs/deepvariant-build-test.md) | DeepVariant comes with scripts to build it on Ubuntu 20.04. To build and run on other Unix-based systems, you will need to modify these scripts.; Prebuilt Binaries | Available at [`gs://deepvariant/`](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you will need a CPU (such as Intel Sandy Bridge) that supports them. You can check the `/proc/cpuinfo` file on your computer, which lists these features under ""flags"". ## Contribution Guidelines. Please [open a pull request](https://github.com/google/deepvariant/compare) if; you wish to contribute to DeepVariant. Note, we have not set up the; infrastructure to merge pull requests externally. If you agree, we will test and; submit the changes internally and mention your contributions in our; [release notes](https://github.com/google/deepvariant/releases). We apologize; for any inconvenience. If you have any difficulty using DeepVariant, feel free to; [open an issue](https://github.com/google/deepvariant/issues/new). If you have; general questions not specific to DeepVariant, we recommend that you post on a; community discussion forum such as [BioStars](https://www.biostars.org/). ## License. [BSD-3-Clause license](LICENSE). ## Acknowledgements. DeepVariant happily makes use of many open source packages. We would like to; specifically call out a few key ones:. * [Boost Graph Library](http://www.boost.org/doc/libs/1_65_1/libs/graph/doc/index.html); * [abseil-cpp](https://github.com/abseil/abseil-cpp) and; [abseil-py](https://github.com/abseil/abseil-py); * [CLIF](https://github.com/google/clif); * [GNU Parallel](https://www.gnu.org/software/parallel/); * [htslib & samtools](http://www.htslib.org/); * [Nucleus](https://github.com/google/nucl
",False,"The text includes installation instructions for DeepVariant, which are in markdown format. However, these instructions are meant to be human-readable and descriptive rather than code or logs. The content involves setting up the environment, building from source, available prebuilt binaries, contribution guidelines, license information, acknowledgments, and links to resources. These sections are written in a natural language format without any syntax that would indicate programming or code."
Deployability,"----; [Docker](docs/deepvariant-quick-start.md) | This is the recommended method.; [Build from source](docs/deepvariant-build-test.md) | DeepVariant comes with scripts to build it on Ubuntu 20.04. To build and run on other Unix-based systems, you will need to modify these scripts.; Prebuilt Binaries | Available at [`gs://deepvariant/`](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you will need a CPU (such as Intel Sandy Bridge) that supports them. You can check the `/proc/cpuinfo` file on your computer, which lists these features under ""flags"". ## Contribution Guidelines. Please [open a pull request](https://github.com/google/deepvariant/compare) if; you wish to contribute to DeepVariant. Note, we have not set up the; infrastructure to merge pull requests externally. If you agree, we will test and; submit the changes internally and mention your contributions in our; [release notes](https://github.com/google/deepvariant/releases). We apologize; for any inconvenience. If you have any difficulty using DeepVariant, feel free to; [open an issue](https://github.com/google/deepvariant/issues/new). If you have; general questions not specific to DeepVariant, we recommend that you post on a; community discussion forum such as [BioStars](https://www.biostars.org/). ## License. [BSD-3-Clause license](LICENSE). ## Acknowledgements. DeepVariant happily makes use of many open source packages. We would like to; specifically call out a few key ones:. * [Boost Graph Library](http://www.boost.org/doc/libs/1_65_1/libs/graph/doc/index.html); * [abseil-cpp](https://github.com/abseil/abseil-cpp) and; [abseil-py](https://github.com/abseil/abseil-py); * [CLIF](https://github.com/google/clif); * [GNU Parallel](https://www.gnu.org/software/parallel/); * [htslib & samtools](http://www.htslib.org/); * [Nucleus](https://github.com/google/nucleus); * [numpy](http://www.numpy.org/); * [SSW Library](https://github.com/mengyao/",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:11334,release,releases,11334,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['release'],['releases'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ----; [Docker](docs/deepvariant-quick-start.md) | This is the recommended method.; [Build from source](docs/deepvariant-build-test.md) | DeepVariant comes with scripts to build it on Ubuntu 20.04. To build and run on other Unix-based systems, you will need to modify these scripts.; Prebuilt Binaries | Available at [`gs://deepvariant/`](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you will need a CPU (such as Intel Sandy Bridge) that supports them. You can check the `/proc/cpuinfo` file on your computer, which lists these features under ""flags"". ## Contribution Guidelines. Please [open a pull request](https://github.com/google/deepvariant/compare) if; you wish to contribute to DeepVariant. Note, we have not set up the; infrastructure to merge pull requests externally. If you agree, we will test and; submit the changes internally and mention your contributions in our; [release notes](https://github.com/google/deepvariant/releases). We apologize; for any inconvenience. If you have any difficulty using DeepVariant, feel free to; [open an issue](https://github.com/google/deepvariant/issues/new). If you have; general questions not specific to DeepVariant, we recommend that you post on a; community discussion forum such as [BioStars](https://www.biostars.org/). ## License. [BSD-3-Clause license](LICENSE). ## Acknowledgements. DeepVariant happily makes use of many open source packages. We would like to; specifically call out a few key ones:. * [Boost Graph Library](http://www.boost.org/doc/libs/1_65_1/libs/graph/doc/index.html); * [abseil-cpp](https://github.com/abseil/abseil-cpp) and; [abseil-py](https://github.com/abseil/abseil-py); * [CLIF](https://github.com/google/clif); * [GNU Parallel](https://www.gnu.org/software/parallel/); * [htslib & samtools](http://www.htslib.org/); * [Nucleus](https://github.com/google/nucleus); * [numpy](http://www.numpy.org/); * [SSW Library](https://github.com/mengyao/
",False,
Deployability,".nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG004_truth.bed; ```. (No need to install bcftools and other tools, because they are now installed in; the DeepVariant images.). ## Run DeepVariant on trio to get 3 single sample VCFs. First, install docker if you don't have it yet: `sudo apt-get -y install; docker.io`. With the example command below, it runs DeepVariant on the trio one by one. This; is for demonstration only. If you're running this on a large cohort, running; serially is not the most effective approach. ```; N_SHARDS=$(nproc) # Or change to the number of cores you want to use; CAPTURE_BED=agilent_sureselect_human_all_exon_v5_b37_targets.bed; VERSION=1.6.1. declare -a trio=(HG002 HG003 HG004); for SAMPLE in ""${trio[@]}""; do; BAM=${SAMPLE}.bam. OUTPUT_VCF=${SAMPLE}.vcf.gz; OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time sudo docker run \; -v ""${DIR}"":""/data"" \; google/deepvariant:${VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/data/hs37d5.fa"" \; --reads=""/data/${BAM}"" \; --regions=""/data/${CAPTURE_BED}"" \; --output_vcf=""/data/${OUTPUT_VCF}"" \; --output_gvcf=""/data/${OUTPUT_GVCF}"" \; --num_shards=${N_SHARDS}; done; ```. Note: The BAM files should provide unique names for each sample in their `SM`; header tag, which is ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:6262,install,install,6262,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,2,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: .nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG004_truth.bed; ```. (No need to install bcftools and other tools, because they are now installed in; the DeepVariant images.). ## Run DeepVariant on trio to get 3 single sample VCFs. First, install docker if you don't have it yet: `sudo apt-get -y install; docker.io`. With the example command below, it runs DeepVariant on the trio one by one. This; is for demonstration only. If you're running this on a large cohort, running; serially is not the most effective approach. ```; N_SHARDS=$(nproc) # Or change to the number of cores you want to use; CAPTURE_BED=agilent_sureselect_human_all_exon_v5_b37_targets.bed; VERSION=1.6.1. declare -a trio=(HG002 HG003 HG004); for SAMPLE in ""${trio[@]}""; do; BAM=${SAMPLE}.bam. OUTPUT_VCF=${SAMPLE}.vcf.gz; OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time sudo docker run \; -v ""${DIR}"":""/data"" \; google/deepvariant:${VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/data/hs37d5.fa"" \; --reads=""/data/${BAM}"" \; --regions=""/data/${CAPTURE_BED}"" \; --output_vcf=""/data/${OUTPUT_VCF}"" \; --output_gvcf=""/data/${OUTPUT_GVCF}"" \; --num_shards=${N_SHARDS}; done; ```. Note: The BAM files should provide unique names for each sample in their `SM`; header tag, which is 
",False,
Deployability,".phased.chr20.bam.bai > input/HG002.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam > input/HG003.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG003.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam > input/HG004.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG004.pfda_challenge.grch38.phased.chr20.bam.bai; ```. ## Running DeepTrio with one command. DeepTrio pipeline consists of 4 steps: `make_examples`, `call_variants`,; `postprocess_variants` and `GLnexus merge`. It is possible to run the first; three steps with one command using the `run_deeptrio` script. GLnexus; is run as a separate command. ### Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}"". time sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type PACBIO \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads_child /input/HG002.pfda_challenge.grch38.phased.chr20.bam \; --reads_parent1 /input/HG003.pfda_challenge.grch38.phased.chr20.bam \; --reads_parent2 /input/HG004.pfda_challenge.grch38.phased.chr20.bam \; --output_vcf_child /output/HG002.output.vcf.gz \; --output_vcf_parent1 /output/HG003.output.vcf.gz \; --output_vcf_parent2 /output/HG004.output.vcf.gz \; --sample_name_child 'HG002' \; --sample_name_parent1 'HG003' \; --sample_name_parent2 'HG004' \; --num_shards $(nproc) \; --intermediate_results_dir /output/intermediate_results_dir \; --output_gvcf_child /output/HG002.g.vcf.gz \; --output_gvcf_par",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:4354,update,update,4354,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,2,"['install', 'update']","['install', 'update']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: .phased.chr20.bam.bai > input/HG002.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam > input/HG003.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG003.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam > input/HG004.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG004.pfda_challenge.grch38.phased.chr20.bam.bai; ```. ## Running DeepTrio with one command. DeepTrio pipeline consists of 4 steps: `make_examples`, `call_variants`,; `postprocess_variants` and `GLnexus merge`. It is possible to run the first; three steps with one command using the `run_deeptrio` script. GLnexus; is run as a separate command. ### Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}"". time sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type PACBIO \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads_child /input/HG002.pfda_challenge.grch38.phased.chr20.bam \; --reads_parent1 /input/HG003.pfda_challenge.grch38.phased.chr20.bam \; --reads_parent2 /input/HG004.pfda_challenge.grch38.phased.chr20.bam \; --output_vcf_child /output/HG002.output.vcf.gz \; --output_vcf_parent1 /output/HG003.output.vcf.gz \; --output_vcf_parent2 /output/HG004.output.vcf.gz \; --sample_name_child 'HG002' \; --sample_name_parent1 'HG003' \; --sample_name_parent2 'HG004' \; --num_shards $(nproc) \; --intermediate_results_dir /output/intermediate_results_dir \; --output_gvcf_child /output/HG002.g.vcf.gz \; --output_gvcf_par
",False,"The text contains a command that uses specific syntax like `$ curl`, `sudo docker pull`, etc., but it also includes explanatory sentences about the DeepTrio pipeline and how to run it on a CPU-only machine. The text combines both code-like commands with descriptive content, making it a mix of programmatic instructions and human-readable explanations. Therefore, while it has some code snippets, the overall context is helpful and not solely focused on logging or documentation that should be filtered out."
Deployability,"/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG004_truth.bed; ```. (No need to install bcftools and other tools, because they are now installed in; the DeepVariant images.). ## Run DeepVariant on trio to get 3 single sample VCFs. First, install docker if you don't have it yet: `sudo apt-get -y install; doc",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:5313,release,release,5313,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: /HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG004_truth.bed; ```. (No need to install bcftools and other tools, because they are now installed in; the DeepVariant images.). ## Run DeepVariant on trio to get 3 single sample VCFs. First, install docker if you don't have it yet: `sudo apt-get -y install; doc
",False,"The text contains a mixture of code commands and explanatory sentences. While there are command lines, the content also includes instructions for installing Docker and running DeepVariant, which provides meaningful human-readable information alongside the code."
Deployability,"001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bai -o HG004.bai; ```. ### Command for downloading the reference file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:4472,release,release,4472,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: 001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bai -o HG004.bai; ```. ### Command for downloading the reference file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d 
",False,
Deployability,"38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepVariant with one command. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can now run DeepVariant with one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir; ```. By specifying `--model_type WGS`, you'll be using a model that is best suited; for Illumina Whole Genome Sequencing data. NOTE: If you want to run each of the steps separately, add `--dry_run=tr",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:2194,pipeline,pipeline,2194,docs/deepvariant-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: 38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepVariant with one command. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can now run DeepVariant with one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir; ```. By specifying `--model_type WGS`, you'll be using a model that is best suited; for Illumina Whole Genome Sequencing data. NOTE: If you want to run each of the steps separately, add `--dry_run=tr
",False,
Deployability,"; Google Cloud Storage. The instructions below show how to download the data; through the corresponding public URLs. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepTrio in one command. Although DeepTrio can be built from a source, we provide a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:1357,update,update,1357,docs/deeptrio-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md,2,"['install', 'update']","['install', 'update']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ; Google Cloud Storage. The instructions below show how to download the data; through the corresponding public URLs. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepTrio in one command. Although DeepTrio can be built from a source, we provide a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4
",False,
Deployability,"; [pbmm2](https://github.com/PacificBiosciences/pbmm2).; 2. Illumina NovaSeq on HG003 aligned with; [BWA MEM](https://github.com/lh3/bwa). The FASTQ files come from the; [PrecisionFDA Truth challenge v2](https://precision.fda.gov/challenges/10/view). They are merged together into a single bam file using `samtools merge`, and then; a new index is created for this hybrid bam using `samtools index`. Note that the; two original bam files must have the same sample name. Finally, we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. ## Background on the hybrid model. This is what the pileup image looks like: The longer PacBio reads are shown at; the top, followed by the shorter Illumina reads at the bottom. ![Example of a hybrid pileup for one variant](images/hybrid_pileup.png). A DeepVariant hybrid model was first trained for the PrecisionFDA Truth; Challenge V2, and this release model is similar except it has been re-trained; with additional datasets including the HG004 truth set that was held out during; the challenge. Interestingly, DeepVariant didn't strictly need any code changes to work on; hybrid data -- it worked the first time we tried. But we knew from many previous; experiments that Illumina reads benefit from being realigned to a haplotype; graph, which is too time consuming and unnecessary for the PacBio long reads. We; added a small code change to specifically realign all the short reads to the; haplotype graph, while leaving longer reads with their original alignments. This; created a small but measurable improvement, and was the only code change we made; to enable the hybrid model, aside from training a dedicated hybrid model and; exposing it for easy use through the --model_type parameter in; `run_deepvariant.py`. Much of the work we put into DeepVariant is in; experimenting with different approaches, training on more and better data, and; carefully evaluating th",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:1212,release,release,1212,docs/deepvariant-hybrid-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ; [pbmm2](https://github.com/PacificBiosciences/pbmm2).; 2. Illumina NovaSeq on HG003 aligned with; [BWA MEM](https://github.com/lh3/bwa). The FASTQ files come from the; [PrecisionFDA Truth challenge v2](https://precision.fda.gov/challenges/10/view). They are merged together into a single bam file using `samtools merge`, and then; a new index is created for this hybrid bam using `samtools index`. Note that the; two original bam files must have the same sample name. Finally, we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. ## Background on the hybrid model. This is what the pileup image looks like: The longer PacBio reads are shown at; the top, followed by the shorter Illumina reads at the bottom. ![Example of a hybrid pileup for one variant](images/hybrid_pileup.png). A DeepVariant hybrid model was first trained for the PrecisionFDA Truth; Challenge V2, and this release model is similar except it has been re-trained; with additional datasets including the HG004 truth set that was held out during; the challenge. Interestingly, DeepVariant didn't strictly need any code changes to work on; hybrid data -- it worked the first time we tried. But we knew from many previous; experiments that Illumina reads benefit from being realigned to a haplotype; graph, which is too time consuming and unnecessary for the PacBio long reads. We; added a small code change to specifically realign all the short reads to the; haplotype graph, while leaving longer reads with their original alignments. This; created a small but measurable improvement, and was the only code change we made; to enable the hybrid model, aside from training a dedicated hybrid model and; exposing it for easy use through the --model_type parameter in; `run_deepvariant.py`. Much of the work we put into DeepVariant is in; experimenting with different approaches, training on more and better data, and; carefully evaluating th
",False,"The text contains complete sentences discussing concepts and ideas related to hybrid model training, data handling, and quality assessment. It includes natural language explanations without code snippets or logs."
Deployability,"<img src=""docs/images/dv_logo.png"" width=50% height=50%>. [![release](https://img.shields.io/badge/release-v1.6.1-green?logo=github)](https://github.com/google/deepvariant/releases); [![announcements](https://img.shields.io/badge/announcements-blue)](https://groups.google.com/d/forum/deepvariant-announcements); [![blog](https://img.shields.io/badge/blog-orange)](https://goo.gl/deepvariant). DeepVariant is a deep learning-based variant caller that takes aligned reads (in; BAM or CRAM format), produces pileup image tensors from them, classifies each; tensor using a convolutional neural network, and finally reports the results in; a standard VCF or gVCF file. DeepVariant supports germline variant-calling in diploid organisms. * NGS (Illumina or Element) data for either a; [whole genome](docs/deepvariant-case-study.md) or; [whole exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and; [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The mod",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:61,release,release,61,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,3,['release'],"['release', 'release-', 'releases']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: <img src=""docs/images/dv_logo.png"" width=50% height=50%>. [![release](https://img.shields.io/badge/release-v1.6.1-green?logo=github)](https://github.com/google/deepvariant/releases); [![announcements](https://img.shields.io/badge/announcements-blue)](https://groups.google.com/d/forum/deepvariant-announcements); [![blog](https://img.shields.io/badge/blog-orange)](https://goo.gl/deepvariant). DeepVariant is a deep learning-based variant caller that takes aligned reads (in; BAM or CRAM format), produces pileup image tensors from them, classifies each; tensor using a convolutional neural network, and finally reports the results in; a standard VCF or gVCF file. DeepVariant supports germline variant-calling in diploid organisms. * NGS (Illumina or Element) data for either a; [whole genome](docs/deepvariant-case-study.md) or; [whole exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and; [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The mod
",False,"The text includes detailed descriptions about the functionality and use cases of the tool, including various case studies. It discusses how DeepVariant operates, its support for different data types and sources, and provides links to further resources. This content is explanatory, descriptive, and written in natural language, making it meaningful and relevant for human readers interested in understanding the tool's capabilities."
Deployability,"E100_NA12878.sorted.chr1.bam""; BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam""; BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam""; TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz""; TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16; ```. ## Download binaries and data. ### Create directories:. ```bash; mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${BIN_DIR}""; mkdir -p ""${DATA_DIR}""; mkdir -p ""${LOG_DIR}""; ```. ### Copy data. ```bash; gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}""; ```. ### Download extra packages. ```bash; sudo apt -y update; sudo apt -y install parallel; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh; bash -x install_nvidia_docker.sh; ```. ## Run make_examples in “training” mode for training and validation sets. Create examples in ""training"" mode (which means these `tensorflow.Example`s will; contain a `label` field). In this tutorial, we create examples on one replicate of HG001 sequenced by; BGISEQ-500 provided on the; [Genome In a Bottle FTP site](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/BGISEQ500/standard_library/readme.txt). In this tutorial, we will split the genome up into the following datasets:. | chrom | Name | Description |; | ----- | --------------------- | -------------------------------------------- |; | chr1 | Training Set | Examples used to train our model. |; | chr21 | Validation / Tune Set | Examples used to evaluate the performance of our model during training.|; | chr20 | Test Set | Examples reserved for testing performance of ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:3907,update,update,3907,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,2,"['install', 'update']","['install', 'update']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: E100_NA12878.sorted.chr1.bam""; BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam""; BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam""; TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz""; TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16; ```. ## Download binaries and data. ### Create directories:. ```bash; mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${BIN_DIR}""; mkdir -p ""${DATA_DIR}""; mkdir -p ""${LOG_DIR}""; ```. ### Copy data. ```bash; gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}""; ```. ### Download extra packages. ```bash; sudo apt -y update; sudo apt -y install parallel; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh; bash -x install_nvidia_docker.sh; ```. ## Run make_examples in “training” mode for training and validation sets. Create examples in ""training"" mode (which means these `tensorflow.Example`s will; contain a `label` field). In this tutorial, we create examples on one replicate of HG001 sequenced by; BGISEQ-500 provided on the; [Genome In a Bottle FTP site](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/BGISEQ500/standard_library/readme.txt). In this tutorial, we will split the genome up into the following datasets:. | chrom | Name | Description |; | ----- | --------------------- | -------------------------------------------- |; | chr1 | Training Set | Examples used to train our model. |; | chr21 | Validation / Tune Set | Examples used to evaluate the performance of our model during training.|; | chr20 | Test Set | Examples reserved for testing performance of 
",False,"The text contains explanatory sentences discussing the creation of datasets and their purposes, as well as instructions for data processing steps. It includes natural language descriptions of the process, which meets the criteria for retention."
Deployability,"G002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,863,645; v1.5 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,863,664; v1.6 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,353,081. <a name=""vfootnote1"">(1)</a>: In v0.5, we experimented with adding whole exome; sequencing data into training data. In v0.6, we took it out because it didn't; improve the WGS accuracy. <a name=""vfootnote2"">(2)</a>: The training data are from the same replicates as; v0.5. The number of examples changed because of the update in; [haplotype_labeler](https://github.com/google/deepvariant/tree/r0.6/deepvariant/labeler/haplotype_labeler.py). <a name=""vfootnote3"">(3)</a>: In v0.8, we used the; [Platinum Genomes Truthset](https://github.com/Illumina/PlatinumGenomes) to; create more training examples outside the GIAB confident regions. <a name=""vfootnote4"">(4)</a>: Previously, we split train/tune by leaving 3 WES; for tuning. Starting from this release, we leave out chr1 and chr20 from; training, and use chr1 for tuning. <a name=""vfootnote5"">(5)</a>: Starting from this version, we padded (100bps on; both sides) of the capture BED and used that for generating training examples.; We also added more `downsample_fraction`. <a name=""vfootnote6"">(6)</a>: (Before v1.0) PacBio is the only one we currently; uses HG002 in training and tuning. <a name=""vfootnote7"">(7)</a>: In v1.0, we train on HG002-HG004 for WGS as well,; but only using examples from the region of NIST truth confident region v4.2; subtracting v3.3.2. <a name=""vfootnote8"">(8)</a>: In v1.0, PacBio training data contains training; examples with haplotag sorted images and unsorted images. <a name=""vfootnote9"">(9)</a>: In v1.1, we exclude HG003 from training. And we; use all NIST truth confident regions for HG001-HG007 (except for HG003) for; training. We've always excluded chr20-22 from training. <a name=""vfootnote10"">(10)</a>: In v1.2, we include new PacBio training data; from Sequel II, Chemistry 2.",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details-training-data.md:5119,release,release,5119,docs/deepvariant-details-training-data.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details-training-data.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: G002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,863,645; v1.5 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,863,664; v1.6 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,353,081. <a name=""vfootnote1"">(1)</a>: In v0.5, we experimented with adding whole exome; sequencing data into training data. In v0.6, we took it out because it didn't; improve the WGS accuracy. <a name=""vfootnote2"">(2)</a>: The training data are from the same replicates as; v0.5. The number of examples changed because of the update in; [haplotype_labeler](https://github.com/google/deepvariant/tree/r0.6/deepvariant/labeler/haplotype_labeler.py). <a name=""vfootnote3"">(3)</a>: In v0.8, we used the; [Platinum Genomes Truthset](https://github.com/Illumina/PlatinumGenomes) to; create more training examples outside the GIAB confident regions. <a name=""vfootnote4"">(4)</a>: Previously, we split train/tune by leaving 3 WES; for tuning. Starting from this release, we leave out chr1 and chr20 from; training, and use chr1 for tuning. <a name=""vfootnote5"">(5)</a>: Starting from this version, we padded (100bps on; both sides) of the capture BED and used that for generating training examples.; We also added more `downsample_fraction`. <a name=""vfootnote6"">(6)</a>: (Before v1.0) PacBio is the only one we currently; uses HG002 in training and tuning. <a name=""vfootnote7"">(7)</a>: In v1.0, we train on HG002-HG004 for WGS as well,; but only using examples from the region of NIST truth confident region v4.2; subtracting v3.3.2. <a name=""vfootnote8"">(8)</a>: In v1.0, PacBio training data contains training; examples with haplotag sorted images and unsorted images. <a name=""vfootnote9"">(9)</a>: In v1.1, we exclude HG003 from training. And we; use all NIST truth confident regions for HG001-HG007 (except for HG003) for; training. We've always excluded chr20-22 from training. <a name=""vfootnote10"">(10)</a>: In v1.2, we include new PacBio training data; from Sequel II, Chemistry 2.
",False,"The text contains version notes describing the changes in dataset usage over various versions of a product, including explanations about which samples were used for training and why certain changes were made. This is descriptive content that is meaningful to human readers."
Deployability,"GAAGTA_L008.posiSrt.markDup.bam -o HG004.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bai -o HG004.bai; ```. ### Command for downloading the reference file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:4274,release,release,4274,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: GAAGTA_L008.posiSrt.markDup.bam -o HG004.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bai -o HG004.bai; ```. ### Command for downloading the reference file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.
",True,"The content consists of command lines and file operations, likely from a shell script. It includes commands like 'aria2c' for downloading files, file paths, and parameters such as '-x10', '-s10'. These are indicative of programmatic instructions rather than meaningful human-readable text."
Deployability,"GS).; * Illumina whole exome data (WES).; * PacBio HiFi whole genome data (PacBio WGS). ## Running DeepTrio. The easiest and recommended way to run DeepTrio is using; `google/deepvariant:deeptrio-latest` docker image. Please refer to the; [quick start guide](deeptrio-quick-start.md) for more details on how to run; DeepTrio using docker. Merging VCFs can be done using; [GLnexus](https://github.com/dnanexus-rnd/GLnexus) which has been optimized for; use with DeepVariant gVCFs. The process is described in the DeepTrio case; studies; ([DeepTrio whole genome sequencing case study](deeptrio-wgs-case-study.md) and; [Using DeepTrio for small variant calling from the trio sequenced with PacBio; HiFi](deeptrio-pacbio-case-study.md)), and in the manuscript,; [""Accurate, scalable cohort variant calls using DeepVariant and GLnexus""](https://www.biorxiv.org/content/10.1101/2020.02.10.942086v2). Please note that DeepTrio can be run with a `run_deeptrio.py` script that; automates all DeepTrio steps and thus greatly simplifies the inference pipeline.; The details of using this script can be found in the section below as well as in; the DeepTrio case studies. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we; recommend running these providing only the parent who contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot p",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:2779,pipeline,pipeline,2779,docs/deeptrio-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: GS).; * Illumina whole exome data (WES).; * PacBio HiFi whole genome data (PacBio WGS). ## Running DeepTrio. The easiest and recommended way to run DeepTrio is using; `google/deepvariant:deeptrio-latest` docker image. Please refer to the; [quick start guide](deeptrio-quick-start.md) for more details on how to run; DeepTrio using docker. Merging VCFs can be done using; [GLnexus](https://github.com/dnanexus-rnd/GLnexus) which has been optimized for; use with DeepVariant gVCFs. The process is described in the DeepTrio case; studies; ([DeepTrio whole genome sequencing case study](deeptrio-wgs-case-study.md) and; [Using DeepTrio for small variant calling from the trio sequenced with PacBio; HiFi](deeptrio-pacbio-case-study.md)), and in the manuscript,; [""Accurate, scalable cohort variant calls using DeepVariant and GLnexus""](https://www.biorxiv.org/content/10.1101/2020.02.10.942086v2). Please note that DeepTrio can be run with a `run_deeptrio.py` script that; automates all DeepTrio steps and thus greatly simplifies the inference pipeline.; The details of using this script can be found in the section below as well as in; the DeepTrio case studies. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we; recommend running these providing only the parent who contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot p
",False,"The text contains complete sentences discussing how to run DeepTrio using Docker and describes processes like merging VCFs with GLnexus, providing references to case studies and a manuscript, and explains input assumptions about the reference genome. These are all explanatory and descriptive content suitable for human-readable understanding."
Deployability,"IC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 9811 | 212 | 155 | 0.978849 | 0.985044 | 0.981937 |; | SNP | 66180 | 57 | 70 | 0.999139 | 0.998944 | 0.999042 |. The baseline we're comparing to is to directly use the WGS model to make the; calls, using this command:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE}-gpu \; run_deepvariant \; --model_type WGS \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/baseline.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. Baseline:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 9620 | 403 | 823 | 0.959792 | 0.924112 | 0.941615 |; | SNP | 66159 | 78 | 83 | 0.998822 | 0.998748 | 0.998785 |. ### Additional things to try. #### Parameters to tune. Starting from the default setting of this tutorial is a good starting point, but; this training case study is by no means the best setting. Training is both a; science and an art. There are many knobs that we could potentially tune. Users; might be able to use different parameters to train a more accurate model even; with the same data, such as `batch_size`, `learning_rate`,; `learning_rate_decay_factor` in modeling.py. #### Downsampling the BAM file to generate more training examples. When generating the training set, we can make some adjustment to create more; training data. For example, when we train the released WGS model for; DeepVariant, for each BAM file, we created an extra set of training examples; using `--downsample_fraction=0.5`, which downsamples the reads and creates; training examples with lower coverage. We found that this makes the trained; model more robust. [GPU machine]: deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:21051,release,released,21051,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['release'],['released'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: IC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 9811 | 212 | 155 | 0.978849 | 0.985044 | 0.981937 |; | SNP | 66180 | 57 | 70 | 0.999139 | 0.998944 | 0.999042 |. The baseline we're comparing to is to directly use the WGS model to make the; calls, using this command:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE}-gpu \; run_deepvariant \; --model_type WGS \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/baseline.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. Baseline:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 9620 | 403 | 823 | 0.959792 | 0.924112 | 0.941615 |; | SNP | 66159 | 78 | 83 | 0.998822 | 0.998748 | 0.998785 |. ### Additional things to try. #### Parameters to tune. Starting from the default setting of this tutorial is a good starting point, but; this training case study is by no means the best setting. Training is both a; science and an art. There are many knobs that we could potentially tune. Users; might be able to use different parameters to train a more accurate model even; with the same data, such as `batch_size`, `learning_rate`,; `learning_rate_decay_factor` in modeling.py. #### Downsampling the BAM file to generate more training examples. When generating the training set, we can make some adjustment to create more; training data. For example, when we train the released WGS model for; DeepVariant, for each BAM file, we created an extra set of training examples; using `--downsample_fraction=0.5`, which downsamples the reads and creates; training examples with lower coverage. We found that this makes the trained; model more robust. [GPU machine]: deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform; 
",False,
Deployability,"OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. NOTE: If you prefer shuffling locally, please take a look at this user-provided; shuffler option:; https://github.com/google/deepvariant/issues/360#issuecomment-1019990366. ### Validation set. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. This took: 5m31.122s. Copy to GCS bucket:. ```bash; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. ## Shuffle each set of examples and generate a data configuration file for each. Shuffling the `tensorflow.Example`s is an important step for training a model.; In our training logic, we shuffle examples globally using a preprocessing step. First, if you have run this step before, and want to rerun it, you might want to; consider cleaning up previous data first to avoid confusion:. ```bash; # (Optional) Clean up existing files.; gsutil -m rm -f ""${OUTPUT_BUCKET}/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; gsutil -m rm -f ""${OUTPUT_BUCKET}/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt""; gsutil rm -f ""${OUTPUT_BUCKET}/example_info.json""; ```. Here we provide examples for running on; [Cloud Dataflow Runner](https://beam.apache.org/documentation/runners/dataflow/); and also [DirectRunner](https://beam.apache.org/documentation/runners/direct/).; Beam can also use",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:7832,configurat,configuration,7832,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['configurat'],['configuration'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. NOTE: If you prefer shuffling locally, please take a look at this user-provided; shuffler option:; https://github.com/google/deepvariant/issues/360#issuecomment-1019990366. ### Validation set. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. This took: 5m31.122s. Copy to GCS bucket:. ```bash; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. ## Shuffle each set of examples and generate a data configuration file for each. Shuffling the `tensorflow.Example`s is an important step for training a model.; In our training logic, we shuffle examples globally using a preprocessing step. First, if you have run this step before, and want to rerun it, you might want to; consider cleaning up previous data first to avoid confusion:. ```bash; # (Optional) Clean up existing files.; gsutil -m rm -f ""${OUTPUT_BUCKET}/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; gsutil -m rm -f ""${OUTPUT_BUCKET}/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt""; gsutil rm -f ""${OUTPUT_BUCKET}/example_info.json""; ```. Here we provide examples for running on; [Cloud Dataflow Runner](https://beam.apache.org/documentation/runners/dataflow/); and also [DirectRunner](https://beam.apache.org/documentation/runners/direct/).; Beam can also use
",False,"The provided text contains a mix of command lines, script instructions, and some descriptive content. While there are code snippets and commands, the majority of the text is not purely code or logs. It includes explanatory sentences about shuffling examples and generating data configurations for training, which are meaningful and in natural language. Therefore, it should not be eliminated."
Deployability,"RSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". time sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; --reads=${PWD}/${BAM} \; --output_vcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; --output_gvcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.g.vcf.gz \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --num_shards=$(nproc); ```. Stage | Time (minutes); -------------------------------- | -----------------; make_examples | 116m37.385s; call_variants | 214m37.055s; postprocess_variants (with gVCF) | 30m59.968s. ### Run hap.py. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; ${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; -f ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -o ${PWD}/happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Ben",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:6270,release,release,6270,docs/deepvariant-vg-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: RSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". time sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; --reads=${PWD}/${BAM} \; --output_vcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; --output_gvcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.g.vcf.gz \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --num_shards=$(nproc); ```. Stage | Time (minutes); -------------------------------- | -----------------; make_examples | 116m37.385s; call_variants | 214m37.055s; postprocess_variants (with gVCF) | 30m59.968s. ### Run hap.py. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; ${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; -f ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -o ${PWD}/happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Ben
",False,"The text contains human-readable sentences and commands that are part of a script, but they also include explanatory content such as the creation of directories and downloading files, which is meaningful for humans. The use of bash scripts and commands indicates it's related to system operations, but since there are descriptive steps (like creating 'benchmark' and 'happy' directories), it is not purely code or logs."
Deployability,"This is an example to run `vg giraffe`, so we can go from FASTQs --> BAM. For simplicity and consistency, we run the following with a; [Google Cloud instance with 64 cores](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). I added more disks because 300G is not enough for the example below. I changed; it to `--boot-disk-size ""1000""`. ## Install softwares that will be used later. ```bash; sudo apt update -y; sudo apt-get -y install aria2 docker.io samtools; ```. ## Download input FASTQ files. ```bash; DATA_DIR=${PWD}/data; mkdir -p ${DATA_DIR}; gcloud storage cp gs://brain-genomics-public/research/sequencing/fastq/novaseq/wgs_pcr_free/35x/HG003.novaseq.pcr-free.35x.R?.fastq.gz ${DATA_DIR}/; ```. ## Download VG files. Get binaries `vg` 1.51.0 and `kmc`:. ```bash; wget https://github.com/refresh-bio/KMC/releases/download/v3.2.2/KMC3.2.2.linux.x64.tar.gz; tar zxf KMC3.2.2.linux.x64.tar.gz bin/kmc; mv bin/kmc ${DATA_DIR}/; wget https://github.com/vgteam/vg/releases/download/v1.51.0/vg -O ${DATA_DIR}/vg; chmod +x ${DATA_DIR}/vg ${DATA_DIR}/kmc; ```. Get the graph (.gbz) and haplotype index (.hapl).; I used `aria2c` to download these files. You can use other approaches as well. ```bash; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.gbz; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.hapl; ```. ## Run `vg giraffe` with one command to get from FASTQs to BAM. Put the paths name into a file named HG003.fq.paths:. ```bash; cat > HG003.fq.paths <<- EOM; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R1.fastq.gz; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R2.fastq.gz; EOM; ```. Run `kmc`` on this file. I used -t$(nproc) to use all cores, and $TMPDIR for a; scratch directory:. ```bash; TMPDIR=$(mktemp -d); time ${D",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:1062,release,releases,1062,docs/deepvariant-vg-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md,1,['release'],['releases'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: This is an example to run `vg giraffe`, so we can go from FASTQs --> BAM. For simplicity and consistency, we run the following with a; [Google Cloud instance with 64 cores](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). I added more disks because 300G is not enough for the example below. I changed; it to `--boot-disk-size ""1000""`. ## Install softwares that will be used later. ```bash; sudo apt update -y; sudo apt-get -y install aria2 docker.io samtools; ```. ## Download input FASTQ files. ```bash; DATA_DIR=${PWD}/data; mkdir -p ${DATA_DIR}; gcloud storage cp gs://brain-genomics-public/research/sequencing/fastq/novaseq/wgs_pcr_free/35x/HG003.novaseq.pcr-free.35x.R?.fastq.gz ${DATA_DIR}/; ```. ## Download VG files. Get binaries `vg` 1.51.0 and `kmc`:. ```bash; wget https://github.com/refresh-bio/KMC/releases/download/v3.2.2/KMC3.2.2.linux.x64.tar.gz; tar zxf KMC3.2.2.linux.x64.tar.gz bin/kmc; mv bin/kmc ${DATA_DIR}/; wget https://github.com/vgteam/vg/releases/download/v1.51.0/vg -O ${DATA_DIR}/vg; chmod +x ${DATA_DIR}/vg ${DATA_DIR}/kmc; ```. Get the graph (.gbz) and haplotype index (.hapl).; I used `aria2c` to download these files. You can use other approaches as well. ```bash; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.gbz; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.hapl; ```. ## Run `vg giraffe` with one command to get from FASTQs to BAM. Put the paths name into a file named HG003.fq.paths:. ```bash; cat > HG003.fq.paths <<- EOM; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R1.fastq.gz; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R2.fastq.gz; EOM; ```. Run `kmc`` on this file. I used -t$(nproc) to use all cores, and $TMPDIR for a; scratch directory:. ```bash; TMPDIR=$(mktemp -d); time ${D
",True,"The text contains code snippets and command lines, including syntax like 'sudo apt update -y;' and 'mkdir -p ${DATA_DIR};'. Additionally, there are multiple code blocks with bash commands, which are typical of program logs or build instructions. These sections describe how to install software and execute commands, which fall under the category of programmatic content that should be filtered out as they do not contain meaningful human-readable sentences."
Deployability,"Variant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepVariant ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:1145,release,release,1145,docs/deepvariant-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Variant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepVariant 
",False,"The text contains a mix of human-readable instructions and code snippets, but the majority consists of meaningful sentences discussing the process of downloading reference genomes and preparing data for variant calling. The presence of bash commands indicates some technical content, but these are part of an explanation rather than primary code to be executed."
Deployability,"_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG004_truth.bed; ```. (No need to install bcftools and other tools, because they are now installed in; the DeepVariant images.). ## Run DeepVariant on trio to get 3 single sample VCFs. First, install docker if you don't have it yet: `sudo apt-get -y install; docker.io`. With the example command below, it runs DeepVariant on the trio one by one. This; is for demonstration only. If you're running this on a large cohort, running; serially is not the most effective approach. ```; N_SHARDS=$(nproc) # Or change to the number of cores you want to use; CAPTURE_BED=agilent_sureselect_human_all_exon_v5_b37_targets.bed; VERSION=1.6.1. declare -a trio=(HG002 HG003 HG004); for SAMPLE in ""${trio[@]}""; do; BAM=${SAMPLE}.bam. OUTPUT_VCF=${SAMPLE}.vcf.gz; OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time sudo docker run \; -v ""${DIR}"":""/data"" \; google/deepvariant:${VERSION} \; /opt/deepvariant/bin/run_deepvariant \;",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:5951,release,release,5951,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: _GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG004_truth.bed; ```. (No need to install bcftools and other tools, because they are now installed in; the DeepVariant images.). ## Run DeepVariant on trio to get 3 single sample VCFs. First, install docker if you don't have it yet: `sudo apt-get -y install; docker.io`. With the example command below, it runs DeepVariant on the trio one by one. This; is for demonstration only. If you're running this on a large cohort, running; serially is not the most effective approach. ```; N_SHARDS=$(nproc) # Or change to the number of cores you want to use; CAPTURE_BED=agilent_sureselect_human_all_exon_v5_b37_targets.bed; VERSION=1.6.1. declare -a trio=(HG002 HG003 HG004); for SAMPLE in ""${trio[@]}""; do; BAM=${SAMPLE}.bam. OUTPUT_VCF=${SAMPLE}.vcf.gz; OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time sudo docker run \; -v ""${DIR}"":""/data"" \; google/deepvariant:${VERSION} \; /opt/deepvariant/bin/run_deepvariant \;
",False,
Deployability,"_analysis reference using; [pbmm2](https://github.com/PacificBiosciences/pbmm2). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/pacbio-case-study-testdata. curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam > input/HG002.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG002.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam > input/HG003.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG003.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam > input/HG004.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG004.pfda_challenge.grch38.phased.chr20.bam.bai; ```. ## Running DeepTrio with one command. DeepTrio pipeline consists of 4 steps: `make_examples`, `call_variants`,; `postprocess_variants` and `GLnexus merge`. It is possible to run the first; three steps with one command using the `run_deeptrio` script. GLnexus; is run as a separate command. ### Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}"". time sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type PACBIO \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads_child /input/HG002.pfda_challenge.grch38.phased.chr20.bam \; --reads_parent1 /input/HG003.pfda_challenge.grch38.phased.chr20.bam \; --reads_parent2 /input/HG004.pfda_challenge.grch38.phased.chr20.bam \; --output_vcf_child /output/HG002.output.vcf.gz \; --output_",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:3975,pipeline,pipeline,3975,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: _analysis reference using; [pbmm2](https://github.com/PacificBiosciences/pbmm2). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/pacbio-case-study-testdata. curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam > input/HG002.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG002.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam > input/HG003.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG003.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam > input/HG004.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG004.pfda_challenge.grch38.phased.chr20.bam.bai; ```. ## Running DeepTrio with one command. DeepTrio pipeline consists of 4 steps: `make_examples`, `call_variants`,; `postprocess_variants` and `GLnexus merge`. It is possible to run the first; three steps with one command using the `run_deeptrio` script. GLnexus; is run as a separate command. ### Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}"". time sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type PACBIO \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads_child /input/HG002.pfda_challenge.grch38.phased.chr20.bam \; --reads_parent1 /input/HG003.pfda_challenge.grch38.phased.chr20.bam \; --reads_parent2 /input/HG004.pfda_challenge.grch38.phased.chr20.bam \; --output_vcf_child /output/HG002.output.vcf.gz \; --output_
",False,
Deployability,"_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG004_truth.bed; ```. (No need to install bcftools and other tools, because they are now installed in; the DeepVariant images.). ## Run DeepVariant on trio to get 3 single sample VCFs. First, install docker if you don't have it yet: `sudo apt-get -y install; docker.io`. With the example command below, it runs DeepVariant on the trio one by one. This; is for demonstration only. If you're running this on a large cohort, running; serially is not the most effective approach. ```; N_SHARDS=$(nproc) # Or change to the number of cores you want to use; CAPTURE_BED=agilent_sureselect_human_all_exon_v5_b37_targets.bed; VERSION=1.6.1. declare -a trio=(HG002 HG003 HG004); for SAMPLE in ""${trio[",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:5742,release,release,5742,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: _benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG004_truth.bed; ```. (No need to install bcftools and other tools, because they are now installed in; the DeepVariant images.). ## Run DeepVariant on trio to get 3 single sample VCFs. First, install docker if you don't have it yet: `sudo apt-get -y install; docker.io`. With the example command below, it runs DeepVariant on the trio one by one. This; is for demonstration only. If you're running this on a large cohort, running; serially is not the most effective approach. ```; N_SHARDS=$(nproc) # Or change to the number of cores you want to use; CAPTURE_BED=agilent_sureselect_human_all_exon_v5_b37_targets.bed; VERSION=1.6.1. declare -a trio=(HG002 HG003 HG004); for SAMPLE in ""${trio[
",True,"The text contains command lines and code snippets, which are primarily logs or programmatic descriptions. It includes commands like 'aria2c', file operations, and variable declarations, typical of a script or log file."
Deployability,"_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG004_truth.bed; ```. (No need to install bcftools an",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:5104,release,release,5104,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: _targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG004_truth.bed; ```. (No need to install bcftools an
",False,"The text consists of command lines for downloading files, which are in the form of human-readable sentences explaining the process. They describe how to execute specific tasks using command-line tools like aria2c. The commands include file paths and parameters, but they are written in a way that is understandable by humans rather than code snippets or logs."
Deployability,"a multiple-GPU machine. ### postprocess_variants. `postprocess_variants` reads all of the output TFRecord files from; `call_variants`, sorts them, combines multi-allelic records, and writes out a; VCF file. When [gVCF output](deepvariant-gvcf-support.md) is requested, it also; outputs a gVCF file which merges the VCF with the non-variant sites. Because `postprocess_variants` combines and sorts the output of `call_variants`,; it needs to see all of the outputs from `call_variants` for a single sample to; merge into a final VCF. `postprocess_variants` is single-threaded and needs a; non-trivial amount of memory to run (20-30 GB), so it is best run on a; single/dual core machine with sufficient memory. ## Updates on DeepVariant since precisionFDA truth challenge and bioRxiv preprint. The DeepVariant team has been hard at work since we first presented the method.; Key changes and improvements include:. * Rearchitected with open source release in mind; * Built on [TensorFlow]; * Increased variant calling accuracy, especially for indels; * Vastly faster with reduced memory usage. We have made a number of improvements to the methodology as well. The biggest; change was to move away from RGB-encoded (3-channel) pileup images and instead; represent the aligned read data using a multi-channel tensor data layout. We; currently represent the data as a 6-channel raw tensor in which we encode:. * The read base (A, C, G, T); * The base's quality score; * The read's mapping quality score; * The read's strand (positive or negative); * Does the read support the allele being evaluated?; * Does the base match the reference genome at this position?. These are all readily derived from the information found in the BAM file; encoding of each read. Additional modeling changes were to move to the inception-v3 architecture and to; train on many more independent sequencing replicates of the ground truth; training samples, including 50% downsampled versions of each of those read sets.; In our te",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:6874,release,release,6874,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: a multiple-GPU machine. ### postprocess_variants. `postprocess_variants` reads all of the output TFRecord files from; `call_variants`, sorts them, combines multi-allelic records, and writes out a; VCF file. When [gVCF output](deepvariant-gvcf-support.md) is requested, it also; outputs a gVCF file which merges the VCF with the non-variant sites. Because `postprocess_variants` combines and sorts the output of `call_variants`,; it needs to see all of the outputs from `call_variants` for a single sample to; merge into a final VCF. `postprocess_variants` is single-threaded and needs a; non-trivial amount of memory to run (20-30 GB), so it is best run on a; single/dual core machine with sufficient memory. ## Updates on DeepVariant since precisionFDA truth challenge and bioRxiv preprint. The DeepVariant team has been hard at work since we first presented the method.; Key changes and improvements include:. * Rearchitected with open source release in mind; * Built on [TensorFlow]; * Increased variant calling accuracy, especially for indels; * Vastly faster with reduced memory usage. We have made a number of improvements to the methodology as well. The biggest; change was to move away from RGB-encoded (3-channel) pileup images and instead; represent the aligned read data using a multi-channel tensor data layout. We; currently represent the data as a 6-channel raw tensor in which we encode:. * The read base (A, C, G, T); * The base's quality score; * The read's mapping quality score; * The read's strand (positive or negative); * Does the read support the allele being evaluated?; * Does the base match the reference genome at this position?. These are all readily derived from the information found in the BAM file; encoding of each read. Additional modeling changes were to move to the inception-v3 architecture and to; train on many more independent sequencing replicates of the ground truth; training samples, including 50% downsampled versions of each of those read sets.; In our te
",False,"The text contains complete sentences in natural language discussing concepts and updates about DeepVariant. It includes information on architecture changes, improvements, and details about data representation."
Deployability,"a through the; corresponding public URLs from these data. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepVariant in one command. Starting from the 0.8 release, we introduced one convenient command that will; run through all 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:1359,update,update,1359,docs/deepvariant-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md,2,"['install', 'update']","['install', 'update']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: a through the; corresponding public URLs from these data. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepVariant in one command. Starting from the 0.8 release, we introduced one convenient command that will; run through all 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; 
",False,
Deployability,"and for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-tra",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:4903,release,release,4903,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: and for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-tra
",False,"The text contains commands for downloading files using aria2c, which are meaningful human-readable sentences explaining the process of obtaining data. It includes descriptive content such as file names and URLs, which provide context and are not purely programmatic or code-based."
Deployability,"better data, and; carefully evaluating the models before releasing them. We did the same with this; hybrid model. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:3179,release,release,3179,docs/deepvariant-hybrid-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: better data, and; carefully evaluating the models before releasing them. We did the same with this; hybrid model. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam
",False,
Deployability,"chemistry Simplex and Duplex reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true` to; the command above to figure out what flags you need in each step. Based on the; different model types, different flags are needed in the `make_examples` step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. ## Benchmark HG002 chr20 output from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002. ```bash; FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. TRUTH_VCF=""HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz""; TRUTH_BED=""HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${INPUT_DIR}/${REF}"" \; -o ""${OUTPUT_DIR}/hg002.duplex.r104.ont.chr20.happy.output"" \; --engine=vcfeval \; --pass-only \; -l ""${",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md:4394,release,release,4394,docs/deepvariant-ont-r104-duplex-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: chemistry Simplex and Duplex reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true` to; the command above to figure out what flags you need in each step. Based on the; different model types, different flags are needed in the `make_examples` step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. ## Benchmark HG002 chr20 output from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002. ```bash; FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. TRUTH_VCF=""HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz""; TRUTH_BED=""HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${INPUT_DIR}/${REF}"" \; -o ""${OUTPUT_DIR}/hg002.duplex.r104.ont.chr20.happy.output"" \; --engine=vcfeval \; --pass-only \; -l ""${
",False,"The text contains bash code snippets, which are programmatic and not meaningful human-readable sentences."
Deployability,"cker pull google/deepvariant:""${BIN_VERSION}"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --haploid_contigs ""${HAPLOID_CONTIGS}"" \; --par_regions_bed ""${INPUT_DIR}/${PAR_BED}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Benchmark X, Y outputs from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v1.0 of the Genome in a Bottle; small variant benchmarks for HG002_chrXY. ```bash; FTPDIR=https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/chrXY_v1.0/GRCh38/SmallVariant. curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.bed > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.bed; curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz.tbi > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz.tbi. TRUTH_VCF=""HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz""; TRUTH_BED=""HG002_GRCh38_chrXY_smallvar_v1.0.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. REGION=""chrX,chrY""; sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${INPUT_DIR}/${REF}"" \; -o ""${OUTPUT_DIR}/hg002.chrXY.happy.output"" \; --engine=vcfeval \; --pass-only \; -l ""${REGION}""; ```. Output:. ```; Benchmark",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md:3610,release,release,3610,docs/deepvariant-xy-calling-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: cker pull google/deepvariant:""${BIN_VERSION}"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --haploid_contigs ""${HAPLOID_CONTIGS}"" \; --par_regions_bed ""${INPUT_DIR}/${PAR_BED}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Benchmark X, Y outputs from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v1.0 of the Genome in a Bottle; small variant benchmarks for HG002_chrXY. ```bash; FTPDIR=https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/chrXY_v1.0/GRCh38/SmallVariant. curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.bed > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.bed; curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz.tbi > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz.tbi. TRUTH_VCF=""HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz""; TRUTH_BED=""HG002_GRCh38_chrXY_smallvar_v1.0.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. REGION=""chrX,chrY""; sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${INPUT_DIR}/${REF}"" \; -o ""${OUTPUT_DIR}/hg002.chrXY.happy.output"" \; --engine=vcfeval \; --pass-only \; -l ""${REGION}""; ```. Output:. ```; Benchmark
",False,"The content includes command lines and descriptions that are part of a script or program, but it also contains sentences like 'Benchmark X, Y outputs from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant.' which are meaningful, explaining the purpose and process. Additionally, there are references to downloading datasets and setting up benchmarks, which contribute to descriptive content. Therefore, while the majority is programmatic, it also includes explanatory text that should be retained."
Deployability,"contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot pipe them into DeepTrio. Duplicate marking may be; performed. In our analyses, there is almost no difference in accuracy with and; without duplicate marking except at lower (<20x) coverages. Finally, we; recommend that you do not perform BQSR. Running BQSR has a small decrease on; accuracy. If you are providing `--regions` or other similar arguments, these should refer; to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. ## Training data. DeepTrio models are trained using the latest publicly avavilable GIAB; benchmarks. You can find more details about the training data for each DeepTrio; model in the; [DeepTrio Training Data document](deeptrio-details-training-data.md). ## DeepVariant dependency. DeepTrio is built on top of DeepVariant and they share most of the components.; Please see [DeepVariant usage guide](deepvariant-details.md) for a full; description of DeepVariant components as well as other consideration for running; DeepVariant pipeline.; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:5022,pipeline,pipeline,5022,docs/deeptrio-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot pipe them into DeepTrio. Duplicate marking may be; performed. In our analyses, there is almost no difference in accuracy with and; without duplicate marking except at lower (<20x) coverages. Finally, we; recommend that you do not perform BQSR. Running BQSR has a small decrease on; accuracy. If you are providing `--regions` or other similar arguments, these should refer; to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. ## Training data. DeepTrio models are trained using the latest publicly avavilable GIAB; benchmarks. You can find more details about the training data for each DeepTrio; model in the; [DeepTrio Training Data document](deeptrio-details-training-data.md). ## DeepVariant dependency. DeepTrio is built on top of DeepVariant and they share most of the components.; Please see [DeepVariant usage guide](deepvariant-details.md) for a full; description of DeepVariant components as well as other consideration for running; DeepVariant pipeline.; 
",False,"The text contains complete, meaningful sentences in natural language discussing concepts and ideas related to the usage and configuration of DeepTrio. It provides explanatory content about input assumptions, training data, dependencies, and building instructions. The text includes human-readable prose and avoids code snippets or programmatic descriptions."
Deployability,d channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai; ```. ## Run DeepVariant on chromosome 20 alignments. ```bash; ulimit -u 10000 # https://stac,MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:1891,release,release,1891,docs/deepvariant-pacbio-model-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: d channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai; ```. ## Run DeepVariant on chromosome 20 alignments. ```bash; ulimit -u 10000 # https://stac
",False,
Deployability,"e ""service account"" so that we can authorize; it. (As you progress in your use of Google Cloud Platform, you will likely find it; useful to create a [Cloud; Organization](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:2368,install,installation,2368,docs/deepvariant-gcp-info.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md,1,['install'],['installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: e ""service account"" so that we can authorize; it. (As you progress in your use of Google Cloud Platform, you will likely find it; useful to create a [Cloud; Organization](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for
",False,
Deployability,"e other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. Consult the instructions at https://beam.apache.org/get-started/quickstart-py/; if you run into any issues. Then, get the script that performs shuffling:. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. Next, we shuffle the data using DataflowRunner. Before that, please make sure; you enable Dataflow API for your project:; http://console.cloud.google.com/flows/enableapi?apiid=dataflow. To access `gs://` path, make sure you run this in your virtual environment:. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. Shuffle using Dataflow. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. Then, you should be able to see the run on:; https://console.cloud.google.com/dataflow?project=YOUR_PROJECT. In order to have the best performance, you might need extra resourc",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:9761,update,update,9761,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,7,"['install', 'update', 'upgrade']","['install', 'update', 'upgrade']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: e other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. Consult the instructions at https://beam.apache.org/get-started/quickstart-py/; if you run into any issues. Then, get the script that performs shuffling:. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. Next, we shuffle the data using DataflowRunner. Before that, please make sure; you enable Dataflow API for your project:; http://console.cloud.google.com/flows/enableapi?apiid=dataflow. To access `gs://` path, make sure you run this in your virtual environment:. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. Shuffle using Dataflow. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. Then, you should be able to see the run on:; https://console.cloud.google.com/dataflow?project=YOUR_PROJECT. In order to have the best performance, you might need extra resourc
",False,
Deployability,"eepVariant is highly accurate for; [many types of NGS data](https://rdcu.be/7Dhl), some users may be interested in; training custom deep learning models that have been optimized for very specific; data. This case study describes one way to train such a custom model using a GPU, in; this case for BGISEQ-500 data. Please note that there is not yet a production-grade training pipeline. This is; just one example of how to train a custom model, and is neither the fastest nor; the cheapest possible configuration. The resulting model also does not represent; the greatest achievable accuracy for BGISEQ-500 data. ## High level summary of result. We demonstrated that by training on 1 replicate of BGISEQ-500 whole genome data; (everything except for chromosome 20-22), we can significantly improve the; accuracy comparing to the WGS model as a baseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; following command:. ```bash; gcloud compute ssh ${host} --zone ${zone}; ```. Once you have logged in, set the variables:. ```bash; YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT; OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant""; VERSION=",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:1298,pipeline,pipeline,1298,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: eepVariant is highly accurate for; [many types of NGS data](https://rdcu.be/7Dhl), some users may be interested in; training custom deep learning models that have been optimized for very specific; data. This case study describes one way to train such a custom model using a GPU, in; this case for BGISEQ-500 data. Please note that there is not yet a production-grade training pipeline. This is; just one example of how to train a custom model, and is neither the fastest nor; the cheapest possible configuration. The resulting model also does not represent; the greatest achievable accuracy for BGISEQ-500 data. ## High level summary of result. We demonstrated that by training on 1 replicate of BGISEQ-500 whole genome data; (everything except for chromosome 20-22), we can significantly improve the; accuracy comparing to the WGS model as a baseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; following command:. ```bash; gcloud compute ssh ${host} --zone ${zone}; ```. Once you have logged in, set the variables:. ```bash; YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT; OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant""; VERSION=
",False,
Deployability,"ered false positives).; These should be bgzipped and tabix indexed and be on a reference consistent with; the one provided with the `--ref` argument. ### call_variants. `call_variants` consumes TFRecord file(s) of tf.Examples protos created; by `make_examples` and a deep learning model checkpoint and evaluates the model; on each example in the input TFRecord. The output here is a TFRecord of; CallVariantsOutput protos. `call_variants` doesn't directly support sharding its; outputs, but accepts a glob or shard-pattern for its inputs. `call_variants` uses around 4 GB per process and uses TensorFlow for evaluation.; When evaluating a model in CPU mode, TensorFlow can make use of multiple cores,; but scaling is sub-linear. In other words, `call_variants` on a 64 core machine; is less than 8x faster than running on a 8 core machine. When using a GPU, `call_variants` is both faster, more efficient, and needs; fewer CPUs. Based on a small number of experiments, currently the most efficient; configuration for `call_variants` on a GPU instance is 4-8 CPUs and 1 GPU.; Compared to our setting in the [whole genome case study], we noticed a 2.5x; speedup on the call_variants step using a single P100 GPU and 8 CPUs. Note that; currently `call_variants` can only use one GPU at most. So it doesn't improve; the speed if you get a multiple-GPU machine. ### postprocess_variants. `postprocess_variants` reads all of the output TFRecord files from; `call_variants`, sorts them, combines multi-allelic records, and writes out a; VCF file. When [gVCF output](deepvariant-gvcf-support.md) is requested, it also; outputs a gVCF file which merges the VCF with the non-variant sites. Because `postprocess_variants` combines and sorts the output of `call_variants`,; it needs to see all of the outputs from `call_variants` for a single sample to; merge into a final VCF. `postprocess_variants` is single-threaded and needs a; non-trivial amount of memory to run (20-30 GB), so it is best run on a; single/d",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:5595,configurat,configuration,5595,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['configurat'],['configuration'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ered false positives).; These should be bgzipped and tabix indexed and be on a reference consistent with; the one provided with the `--ref` argument. ### call_variants. `call_variants` consumes TFRecord file(s) of tf.Examples protos created; by `make_examples` and a deep learning model checkpoint and evaluates the model; on each example in the input TFRecord. The output here is a TFRecord of; CallVariantsOutput protos. `call_variants` doesn't directly support sharding its; outputs, but accepts a glob or shard-pattern for its inputs. `call_variants` uses around 4 GB per process and uses TensorFlow for evaluation.; When evaluating a model in CPU mode, TensorFlow can make use of multiple cores,; but scaling is sub-linear. In other words, `call_variants` on a 64 core machine; is less than 8x faster than running on a 8 core machine. When using a GPU, `call_variants` is both faster, more efficient, and needs; fewer CPUs. Based on a small number of experiments, currently the most efficient; configuration for `call_variants` on a GPU instance is 4-8 CPUs and 1 GPU.; Compared to our setting in the [whole genome case study], we noticed a 2.5x; speedup on the call_variants step using a single P100 GPU and 8 CPUs. Note that; currently `call_variants` can only use one GPU at most. So it doesn't improve; the speed if you get a multiple-GPU machine. ### postprocess_variants. `postprocess_variants` reads all of the output TFRecord files from; `call_variants`, sorts them, combines multi-allelic records, and writes out a; VCF file. When [gVCF output](deepvariant-gvcf-support.md) is requested, it also; outputs a gVCF file which merges the VCF with the non-variant sites. Because `postprocess_variants` combines and sorts the output of `call_variants`,; it needs to see all of the outputs from `call_variants` for a single sample to; merge into a final VCF. `postprocess_variants` is single-threaded and needs a; non-trivial amount of memory to run (20-30 GB), so it is best run on a; single/d
",True,"The text consists primarily of code snippets, function descriptions, and programmatic details which are typically not meaningful human-readable content."
Deployability,"fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. This should create a subdirectory in the current directory containing the actual; data files:. ```bash; ls -1 ${INPUT_DIR}; ```. outputting:. ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ### Model location (optional). Starting from r0.8, we put the model files inside the released Docker images.; So there is no need to download model files anymore. If you want to find the; model files of all releases, you can find them in our bucket on the Google Cloud; Storage. You can view them in the browser:; https://console.cloud.google.com/storage/browser/deepvariant/models/DeepVariant. ## Run DeepVariant with one command. DeepVariant consists of 3 main binaries: `make_examples`, `call_variants`, and; `postprocess_variants`. To make it easier to run, we create one entrypoint that; can be directly run as a docker command. If you want to see the details, you can; read through [run_deepvariant.py]. ```bash; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; ```. You can run everything with the following command:. ```bash; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:3666,release,releases,3666,docs/deepvariant-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md,1,['release'],['releases'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. This should create a subdirectory in the current directory containing the actual; data files:. ```bash; ls -1 ${INPUT_DIR}; ```. outputting:. ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ### Model location (optional). Starting from r0.8, we put the model files inside the released Docker images.; So there is no need to download model files anymore. If you want to find the; model files of all releases, you can find them in our bucket on the Google Cloud; Storage. You can view them in the browser:; https://console.cloud.google.com/storage/browser/deepvariant/models/DeepVariant. ## Run DeepVariant with one command. DeepVariant consists of 3 main binaries: `make_examples`, `call_variants`, and; `postprocess_variants`. To make it easier to run, we create one entrypoint that; can be directly run as a docker command. If you want to see the details, you can; read through [run_deepvariant.py]. ```bash; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; ```. You can run everything with the following command:. ```bash; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf
",False,
Deployability,"ffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_DIR}""/validation_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_DIR}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DirectRunner \; --direct_num_workers=0; ```. Here is the validation_set:. ```bash; cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt""; ```. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 5591; # class1: 31854; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${T",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:12841,configurat,configuration,12841,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['configurat'],['configuration'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_DIR}""/validation_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_DIR}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DirectRunner \; --direct_num_workers=0; ```. Here is the validation_set:. ```bash; cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt""; ```. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 5591; # class1: 31854; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${T
",True,"The text consists of command lines and parameters used in a script, which are typically associated with code or program execution rather than meaningful human-readable content."
Deployability,"ge.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DI",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:4678,release,release,4678,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ge.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DI
",False,"The text contains a series of shell commands used to download various data files related to exome case studies. These commands are in the form of code snippets, which are primarily for programmatic execution rather than providing meaningful human-readable sentences. The content includes parameterized commands and file downloads, which fall under API documentation or build system output."
Deployability,"h preemptible pricing, the cost is $2.84 for a; 30x whole genome and $0.21 for whole exome (not considering preemption).; * **Speed** - See [metrics](docs/metrics.md) for the runtime of all supported; datatypes on a 64-core CPU-only machine</sup>. Multiple options for; acceleration exist.; * **Usage options** - DeepVariant can be run via Docker or binaries, using; both on-premise hardware or in the cloud, with support for hardware; accelerators like GPUs and TPUs. <a name=""myfootnote1"">(1)</a>: Time estimates do not include mapping. ## How DeepVariant works. ![Stages in DeepVariant](docs/images/inference_flow_diagram.svg). For more information on the pileup images and how to read them, please see the; [""Looking through DeepVariant's Eyes"" blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant relies on [Nucleus](https://github.com/google/nucleus), a library of; Python and C++ code for reading and writing data in common genomics file formats; (like SAM and VCF) designed for painless integration with the; [TensorFlow](https://www.tensorflow.org/) machine learning framework. Nucleus; was built with DeepVariant in mind and open-sourced separately so it can be used; by anyone in the genomics research community for other projects. See this blog; post on; [Using Nucleus and TensorFlow for DNA Sequencing Error Correction](https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/). ## DeepVariant Setup. ### Prerequisites. * Unix-like operating system (cannot run on Windows); * Python 3.8. ### Official Solutions. Below are the official solutions provided by the; [Genomics team in Google Health](https://health.google/health-research/). Name | Description; :-------------------------------------------------------------------------------------------------: | -----------; [Docker](docs/deepvariant-quick-start.md) | This is the recommended method.; [Build from sou",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:9480,integrat,integration,9480,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['integrat'],['integration'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: h preemptible pricing, the cost is $2.84 for a; 30x whole genome and $0.21 for whole exome (not considering preemption).; * **Speed** - See [metrics](docs/metrics.md) for the runtime of all supported; datatypes on a 64-core CPU-only machine</sup>. Multiple options for; acceleration exist.; * **Usage options** - DeepVariant can be run via Docker or binaries, using; both on-premise hardware or in the cloud, with support for hardware; accelerators like GPUs and TPUs. <a name=""myfootnote1"">(1)</a>: Time estimates do not include mapping. ## How DeepVariant works. ![Stages in DeepVariant](docs/images/inference_flow_diagram.svg). For more information on the pileup images and how to read them, please see the; [""Looking through DeepVariant's Eyes"" blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant relies on [Nucleus](https://github.com/google/nucleus), a library of; Python and C++ code for reading and writing data in common genomics file formats; (like SAM and VCF) designed for painless integration with the; [TensorFlow](https://www.tensorflow.org/) machine learning framework. Nucleus; was built with DeepVariant in mind and open-sourced separately so it can be used; by anyone in the genomics research community for other projects. See this blog; post on; [Using Nucleus and TensorFlow for DNA Sequencing Error Correction](https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/). ## DeepVariant Setup. ### Prerequisites. * Unix-like operating system (cannot run on Windows); * Python 3.8. ### Official Solutions. Below are the official solutions provided by the; [Genomics team in Google Health](https://health.google/health-research/). Name | Description; :-------------------------------------------------------------------------------------------------: | -----------; [Docker](docs/deepvariant-quick-start.md) | This is the recommended method.; [Build from sou
",False,"The text contains meaningful human-readable sentences discussing the features and usage of DeepVariant, including explanations about its functionality, prerequisites, and setup instructions. It includes links to documentation and blog posts for further reference but does not consist solely of code snippets, logs, or API specifications."
Deployability,"he ground truth; training samples, including 50% downsampled versions of each of those read sets.; In our testing this allowed the model to better generalize to other data types. In the end these changes reduced our error rate by more than 50% on the held out; evaluation sample (NA24385 / HG002) as compared to our results in the; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results/):. DeepVariant April 2016 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 4175 | 2839 | 0.987882 | 0.991728 | 0.989802; SNP | 1689 | 832 | 0.999447 | 0.999728 | 0.999587. DeepVariant December 2017 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 2384 | 1811 | 0.993081 | 0.994954 | 0.994017; SNP | 735 | 363 | 0.999759 | 0.999881 | 0.999820. See the [whole genome case study], which we update with each release of; DeepVariant, for the latest results. You can also see the [Colab example] to see how you can visualize the pileup; images. ## Training data over time. For the models we've released over time, you can find more details about the; training data in; [DeepVariant training data](deepvariant-details-training-data.md). ## CRAM support. As of v0.7, DeepVariant accepts CRAM files as input in addition to BAM files. As of v0.9.0, we changed the default to use the reference file specified by the; `--ref` flag, instead of the path to the original reference in the CRAM file; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:8812,update,update,8812,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,2,"['release', 'update']","['release', 'update']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: he ground truth; training samples, including 50% downsampled versions of each of those read sets.; In our testing this allowed the model to better generalize to other data types. In the end these changes reduced our error rate by more than 50% on the held out; evaluation sample (NA24385 / HG002) as compared to our results in the; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results/):. DeepVariant April 2016 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 4175 | 2839 | 0.987882 | 0.991728 | 0.989802; SNP | 1689 | 832 | 0.999447 | 0.999728 | 0.999587. DeepVariant December 2017 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 2384 | 1811 | 0.993081 | 0.994954 | 0.994017; SNP | 735 | 363 | 0.999759 | 0.999881 | 0.999820. See the [whole genome case study], which we update with each release of; DeepVariant, for the latest results. You can also see the [Colab example] to see how you can visualize the pileup; images. ## Training data over time. For the models we've released over time, you can find more details about the; training data in; [DeepVariant training data](deepvariant-details-training-data.md). ## CRAM support. As of v0.7, DeepVariant accepts CRAM files as input in addition to BAM files. As of v0.9.0, we changed the default to use the reference file specified by the; `--ref` flag, instead of the path to the original reference in the CRAM file; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](
",False,"The text includes several sentences that are complete, meaningful, and in natural language. They discuss the performance of DeepVariant over time, its support for different file formats, and provide links to further information. These sentences are part of explanatory content, which should be kept as they contribute to human-readable prose."
Deployability,"he held out; evaluation sample (NA24385 / HG002) as compared to our results in the; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results/):. DeepVariant April 2016 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 4175 | 2839 | 0.987882 | 0.991728 | 0.989802; SNP | 1689 | 832 | 0.999447 | 0.999728 | 0.999587. DeepVariant December 2017 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 2384 | 1811 | 0.993081 | 0.994954 | 0.994017; SNP | 735 | 363 | 0.999759 | 0.999881 | 0.999820. See the [whole genome case study], which we update with each release of; DeepVariant, for the latest results. You can also see the [Colab example] to see how you can visualize the pileup; images. ## Training data over time. For the models we've released over time, you can find more details about the; training data in; [DeepVariant training data](deepvariant-details-training-data.md). ## CRAM support. As of v0.7, DeepVariant accepts CRAM files as input in addition to BAM files. As of v0.9.0, we changed the default to use the reference file specified by the; `--ref` flag, instead of the path to the original reference in the CRAM file; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 3",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:9013,release,released,9013,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['release'],['released'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: he held out; evaluation sample (NA24385 / HG002) as compared to our results in the; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results/):. DeepVariant April 2016 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 4175 | 2839 | 0.987882 | 0.991728 | 0.989802; SNP | 1689 | 832 | 0.999447 | 0.999728 | 0.999587. DeepVariant December 2017 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 2384 | 1811 | 0.993081 | 0.994954 | 0.994017; SNP | 735 | 363 | 0.999759 | 0.999881 | 0.999820. See the [whole genome case study], which we update with each release of; DeepVariant, for the latest results. You can also see the [Colab example] to see how you can visualize the pileup; images. ## Training data over time. For the models we've released over time, you can find more details about the; training data in; [DeepVariant training data](deepvariant-details-training-data.md). ## CRAM support. As of v0.7, DeepVariant accepts CRAM files as input in addition to BAM files. As of v0.9.0, we changed the default to use the reference file specified by the; `--ref` flag, instead of the path to the original reference in the CRAM file; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 3
",True,"The text contains code snippets, data tables, and program logs which primarily consist of statistical metrics (False Negative (FN), False Positive (FP), Recall, Precision, F1_Score) that are typical in evaluation reports but lack meaningful human-readable sentences. The presence of specific code-related terms, formatting, and numerical data without contextual narrative makes this text unsuitable for inclusion in a general audience's reading."
Deployability,"hmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG004_truth.bed; ```. (No need to install bcftools and other tools, because they are now installed in; the DeepVariant images.). ## Run DeepVariant on trio to get 3 single sample VCFs. First, install docker if you don't have it yet: `sudo apt-get -y install; docker.io`. With the example command below, it runs DeepVariant on the trio one by one. This; is for demonstration only. If you're running this on a large cohort, running; serially is not the most effective approach. ```; N_SHARDS=",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:5541,release,release,5541,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: hmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG004_truth.bed; ```. (No need to install bcftools and other tools, because they are now installed in; the DeepVariant images.). ## Run DeepVariant on trio to get 3 single sample VCFs. First, install docker if you don't have it yet: `sudo apt-get -y install; docker.io`. With the example command below, it runs DeepVariant on the trio one by one. This; is for demonstration only. If you're running this on a large cohort, running; serially is not the most effective approach. ```; N_SHARDS=
",False,
Deployability,"is on a large cohort, running; serially is not the most effective approach. ```; N_SHARDS=$(nproc) # Or change to the number of cores you want to use; CAPTURE_BED=agilent_sureselect_human_all_exon_v5_b37_targets.bed; VERSION=1.6.1. declare -a trio=(HG002 HG003 HG004); for SAMPLE in ""${trio[@]}""; do; BAM=${SAMPLE}.bam. OUTPUT_VCF=${SAMPLE}.vcf.gz; OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time sudo docker run \; -v ""${DIR}"":""/data"" \; google/deepvariant:${VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/data/hs37d5.fa"" \; --reads=""/data/${BAM}"" \; --regions=""/data/${CAPTURE_BED}"" \; --output_vcf=""/data/${OUTPUT_VCF}"" \; --output_gvcf=""/data/${OUTPUT_GVCF}"" \; --num_shards=${N_SHARDS}; done; ```. Note: The BAM files should provide unique names for each sample in their `SM`; header tag, which is usually derived from a command-line flag to the read; aligner. If your BAM files don't have unique `SM` tags (and if it's not feasible; to adjust the alignment pipeline), add the `--sample_name=XYZ` flag to; `run_deepvariant` to override the sample name written into the gVCF file header. ## Merge the trio samples using GLnexus. ### Run GLnexus to merge 3 gVCFs. And then run GLnexus with this config:. ```; sudo docker pull quay.io/mlin/glnexus:v1.2.7. time sudo docker run \; -v ""${DIR}"":""/data"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWES \; --bed ""/data/${CAPTURE_BED}"" \; /data/HG004.g.vcf.gz /data/HG003.g.vcf.gz /data/HG002.g.vcf.gz \; | sudo docker run -i google/deepvariant:${VERSION} bcftools view - \; | sudo docker run -i google/deepvariant:${VERSION} bgzip -c \; > ${DIR}/deepvariant.cohort.vcf.gz; ```. When we ran on this WES trio, it took only about 13 seconds. For more details on; performance, see; [GLnexus performance guide](https://github.com/dnanexus-rnd/GLnexus/wiki/Performance). For a WGS cohort, we recommend using `--config DeepVariantWGS` instead of; `DeepVariantWES`. Another preset `DeepVariant_unfil",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:7455,pipeline,pipeline,7455,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: is on a large cohort, running; serially is not the most effective approach. ```; N_SHARDS=$(nproc) # Or change to the number of cores you want to use; CAPTURE_BED=agilent_sureselect_human_all_exon_v5_b37_targets.bed; VERSION=1.6.1. declare -a trio=(HG002 HG003 HG004); for SAMPLE in ""${trio[@]}""; do; BAM=${SAMPLE}.bam. OUTPUT_VCF=${SAMPLE}.vcf.gz; OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time sudo docker run \; -v ""${DIR}"":""/data"" \; google/deepvariant:${VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/data/hs37d5.fa"" \; --reads=""/data/${BAM}"" \; --regions=""/data/${CAPTURE_BED}"" \; --output_vcf=""/data/${OUTPUT_VCF}"" \; --output_gvcf=""/data/${OUTPUT_GVCF}"" \; --num_shards=${N_SHARDS}; done; ```. Note: The BAM files should provide unique names for each sample in their `SM`; header tag, which is usually derived from a command-line flag to the read; aligner. If your BAM files don't have unique `SM` tags (and if it's not feasible; to adjust the alignment pipeline), add the `--sample_name=XYZ` flag to; `run_deepvariant` to override the sample name written into the gVCF file header. ## Merge the trio samples using GLnexus. ### Run GLnexus to merge 3 gVCFs. And then run GLnexus with this config:. ```; sudo docker pull quay.io/mlin/glnexus:v1.2.7. time sudo docker run \; -v ""${DIR}"":""/data"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWES \; --bed ""/data/${CAPTURE_BED}"" \; /data/HG004.g.vcf.gz /data/HG003.g.vcf.gz /data/HG002.g.vcf.gz \; | sudo docker run -i google/deepvariant:${VERSION} bcftools view - \; | sudo docker run -i google/deepvariant:${VERSION} bgzip -c \; > ${DIR}/deepvariant.cohort.vcf.gz; ```. When we ran on this WES trio, it took only about 13 seconds. For more details on; performance, see; [GLnexus performance guide](https://github.com/dnanexus-rnd/GLnexus/wiki/Performance). For a WGS cohort, we recommend using `--config DeepVariantWGS` instead of; `DeepVariantWES`. Another preset `DeepVariant_unfil
",False,"The text contains complete, meaningful sentences in natural language. It discusses concepts such as running a large cohort study, merging samples with GLnexus, and performance considerations. The content is explanatory and descriptive without consisting primarily of code or logs."
Deployability,"lation rate for a merged; VCF. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:1293,release,release,1293,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: lation rate for a merged; VCF. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004
",False,
Deployability,"likely; > to be missed. All candidate windows across the genome are considered; > independently. De Bruijn graphs are constructed using multiple fixed k-mer; > sizes (from 20 to 75, inclusive, with increments of 5) out of the reference; > genome bases for the candidate window, as well as all overlapping reads. Edges; > are given a weight determined by how many times they are observed in the; > reads. We trim any edges with weight less than three, except that edges found; > in the reference are never trimmed. Candidate haplotypes are generated by; > traversing the assembly graphs and the top two most likely haplotypes are; > selected that best explain the read evidence. The likelihood function used to; > score haplotypes is a traditional pair HMM with fixed parameters that do not; > depend on base quality scores. This likelihood function assumes that each read; > is independent. Finally, each read is then realigned to its most likely; > haplotype. This procedure updates both the position and the CIGAR string for; > each read. Local realignment is not performed for long reads (PacBio, and other similar; technologies). The realigner step can optionally be switched off using; `--norealign_reads`. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. This can be done by passing the following parameters:; `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`. Note that this is meant for debugging and produces a bam file for every; candidate variant, which can result in millions of tiny bam files, so when using; this, narrow down the DeepVariant run using `--regions` to just the variants you; want to inspect more closely. ## How are `AD` and `DP` values calculated?. In order to efficiently perform variant calling, DeepVariant partitions the; genome into chunks (set by `--partition_size`), and will read in a max number of; reads into each partition (set by `--max_reads_per_partiti",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:11882,update,updates,11882,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['update'],['updates'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: likely; > to be missed. All candidate windows across the genome are considered; > independently. De Bruijn graphs are constructed using multiple fixed k-mer; > sizes (from 20 to 75, inclusive, with increments of 5) out of the reference; > genome bases for the candidate window, as well as all overlapping reads. Edges; > are given a weight determined by how many times they are observed in the; > reads. We trim any edges with weight less than three, except that edges found; > in the reference are never trimmed. Candidate haplotypes are generated by; > traversing the assembly graphs and the top two most likely haplotypes are; > selected that best explain the read evidence. The likelihood function used to; > score haplotypes is a traditional pair HMM with fixed parameters that do not; > depend on base quality scores. This likelihood function assumes that each read; > is independent. Finally, each read is then realigned to its most likely; > haplotype. This procedure updates both the position and the CIGAR string for; > each read. Local realignment is not performed for long reads (PacBio, and other similar; technologies). The realigner step can optionally be switched off using; `--norealign_reads`. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. This can be done by passing the following parameters:; `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`. Note that this is meant for debugging and produces a bam file for every; candidate variant, which can result in millions of tiny bam files, so when using; this, narrow down the DeepVariant run using `--regions` to just the variants you; want to inspect more closely. ## How are `AD` and `DP` values calculated?. In order to efficiently perform variant calling, DeepVariant partitions the; genome into chunks (set by `--partition_size`), and will read in a max number of; reads into each partition (set by `--max_reads_per_partiti
",False,"The text contains complete, meaningful sentences discussing the process of variant calling in DeepVariant. It includes explanations about how AD and DP values are calculated, genome partitioning, read processing, and realignment procedures. These are all part of the documentation that provides explanatory information for users."
Deployability,"mistry Simplex and Duplex reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. ## Benchmark HG003 chr20 output from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. TRUTH_VCF=""HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz""; TRUTH_BED=""HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${INPUT_DIR}/${REF}"" \; -o ""${OUTPUT_DIR}/hg003.ul.r104.ont.chr20.happy.output"" \; --engine=vcfeval \; --pass-only \; -l ""${RE",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md:4190,release,release,4190,docs/deepvariant-ont-r104-simplex-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: mistry Simplex and Duplex reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. ## Benchmark HG003 chr20 output from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. TRUTH_VCF=""HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz""; TRUTH_BED=""HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${INPUT_DIR}/${REF}"" \; -o ""${OUTPUT_DIR}/hg003.ul.r104.ont.chr20.happy.output"" \; --engine=vcfeval \; --pass-only \; -l ""${RE
",False,
Deployability,"nchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai; ```. ## Running DeepVariant. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can run DeepVariant with just one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. Here we specify `--regions chr20` to run on just chromosome 20, saving time so; you can run this case study within about half an hour (tested on 64 CPUs). ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ""HYBRID_PACBIO_ILLUMINA"" \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir; ```. By specifying `--mo",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:4245,pipeline,pipeline,4245,docs/deepvariant-hybrid-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: nchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai; ```. ## Running DeepVariant. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can run DeepVariant with just one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. Here we specify `--regions chr20` to run on just chromosome 20, saving time so; you can run this case study within about half an hour (tested on 64 CPUs). ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ""HYBRID_PACBIO_ILLUMINA"" \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir; ```. By specifying `--mo
",False,"The text contains command lines and code snippets, but it also includes explanatory paragraphs. The majority of the content is not purely code or logs, as there are complete sentences discussing the process and steps involved in running DeepVariant. Therefore, this text should be kept."
Deployability,"needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output_child.tfrecord.gz; call_variants_output_parent1.tfrecord.gz; call_variants_output_parent2.tfrecord.gz. gvcf_child.tfrecord-?????-of-?????.gz; gvcf_parent1.tfrecord-?????-of-?????.gz; gvcf_parent2.tfrecord-?????-of-?????.gz. make_examples_child.tfrecord-?????-of-?????.gz; make_examples_parent1.tfrecord-?????-of-?????.gz; make_examples_parent2.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Merge VCFs using GLnexus. At this step we take all 3 VCFs generated in the previous step and merge them; using GLnexus. ```bash; # bcftools and bgzip are now included in our docker images.; # You can also install them separately.; sudo docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bcftools view - \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bgzip -c > output/HG002_trio_merged.vcf.gz; ```. After completion of GLnexus command we should have a new merged VCF file in the; output directory. ```; HG002_trio_merged.vcf.gz; ```. ## Benchmark on chr20. ### Calculate mendelian violation rate. ```bash; sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; realtimegenomics/rtg-tools format \; -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". FILE=""reference/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/ind-id/pat-id/mat-id: 0=unknown; #sex: 1=male;",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:6691,install,install,6691,docs/deeptrio-wgs-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output_child.tfrecord.gz; call_variants_output_parent1.tfrecord.gz; call_variants_output_parent2.tfrecord.gz. gvcf_child.tfrecord-?????-of-?????.gz; gvcf_parent1.tfrecord-?????-of-?????.gz; gvcf_parent2.tfrecord-?????-of-?????.gz. make_examples_child.tfrecord-?????-of-?????.gz; make_examples_parent1.tfrecord-?????-of-?????.gz; make_examples_parent2.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Merge VCFs using GLnexus. At this step we take all 3 VCFs generated in the previous step and merge them; using GLnexus. ```bash; # bcftools and bgzip are now included in our docker images.; # You can also install them separately.; sudo docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bcftools view - \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bgzip -c > output/HG002_trio_merged.vcf.gz; ```. After completion of GLnexus command we should have a new merged VCF file in the; output directory. ```; HG002_trio_merged.vcf.gz; ```. ## Benchmark on chr20. ### Calculate mendelian violation rate. ```bash; sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; realtimegenomics/rtg-tools format \; -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". FILE=""reference/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/ind-id/pat-id/mat-id: 0=unknown; #sex: 1=male;
",False,
Deployability,"ogle Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""co",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:2877,install,installation,2877,docs/deepvariant-gcp-info.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md,1,['install'],['installation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ogle Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""co
",False,"The text contains complete sentences and explanatory content about using Google Cloud SDK tools for setup and managing Compute Engine instances. It includes instructions with examples of commands to run, which is useful human-readable information."
Deployability,"olation rate for a merged VCF. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:1224,release,release,1224,docs/deeptrio-wgs-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: olation rate for a merged VCF. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004
",False,"The text contains a mix of natural language sentences and code snippets, but the majority is descriptive and explanatory. The bash commands are part of the instructions for downloading data, which is a human-readable explanation of the process. There are no code snippets that would make it difficult to read or understand without additional context."
Deployability,"oogle.com/compute). You don't need to create; Compute Engine instances at this time, but simply visiting this page will; initialize your compute engine ""service account"" so that we can authorize; it. (As you progress in your use of Google Cloud Platform, you will likely find it; useful to create a [Cloud; Organization](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisi",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:2237,install,install,2237,docs/deepvariant-gcp-info.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md,2,['install'],"['install', 'installation']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: oogle.com/compute). You don't need to create; Compute Engine instances at this time, but simply visiting this page will; initialize your compute engine ""service account"" so that we can authorize; it. (As you progress in your use of Google Cloud Platform, you will likely find it; useful to create a [Cloud; Organization](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisi
",False,"The content includes a series of instructions and explanations about using the Google Cloud Platform. It uses natural language sentences, provides guidance on setting up tools, managing projects, and configuring services. These are all meaningful and human-readable content that would be useful for someone learning how to use GCP."
Deployability,"ootnote3"">(3)</a>: In v0.8, we used the; [Platinum Genomes Truthset](https://github.com/Illumina/PlatinumGenomes) to; create more training examples outside the GIAB confident regions. <a name=""vfootnote4"">(4)</a>: Previously, we split train/tune by leaving 3 WES; for tuning. Starting from this release, we leave out chr1 and chr20 from; training, and use chr1 for tuning. <a name=""vfootnote5"">(5)</a>: Starting from this version, we padded (100bps on; both sides) of the capture BED and used that for generating training examples.; We also added more `downsample_fraction`. <a name=""vfootnote6"">(6)</a>: (Before v1.0) PacBio is the only one we currently; uses HG002 in training and tuning. <a name=""vfootnote7"">(7)</a>: In v1.0, we train on HG002-HG004 for WGS as well,; but only using examples from the region of NIST truth confident region v4.2; subtracting v3.3.2. <a name=""vfootnote8"">(8)</a>: In v1.0, PacBio training data contains training; examples with haplotag sorted images and unsorted images. <a name=""vfootnote9"">(9)</a>: In v1.1, we exclude HG003 from training. And we; use all NIST truth confident regions for HG001-HG007 (except for HG003) for; training. We've always excluded chr20-22 from training. <a name=""vfootnote10"">(10)</a>: In v1.2, we include new PacBio training data; from Sequel II, Chemistry 2.2. <a name=""vfootnote11"">(11)</a>: Between v1.1 and v1.2, we fixed an issue where; make_examples can generate fewer class 0 (REF) training examples than before.; This is the reason for more training examples in v1.2 when number of samples; didn't increase. <a name=""vfootnote12"">(12)</a>: In v1.2, we created BAM files with 100bp reads; and 125bp reads by trimming to augment the training data. ## Training data:. See ""[An Extensive Sequence Dataset of Gold-Standard Samples for Benchmarking and Development](https://doi.org/10.1101/2020.12.11.422022)""; for a publicly available set of data we released. Data download information can; be found in the supplementary material.; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details-training-data.md:6742,release,released,6742,docs/deepvariant-details-training-data.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details-training-data.md,1,['release'],['released'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ootnote3"">(3)</a>: In v0.8, we used the; [Platinum Genomes Truthset](https://github.com/Illumina/PlatinumGenomes) to; create more training examples outside the GIAB confident regions. <a name=""vfootnote4"">(4)</a>: Previously, we split train/tune by leaving 3 WES; for tuning. Starting from this release, we leave out chr1 and chr20 from; training, and use chr1 for tuning. <a name=""vfootnote5"">(5)</a>: Starting from this version, we padded (100bps on; both sides) of the capture BED and used that for generating training examples.; We also added more `downsample_fraction`. <a name=""vfootnote6"">(6)</a>: (Before v1.0) PacBio is the only one we currently; uses HG002 in training and tuning. <a name=""vfootnote7"">(7)</a>: In v1.0, we train on HG002-HG004 for WGS as well,; but only using examples from the region of NIST truth confident region v4.2; subtracting v3.3.2. <a name=""vfootnote8"">(8)</a>: In v1.0, PacBio training data contains training; examples with haplotag sorted images and unsorted images. <a name=""vfootnote9"">(9)</a>: In v1.1, we exclude HG003 from training. And we; use all NIST truth confident regions for HG001-HG007 (except for HG003) for; training. We've always excluded chr20-22 from training. <a name=""vfootnote10"">(10)</a>: In v1.2, we include new PacBio training data; from Sequel II, Chemistry 2.2. <a name=""vfootnote11"">(11)</a>: Between v1.1 and v1.2, we fixed an issue where; make_examples can generate fewer class 0 (REF) training examples than before.; This is the reason for more training examples in v1.2 when number of samples; didn't increase. <a name=""vfootnote12"">(12)</a>: In v1.2, we created BAM files with 100bp reads; and 125bp reads by trimming to augment the training data. ## Training data:. See ""[An Extensive Sequence Dataset of Gold-Standard Samples for Benchmarking and Development](https://doi.org/10.1101/2020.12.11.422022)""; for a publicly available set of data we released. Data download information can; be found in the supplementary material.; 
",False,"The text contains several footnotes discussing updates and changes to training data over different versions, which are meaningful human-readable sentences explaining data usage and modifications."
Deployability,"pis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepTrio with one command. DeepTrio pipeline consists of 4 steps: `make_examples`, `call_variants`,; `postprocess_variants` and `GLnexus merge`. It is possible to run DeepTrio with; one command using the `run_deepvariant` script. GLnexus is run as a separate; command. ### Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}"". time sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads_child /input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam \; --reads_parent1 /input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam \; --reads_parent2 /input/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam \; --output_vcf_child /output/HG002.output.vcf.gz \; --output_vcf_parent1 /output/HG003.output",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:3908,pipeline,pipeline,3908,docs/deeptrio-wgs-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: pis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepTrio with one command. DeepTrio pipeline consists of 4 steps: `make_examples`, `call_variants`,; `postprocess_variants` and `GLnexus merge`. It is possible to run DeepTrio with; one command using the `run_deepvariant` script. GLnexus is run as a separate; command. ### Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}"". time sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads_child /input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam \; --reads_parent1 /input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam \; --reads_parent2 /input/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam \; --output_vcf_child /output/HG002.output.vcf.gz \; --output_vcf_parent1 /output/HG003.output
",True,"The text consists of command lines and code snippets used to run a pipeline. It includes shell commands, variable assignments, and script execution which are more programmatic in nature and should be filtered out."
Deployability,"reprocessing step. First, if you have run this step before, and want to rerun it, you might want to; consider cleaning up previous data first to avoid confusion:. ```bash; # (Optional) Clean up existing files.; gsutil -m rm -f ""${OUTPUT_BUCKET}/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; gsutil -m rm -f ""${OUTPUT_BUCKET}/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt""; gsutil rm -f ""${OUTPUT_BUCKET}/example_info.json""; ```. Here we provide examples for running on; [Cloud Dataflow Runner](https://beam.apache.org/documentation/runners/dataflow/); and also [DirectRunner](https://beam.apache.org/documentation/runners/direct/).; Beam can also use other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. Consult the instructions at https://beam.apache.org/get-started/quickstart-py/; if you run into any issues. Then, get the script that performs shuffling:. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. Next, we shuffle the data using DataflowRunner. Before that, please make sure; you enable Dataflow API for your project:; http://console.cloud.google.com/flows/enableapi?apiid=dataflow. To access `gs://` path, make sure you run this in your virtual environment:. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfre",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:9002,install,install,9002,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: reprocessing step. First, if you have run this step before, and want to rerun it, you might want to; consider cleaning up previous data first to avoid confusion:. ```bash; # (Optional) Clean up existing files.; gsutil -m rm -f ""${OUTPUT_BUCKET}/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; gsutil -m rm -f ""${OUTPUT_BUCKET}/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt""; gsutil rm -f ""${OUTPUT_BUCKET}/example_info.json""; ```. Here we provide examples for running on; [Cloud Dataflow Runner](https://beam.apache.org/documentation/runners/dataflow/); and also [DirectRunner](https://beam.apache.org/documentation/runners/direct/).; Beam can also use other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. Consult the instructions at https://beam.apache.org/get-started/quickstart-py/; if you run into any issues. Then, get the script that performs shuffling:. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. Next, we shuffle the data using DataflowRunner. Before that, please make sure; you enable Dataflow API for your project:; http://console.cloud.google.com/flows/enableapi?apiid=dataflow. To access `gs://` path, make sure you run this in your virtual environment:. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfre
",False,"The provided text contains detailed, meaningful sentences that explain how to set up and use Apache Beam for data processing, including steps for installing dependencies, running pipelines with DataflowRunner, shuffling data, and handling TensorFlow records. These are explanatory and human-readable instructions."
Deployability,"ry_run=true` to the command above, which will print out all the commands; but not execute them. This will generate 5 files and 1 directory in `${OUTPUT_DIR}`:. ```bash; ls -1 ${OUTPUT_DIR}; ```. outputting:. ```; intermediate_results_dir; output.g.vcf.gz; output.g.vcf.gz.tbi; output.vcf.gz; output.vcf.gz.tbi; output.visual_report.html; ```. The directory ""intermediate_results_dir"" exists because; `--intermediate_results_dir /output/intermediate_results_dir` is specified. This; directory contains the intermediate output of make_examples and call_variants; steps. For more information about `output.visual_report.html`, see the; [VCF stats report documentation](deepvariant-vcf-stats-report.md). ## Notes on GPU image. If you are using GPUs, you can pull the GPU version, and make sure you run with; `--gpus 1`. `call_variants` is the only step that uses the GPU, and can only use; one at a time. `make_examples` and `postprocess_variants` do not run on GPU. For an example to install GPU driver and docker, see [install_nvidia_docker.sh]. ```; sudo docker run --gpus 1 \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; ...; ```. ## Notes on Singularity. ### CPU version. ```; # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional.; --num_shards=1 \ *",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:5846,install,install,5846,docs/deepvariant-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ry_run=true` to the command above, which will print out all the commands; but not execute them. This will generate 5 files and 1 directory in `${OUTPUT_DIR}`:. ```bash; ls -1 ${OUTPUT_DIR}; ```. outputting:. ```; intermediate_results_dir; output.g.vcf.gz; output.g.vcf.gz.tbi; output.vcf.gz; output.vcf.gz.tbi; output.visual_report.html; ```. The directory ""intermediate_results_dir"" exists because; `--intermediate_results_dir /output/intermediate_results_dir` is specified. This; directory contains the intermediate output of make_examples and call_variants; steps. For more information about `output.visual_report.html`, see the; [VCF stats report documentation](deepvariant-vcf-stats-report.md). ## Notes on GPU image. If you are using GPUs, you can pull the GPU version, and make sure you run with; `--gpus 1`. `call_variants` is the only step that uses the GPU, and can only use; one at a time. `make_examples` and `postprocess_variants` do not run on GPU. For an example to install GPU driver and docker, see [install_nvidia_docker.sh]. ```; sudo docker run --gpus 1 \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; ...; ```. ## Notes on Singularity. ### CPU version. ```; # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional.; --num_shards=1 \ *
",False,
Deployability,"sampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" samtools; ```. and. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" bcftools; ```. You can read more about samtools and bcftools here: http://www.htslib.org/doc/. ## Commands for requesting machines used in case studies. We report runtime in our case studies documentation. In order to make sure the; results we report are reproducible without too much variation, we provide the; commands we used here to show you what kind of machines we ran the case studies; on. This is NOT the fastest or cheapest configuration. ### Command for a CPU-only machine on Google Cloud Platform. We used a 64-core (vCPU) machine with 240GiB of memory and no GPU, on the Google; Cloud Platform. Specifying the CPU platform also allows us to report the runtime; more consistently. ```shell; gcloud compute instances create ""${USER}-cpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-64"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. ### Command for a GPU machine on Google Cloud Platform. ```shell; gcloud compute instances create ""${USER}-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-p",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:11360,configurat,configuration,11360,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['configurat'],['configuration'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: sampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" samtools; ```. and. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" bcftools; ```. You can read more about samtools and bcftools here: http://www.htslib.org/doc/. ## Commands for requesting machines used in case studies. We report runtime in our case studies documentation. In order to make sure the; results we report are reproducible without too much variation, we provide the; commands we used here to show you what kind of machines we ran the case studies; on. This is NOT the fastest or cheapest configuration. ### Command for a CPU-only machine on Google Cloud Platform. We used a 64-core (vCPU) machine with 240GiB of memory and no GPU, on the Google; Cloud Platform. Specifying the CPU platform also allows us to report the runtime; more consistently. ```shell; gcloud compute instances create ""${USER}-cpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-64"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. ### Command for a GPU machine on Google Cloud Platform. ```shell; gcloud compute instances create ""${USER}-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-p
",False,"The text contains meaningful human-readable sentences discussing the setup and configuration of computational resources for case studies, including descriptions of commands used to create instances on Google Cloud Platform. The content is explanatory and in natural language."
Deployability,"sequencing data], in this; study we describe applying DeepVariant to a real exome sample using a single; machine. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation, `hap.py` will intersect this BED with the GIAB; confident regions. ```bash; HTTPDIR=https://sto",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md:1079,release,release,1079,docs/deepvariant-exome-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: sequencing data], in this; study we describe applying DeepVariant to a real exome sample using a single; machine. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation, `hap.py` will intersect this BED with the GIAB; confident regions. ```bash; HTTPDIR=https://sto
",False,"The text contains multiple bash commands for downloading and processing data, but it also includes sentences that describe the process and purpose of using specific tools like Docker and hap.py. While there are code snippets present, they are accompanied by explanatory text that explains their role in the study, making the overall content human-readable and meaningful."
Deployability,"sionFDA Truth Challenge V2](https://precision.fda.gov/challenges/10/results); for All Benchmark Regions for ONT, PacBio, and Multiple Technologies; categories, and 2016; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results); for best SNP Performance. DeepVariant maintains high accuracy across data; from different sequencing technologies, prep methods, and species. For; [lower coverage](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/),; using DeepVariant makes an especially great difference. See; [metrics](docs/metrics.md) for the latest accuracy numbers on each of the; sequencing types.; * **Flexibility** - Out-of-the-box use for; [PCR-positive](https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html); samples and; [low quality sequencing runs](https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/),; and easy adjustments for; [different sequencing technologies](https://google.github.io/deepvariant/posts/2019-01-14-highly-accurate-snp-and-indel-calling-on-pacbio-ccs-with-deepvariant/); and; [non-human species](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).; * **Ease of use** - No filtering is needed beyond setting your preferred; minimum quality threshold.; * **Cost effectiveness** - With a single non-preemptible n1-standard-16; machine on Google Cloud, it costs ~$11.8 to call a 30x whole genome and; ~$0.89 to call an exome. With preemptible pricing, the cost is $2.84 for a; 30x whole genome and $0.21 for whole exome (not considering preemption).; * **Speed** - See [metrics](docs/metrics.md) for the runtime of all supported; datatypes on a 64-core CPU-only machine</sup>. Multiple options for; acceleration exist.; * **Usage options** - DeepVariant can be run via Docker or binaries, using; both on",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:7768,pipeline,pipelines-on-noisy-wgs-data,7768,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['pipeline'],['pipelines-on-noisy-wgs-data'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: sionFDA Truth Challenge V2](https://precision.fda.gov/challenges/10/results); for All Benchmark Regions for ONT, PacBio, and Multiple Technologies; categories, and 2016; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results); for best SNP Performance. DeepVariant maintains high accuracy across data; from different sequencing technologies, prep methods, and species. For; [lower coverage](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/),; using DeepVariant makes an especially great difference. See; [metrics](docs/metrics.md) for the latest accuracy numbers on each of the; sequencing types.; * **Flexibility** - Out-of-the-box use for; [PCR-positive](https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html); samples and; [low quality sequencing runs](https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/),; and easy adjustments for; [different sequencing technologies](https://google.github.io/deepvariant/posts/2019-01-14-highly-accurate-snp-and-indel-calling-on-pacbio-ccs-with-deepvariant/); and; [non-human species](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).; * **Ease of use** - No filtering is needed beyond setting your preferred; minimum quality threshold.; * **Cost effectiveness** - With a single non-preemptible n1-standard-16; machine on Google Cloud, it costs ~$11.8 to call a 30x whole genome and; ~$0.89 to call an exome. With preemptible pricing, the cost is $2.84 for a; 30x whole genome and $0.21 for whole exome (not considering preemption).; * **Speed** - See [metrics](docs/metrics.md) for the runtime of all supported; datatypes on a 64-core CPU-only machine</sup>. Multiple options for; acceleration exist.; * **Usage options** - DeepVariant can be run via Docker or binaries, using; both on
",False,"The content consists of multiple sentences discussing the features and performance of DeepVariant. It includes technical details about accuracy, flexibility, ease of use, cost effectiveness, speed, and usage options. These are all meaningful, explanatory, and human-readable sentences that describe the capabilities of the tool."
Deployability,"taflow?project=YOUR_PROJECT. In order to have the best performance, you might need extra resources such as; machines or IPs within a region. That will not be in the scope of this case; study here. The output path can be found in the dataset_config file by:. ```bash; gsutil cat ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; ```. In the output, the `tfrecord_path` should be valid paths in gs://. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 44516; # class1: 173673; # class2: 124569; #; # --input_pattern_list=OUTPUT_BUCKET/training_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_BUCKET/training_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_GCS_BUCKET/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 342758; ```. We can shuffle the validation set locally using; [DirectRunner](https://beam.apache.org/documentation/runners/direct/). Adding; `--direct_num_workers=0` sets the number of threads/subprocess to the number of; cores of the machine where the pipeline is running. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_DIR}""/validation_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_DIR}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DirectRunner \; --direct_num_workers=0; ```. Here is the validation_set:. ```bash; cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt""; ```. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 5591; # class1: 31854; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:11775,pipeline,pipeline,11775,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['pipeline'],['pipeline'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: taflow?project=YOUR_PROJECT. In order to have the best performance, you might need extra resources such as; machines or IPs within a region. That will not be in the scope of this case; study here. The output path can be found in the dataset_config file by:. ```bash; gsutil cat ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; ```. In the output, the `tfrecord_path` should be valid paths in gs://. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 44516; # class1: 173673; # class2: 124569; #; # --input_pattern_list=OUTPUT_BUCKET/training_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_BUCKET/training_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_GCS_BUCKET/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 342758; ```. We can shuffle the validation set locally using; [DirectRunner](https://beam.apache.org/documentation/runners/direct/). Adding; `--direct_num_workers=0` sets the number of threads/subprocess to the number of; cores of the machine where the pipeline is running. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_DIR}""/validation_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_DIR}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DirectRunner \; --direct_num_workers=0; ```. Here is the validation_set:. ```bash; cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt""; ```. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 5591; # class1: 31854; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-
",False,"The text contains code snippets and commands, which are primarily logs or programmatic descriptions."
Deployability,"tall beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. Consult the instructions at https://beam.apache.org/get-started/quickstart-py/; if you run into any issues. Then, get the script that performs shuffling:. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. Next, we shuffle the data using DataflowRunner. Before that, please make sure; you enable Dataflow API for your project:; http://console.cloud.google.com/flows/enableapi?apiid=dataflow. To access `gs://` path, make sure you run this in your virtual environment:. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. Shuffle using Dataflow. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. Then, you should be able to see the run on:; https://console.cloud.google.com/dataflow?project=YOUR_PROJECT. In order to have the best performance, you might need extra resources such as; machines or IPs within a region. That will not be in the scope of this case; study here. The output path can be found in the d",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:9943,install,install,9943,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: tall beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. Consult the instructions at https://beam.apache.org/get-started/quickstart-py/; if you run into any issues. Then, get the script that performs shuffling:. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. Next, we shuffle the data using DataflowRunner. Before that, please make sure; you enable Dataflow API for your project:; http://console.cloud.google.com/flows/enableapi?apiid=dataflow. To access `gs://` path, make sure you run this in your virtual environment:. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. Shuffle using Dataflow. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. Then, you should be able to see the run on:; https://console.cloud.google.com/dataflow?project=YOUR_PROJECT. In order to have the best performance, you might need extra resources such as; machines or IPs within a region. That will not be in the scope of this case; study here. The output path can be found in the d
",False,"The text includes detailed step-by-step instructions for installing and setting up dependencies, executing scripts, and configuring data processing pipelines. While it involves code snippets and commands, it also contains explanatory text that discusses the purpose of each step and provides guidance for users. The content is not purely code or logs but includes enough descriptive material to be considered human-readable."
Deployability,"tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. This should create a subdirectory in the current directory containing the actual; data files:. ```bash; ls -1 ${INPUT_DIR}; ```. outputting:. ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ### Model location (optional). Starting from r0.8, we put the model files inside the released Docker images.; So there is no need to download model files anymore. If you want to find the; model files of all releases, you can find them in our bucket on the Google Cloud; Storage. You can view them in the browser:; https://console.cloud.google.com/storage/browser/deepvariant/models/DeepVariant. ## Run DeepVariant with one command. DeepVariant consists of 3 main binaries: `make_examples`, `call_variants`, and; `postprocess_variants`. To make it easier to run, we create one entrypoint that; can be directly run as a docker command. If you want to see the details, you can; read through [run_deepvariant.py]. ```bash; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; ```. You can run everything with the following command:. ```bash; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; -",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:3544,release,released,3544,docs/deepvariant-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md,1,['release'],['released'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. This should create a subdirectory in the current directory containing the actual; data files:. ```bash; ls -1 ${INPUT_DIR}; ```. outputting:. ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ### Model location (optional). Starting from r0.8, we put the model files inside the released Docker images.; So there is no need to download model files anymore. If you want to find the; model files of all releases, you can find them in our bucket on the Google Cloud; Storage. You can view them in the browser:; https://console.cloud.google.com/storage/browser/deepvariant/models/DeepVariant. ## Run DeepVariant with one command. DeepVariant consists of 3 main binaries: `make_examples`, `call_variants`, and; `postprocess_variants`. To make it easier to run, we create one entrypoint that; can be directly run as a docker command. If you want to see the details, you can; read through [run_deepvariant.py]. ```bash; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; ```. You can run everything with the following command:. ```bash; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; -
",False,"The text contains a series of commands and script snippets, but it also includes explanatory paragraphs that describe the purpose of the code and provide context. The lines starting with '## Model location' and others discuss how to find model files and run DeepVariant, which are meaningful sentences explaining the process. While there are command lines, they are accompanied by descriptive text making them non-code snippets. Additionally, the presence of human-readable explanations indicates that this content should not be filtered out."
Deployability,"ther/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG004_truth.bed; ```. (No need to install bcftools and other tools, because they are now installed in; the DeepVariant images.). ## Run DeepVariant on trio to get 3 single sample VCFs. First, install docker if you don't have it yet: `sudo apt-get -y install; docker.io`. With the example command below, it runs DeepVariant on the trio one by one. This; is for demonstration only. If you're running this on a large cohort, running; serially is not the most effective approach. ```; N_SHARDS=$(nproc) # Or change to the number of cores you want to use; CAPTURE_BED=agilent_sureselect_human_all_exon_v5_b37_targets.bed; VERSION=1.6.1. declare -a trio=(HG002 HG003 HG004); for SAMPLE in ""${trio[@]}""; do; BAM=${SAMPLE}.bam. OUTPUT_VCF=${SAMPLE}.vcf.gz; OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time sudo docker run \; -v ""${DIR}"":""/data"" \; google/deepvariant:${VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/data/hs37d5.fa"" \; --reads=""/data/${BAM}"" \; --regions=""/data/${CAPTURE_BED}"" \; --output_vcf=""/data/${OUTPUT_VCF}"" \; --output_gvcf=""/data/${OUT",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:6104,install,install,6104,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,2,['install'],"['install', 'installed']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ther/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG004_truth.bed; ```. (No need to install bcftools and other tools, because they are now installed in; the DeepVariant images.). ## Run DeepVariant on trio to get 3 single sample VCFs. First, install docker if you don't have it yet: `sudo apt-get -y install; docker.io`. With the example command below, it runs DeepVariant on the trio one by one. This; is for demonstration only. If you're running this on a large cohort, running; serially is not the most effective approach. ```; N_SHARDS=$(nproc) # Or change to the number of cores you want to use; CAPTURE_BED=agilent_sureselect_human_all_exon_v5_b37_targets.bed; VERSION=1.6.1. declare -a trio=(HG002 HG003 HG004); for SAMPLE in ""${trio[@]}""; do; BAM=${SAMPLE}.bam. OUTPUT_VCF=${SAMPLE}.vcf.gz; OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time sudo docker run \; -v ""${DIR}"":""/data"" \; google/deepvariant:${VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/data/hs37d5.fa"" \; --reads=""/data/${BAM}"" \; --regions=""/data/${CAPTURE_BED}"" \; --output_vcf=""/data/${OUTPUT_VCF}"" \; --output_gvcf=""/data/${OUT
",True,"The text consists primarily of command lines and code snippets, including syntax like semicolons and variable substitutions. It appears to be a script or programmatic instructions, which should be filtered out."
Deployability,"this case study we describe applying DeepVariant to PacBio HiFi reads to call; variants. We will call small variants from a publicly available whole genome; HiFi dataset from PacBio. Starting in v1.4.0, PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchm",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:1091,install,install,1091,docs/deepvariant-pacbio-model-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md,2,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: this case study we describe applying DeepVariant to PacBio HiFi reads to call; variants. We will call small variants from a publicly available whole genome; HiFi dataset from PacBio. Starting in v1.4.0, PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchm
",False,"The text contains detailed, step-by-step instructions for setting up an environment, downloading references, and performing data benchmarking using specific commands. While it includes some technical terms, the content is structured as explanatory prose that guides the user through a process. There are no code snippets or logs present, which would typically indicate elimination."
Deployability,"tputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output_child.tfrecord.gz; call_variants_output_parent1.tfrecord.gz; call_variants_output_parent2.tfrecord.gz. gvcf_child.tfrecord-?????-of-?????.gz; gvcf_parent1.tfrecord-?????-of-?????.gz; gvcf_parent2.tfrecord-?????-of-?????.gz. make_examples_child.tfrecord-?????-of-?????.gz; make_examples_parent1.tfrecord-?????-of-?????.gz; make_examples_parent2.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md) or; [DeepVariant PacBio case study](deepvariant-pacbio-model-case-study.md). ## Merge VCFs using GLnexus. At this step we take all 3 VCFs generated in the previous step and merge them; using GLnexus. ```bash; sudo docker pull quay.io/mlin/glnexus:v1.2.7. # bcftools and bgzip are now included in our docker images.; # You can also install them separately.; sudo docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bcftools view - \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bgzip -c > output/HG002_trio_merged.vcf.gz; ```. After completion of GLnexus command we should have a new merged VCF file in the; output directory. ```; HG002_trio_merged.vcf.gz; ```. ## Benchmark on chr20. ### Calculate Mendelian Violation rate. ```bash; sudo docker pull realtimegenomics/rtg-tools. sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; realtimegenomics/rtg-tools format \; -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". FILE=""reference/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/i",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:6918,install,install,6918,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: tputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output_child.tfrecord.gz; call_variants_output_parent1.tfrecord.gz; call_variants_output_parent2.tfrecord.gz. gvcf_child.tfrecord-?????-of-?????.gz; gvcf_parent1.tfrecord-?????-of-?????.gz; gvcf_parent2.tfrecord-?????-of-?????.gz. make_examples_child.tfrecord-?????-of-?????.gz; make_examples_parent1.tfrecord-?????-of-?????.gz; make_examples_parent2.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md) or; [DeepVariant PacBio case study](deepvariant-pacbio-model-case-study.md). ## Merge VCFs using GLnexus. At this step we take all 3 VCFs generated in the previous step and merge them; using GLnexus. ```bash; sudo docker pull quay.io/mlin/glnexus:v1.2.7. # bcftools and bgzip are now included in our docker images.; # You can also install them separately.; sudo docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bcftools view - \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bgzip -c > output/HG002_trio_merged.vcf.gz; ```. After completion of GLnexus command we should have a new merged VCF file in the; output directory. ```; HG002_trio_merged.vcf.gz; ```. ## Benchmark on chr20. ### Calculate Mendelian Violation rate. ```bash; sudo docker pull realtimegenomics/rtg-tools. sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; realtimegenomics/rtg-tools format \; -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". FILE=""reference/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/i
",True,"The text is a code snippet containing bash commands and file paths, which indicates program execution logs or API documentation. The presence of specific syntax like `sudo docker run` and file operations suggests it's related to system commands rather than human-readable explanations."
Deployability,"training logic, we shuffle examples globally using a preprocessing step. First, if you have run this step before, and want to rerun it, you might want to; consider cleaning up previous data first to avoid confusion:. ```bash; # (Optional) Clean up existing files.; gsutil -m rm -f ""${OUTPUT_BUCKET}/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; gsutil -m rm -f ""${OUTPUT_BUCKET}/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt""; gsutil rm -f ""${OUTPUT_BUCKET}/example_info.json""; ```. Here we provide examples for running on; [Cloud Dataflow Runner](https://beam.apache.org/documentation/runners/dataflow/); and also [DirectRunner](https://beam.apache.org/documentation/runners/direct/).; Beam can also use other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. Consult the instructions at https://beam.apache.org/get-started/quickstart-py/; if you run into any issues. Then, get the script that performs shuffling:. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. Next, we shuffle the data using DataflowRunner. Before that, please make sure; you enable Dataflow API for your project:; http://console.cloud.google.com/flows/enableapi?apiid=dataflow. To access `gs://` path, make sure you run this in your virtual environment:. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 instal",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:8954,install,install,8954,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: training logic, we shuffle examples globally using a preprocessing step. First, if you have run this step before, and want to rerun it, you might want to; consider cleaning up previous data first to avoid confusion:. ```bash; # (Optional) Clean up existing files.; gsutil -m rm -f ""${OUTPUT_BUCKET}/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; gsutil -m rm -f ""${OUTPUT_BUCKET}/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt""; gsutil rm -f ""${OUTPUT_BUCKET}/example_info.json""; ```. Here we provide examples for running on; [Cloud Dataflow Runner](https://beam.apache.org/documentation/runners/dataflow/); and also [DirectRunner](https://beam.apache.org/documentation/runners/direct/).; Beam can also use other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. Consult the instructions at https://beam.apache.org/get-started/quickstart-py/; if you run into any issues. Then, get the script that performs shuffling:. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. Next, we shuffle the data using DataflowRunner. Before that, please make sure; you enable Dataflow API for your project:; http://console.cloud.google.com/flows/enableapi?apiid=dataflow. To access `gs://` path, make sure you run this in your virtual environment:. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 instal
",False,
Deployability,"training the hybrid model. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 503014 | 1487 | 2767 | 0.997053 | 0.994781 | 0.995916 |; | SNP | 3323624 | 3871 | 2273 | 0.998837 | 0.999317 | 0.999077 |. [See VCF stats report.](https://storage.googleapis.com/deepvariant/visual_reports/DeepVariant/1.6.1/HYBRID/deepvariant.output.visual_report.html). ## Inspect outputs that produced the metrics above. The DeepVariant VCFs, gVCFs, and hap.py evaluation outputs are available at:. ```; gs://deepvariant/case-study-outputs; ```. You can also inspect them in a web browser here:; https://42basepairs.com/browse/gs/deepvariant/case-study-outputs. ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; # Get the script.; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deepvariant.sh. # WGS; bash inference_deepvariant.sh --model_preset WGS. # WES; bash inference_deepvariant.sh --model_preset WES. # PacBio; bash inference_deepvariant.sh --model_preset PACBIO. # ONT_R104; bash inference_deepvariant.sh --model_preset ONT_R104. # Hybrid; bash inference_deepvariant.sh --model_preset HYBRID_PACBIO_ILLUMINA; ```. Runtime metrics are taken from the resulting log after each stage of; DeepVariant. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 runs produced the same output. [CPU instance with 64 CPUs]: deepvariant-details.md#command-for-a-cpu-only-mach",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md:4980,configurat,configuration,4980,docs/metrics.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md,1,['configurat'],['configuration'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: training the hybrid model. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 503014 | 1487 | 2767 | 0.997053 | 0.994781 | 0.995916 |; | SNP | 3323624 | 3871 | 2273 | 0.998837 | 0.999317 | 0.999077 |. [See VCF stats report.](https://storage.googleapis.com/deepvariant/visual_reports/DeepVariant/1.6.1/HYBRID/deepvariant.output.visual_report.html). ## Inspect outputs that produced the metrics above. The DeepVariant VCFs, gVCFs, and hap.py evaluation outputs are available at:. ```; gs://deepvariant/case-study-outputs; ```. You can also inspect them in a web browser here:; https://42basepairs.com/browse/gs/deepvariant/case-study-outputs. ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; # Get the script.; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deepvariant.sh. # WGS; bash inference_deepvariant.sh --model_preset WGS. # WES; bash inference_deepvariant.sh --model_preset WES. # PacBio; bash inference_deepvariant.sh --model_preset PACBIO. # ONT_R104; bash inference_deepvariant.sh --model_preset ONT_R104. # Hybrid; bash inference_deepvariant.sh --model_preset HYBRID_PACBIO_ILLUMINA; ```. Runtime metrics are taken from the resulting log after each stage of; DeepVariant. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 runs produced the same output. [CPU instance with 64 CPUs]: deepvariant-details.md#command-for-a-cpu-only-mach
",False,"The text contains detailed explanations and instructions, including steps to reproduce metrics, model presets, and descriptions of data outputs. It also includes links to resources for further information. This text is not primarily code or logs but rather a guide for users."
Deployability,"ts as well. See this workflow:. ![workflow](images/cohort-workflow.png?raw=true ""DeepVariant+GLnexus cohort workflow""). A few things to note before we start:. * It is recommended to use BAM files with original quality scores. In the case; that BAM files went through recalibration, optional DV flags can be used in; order to use original scores: `--parse_sam_aux_fields`,; `--use_original_quality_scores`.; * DeepVariant optionally allows gVCF output. This option is required for; further GLnexus analysis in this document. ## Dataset. The Whole Exome Sequencing (WES) dataset we're using is from:. [ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/). * HG002_NA24385_son; * HG003_NA24149_father; * HG004_NA24143_mother. ### Commands for downloading the input BAMs. Just for convenience, we use aria2 to download our data. You can change it to; whatever other tools (wget, curl) that you prefer. To install aria2, you can run: `sudo apt-get -y install aria2`. ```; DIR=""${PWD}/trio""; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam -o HG002.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai -o HG002.bai; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bam -o HG003.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:1983,install,install,1983,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,2,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ts as well. See this workflow:. ![workflow](images/cohort-workflow.png?raw=true ""DeepVariant+GLnexus cohort workflow""). A few things to note before we start:. * It is recommended to use BAM files with original quality scores. In the case; that BAM files went through recalibration, optional DV flags can be used in; order to use original scores: `--parse_sam_aux_fields`,; `--use_original_quality_scores`.; * DeepVariant optionally allows gVCF output. This option is required for; further GLnexus analysis in this document. ## Dataset. The Whole Exome Sequencing (WES) dataset we're using is from:. [ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/). * HG002_NA24385_son; * HG003_NA24149_father; * HG004_NA24143_mother. ### Commands for downloading the input BAMs. Just for convenience, we use aria2 to download our data. You can change it to; whatever other tools (wget, curl) that you prefer. To install aria2, you can run: `sudo apt-get -y install aria2`. ```; DIR=""${PWD}/trio""; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam -o HG002.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai -o HG002.bai; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bam -o HG003.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG
",True,"The text contains code snippets, specifically the use of aria2c commands and bash syntax which are typically associated with programming or script writing. Additionally, there are configuration details regarding file paths and downloading processes that fall under programmatic instructions rather than meaningful human-readable content."
Deployability,"use the way Docker works, input and output directories; have to be mounted and then files are referred to by their mounted location,; which can be confusing. To check that files are visible inside the Docker; container, you can `ls` inside the container. For example, using the setup shown; in the README and looking inside the `/input` volume:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; ls /input; ```. Mounting directories with Docker can be confusing. One trick to make this; simpler is to set both sides as your `$HOME`, so the paths are the same inside; and outside the Docker container. ```; echo $HOME # see what your home directory is first.; ls $HOME; BIN_VERSION=""1.6.1""; sudo docker run \; -v ""${HOME}"":""${HOME}"" \; google/deepvariant:""${BIN_VERSION}"" \; ls $HOME; ```. ## How do I run multi-sample calling?. Since the DeepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. We; have tested and confirmed that this does not affect GPU usage or inference. So; you can continue running DeepVariant without being worried about this message. ## How much GPU memory is needed for the Keras models?. 16GB. In our test, we observe the model occupying 16GB GPU memory. ## Do models from before r1.6.0 work",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:8486,release,release,8486,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['release'],['release'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: use the way Docker works, input and output directories; have to be mounted and then files are referred to by their mounted location,; which can be confusing. To check that files are visible inside the Docker; container, you can `ls` inside the container. For example, using the setup shown; in the README and looking inside the `/input` volume:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; ls /input; ```. Mounting directories with Docker can be confusing. One trick to make this; simpler is to set both sides as your `$HOME`, so the paths are the same inside; and outside the Docker container. ```; echo $HOME # see what your home directory is first.; ls $HOME; BIN_VERSION=""1.6.1""; sudo docker run \; -v ""${HOME}"":""${HOME}"" \; google/deepvariant:""${BIN_VERSION}"" \; ls $HOME; ```. ## How do I run multi-sample calling?. Since the DeepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. We; have tested and confirmed that this does not affect GPU usage or inference. So; you can continue running DeepVariant without being worried about this message. ## How much GPU memory is needed for the Keras models?. 16GB. In our test, we observe the model occupying 16GB GPU memory. ## Do models from before r1.6.0 work
",False,
Deployability,"vcf.gz; HG002.output.vcf.gz.tbi; HG002.output.visual_report.html; HG003.g.vcf.gz; HG003.g.vcf.gz.tbi; HG003.output.vcf.gz; HG003.output.vcf.gz.tbi; HG003.output.visual_report.html; HG004.g.vcf.gz; HG004.g.vcf.gz.tbi; HG004.output.vcf.gz; HG004.output.vcf.gz.tbi; HG004.output.visual_report.html; intermediate_results_dir; ```. The directory ""intermediate_results_dir"" exists because; `--intermediate_results_dir /output/intermediate_results_dir` is specified. This; directory contains the intermediate output of make_examples and call_variants; steps. For more information about the `HG00*.output.visual_report.html` files, see the; [VCF stats report documentation](deepvariant-vcf-stats-report.md). ## Notes on GPU image. If you are using GPUs, you can pull the GPU version, and make sure you run with; `--gpus 1`. `call_variants` is the only step that uses the GPU, and can only use; one at a time. `make_examples` and `postprocess_variants` do not run on GPU. For an example to install GPU driver and docker, see [install_nvidia_docker.sh]. ```; sudo docker run --gpus 1 \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; ...; ```. ## Notes on Singularity. ### CPU version. ```; # Pull the image.; singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Run DeepTrio.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type=WGS \; --ref=""${INPUT_DIR}""/GRCh38_no_alt_analysis_set.fasta \; --reads_child=""${INPUT_DIR}""/HG002.chr20.10_10p1mb.bam \; --reads_parent1=""${INPUT_DIR}""/HG003.chr20.10_10p1mb.bam \; --reads_parent2=""${INPUT_DIR}""/HG004.chr20.10_10p1mb.bam \; --output_vcf_child ""${OUTPUT_DIR}""/HG002.output.vcf.gz \; --output_vcf_parent1 ""${OUTPUT_DIR}""/HG003.output.vcf.gz \; --output_vcf_parent2 ""${OUTPUT_DIR}""/HG004.output.vcf.gz \; --sample_",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:8094,install,install,8094,docs/deeptrio-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md,1,['install'],['install'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: vcf.gz; HG002.output.vcf.gz.tbi; HG002.output.visual_report.html; HG003.g.vcf.gz; HG003.g.vcf.gz.tbi; HG003.output.vcf.gz; HG003.output.vcf.gz.tbi; HG003.output.visual_report.html; HG004.g.vcf.gz; HG004.g.vcf.gz.tbi; HG004.output.vcf.gz; HG004.output.vcf.gz.tbi; HG004.output.visual_report.html; intermediate_results_dir; ```. The directory ""intermediate_results_dir"" exists because; `--intermediate_results_dir /output/intermediate_results_dir` is specified. This; directory contains the intermediate output of make_examples and call_variants; steps. For more information about the `HG00*.output.visual_report.html` files, see the; [VCF stats report documentation](deepvariant-vcf-stats-report.md). ## Notes on GPU image. If you are using GPUs, you can pull the GPU version, and make sure you run with; `--gpus 1`. `call_variants` is the only step that uses the GPU, and can only use; one at a time. `make_examples` and `postprocess_variants` do not run on GPU. For an example to install GPU driver and docker, see [install_nvidia_docker.sh]. ```; sudo docker run --gpus 1 \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; ...; ```. ## Notes on Singularity. ### CPU version. ```; # Pull the image.; singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Run DeepTrio.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type=WGS \; --ref=""${INPUT_DIR}""/GRCh38_no_alt_analysis_set.fasta \; --reads_child=""${INPUT_DIR}""/HG002.chr20.10_10p1mb.bam \; --reads_parent1=""${INPUT_DIR}""/HG003.chr20.10_10p1mb.bam \; --reads_parent2=""${INPUT_DIR}""/HG004.chr20.10_10p1mb.bam \; --output_vcf_child ""${OUTPUT_DIR}""/HG002.output.vcf.gz \; --output_vcf_parent1 ""${OUTPUT_DIR}""/HG003.output.vcf.gz \; --output_vcf_parent2 ""${OUTPUT_DIR}""/HG004.output.vcf.gz \; --sample_
",False,"The text contains multiple paths and file names, which are technical details about the processing of files by a script or program. However, there is no clear sentence structure indicating explanation or discussion beyond these references to file operations. The content primarily lists filenames, commands, and directory structures, which are indicative of logs or API documentation."
Energy Efficiency," documentation](deepvariant-quick-start.md), it looks like this:; `${OUTPUT_DIR}/make_examples.tfrecord-00000-of-00001.gz`. ## Running show_examples. Once you have a make_examples output tfrecord file, then you can run; `show_examples` to see the pileup images inside:. ```bash; # Continuing from the quick start linked above:; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". BIN_VERSION=""1.6.1"" # show_examples is available only in version 1.0.0 and later.; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \; --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \; --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \; --output=/output/pileup \; --num_records=20 \; --curate. # And then your images are here:; ls ""${OUTPUT_DIR}""/pileup*.png; ```. ## Try it with these powerful optional parameters. * Filter to regions? Use e.g. `--regions chr20:1-3000000` or paths to BED or; BEDPE files.; * Filter to records from a VCF? Use `--vcf variants.vcf`. This can be a piece; of a VCF, e.g. grepping a hap.py output VCF for false positives. This is a; powerful way to pick out variants of interest and investigate them in more; depth.; * Stop after a certain number of examples, e.g. 10? Use `--num_records 10`.; * Sharded examples? Use for example, `--examples make_examples.tfrecord@64.gz`; to search through them all. This is best paired with `--regions` or `--vcf`; to narrow down to a small number of examples of interest. You can also use; the actual filename of a single make_examples file to only read that one, as; shown in the sample code above.; * Use `--curate` to create a TSV file with concepts for each pileup. Then; filter that TSV in any way you want and read that filtered TSV in using; `--filter_by_tsv` to e.g. get pileup images only for examples with low; ma",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md:2723,power,powerful,2723,docs/show-examples.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md,1,['power'],['powerful'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  documentation](deepvariant-quick-start.md), it looks like this:; `${OUTPUT_DIR}/make_examples.tfrecord-00000-of-00001.gz`. ## Running show_examples. Once you have a make_examples output tfrecord file, then you can run; `show_examples` to see the pileup images inside:. ```bash; # Continuing from the quick start linked above:; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". BIN_VERSION=""1.6.1"" # show_examples is available only in version 1.0.0 and later.; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \; --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \; --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \; --output=/output/pileup \; --num_records=20 \; --curate. # And then your images are here:; ls ""${OUTPUT_DIR}""/pileup*.png; ```. ## Try it with these powerful optional parameters. * Filter to regions? Use e.g. `--regions chr20:1-3000000` or paths to BED or; BEDPE files.; * Filter to records from a VCF? Use `--vcf variants.vcf`. This can be a piece; of a VCF, e.g. grepping a hap.py output VCF for false positives. This is a; powerful way to pick out variants of interest and investigate them in more; depth.; * Stop after a certain number of examples, e.g. 10? Use `--num_records 10`.; * Sharded examples? Use for example, `--examples make_examples.tfrecord@64.gz`; to search through them all. This is best paired with `--regions` or `--vcf`; to narrow down to a small number of examples of interest. You can also use; the actual filename of a single make_examples file to only read that one, as; shown in the sample code above.; * Use `--curate` to create a TSV file with concepts for each pileup. Then; filter that TSV in any way you want and read that filtered TSV in using; `--filter_by_tsv` to e.g. get pileup images only for examples with low; ma
",False,"The text contains complete, meaningful sentences in natural language explaining how to use the tool, including optional parameters and their usage. It also includes instructions for running commands and describes features like filtering by regions or VCF files."
Energy Efficiency," variant, which can result in millions of tiny bam files, so when using; this, narrow down the DeepVariant run using `--regions` to just the variants you; want to inspect more closely. ## How are `AD` and `DP` values calculated?. In order to efficiently perform variant calling, DeepVariant partitions the; genome into chunks (set by `--partition_size`), and will read in a max number of; reads into each partition (set by `--max_reads_per_partition`). By default,; `--partition_size` is set to 1000 and `--max_reads_per_partition` is set to; 1500. The `AD` and `DP` values are based on the read depths constrained by; `--max_reads_per_partition`. For example, if you have a depth of 2000x at a given site, DeepVariant will; subsample 1500 reads, and `DP` or `AD` will be capped at 1500. If you want to; calculate the true `AD` and `DP` values at high-depth regions, you can set; `--max_reads_per_partition=0` to calculate `AD` and `DP` using all reads. In; practice, capping reads per partition reduces runtimes with little/no impact on; accuracy. ## Missing variant calls near the edge of a contig. This is a known issue that we don't currently address. Please see:; https://github.com/google/deepvariant/issues/505 for more context. ## Why does DeepVariant PASS variants that have such a low read depth ~2 ?. Please see the answers provided by [Paul Grosu](https://github.com/pgrosu) in; this [issue thread](https://github.com/google/deepvariant/issues/684). We thank; Paul for providing a detailed description and reasoning. ## Singularity related questions:. ### `TMPDIR`. If you have issues with `TMPDIR` when running with Singularity, try adding this; to your command:. ```bash; export TMPDIR=""$PWD/tmp_dir""; ```. See https://github.com/google/deepvariant/issues/524#issuecomment-1067597987. ### Issues with `/mnt/`. User reported that sometimes their setup uses `/mnt/`, which exists in our; Docker image, and it has caused an issue in Singularity. You can use `-B` in Singularity to avoid thi",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:13455,reduce,reduces,13455,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['reduce'],['reduces'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  variant, which can result in millions of tiny bam files, so when using; this, narrow down the DeepVariant run using `--regions` to just the variants you; want to inspect more closely. ## How are `AD` and `DP` values calculated?. In order to efficiently perform variant calling, DeepVariant partitions the; genome into chunks (set by `--partition_size`), and will read in a max number of; reads into each partition (set by `--max_reads_per_partition`). By default,; `--partition_size` is set to 1000 and `--max_reads_per_partition` is set to; 1500. The `AD` and `DP` values are based on the read depths constrained by; `--max_reads_per_partition`. For example, if you have a depth of 2000x at a given site, DeepVariant will; subsample 1500 reads, and `DP` or `AD` will be capped at 1500. If you want to; calculate the true `AD` and `DP` values at high-depth regions, you can set; `--max_reads_per_partition=0` to calculate `AD` and `DP` using all reads. In; practice, capping reads per partition reduces runtimes with little/no impact on; accuracy. ## Missing variant calls near the edge of a contig. This is a known issue that we don't currently address. Please see:; https://github.com/google/deepvariant/issues/505 for more context. ## Why does DeepVariant PASS variants that have such a low read depth ~2 ?. Please see the answers provided by [Paul Grosu](https://github.com/pgrosu) in; this [issue thread](https://github.com/google/deepvariant/issues/684). We thank; Paul for providing a detailed description and reasoning. ## Singularity related questions:. ### `TMPDIR`. If you have issues with `TMPDIR` when running with Singularity, try adding this; to your command:. ```bash; export TMPDIR=""$PWD/tmp_dir""; ```. See https://github.com/google/deepvariant/issues/524#issuecomment-1067597987. ### Issues with `/mnt/`. User reported that sometimes their setup uses `/mnt/`, which exists in our; Docker image, and it has caused an issue in Singularity. You can use `-B` in Singularity to avoid thi
",True,"The text contains API documentation or specifications such as parameter lists, return types, and code snippets (e.g., bash commands, export statements), which are typically meant for programmatic reference rather than human-readable content. It also includes error messages, issue references, and configuration advice, which may be generated by the system rather than human writing."
Energy Efficiency," your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure; ```. NOTE: To create an instance *without GPU*, simply omit the last line from the; command. Check that the instance has been created and started:. ```shell; gcloud compute instances list; ```. which should produce output like:. ```; NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS; [USER]-deepvariant-quickstart us-west1-b n1-standard-8 10.138.0.4 35.185.203.59 RUNNING; ```. Then connect to your instance via SSH:. ```shell",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:3584,allocate,allocate,3584,docs/deepvariant-gcp-info.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md,1,['allocate'],['allocate'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure; ```. NOTE: To create an instance *without GPU*, simply omit the last line from the; command. Check that the instance has been created and started:. ```shell; gcloud compute instances list; ```. which should produce output like:. ```; NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS; [USER]-deepvariant-quickstart us-west1-b n1-standard-8 10.138.0.4 35.185.203.59 RUNNING; ```. Then connect to your instance via SSH:. ```shell
",True,"The text contains code snippets, commands, and programmatic instructions intended for executing actions on a Compute Engine (GCP). It includes shell commands with syntax highlighting, which indicates that it is meant for automated or script-based operations. The content is primarily procedural, detailing how to set up an instance with specific configurations, which aligns more with a setup guide rather than explanatory prose. Additionally, the presence of code blocks and command lines suggests this is not meant for human-readable purposes but for executing specific tasks in a system."
Energy Efficiency,"<img src=""docs/images/dv_logo.png"" width=50% height=50%>. [![release](https://img.shields.io/badge/release-v1.6.1-green?logo=github)](https://github.com/google/deepvariant/releases); [![announcements](https://img.shields.io/badge/announcements-blue)](https://groups.google.com/d/forum/deepvariant-announcements); [![blog](https://img.shields.io/badge/blog-orange)](https://goo.gl/deepvariant). DeepVariant is a deep learning-based variant caller that takes aligned reads (in; BAM or CRAM format), produces pileup image tensors from them, classifies each; tensor using a convolutional neural network, and finally reports the results in; a standard VCF or gVCF file. DeepVariant supports germline variant-calling in diploid organisms. * NGS (Illumina or Element) data for either a; [whole genome](docs/deepvariant-case-study.md) or; [whole exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and; [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The mod",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:114,green,green,114,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['green'],['green'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: <img src=""docs/images/dv_logo.png"" width=50% height=50%>. [![release](https://img.shields.io/badge/release-v1.6.1-green?logo=github)](https://github.com/google/deepvariant/releases); [![announcements](https://img.shields.io/badge/announcements-blue)](https://groups.google.com/d/forum/deepvariant-announcements); [![blog](https://img.shields.io/badge/blog-orange)](https://goo.gl/deepvariant). DeepVariant is a deep learning-based variant caller that takes aligned reads (in; BAM or CRAM format), produces pileup image tensors from them, classifies each; tensor using a convolutional neural network, and finally reports the results in; a standard VCF or gVCF file. DeepVariant supports germline variant-calling in diploid organisms. * NGS (Illumina or Element) data for either a; [whole genome](docs/deepvariant-case-study.md) or; [whole exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and; [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The mod
",False,"The text contains a mix of explanatory sentences about DeepVariant's capabilities and function. It discusses various input types (BAM/CRAM, PacBio, Oxford Nanopore), supported workflows (whole genome, exome, RNA-seq, hybrid, pangenome), and provides context on genotypes and data types it can process. There are also links to case studies which add additional explanatory information. While there are some technical terms, the overall content is descriptive and meant for human readers to understand how DeepVariant operates."
Energy Efficiency,"a multiple-GPU machine. ### postprocess_variants. `postprocess_variants` reads all of the output TFRecord files from; `call_variants`, sorts them, combines multi-allelic records, and writes out a; VCF file. When [gVCF output](deepvariant-gvcf-support.md) is requested, it also; outputs a gVCF file which merges the VCF with the non-variant sites. Because `postprocess_variants` combines and sorts the output of `call_variants`,; it needs to see all of the outputs from `call_variants` for a single sample to; merge into a final VCF. `postprocess_variants` is single-threaded and needs a; non-trivial amount of memory to run (20-30 GB), so it is best run on a; single/dual core machine with sufficient memory. ## Updates on DeepVariant since precisionFDA truth challenge and bioRxiv preprint. The DeepVariant team has been hard at work since we first presented the method.; Key changes and improvements include:. * Rearchitected with open source release in mind; * Built on [TensorFlow]; * Increased variant calling accuracy, especially for indels; * Vastly faster with reduced memory usage. We have made a number of improvements to the methodology as well. The biggest; change was to move away from RGB-encoded (3-channel) pileup images and instead; represent the aligned read data using a multi-channel tensor data layout. We; currently represent the data as a 6-channel raw tensor in which we encode:. * The read base (A, C, G, T); * The base's quality score; * The read's mapping quality score; * The read's strand (positive or negative); * Does the read support the allele being evaluated?; * Does the base match the reference genome at this position?. These are all readily derived from the information found in the BAM file; encoding of each read. Additional modeling changes were to move to the inception-v3 architecture and to; train on many more independent sequencing replicates of the ground truth; training samples, including 50% downsampled versions of each of those read sets.; In our te",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:6998,reduce,reduced,6998,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['reduce'],['reduced'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: a multiple-GPU machine. ### postprocess_variants. `postprocess_variants` reads all of the output TFRecord files from; `call_variants`, sorts them, combines multi-allelic records, and writes out a; VCF file. When [gVCF output](deepvariant-gvcf-support.md) is requested, it also; outputs a gVCF file which merges the VCF with the non-variant sites. Because `postprocess_variants` combines and sorts the output of `call_variants`,; it needs to see all of the outputs from `call_variants` for a single sample to; merge into a final VCF. `postprocess_variants` is single-threaded and needs a; non-trivial amount of memory to run (20-30 GB), so it is best run on a; single/dual core machine with sufficient memory. ## Updates on DeepVariant since precisionFDA truth challenge and bioRxiv preprint. The DeepVariant team has been hard at work since we first presented the method.; Key changes and improvements include:. * Rearchitected with open source release in mind; * Built on [TensorFlow]; * Increased variant calling accuracy, especially for indels; * Vastly faster with reduced memory usage. We have made a number of improvements to the methodology as well. The biggest; change was to move away from RGB-encoded (3-channel) pileup images and instead; represent the aligned read data using a multi-channel tensor data layout. We; currently represent the data as a 6-channel raw tensor in which we encode:. * The read base (A, C, G, T); * The base's quality score; * The read's mapping quality score; * The read's strand (positive or negative); * Does the read support the allele being evaluated?; * Does the base match the reference genome at this position?. These are all readily derived from the information found in the BAM file; encoding of each read. Additional modeling changes were to move to the inception-v3 architecture and to; train on many more independent sequencing replicates of the ground truth; training samples, including 50% downsampled versions of each of those read sets.; In our te
",False,
Energy Efficiency,"ange was to move away from RGB-encoded (3-channel) pileup images and instead; represent the aligned read data using a multi-channel tensor data layout. We; currently represent the data as a 6-channel raw tensor in which we encode:. * The read base (A, C, G, T); * The base's quality score; * The read's mapping quality score; * The read's strand (positive or negative); * Does the read support the allele being evaluated?; * Does the base match the reference genome at this position?. These are all readily derived from the information found in the BAM file; encoding of each read. Additional modeling changes were to move to the inception-v3 architecture and to; train on many more independent sequencing replicates of the ground truth; training samples, including 50% downsampled versions of each of those read sets.; In our testing this allowed the model to better generalize to other data types. In the end these changes reduced our error rate by more than 50% on the held out; evaluation sample (NA24385 / HG002) as compared to our results in the; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results/):. DeepVariant April 2016 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 4175 | 2839 | 0.987882 | 0.991728 | 0.989802; SNP | 1689 | 832 | 0.999447 | 0.999728 | 0.999587. DeepVariant December 2017 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 2384 | 1811 | 0.993081 | 0.994954 | 0.994017; SNP | 735 | 363 | 0.999759 | 0.999881 | 0.999820. See the [whole genome case study], which we update with each release of; DeepVariant, for the latest results. You can also see the [Colab example] to see how you can visualize the pileup; images. ## Training data over time. For the models we've released over time, you can find more details about the; training data in; [DeepVariant t",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:8026,reduce,reduced,8026,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['reduce'],['reduced'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ange was to move away from RGB-encoded (3-channel) pileup images and instead; represent the aligned read data using a multi-channel tensor data layout. We; currently represent the data as a 6-channel raw tensor in which we encode:. * The read base (A, C, G, T); * The base's quality score; * The read's mapping quality score; * The read's strand (positive or negative); * Does the read support the allele being evaluated?; * Does the base match the reference genome at this position?. These are all readily derived from the information found in the BAM file; encoding of each read. Additional modeling changes were to move to the inception-v3 architecture and to; train on many more independent sequencing replicates of the ground truth; training samples, including 50% downsampled versions of each of those read sets.; In our testing this allowed the model to better generalize to other data types. In the end these changes reduced our error rate by more than 50% on the held out; evaluation sample (NA24385 / HG002) as compared to our results in the; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results/):. DeepVariant April 2016 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 4175 | 2839 | 0.987882 | 0.991728 | 0.989802; SNP | 1689 | 832 | 0.999447 | 0.999728 | 0.999587. DeepVariant December 2017 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 2384 | 1811 | 0.993081 | 0.994954 | 0.994017; SNP | 735 | 363 | 0.999759 | 0.999881 | 0.999820. See the [whole genome case study], which we update with each release of; DeepVariant, for the latest results. You can also see the [Colab example] to see how you can visualize the pileup; images. ## Training data over time. For the models we've released over time, you can find more details about the; training data in; [DeepVariant t
",True,"The text contains code snippets and API documentation, such as parameter lists (read base, quality score, etc.), which are considered programmatic descriptions. Additionally, there are links to specific resources and references, but the main content is more aligned with technical details rather than human-readable prose."
Energy Efficiency,"ered false positives).; These should be bgzipped and tabix indexed and be on a reference consistent with; the one provided with the `--ref` argument. ### call_variants. `call_variants` consumes TFRecord file(s) of tf.Examples protos created; by `make_examples` and a deep learning model checkpoint and evaluates the model; on each example in the input TFRecord. The output here is a TFRecord of; CallVariantsOutput protos. `call_variants` doesn't directly support sharding its; outputs, but accepts a glob or shard-pattern for its inputs. `call_variants` uses around 4 GB per process and uses TensorFlow for evaluation.; When evaluating a model in CPU mode, TensorFlow can make use of multiple cores,; but scaling is sub-linear. In other words, `call_variants` on a 64 core machine; is less than 8x faster than running on a 8 core machine. When using a GPU, `call_variants` is both faster, more efficient, and needs; fewer CPUs. Based on a small number of experiments, currently the most efficient; configuration for `call_variants` on a GPU instance is 4-8 CPUs and 1 GPU.; Compared to our setting in the [whole genome case study], we noticed a 2.5x; speedup on the call_variants step using a single P100 GPU and 8 CPUs. Note that; currently `call_variants` can only use one GPU at most. So it doesn't improve; the speed if you get a multiple-GPU machine. ### postprocess_variants. `postprocess_variants` reads all of the output TFRecord files from; `call_variants`, sorts them, combines multi-allelic records, and writes out a; VCF file. When [gVCF output](deepvariant-gvcf-support.md) is requested, it also; outputs a gVCF file which merges the VCF with the non-variant sites. Because `postprocess_variants` combines and sorts the output of `call_variants`,; it needs to see all of the outputs from `call_variants` for a single sample to; merge into a final VCF. `postprocess_variants` is single-threaded and needs a; non-trivial amount of memory to run (20-30 GB), so it is best run on a; single/d",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:5584,efficient,efficient,5584,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['efficient'],['efficient'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ered false positives).; These should be bgzipped and tabix indexed and be on a reference consistent with; the one provided with the `--ref` argument. ### call_variants. `call_variants` consumes TFRecord file(s) of tf.Examples protos created; by `make_examples` and a deep learning model checkpoint and evaluates the model; on each example in the input TFRecord. The output here is a TFRecord of; CallVariantsOutput protos. `call_variants` doesn't directly support sharding its; outputs, but accepts a glob or shard-pattern for its inputs. `call_variants` uses around 4 GB per process and uses TensorFlow for evaluation.; When evaluating a model in CPU mode, TensorFlow can make use of multiple cores,; but scaling is sub-linear. In other words, `call_variants` on a 64 core machine; is less than 8x faster than running on a 8 core machine. When using a GPU, `call_variants` is both faster, more efficient, and needs; fewer CPUs. Based on a small number of experiments, currently the most efficient; configuration for `call_variants` on a GPU instance is 4-8 CPUs and 1 GPU.; Compared to our setting in the [whole genome case study], we noticed a 2.5x; speedup on the call_variants step using a single P100 GPU and 8 CPUs. Note that; currently `call_variants` can only use one GPU at most. So it doesn't improve; the speed if you get a multiple-GPU machine. ### postprocess_variants. `postprocess_variants` reads all of the output TFRecord files from; `call_variants`, sorts them, combines multi-allelic records, and writes out a; VCF file. When [gVCF output](deepvariant-gvcf-support.md) is requested, it also; outputs a gVCF file which merges the VCF with the non-variant sites. Because `postprocess_variants` combines and sorts the output of `call_variants`,; it needs to see all of the outputs from `call_variants` for a single sample to; merge into a final VCF. `postprocess_variants` is single-threaded and needs a; non-trivial amount of memory to run (20-30 GB), so it is best run on a; single/d
",False,"The text contains descriptive explanations about the tools 'call_variants' and 'postprocess_variants', including their functionalities, resource requirements, and performance considerations. These are complete sentences discussing concepts in natural language, making them meaningful for human readers."
Energy Efficiency,"is; important to first determine whether a candidate variant was proposed by; DeepVariant. A potential variant requires at least 2 reads to support a variant; and a minimum fraction of reads supporting the variant (0.12 for SNPs and PacBio; Indels, 0.06 for Illumina Indels). All sites that have been generated as; candidates are written in the VCF file, so if you do not see a row in the VCF; file for the variant in question, it means that a candidate was not made.; However, within these sites certain possible alleles may have been pruned in; reporting. To see all alleles, you may add: `--debug_output_all_candidates=ALT`; in the postprocess_variants step. To increase the sensitivity of DeepVariant to these sites, you may add the; following parameters, here shown with their defaults:. ```; --make_examples_extra_args=""vsc_min_count_snps=2,vsc_min_fraction_snps=0.12,vsc_min_count_indels=2,vsc_min_fraction_indels=0.06""; ```. It is sometimes also the case that realignment of the reads within DeepVariant; changes or reduces the evidence supporting the variant. To check for this, try; using the `--norealign_reads` flag to turn off realignment temporarily. Note; that we don't recommend turning off the realigner for Illumina data in general; cases because the realigner improves accuracy overall. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. See the ""What is the realigner and how does it work?"" section; for instructions. **Missing variants where a candidate is generated:**. If a candidate is made, but is called as reference (either 0/0 or ./.) it means; that the neural network processed the genomic region, but based on all of its; learned experience from training data, it decided the highest probability for; the position was as non-variant. Some of the reasons that DeepVariant may; suspect a false positive are: strand-bias in reads, low mapping quality in; reads, low base quality in reads, and overall low coverage. In additi",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:1355,reduce,reduces,1355,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['reduce'],['reduces'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: is; important to first determine whether a candidate variant was proposed by; DeepVariant. A potential variant requires at least 2 reads to support a variant; and a minimum fraction of reads supporting the variant (0.12 for SNPs and PacBio; Indels, 0.06 for Illumina Indels). All sites that have been generated as; candidates are written in the VCF file, so if you do not see a row in the VCF; file for the variant in question, it means that a candidate was not made.; However, within these sites certain possible alleles may have been pruned in; reporting. To see all alleles, you may add: `--debug_output_all_candidates=ALT`; in the postprocess_variants step. To increase the sensitivity of DeepVariant to these sites, you may add the; following parameters, here shown with their defaults:. ```; --make_examples_extra_args=""vsc_min_count_snps=2,vsc_min_fraction_snps=0.12,vsc_min_count_indels=2,vsc_min_fraction_indels=0.06""; ```. It is sometimes also the case that realignment of the reads within DeepVariant; changes or reduces the evidence supporting the variant. To check for this, try; using the `--norealign_reads` flag to turn off realignment temporarily. Note; that we don't recommend turning off the realigner for Illumina data in general; cases because the realigner improves accuracy overall. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. See the ""What is the realigner and how does it work?"" section; for instructions. **Missing variants where a candidate is generated:**. If a candidate is made, but is called as reference (either 0/0 or ./.) it means; that the neural network processed the genomic region, but based on all of its; learned experience from training data, it decided the highest probability for; the position was as non-variant. Some of the reasons that DeepVariant may; suspect a false positive are: strand-bias in reads, low mapping quality in; reads, low base quality in reads, and overall low coverage. In additi
",False,"The text contains complete sentences discussing concepts related to variant detection in the DeepVariant tool, including explanations of parameters and flags used in the tool. It provides information on how variants are identified, handled, and outputted, which is descriptive and meaningful for human readers."
Energy Efficiency,"nally, each read is then realigned to its most likely; > haplotype. This procedure updates both the position and the CIGAR string for; > each read. Local realignment is not performed for long reads (PacBio, and other similar; technologies). The realigner step can optionally be switched off using; `--norealign_reads`. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. This can be done by passing the following parameters:; `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`. Note that this is meant for debugging and produces a bam file for every; candidate variant, which can result in millions of tiny bam files, so when using; this, narrow down the DeepVariant run using `--regions` to just the variants you; want to inspect more closely. ## How are `AD` and `DP` values calculated?. In order to efficiently perform variant calling, DeepVariant partitions the; genome into chunks (set by `--partition_size`), and will read in a max number of; reads into each partition (set by `--max_reads_per_partition`). By default,; `--partition_size` is set to 1000 and `--max_reads_per_partition` is set to; 1500. The `AD` and `DP` values are based on the read depths constrained by; `--max_reads_per_partition`. For example, if you have a depth of 2000x at a given site, DeepVariant will; subsample 1500 reads, and `DP` or `AD` will be capped at 1500. If you want to; calculate the true `AD` and `DP` values at high-depth regions, you can set; `--max_reads_per_partition=0` to calculate `AD` and `DP` using all reads. In; practice, capping reads per partition reduces runtimes with little/no impact on; accuracy. ## Missing variant calls near the edge of a contig. This is a known issue that we don't currently address. Please see:; https://github.com/google/deepvariant/issues/505 for more context. ## Why does DeepVariant PASS variants that have such a low read depth ~2 ?. Please see the answers provid",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:12701,efficient,efficiently,12701,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['efficient'],['efficiently'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: nally, each read is then realigned to its most likely; > haplotype. This procedure updates both the position and the CIGAR string for; > each read. Local realignment is not performed for long reads (PacBio, and other similar; technologies). The realigner step can optionally be switched off using; `--norealign_reads`. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. This can be done by passing the following parameters:; `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`. Note that this is meant for debugging and produces a bam file for every; candidate variant, which can result in millions of tiny bam files, so when using; this, narrow down the DeepVariant run using `--regions` to just the variants you; want to inspect more closely. ## How are `AD` and `DP` values calculated?. In order to efficiently perform variant calling, DeepVariant partitions the; genome into chunks (set by `--partition_size`), and will read in a max number of; reads into each partition (set by `--max_reads_per_partition`). By default,; `--partition_size` is set to 1000 and `--max_reads_per_partition` is set to; 1500. The `AD` and `DP` values are based on the read depths constrained by; `--max_reads_per_partition`. For example, if you have a depth of 2000x at a given site, DeepVariant will; subsample 1500 reads, and `DP` or `AD` will be capped at 1500. If you want to; calculate the true `AD` and `DP` values at high-depth regions, you can set; `--max_reads_per_partition=0` to calculate `AD` and `DP` using all reads. In; practice, capping reads per partition reduces runtimes with little/no impact on; accuracy. ## Missing variant calls near the edge of a contig. This is a known issue that we don't currently address. Please see:; https://github.com/google/deepvariant/issues/505 for more context. ## Why does DeepVariant PASS variants that have such a low read depth ~2 ?. Please see the answers provid
",False,"The text contains complete sentences discussing concepts and processes related to variant calling in DeepVariant, such as realignment of reads, parameters for partitioning the genome, AD and DP value calculations, and handling missing variants near contig edges. These are explanatory and descriptive content meant for humans, not code or logs."
Energy Efficiency,"nd regions where we are confident in our calls (i.e., calls; within these regions and not in the truth_vcf are considered false positives).; These should be bgzipped and tabix indexed and be on a reference consistent with; the one provided with the `--ref` argument. ### call_variants. `call_variants` consumes TFRecord file(s) of tf.Examples protos created; by `make_examples` and a deep learning model checkpoint and evaluates the model; on each example in the input TFRecord. The output here is a TFRecord of; CallVariantsOutput protos. `call_variants` doesn't directly support sharding its; outputs, but accepts a glob or shard-pattern for its inputs. `call_variants` uses around 4 GB per process and uses TensorFlow for evaluation.; When evaluating a model in CPU mode, TensorFlow can make use of multiple cores,; but scaling is sub-linear. In other words, `call_variants` on a 64 core machine; is less than 8x faster than running on a 8 core machine. When using a GPU, `call_variants` is both faster, more efficient, and needs; fewer CPUs. Based on a small number of experiments, currently the most efficient; configuration for `call_variants` on a GPU instance is 4-8 CPUs and 1 GPU.; Compared to our setting in the [whole genome case study], we noticed a 2.5x; speedup on the call_variants step using a single P100 GPU and 8 CPUs. Note that; currently `call_variants` can only use one GPU at most. So it doesn't improve; the speed if you get a multiple-GPU machine. ### postprocess_variants. `postprocess_variants` reads all of the output TFRecord files from; `call_variants`, sorts them, combines multi-allelic records, and writes out a; VCF file. When [gVCF output](deepvariant-gvcf-support.md) is requested, it also; outputs a gVCF file which merges the VCF with the non-variant sites. Because `postprocess_variants` combines and sorts the output of `call_variants`,; it needs to see all of the outputs from `call_variants` for a single sample to; merge into a final VCF. `postprocess_varia",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:5491,efficient,efficient,5491,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['efficient'],['efficient'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: nd regions where we are confident in our calls (i.e., calls; within these regions and not in the truth_vcf are considered false positives).; These should be bgzipped and tabix indexed and be on a reference consistent with; the one provided with the `--ref` argument. ### call_variants. `call_variants` consumes TFRecord file(s) of tf.Examples protos created; by `make_examples` and a deep learning model checkpoint and evaluates the model; on each example in the input TFRecord. The output here is a TFRecord of; CallVariantsOutput protos. `call_variants` doesn't directly support sharding its; outputs, but accepts a glob or shard-pattern for its inputs. `call_variants` uses around 4 GB per process and uses TensorFlow for evaluation.; When evaluating a model in CPU mode, TensorFlow can make use of multiple cores,; but scaling is sub-linear. In other words, `call_variants` on a 64 core machine; is less than 8x faster than running on a 8 core machine. When using a GPU, `call_variants` is both faster, more efficient, and needs; fewer CPUs. Based on a small number of experiments, currently the most efficient; configuration for `call_variants` on a GPU instance is 4-8 CPUs and 1 GPU.; Compared to our setting in the [whole genome case study], we noticed a 2.5x; speedup on the call_variants step using a single P100 GPU and 8 CPUs. Note that; currently `call_variants` can only use one GPU at most. So it doesn't improve; the speed if you get a multiple-GPU machine. ### postprocess_variants. `postprocess_variants` reads all of the output TFRecord files from; `call_variants`, sorts them, combines multi-allelic records, and writes out a; VCF file. When [gVCF output](deepvariant-gvcf-support.md) is requested, it also; outputs a gVCF file which merges the VCF with the non-variant sites. Because `postprocess_variants` combines and sorts the output of `call_variants`,; it needs to see all of the outputs from `call_variants` for a single sample to; merge into a final VCF. `postprocess_varia
",True,"The text contains API documentation or specifications such as parameter lists, return types, and usage descriptions which are typical of programmatic content."
Energy Efficiency,"ng from the quick start linked above:; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". BIN_VERSION=""1.6.1"" # show_examples is available only in version 1.0.0 and later.; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \; --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \; --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \; --output=/output/pileup \; --num_records=20 \; --curate. # And then your images are here:; ls ""${OUTPUT_DIR}""/pileup*.png; ```. ## Try it with these powerful optional parameters. * Filter to regions? Use e.g. `--regions chr20:1-3000000` or paths to BED or; BEDPE files.; * Filter to records from a VCF? Use `--vcf variants.vcf`. This can be a piece; of a VCF, e.g. grepping a hap.py output VCF for false positives. This is a; powerful way to pick out variants of interest and investigate them in more; depth.; * Stop after a certain number of examples, e.g. 10? Use `--num_records 10`.; * Sharded examples? Use for example, `--examples make_examples.tfrecord@64.gz`; to search through them all. This is best paired with `--regions` or `--vcf`; to narrow down to a small number of examples of interest. You can also use; the actual filename of a single make_examples file to only read that one, as; shown in the sample code above.; * Use `--curate` to create a TSV file with concepts for each pileup. Then; filter that TSV in any way you want and read that filtered TSV in using; `--filter_by_tsv` to e.g. get pileup images only for examples with low; mapping quality, many errors, nearby variants, or any other concepts.; Filtering can be done any way you want, `grep` would be an easy option (the; TSV's header is not needed).; * Write out example tfrecords using `--write_tfrecords` after applying any; filtering using the options above.; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md:3000,power,powerful,3000,docs/show-examples.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md,1,['power'],['powerful'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ng from the quick start linked above:; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". BIN_VERSION=""1.6.1"" # show_examples is available only in version 1.0.0 and later.; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \; --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \; --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \; --output=/output/pileup \; --num_records=20 \; --curate. # And then your images are here:; ls ""${OUTPUT_DIR}""/pileup*.png; ```. ## Try it with these powerful optional parameters. * Filter to regions? Use e.g. `--regions chr20:1-3000000` or paths to BED or; BEDPE files.; * Filter to records from a VCF? Use `--vcf variants.vcf`. This can be a piece; of a VCF, e.g. grepping a hap.py output VCF for false positives. This is a; powerful way to pick out variants of interest and investigate them in more; depth.; * Stop after a certain number of examples, e.g. 10? Use `--num_records 10`.; * Sharded examples? Use for example, `--examples make_examples.tfrecord@64.gz`; to search through them all. This is best paired with `--regions` or `--vcf`; to narrow down to a small number of examples of interest. You can also use; the actual filename of a single make_examples file to only read that one, as; shown in the sample code above.; * Use `--curate` to create a TSV file with concepts for each pileup. Then; filter that TSV in any way you want and read that filtered TSV in using; `--filter_by_tsv` to e.g. get pileup images only for examples with low; mapping quality, many errors, nearby variants, or any other concepts.; Filtering can be done any way you want, `grep` would be an easy option (the; TSV's header is not needed).; * Write out example tfrecords using `--write_tfrecords` after applying any; filtering using the options above.; 
",False,"The text contains natural language sentences explaining how to use the tool, including optional parameters and examples of usage. It does not consist solely of code snippets or logs."
Energy Efficiency,"output_vcf=output/HG005.output.vcf.gz \; --num_shards=$(nproc) \; --regions=data/chr20_CDS_3x.bed \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir output/intermediate_results_dir; ```. **Flag summary**. * `--model_type` - Sets the model and options, but we will override the model; with `--customized model`.; * `--customized_model` - Points to a model trained using RNA-seq data.; * `--ref` - Specifies the reference sequence.; * `--reads` - Specifies the input bam file.; * `--output_vcf` - Specifies the output variant file.; * `--num_shards` - Sets the number of shards to the number of available; processors (`$(nproc)`). This is used to perform parallelization.; * `--regions` - Restricts analysis to 3x chr20 CDS regions only.; * `--make_examples_extra_args=` - Passes additional arguments to; make_examples.; * `split_skip_reads=true` - *Important!* This flag is critical for RNA-seq; variant calling to work properly. It enables RNA-seq data to be; processed efficiently.; * `channels=''` - Resets the channel list to be appropriate for the; RNA-seq model.; * `--intermediate_results_dir` - Outputs results to an intermediate directory. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; sudo docker run \; -v $(pwd):$(pwd) \; -w $(pwd) \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; output/HG005.output.vcf.gz \; -f benchmark/chr20_CDS_3x.benchmark_regions.bed \; -r reference/GRCh38_no_alt_analysis_set.fasta \; -o happy/happy.output \; --engine=vcfeval \; --pass-only \; --target-regions=data/chr20_CDS_3x.bed \; --threads=$(nproc); ```. **Flag summary**. * `-f` - Sets the benchmark regions (regions of interest that we want to; benchmark.); * `-r` - Sets the reference genome.; * `-o` - Specifies the output location.; * `--engine` - Sets the variant comparison engine. See; [hap",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:8546,efficient,efficiently,8546,docs/deepvariant-rnaseq-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md,1,['efficient'],['efficiently'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: output_vcf=output/HG005.output.vcf.gz \; --num_shards=$(nproc) \; --regions=data/chr20_CDS_3x.bed \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir output/intermediate_results_dir; ```. **Flag summary**. * `--model_type` - Sets the model and options, but we will override the model; with `--customized model`.; * `--customized_model` - Points to a model trained using RNA-seq data.; * `--ref` - Specifies the reference sequence.; * `--reads` - Specifies the input bam file.; * `--output_vcf` - Specifies the output variant file.; * `--num_shards` - Sets the number of shards to the number of available; processors (`$(nproc)`). This is used to perform parallelization.; * `--regions` - Restricts analysis to 3x chr20 CDS regions only.; * `--make_examples_extra_args=` - Passes additional arguments to; make_examples.; * `split_skip_reads=true` - *Important!* This flag is critical for RNA-seq; variant calling to work properly. It enables RNA-seq data to be; processed efficiently.; * `channels=''` - Resets the channel list to be appropriate for the; RNA-seq model.; * `--intermediate_results_dir` - Outputs results to an intermediate directory. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; sudo docker run \; -v $(pwd):$(pwd) \; -w $(pwd) \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; output/HG005.output.vcf.gz \; -f benchmark/chr20_CDS_3x.benchmark_regions.bed \; -r reference/GRCh38_no_alt_analysis_set.fasta \; -o happy/happy.output \; --engine=vcfeval \; --pass-only \; --target-regions=data/chr20_CDS_3x.bed \; --threads=$(nproc); ```. **Flag summary**. * `-f` - Sets the benchmark regions (regions of interest that we want to; benchmark.); * `-r` - Sets the reference genome.; * `-o` - Specifies the output location.; * `--engine` - Sets the variant comparison engine. See; [hap
",False,"The text contains complete sentences and explanations about command-line flags and their usage in a script, which is meaningful for human readers."
Energy Efficiency,"rshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The models included with DeepVariant are only trained on human data. For; other organisms, see the; [blog post on non-human variant-calling](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/); for some possible pitfalls and how to handle them. ## DeepTrio. DeepTrio is a deep learning-based trio variant caller built on top of; DeepVariant. DeepTrio extends DeepVariant's functionality, allowing it to; utilize the power of neural networks to predict genomic variants in trios or; duos. See [this page](docs/deeptrio-details.md) for more details and; instructions on how to run DeepTrio. DeepTrio supports germline variant-calling in diploid organisms for the; following types of input data:. * NGS (Illumina) data for either; [whole genome](docs/deeptrio-wgs-case-study.md) or whole exome.; * PacBio HiFi data, see the; [PacBio case study](docs/deeptrio-pacbio-case-study.md). Please also note:. * All DeepTrio models were trained on human data.; * It is possible to use DeepTrio with only 2 samples (child, and one parent).; * External tool [GLnexus](https://github.com/dnanexus-rnd/GLnexus) is used to; merge output VCFs. ## How to run DeepVariant. We recommend using our Docker solution. The command will look like this:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --mo",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:2491,power,power,2491,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['power'],['power'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: rshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The models included with DeepVariant are only trained on human data. For; other organisms, see the; [blog post on non-human variant-calling](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/); for some possible pitfalls and how to handle them. ## DeepTrio. DeepTrio is a deep learning-based trio variant caller built on top of; DeepVariant. DeepTrio extends DeepVariant's functionality, allowing it to; utilize the power of neural networks to predict genomic variants in trios or; duos. See [this page](docs/deeptrio-details.md) for more details and; instructions on how to run DeepTrio. DeepTrio supports germline variant-calling in diploid organisms for the; following types of input data:. * NGS (Illumina) data for either; [whole genome](docs/deeptrio-wgs-case-study.md) or whole exome.; * PacBio HiFi data, see the; [PacBio case study](docs/deeptrio-pacbio-case-study.md). Please also note:. * All DeepTrio models were trained on human data.; * It is possible to use DeepTrio with only 2 samples (child, and one parent).; * External tool [GLnexus](https://github.com/dnanexus-rnd/GLnexus) is used to; merge output VCFs. ## How to run DeepVariant. We recommend using our Docker solution. The command will look like this:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --mo
",False,
Integrability," They would create five resultant gVCF record values with `--gvcf_gq_binsize 5`,; with relevant values of:. ```bash; start | INFO | GQ; ------------------; 1 | END=3 | 8; 4 | END=4 | 27; 5 | END=7 | 47; 8 | END=8 | 45; 9 | END=9 | 33; ```. By synthetically downsampling a 50x coverage whole genome and applying different; GQ binning strategies, we see how the size of the resultant data varies as the; two factors change. The below figure shows the size of output (measured as the; number of records generated relative to the baseline of a 50x whole genome with; `--gvcf_gq_binsize 1`) at different coverage levels, for GQ bins of size 1, 3,; 5, and 10. The value of each bar is written in blue font above it for clarity. ![gVCF size](images/DeepVariant-gvcf-sizes-figure.png?raw=true ""DeepVariant gVCF sizes""). ### Runtime. Despite the creation of many additional records, the running time of; `make_examples` increases minimally when gVCF support is enabled. The; single-threaded `postprocess_variants` program is more adversely affected, with; observed runtimes increasing on the [WGS case study] from ~25 minutes to 5-7; hours depending on genome coverage. ### New option to include MED_DP. Starting in v1.2.0, we added a flag to enable adding MED_DP (median read; coverage seen in the block) in addition to the default MIN_DP (minimum read; coverage seen in the block). To test it, you can follow the steps in [Quick Start], and in the step where; you run the one-step script `/opt/deepvariant/bin/run_deepvariant`, add this; flag:. ```bash; --make_examples_extra_args=""include_med_dp=true""; ```. Then, if you look at your output gVCF, you'll see the additional MED_DP; information, like:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT NA12878; chr20 10000000 . T <*> 0 . END=10000116 GT:GQ:MIN_DP:MED_DP:PL 0/0:50:45:58:0,135,1349; ```. [VCF format]: https://samtools.github.io/hts-specs/VCFv4.3.pdf; [WGS case study]: deepvariant-case-study.md; [Quick Start]: deepvariant-quick-start.md; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:6445,depend,depending,6445,docs/deepvariant-gvcf-support.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md,1,['depend'],['depending'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  They would create five resultant gVCF record values with `--gvcf_gq_binsize 5`,; with relevant values of:. ```bash; start | INFO | GQ; ------------------; 1 | END=3 | 8; 4 | END=4 | 27; 5 | END=7 | 47; 8 | END=8 | 45; 9 | END=9 | 33; ```. By synthetically downsampling a 50x coverage whole genome and applying different; GQ binning strategies, we see how the size of the resultant data varies as the; two factors change. The below figure shows the size of output (measured as the; number of records generated relative to the baseline of a 50x whole genome with; `--gvcf_gq_binsize 1`) at different coverage levels, for GQ bins of size 1, 3,; 5, and 10. The value of each bar is written in blue font above it for clarity. ![gVCF size](images/DeepVariant-gvcf-sizes-figure.png?raw=true ""DeepVariant gVCF sizes""). ### Runtime. Despite the creation of many additional records, the running time of; `make_examples` increases minimally when gVCF support is enabled. The; single-threaded `postprocess_variants` program is more adversely affected, with; observed runtimes increasing on the [WGS case study] from ~25 minutes to 5-7; hours depending on genome coverage. ### New option to include MED_DP. Starting in v1.2.0, we added a flag to enable adding MED_DP (median read; coverage seen in the block) in addition to the default MIN_DP (minimum read; coverage seen in the block). To test it, you can follow the steps in [Quick Start], and in the step where; you run the one-step script `/opt/deepvariant/bin/run_deepvariant`, add this; flag:. ```bash; --make_examples_extra_args=""include_med_dp=true""; ```. Then, if you look at your output gVCF, you'll see the additional MED_DP; information, like:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT NA12878; chr20 10000000 . T <*> 0 . END=10000116 GT:GQ:MIN_DP:MED_DP:PL 0/0:50:45:58:0,135,1349; ```. [VCF format]: https://samtools.github.io/hts-specs/VCFv4.3.pdf; [WGS case study]: deepvariant-case-study.md; [Quick Start]: deepvariant-quick-start.md; 
",False,"The content contains complete, meaningful sentences in natural language discussing concepts and ideas related to the software functionality, such as adding a new option, testing it, and its effects. It also includes explanatory text about how the tool works and outputs. There are no code snippets, logs, or API documentation present."
Integrability," \; -v ""${HOME}"":""${HOME}"" \; google/deepvariant:""${BIN_VERSION}"" \; ls $HOME; ```. ## How do I run multi-sample calling?. Since the DeepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. We; have tested and confirmed that this does not affect GPU usage or inference. So; you can continue running DeepVariant without being worried about this message. ## How much GPU memory is needed for the Keras models?. 16GB. In our test, we observe the model occupying 16GB GPU memory. ## Do models from before r1.6.0 work with current inference code?. No. We have moved from Slim to Keras. All models before `1.6.0` were trained in; Slim platform. So they are not compatible with `1.6.0` anymore. ## Can call_variants be run on multiple GPUs?. No. Although possible, we have not implemented the multi-GPU capability in GPU; inference yet. ## Can model_train be run on multiple GPUs?. No. TensorFlow's Estimator API does provide support for running training on; multiple GPUs through the use of a DistributionStrategy. However,; DistributionStrategy cannot be used with exponential moving average (EMA), which; is present in the DeepVariant codebase. ## What is the realigner and how does it work?. From the; [DeepVariant 2018 manuscript](https://www.nature.com/articles/nbt.4235.epdf?author_access_token=q4ZmzqvvcGBqTuKy",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:9370,message,message,9370,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['message'],['message'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  \; -v ""${HOME}"":""${HOME}"" \; google/deepvariant:""${BIN_VERSION}"" \; ls $HOME; ```. ## How do I run multi-sample calling?. Since the DeepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. We; have tested and confirmed that this does not affect GPU usage or inference. So; you can continue running DeepVariant without being worried about this message. ## How much GPU memory is needed for the Keras models?. 16GB. In our test, we observe the model occupying 16GB GPU memory. ## Do models from before r1.6.0 work with current inference code?. No. We have moved from Slim to Keras. All models before `1.6.0` were trained in; Slim platform. So they are not compatible with `1.6.0` anymore. ## Can call_variants be run on multiple GPUs?. No. Although possible, we have not implemented the multi-GPU capability in GPU; inference yet. ## Can model_train be run on multiple GPUs?. No. TensorFlow's Estimator API does provide support for running training on; multiple GPUs through the use of a DistributionStrategy. However,; DistributionStrategy cannot be used with exponential moving average (EMA), which; is present in the DeepVariant codebase. ## What is the realigner and how does it work?. From the; [DeepVariant 2018 manuscript](https://www.nature.com/articles/nbt.4235.epdf?author_access_token=q4ZmzqvvcGBqTuKy
",False,
Integrability," handles the creation; of the final gVCF file that incorporates both the non-variant records and the; true variants discovered by the previous programs. Two additional flags are required in `postprocess_variants`, the input; `--nonvariant_site_tfrecord_path <filename>` which corresponds to the TFRecord; of Variant protocol buffers created in `make_examples`, and the output; `--gvcf_outfile <filename>` which is the final gVCF output. A concrete example call, using variables defined in the [WGS case study] and in; the above `make_examples` example:. ```bash; OUTPUT_GVCF=""${OUTPUT_DIR}/HG002.output.g.vcf.gz"". ( time python ""${BIN_DIR}""/postprocess_variants.zip \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${OUTPUT_VCF}"" \; --nonvariant_site_tfrecord_path ""${GVCF_TFRECORDS}"" \; --gvcf_outfile ""${OUTPUT_GVCF}""; ) >""${LOG_DIR}/postprocess_variants.log"" 2>&1; ```. ## Storage and runtime considerations. The number of non-variant records created when running DeepVariant in gVCF; depends highly on the sequencing depth of the input sample. This is because the; gVCF records at adjacent sites are merged when the genotype qualities are equal,; and we limit the possible genotype quality seen to be at most 50. For; deeply-sequenced individuals (e.g. 30-50x coverage), many sites hit the GQ=50; cap and are merged into few records. Samples with lower sequencing depth have; more sites within the dynamic range of the binomial model used to estimate; non-variant site genotype quality, and thus more records are created. To mitigate this effect, the `make_examples` program has a flag; `--gvcf_gq_binsize <int>`. This flag allows the merging of adjacent records that; all have GQ values within a bin of the given size, and for each record emits the; minimum GQ value seen within the bin. For example, setting `--gvcf_gq_binsize 5` has the effect that adjacent records; with GQ=0; GQ in [1, 5]; GQ in [6, 10]; GQ in [11, 15]; etc. are binned; together. A concrete example sho",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:4082,depend,depends,4082,docs/deepvariant-gvcf-support.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md,1,['depend'],['depends'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  handles the creation; of the final gVCF file that incorporates both the non-variant records and the; true variants discovered by the previous programs. Two additional flags are required in `postprocess_variants`, the input; `--nonvariant_site_tfrecord_path <filename>` which corresponds to the TFRecord; of Variant protocol buffers created in `make_examples`, and the output; `--gvcf_outfile <filename>` which is the final gVCF output. A concrete example call, using variables defined in the [WGS case study] and in; the above `make_examples` example:. ```bash; OUTPUT_GVCF=""${OUTPUT_DIR}/HG002.output.g.vcf.gz"". ( time python ""${BIN_DIR}""/postprocess_variants.zip \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${OUTPUT_VCF}"" \; --nonvariant_site_tfrecord_path ""${GVCF_TFRECORDS}"" \; --gvcf_outfile ""${OUTPUT_GVCF}""; ) >""${LOG_DIR}/postprocess_variants.log"" 2>&1; ```. ## Storage and runtime considerations. The number of non-variant records created when running DeepVariant in gVCF; depends highly on the sequencing depth of the input sample. This is because the; gVCF records at adjacent sites are merged when the genotype qualities are equal,; and we limit the possible genotype quality seen to be at most 50. For; deeply-sequenced individuals (e.g. 30-50x coverage), many sites hit the GQ=50; cap and are merged into few records. Samples with lower sequencing depth have; more sites within the dynamic range of the binomial model used to estimate; non-variant site genotype quality, and thus more records are created. To mitigate this effect, the `make_examples` program has a flag; `--gvcf_gq_binsize <int>`. This flag allows the merging of adjacent records that; all have GQ values within a bin of the given size, and for each record emits the; minimum GQ value seen within the bin. For example, setting `--gvcf_gq_binsize 5` has the effect that adjacent records; with GQ=0; GQ in [1, 5]; GQ in [6, 10]; GQ in [11, 15]; etc. are binned; together. A concrete example sho
",True,"The provided text contains code snippets and command lines, which fall under the category of program logs or API documentation."
Integrability,"# DeepVariant quick start. This is an explanation of how to use DeepVariant. ## Background. To get started, you'll need the DeepVariant programs (and some packages they; depend on), some test data, and of course a place to run them. We've provided a Docker image, and some test data in a bucket on Google Cloud; Storage. The instructions below show how to download the data through the; corresponding public URLs from these data. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepVariant in one command. Starting from the 0.8 release, we introduced one convenient command that will; run through all 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:170,depend,depend,170,docs/deepvariant-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md,1,['depend'],['depend'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # DeepVariant quick start. This is an explanation of how to use DeepVariant. ## Background. To get started, you'll need the DeepVariant programs (and some packages they; depend on), some test data, and of course a place to run them. We've provided a Docker image, and some test data in a bucket on Google Cloud; Storage. The instructions below show how to download the data through the; corresponding public URLs from these data. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepVariant in one command. Starting from the 0.8 release, we introduced one convenient command that will; run through all 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; 
",True,"The content is primarily programmatic instructions and code snippets, such as bash commands and Docker image pulls. There are no meaningful, complete sentences in natural language."
Integrability,"# Using DeepVariant for small variant calling from PacBio HiFi reads. #### Author: William Rowell <wrowell@pacificbiosciences.com>. In this case study we describe applying DeepVariant to PacBio HiFi reads to call; variants. We will call small variants from a publicly available whole genome; HiFi dataset from PacBio. Starting in v1.4.0, PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchma",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:747,depend,dependencies,747,docs/deepvariant-pacbio-model-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md,1,['depend'],['dependencies'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # Using DeepVariant for small variant calling from PacBio HiFi reads. #### Author: William Rowell <wrowell@pacificbiosciences.com>. In this case study we describe applying DeepVariant to PacBio HiFi reads to call; variants. We will call small variants from a publicly available whole genome; HiFi dataset from PacBio. Starting in v1.4.0, PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchma
",False,
Integrability,"## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md:466,depend,dependencies,466,docs/deepvariant-build-test.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md,4,"['depend', 'message']","['dependencies', 'message']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; 
",False,"The text is a set of step-by-step instructions on how to build and prepare DeepVariant on Ubuntu 20.04, including commands to install prerequisites and build scripts. These are clear, actionable steps that make up a human-readable guide for setting up the software. The content does not consist solely of code snippets or logs but rather explanatory text explaining the process and providing commands to execute."
Integrability,"OUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; ls /input; ```. Mounting directories with Docker can be confusing. One trick to make this; simpler is to set both sides as your `$HOME`, so the paths are the same inside; and outside the Docker container. ```; echo $HOME # see what your home directory is first.; ls $HOME; BIN_VERSION=""1.6.1""; sudo docker run \; -v ""${HOME}"":""${HOME}"" \; google/deepvariant:""${BIN_VERSION}"" \; ls $HOME; ```. ## How do I run multi-sample calling?. Since the DeepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. We; have tested and confirmed that this does not affect GPU usage or inference. So; you can continue running DeepVariant without being worried about this message. ## How much GPU memory is needed for the Keras models?. 16GB. In our test, we observe the model occupying 16GB GPU memory. ## Do models from before r1.6.0 work with current inference code?. No. We have moved from Slim to Keras. All models before `1.6.0` were trained in; Slim platform. So they are not compatible with `1.6.0` anymore. ## Can call_variants be run on multiple GPUs?. No. Although possible, we have not implemented the multi-GPU capability in GPU; inference yet. ## Can model_train be run on multiple GPUs?. No. TensorFlow's Estimator API does provide support for runn",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:8943,message,message,8943,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['message'],['message'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: OUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; ls /input; ```. Mounting directories with Docker can be confusing. One trick to make this; simpler is to set both sides as your `$HOME`, so the paths are the same inside; and outside the Docker container. ```; echo $HOME # see what your home directory is first.; ls $HOME; BIN_VERSION=""1.6.1""; sudo docker run \; -v ""${HOME}"":""${HOME}"" \; google/deepvariant:""${BIN_VERSION}"" \; ls $HOME; ```. ## How do I run multi-sample calling?. Since the DeepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. We; have tested and confirmed that this does not affect GPU usage or inference. So; you can continue running DeepVariant without being worried about this message. ## How much GPU memory is needed for the Keras models?. 16GB. In our test, we observe the model occupying 16GB GPU memory. ## Do models from before r1.6.0 work with current inference code?. No. We have moved from Slim to Keras. All models before `1.6.0` were trained in; Slim platform. So they are not compatible with `1.6.0` anymore. ## Can call_variants be run on multiple GPUs?. No. Although possible, we have not implemented the multi-GPU capability in GPU; inference yet. ## Can model_train be run on multiple GPUs?. No. TensorFlow's Estimator API does provide support for runn
",True,"The content contains API documentation or specifications such as parameter lists and return types, which are typically found in programmatic descriptions that should be filtered out."
Integrability,"Port.png?raw=true ""Preview on Port 8080""). Once it starts, you can see many metrics, including accuracy, speed, etc. You; will need to wait for `train` to run for a while before the plots will appear. ### Test the model. Now that we have performed training, we can test the performance of the new; model using our holdout dataset (chr20). The following one-step command can be used to call DeepVariant and run our newly; trained model:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ""${DOCKER_IMAGE}-gpu"" \; run_deepvariant \; --model_type WGS \; --customized_model ""${BEST_CHECKPOINT}"" \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/test_set.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. In v1.4.0, by using `--model_type WGS`, `run_deepvariant` will automatically add; `insert_size` as an extra channel in the `make_examples` step. So we don't need; to add it in `--make_examples_extra_args`. When the `call_variants` step is run, you might see messages like:. ```; E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. You can use `nvidia-smi` to confirm whether the GPUs are used. If so, you can; ignore the message. Once this is done, we have the final callset in VCF format here:; `${OUTPUT_DIR}/test_set.vcf.gz`. Next step is to run `hap.py` to complete the; evaluation on chromosome 20:. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. time sudo docker run -it \; -v ""${DATA_DIR}:${DATA_DIR}"" \; -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/test_set.vcf.gz"" \; -f ""${TRUTH_BED}"" \; -r ""${REF}"" \; -o ""${OUTPUT_DIR}/chr20-calling.happy.output"" \; -l chr20 \; --engine=vcfeval \; --pass-only; ```. The output of `hap.py` is here:. ```; [I] Total VCF records: 3775119; [I] Non-reference VCF records: 3775119; [W] overlapping records at c",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:17267,message,messages,17267,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['message'],['messages'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Port.png?raw=true ""Preview on Port 8080""). Once it starts, you can see many metrics, including accuracy, speed, etc. You; will need to wait for `train` to run for a while before the plots will appear. ### Test the model. Now that we have performed training, we can test the performance of the new; model using our holdout dataset (chr20). The following one-step command can be used to call DeepVariant and run our newly; trained model:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ""${DOCKER_IMAGE}-gpu"" \; run_deepvariant \; --model_type WGS \; --customized_model ""${BEST_CHECKPOINT}"" \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/test_set.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. In v1.4.0, by using `--model_type WGS`, `run_deepvariant` will automatically add; `insert_size` as an extra channel in the `make_examples` step. So we don't need; to add it in `--make_examples_extra_args`. When the `call_variants` step is run, you might see messages like:. ```; E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. You can use `nvidia-smi` to confirm whether the GPUs are used. If so, you can; ignore the message. Once this is done, we have the final callset in VCF format here:; `${OUTPUT_DIR}/test_set.vcf.gz`. Next step is to run `hap.py` to complete the; evaluation on chromosome 20:. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. time sudo docker run -it \; -v ""${DATA_DIR}:${DATA_DIR}"" \; -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/test_set.vcf.gz"" \; -f ""${TRUTH_BED}"" \; -r ""${REF}"" \; -o ""${OUTPUT_DIR}/chr20-calling.happy.output"" \; -l chr20 \; --engine=vcfeval \; --pass-only; ```. The output of `hap.py` is here:. ```; [I] Total VCF records: 3775119; [I] Non-reference VCF records: 3775119; [W] overlapping records at c
",True,"The content primarily consists of code snippets, command lines, and logs. It includes bash commands with syntax highlighting, code blocks for model training and testing, error messages from the system, and output from scripts like hap.py. Additionally, it contains configuration steps such as mounting directories and setting up Docker images. This text is more technical in nature and does not contain meaningful human-readable sentences that would be understandable without prior knowledge of the specific tools and processes involved."
Integrability,"all, using variables defined in the [WGS case study]:. ```bash; GVCF_TFRECORDS=""${OUTPUT_DIR}/HG002.gvcf.tfrecord@${N_SHARDS}.gz"". ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`; ```. NOTE: gVCF outputs are only valid when `make_examples` is run in ""calling"" mode;; if attempted to run in ""training"" mode the program will exit and notify the user; of the error. ### `postprocess_variants`. When run in gVCF mode, the `postprocess_variants` program handles the creation; of the final gVCF file that incorporates both the non-variant records and the; true variants discovered by the previous programs. Two additional flags are required in `postprocess_variants`, the input; `--nonvariant_site_tfrecord_path <filename>` which corresponds to the TFRecord; of Variant protocol buffers created in `make_examples`, and the output; `--gvcf_outfile <filename>` which is the final gVCF output. A concrete example call, using variables defined in the [WGS case study] and in; the above `make_examples` example:. ```bash; OUTPUT_GVCF=""${OUTPUT_DIR}/HG002.output.g.vcf.gz"". ( time python ""${BIN_DIR}""/postprocess_variants.zip \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${OUTPUT_VCF}"" \; --nonvariant_site_tfrecord_path ""${GVCF_TFRECORDS}"" \; --gvcf_outfile ""${OUTPUT_GVCF}""; ) >""${LOG_DIR}/postprocess_variants.log"" 2>&1; ```. ## Storage and runtime considerations. The number of non-variant records created when running DeepVariant in gVCF; depends highly on the sequencing depth of the input sample. This is because the; gVCF records at adjacent sites are merged when the genotype qualities are equal,; and we limit the possible genotype quality seen to be at most 50. For; deeply-sequenced individuals (e.g. 30-50x coverage)",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:3389,protocol,protocol,3389,docs/deepvariant-gvcf-support.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md,1,['protocol'],['protocol'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: all, using variables defined in the [WGS case study]:. ```bash; GVCF_TFRECORDS=""${OUTPUT_DIR}/HG002.gvcf.tfrecord@${N_SHARDS}.gz"". ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`; ```. NOTE: gVCF outputs are only valid when `make_examples` is run in ""calling"" mode;; if attempted to run in ""training"" mode the program will exit and notify the user; of the error. ### `postprocess_variants`. When run in gVCF mode, the `postprocess_variants` program handles the creation; of the final gVCF file that incorporates both the non-variant records and the; true variants discovered by the previous programs. Two additional flags are required in `postprocess_variants`, the input; `--nonvariant_site_tfrecord_path <filename>` which corresponds to the TFRecord; of Variant protocol buffers created in `make_examples`, and the output; `--gvcf_outfile <filename>` which is the final gVCF output. A concrete example call, using variables defined in the [WGS case study] and in; the above `make_examples` example:. ```bash; OUTPUT_GVCF=""${OUTPUT_DIR}/HG002.output.g.vcf.gz"". ( time python ""${BIN_DIR}""/postprocess_variants.zip \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${OUTPUT_VCF}"" \; --nonvariant_site_tfrecord_path ""${GVCF_TFRECORDS}"" \; --gvcf_outfile ""${OUTPUT_GVCF}""; ) >""${LOG_DIR}/postprocess_variants.log"" 2>&1; ```. ## Storage and runtime considerations. The number of non-variant records created when running DeepVariant in gVCF; depends highly on the sequencing depth of the input sample. This is because the; gVCF records at adjacent sites are merged when the genotype qualities are equal,; and we limit the possible genotype quality seen to be at most 50. For; deeply-sequenced individuals (e.g. 30-50x coverage)
",True,"The text contains code snippets from command lines and bash scripts, including variable assignments and pipeline configurations."
Integrability,"ance of the new; model using our holdout dataset (chr20). The following one-step command can be used to call DeepVariant and run our newly; trained model:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ""${DOCKER_IMAGE}-gpu"" \; run_deepvariant \; --model_type WGS \; --customized_model ""${BEST_CHECKPOINT}"" \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/test_set.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. In v1.4.0, by using `--model_type WGS`, `run_deepvariant` will automatically add; `insert_size` as an extra channel in the `make_examples` step. So we don't need; to add it in `--make_examples_extra_args`. When the `call_variants` step is run, you might see messages like:. ```; E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. You can use `nvidia-smi` to confirm whether the GPUs are used. If so, you can; ignore the message. Once this is done, we have the final callset in VCF format here:; `${OUTPUT_DIR}/test_set.vcf.gz`. Next step is to run `hap.py` to complete the; evaluation on chromosome 20:. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. time sudo docker run -it \; -v ""${DATA_DIR}:${DATA_DIR}"" \; -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/test_set.vcf.gz"" \; -f ""${TRUTH_BED}"" \; -r ""${REF}"" \; -o ""${OUTPUT_DIR}/chr20-calling.happy.output"" \; -l chr20 \; --engine=vcfeval \; --pass-only; ```. The output of `hap.py` is here:. ```; [I] Total VCF records: 3775119; [I] Non-reference VCF records: 3775119; [W] overlapping records at chr20:60402030 for sample 0; [W] Variants that overlap on the reference allele: 1; [I] Total VCF records: 132914; [I] Non-reference VCF records: 96273; 2023-10-14 20:09:55,773 WARNING Creating template for vcfeval. You can speed this up by supplying a SDF template that corre; spond",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:17539,message,message,17539,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['message'],['message'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ance of the new; model using our holdout dataset (chr20). The following one-step command can be used to call DeepVariant and run our newly; trained model:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ""${DOCKER_IMAGE}-gpu"" \; run_deepvariant \; --model_type WGS \; --customized_model ""${BEST_CHECKPOINT}"" \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/test_set.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. In v1.4.0, by using `--model_type WGS`, `run_deepvariant` will automatically add; `insert_size` as an extra channel in the `make_examples` step. So we don't need; to add it in `--make_examples_extra_args`. When the `call_variants` step is run, you might see messages like:. ```; E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. You can use `nvidia-smi` to confirm whether the GPUs are used. If so, you can; ignore the message. Once this is done, we have the final callset in VCF format here:; `${OUTPUT_DIR}/test_set.vcf.gz`. Next step is to run `hap.py` to complete the; evaluation on chromosome 20:. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. time sudo docker run -it \; -v ""${DATA_DIR}:${DATA_DIR}"" \; -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/test_set.vcf.gz"" \; -f ""${TRUTH_BED}"" \; -r ""${REF}"" \; -o ""${OUTPUT_DIR}/chr20-calling.happy.output"" \; -l chr20 \; --engine=vcfeval \; --pass-only; ```. The output of `hap.py` is here:. ```; [I] Total VCF records: 3775119; [I] Non-reference VCF records: 3775119; [W] overlapping records at chr20:60402030 for sample 0; [W] Variants that overlap on the reference allele: 1; [I] Total VCF records: 132914; [I] Non-reference VCF records: 96273; 2023-10-14 20:09:55,773 WARNING Creating template for vcfeval. You can speed this up by supplying a SDF template that corre; spond
",False,
Integrability,"contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot pipe them into DeepTrio. Duplicate marking may be; performed. In our analyses, there is almost no difference in accuracy with and; without duplicate marking except at lower (<20x) coverages. Finally, we; recommend that you do not perform BQSR. Running BQSR has a small decrease on; accuracy. If you are providing `--regions` or other similar arguments, these should refer; to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. ## Training data. DeepTrio models are trained using the latest publicly avavilable GIAB; benchmarks. You can find more details about the training data for each DeepTrio; model in the; [DeepTrio Training Data document](deeptrio-details-training-data.md). ## DeepVariant dependency. DeepTrio is built on top of DeepVariant and they share most of the components.; Please see [DeepVariant usage guide](deepvariant-details.md) for a full; description of DeepVariant components as well as other consideration for running; DeepVariant pipeline.; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:4763,depend,dependency,4763,docs/deeptrio-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md,1,['depend'],['dependency'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot pipe them into DeepTrio. Duplicate marking may be; performed. In our analyses, there is almost no difference in accuracy with and; without duplicate marking except at lower (<20x) coverages. Finally, we; recommend that you do not perform BQSR. Running BQSR has a small decrease on; accuracy. If you are providing `--regions` or other similar arguments, these should refer; to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. ## Training data. DeepTrio models are trained using the latest publicly avavilable GIAB; benchmarks. You can find more details about the training data for each DeepTrio; model in the; [DeepTrio Training Data document](deeptrio-details-training-data.md). ## DeepVariant dependency. DeepTrio is built on top of DeepVariant and they share most of the components.; Please see [DeepVariant usage guide](deepvariant-details.md) for a full; description of DeepVariant components as well as other consideration for running; DeepVariant pipeline.; 
",False,"The text contains complete, meaningful sentences in natural language. It includes explanatory content about the use and setup of DeepTrio, its dependencies, input assumptions, and training data."
Integrability,"cross the genome are; > selected for reassembly by looking for any evidence of possible genetic; > variation, such as mismatching or soft clipped bases. The selection criteria; > for a candidate window are very permissive so that true variation is unlikely; > to be missed. All candidate windows across the genome are considered; > independently. De Bruijn graphs are constructed using multiple fixed k-mer; > sizes (from 20 to 75, inclusive, with increments of 5) out of the reference; > genome bases for the candidate window, as well as all overlapping reads. Edges; > are given a weight determined by how many times they are observed in the; > reads. We trim any edges with weight less than three, except that edges found; > in the reference are never trimmed. Candidate haplotypes are generated by; > traversing the assembly graphs and the top two most likely haplotypes are; > selected that best explain the read evidence. The likelihood function used to; > score haplotypes is a traditional pair HMM with fixed parameters that do not; > depend on base quality scores. This likelihood function assumes that each read; > is independent. Finally, each read is then realigned to its most likely; > haplotype. This procedure updates both the position and the CIGAR string for; > each read. Local realignment is not performed for long reads (PacBio, and other similar; technologies). The realigner step can optionally be switched off using; `--norealign_reads`. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. This can be done by passing the following parameters:; `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`. Note that this is meant for debugging and produces a bam file for every; candidate variant, which can result in millions of tiny bam files, so when using; this, narrow down the DeepVariant run using `--regions` to just the variants you; want to inspect more closely. ## How are `",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:11699,depend,depend,11699,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['depend'],['depend'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: cross the genome are; > selected for reassembly by looking for any evidence of possible genetic; > variation, such as mismatching or soft clipped bases. The selection criteria; > for a candidate window are very permissive so that true variation is unlikely; > to be missed. All candidate windows across the genome are considered; > independently. De Bruijn graphs are constructed using multiple fixed k-mer; > sizes (from 20 to 75, inclusive, with increments of 5) out of the reference; > genome bases for the candidate window, as well as all overlapping reads. Edges; > are given a weight determined by how many times they are observed in the; > reads. We trim any edges with weight less than three, except that edges found; > in the reference are never trimmed. Candidate haplotypes are generated by; > traversing the assembly graphs and the top two most likely haplotypes are; > selected that best explain the read evidence. The likelihood function used to; > score haplotypes is a traditional pair HMM with fixed parameters that do not; > depend on base quality scores. This likelihood function assumes that each read; > is independent. Finally, each read is then realigned to its most likely; > haplotype. This procedure updates both the position and the CIGAR string for; > each read. Local realignment is not performed for long reads (PacBio, and other similar; technologies). The realigner step can optionally be switched off using; `--norealign_reads`. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. This can be done by passing the following parameters:; `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`. Note that this is meant for debugging and produces a bam file for every; candidate variant, which can result in millions of tiny bam files, so when using; this, narrow down the DeepVariant run using `--regions` to just the variants you; want to inspect more closely. ## How are `
",False,"The text contains complete, meaningful sentences in natural language discussing concepts and ideas related to genome analysis and computational biology methods."
Integrability,"d into a single record. Section 5.5 of the [VCF format] specification gives a description of the gVCF; format and example output, partially reproduced below. The gVCF output of; DeepVariant is syntactically and semantically equivalent to this example. ```bash; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT Sample; 1 4370 . G <*> . . END=4383 GT:GQ 0/0:37; 1 4384 . C <*> . . END=4388 GT:GQ 0/0:41; 1 4389 . T TC,<*> 50 . . GT:GQ 0/1:50; 1 4390 . C <*> . . END=4390 GT:GQ 0/0:3; ```. ## Creating gVCF output with DeepVariant. The exact same three programs (`make_examples`, `call_variants`, and; `postprocess_variants`) are used when creating gVCF output as in the [WGS case; study]. However, additional flags must be passed to the `make_examples` and; `postprocess_variants` steps. ### `make_examples`. The `make_examples` program is where the gVCF records are computed. One additional flag is required in `make_examples`, the `--gvcf <filename>`; flag. This specifies an additional output, which is a TFRecord file of Variant; protocol buffers. If running with multiple processes, the sharding applied to; this output filename must be the same as that applied to the `--examples`; output. A concrete example call, using variables defined in the [WGS case study]:. ```bash; GVCF_TFRECORDS=""${OUTPUT_DIR}/HG002.gvcf.tfrecord@${N_SHARDS}.gz"". ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`; ```. NOTE: gVCF outputs are only valid when `make_examples` is run in ""calling"" mode;; if attempted to run in ""training"" mode the program will exit and notify the user; of the error. ### `postprocess_variants`. When run in gVCF mode, the `postprocess_variants` program handles the creation; of the final gVCF file that incorporates both the non-varian",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:2185,protocol,protocol,2185,docs/deepvariant-gvcf-support.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md,1,['protocol'],['protocol'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: d into a single record. Section 5.5 of the [VCF format] specification gives a description of the gVCF; format and example output, partially reproduced below. The gVCF output of; DeepVariant is syntactically and semantically equivalent to this example. ```bash; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT Sample; 1 4370 . G <*> . . END=4383 GT:GQ 0/0:37; 1 4384 . C <*> . . END=4388 GT:GQ 0/0:41; 1 4389 . T TC,<*> 50 . . GT:GQ 0/1:50; 1 4390 . C <*> . . END=4390 GT:GQ 0/0:3; ```. ## Creating gVCF output with DeepVariant. The exact same three programs (`make_examples`, `call_variants`, and; `postprocess_variants`) are used when creating gVCF output as in the [WGS case; study]. However, additional flags must be passed to the `make_examples` and; `postprocess_variants` steps. ### `make_examples`. The `make_examples` program is where the gVCF records are computed. One additional flag is required in `make_examples`, the `--gvcf <filename>`; flag. This specifies an additional output, which is a TFRecord file of Variant; protocol buffers. If running with multiple processes, the sharding applied to; this output filename must be the same as that applied to the `--examples`; output. A concrete example call, using variables defined in the [WGS case study]:. ```bash; GVCF_TFRECORDS=""${OUTPUT_DIR}/HG002.gvcf.tfrecord@${N_SHARDS}.gz"". ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`; ```. NOTE: gVCF outputs are only valid when `make_examples` is run in ""calling"" mode;; if attempted to run in ""training"" mode the program will exit and notify the user; of the error. ### `postprocess_variants`. When run in gVCF mode, the `postprocess_variants` program handles the creation; of the final gVCF file that incorporates both the non-varian
",False,
Integrability,"dels, but be aware that training is already a balance between reducing; false negatives and positives, and it may not be possible to call variants like; the one you are seeing without increasing overall false positives by a greater; amount. ## How does DeepVariant use pileup images to call variants?. See this; [blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). ## What happens if I change the pileup_image_height?. If the actual depth in a particular region is greater than the pileup image; height, DeepVariant randomly downsamples reads until the image has been filled; up. For the default DeepVariant models (height 100), an image can accommodate at; most 95 reads in a given region (5 rows are reserved for the reference; sequence). You may be able to successfully run our pretrained models with a different; pileup image height (via `--pileup_image_height` in `make_examples.py`),; depending on the new height, but we generally do not recommend using different; image heights at training and inference time. If you wish to use a different; pileup image height, we recommend retraining a new model with images of that; height. If you are working with extremely high coverage sequencing data for applications; such as somatic sequencing, we recommend using a somatic caller instead of; DeepVariant, which is a germline caller. ## Can I use DeepVariant for somatic (non-germline) calling?. We do not recommend using DeepVariant for somatic calling. We do have a; prototype implementation for somatic calling, which can take a tumor and normal; BAM and call subclonal variants. However, we don't yet have enough confidence in; the available truth sets, and that they come from a diverse enough sampling of; cancers with mutational profiles, for us to be certain in releasing something of; high quality. We're watching developments in the area of these truth sets and; hope to be able to further develop the somatic caller in the future. ## Can I",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:4998,depend,depending,4998,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['depend'],['depending'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: dels, but be aware that training is already a balance between reducing; false negatives and positives, and it may not be possible to call variants like; the one you are seeing without increasing overall false positives by a greater; amount. ## How does DeepVariant use pileup images to call variants?. See this; [blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). ## What happens if I change the pileup_image_height?. If the actual depth in a particular region is greater than the pileup image; height, DeepVariant randomly downsamples reads until the image has been filled; up. For the default DeepVariant models (height 100), an image can accommodate at; most 95 reads in a given region (5 rows are reserved for the reference; sequence). You may be able to successfully run our pretrained models with a different; pileup image height (via `--pileup_image_height` in `make_examples.py`),; depending on the new height, but we generally do not recommend using different; image heights at training and inference time. If you wish to use a different; pileup image height, we recommend retraining a new model with images of that; height. If you are working with extremely high coverage sequencing data for applications; such as somatic sequencing, we recommend using a somatic caller instead of; DeepVariant, which is a germline caller. ## Can I use DeepVariant for somatic (non-germline) calling?. We do not recommend using DeepVariant for somatic calling. We do have a; prototype implementation for somatic calling, which can take a tumor and normal; BAM and call subclonal variants. However, we don't yet have enough confidence in; the available truth sets, and that they come from a diverse enough sampling of; cancers with mutational profiles, for us to be certain in releasing something of; high quality. We're watching developments in the area of these truth sets and; hope to be able to further develop the somatic caller in the future. ## Can I
",False,"The text contains complete, meaningful sentences discussing how DeepVariant uses pileup images, what happens when the pileup image height is changed, and whether it can be used for somatic calling. It includes explanations and recommendations, making it human-readable and meaningful."
Integrability,"h preemptible pricing, the cost is $2.84 for a; 30x whole genome and $0.21 for whole exome (not considering preemption).; * **Speed** - See [metrics](docs/metrics.md) for the runtime of all supported; datatypes on a 64-core CPU-only machine</sup>. Multiple options for; acceleration exist.; * **Usage options** - DeepVariant can be run via Docker or binaries, using; both on-premise hardware or in the cloud, with support for hardware; accelerators like GPUs and TPUs. <a name=""myfootnote1"">(1)</a>: Time estimates do not include mapping. ## How DeepVariant works. ![Stages in DeepVariant](docs/images/inference_flow_diagram.svg). For more information on the pileup images and how to read them, please see the; [""Looking through DeepVariant's Eyes"" blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant relies on [Nucleus](https://github.com/google/nucleus), a library of; Python and C++ code for reading and writing data in common genomics file formats; (like SAM and VCF) designed for painless integration with the; [TensorFlow](https://www.tensorflow.org/) machine learning framework. Nucleus; was built with DeepVariant in mind and open-sourced separately so it can be used; by anyone in the genomics research community for other projects. See this blog; post on; [Using Nucleus and TensorFlow for DNA Sequencing Error Correction](https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/). ## DeepVariant Setup. ### Prerequisites. * Unix-like operating system (cannot run on Windows); * Python 3.8. ### Official Solutions. Below are the official solutions provided by the; [Genomics team in Google Health](https://health.google/health-research/). Name | Description; :-------------------------------------------------------------------------------------------------: | -----------; [Docker](docs/deepvariant-quick-start.md) | This is the recommended method.; [Build from sou",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:9480,integrat,integration,9480,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['integrat'],['integration'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: h preemptible pricing, the cost is $2.84 for a; 30x whole genome and $0.21 for whole exome (not considering preemption).; * **Speed** - See [metrics](docs/metrics.md) for the runtime of all supported; datatypes on a 64-core CPU-only machine</sup>. Multiple options for; acceleration exist.; * **Usage options** - DeepVariant can be run via Docker or binaries, using; both on-premise hardware or in the cloud, with support for hardware; accelerators like GPUs and TPUs. <a name=""myfootnote1"">(1)</a>: Time estimates do not include mapping. ## How DeepVariant works. ![Stages in DeepVariant](docs/images/inference_flow_diagram.svg). For more information on the pileup images and how to read them, please see the; [""Looking through DeepVariant's Eyes"" blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant relies on [Nucleus](https://github.com/google/nucleus), a library of; Python and C++ code for reading and writing data in common genomics file formats; (like SAM and VCF) designed for painless integration with the; [TensorFlow](https://www.tensorflow.org/) machine learning framework. Nucleus; was built with DeepVariant in mind and open-sourced separately so it can be used; by anyone in the genomics research community for other projects. See this blog; post on; [Using Nucleus and TensorFlow for DNA Sequencing Error Correction](https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/). ## DeepVariant Setup. ### Prerequisites. * Unix-like operating system (cannot run on Windows); * Python 3.8. ### Official Solutions. Below are the official solutions provided by the; [Genomics team in Google Health](https://health.google/health-research/). Name | Description; :-------------------------------------------------------------------------------------------------: | -----------; [Docker](docs/deepvariant-quick-start.md) | This is the recommended method.; [Build from sou
",True,"The content consists primarily of program documentation and API specifications. It includes code references, parameter lists, setup instructions, and links to external documents, which are typical indicators that the text should be filtered out according to the evaluation criteria."
Integrability,"this case study we describe applying DeepVariant to PacBio HiFi reads to call; variants. We will call small variants from a publicly available whole genome; HiFi dataset from PacBio. Starting in v1.4.0, PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchm",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:1099,depend,dependencies,1099,docs/deepvariant-pacbio-model-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md,1,['depend'],['dependencies'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: this case study we describe applying DeepVariant to PacBio HiFi reads to call; variants. We will call small variants from a publicly available whole genome; HiFi dataset from PacBio. Starting in v1.4.0, PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchm
",True,"The text consists primarily of code snippets and command lines, which are typical of program logs or API documentation."
Modifiability," ------------- | ---------------- | --------------- |; | INDEL | 29 | 0 | 0 | 1.0 | 1.0 | 1.0 |; | SNP | 683 | 2 | 0 | 0.99708 | 1.0 | 0.998538 |. #### HG004:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 32 | 1 | 1 | 0.969697 | 0.969697 | 0.969697 |; | SNP | 677 | 2 | 0 | 0.997054 | 1.0 | 0.998525 |. * See VCF stats report (for all chromosomes); - [HG002](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG002.output.visual_report.html); - [HG003](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG003.output.visual_report.html); - [HG004](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG004.output.visual_report.html). ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); For bigger datasets (WGS and PACBIO), we used bigger disk size (900G).; This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deeptrio.sh. # WGS; bash inference_deeptrio.sh --model_preset WGS. # WES; bash inference_deeptrio.sh --model_preset WES. # PacBio; bash inference_deeptrio.sh --model_preset PACBIO. ```. Runtime metrics are taken from the resulting log after each stage of; DeepTrio. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 runs produced the same output. [CPU instance with 64 CPUs]: deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics-deeptrio.md:6121,config,configuration,6121,docs/metrics-deeptrio.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics-deeptrio.md,1,['config'],['configuration'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  ------------- | ---------------- | --------------- |; | INDEL | 29 | 0 | 0 | 1.0 | 1.0 | 1.0 |; | SNP | 683 | 2 | 0 | 0.99708 | 1.0 | 0.998538 |. #### HG004:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 32 | 1 | 1 | 0.969697 | 0.969697 | 0.969697 |; | SNP | 677 | 2 | 0 | 0.997054 | 1.0 | 0.998525 |. * See VCF stats report (for all chromosomes); - [HG002](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG002.output.visual_report.html); - [HG003](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG003.output.visual_report.html); - [HG004](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG004.output.visual_report.html). ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); For bigger datasets (WGS and PACBIO), we used bigger disk size (900G).; This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deeptrio.sh. # WGS; bash inference_deeptrio.sh --model_preset WGS. # WES; bash inference_deeptrio.sh --model_preset WES. # PacBio; bash inference_deeptrio.sh --model_preset PACBIO. ```. Runtime metrics are taken from the resulting log after each stage of; DeepTrio. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 runs produced the same output. [CPU instance with 64 CPUs]: deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform; 
",False,
Modifiability,"# Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data. DeepVariant is an analysis pipeline that uses a deep neural network to call; genetic variants from next-generation DNA sequencing (NGS) data. While; DeepVariant is highly accurate for; [many types of NGS data](https://rdcu.be/7Dhl), some users may be interested in; training custom deep learning models that have been optimized for very specific; data. This case study describes one way to train such a custom model using a GPU, in; this case for BGISEQ-500 data. Please note that there is not yet a production-grade training pipeline. This is; just one example of how to train a custom model, and is neither the fastest nor; the cheapest possible configuration. The resulting model also does not represent; the greatest achievable accuracy for BGISEQ-500 data. ## High level summary of result. We demonstrated that by training on 1 replicate of BGISEQ-500 whole genome data; (everything except for chromosome 20-22), we can significantly improve the; accuracy comparing to the WGS model as a baseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; followi",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:746,config,configuration,746,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['config'],['configuration'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data. DeepVariant is an analysis pipeline that uses a deep neural network to call; genetic variants from next-generation DNA sequencing (NGS) data. While; DeepVariant is highly accurate for; [many types of NGS data](https://rdcu.be/7Dhl), some users may be interested in; training custom deep learning models that have been optimized for very specific; data. This case study describes one way to train such a custom model using a GPU, in; this case for BGISEQ-500 data. Please note that there is not yet a production-grade training pipeline. This is; just one example of how to train a custom model, and is neither the fastest nor; the cheapest possible configuration. The resulting model also does not represent; the greatest achievable accuracy for BGISEQ-500 data. ## High level summary of result. We demonstrated that by training on 1 replicate of BGISEQ-500 whole genome data; (everything except for chromosome 20-22), we can significantly improve the; accuracy comparing to the WGS model as a baseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; followi
",False,"The content includes detailed explanatory text about training a custom variant caller for BGISEQ-500 data, including descriptions of the process, results, and instructions for requesting a GPU machine. It contains complete sentences in natural language discussing concepts and ideas related to genetic variant calling and deep learning model training."
Modifiability,"# Best practices for multi-sample variant calling with DeepVariant (WES trio demonstration). ## Overview. This document outlines all the steps and considerations for calling and merging; a trio using DeepVariant and [GLnexus](https://github.com/dnanexus-rnd/GLnexus).; These best practices were developed and evaluated as described in the article; published in _Bioinformatics_:; [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://doi.org/10.1093/bioinformatics/btaa1081); (2021). The process involves 3 major stages: running DeepVariant to create individual; genome call sets, running GLnexus to merge call sets, and analyzing the merged; call set. NOTE: This case study demonstrates an example of how to run DeepVariant; end-to-end on one machine. The steps below were done on a machine with this; [example command to start a machine](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). The steps in this document can be extended to merge larger cohorts as well. See this workflow:. ![workflow](images/cohort-workflow.png?raw=true ""DeepVariant+GLnexus cohort workflow""). A few things to note before we start:. * It is recommended to use BAM files with original quality scores. In the case; that BAM files went through recalibration, optional DV flags can be used in; order to use original scores: `--parse_sam_aux_fields`,; `--use_original_quality_scores`.; * DeepVariant optionally allows gVCF output. This option is required for; further GLnexus analysis in this document. ## Dataset. The Whole Exome Sequencing (WES) dataset we're using is from:. [ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/). * HG002_NA24385_son; * HG003_NA24149_father; * HG004_NA24143_mother. ### Commands for downloading the input BAMs. Just for convenience, we use aria2 to download our data. You can change it to; whatever other tools (wget, curl) that you prefer. To install aria2, you",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:981,extend,extended,981,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['extend'],['extended'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # Best practices for multi-sample variant calling with DeepVariant (WES trio demonstration). ## Overview. This document outlines all the steps and considerations for calling and merging; a trio using DeepVariant and [GLnexus](https://github.com/dnanexus-rnd/GLnexus).; These best practices were developed and evaluated as described in the article; published in _Bioinformatics_:; [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://doi.org/10.1093/bioinformatics/btaa1081); (2021). The process involves 3 major stages: running DeepVariant to create individual; genome call sets, running GLnexus to merge call sets, and analyzing the merged; call set. NOTE: This case study demonstrates an example of how to run DeepVariant; end-to-end on one machine. The steps below were done on a machine with this; [example command to start a machine](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). The steps in this document can be extended to merge larger cohorts as well. See this workflow:. ![workflow](images/cohort-workflow.png?raw=true ""DeepVariant+GLnexus cohort workflow""). A few things to note before we start:. * It is recommended to use BAM files with original quality scores. In the case; that BAM files went through recalibration, optional DV flags can be used in; order to use original scores: `--parse_sam_aux_fields`,; `--use_original_quality_scores`.; * DeepVariant optionally allows gVCF output. This option is required for; further GLnexus analysis in this document. ## Dataset. The Whole Exome Sequencing (WES) dataset we're using is from:. [ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/). * HG002_NA24385_son; * HG003_NA24149_father; * HG004_NA24143_mother. ### Commands for downloading the input BAMs. Just for convenience, we use aria2 to download our data. You can change it to; whatever other tools (wget, curl) that you prefer. To install aria2, you
",False,"The provided text contains detailed steps and explanations about using DeepVariant for multi-sample variant calling with a WES trio dataset. It includes commands for downloading data and notes on recommended practices. The content is in natural language discussing concepts, steps, and best practices without any code snippets or logs."
Modifiability,"# DeepTrio. ## Overview. DeepTrio is built on top of DeepVariant. It is intended for variant calling of; trios or duos. The main advantage of DeepTrio is that genetic inheritance is; considered by a neural network for calling variants in trio samples. Also,; variant candidates are generated from all samples at once, which ensures a; genotype call is made for any position in the trio with a variant. Since; DeepTrio is built on top of DeepVariant,; [general information](deepvariant-details.md) for DeepVariant also applies to; DeepTrio. At the highest level, a user needs to provide the following:. 1. A reference genome in [FASTA](https://en.wikipedia.org/wiki/FASTA_format); format and its corresponding; [.fai index file](http://www.htslib.org/doc/faidx.html) generated using the; `samtools faidx` command. 1. An aligned reads files for child and one or two parents in; [BAM](http://genome.sph.umich.edu/wiki/BAM) format and its corresponding; index file (.bai). The reads must be aligned to the reference genome; described above. The output of DeepTrio is a set of variants in; [VCF](https://samtools.github.io/hts-specs/VCFv4.3.pdf) format representing the; child and one or two parents. Similar to DeepVariant, DeepTrio is composed of three stages: `make_examples`,; `call_variants`, and `postprocess_variants`. Some of the components (; `call_variants`, `postprocess_variants`) are shared with DeepVariant, and; `make_examples` is specialized for DeepTrio. More details about each program are; described in detail in the; [Inputs and outputs](deepvariant-details.md#inputs-and-outputs) section of the; DeepVariant documentation. DeepTrio comes with three models for different types of input data:. * Illumina whole genome data (WGS).; * Illumina whole exome data (WES).; * PacBio HiFi whole genome data (PacBio WGS). ## Running DeepTrio. The easiest and recommended way to run DeepTrio is using; `google/deepvariant:deeptrio-latest` docker image. Please refer to the; [quick start guide](dee",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:167,inherit,inheritance,167,docs/deeptrio-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md,1,['inherit'],['inheritance'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # DeepTrio. ## Overview. DeepTrio is built on top of DeepVariant. It is intended for variant calling of; trios or duos. The main advantage of DeepTrio is that genetic inheritance is; considered by a neural network for calling variants in trio samples. Also,; variant candidates are generated from all samples at once, which ensures a; genotype call is made for any position in the trio with a variant. Since; DeepTrio is built on top of DeepVariant,; [general information](deepvariant-details.md) for DeepVariant also applies to; DeepTrio. At the highest level, a user needs to provide the following:. 1. A reference genome in [FASTA](https://en.wikipedia.org/wiki/FASTA_format); format and its corresponding; [.fai index file](http://www.htslib.org/doc/faidx.html) generated using the; `samtools faidx` command. 1. An aligned reads files for child and one or two parents in; [BAM](http://genome.sph.umich.edu/wiki/BAM) format and its corresponding; index file (.bai). The reads must be aligned to the reference genome; described above. The output of DeepTrio is a set of variants in; [VCF](https://samtools.github.io/hts-specs/VCFv4.3.pdf) format representing the; child and one or two parents. Similar to DeepVariant, DeepTrio is composed of three stages: `make_examples`,; `call_variants`, and `postprocess_variants`. Some of the components (; `call_variants`, `postprocess_variants`) are shared with DeepVariant, and; `make_examples` is specialized for DeepTrio. More details about each program are; described in detail in the; [Inputs and outputs](deepvariant-details.md#inputs-and-outputs) section of the; DeepVariant documentation. DeepTrio comes with three models for different types of input data:. * Illumina whole genome data (WGS).; * Illumina whole exome data (WES).; * PacBio HiFi whole genome data (PacBio WGS). ## Running DeepTrio. The easiest and recommended way to run DeepTrio is using; `google/deepvariant:deeptrio-latest` docker image. Please refer to the; [quick start guide](dee
",False,"The text consists of detailed explanations about the program, its features, usage instructions, and requirements. It includes descriptive content discussing the purpose of DeepTrio, its advantages, components, input requirements, output format, and running instructions using a Docker image. The sentences are complete and in natural language, providing explanatory information suitable for humans to understand how to use the tool effectively."
Modifiability,"# Using DeepVariant for small variant calling from PacBio HiFi reads. #### Author: William Rowell <wrowell@pacificbiosciences.com>. In this case study we describe applying DeepVariant to PacBio HiFi reads to call; variants. We will call small variants from a publicly available whole genome; HiFi dataset from PacBio. Starting in v1.4.0, PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchma",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:930,config,configuration,930,docs/deepvariant-pacbio-model-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md,4,['config'],"['config', 'configuration']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # Using DeepVariant for small variant calling from PacBio HiFi reads. #### Author: William Rowell <wrowell@pacificbiosciences.com>. In this case study we describe applying DeepVariant to PacBio HiFi reads to call; variants. We will call small variants from a publicly available whole genome; HiFi dataset from PacBio. Starting in v1.4.0, PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchma
",True,"The text is a set of detailed, step-by-step instructions for setting up and running DeepVariant with PacBio HiFi reads. It includes code snippets, such as bash commands, which are typically associated with programmatic execution rather than human-readable explanations. The presence of specific tool installations, configuration steps, and command lines indicates that the content is more aligned with a technical guide or script rather than general descriptive text."
Modifiability,"## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md:1555,config,configuring,1555,docs/deepvariant-build-test.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md,1,['config'],['configuring'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; 
",False,"The content includes detailed instructions on how to build and configure DeepVariant, including code snippets (e.g., shell commands) and setup steps. However, the presence of these commands might be more about automation than human-readable prose. Despite this, there are complete sentences discussing concepts like installation prerequisites and configuration options, which fit into explanatory or descriptive content. Thus, it should not be filtered out."
Modifiability,".init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cl",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14638,config,configured,14638,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,2,['config'],"['config', 'configured']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: .init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cl
",False,"The text contains several commands and code snippets, such as gsutil commands and configuration settings like --config.num_epochs=10. There are also mentions of training parameters and steps, which could be part of a programmatic setup rather than meaningful human-readable content."
Modifiability,"05.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/xy-case-study-testdata; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai. HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata; curl ${HTTPDIR}/GRCh38_PAR.bed > ${INPUT_DIR}/GRCh38_PAR.bed. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG002.pfda_challenge.grch38.chrXY.bam""; THREADS=$(nproc); REGION=""chrX chrY""; HAPLOID_CONTIGS=""chrX,chrY""; PAR_BED=""GRCh38_PAR.bed"". # Set up output variable; OUTPUT_VCF=""HG002_pacbio_hifi.chrXY.output.vcf.gz""; OUTPUT_GVCF=""HG002_pacbio_hifi.chrXY.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --haploid_contigs ""${HAPLOID_CONTIGS}"" \; --par_regions_bed ""${INPUT_DIR}/${PAR_BED}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Benchmark X, Y outputs from DeepVaria",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md:2268,variab,variable,2268,docs/deepvariant-xy-calling-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md,1,['variab'],['variable'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: 05.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/xy-case-study-testdata; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai. HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata; curl ${HTTPDIR}/GRCh38_PAR.bed > ${INPUT_DIR}/GRCh38_PAR.bed. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG002.pfda_challenge.grch38.chrXY.bam""; THREADS=$(nproc); REGION=""chrX chrY""; HAPLOID_CONTIGS=""chrX,chrY""; PAR_BED=""GRCh38_PAR.bed"". # Set up output variable; OUTPUT_VCF=""HG002_pacbio_hifi.chrXY.output.vcf.gz""; OUTPUT_GVCF=""HG002_pacbio_hifi.chrXY.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --haploid_contigs ""${HAPLOID_CONTIGS}"" \; --par_regions_bed ""${INPUT_DIR}/${PAR_BED}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Benchmark X, Y outputs from DeepVaria
",False,"The text provided contains a combination of commands and script snippets. However, it also includes sentences that describe the process of running DeepVariant and setting up inputs and outputs. The content is a mix of shell commands and explanatory text. While there are code elements present, they serve to explain the workflow rather than being pure code samples. Therefore, this text should not be filtered out but kept as it provides both operational instructions and context."
Modifiability,"1/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG002 Duplex chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam > ${INPUT_DIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam; curl ${HTTPDIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam.bai > ${INPUT_DIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam.bai. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam""; THREADS=$(nproc); REGION=""chr20"". # Set up output variable; OUTPUT_VCF=""HG002_R1041_Duplex_Dorado_v0.1.1_GRCh38.chr20.output.vcf.gz""; OUTPUT_GVCF=""HG002_R1041_Duplex_Dorado_v0.1.1_GRCh38.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ONT_R104 \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. By specifying `--model_type ONT_R104`, you'll be using a model that is best; suited for Oxford Nanopore R10.4.1 chemistry Simplex and Duplex reads. ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md:2430,variab,variable,2430,docs/deepvariant-ont-r104-duplex-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md,1,['variab'],['variable'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: 1/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG002 Duplex chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam > ${INPUT_DIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam; curl ${HTTPDIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam.bai > ${INPUT_DIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam.bai. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam""; THREADS=$(nproc); REGION=""chr20"". # Set up output variable; OUTPUT_VCF=""HG002_R1041_Duplex_Dorado_v0.1.1_GRCh38.chr20.output.vcf.gz""; OUTPUT_GVCF=""HG002_R1041_Duplex_Dorado_v0.1.1_GRCh38.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ONT_R104 \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. By specifying `--model_type ONT_R104`, you'll be using a model that is best; suited for Oxford Nanopore R10.4.1 chemistry Simplex and Duplex reads. 
",False,
Modifiability,"54; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:13454,config,config,13454,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['config'],['config'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: 54; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted
",False,"The text includes command lines and configuration details for training a model, which might be programmatic in nature. However, it also contains explanatory sentences about the process, such as when mentioning the need for a configuration file and steps to begin training. While there are code snippets present, the overall context is more about explaining how the training works rather than just code."
Modifiability,"???.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 G",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:13729,config,config,13729,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['config'],['config'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ???.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 G
",False,"The text contains complete sentences discussing the training process, configuration steps, and parameters, which are meaningful and not primarily logs or code snippets."
Modifiability,"IR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`; ```. NOTE: gVCF outputs are only valid when `make_examples` is run in ""calling"" mode;; if attempted to run in ""training"" mode the program will exit and notify the user; of the error. ### `postprocess_variants`. When run in gVCF mode, the `postprocess_variants` program handles the creation; of the final gVCF file that incorporates both the non-variant records and the; true variants discovered by the previous programs. Two additional flags are required in `postprocess_variants`, the input; `--nonvariant_site_tfrecord_path <filename>` which corresponds to the TFRecord; of Variant protocol buffers created in `make_examples`, and the output; `--gvcf_outfile <filename>` which is the final gVCF output. A concrete example call, using variables defined in the [WGS case study] and in; the above `make_examples` example:. ```bash; OUTPUT_GVCF=""${OUTPUT_DIR}/HG002.output.g.vcf.gz"". ( time python ""${BIN_DIR}""/postprocess_variants.zip \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${OUTPUT_VCF}"" \; --nonvariant_site_tfrecord_path ""${GVCF_TFRECORDS}"" \; --gvcf_outfile ""${OUTPUT_GVCF}""; ) >""${LOG_DIR}/postprocess_variants.log"" 2>&1; ```. ## Storage and runtime considerations. The number of non-variant records created when running DeepVariant in gVCF; depends highly on the sequencing depth of the input sample. This is because the; gVCF records at adjacent sites are merged when the genotype qualities are equal,; and we limit the possible genotype quality seen to be at most 50. For; deeply-sequenced individuals (e.g. 30-50x coverage), many sites hit the GQ=50; cap and are merged into few records. Samples with lower sequencing depth have; more sites within the dynamic range of the binomial model used to estimate; non-variant site g",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:3541,variab,variables,3541,docs/deepvariant-gvcf-support.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md,1,['variab'],['variables'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: IR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`; ```. NOTE: gVCF outputs are only valid when `make_examples` is run in ""calling"" mode;; if attempted to run in ""training"" mode the program will exit and notify the user; of the error. ### `postprocess_variants`. When run in gVCF mode, the `postprocess_variants` program handles the creation; of the final gVCF file that incorporates both the non-variant records and the; true variants discovered by the previous programs. Two additional flags are required in `postprocess_variants`, the input; `--nonvariant_site_tfrecord_path <filename>` which corresponds to the TFRecord; of Variant protocol buffers created in `make_examples`, and the output; `--gvcf_outfile <filename>` which is the final gVCF output. A concrete example call, using variables defined in the [WGS case study] and in; the above `make_examples` example:. ```bash; OUTPUT_GVCF=""${OUTPUT_DIR}/HG002.output.g.vcf.gz"". ( time python ""${BIN_DIR}""/postprocess_variants.zip \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${OUTPUT_VCF}"" \; --nonvariant_site_tfrecord_path ""${GVCF_TFRECORDS}"" \; --gvcf_outfile ""${OUTPUT_GVCF}""; ) >""${LOG_DIR}/postprocess_variants.log"" 2>&1; ```. ## Storage and runtime considerations. The number of non-variant records created when running DeepVariant in gVCF; depends highly on the sequencing depth of the input sample. This is because the; gVCF records at adjacent sites are merged when the genotype qualities are equal,; and we limit the possible genotype quality seen to be at most 50. For; deeply-sequenced individuals (e.g. 30-50x coverage), many sites hit the GQ=50; cap and are merged into few records. Samples with lower sequencing depth have; more sites within the dynamic range of the binomial model used to estimate; non-variant site g
",False,"The text contains complete, meaningful sentences discussing storage and runtime considerations in natural language, which are relevant for human understanding."
Modifiability,"OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. NOTE: If you prefer shuffling locally, please take a look at this user-provided; shuffler option:; https://github.com/google/deepvariant/issues/360#issuecomment-1019990366. ### Validation set. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. This took: 5m31.122s. Copy to GCS bucket:. ```bash; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. ## Shuffle each set of examples and generate a data configuration file for each. Shuffling the `tensorflow.Example`s is an important step for training a model.; In our training logic, we shuffle examples globally using a preprocessing step. First, if you have run this step before, and want to rerun it, you might want to; consider cleaning up previous data first to avoid confusion:. ```bash; # (Optional) Clean up existing files.; gsutil -m rm -f ""${OUTPUT_BUCKET}/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; gsutil -m rm -f ""${OUTPUT_BUCKET}/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt""; gsutil rm -f ""${OUTPUT_BUCKET}/example_info.json""; ```. Here we provide examples for running on; [Cloud Dataflow Runner](https://beam.apache.org/documentation/runners/dataflow/); and also [DirectRunner](https://beam.apache.org/documentation/runners/direct/).; Beam can also use",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:7832,config,configuration,7832,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['config'],['configuration'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. NOTE: If you prefer shuffling locally, please take a look at this user-provided; shuffler option:; https://github.com/google/deepvariant/issues/360#issuecomment-1019990366. ### Validation set. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. This took: 5m31.122s. Copy to GCS bucket:. ```bash; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. ## Shuffle each set of examples and generate a data configuration file for each. Shuffling the `tensorflow.Example`s is an important step for training a model.; In our training logic, we shuffle examples globally using a preprocessing step. First, if you have run this step before, and want to rerun it, you might want to; consider cleaning up previous data first to avoid confusion:. ```bash; # (Optional) Clean up existing files.; gsutil -m rm -f ""${OUTPUT_BUCKET}/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; gsutil -m rm -f ""${OUTPUT_BUCKET}/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt""; gsutil rm -f ""${OUTPUT_BUCKET}/example_info.json""; ```. Here we provide examples for running on; [Cloud Dataflow Runner](https://beam.apache.org/documentation/runners/dataflow/); and also [DirectRunner](https://beam.apache.org/documentation/runners/direct/).; Beam can also use
",False,
Modifiability,"_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoi",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:13622,config,config,13622,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['config'],['config'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: _label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoi
",False,
Modifiability,"aseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; following command:. ```bash; gcloud compute ssh ${host} --zone ${zone}; ```. Once you have logged in, set the variables:. ```bash; YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT; OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant""; VERSION=""1.6.1""; DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard""; GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study""; DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input""; BIN_DIR=""${INPUT_DIR}/bin""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_DIR=""${BASE}/output""; LOG_DIR=""${OUTPUT_DIR}/logs""; SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa""; BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam""; BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam""; BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam""; TRUTH_VCF=""",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:2104,variab,variables,2104,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['variab'],['variables'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: aseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; following command:. ```bash; gcloud compute ssh ${host} --zone ${zone}; ```. Once you have logged in, set the variables:. ```bash; YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT; OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant""; VERSION=""1.6.1""; DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard""; GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study""; DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input""; BIN_DIR=""${INPUT_DIR}/bin""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_DIR=""${BASE}/output""; LOG_DIR=""${OUTPUT_DIR}/logs""; SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa""; BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam""; BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam""; BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam""; TRUTH_VCF=""
",True,"The text consists primarily of code snippets, including bash commands and variable assignments, which are typically associated with program execution rather than meaningful human-readable content. Additionally, the presence of configuration settings for Google Cloud and directory paths suggests a focus on technical implementation details that are more relevant to programming and system administration."
Modifiability,"dation_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_DIR}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DirectRunner \; --direct_num_workers=0; ```. Here is the validation_set:. ```bash; cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt""; ```. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 5591; # class1: 31854; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:12983,config,config,12983,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['config'],['config'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: dation_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_DIR}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DirectRunner \; --direct_num_workers=0; ```. Here is the validation_set:. ```bash; cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt""; ```. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 5591; # class1: 31854; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1
",False,"The text contains complete, meaningful sentences in natural language explaining the configuration and training process."
Modifiability,"e is running. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_DIR}""/validation_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_DIR}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DirectRunner \; --direct_num_workers=0; ```. Here is the validation_set:. ```bash; cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt""; ```. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 5591; # class1: 31854; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:12783,config,config,12783,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['config'],['config'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: e is running. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_DIR}""/validation_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_DIR}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DirectRunner \; --direct_num_workers=0; ```. Here is the validation_set:. ```bash; cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt""; ```. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 5591; # class1: 31854; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 
",False,"The content contains complete, meaningful sentences in natural language discussing the process of training and configuration files."
Modifiability,"eate local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG002 Duplex chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam > ${INPUT_DIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam; curl ${HTTPDIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam.bai > ${INPUT_DIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam.bai. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam""; THREADS=$(nproc); REGION=""chr20"". # Set up output variable; OUTPUT_VCF=""HG002_R1041_Duplex_Dorado_v0.1.1_GRCh38.chr20.output.vcf.gz""; OUTPUT_GVCF=""HG002_R1041_Duplex_Dorado_v0.1.1_GRCh38.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ONT_R104 \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md:2254,variab,variables,2254,docs/deepvariant-ont-r104-duplex-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md,1,['variab'],['variables'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: eate local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG002 Duplex chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam > ${INPUT_DIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam; curl ${HTTPDIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam.bai > ${INPUT_DIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam.bai. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam""; THREADS=$(nproc); REGION=""chr20"". # Set up output variable; OUTPUT_VCF=""HG002_R1041_Duplex_Dorado_v0.1.1_GRCh38.chr20.output.vcf.gz""; OUTPUT_GVCF=""HG002_R1041_Duplex_Dorado_v0.1.1_GRCh38.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ONT_R104 \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${
",False,"The provided text contains a mixture of shell commands and configuration steps, but it also includes some descriptive text such as 'We will run DeepVariant from docker using the `run_deepvariant` script' which is a natural language explanation. Additionally, there are comments like '# Download reference to input directory' that provide context for the code. Therefore, while most of the content is code-based, it also contains meaningful human-readable sentences and comments, so it should not be entirely eliminated."
Modifiability,"ered false positives).; These should be bgzipped and tabix indexed and be on a reference consistent with; the one provided with the `--ref` argument. ### call_variants. `call_variants` consumes TFRecord file(s) of tf.Examples protos created; by `make_examples` and a deep learning model checkpoint and evaluates the model; on each example in the input TFRecord. The output here is a TFRecord of; CallVariantsOutput protos. `call_variants` doesn't directly support sharding its; outputs, but accepts a glob or shard-pattern for its inputs. `call_variants` uses around 4 GB per process and uses TensorFlow for evaluation.; When evaluating a model in CPU mode, TensorFlow can make use of multiple cores,; but scaling is sub-linear. In other words, `call_variants` on a 64 core machine; is less than 8x faster than running on a 8 core machine. When using a GPU, `call_variants` is both faster, more efficient, and needs; fewer CPUs. Based on a small number of experiments, currently the most efficient; configuration for `call_variants` on a GPU instance is 4-8 CPUs and 1 GPU.; Compared to our setting in the [whole genome case study], we noticed a 2.5x; speedup on the call_variants step using a single P100 GPU and 8 CPUs. Note that; currently `call_variants` can only use one GPU at most. So it doesn't improve; the speed if you get a multiple-GPU machine. ### postprocess_variants. `postprocess_variants` reads all of the output TFRecord files from; `call_variants`, sorts them, combines multi-allelic records, and writes out a; VCF file. When [gVCF output](deepvariant-gvcf-support.md) is requested, it also; outputs a gVCF file which merges the VCF with the non-variant sites. Because `postprocess_variants` combines and sorts the output of `call_variants`,; it needs to see all of the outputs from `call_variants` for a single sample to; merge into a final VCF. `postprocess_variants` is single-threaded and needs a; non-trivial amount of memory to run (20-30 GB), so it is best run on a; single/d",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:5595,config,configuration,5595,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['config'],['configuration'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ered false positives).; These should be bgzipped and tabix indexed and be on a reference consistent with; the one provided with the `--ref` argument. ### call_variants. `call_variants` consumes TFRecord file(s) of tf.Examples protos created; by `make_examples` and a deep learning model checkpoint and evaluates the model; on each example in the input TFRecord. The output here is a TFRecord of; CallVariantsOutput protos. `call_variants` doesn't directly support sharding its; outputs, but accepts a glob or shard-pattern for its inputs. `call_variants` uses around 4 GB per process and uses TensorFlow for evaluation.; When evaluating a model in CPU mode, TensorFlow can make use of multiple cores,; but scaling is sub-linear. In other words, `call_variants` on a 64 core machine; is less than 8x faster than running on a 8 core machine. When using a GPU, `call_variants` is both faster, more efficient, and needs; fewer CPUs. Based on a small number of experiments, currently the most efficient; configuration for `call_variants` on a GPU instance is 4-8 CPUs and 1 GPU.; Compared to our setting in the [whole genome case study], we noticed a 2.5x; speedup on the call_variants step using a single P100 GPU and 8 CPUs. Note that; currently `call_variants` can only use one GPU at most. So it doesn't improve; the speed if you get a multiple-GPU machine. ### postprocess_variants. `postprocess_variants` reads all of the output TFRecord files from; `call_variants`, sorts them, combines multi-allelic records, and writes out a; VCF file. When [gVCF output](deepvariant-gvcf-support.md) is requested, it also; outputs a gVCF file which merges the VCF with the non-variant sites. Because `postprocess_variants` combines and sorts the output of `call_variants`,; it needs to see all of the outputs from `call_variants` for a single sample to; merge into a final VCF. `postprocess_variants` is single-threaded and needs a; non-trivial amount of memory to run (20-30 GB), so it is best run on a; single/d
",False,"The text contains complete sentences that are explanatory and descriptive, discussing concepts related to the functionality of various components in a system. It includes descriptions of processes like `call_variants` and `postprocess_variants`, along with their requirements and effects on performance, which are meaningful for human understanding."
Modifiability,"ets.bed; VERSION=1.6.1. declare -a trio=(HG002 HG003 HG004); for SAMPLE in ""${trio[@]}""; do; BAM=${SAMPLE}.bam. OUTPUT_VCF=${SAMPLE}.vcf.gz; OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time sudo docker run \; -v ""${DIR}"":""/data"" \; google/deepvariant:${VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/data/hs37d5.fa"" \; --reads=""/data/${BAM}"" \; --regions=""/data/${CAPTURE_BED}"" \; --output_vcf=""/data/${OUTPUT_VCF}"" \; --output_gvcf=""/data/${OUTPUT_GVCF}"" \; --num_shards=${N_SHARDS}; done; ```. Note: The BAM files should provide unique names for each sample in their `SM`; header tag, which is usually derived from a command-line flag to the read; aligner. If your BAM files don't have unique `SM` tags (and if it's not feasible; to adjust the alignment pipeline), add the `--sample_name=XYZ` flag to; `run_deepvariant` to override the sample name written into the gVCF file header. ## Merge the trio samples using GLnexus. ### Run GLnexus to merge 3 gVCFs. And then run GLnexus with this config:. ```; sudo docker pull quay.io/mlin/glnexus:v1.2.7. time sudo docker run \; -v ""${DIR}"":""/data"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWES \; --bed ""/data/${CAPTURE_BED}"" \; /data/HG004.g.vcf.gz /data/HG003.g.vcf.gz /data/HG002.g.vcf.gz \; | sudo docker run -i google/deepvariant:${VERSION} bcftools view - \; | sudo docker run -i google/deepvariant:${VERSION} bgzip -c \; > ${DIR}/deepvariant.cohort.vcf.gz; ```. When we ran on this WES trio, it took only about 13 seconds. For more details on; performance, see; [GLnexus performance guide](https://github.com/dnanexus-rnd/GLnexus/wiki/Performance). For a WGS cohort, we recommend using `--config DeepVariantWGS` instead of; `DeepVariantWES`. Another preset `DeepVariant_unfiltered` is available in; `glnexus:v1.2.7` or later versions for merging DeepVariant gVCFs with no QC; filters or genotype revision (see; [GitHub issue #326](https://github.com/google/deepvariant/issues/326) fo",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:7690,config,config,7690,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['config'],['config'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ets.bed; VERSION=1.6.1. declare -a trio=(HG002 HG003 HG004); for SAMPLE in ""${trio[@]}""; do; BAM=${SAMPLE}.bam. OUTPUT_VCF=${SAMPLE}.vcf.gz; OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time sudo docker run \; -v ""${DIR}"":""/data"" \; google/deepvariant:${VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/data/hs37d5.fa"" \; --reads=""/data/${BAM}"" \; --regions=""/data/${CAPTURE_BED}"" \; --output_vcf=""/data/${OUTPUT_VCF}"" \; --output_gvcf=""/data/${OUTPUT_GVCF}"" \; --num_shards=${N_SHARDS}; done; ```. Note: The BAM files should provide unique names for each sample in their `SM`; header tag, which is usually derived from a command-line flag to the read; aligner. If your BAM files don't have unique `SM` tags (and if it's not feasible; to adjust the alignment pipeline), add the `--sample_name=XYZ` flag to; `run_deepvariant` to override the sample name written into the gVCF file header. ## Merge the trio samples using GLnexus. ### Run GLnexus to merge 3 gVCFs. And then run GLnexus with this config:. ```; sudo docker pull quay.io/mlin/glnexus:v1.2.7. time sudo docker run \; -v ""${DIR}"":""/data"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWES \; --bed ""/data/${CAPTURE_BED}"" \; /data/HG004.g.vcf.gz /data/HG003.g.vcf.gz /data/HG002.g.vcf.gz \; | sudo docker run -i google/deepvariant:${VERSION} bcftools view - \; | sudo docker run -i google/deepvariant:${VERSION} bgzip -c \; > ${DIR}/deepvariant.cohort.vcf.gz; ```. When we ran on this WES trio, it took only about 13 seconds. For more details on; performance, see; [GLnexus performance guide](https://github.com/dnanexus-rnd/GLnexus/wiki/Performance). For a WGS cohort, we recommend using `--config DeepVariantWGS` instead of; `DeepVariantWES`. Another preset `DeepVariant_unfiltered` is available in; `glnexus:v1.2.7` or later versions for merging DeepVariant gVCFs with no QC; filters or genotype revision (see; [GitHub issue #326](https://github.com/google/deepvariant/issues/326) fo
",False,
Modifiability,"examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:13755,config,config,13755,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['config'],['config'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the 
",False,"The text contains complete, meaningful sentences in natural language discussing concepts related to training parameters and configurations."
Modifiability,"ffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_DIR}""/validation_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_DIR}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DirectRunner \; --direct_num_workers=0; ```. Here is the validation_set:. ```bash; cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt""; ```. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 5591; # class1: 31854; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${T",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:12841,config,configuration,12841,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['config'],['configuration'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_DIR}""/validation_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_DIR}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DirectRunner \; --direct_num_workers=0; ```. Here is the validation_set:. ```bash; cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt""; ```. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 5591; # class1: 31854; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${T
",False,"The text contains both code snippets and explanatory content. The initial lines are command lines with parameters which can be considered as code, but later sections include descriptions about configuration files and training parameters, which are meaningful sentences in natural language."
Modifiability,"fig file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; --",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:13788,config,config,13788,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['config'],['config'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: fig file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; --
",False,"The text contains complete, meaningful sentences in natural language such as discussing the need for a configuration file, explaining how to retrieve it, steps to begin training, summary statistics, and descriptions of validation processes. These are all human-readable and not code or logs."
Modifiability,"frecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:13539,config,config,13539,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['config'],['config'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: frecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint 
",False,
Modifiability,"ique `SM` tags (and if it's not feasible; to adjust the alignment pipeline), add the `--sample_name=XYZ` flag to; `run_deepvariant` to override the sample name written into the gVCF file header. ## Merge the trio samples using GLnexus. ### Run GLnexus to merge 3 gVCFs. And then run GLnexus with this config:. ```; sudo docker pull quay.io/mlin/glnexus:v1.2.7. time sudo docker run \; -v ""${DIR}"":""/data"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWES \; --bed ""/data/${CAPTURE_BED}"" \; /data/HG004.g.vcf.gz /data/HG003.g.vcf.gz /data/HG002.g.vcf.gz \; | sudo docker run -i google/deepvariant:${VERSION} bcftools view - \; | sudo docker run -i google/deepvariant:${VERSION} bgzip -c \; > ${DIR}/deepvariant.cohort.vcf.gz; ```. When we ran on this WES trio, it took only about 13 seconds. For more details on; performance, see; [GLnexus performance guide](https://github.com/dnanexus-rnd/GLnexus/wiki/Performance). For a WGS cohort, we recommend using `--config DeepVariantWGS` instead of; `DeepVariantWES`. Another preset `DeepVariant_unfiltered` is available in; `glnexus:v1.2.7` or later versions for merging DeepVariant gVCFs with no QC; filters or genotype revision (see; [GitHub issue #326](https://github.com/google/deepvariant/issues/326) for a; potential use case). The details of these presets can be found; [here](../deepvariant/cohort_best_practice). ## Annotate the merged VCF with Mendelian discordance information using RTG Tools. Create an SDF template from our reference file:. ```; sudo docker run \; -v ""${DIR}"":""/data"" \; realtimegenomics/rtg-tools format \; -o /data/hs37d5.sdf /data/hs37d5.fa; ```. Create a PED file `$DIR/trio.ped` that looks like this (with the sample name; of the trio):. ```; FILE=""${DIR}/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/ind-id/pat-id/mat-id: 0=unknown; #sex: 1=male; 2=female; 0=unknown; #phenotype: -9=missing, 0=missing; 1=unaffected; 2=affected; #; #fam-id ind-id pat-id mat-id sex ph",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:8384,config,config,8384,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['config'],['config'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ique `SM` tags (and if it's not feasible; to adjust the alignment pipeline), add the `--sample_name=XYZ` flag to; `run_deepvariant` to override the sample name written into the gVCF file header. ## Merge the trio samples using GLnexus. ### Run GLnexus to merge 3 gVCFs. And then run GLnexus with this config:. ```; sudo docker pull quay.io/mlin/glnexus:v1.2.7. time sudo docker run \; -v ""${DIR}"":""/data"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWES \; --bed ""/data/${CAPTURE_BED}"" \; /data/HG004.g.vcf.gz /data/HG003.g.vcf.gz /data/HG002.g.vcf.gz \; | sudo docker run -i google/deepvariant:${VERSION} bcftools view - \; | sudo docker run -i google/deepvariant:${VERSION} bgzip -c \; > ${DIR}/deepvariant.cohort.vcf.gz; ```. When we ran on this WES trio, it took only about 13 seconds. For more details on; performance, see; [GLnexus performance guide](https://github.com/dnanexus-rnd/GLnexus/wiki/Performance). For a WGS cohort, we recommend using `--config DeepVariantWGS` instead of; `DeepVariantWES`. Another preset `DeepVariant_unfiltered` is available in; `glnexus:v1.2.7` or later versions for merging DeepVariant gVCFs with no QC; filters or genotype revision (see; [GitHub issue #326](https://github.com/google/deepvariant/issues/326) for a; potential use case). The details of these presets can be found; [here](../deepvariant/cohort_best_practice). ## Annotate the merged VCF with Mendelian discordance information using RTG Tools. Create an SDF template from our reference file:. ```; sudo docker run \; -v ""${DIR}"":""/data"" \; realtimegenomics/rtg-tools format \; -o /data/hs37d5.sdf /data/hs37d5.fa; ```. Create a PED file `$DIR/trio.ped` that looks like this (with the sample name; of the trio):. ```; FILE=""${DIR}/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/ind-id/pat-id/mat-id: 0=unknown; #sex: 1=male; 2=female; 0=unknown; #phenotype: -9=missing, 0=missing; 1=unaffected; 2=affected; #; #fam-id ind-id pat-id mat-id sex ph
",False,
Modifiability,"ivalent to this example. ```bash; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT Sample; 1 4370 . G <*> . . END=4383 GT:GQ 0/0:37; 1 4384 . C <*> . . END=4388 GT:GQ 0/0:41; 1 4389 . T TC,<*> 50 . . GT:GQ 0/1:50; 1 4390 . C <*> . . END=4390 GT:GQ 0/0:3; ```. ## Creating gVCF output with DeepVariant. The exact same three programs (`make_examples`, `call_variants`, and; `postprocess_variants`) are used when creating gVCF output as in the [WGS case; study]. However, additional flags must be passed to the `make_examples` and; `postprocess_variants` steps. ### `make_examples`. The `make_examples` program is where the gVCF records are computed. One additional flag is required in `make_examples`, the `--gvcf <filename>`; flag. This specifies an additional output, which is a TFRecord file of Variant; protocol buffers. If running with multiple processes, the sharding applied to; this output filename must be the same as that applied to the `--examples`; output. A concrete example call, using variables defined in the [WGS case study]:. ```bash; GVCF_TFRECORDS=""${OUTPUT_DIR}/HG002.gvcf.tfrecord@${N_SHARDS}.gz"". ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`; ```. NOTE: gVCF outputs are only valid when `make_examples` is run in ""calling"" mode;; if attempted to run in ""training"" mode the program will exit and notify the user; of the error. ### `postprocess_variants`. When run in gVCF mode, the `postprocess_variants` program handles the creation; of the final gVCF file that incorporates both the non-variant records and the; true variants discovered by the previous programs. Two additional flags are required in `postprocess_variants`, the input; `--nonvariant_site_tfrecord_path <filename>` which corresponds to the TFRecord; of Var",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:2378,variab,variables,2378,docs/deepvariant-gvcf-support.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md,1,['variab'],['variables'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ivalent to this example. ```bash; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT Sample; 1 4370 . G <*> . . END=4383 GT:GQ 0/0:37; 1 4384 . C <*> . . END=4388 GT:GQ 0/0:41; 1 4389 . T TC,<*> 50 . . GT:GQ 0/1:50; 1 4390 . C <*> . . END=4390 GT:GQ 0/0:3; ```. ## Creating gVCF output with DeepVariant. The exact same three programs (`make_examples`, `call_variants`, and; `postprocess_variants`) are used when creating gVCF output as in the [WGS case; study]. However, additional flags must be passed to the `make_examples` and; `postprocess_variants` steps. ### `make_examples`. The `make_examples` program is where the gVCF records are computed. One additional flag is required in `make_examples`, the `--gvcf <filename>`; flag. This specifies an additional output, which is a TFRecord file of Variant; protocol buffers. If running with multiple processes, the sharding applied to; this output filename must be the same as that applied to the `--examples`; output. A concrete example call, using variables defined in the [WGS case study]:. ```bash; GVCF_TFRECORDS=""${OUTPUT_DIR}/HG002.gvcf.tfrecord@${N_SHARDS}.gz"". ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`; ```. NOTE: gVCF outputs are only valid when `make_examples` is run in ""calling"" mode;; if attempted to run in ""training"" mode the program will exit and notify the user; of the error. ### `postprocess_variants`. When run in gVCF mode, the `postprocess_variants` program handles the creation; of the final gVCF file that incorporates both the non-variant records and the; true variants discovered by the previous programs. Two additional flags are required in `postprocess_variants`, the input; `--nonvariant_site_tfrecord_path <filename>` which corresponds to the TFRecord; of Var
",False,"The text contains meaningful sentences discussing the creation of gVCF output and the usage of specific programs with additional flags, providing clear explanations and instructions."
Modifiability,"les` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output_child.tfrecord.gz; call_variants_output_parent1.tfrecord.gz; call_variants_output_parent2.tfrecord.gz. gvcf_child.tfrecord-?????-of-?????.gz; gvcf_parent1.tfrecord-?????-of-?????.gz; gvcf_parent2.tfrecord-?????-of-?????.gz. make_examples_child.tfrecord-?????-of-?????.gz; make_examples_parent1.tfrecord-?????-of-?????.gz; make_examples_parent2.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Merge VCFs using GLnexus. At this step we take all 3 VCFs generated in the previous step and merge them; using GLnexus. ```bash; # bcftools and bgzip are now included in our docker images.; # You can also install them separately.; sudo docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bcftools view - \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bgzip -c > output/HG002_trio_merged.vcf.gz; ```. After completion of GLnexus command we should have a new merged VCF file in the; output directory. ```; HG002_trio_merged.vcf.gz; ```. ## Benchmark on chr20. ### Calculate mendelian violation rate. ```bash; sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; realtimegenomics/rtg-tools format \; -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". FILE=""reference/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/ind-id/pat-id/mat-id: 0=unknown; #sex: 1=male; 2=female; 0=unknown; #phenotype: -9=missing, 0=missing; 1=unaffected; 2=affected; #; #fam-id ind-id pat-id mat-id sex phen; 1 HG002 HG003 HG",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:6831,config,config,6831,docs/deeptrio-wgs-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md,1,['config'],['config'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: les` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output_child.tfrecord.gz; call_variants_output_parent1.tfrecord.gz; call_variants_output_parent2.tfrecord.gz. gvcf_child.tfrecord-?????-of-?????.gz; gvcf_parent1.tfrecord-?????-of-?????.gz; gvcf_parent2.tfrecord-?????-of-?????.gz. make_examples_child.tfrecord-?????-of-?????.gz; make_examples_parent1.tfrecord-?????-of-?????.gz; make_examples_parent2.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Merge VCFs using GLnexus. At this step we take all 3 VCFs generated in the previous step and merge them; using GLnexus. ```bash; # bcftools and bgzip are now included in our docker images.; # You can also install them separately.; sudo docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bcftools view - \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bgzip -c > output/HG002_trio_merged.vcf.gz; ```. After completion of GLnexus command we should have a new merged VCF file in the; output directory. ```; HG002_trio_merged.vcf.gz; ```. ## Benchmark on chr20. ### Calculate mendelian violation rate. ```bash; sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; realtimegenomics/rtg-tools format \; -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". FILE=""reference/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/ind-id/pat-id/mat-id: 0=unknown; #sex: 1=male; 2=female; 0=unknown; #phenotype: -9=missing, 0=missing; 1=unaffected; 2=affected; #; #fam-id ind-id pat-id mat-id sex phen; 1 HG002 HG003 HG
",False,"The text contains both code snippets and API documentation. It includes bash commands which are code examples, and describes using specific tools (e.g., bcftools) with parameters and configurations. Additionally, there's mention of file paths and configurations for Docker runs. There are also logs indicating the completion of GLnexus commands."
Modifiability,"nfiguration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once tr",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:13885,config,config,13885,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['config'],['config'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: nfiguration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once tr
",False,"The text includes natural language sentences discussing training parameters, steps, and configurations, which are meaningful human-readable content."
Modifiability,"nt-case-study"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input/data""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG003 Ultra-long chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam.bai > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam.bai. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG003_R104_sup_merged.80x.chr20.bam""; THREADS=$(nproc); REGION=""chr20"". # Set up output variable; OUTPUT_VCF=""HG003_UL_R1041_Guppy6_sup_2_GRCh38.chr20.output.vcf.gz""; OUTPUT_GVCF=""HG003_UL_R1041_Guppy6_sup_2_GRCh38.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ONT_R104 \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. By specifyi",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md:2091,variab,variables,2091,docs/deepvariant-ont-r104-simplex-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md,1,['variab'],['variables'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: nt-case-study"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input/data""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG003 Ultra-long chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam.bai > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam.bai. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG003_R104_sup_merged.80x.chr20.bam""; THREADS=$(nproc); REGION=""chr20"". # Set up output variable; OUTPUT_VCF=""HG003_UL_R1041_Guppy6_sup_2_GRCh38.chr20.output.vcf.gz""; OUTPUT_GVCF=""HG003_UL_R1041_Guppy6_sup_2_GRCh38.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ONT_R104 \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. By specifyi
",False,"The text contains a series of shell commands used to set up directories and download data, followed by the command to run DeepVariant. These are not purely code snippets but part of a script that configures the environment for analysis. While there is some code, it also includes explanatory steps such as downloading data from specific URLs, setting up input and output directories, etc. It does not solely consist of code or logs but mixes configuration with operational instructions."
Modifiability,"own on the y-axis, as a percentage of the total runtime for the; given task. If all regions had the same runtime, the curve would be a; straight diagonal line. The extent to which the curve bends to the upper; left corner shows how much some regions take disproportionately longer than; others. Hover the cursor over the lines to see the exact percentages.; 3. ""Total runtime for each task"": Each point is a task. Hover over each point; to see the runtime calculated into hours, minutes, and seconds. Drag a; rectangle around some of the tasks to see them highlighted in the Pareto; curve. Often the tasks with longer runtimes in the chart will be the same; tasks with Pareto curves leaning to the upper left, indicating that for; tasks than run longer than others, the cause is with a subset of the regions; not with an overall slowdown of all regions.; 4. ""Stage runtimes for each task"": A histogram of how long each stage takes for; the different tasks. Often the `make pileup images` stage will show more; variability here than other stages.; 5. ""Top runtime regions"" and ""Median runtime regions"": This shows some; individual regions to give more context for some of the trends seen in other; charts. Pay attention especially to the differences between the y-axis; limits in these two charts. The long-running regions are often taking; hundreds of times longer than median regions, with the runtime also taken up; by different stages.; 6. ""The longest-running regions that produced no examples"": This profiles some; individual regions that yielded zero output examples. Also look at the; subtitle to see what percentage of the total runtime is taken up by; processing these zero-example regions.; 7. ""Runtime by stage for ..."": When there are more than 5000 regions, there; will be two charts here, one for the bottom 99% of regions and one for the; top 100 regions (both by total runtime). If fewer than 5000 regions, there; will only be one chart showing all the regions. This is similar to the;",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md:5893,variab,variability,5893,docs/runtime-by-region.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md,1,['variab'],['variability'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: own on the y-axis, as a percentage of the total runtime for the; given task. If all regions had the same runtime, the curve would be a; straight diagonal line. The extent to which the curve bends to the upper; left corner shows how much some regions take disproportionately longer than; others. Hover the cursor over the lines to see the exact percentages.; 3. ""Total runtime for each task"": Each point is a task. Hover over each point; to see the runtime calculated into hours, minutes, and seconds. Drag a; rectangle around some of the tasks to see them highlighted in the Pareto; curve. Often the tasks with longer runtimes in the chart will be the same; tasks with Pareto curves leaning to the upper left, indicating that for; tasks than run longer than others, the cause is with a subset of the regions; not with an overall slowdown of all regions.; 4. ""Stage runtimes for each task"": A histogram of how long each stage takes for; the different tasks. Often the `make pileup images` stage will show more; variability here than other stages.; 5. ""Top runtime regions"" and ""Median runtime regions"": This shows some; individual regions to give more context for some of the trends seen in other; charts. Pay attention especially to the differences between the y-axis; limits in these two charts. The long-running regions are often taking; hundreds of times longer than median regions, with the runtime also taken up; by different stages.; 6. ""The longest-running regions that produced no examples"": This profiles some; individual regions that yielded zero output examples. Also look at the; subtitle to see what percentage of the total runtime is taken up by; processing these zero-example regions.; 7. ""Runtime by stage for ..."": When there are more than 5000 regions, there; will be two charts here, one for the bottom 99% of regions and one for the; top 100 regions (both by total runtime). If fewer than 5000 regions, there; will only be one chart showing all the regions. This is similar to the;
",True,"The text consists primarily of programmatic descriptions and explanations of charts and graphs related to task runtimes, which are part of an analysis process rather than human-readable prose."
Modifiability,"reate local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}/data"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/xy-case-study-testdata; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai. HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata; curl ${HTTPDIR}/GRCh38_PAR.bed > ${INPUT_DIR}/GRCh38_PAR.bed. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG002.pfda_challenge.grch38.chrXY.bam""; THREADS=$(nproc); REGION=""chrX chrY""; HAPLOID_CONTIGS=""chrX,chrY""; PAR_BED=""GRCh38_PAR.bed"". # Set up output variable; OUTPUT_VCF=""HG002_pacbio_hifi.chrXY.output.vcf.gz""; OUTPUT_GVCF=""HG002_pacbio_hifi.chrXY.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --haploid_contigs ""${HAPLO",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md:2063,variab,variables,2063,docs/deepvariant-xy-calling-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md,1,['variab'],['variables'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: reate local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}/data"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/xy-case-study-testdata; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai. HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata; curl ${HTTPDIR}/GRCh38_PAR.bed > ${INPUT_DIR}/GRCh38_PAR.bed. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG002.pfda_challenge.grch38.chrXY.bam""; THREADS=$(nproc); REGION=""chrX chrY""; HAPLOID_CONTIGS=""chrX,chrY""; PAR_BED=""GRCh38_PAR.bed"". # Set up output variable; OUTPUT_VCF=""HG002_pacbio_hifi.chrXY.output.vcf.gz""; OUTPUT_GVCF=""HG002_pacbio_hifi.chrXY.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --haploid_contigs ""${HAPLO
",True,"The text consists primarily of code snippets, shell commands, and API documentation. It includes command lines with syntax like 'curl', 'gunzip', 'mkdir -p', and references to variables like '${INPUT_DIR}', '${OUTPUT_DIR}'. There are also script calls such as 'run_deepvariant' with arguments, which are more suited for programmatic execution rather than human-readable content."
Modifiability,"rshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The models included with DeepVariant are only trained on human data. For; other organisms, see the; [blog post on non-human variant-calling](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/); for some possible pitfalls and how to handle them. ## DeepTrio. DeepTrio is a deep learning-based trio variant caller built on top of; DeepVariant. DeepTrio extends DeepVariant's functionality, allowing it to; utilize the power of neural networks to predict genomic variants in trios or; duos. See [this page](docs/deeptrio-details.md) for more details and; instructions on how to run DeepTrio. DeepTrio supports germline variant-calling in diploid organisms for the; following types of input data:. * NGS (Illumina) data for either; [whole genome](docs/deeptrio-wgs-case-study.md) or whole exome.; * PacBio HiFi data, see the; [PacBio case study](docs/deeptrio-pacbio-case-study.md). Please also note:. * All DeepTrio models were trained on human data.; * It is possible to use DeepTrio with only 2 samples (child, and one parent).; * External tool [GLnexus](https://github.com/dnanexus-rnd/GLnexus) is used to; merge output VCFs. ## How to run DeepVariant. We recommend using our Docker solution. The command will look like this:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --mo",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:2426,extend,extends,2426,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['extend'],['extends'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: rshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The models included with DeepVariant are only trained on human data. For; other organisms, see the; [blog post on non-human variant-calling](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/); for some possible pitfalls and how to handle them. ## DeepTrio. DeepTrio is a deep learning-based trio variant caller built on top of; DeepVariant. DeepTrio extends DeepVariant's functionality, allowing it to; utilize the power of neural networks to predict genomic variants in trios or; duos. See [this page](docs/deeptrio-details.md) for more details and; instructions on how to run DeepTrio. DeepTrio supports germline variant-calling in diploid organisms for the; following types of input data:. * NGS (Illumina) data for either; [whole genome](docs/deeptrio-wgs-case-study.md) or whole exome.; * PacBio HiFi data, see the; [PacBio case study](docs/deeptrio-pacbio-case-study.md). Please also note:. * All DeepTrio models were trained on human data.; * It is possible to use DeepTrio with only 2 samples (child, and one parent).; * External tool [GLnexus](https://github.com/dnanexus-rnd/GLnexus) is used to; merge output VCFs. ## How to run DeepVariant. We recommend using our Docker solution. The command will look like this:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --mo
",False,
Modifiability,"ry:. ```; call_variants_output_child.tfrecord.gz; call_variants_output_parent1.tfrecord.gz; call_variants_output_parent2.tfrecord.gz. gvcf_child.tfrecord-?????-of-?????.gz; gvcf_parent1.tfrecord-?????-of-?????.gz; gvcf_parent2.tfrecord-?????-of-?????.gz. make_examples_child.tfrecord-?????-of-?????.gz; make_examples_parent1.tfrecord-?????-of-?????.gz; make_examples_parent2.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md) or; [DeepVariant PacBio case study](deepvariant-pacbio-model-case-study.md). ## Merge VCFs using GLnexus. At this step we take all 3 VCFs generated in the previous step and merge them; using GLnexus. ```bash; sudo docker pull quay.io/mlin/glnexus:v1.2.7. # bcftools and bgzip are now included in our docker images.; # You can also install them separately.; sudo docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bcftools view - \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bgzip -c > output/HG002_trio_merged.vcf.gz; ```. After completion of GLnexus command we should have a new merged VCF file in the; output directory. ```; HG002_trio_merged.vcf.gz; ```. ## Benchmark on chr20. ### Calculate Mendelian Violation rate. ```bash; sudo docker pull realtimegenomics/rtg-tools. sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; realtimegenomics/rtg-tools format \; -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". FILE=""reference/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/ind-id/pat-id/mat-id: 0=unknown; #sex: 1=male; 2=female; 0=unknown; #phenotype: -9=missing, 0=missing; 1=unaffected; 2=affected; #; #fam-id in",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:7058,config,config,7058,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,1,['config'],['config'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ry:. ```; call_variants_output_child.tfrecord.gz; call_variants_output_parent1.tfrecord.gz; call_variants_output_parent2.tfrecord.gz. gvcf_child.tfrecord-?????-of-?????.gz; gvcf_parent1.tfrecord-?????-of-?????.gz; gvcf_parent2.tfrecord-?????-of-?????.gz. make_examples_child.tfrecord-?????-of-?????.gz; make_examples_parent1.tfrecord-?????-of-?????.gz; make_examples_parent2.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md) or; [DeepVariant PacBio case study](deepvariant-pacbio-model-case-study.md). ## Merge VCFs using GLnexus. At this step we take all 3 VCFs generated in the previous step and merge them; using GLnexus. ```bash; sudo docker pull quay.io/mlin/glnexus:v1.2.7. # bcftools and bgzip are now included in our docker images.; # You can also install them separately.; sudo docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bcftools view - \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bgzip -c > output/HG002_trio_merged.vcf.gz; ```. After completion of GLnexus command we should have a new merged VCF file in the; output directory. ```; HG002_trio_merged.vcf.gz; ```. ## Benchmark on chr20. ### Calculate Mendelian Violation rate. ```bash; sudo docker pull realtimegenomics/rtg-tools. sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; realtimegenomics/rtg-tools format \; -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". FILE=""reference/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/ind-id/pat-id/mat-id: 0=unknown; #sex: 1=male; 2=female; 0=unknown; #phenotype: -9=missing, 0=missing; 1=unaffected; 2=affected; #; #fam-id in
",True,"The text consists primarily of code snippets and commands, which are likely part of program logs or API documentation. The presence of syntax like 'bash', 'sudo docker pull', and command lines indicates this."
Modifiability,"sampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" samtools; ```. and. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" bcftools; ```. You can read more about samtools and bcftools here: http://www.htslib.org/doc/. ## Commands for requesting machines used in case studies. We report runtime in our case studies documentation. In order to make sure the; results we report are reproducible without too much variation, we provide the; commands we used here to show you what kind of machines we ran the case studies; on. This is NOT the fastest or cheapest configuration. ### Command for a CPU-only machine on Google Cloud Platform. We used a 64-core (vCPU) machine with 240GiB of memory and no GPU, on the Google; Cloud Platform. Specifying the CPU platform also allows us to report the runtime; more consistently. ```shell; gcloud compute instances create ""${USER}-cpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-64"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. ### Command for a GPU machine on Google Cloud Platform. ```shell; gcloud compute instances create ""${USER}-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-p",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:11360,config,configuration,11360,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['config'],['configuration'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: sampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" samtools; ```. and. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" bcftools; ```. You can read more about samtools and bcftools here: http://www.htslib.org/doc/. ## Commands for requesting machines used in case studies. We report runtime in our case studies documentation. In order to make sure the; results we report are reproducible without too much variation, we provide the; commands we used here to show you what kind of machines we ran the case studies; on. This is NOT the fastest or cheapest configuration. ### Command for a CPU-only machine on Google Cloud Platform. We used a 64-core (vCPU) machine with 240GiB of memory and no GPU, on the Google; Cloud Platform. Specifying the CPU platform also allows us to report the runtime; more consistently. ```shell; gcloud compute instances create ""${USER}-cpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-64"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. ### Command for a GPU machine on Google Cloud Platform. ```shell; gcloud compute instances create ""${USER}-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-p
",False,"The text contains commands for running shell scripts and configuring Google Cloud instances. While there are some descriptive sentences, the primary content is a series of code snippets and configuration details which may not be considered meaningful human-readable text on their own."
Modifiability,"t""; ```. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 5591; # class1: 31854; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perf",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:13424,config,config,13424,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['config'],['config'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: t""; ```. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 5591; # class1: 31854; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perf
",False,"The text contains complete sentences in natural language discussing the process of training, including commands and explanations. It is not primarily code or logs."
Modifiability,"this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOI",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14290,config,config,14290,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['config'],['config'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOI
",True,"The provided text consists primarily of code snippets, specifically shell commands and parameters used in a bash script. It includes command lines with syntax such as pipes (|), semicolons (;), backticks (`), and variables like ${USER}, ${DOCKER_IMAGE}, etc. These are indicative of programmatic usage rather than meaningful human-readable sentences."
Modifiability,"training the hybrid model. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 503014 | 1487 | 2767 | 0.997053 | 0.994781 | 0.995916 |; | SNP | 3323624 | 3871 | 2273 | 0.998837 | 0.999317 | 0.999077 |. [See VCF stats report.](https://storage.googleapis.com/deepvariant/visual_reports/DeepVariant/1.6.1/HYBRID/deepvariant.output.visual_report.html). ## Inspect outputs that produced the metrics above. The DeepVariant VCFs, gVCFs, and hap.py evaluation outputs are available at:. ```; gs://deepvariant/case-study-outputs; ```. You can also inspect them in a web browser here:; https://42basepairs.com/browse/gs/deepvariant/case-study-outputs. ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; # Get the script.; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deepvariant.sh. # WGS; bash inference_deepvariant.sh --model_preset WGS. # WES; bash inference_deepvariant.sh --model_preset WES. # PacBio; bash inference_deepvariant.sh --model_preset PACBIO. # ONT_R104; bash inference_deepvariant.sh --model_preset ONT_R104. # Hybrid; bash inference_deepvariant.sh --model_preset HYBRID_PACBIO_ILLUMINA; ```. Runtime metrics are taken from the resulting log after each stage of; DeepVariant. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 runs produced the same output. [CPU instance with 64 CPUs]: deepvariant-details.md#command-for-a-cpu-only-mach",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md:4980,config,configuration,4980,docs/metrics.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md,1,['config'],['configuration'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: training the hybrid model. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 503014 | 1487 | 2767 | 0.997053 | 0.994781 | 0.995916 |; | SNP | 3323624 | 3871 | 2273 | 0.998837 | 0.999317 | 0.999077 |. [See VCF stats report.](https://storage.googleapis.com/deepvariant/visual_reports/DeepVariant/1.6.1/HYBRID/deepvariant.output.visual_report.html). ## Inspect outputs that produced the metrics above. The DeepVariant VCFs, gVCFs, and hap.py evaluation outputs are available at:. ```; gs://deepvariant/case-study-outputs; ```. You can also inspect them in a web browser here:; https://42basepairs.com/browse/gs/deepvariant/case-study-outputs. ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; # Get the script.; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deepvariant.sh. # WGS; bash inference_deepvariant.sh --model_preset WGS. # WES; bash inference_deepvariant.sh --model_preset WES. # PacBio; bash inference_deepvariant.sh --model_preset PACBIO. # ONT_R104; bash inference_deepvariant.sh --model_preset ONT_R104. # Hybrid; bash inference_deepvariant.sh --model_preset HYBRID_PACBIO_ILLUMINA; ```. Runtime metrics are taken from the resulting log after each stage of; DeepVariant. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 runs produced the same output. [CPU instance with 64 CPUs]: deepvariant-details.md#command-for-a-cpu-only-mach
",False,
Modifiability,"ure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG003 Ultra-long chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam.bai > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam.bai. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG003_R104_sup_merged.80x.chr20.bam""; THREADS=$(nproc); REGION=""chr20"". # Set up output variable; OUTPUT_VCF=""HG003_UL_R1041_Guppy6_sup_2_GRCh38.chr20.output.vcf.gz""; OUTPUT_GVCF=""HG003_UL_R1041_Guppy6_sup_2_GRCh38.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ONT_R104 \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. By specifying `--model_type ONT_R104`, you'll be using a model that is best; suited for Oxford Nanopore R10.4.1 chemistry Simplex and Duplex reads. NOTE: If you ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md:2235,variab,variable,2235,docs/deepvariant-ont-r104-simplex-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md,1,['variab'],['variable'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG003 Ultra-long chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam.bai > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam.bai. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG003_R104_sup_merged.80x.chr20.bam""; THREADS=$(nproc); REGION=""chr20"". # Set up output variable; OUTPUT_VCF=""HG003_UL_R1041_Guppy6_sup_2_GRCh38.chr20.output.vcf.gz""; OUTPUT_GVCF=""HG003_UL_R1041_Guppy6_sup_2_GRCh38.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ONT_R104 \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. By specifying `--model_type ONT_R104`, you'll be using a model that is best; suited for Oxford Nanopore R10.4.1 chemistry Simplex and Duplex reads. NOTE: If you 
",False,"The text contains complete, meaningful sentences in natural language. It includes explanatory content about using a specific model type and notes that provide additional information."
Modifiability,"us 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualiz",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14334,config,config,14334,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['config'],['config'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: us 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualiz
",False,
Modifiability,"v ""${DIR}"":""/data"" \; google/deepvariant:${VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/data/hs37d5.fa"" \; --reads=""/data/${BAM}"" \; --regions=""/data/${CAPTURE_BED}"" \; --output_vcf=""/data/${OUTPUT_VCF}"" \; --output_gvcf=""/data/${OUTPUT_GVCF}"" \; --num_shards=${N_SHARDS}; done; ```. Note: The BAM files should provide unique names for each sample in their `SM`; header tag, which is usually derived from a command-line flag to the read; aligner. If your BAM files don't have unique `SM` tags (and if it's not feasible; to adjust the alignment pipeline), add the `--sample_name=XYZ` flag to; `run_deepvariant` to override the sample name written into the gVCF file header. ## Merge the trio samples using GLnexus. ### Run GLnexus to merge 3 gVCFs. And then run GLnexus with this config:. ```; sudo docker pull quay.io/mlin/glnexus:v1.2.7. time sudo docker run \; -v ""${DIR}"":""/data"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWES \; --bed ""/data/${CAPTURE_BED}"" \; /data/HG004.g.vcf.gz /data/HG003.g.vcf.gz /data/HG002.g.vcf.gz \; | sudo docker run -i google/deepvariant:${VERSION} bcftools view - \; | sudo docker run -i google/deepvariant:${VERSION} bgzip -c \; > ${DIR}/deepvariant.cohort.vcf.gz; ```. When we ran on this WES trio, it took only about 13 seconds. For more details on; performance, see; [GLnexus performance guide](https://github.com/dnanexus-rnd/GLnexus/wiki/Performance). For a WGS cohort, we recommend using `--config DeepVariantWGS` instead of; `DeepVariantWES`. Another preset `DeepVariant_unfiltered` is available in; `glnexus:v1.2.7` or later versions for merging DeepVariant gVCFs with no QC; filters or genotype revision (see; [GitHub issue #326](https://github.com/google/deepvariant/issues/326) for a; potential use case). The details of these presets can be found; [here](../deepvariant/cohort_best_practice). ## Annotate the merged VCF with Mendelian discordance information using RTG Tools. C",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:7860,config,config,7860,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['config'],['config'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: v ""${DIR}"":""/data"" \; google/deepvariant:${VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/data/hs37d5.fa"" \; --reads=""/data/${BAM}"" \; --regions=""/data/${CAPTURE_BED}"" \; --output_vcf=""/data/${OUTPUT_VCF}"" \; --output_gvcf=""/data/${OUTPUT_GVCF}"" \; --num_shards=${N_SHARDS}; done; ```. Note: The BAM files should provide unique names for each sample in their `SM`; header tag, which is usually derived from a command-line flag to the read; aligner. If your BAM files don't have unique `SM` tags (and if it's not feasible; to adjust the alignment pipeline), add the `--sample_name=XYZ` flag to; `run_deepvariant` to override the sample name written into the gVCF file header. ## Merge the trio samples using GLnexus. ### Run GLnexus to merge 3 gVCFs. And then run GLnexus with this config:. ```; sudo docker pull quay.io/mlin/glnexus:v1.2.7. time sudo docker run \; -v ""${DIR}"":""/data"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWES \; --bed ""/data/${CAPTURE_BED}"" \; /data/HG004.g.vcf.gz /data/HG003.g.vcf.gz /data/HG002.g.vcf.gz \; | sudo docker run -i google/deepvariant:${VERSION} bcftools view - \; | sudo docker run -i google/deepvariant:${VERSION} bgzip -c \; > ${DIR}/deepvariant.cohort.vcf.gz; ```. When we ran on this WES trio, it took only about 13 seconds. For more details on; performance, see; [GLnexus performance guide](https://github.com/dnanexus-rnd/GLnexus/wiki/Performance). For a WGS cohort, we recommend using `--config DeepVariantWGS` instead of; `DeepVariantWES`. Another preset `DeepVariant_unfiltered` is available in; `glnexus:v1.2.7` or later versions for merging DeepVariant gVCFs with no QC; filters or genotype revision (see; [GitHub issue #326](https://github.com/google/deepvariant/issues/326) for a; potential use case). The details of these presets can be found; [here](../deepvariant/cohort_best_practice). ## Annotate the merged VCF with Mendelian discordance information using RTG Tools. C
",False,"The text contains complete, meaningful sentences in natural language discussing concepts and ideas related to data processing steps, such as running DeepVariant, merging gVCFs using GLnexus, performance considerations, and recommendations for different presets. It also includes instructions on how to annotate a merged VCF file with Mendelian discordance information. There are no code snippets, logs, API documentation, or other programmatic content that would require elimination."
Performance," -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14362,perform,perform,14362,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['perform'],['perform'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; 
",False,
Performance," apt -y update; sudo apt -y install parallel; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh; bash -x install_nvidia_docker.sh; ```. ## Run make_examples in “training” mode for training and validation sets. Create examples in ""training"" mode (which means these `tensorflow.Example`s will; contain a `label` field). In this tutorial, we create examples on one replicate of HG001 sequenced by; BGISEQ-500 provided on the; [Genome In a Bottle FTP site](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/BGISEQ500/standard_library/readme.txt). In this tutorial, we will split the genome up into the following datasets:. | chrom | Name | Description |; | ----- | --------------------- | -------------------------------------------- |; | chr1 | Training Set | Examples used to train our model. |; | chr21 | Validation / Tune Set | Examples used to evaluate the performance of our model during training.|; | chr20 | Test Set | Examples reserved for testing performance of our trained model. |. Note that normally, the training dataset will be much larger (e.g. chr1-19),; rather than just a single chromosome. We use just chr1 here to demonstrate how; customized training works. For the definition of these 3 sets in commonly used machine learning; terminology, please refer to; [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary/). ### Training set. First, to set up, lets pull the docker images. ```bash; sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image.; sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image.; ```. The `make_examples` step doesn't use GPU, so we will not require the GPU-enabled; image. ```bash; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHAR",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:4910,perform,performance,4910,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['perform'],['performance'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  apt -y update; sudo apt -y install parallel; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh; bash -x install_nvidia_docker.sh; ```. ## Run make_examples in “training” mode for training and validation sets. Create examples in ""training"" mode (which means these `tensorflow.Example`s will; contain a `label` field). In this tutorial, we create examples on one replicate of HG001 sequenced by; BGISEQ-500 provided on the; [Genome In a Bottle FTP site](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/BGISEQ500/standard_library/readme.txt). In this tutorial, we will split the genome up into the following datasets:. | chrom | Name | Description |; | ----- | --------------------- | -------------------------------------------- |; | chr1 | Training Set | Examples used to train our model. |; | chr21 | Validation / Tune Set | Examples used to evaluate the performance of our model during training.|; | chr20 | Test Set | Examples reserved for testing performance of our trained model. |. Note that normally, the training dataset will be much larger (e.g. chr1-19),; rather than just a single chromosome. We use just chr1 here to demonstrate how; customized training works. For the definition of these 3 sets in commonly used machine learning; terminology, please refer to; [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary/). ### Training set. First, to set up, lets pull the docker images. ```bash; sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image.; sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image.; ```. The `make_examples` step doesn't use GPU, so we will not require the GPU-enabled; image. ```bash; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHAR
",False,"The text contains meaningful sentences discussing the setup and process of training a model, including explanations about dataset creation, splitting into training, validation, and test sets, and instructions for setting up Docker images. The content is human-readable prose that provides context and guidance."
Performance," in their `SM`; header tag, which is usually derived from a command-line flag to the read; aligner. If your BAM files don't have unique `SM` tags (and if it's not feasible; to adjust the alignment pipeline), add the `--sample_name=XYZ` flag to; `run_deepvariant` to override the sample name written into the gVCF file header. ## Merge the trio samples using GLnexus. ### Run GLnexus to merge 3 gVCFs. And then run GLnexus with this config:. ```; sudo docker pull quay.io/mlin/glnexus:v1.2.7. time sudo docker run \; -v ""${DIR}"":""/data"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWES \; --bed ""/data/${CAPTURE_BED}"" \; /data/HG004.g.vcf.gz /data/HG003.g.vcf.gz /data/HG002.g.vcf.gz \; | sudo docker run -i google/deepvariant:${VERSION} bcftools view - \; | sudo docker run -i google/deepvariant:${VERSION} bgzip -c \; > ${DIR}/deepvariant.cohort.vcf.gz; ```. When we ran on this WES trio, it took only about 13 seconds. For more details on; performance, see; [GLnexus performance guide](https://github.com/dnanexus-rnd/GLnexus/wiki/Performance). For a WGS cohort, we recommend using `--config DeepVariantWGS` instead of; `DeepVariantWES`. Another preset `DeepVariant_unfiltered` is available in; `glnexus:v1.2.7` or later versions for merging DeepVariant gVCFs with no QC; filters or genotype revision (see; [GitHub issue #326](https://github.com/google/deepvariant/issues/326) for a; potential use case). The details of these presets can be found; [here](../deepvariant/cohort_best_practice). ## Annotate the merged VCF with Mendelian discordance information using RTG Tools. Create an SDF template from our reference file:. ```; sudo docker run \; -v ""${DIR}"":""/data"" \; realtimegenomics/rtg-tools format \; -o /data/hs37d5.sdf /data/hs37d5.fa; ```. Create a PED file `$DIR/trio.ped` that looks like this (with the sample name; of the trio):. ```; FILE=""${DIR}/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/ind-id/pat-id/mat-id: 0=unknown; #se",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:8239,perform,performance,8239,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,2,['perform'],['performance'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  in their `SM`; header tag, which is usually derived from a command-line flag to the read; aligner. If your BAM files don't have unique `SM` tags (and if it's not feasible; to adjust the alignment pipeline), add the `--sample_name=XYZ` flag to; `run_deepvariant` to override the sample name written into the gVCF file header. ## Merge the trio samples using GLnexus. ### Run GLnexus to merge 3 gVCFs. And then run GLnexus with this config:. ```; sudo docker pull quay.io/mlin/glnexus:v1.2.7. time sudo docker run \; -v ""${DIR}"":""/data"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWES \; --bed ""/data/${CAPTURE_BED}"" \; /data/HG004.g.vcf.gz /data/HG003.g.vcf.gz /data/HG002.g.vcf.gz \; | sudo docker run -i google/deepvariant:${VERSION} bcftools view - \; | sudo docker run -i google/deepvariant:${VERSION} bgzip -c \; > ${DIR}/deepvariant.cohort.vcf.gz; ```. When we ran on this WES trio, it took only about 13 seconds. For more details on; performance, see; [GLnexus performance guide](https://github.com/dnanexus-rnd/GLnexus/wiki/Performance). For a WGS cohort, we recommend using `--config DeepVariantWGS` instead of; `DeepVariantWES`. Another preset `DeepVariant_unfiltered` is available in; `glnexus:v1.2.7` or later versions for merging DeepVariant gVCFs with no QC; filters or genotype revision (see; [GitHub issue #326](https://github.com/google/deepvariant/issues/326) for a; potential use case). The details of these presets can be found; [here](../deepvariant/cohort_best_practice). ## Annotate the merged VCF with Mendelian discordance information using RTG Tools. Create an SDF template from our reference file:. ```; sudo docker run \; -v ""${DIR}"":""/data"" \; realtimegenomics/rtg-tools format \; -o /data/hs37d5.sdf /data/hs37d5.fa; ```. Create a PED file `$DIR/trio.ped` that looks like this (with the sample name; of the trio):. ```; FILE=""${DIR}/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/ind-id/pat-id/mat-id: 0=unknown; #se
",True,"The text consists primarily of code snippets, commands, and configuration details which are more appropriate for a programmatic context rather than human-readable content."
Performance," install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. Shuffle using Dataflow. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. Then, you should be able to see the run on:; https://console.cloud.google.com/dataflow?project=YOUR_PROJECT. In order to have the best performance, you might need extra resources such as; machines or IPs within a region. That will not be in the scope of this case; study here. The output path can be found in the dataset_config file by:. ```bash; gsutil cat ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; ```. In the output, the `tfrecord_path` should be valid paths in gs://. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 44516; # class1: 173673; # class2: 124569; #; # --input_pattern_list=OUTPUT_BUCKET/training_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_BUCKET/training_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_GCS_BUCKET/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 342758; ```. We can shuffle the validation set locally using; [DirectRunner](https://beam.apache.org/documentation/runners/direct/). Adding; `--direct_num_workers=0` sets the number of threads/subprocess to the number of; cores of the machine where the pipeline is running. ```bash; tim",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:10779,perform,performance,10779,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['perform'],['performance'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. Shuffle using Dataflow. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. Then, you should be able to see the run on:; https://console.cloud.google.com/dataflow?project=YOUR_PROJECT. In order to have the best performance, you might need extra resources such as; machines or IPs within a region. That will not be in the scope of this case; study here. The output path can be found in the dataset_config file by:. ```bash; gsutil cat ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; ```. In the output, the `tfrecord_path` should be valid paths in gs://. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 44516; # class1: 173673; # class2: 124569; #; # --input_pattern_list=OUTPUT_BUCKET/training_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_BUCKET/training_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_GCS_BUCKET/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 342758; ```. We can shuffle the validation set locally using; [DirectRunner](https://beam.apache.org/documentation/runners/direct/). Adding; `--direct_num_workers=0` sets the number of threads/subprocess to the number of; cores of the machine where the pipeline is running. ```bash; tim
",False,"The text contains meaningful, explanatory sentences and human-written prose discussing concepts and ideas related to shuffling a validation set using Dataflow and Apache Beam."
Performance," the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir""; tensorboard --logdir ${TRAINING_DIR} --port=8080; ```. After it started, I clicked on the “Web Preview” on the top right of the mini; terminal:. ![WebPreview](images/WebPreview.png?raw=true ""Web Preview""). And clicked on ""Preview on port 8080"":. ![PreviewOnPort](images/PreviewOnPort.png?raw=true ""Preview on Port 8080""). Once it starts, you can see many metrics, including accuracy, speed, etc. You; will need to wait for `train` to run for a while before the plots will appear. ### Test the model. Now that we have performed training, we can test the performance of the new; model using our holdout dataset (chr20). The following one-step command can be used to call DeepVariant and run our newly; trained model:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ""${DOCKER_IMAGE}-gpu"" \; run_deepvariant \; --model_type WGS \; --customized_model ""${BEST_CHECKPOINT}"" \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/test_set.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. In v1.4.0, by using `--model_type WGS`, `run_deepvariant` will automatically add; `insert_size` as an extra channel in the `make_examples` step. So we don't need; to add it in `--make_examples_extra_args`. When the `call_variants` step is run, you might see messages like:. ```; E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. You can use `nvidia-smi` to confirm whether the GPUs are used. If so, you can;",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:16486,perform,performed,16486,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,2,['perform'],"['performance', 'performed']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir""; tensorboard --logdir ${TRAINING_DIR} --port=8080; ```. After it started, I clicked on the “Web Preview” on the top right of the mini; terminal:. ![WebPreview](images/WebPreview.png?raw=true ""Web Preview""). And clicked on ""Preview on port 8080"":. ![PreviewOnPort](images/PreviewOnPort.png?raw=true ""Preview on Port 8080""). Once it starts, you can see many metrics, including accuracy, speed, etc. You; will need to wait for `train` to run for a while before the plots will appear. ### Test the model. Now that we have performed training, we can test the performance of the new; model using our holdout dataset (chr20). The following one-step command can be used to call DeepVariant and run our newly; trained model:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ""${DOCKER_IMAGE}-gpu"" \; run_deepvariant \; --model_type WGS \; --customized_model ""${BEST_CHECKPOINT}"" \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/test_set.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. In v1.4.0, by using `--model_type WGS`, `run_deepvariant` will automatically add; `insert_size` as an extra channel in the `make_examples` step. So we don't need; to add it in `--make_examples_extra_args`. When the `call_variants` step is run, you might see messages like:. ```; E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. You can use `nvidia-smi` to confirm whether the GPUs are used. If so, you can;
",False,
Performance,"# Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data. DeepVariant is an analysis pipeline that uses a deep neural network to call; genetic variants from next-generation DNA sequencing (NGS) data. While; DeepVariant is highly accurate for; [many types of NGS data](https://rdcu.be/7Dhl), some users may be interested in; training custom deep learning models that have been optimized for very specific; data. This case study describes one way to train such a custom model using a GPU, in; this case for BGISEQ-500 data. Please note that there is not yet a production-grade training pipeline. This is; just one example of how to train a custom model, and is neither the fastest nor; the cheapest possible configuration. The resulting model also does not represent; the greatest achievable accuracy for BGISEQ-500 data. ## High level summary of result. We demonstrated that by training on 1 replicate of BGISEQ-500 whole genome data; (everything except for chromosome 20-22), we can significantly improve the; accuracy comparing to the WGS model as a baseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; followi",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:416,optimiz,optimized,416,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['optimiz'],['optimized'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data. DeepVariant is an analysis pipeline that uses a deep neural network to call; genetic variants from next-generation DNA sequencing (NGS) data. While; DeepVariant is highly accurate for; [many types of NGS data](https://rdcu.be/7Dhl), some users may be interested in; training custom deep learning models that have been optimized for very specific; data. This case study describes one way to train such a custom model using a GPU, in; this case for BGISEQ-500 data. Please note that there is not yet a production-grade training pipeline. This is; just one example of how to train a custom model, and is neither the fastest nor; the cheapest possible configuration. The resulting model also does not represent; the greatest achievable accuracy for BGISEQ-500 data. ## High level summary of result. We demonstrated that by training on 1 replicate of BGISEQ-500 whole genome data; (everything except for chromosome 20-22), we can significantly improve the; accuracy comparing to the WGS model as a baseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; followi
",False,"This text is not primarily code or logs. It includes detailed explanations about training a variant caller for specific data, comparisons of accuracy, instructions on how to request a GPU machine, and general guidance which are all meaningful and in natural language."
Performance,"# Best practices for multi-sample variant calling with DeepVariant (WES trio demonstration). ## Overview. This document outlines all the steps and considerations for calling and merging; a trio using DeepVariant and [GLnexus](https://github.com/dnanexus-rnd/GLnexus).; These best practices were developed and evaluated as described in the article; published in _Bioinformatics_:; [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://doi.org/10.1093/bioinformatics/btaa1081); (2021). The process involves 3 major stages: running DeepVariant to create individual; genome call sets, running GLnexus to merge call sets, and analyzing the merged; call set. NOTE: This case study demonstrates an example of how to run DeepVariant; end-to-end on one machine. The steps below were done on a machine with this; [example command to start a machine](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). The steps in this document can be extended to merge larger cohorts as well. See this workflow:. ![workflow](images/cohort-workflow.png?raw=true ""DeepVariant+GLnexus cohort workflow""). A few things to note before we start:. * It is recommended to use BAM files with original quality scores. In the case; that BAM files went through recalibration, optional DV flags can be used in; order to use original scores: `--parse_sam_aux_fields`,; `--use_original_quality_scores`.; * DeepVariant optionally allows gVCF output. This option is required for; further GLnexus analysis in this document. ## Dataset. The Whole Exome Sequencing (WES) dataset we're using is from:. [ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/). * HG002_NA24385_son; * HG003_NA24149_father; * HG004_NA24143_mother. ### Commands for downloading the input BAMs. Just for convenience, we use aria2 to download our data. You can change it to; whatever other tools (wget, curl) that you prefer. To install aria2, you",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:391,scalab,scalable,391,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['scalab'],['scalable'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # Best practices for multi-sample variant calling with DeepVariant (WES trio demonstration). ## Overview. This document outlines all the steps and considerations for calling and merging; a trio using DeepVariant and [GLnexus](https://github.com/dnanexus-rnd/GLnexus).; These best practices were developed and evaluated as described in the article; published in _Bioinformatics_:; [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://doi.org/10.1093/bioinformatics/btaa1081); (2021). The process involves 3 major stages: running DeepVariant to create individual; genome call sets, running GLnexus to merge call sets, and analyzing the merged; call set. NOTE: This case study demonstrates an example of how to run DeepVariant; end-to-end on one machine. The steps below were done on a machine with this; [example command to start a machine](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). The steps in this document can be extended to merge larger cohorts as well. See this workflow:. ![workflow](images/cohort-workflow.png?raw=true ""DeepVariant+GLnexus cohort workflow""). A few things to note before we start:. * It is recommended to use BAM files with original quality scores. In the case; that BAM files went through recalibration, optional DV flags can be used in; order to use original scores: `--parse_sam_aux_fields`,; `--use_original_quality_scores`.; * DeepVariant optionally allows gVCF output. This option is required for; further GLnexus analysis in this document. ## Dataset. The Whole Exome Sequencing (WES) dataset we're using is from:. [ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/). * HG002_NA24385_son; * HG003_NA24149_father; * HG004_NA24143_mother. ### Commands for downloading the input BAMs. Just for convenience, we use aria2 to download our data. You can change it to; whatever other tools (wget, curl) that you prefer. To install aria2, you
",False,"The text contains complete, meaningful sentences discussing best practices and procedures in multi-sample variant calling with DeepVariant, including dataset descriptions, commands for downloading inputs, and workflow notes. It also includes references to figures and resources, which contribute to its readability as explanatory content. Therefore, it should not be eliminated."
Performance,"-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}""; ```. ### Download extra packages. ```bash; sudo apt -y update; sudo apt -y install parallel; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh; bash -x install_nvidia_docker.sh; ```. ## Run make_examples in “training” mode for training and validation sets. Create examples in ""training"" mode (which means these `tensorflow.Example`s will; contain a `label` field). In this tutorial, we create examples on one replicate of HG001 sequenced by; BGISEQ-500 provided on the; [Genome In a Bottle FTP site](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/BGISEQ500/standard_library/readme.txt). In this tutorial, we will split the genome up into the following datasets:. | chrom | Name | Description |; | ----- | --------------------- | -------------------------------------------- |; | chr1 | Training Set | Examples used to train our model. |; | chr21 | Validation / Tune Set | Examples used to evaluate the performance of our model during training.|; | chr20 | Test Set | Examples reserved for testing performance of our trained model. |. Note that normally, the training dataset will be much larger (e.g. chr1-19),; rather than just a single chromosome. We use just chr1 here to demonstrate how; customized training works. For the definition of these 3 sets in commonly used machine learning; terminology, please refer to; [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary/). ### Training set. First, to set up, lets pull the docker images. ```bash; sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image.; sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image.; ```. The `make_examples` step doesn't use GPU, so we will not require the GPU-enabled; image. ```bash; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:4815,perform,performance,4815,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['perform'],['performance'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: -SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}""; ```. ### Download extra packages. ```bash; sudo apt -y update; sudo apt -y install parallel; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh; bash -x install_nvidia_docker.sh; ```. ## Run make_examples in “training” mode for training and validation sets. Create examples in ""training"" mode (which means these `tensorflow.Example`s will; contain a `label` field). In this tutorial, we create examples on one replicate of HG001 sequenced by; BGISEQ-500 provided on the; [Genome In a Bottle FTP site](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/BGISEQ500/standard_library/readme.txt). In this tutorial, we will split the genome up into the following datasets:. | chrom | Name | Description |; | ----- | --------------------- | -------------------------------------------- |; | chr1 | Training Set | Examples used to train our model. |; | chr21 | Validation / Tune Set | Examples used to evaluate the performance of our model during training.|; | chr20 | Test Set | Examples reserved for testing performance of our trained model. |. Note that normally, the training dataset will be much larger (e.g. chr1-19),; rather than just a single chromosome. We use just chr1 here to demonstrate how; customized training works. For the definition of these 3 sets in commonly used machine learning; terminology, please refer to; [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary/). ### Training set. First, to set up, lets pull the docker images. ```bash; sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image.; sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image.; ```. The `make_examples` step doesn't use GPU, so we will not require the GPU-enabled; image. ```bash; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF
",True,"The text consists primarily of code snippets and command lines. It includes bash commands, docker pulls, and make_example instructions which are more programmatic and not meant for human-readable content."
Performance,".init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cl",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14601,perform,performing,14601,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['perform'],['performing'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: .init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cl
",False,"The text contains meaningful, explanatory sentences discussing training parameters, dataset statistics, validation procedures, and checkpoint management. It includes instructions for users on how to execute training, monitor progress, and retrieve results, which are all part of a human-readable documentation."
Performance,"04<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 214,302,681; v1.3 | Same model as v1.2 |; v1.4 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,863,645; v1.5 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,863,664; v1.6 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,353,081. <a name=""vfootnote1"">(1)</a>: In v0.5, we experimented with adding whole exome; sequencing data into training data. In v0.6, we took it out because it didn't; improve the WGS accuracy. <a name=""vfootnote2"">(2)</a>: The training data are from the same replicates as; v0.5. The number of examples changed because of the update in; [haplotype_labeler](https://github.com/google/deepvariant/tree/r0.6/deepvariant/labeler/haplotype_labeler.py). <a name=""vfootnote3"">(3)</a>: In v0.8, we used the; [Platinum Genomes Truthset](https://github.com/Illumina/PlatinumGenomes) to; create more training examples outside the GIAB confident regions. <a name=""vfootnote4"">(4)</a>: Previously, we split train/tune by leaving 3 WES; for tuning. Starting from this release, we leave out chr1 and chr20 from; training, and use chr1 for tuning. <a name=""vfootnote5"">(5)</a>: Starting from this version, we padded (100bps on; both sides) of the capture BED and used that for generating training examples.; We also added more `downsample_fraction`. <a name=""vfootnote6"">(6)</a>: (Before v1.0) PacBio is the only one we currently; uses HG002 in training and tuning. <a name=""vfootnote7"">(7)</a>: In v1.0, we train on HG002-HG004 for WGS as well,; but only using examples from the region of NIST truth confident region v4.2; subtracting v3.3.2. <a name=""vfootnote8"">(8)</a>: In v1.0, PacBio training data contains training; examples with haplotag sorted images and unsorted images. <a name=""vfootnote9"">(9)</a>: In v1.1, we exclude HG003 from training. And we; use all NIST truth confident regions for HG001-HG007 (except for HG003) for; training. We've always excluded chr20-22 from training. <a name=""vf",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details-training-data.md:5065,tune,tune,5065,docs/deepvariant-details-training-data.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details-training-data.md,1,['tune'],['tune'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: 04<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 214,302,681; v1.3 | Same model as v1.2 |; v1.4 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,863,645; v1.5 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,863,664; v1.6 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,353,081. <a name=""vfootnote1"">(1)</a>: In v0.5, we experimented with adding whole exome; sequencing data into training data. In v0.6, we took it out because it didn't; improve the WGS accuracy. <a name=""vfootnote2"">(2)</a>: The training data are from the same replicates as; v0.5. The number of examples changed because of the update in; [haplotype_labeler](https://github.com/google/deepvariant/tree/r0.6/deepvariant/labeler/haplotype_labeler.py). <a name=""vfootnote3"">(3)</a>: In v0.8, we used the; [Platinum Genomes Truthset](https://github.com/Illumina/PlatinumGenomes) to; create more training examples outside the GIAB confident regions. <a name=""vfootnote4"">(4)</a>: Previously, we split train/tune by leaving 3 WES; for tuning. Starting from this release, we leave out chr1 and chr20 from; training, and use chr1 for tuning. <a name=""vfootnote5"">(5)</a>: Starting from this version, we padded (100bps on; both sides) of the capture BED and used that for generating training examples.; We also added more `downsample_fraction`. <a name=""vfootnote6"">(6)</a>: (Before v1.0) PacBio is the only one we currently; uses HG002 in training and tuning. <a name=""vfootnote7"">(7)</a>: In v1.0, we train on HG002-HG004 for WGS as well,; but only using examples from the region of NIST truth confident region v4.2; subtracting v3.3.2. <a name=""vfootnote8"">(8)</a>: In v1.0, PacBio training data contains training; examples with haplotag sorted images and unsorted images. <a name=""vfootnote9"">(9)</a>: In v1.1, we exclude HG003 from training. And we; use all NIST truth confident regions for HG001-HG007 (except for HG003) for; training. We've always excluded chr20-22 from training. <a name=""vf
",False,"The text contains multiple version notes, footnotes, and detailed descriptions of changes in different versions of a model, which are meaningful and provide explanatory content about the development process."
Performance,"; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a T",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14435,tune,tune,14435,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,2,['tune'],['tune'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a T
",False,"The text contains natural language explanations and setup instructions, including how to use TensorBoard for visualization."
Performance,"; --direct_num_workers=0; ```. Here is the validation_set:. ```bash; cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt""; ```. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 5591; # class1: 31854; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoc",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:13211,optimiz,optimized,13211,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['optimiz'],['optimized'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ; --direct_num_workers=0; ```. Here is the validation_set:. ```bash; cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt""; ```. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 5591; # class1: 31854; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoc
",False,"The text contains complete sentences and descriptive content discussing the process of training, including parameters and commands."
Performance,"; OUTPUT_GVCF=""HG002_pacbio_hifi.chrXY.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --haploid_contigs ""${HAPLOID_CONTIGS}"" \; --par_regions_bed ""${INPUT_DIR}/${PAR_BED}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Benchmark X, Y outputs from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v1.0 of the Genome in a Bottle; small variant benchmarks for HG002_chrXY. ```bash; FTPDIR=https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/chrXY_v1.0/GRCh38/SmallVariant. curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.bed > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.bed; curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz.tbi > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz.tbi. TRUTH_VCF=""HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz""; TRUTH_BED=""HG002_GRCh38_chrXY_smallvar_v1.0.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. REGION=""chrX,chrY""; sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md:3346,perform,performance,3346,docs/deepvariant-xy-calling-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md,1,['perform'],['performance'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ; OUTPUT_GVCF=""HG002_pacbio_hifi.chrXY.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --haploid_contigs ""${HAPLOID_CONTIGS}"" \; --par_regions_bed ""${INPUT_DIR}/${PAR_BED}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Benchmark X, Y outputs from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v1.0 of the Genome in a Bottle; small variant benchmarks for HG002_chrXY. ```bash; FTPDIR=https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/chrXY_v1.0/GRCh38/SmallVariant. curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.bed > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.bed; curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz.tbi > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz.tbi. TRUTH_VCF=""HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz""; TRUTH_BED=""HG002_GRCh38_chrXY_smallvar_v1.0.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. REGION=""chrX,chrY""; sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/
",False,"The content contains meaningful sentences discussing the use of computational tools (DeepVariant and hap.py) for variant calling, along with instructions on how to download benchmark datasets. The text includes explanatory descriptions and human-readable prose that provides context and guidance on the process."
Performance,"IC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 9811 | 212 | 155 | 0.978849 | 0.985044 | 0.981937 |; | SNP | 66180 | 57 | 70 | 0.999139 | 0.998944 | 0.999042 |. The baseline we're comparing to is to directly use the WGS model to make the; calls, using this command:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE}-gpu \; run_deepvariant \; --model_type WGS \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/baseline.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. Baseline:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 9620 | 403 | 823 | 0.959792 | 0.924112 | 0.941615 |; | SNP | 66159 | 78 | 83 | 0.998822 | 0.998748 | 0.998785 |. ### Additional things to try. #### Parameters to tune. Starting from the default setting of this tutorial is a good starting point, but; this training case study is by no means the best setting. Training is both a; science and an art. There are many knobs that we could potentially tune. Users; might be able to use different parameters to train a more accurate model even; with the same data, such as `batch_size`, `learning_rate`,; `learning_rate_decay_factor` in modeling.py. #### Downsampling the BAM file to generate more training examples. When generating the training set, we can make some adjustment to create more; training data. For example, when we train the released WGS model for; DeepVariant, for each BAM file, we created an extra set of training examples; using `--downsample_fraction=0.5`, which downsamples the reads and creates; training examples with lower coverage. We found that this makes the trained; model more robust. [GPU machine]: deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:20430,tune,tune,20430,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,2,['tune'],['tune'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: IC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 9811 | 212 | 155 | 0.978849 | 0.985044 | 0.981937 |; | SNP | 66180 | 57 | 70 | 0.999139 | 0.998944 | 0.999042 |. The baseline we're comparing to is to directly use the WGS model to make the; calls, using this command:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE}-gpu \; run_deepvariant \; --model_type WGS \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/baseline.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. Baseline:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 9620 | 403 | 823 | 0.959792 | 0.924112 | 0.941615 |; | SNP | 66159 | 78 | 83 | 0.998822 | 0.998748 | 0.998785 |. ### Additional things to try. #### Parameters to tune. Starting from the default setting of this tutorial is a good starting point, but; this training case study is by no means the best setting. Training is both a; science and an art. There are many knobs that we could potentially tune. Users; might be able to use different parameters to train a more accurate model even; with the same data, such as `batch_size`, `learning_rate`,; `learning_rate_decay_factor` in modeling.py. #### Downsampling the BAM file to generate more training examples. When generating the training set, we can make some adjustment to create more; training data. For example, when we train the released WGS model for; DeepVariant, for each BAM file, we created an extra set of training examples; using `--downsample_fraction=0.5`, which downsamples the reads and creates; training examples with lower coverage. We found that this makes the trained; model more robust. [GPU machine]: deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform; 
",False,"The content includes meaningful sentences discussing parameters to tune and downsampling methods, which are part of a training setup explanation."
Performance,"e; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 30x whole genome sample in BAM and CRAM format. Filetype | Size (Gb) | Runtime (min); -------- | --------- | -------------; BAM | 66.99 | 79m47.37307s; CRAM | 37.85 | 96m53.477s; Ratio | 56.50% | 121.43%. * BAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" samtools; ```. and. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" bcftools; ```. You can read more about samtools and bcftools here: http://www.htslib.org/doc/. ## Commands for requesting machines used in case studies. We report runtime in our case studies documentation. In order to make sure the; results we report are reproducible without too much variation, we provide the; commands we used here to show you what kind of machines we ran the case studies; on. This is NOT the fastest or cheapest configuration. ### Command for a CPU-only mach",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:10398,perform,performance-testdata,10398,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['perform'],['performance-testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: e; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 30x whole genome sample in BAM and CRAM format. Filetype | Size (Gb) | Runtime (min); -------- | --------- | -------------; BAM | 66.99 | 79m47.37307s; CRAM | 37.85 | 96m53.477s; Ratio | 56.50% | 121.43%. * BAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" samtools; ```. and. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" bcftools; ```. You can read more about samtools and bcftools here: http://www.htslib.org/doc/. ## Commands for requesting machines used in case studies. We report runtime in our case studies documentation. In order to make sure the; results we report are reproducible without too much variation, we provide the; commands we used here to show you what kind of machines we ran the case studies; on. This is NOT the fastest or cheapest configuration. ### Command for a CPU-only mach
",False,"The text contains complete sentences and explanatory content that describe file sizes, runtimes, tools used (samtools, bcftools), and commands related to performance testing. It also provides links for further information and context."
Performance,"eepVariant is highly accurate for; [many types of NGS data](https://rdcu.be/7Dhl), some users may be interested in; training custom deep learning models that have been optimized for very specific; data. This case study describes one way to train such a custom model using a GPU, in; this case for BGISEQ-500 data. Please note that there is not yet a production-grade training pipeline. This is; just one example of how to train a custom model, and is neither the fastest nor; the cheapest possible configuration. The resulting model also does not represent; the greatest achievable accuracy for BGISEQ-500 data. ## High level summary of result. We demonstrated that by training on 1 replicate of BGISEQ-500 whole genome data; (everything except for chromosome 20-22), we can significantly improve the; accuracy comparing to the WGS model as a baseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; following command:. ```bash; gcloud compute ssh ${host} --zone ${zone}; ```. Once you have logged in, set the variables:. ```bash; YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT; OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant""; VERSION=",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:1307,optimiz,optimization,1307,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['optimiz'],['optimization'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: eepVariant is highly accurate for; [many types of NGS data](https://rdcu.be/7Dhl), some users may be interested in; training custom deep learning models that have been optimized for very specific; data. This case study describes one way to train such a custom model using a GPU, in; this case for BGISEQ-500 data. Please note that there is not yet a production-grade training pipeline. This is; just one example of how to train a custom model, and is neither the fastest nor; the cheapest possible configuration. The resulting model also does not represent; the greatest achievable accuracy for BGISEQ-500 data. ## High level summary of result. We demonstrated that by training on 1 replicate of BGISEQ-500 whole genome data; (everything except for chromosome 20-22), we can significantly improve the; accuracy comparing to the WGS model as a baseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; following command:. ```bash; gcloud compute ssh ${host} --zone ${zone}; ```. Once you have logged in, set the variables:. ```bash; YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT; OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant""; VERSION=
",False,
Performance,"et.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; gsutil -m rm -f ""${OUTPUT_BUCKET}/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt""; gsutil rm -f ""${OUTPUT_BUCKET}/example_info.json""; ```. Here we provide examples for running on; [Cloud Dataflow Runner](https://beam.apache.org/documentation/runners/dataflow/); and also [DirectRunner](https://beam.apache.org/documentation/runners/direct/).; Beam can also use other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. Consult the instructions at https://beam.apache.org/get-started/quickstart-py/; if you run into any issues. Then, get the script that performs shuffling:. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. Next, we shuffle the data using DataflowRunner. Before that, please make sure; you enable Dataflow API for your project:; http://console.cloud.google.com/flows/enableapi?apiid=dataflow. To access `gs://` path, make sure you run this in your virtual environment:. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. Shuffle using Dataflow. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_patt",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:9261,perform,performs,9261,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['perform'],['performs'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: et.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; gsutil -m rm -f ""${OUTPUT_BUCKET}/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt""; gsutil rm -f ""${OUTPUT_BUCKET}/example_info.json""; ```. Here we provide examples for running on; [Cloud Dataflow Runner](https://beam.apache.org/documentation/runners/dataflow/); and also [DirectRunner](https://beam.apache.org/documentation/runners/direct/).; Beam can also use other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. Consult the instructions at https://beam.apache.org/get-started/quickstart-py/; if you run into any issues. Then, get the script that performs shuffling:. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. Next, we shuffle the data using DataflowRunner. Before that, please make sure; you enable Dataflow API for your project:; http://console.cloud.google.com/flows/enableapi?apiid=dataflow. To access `gs://` path, make sure you run this in your virtual environment:. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. Shuffle using Dataflow. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_patt
",False,
Performance,"gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. By specifying `--model_type ONT_R104`, you'll be using a model that is best; suited for Oxford Nanopore R10.4.1 chemistry Simplex and Duplex reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true` to; the command above to figure out what flags you need in each step. Based on the; different model types, different flags are needed in the `make_examples` step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. ## Benchmark HG002 chr20 output from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002. ```bash; FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. TRUTH_VCF=""HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz""; TRUTH_BED=""HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; j",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md:4149,perform,performance,4149,docs/deepvariant-ont-r104-duplex-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md,1,['perform'],['performance'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. By specifying `--model_type ONT_R104`, you'll be using a model that is best; suited for Oxford Nanopore R10.4.1 chemistry Simplex and Duplex reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true` to; the command above to figure out what flags you need in each step. Based on the; different model types, different flags are needed in the `make_examples` step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. ## Benchmark HG002 chr20 output from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002. ```bash; FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. TRUTH_VCF=""HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz""; TRUTH_BED=""HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; j
",False,The text contains human-readable sentences discussing concepts like benchmarking against GIAB dataset and steps for downloading data. There are no code snippets or logs present.
Performance,"gz`). ```bash; BIN_VERSION=""1.4.0"". sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; google/deepvariant:""${BIN_VERSION}"" \; run_deepvariant \; --model_type=WES \; --customized_model=model/model.ckpt \; --ref=reference/GRCh38_no_alt_analysis_set.fasta \; --reads=data/hg005_gm26107.mrna.grch38.bam \; --output_vcf=output/HG005.output.vcf.gz \; --num_shards=$(nproc) \; --regions=data/chr20_CDS_3x.bed \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir output/intermediate_results_dir; ```. **Flag summary**. * `--model_type` - Sets the model and options, but we will override the model; with `--customized model`.; * `--customized_model` - Points to a model trained using RNA-seq data.; * `--ref` - Specifies the reference sequence.; * `--reads` - Specifies the input bam file.; * `--output_vcf` - Specifies the output variant file.; * `--num_shards` - Sets the number of shards to the number of available; processors (`$(nproc)`). This is used to perform parallelization.; * `--regions` - Restricts analysis to 3x chr20 CDS regions only.; * `--make_examples_extra_args=` - Passes additional arguments to; make_examples.; * `split_skip_reads=true` - *Important!* This flag is critical for RNA-seq; variant calling to work properly. It enables RNA-seq data to be; processed efficiently.; * `channels=''` - Resets the channel list to be appropriate for the; RNA-seq model.; * `--intermediate_results_dir` - Outputs results to an intermediate directory. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; sudo docker run \; -v $(pwd):$(pwd) \; -w $(pwd) \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; output/HG005.output.vcf.gz \; -f benchmark/chr20_CDS_3x.benchmark_regions.bed \; -r reference/GRCh38_no_alt_analysis_set.fasta \; -o happy/happy.output \; --engine=vcfeval \; --pass-only \; --ta",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:8221,perform,perform,8221,docs/deepvariant-rnaseq-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md,1,['perform'],['perform'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: gz`). ```bash; BIN_VERSION=""1.4.0"". sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; google/deepvariant:""${BIN_VERSION}"" \; run_deepvariant \; --model_type=WES \; --customized_model=model/model.ckpt \; --ref=reference/GRCh38_no_alt_analysis_set.fasta \; --reads=data/hg005_gm26107.mrna.grch38.bam \; --output_vcf=output/HG005.output.vcf.gz \; --num_shards=$(nproc) \; --regions=data/chr20_CDS_3x.bed \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir output/intermediate_results_dir; ```. **Flag summary**. * `--model_type` - Sets the model and options, but we will override the model; with `--customized model`.; * `--customized_model` - Points to a model trained using RNA-seq data.; * `--ref` - Specifies the reference sequence.; * `--reads` - Specifies the input bam file.; * `--output_vcf` - Specifies the output variant file.; * `--num_shards` - Sets the number of shards to the number of available; processors (`$(nproc)`). This is used to perform parallelization.; * `--regions` - Restricts analysis to 3x chr20 CDS regions only.; * `--make_examples_extra_args=` - Passes additional arguments to; make_examples.; * `split_skip_reads=true` - *Important!* This flag is critical for RNA-seq; variant calling to work properly. It enables RNA-seq data to be; processed efficiently.; * `channels=''` - Resets the channel list to be appropriate for the; RNA-seq model.; * `--intermediate_results_dir` - Outputs results to an intermediate directory. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; sudo docker run \; -v $(pwd):$(pwd) \; -w $(pwd) \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; output/HG005.output.vcf.gz \; -f benchmark/chr20_CDS_3x.benchmark_regions.bed \; -r reference/GRCh38_no_alt_analysis_set.fasta \; -o happy/happy.output \; --engine=vcfeval \; --pass-only \; --ta
",False,"The text contains a bash command script with arguments and explanations. It includes a description of each flag, which is more of an explanatory content rather than purely code or logs. The presence of human-readable sentences discussing the function of each argument and flags makes this text appropriate to keep."
Performance,"h more information on; the input and output file formats and how to work with them.; * [Best practices for multi-sample variant calling with DeepVariant](docs/trio-merge-case-study.md); * [(Advanced) Training tutorial](docs/deepvariant-training-case-study.md); * [DeepVariant's Frequently Asked Questions, FAQ](docs/FAQ.md). ## How to cite. If you're using DeepVariant in your work, please cite:. [A universal SNP and small-indel variant caller using deep neural networks. *Nature Biotechnology* 36, 983–987 (2018).](https://rdcu.be/7Dhl) <br/>; Ryan Poplin, Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas Colthurst, Alexander Ku, Dan Newburger, Jojo Dijamco, Nam Nguyen, Pegah T. Afshar, Sam S. Gross, Lizzie Dorfman, Cory Y. McLean, and Mark A. DePristo.<br/>; doi: https://doi.org/10.1038/nbt.4235. Additionally, if you are generating multi-sample calls using our; [DeepVariant and GLnexus Best Practices](docs/trio-merge-case-study.md), please; cite:. [Accurate, scalable cohort variant calls using DeepVariant and GLnexus.; _Bioinformatics_ (2021).](https://doi.org/10.1093/bioinformatics/btaa1081)<br/>; Taedong Yun, Helen Li, Pi-Chuan Chang, Michael F. Lin, Andrew Carroll, and Cory; Y. McLean.<br/>; doi: https://doi.org/10.1093/bioinformatics/btaa1081. ## Why Use DeepVariant?. * **High accuracy** - DeepVariant won 2020; [PrecisionFDA Truth Challenge V2](https://precision.fda.gov/challenges/10/results); for All Benchmark Regions for ONT, PacBio, and Multiple Technologies; categories, and 2016; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results); for best SNP Performance. DeepVariant maintains high accuracy across data; from different sequencing technologies, prep methods, and species. For; [lower coverage](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/),; using DeepVariant makes an especially great difference. See; [metrics](docs/metrics.md) for",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:6428,scalab,scalable,6428,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['scalab'],['scalable'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: h more information on; the input and output file formats and how to work with them.; * [Best practices for multi-sample variant calling with DeepVariant](docs/trio-merge-case-study.md); * [(Advanced) Training tutorial](docs/deepvariant-training-case-study.md); * [DeepVariant's Frequently Asked Questions, FAQ](docs/FAQ.md). ## How to cite. If you're using DeepVariant in your work, please cite:. [A universal SNP and small-indel variant caller using deep neural networks. *Nature Biotechnology* 36, 983–987 (2018).](https://rdcu.be/7Dhl) <br/>; Ryan Poplin, Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas Colthurst, Alexander Ku, Dan Newburger, Jojo Dijamco, Nam Nguyen, Pegah T. Afshar, Sam S. Gross, Lizzie Dorfman, Cory Y. McLean, and Mark A. DePristo.<br/>; doi: https://doi.org/10.1038/nbt.4235. Additionally, if you are generating multi-sample calls using our; [DeepVariant and GLnexus Best Practices](docs/trio-merge-case-study.md), please; cite:. [Accurate, scalable cohort variant calls using DeepVariant and GLnexus.; _Bioinformatics_ (2021).](https://doi.org/10.1093/bioinformatics/btaa1081)<br/>; Taedong Yun, Helen Li, Pi-Chuan Chang, Michael F. Lin, Andrew Carroll, and Cory; Y. McLean.<br/>; doi: https://doi.org/10.1093/bioinformatics/btaa1081. ## Why Use DeepVariant?. * **High accuracy** - DeepVariant won 2020; [PrecisionFDA Truth Challenge V2](https://precision.fda.gov/challenges/10/results); for All Benchmark Regions for ONT, PacBio, and Multiple Technologies; categories, and 2016; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results); for best SNP Performance. DeepVariant maintains high accuracy across data; from different sequencing technologies, prep methods, and species. For; [lower coverage](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/),; using DeepVariant makes an especially great difference. See; [metrics](docs/metrics.md) for
",False,
Performance,"iant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DI",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14117,tune,tune,14117,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['tune'],['tune'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: iant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DI
",True,"The content consists of a script with command lines and parameters used for training. It includes code syntax, commands, and parameters that are typically found in programmatic instructions rather than human-readable explanations."
Performance,"ified by the; `--ref` flag, instead of the path to the original reference in the CRAM file; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 30x whole genome sample in BAM and CRAM format. Filetype | Size (Gb) | Runtime (min); -------- | --------- | -------------; BAM | 66.99 | 79m47.37307s; CRAM | 37.85 | 96m53.477s; Ratio | 56.50% | 121.43%. * BAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" samtools; ```. and. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" bcftools; ```. You can read more about samtools and bcftools here: http://www.htslib.org/doc/. ## Commands for requesting machines used in case studies. We report runtime in our case studies documentation. In order to make sure the; results we report are reproducible without too much variation, we provide the; commands we used here to show you what kind of machines we ran the case studie",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:10306,perform,performance-testdata,10306,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['perform'],['performance-testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ified by the; `--ref` flag, instead of the path to the original reference in the CRAM file; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 30x whole genome sample in BAM and CRAM format. Filetype | Size (Gb) | Runtime (min); -------- | --------- | -------------; BAM | 66.99 | 79m47.37307s; CRAM | 37.85 | 96m53.477s; Ratio | 56.50% | 121.43%. * BAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" samtools; ```. and. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" bcftools; ```. You can read more about samtools and bcftools here: http://www.htslib.org/doc/. ## Commands for requesting machines used in case studies. We report runtime in our case studies documentation. In order to make sure the; results we report are reproducible without too much variation, we provide the; commands we used here to show you what kind of machines we ran the case studie
",True,"The text contains command lines and code snippets (e.g., `docker run google/deepvariant:"
Performance,"n the; [Inputs and outputs](deepvariant-details.md#inputs-and-outputs) section of the; DeepVariant documentation. DeepTrio comes with three models for different types of input data:. * Illumina whole genome data (WGS).; * Illumina whole exome data (WES).; * PacBio HiFi whole genome data (PacBio WGS). ## Running DeepTrio. The easiest and recommended way to run DeepTrio is using; `google/deepvariant:deeptrio-latest` docker image. Please refer to the; [quick start guide](deeptrio-quick-start.md) for more details on how to run; DeepTrio using docker. Merging VCFs can be done using; [GLnexus](https://github.com/dnanexus-rnd/GLnexus) which has been optimized for; use with DeepVariant gVCFs. The process is described in the DeepTrio case; studies; ([DeepTrio whole genome sequencing case study](deeptrio-wgs-case-study.md) and; [Using DeepTrio for small variant calling from the trio sequenced with PacBio; HiFi](deeptrio-pacbio-case-study.md)), and in the manuscript,; [""Accurate, scalable cohort variant calls using DeepVariant and GLnexus""](https://www.biorxiv.org/content/10.1101/2020.02.10.942086v2). Please note that DeepTrio can be run with a `run_deeptrio.py` script that; automates all DeepTrio steps and thus greatly simplifies the inference pipeline.; The details of using this script can be found in the section below as well as in; the DeepTrio case studies. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we; recommend running these providing only the parent who contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" v",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:2509,scalab,scalable,2509,docs/deeptrio-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md,1,['scalab'],['scalable'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: n the; [Inputs and outputs](deepvariant-details.md#inputs-and-outputs) section of the; DeepVariant documentation. DeepTrio comes with three models for different types of input data:. * Illumina whole genome data (WGS).; * Illumina whole exome data (WES).; * PacBio HiFi whole genome data (PacBio WGS). ## Running DeepTrio. The easiest and recommended way to run DeepTrio is using; `google/deepvariant:deeptrio-latest` docker image. Please refer to the; [quick start guide](deeptrio-quick-start.md) for more details on how to run; DeepTrio using docker. Merging VCFs can be done using; [GLnexus](https://github.com/dnanexus-rnd/GLnexus) which has been optimized for; use with DeepVariant gVCFs. The process is described in the DeepTrio case; studies; ([DeepTrio whole genome sequencing case study](deeptrio-wgs-case-study.md) and; [Using DeepTrio for small variant calling from the trio sequenced with PacBio; HiFi](deeptrio-pacbio-case-study.md)), and in the manuscript,; [""Accurate, scalable cohort variant calls using DeepVariant and GLnexus""](https://www.biorxiv.org/content/10.1101/2020.02.10.942086v2). Please note that DeepTrio can be run with a `run_deeptrio.py` script that; automates all DeepTrio steps and thus greatly simplifies the inference pipeline.; The details of using this script can be found in the section below as well as in; the DeepTrio case studies. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we; recommend running these providing only the parent who contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" v
",False,
Performance,"nally, each read is then realigned to its most likely; > haplotype. This procedure updates both the position and the CIGAR string for; > each read. Local realignment is not performed for long reads (PacBio, and other similar; technologies). The realigner step can optionally be switched off using; `--norealign_reads`. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. This can be done by passing the following parameters:; `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`. Note that this is meant for debugging and produces a bam file for every; candidate variant, which can result in millions of tiny bam files, so when using; this, narrow down the DeepVariant run using `--regions` to just the variants you; want to inspect more closely. ## How are `AD` and `DP` values calculated?. In order to efficiently perform variant calling, DeepVariant partitions the; genome into chunks (set by `--partition_size`), and will read in a max number of; reads into each partition (set by `--max_reads_per_partition`). By default,; `--partition_size` is set to 1000 and `--max_reads_per_partition` is set to; 1500. The `AD` and `DP` values are based on the read depths constrained by; `--max_reads_per_partition`. For example, if you have a depth of 2000x at a given site, DeepVariant will; subsample 1500 reads, and `DP` or `AD` will be capped at 1500. If you want to; calculate the true `AD` and `DP` values at high-depth regions, you can set; `--max_reads_per_partition=0` to calculate `AD` and `DP` using all reads. In; practice, capping reads per partition reduces runtimes with little/no impact on; accuracy. ## Missing variant calls near the edge of a contig. This is a known issue that we don't currently address. Please see:; https://github.com/google/deepvariant/issues/505 for more context. ## Why does DeepVariant PASS variants that have such a low read depth ~2 ?. Please see the answers provid",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:12713,perform,perform,12713,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['perform'],['perform'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: nally, each read is then realigned to its most likely; > haplotype. This procedure updates both the position and the CIGAR string for; > each read. Local realignment is not performed for long reads (PacBio, and other similar; technologies). The realigner step can optionally be switched off using; `--norealign_reads`. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. This can be done by passing the following parameters:; `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`. Note that this is meant for debugging and produces a bam file for every; candidate variant, which can result in millions of tiny bam files, so when using; this, narrow down the DeepVariant run using `--regions` to just the variants you; want to inspect more closely. ## How are `AD` and `DP` values calculated?. In order to efficiently perform variant calling, DeepVariant partitions the; genome into chunks (set by `--partition_size`), and will read in a max number of; reads into each partition (set by `--max_reads_per_partition`). By default,; `--partition_size` is set to 1000 and `--max_reads_per_partition` is set to; 1500. The `AD` and `DP` values are based on the read depths constrained by; `--max_reads_per_partition`. For example, if you have a depth of 2000x at a given site, DeepVariant will; subsample 1500 reads, and `DP` or `AD` will be capped at 1500. If you want to; calculate the true `AD` and `DP` values at high-depth regions, you can set; `--max_reads_per_partition=0` to calculate `AD` and `DP` using all reads. In; practice, capping reads per partition reduces runtimes with little/no impact on; accuracy. ## Missing variant calls near the edge of a contig. This is a known issue that we don't currently address. Please see:; https://github.com/google/deepvariant/issues/505 for more context. ## Why does DeepVariant PASS variants that have such a low read depth ~2 ?. Please see the answers provid
",False,"The text contains complete, meaningful sentences in natural language discussing concepts and ideas related to variant calling in DeepVariant."
Performance,"o see how you can visualize the pileup; images. ## Training data over time. For the models we've released over time, you can find more details about the; training data in; [DeepVariant training data](deepvariant-details-training-data.md). ## CRAM support. As of v0.7, DeepVariant accepts CRAM files as input in addition to BAM files. As of v0.9.0, we changed the default to use the reference file specified by the; `--ref` flag, instead of the path to the original reference in the CRAM file; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 30x whole genome sample in BAM and CRAM format. Filetype | Size (Gb) | Runtime (min); -------- | --------- | -------------; BAM | 66.99 | 79m47.37307s; CRAM | 37.85 | 96m53.477s; Ratio | 56.50% | 121.43%. * BAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" samtools; ```. and. ```bash; docker run google/deepvariant:""${BIN",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:9960,perform,performance,9960,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['perform'],['performance'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: o see how you can visualize the pileup; images. ## Training data over time. For the models we've released over time, you can find more details about the; training data in; [DeepVariant training data](deepvariant-details-training-data.md). ## CRAM support. As of v0.7, DeepVariant accepts CRAM files as input in addition to BAM files. As of v0.9.0, we changed the default to use the reference file specified by the; `--ref` flag, instead of the path to the original reference in the CRAM file; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 30x whole genome sample in BAM and CRAM format. Filetype | Size (Gb) | Runtime (min); -------- | --------- | -------------; BAM | 66.99 | 79m47.37307s; CRAM | 37.85 | 96m53.477s; Ratio | 56.50% | 121.43%. * BAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" samtools; ```. and. ```bash; docker run google/deepvariant:""${BIN
",False,
Performance,"omosomes (X and Y), we; recommend running these providing only the parent who contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot pipe them into DeepTrio. Duplicate marking may be; performed. In our analyses, there is almost no difference in accuracy with and; without duplicate marking except at lower (<20x) coverages. Finally, we; recommend that you do not perform BQSR. Running BQSR has a small decrease on; accuracy. If you are providing `--regions` or other similar arguments, these should refer; to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. ## Training data. DeepTrio models are trained using the latest publicly avavilable GIAB; benchmarks. You can find more details about the training data for each DeepTrio; model in the; [DeepTrio Training Data document](deeptrio-details-training-data.md). ## DeepVariant dependency. DeepTrio is built on top of DeepVariant and they share most of the components.; Please see [DeepVariant usage guide](deepvariant-details.md) for a full; description of DeepVariant c",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:3969,perform,perform,3969,docs/deeptrio-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md,1,['perform'],['perform'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: omosomes (X and Y), we; recommend running these providing only the parent who contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot pipe them into DeepTrio. Duplicate marking may be; performed. In our analyses, there is almost no difference in accuracy with and; without duplicate marking except at lower (<20x) coverages. Finally, we; recommend that you do not perform BQSR. Running BQSR has a small decrease on; accuracy. If you are providing `--regions` or other similar arguments, these should refer; to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. ## Training data. DeepTrio models are trained using the latest publicly avavilable GIAB; benchmarks. You can find more details about the training data for each DeepTrio; model in the; [DeepTrio Training Data document](deeptrio-details-training-data.md). ## DeepVariant dependency. DeepTrio is built on top of DeepVariant and they share most of the components.; Please see [DeepVariant usage guide](deepvariant-details.md) for a full; description of DeepVariant c
",False,"The text contains complete, meaningful sentences in natural language, such as explaining input assumptions, recommendations, and training data. It also includes human-readable content discussing concepts and ideas related to the tool's usage."
Performance,"one or two parents. Similar to DeepVariant, DeepTrio is composed of three stages: `make_examples`,; `call_variants`, and `postprocess_variants`. Some of the components (; `call_variants`, `postprocess_variants`) are shared with DeepVariant, and; `make_examples` is specialized for DeepTrio. More details about each program are; described in detail in the; [Inputs and outputs](deepvariant-details.md#inputs-and-outputs) section of the; DeepVariant documentation. DeepTrio comes with three models for different types of input data:. * Illumina whole genome data (WGS).; * Illumina whole exome data (WES).; * PacBio HiFi whole genome data (PacBio WGS). ## Running DeepTrio. The easiest and recommended way to run DeepTrio is using; `google/deepvariant:deeptrio-latest` docker image. Please refer to the; [quick start guide](deeptrio-quick-start.md) for more details on how to run; DeepTrio using docker. Merging VCFs can be done using; [GLnexus](https://github.com/dnanexus-rnd/GLnexus) which has been optimized for; use with DeepVariant gVCFs. The process is described in the DeepTrio case; studies; ([DeepTrio whole genome sequencing case study](deeptrio-wgs-case-study.md) and; [Using DeepTrio for small variant calling from the trio sequenced with PacBio; HiFi](deeptrio-pacbio-case-study.md)), and in the manuscript,; [""Accurate, scalable cohort variant calls using DeepVariant and GLnexus""](https://www.biorxiv.org/content/10.1101/2020.02.10.942086v2). Please note that DeepTrio can be run with a `run_deeptrio.py` script that; automates all DeepTrio steps and thus greatly simplifies the inference pipeline.; The details of using this script can be found in the section below as well as in; the DeepTrio case studies. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we; recommend running these providing only the parent who contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples)",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:2176,optimiz,optimized,2176,docs/deeptrio-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md,1,['optimiz'],['optimized'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: one or two parents. Similar to DeepVariant, DeepTrio is composed of three stages: `make_examples`,; `call_variants`, and `postprocess_variants`. Some of the components (; `call_variants`, `postprocess_variants`) are shared with DeepVariant, and; `make_examples` is specialized for DeepTrio. More details about each program are; described in detail in the; [Inputs and outputs](deepvariant-details.md#inputs-and-outputs) section of the; DeepVariant documentation. DeepTrio comes with three models for different types of input data:. * Illumina whole genome data (WGS).; * Illumina whole exome data (WES).; * PacBio HiFi whole genome data (PacBio WGS). ## Running DeepTrio. The easiest and recommended way to run DeepTrio is using; `google/deepvariant:deeptrio-latest` docker image. Please refer to the; [quick start guide](deeptrio-quick-start.md) for more details on how to run; DeepTrio using docker. Merging VCFs can be done using; [GLnexus](https://github.com/dnanexus-rnd/GLnexus) which has been optimized for; use with DeepVariant gVCFs. The process is described in the DeepTrio case; studies; ([DeepTrio whole genome sequencing case study](deeptrio-wgs-case-study.md) and; [Using DeepTrio for small variant calling from the trio sequenced with PacBio; HiFi](deeptrio-pacbio-case-study.md)), and in the manuscript,; [""Accurate, scalable cohort variant calls using DeepVariant and GLnexus""](https://www.biorxiv.org/content/10.1101/2020.02.10.942086v2). Please note that DeepTrio can be run with a `run_deeptrio.py` script that; automates all DeepTrio steps and thus greatly simplifies the inference pipeline.; The details of using this script can be found in the section below as well as in; the DeepTrio case studies. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we; recommend running these providing only the parent who contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples)
",True,"The text consists primarily of code snippets or programmatic descriptions, including mentions of stages (`make_examples`, `call_variants`, etc.), usage instructions, and references to specific scripts and tools. While there are some descriptive content, the dominant focus is on the technical aspects related to running and configuring DeepTrio, which aligns more with API documentation or build system output."
Performance,"peline.; The details of using this script can be found in the section below as well as in; the DeepTrio case studies. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we; recommend running these providing only the parent who contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot pipe them into DeepTrio. Duplicate marking may be; performed. In our analyses, there is almost no difference in accuracy with and; without duplicate marking except at lower (<20x) coverages. Finally, we; recommend that you do not perform BQSR. Running BQSR has a small decrease on; accuracy. If you are providing `--regions` or other similar arguments, these should refer; to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. ## Training data. DeepTrio models are trained using the latest publicly avavilable GIAB; benchmarks. You can find more details about the training data for each DeepTrio; model in the; [DeepTrio Training Data document](deeptrio-details-training-data.md). ## DeepVariant dependency. DeepTri",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:3790,perform,performed,3790,docs/deeptrio-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md,1,['perform'],['performed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: peline.; The details of using this script can be found in the section below as well as in; the DeepTrio case studies. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we; recommend running these providing only the parent who contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot pipe them into DeepTrio. Duplicate marking may be; performed. In our analyses, there is almost no difference in accuracy with and; without duplicate marking except at lower (<20x) coverages. Finally, we; recommend that you do not perform BQSR. Running BQSR has a small decrease on; accuracy. If you are providing `--regions` or other similar arguments, these should refer; to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. ## Training data. DeepTrio models are trained using the latest publicly avavilable GIAB; benchmarks. You can find more details about the training data for each DeepTrio; model in the; [DeepTrio Training Data document](deeptrio-details-training-data.md). ## DeepVariant dependency. DeepTri
",False,"The text contains complete, meaningful sentences in natural language discussing concepts and ideas related to using the script, input assumptions, training data, and dependencies."
Performance,"pendently. De Bruijn graphs are constructed using multiple fixed k-mer; > sizes (from 20 to 75, inclusive, with increments of 5) out of the reference; > genome bases for the candidate window, as well as all overlapping reads. Edges; > are given a weight determined by how many times they are observed in the; > reads. We trim any edges with weight less than three, except that edges found; > in the reference are never trimmed. Candidate haplotypes are generated by; > traversing the assembly graphs and the top two most likely haplotypes are; > selected that best explain the read evidence. The likelihood function used to; > score haplotypes is a traditional pair HMM with fixed parameters that do not; > depend on base quality scores. This likelihood function assumes that each read; > is independent. Finally, each read is then realigned to its most likely; > haplotype. This procedure updates both the position and the CIGAR string for; > each read. Local realignment is not performed for long reads (PacBio, and other similar; technologies). The realigner step can optionally be switched off using; `--norealign_reads`. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. This can be done by passing the following parameters:; `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`. Note that this is meant for debugging and produces a bam file for every; candidate variant, which can result in millions of tiny bam files, so when using; this, narrow down the DeepVariant run using `--regions` to just the variants you; want to inspect more closely. ## How are `AD` and `DP` values calculated?. In order to efficiently perform variant calling, DeepVariant partitions the; genome into chunks (set by `--partition_size`), and will read in a max number of; reads into each partition (set by `--max_reads_per_partition`). By default,; `--partition_size` is set to 1000 and `--max_reads_per_partition` is",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:11972,perform,performed,11972,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['perform'],['performed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: pendently. De Bruijn graphs are constructed using multiple fixed k-mer; > sizes (from 20 to 75, inclusive, with increments of 5) out of the reference; > genome bases for the candidate window, as well as all overlapping reads. Edges; > are given a weight determined by how many times they are observed in the; > reads. We trim any edges with weight less than three, except that edges found; > in the reference are never trimmed. Candidate haplotypes are generated by; > traversing the assembly graphs and the top two most likely haplotypes are; > selected that best explain the read evidence. The likelihood function used to; > score haplotypes is a traditional pair HMM with fixed parameters that do not; > depend on base quality scores. This likelihood function assumes that each read; > is independent. Finally, each read is then realigned to its most likely; > haplotype. This procedure updates both the position and the CIGAR string for; > each read. Local realignment is not performed for long reads (PacBio, and other similar; technologies). The realigner step can optionally be switched off using; `--norealign_reads`. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. This can be done by passing the following parameters:; `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`. Note that this is meant for debugging and produces a bam file for every; candidate variant, which can result in millions of tiny bam files, so when using; this, narrow down the DeepVariant run using `--regions` to just the variants you; want to inspect more closely. ## How are `AD` and `DP` values calculated?. In order to efficiently perform variant calling, DeepVariant partitions the; genome into chunks (set by `--partition_size`), and will read in a max number of; reads into each partition (set by `--max_reads_per_partition`). By default,; `--partition_size` is set to 1000 and `--max_reads_per_partition` is
",False,"The text contains complete sentences discussing the process of variant calling in DeepVariant. It includes explanatory content about parameters and their functions, which are meaningful for human readers."
Performance,"res its input files to satisfy a few basic requirements to; be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be; indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible""; version of the genome reference provided as the `--ref`. By compatible here we; mean the BAM and FASTA share at least a reasonable set of common contigs, as; DeepVariant will only process contigs shared by both the BAM and reference. As; an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you; provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only; process variants on the shared contigs, effectively excluding the hs37d5 contig; present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you; cannot pipe it into DeepVariant. Duplicate marking may be performed, in our; analyses there is almost no difference in accuracy except at lower (<20x); coverages. Finally, we recommend that you do not perform BQSR. Running BQSR has; a small decrease on accuracy. It is not necessary to do any form of indel; realignment, though there is not a difference in DeepVariant accuracy either; way. Third, if you are providing `--regions` or other similar arguments these should; refer to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and; `confident_regions` arguments should point to VCF and BED files containing the; true variants and regions where we are confident in our calls (i.e., calls; within these",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:3513,perform,performed,3513,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['perform'],['performed'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: res its input files to satisfy a few basic requirements to; be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be; indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible""; version of the genome reference provided as the `--ref`. By compatible here we; mean the BAM and FASTA share at least a reasonable set of common contigs, as; DeepVariant will only process contigs shared by both the BAM and reference. As; an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you; provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only; process variants on the shared contigs, effectively excluding the hs37d5 contig; present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you; cannot pipe it into DeepVariant. Duplicate marking may be performed, in our; analyses there is almost no difference in accuracy except at lower (<20x); coverages. Finally, we recommend that you do not perform BQSR. Running BQSR has; a small decrease on accuracy. It is not necessary to do any form of indel; realignment, though there is not a difference in DeepVariant accuracy either; way. Third, if you are providing `--regions` or other similar arguments these should; refer to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and; `confident_regions` arguments should point to VCF and BED files containing the; true variants and regions where we are confident in our calls (i.e., calls; within these
",False,"The text contains complete, meaningful sentences discussing requirements for processing files with DeepVariant. It includes explanatory content on how to prepare the reference genome, alignment of BAM files, handling of regions, and specific recommendations about arguments and VCF/BED files."
Performance,"sionFDA Truth Challenge V2](https://precision.fda.gov/challenges/10/results); for All Benchmark Regions for ONT, PacBio, and Multiple Technologies; categories, and 2016; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results); for best SNP Performance. DeepVariant maintains high accuracy across data; from different sequencing technologies, prep methods, and species. For; [lower coverage](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/),; using DeepVariant makes an especially great difference. See; [metrics](docs/metrics.md) for the latest accuracy numbers on each of the; sequencing types.; * **Flexibility** - Out-of-the-box use for; [PCR-positive](https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html); samples and; [low quality sequencing runs](https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/),; and easy adjustments for; [different sequencing technologies](https://google.github.io/deepvariant/posts/2019-01-14-highly-accurate-snp-and-indel-calling-on-pacbio-ccs-with-deepvariant/); and; [non-human species](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).; * **Ease of use** - No filtering is needed beyond setting your preferred; minimum quality threshold.; * **Cost effectiveness** - With a single non-preemptible n1-standard-16; machine on Google Cloud, it costs ~$11.8 to call a 30x whole genome and; ~$0.89 to call an exome. With preemptible pricing, the cost is $2.84 for a; 30x whole genome and $0.21 for whole exome (not considering preemption).; * **Speed** - See [metrics](docs/metrics.md) for the runtime of all supported; datatypes on a 64-core CPU-only machine</sup>. Multiple options for; acceleration exist.; * **Usage options** - DeepVariant can be run via Docker or binaries, using; both on",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:7749,perform,performance-of-ngs-pipelines-on-noisy-wgs-data,7749,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['perform'],['performance-of-ngs-pipelines-on-noisy-wgs-data'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: sionFDA Truth Challenge V2](https://precision.fda.gov/challenges/10/results); for All Benchmark Regions for ONT, PacBio, and Multiple Technologies; categories, and 2016; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results); for best SNP Performance. DeepVariant maintains high accuracy across data; from different sequencing technologies, prep methods, and species. For; [lower coverage](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/),; using DeepVariant makes an especially great difference. See; [metrics](docs/metrics.md) for the latest accuracy numbers on each of the; sequencing types.; * **Flexibility** - Out-of-the-box use for; [PCR-positive](https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html); samples and; [low quality sequencing runs](https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/),; and easy adjustments for; [different sequencing technologies](https://google.github.io/deepvariant/posts/2019-01-14-highly-accurate-snp-and-indel-calling-on-pacbio-ccs-with-deepvariant/); and; [non-human species](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).; * **Ease of use** - No filtering is needed beyond setting your preferred; minimum quality threshold.; * **Cost effectiveness** - With a single non-preemptible n1-standard-16; machine on Google Cloud, it costs ~$11.8 to call a 30x whole genome and; ~$0.89 to call an exome. With preemptible pricing, the cost is $2.84 for a; 30x whole genome and $0.21 for whole exome (not considering preemption).; * **Speed** - See [metrics](docs/metrics.md) for the runtime of all supported; datatypes on a 64-core CPU-only machine</sup>. Multiple options for; acceleration exist.; * **Usage options** - DeepVariant can be run via Docker or binaries, using; both on
",False,"The text contains multiple links and technical descriptions about sequencing technologies, which is more than just code. It includes explanations of features like flexibility, ease of use, cost effectiveness, speed, and usage options, which are meaningful sentences discussing concepts in natural language."
Performance,"the reference genome FASTA, passed in using the `--ref` flag, must be; indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible""; version of the genome reference provided as the `--ref`. By compatible here we; mean the BAM and FASTA share at least a reasonable set of common contigs, as; DeepVariant will only process contigs shared by both the BAM and reference. As; an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you; provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only; process variants on the shared contigs, effectively excluding the hs37d5 contig; present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you; cannot pipe it into DeepVariant. Duplicate marking may be performed, in our; analyses there is almost no difference in accuracy except at lower (<20x); coverages. Finally, we recommend that you do not perform BQSR. Running BQSR has; a small decrease on accuracy. It is not necessary to do any form of indel; realignment, though there is not a difference in DeepVariant accuracy either; way. Third, if you are providing `--regions` or other similar arguments these should; refer to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and; `confident_regions` arguments should point to VCF and BED files containing the; true variants and regions where we are confident in our calls (i.e., calls; within these regions and not in the truth_vcf are considered false positives).; These should be bgzippe",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:3656,perform,perform,3656,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['perform'],['perform'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: the reference genome FASTA, passed in using the `--ref` flag, must be; indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible""; version of the genome reference provided as the `--ref`. By compatible here we; mean the BAM and FASTA share at least a reasonable set of common contigs, as; DeepVariant will only process contigs shared by both the BAM and reference. As; an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you; provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only; process variants on the shared contigs, effectively excluding the hs37d5 contig; present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you; cannot pipe it into DeepVariant. Duplicate marking may be performed, in our; analyses there is almost no difference in accuracy except at lower (<20x); coverages. Finally, we recommend that you do not perform BQSR. Running BQSR has; a small decrease on accuracy. It is not necessary to do any form of indel; realignment, though there is not a difference in DeepVariant accuracy either; way. Third, if you are providing `--regions` or other similar arguments these should; refer to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and; `confident_regions` arguments should point to VCF and BED files containing the; true variants and regions where we are confident in our calls (i.e., calls; within these regions and not in the truth_vcf are considered false positives).; These should be bgzippe
",False,
Performance,"this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOI",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14189,tune,tune,14189,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['tune'],['tune'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOI
",True,"The text consists primarily of code snippets and command lines, which are not meaningful human-readable sentences. There is also a mix of API descriptions related to training configurations and parameters."
Performance,"une_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14569,perform,performing,14569,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['perform'],['performing'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: une_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary
",False,"The text contains complete sentences discussing training parameters, steps, and evaluation methods in natural language. It includes explanatory content about configurations and checkpoints without any code snippets or logs."
Performance,"vcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. By specifying `--model_type ONT_R104`, you'll be using a model that is best; suited for Oxford Nanopore R10.4.1 chemistry Simplex and Duplex reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. ## Benchmark HG003 chr20 output from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. TRUTH_VCF=""HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz""; TRUTH_BED=""HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md:3945,perform,performance,3945,docs/deepvariant-ont-r104-simplex-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md,1,['perform'],['performance'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: vcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. By specifying `--model_type ONT_R104`, you'll be using a model that is best; suited for Oxford Nanopore R10.4.1 chemistry Simplex and Duplex reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. ## Benchmark HG003 chr20 output from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. TRUTH_VCF=""HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz""; TRUTH_BED=""HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \
",False,
Performance,"ype implementation for somatic calling, which can take a tumor and normal; BAM and call subclonal variants. However, we don't yet have enough confidence in; the available truth sets, and that they come from a diverse enough sampling of; cancers with mutational profiles, for us to be certain in releasing something of; high quality. We're watching developments in the area of these truth sets and; hope to be able to further develop the somatic caller in the future. ## Can I use DeepVariant on plant genomes?. DeepVariant has previously been applied to plant species. In the case of rice,; there was good evidence of high accuracy. You can see; [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant).; However, these rice genomes were diploid and with a similar variant density of; humans. DeepVariant is currently written to be a diploid variant caller. So if the plant; species you are working with is polyploid, it is not yet clear how DeepVariant; will perform. That is because even with re-training, DeepVariant can only; produce variant calls that are homozygous alternate, heterozygous, or homozygous; reference, which don't have much meaning in a tetraploid genome, for example. ## Can I use DeepVariant on other non-human species?. See this; [blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). ## How do I build/run DeepVariant?. In general, we recommend running DeepVariant using Docker for the simplest; setup. If you are building from source because you want to experiment with; changes to the codebase, we still recommend Docker. You can clone the; DeepVariant repo, modify the source code, and build a Docker image with your; changes using the provided Dockerfile. ## Why can't it find one of the input files? E.g., ""Could not open"". This often happens because the way Docker works, input and output di",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:6630,perform,perform,6630,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['perform'],['perform'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ype implementation for somatic calling, which can take a tumor and normal; BAM and call subclonal variants. However, we don't yet have enough confidence in; the available truth sets, and that they come from a diverse enough sampling of; cancers with mutational profiles, for us to be certain in releasing something of; high quality. We're watching developments in the area of these truth sets and; hope to be able to further develop the somatic caller in the future. ## Can I use DeepVariant on plant genomes?. DeepVariant has previously been applied to plant species. In the case of rice,; there was good evidence of high accuracy. You can see; [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant).; However, these rice genomes were diploid and with a similar variant density of; humans. DeepVariant is currently written to be a diploid variant caller. So if the plant; species you are working with is polyploid, it is not yet clear how DeepVariant; will perform. That is because even with re-training, DeepVariant can only; produce variant calls that are homozygous alternate, heterozygous, or homozygous; reference, which don't have much meaning in a tetraploid genome, for example. ## Can I use DeepVariant on other non-human species?. See this; [blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). ## How do I build/run DeepVariant?. In general, we recommend running DeepVariant using Docker for the simplest; setup. If you are building from source because you want to experiment with; changes to the codebase, we still recommend Docker. You can clone the; DeepVariant repo, modify the source code, and build a Docker image with your; changes using the provided Dockerfile. ## Why can't it find one of the input files? E.g., ""Could not open"". This often happens because the way Docker works, input and output di
",False,"The text contains complete sentences discussing the use of DeepVariant for variant calling in different species and setup instructions. It includes human-readable explanations, technical considerations, and helpful guides."
Safety,"rshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The models included with DeepVariant are only trained on human data. For; other organisms, see the; [blog post on non-human variant-calling](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/); for some possible pitfalls and how to handle them. ## DeepTrio. DeepTrio is a deep learning-based trio variant caller built on top of; DeepVariant. DeepTrio extends DeepVariant's functionality, allowing it to; utilize the power of neural networks to predict genomic variants in trios or; duos. See [this page](docs/deeptrio-details.md) for more details and; instructions on how to run DeepTrio. DeepTrio supports germline variant-calling in diploid organisms for the; following types of input data:. * NGS (Illumina) data for either; [whole genome](docs/deeptrio-wgs-case-study.md) or whole exome.; * PacBio HiFi data, see the; [PacBio case study](docs/deeptrio-pacbio-case-study.md). Please also note:. * All DeepTrio models were trained on human data.; * It is possible to use DeepTrio with only 2 samples (child, and one parent).; * External tool [GLnexus](https://github.com/dnanexus-rnd/GLnexus) is used to; merge output VCFs. ## How to run DeepVariant. We recommend using our Docker solution. The command will look like this:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --mo",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:2519,predict,predict,2519,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['predict'],['predict'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: rshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The models included with DeepVariant are only trained on human data. For; other organisms, see the; [blog post on non-human variant-calling](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/); for some possible pitfalls and how to handle them. ## DeepTrio. DeepTrio is a deep learning-based trio variant caller built on top of; DeepVariant. DeepTrio extends DeepVariant's functionality, allowing it to; utilize the power of neural networks to predict genomic variants in trios or; duos. See [this page](docs/deeptrio-details.md) for more details and; instructions on how to run DeepTrio. DeepTrio supports germline variant-calling in diploid organisms for the; following types of input data:. * NGS (Illumina) data for either; [whole genome](docs/deeptrio-wgs-case-study.md) or whole exome.; * PacBio HiFi data, see the; [PacBio case study](docs/deeptrio-pacbio-case-study.md). Please also note:. * All DeepTrio models were trained on human data.; * It is possible to use DeepTrio with only 2 samples (child, and one parent).; * External tool [GLnexus](https://github.com/dnanexus-rnd/GLnexus) is used to; merge output VCFs. ## How to run DeepVariant. We recommend using our Docker solution. The command will look like this:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --mo
",False,"The provided text includes a mix of natural language explanations and some technical commands, but it doesn't primarily consist of code or logs. It is an informative document discussing the usage and capabilities of DeepVariant and related tools, which should be kept as meaningful content for human readers."
Safety,"tion set. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. This took: 5m31.122s. Copy to GCS bucket:. ```bash; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. ## Shuffle each set of examples and generate a data configuration file for each. Shuffling the `tensorflow.Example`s is an important step for training a model.; In our training logic, we shuffle examples globally using a preprocessing step. First, if you have run this step before, and want to rerun it, you might want to; consider cleaning up previous data first to avoid confusion:. ```bash; # (Optional) Clean up existing files.; gsutil -m rm -f ""${OUTPUT_BUCKET}/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; gsutil -m rm -f ""${OUTPUT_BUCKET}/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt""; gsutil rm -f ""${OUTPUT_BUCKET}/example_info.json""; ```. Here we provide examples for running on; [Cloud Dataflow Runner](https://beam.apache.org/documentation/runners/dataflow/); and also [DirectRunner](https://beam.apache.org/documentation/runners/direct/).; Beam can also use other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the vir",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:8147,avoid,avoid,8147,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['avoid'],['avoid'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: tion set. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. This took: 5m31.122s. Copy to GCS bucket:. ```bash; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. ## Shuffle each set of examples and generate a data configuration file for each. Shuffling the `tensorflow.Example`s is an important step for training a model.; In our training logic, we shuffle examples globally using a preprocessing step. First, if you have run this step before, and want to rerun it, you might want to; consider cleaning up previous data first to avoid confusion:. ```bash; # (Optional) Clean up existing files.; gsutil -m rm -f ""${OUTPUT_BUCKET}/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; gsutil -m rm -f ""${OUTPUT_BUCKET}/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt""; gsutil rm -f ""${OUTPUT_BUCKET}/example_info.json""; ```. Here we provide examples for running on; [Cloud Dataflow Runner](https://beam.apache.org/documentation/runners/dataflow/); and also [DirectRunner](https://beam.apache.org/documentation/runners/direct/).; Beam can also use other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the vir
",False,
Safety,"un using `--regions` to just the variants you; want to inspect more closely. ## How are `AD` and `DP` values calculated?. In order to efficiently perform variant calling, DeepVariant partitions the; genome into chunks (set by `--partition_size`), and will read in a max number of; reads into each partition (set by `--max_reads_per_partition`). By default,; `--partition_size` is set to 1000 and `--max_reads_per_partition` is set to; 1500. The `AD` and `DP` values are based on the read depths constrained by; `--max_reads_per_partition`. For example, if you have a depth of 2000x at a given site, DeepVariant will; subsample 1500 reads, and `DP` or `AD` will be capped at 1500. If you want to; calculate the true `AD` and `DP` values at high-depth regions, you can set; `--max_reads_per_partition=0` to calculate `AD` and `DP` using all reads. In; practice, capping reads per partition reduces runtimes with little/no impact on; accuracy. ## Missing variant calls near the edge of a contig. This is a known issue that we don't currently address. Please see:; https://github.com/google/deepvariant/issues/505 for more context. ## Why does DeepVariant PASS variants that have such a low read depth ~2 ?. Please see the answers provided by [Paul Grosu](https://github.com/pgrosu) in; this [issue thread](https://github.com/google/deepvariant/issues/684). We thank; Paul for providing a detailed description and reasoning. ## Singularity related questions:. ### `TMPDIR`. If you have issues with `TMPDIR` when running with Singularity, try adding this; to your command:. ```bash; export TMPDIR=""$PWD/tmp_dir""; ```. See https://github.com/google/deepvariant/issues/524#issuecomment-1067597987. ### Issues with `/mnt/`. User reported that sometimes their setup uses `/mnt/`, which exists in our; Docker image, and it has caused an issue in Singularity. You can use `-B` in Singularity to avoid this issue. See:; https://github.com/google/deepvariant/issues/530#issuecomment-1076923302 for; more details. ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:14451,avoid,avoid,14451,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['avoid'],['avoid'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: un using `--regions` to just the variants you; want to inspect more closely. ## How are `AD` and `DP` values calculated?. In order to efficiently perform variant calling, DeepVariant partitions the; genome into chunks (set by `--partition_size`), and will read in a max number of; reads into each partition (set by `--max_reads_per_partition`). By default,; `--partition_size` is set to 1000 and `--max_reads_per_partition` is set to; 1500. The `AD` and `DP` values are based on the read depths constrained by; `--max_reads_per_partition`. For example, if you have a depth of 2000x at a given site, DeepVariant will; subsample 1500 reads, and `DP` or `AD` will be capped at 1500. If you want to; calculate the true `AD` and `DP` values at high-depth regions, you can set; `--max_reads_per_partition=0` to calculate `AD` and `DP` using all reads. In; practice, capping reads per partition reduces runtimes with little/no impact on; accuracy. ## Missing variant calls near the edge of a contig. This is a known issue that we don't currently address. Please see:; https://github.com/google/deepvariant/issues/505 for more context. ## Why does DeepVariant PASS variants that have such a low read depth ~2 ?. Please see the answers provided by [Paul Grosu](https://github.com/pgrosu) in; this [issue thread](https://github.com/google/deepvariant/issues/684). We thank; Paul for providing a detailed description and reasoning. ## Singularity related questions:. ### `TMPDIR`. If you have issues with `TMPDIR` when running with Singularity, try adding this; to your command:. ```bash; export TMPDIR=""$PWD/tmp_dir""; ```. See https://github.com/google/deepvariant/issues/524#issuecomment-1067597987. ### Issues with `/mnt/`. User reported that sometimes their setup uses `/mnt/`, which exists in our; Docker image, and it has caused an issue in Singularity. You can use `-B` in Singularity to avoid this issue. See:; https://github.com/google/deepvariant/issues/530#issuecomment-1076923302 for; more details. 
",False,"The text contains complete sentences and explanatory content about the behavior of DeepVariant's variant calling process, including issues related to configuration settings like `--partition_size` and `--max_reads_per_partition`, as well as guidance on resolving specific problems encountered during runtime (e.g., `TMPDIR` issues in Singularity). It also links to GitHub issues for further information. This text is a mix of descriptive prose and problem-solving advice, which are considered meaningful and relevant for human readers interested in understanding and troubleshooting DeepVariant's operations."
Security," you should create one at; [cloud.google.com](https://cloud.google.com). You should then [enable; billing for your; account](https://support.google.com/cloud/answer/6288653?hl=en) but note; that if your account is new, [you receive $300 of free; credit](https://cloud.google.com/free/). Once your cloud account is set up,; you should be able to log in to the [Cloud; Console](https://console.cloud.google.com) to view or administer your cloud; resources. * From the Cloud Console, [set up a; project](https://cloud.google.com/resource-manager/docs/creating-managing-projects); to house all of the cloud resources (storage, compute, services) that you; will associate with your use of DeepVariant. For example, if your; organization is AcmeCorp, you might call your project; `acmecorp-deepvariant`. * Finally, please visit the [""Compute Engine"" page on Cloud; Console](https://console.cloud.google.com/compute). You don't need to create; Compute Engine instances at this time, but simply visiting this page will; initialize your compute engine ""service account"" so that we can authorize; it. (As you progress in your use of Google Cloud Platform, you will likely find it; useful to create a [Cloud; Organization](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation proc",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:1461,authoriz,authorize,1461,docs/deepvariant-gcp-info.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md,1,['authoriz'],['authorize'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  you should create one at; [cloud.google.com](https://cloud.google.com). You should then [enable; billing for your; account](https://support.google.com/cloud/answer/6288653?hl=en) but note; that if your account is new, [you receive $300 of free; credit](https://cloud.google.com/free/). Once your cloud account is set up,; you should be able to log in to the [Cloud; Console](https://console.cloud.google.com) to view or administer your cloud; resources. * From the Cloud Console, [set up a; project](https://cloud.google.com/resource-manager/docs/creating-managing-projects); to house all of the cloud resources (storage, compute, services) that you; will associate with your use of DeepVariant. For example, if your; organization is AcmeCorp, you might call your project; `acmecorp-deepvariant`. * Finally, please visit the [""Compute Engine"" page on Cloud; Console](https://console.cloud.google.com/compute). You don't need to create; Compute Engine instances at this time, but simply visiting this page will; initialize your compute engine ""service account"" so that we can authorize; it. (As you progress in your use of Google Cloud Platform, you will likely find it; useful to create a [Cloud; Organization](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation proc
",False,"The content includes a mixture of natural language sentences explaining setup steps and usage of Google Cloud tools. While there are links to API documentation, the majority of the text is descriptive and guides the user through the process of enabling billing, setting up projects, installing SDKs, etc., which are meaningful for human readers."
Security,"## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md:224,access,access,224,docs/deepvariant-build-test.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md,1,['access'],['access'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; 
",False,
Security,"PUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. Then, you should be able to see the run on:; https://console.cloud.google.com/dataflow?project=YOUR_PROJECT. In order to have the best performance, you might need extra resources such as; machines or IPs within a region. That will not be in the scope of this case; study here. The output path can be found in the dataset_config file by:. ```bash; gsutil cat ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; ```. In the output, the `tfrecord_path` should be valid paths in gs://. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 44516; # class1: 173673; # class2: 124569; #; # --input_pattern_list=OUTPUT_BUCKET/training_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_BUCKET/training_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_GCS_BUCKET/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 342758; ```. We can shuffle the validation set locally using; [DirectRunner](https://beam.apache.org/documentation/runners/direct/). Adding; `--direct_num_workers=0` sets the number of threads/subprocess to the number of; cores of the machine where the pipeline is running. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_DIR}""/validation_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_DIR}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DirectRunner \; --direct_num_workers=0; ```. Here is the validation_set:. ```bash; cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt""; ```. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 5591; # class1: 31854; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:11554,validat,validation,11554,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['validat'],['validation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: PUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. Then, you should be able to see the run on:; https://console.cloud.google.com/dataflow?project=YOUR_PROJECT. In order to have the best performance, you might need extra resources such as; machines or IPs within a region. That will not be in the scope of this case; study here. The output path can be found in the dataset_config file by:. ```bash; gsutil cat ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; ```. In the output, the `tfrecord_path` should be valid paths in gs://. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 44516; # class1: 173673; # class2: 124569; #; # --input_pattern_list=OUTPUT_BUCKET/training_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_BUCKET/training_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_GCS_BUCKET/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 342758; ```. We can shuffle the validation set locally using; [DirectRunner](https://beam.apache.org/documentation/runners/direct/). Adding; `--direct_num_workers=0` sets the number of threads/subprocess to the number of; cores of the machine where the pipeline is running. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_DIR}""/validation_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_DIR}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DirectRunner \; --direct_num_workers=0; ```. Here is the validation_set:. ```bash; cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt""; ```. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 5591; # class1: 31854; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output
",False,
Security,"_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz""; TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16; ```. ## Download binaries and data. ### Create directories:. ```bash; mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${BIN_DIR}""; mkdir -p ""${DATA_DIR}""; mkdir -p ""${LOG_DIR}""; ```. ### Copy data. ```bash; gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}""; ```. ### Download extra packages. ```bash; sudo apt -y update; sudo apt -y install parallel; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh; bash -x install_nvidia_docker.sh; ```. ## Run make_examples in “training” mode for training and validation sets. Create examples in ""training"" mode (which means these `tensorflow.Example`s will; contain a `label` field). In this tutorial, we create examples on one replicate of HG001 sequenced by; BGISEQ-500 provided on the; [Genome In a Bottle FTP site](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/BGISEQ500/standard_library/readme.txt). In this tutorial, we will split the genome up into the following datasets:. | chrom | Name | Description |; | ----- | --------------------- | -------------------------------------------- |; | chr1 | Training Set | Examples used to train our model. |; | chr21 | Validation / Tune Set | Examples used to evaluate the performance of our model during training.|; | chr20 | Test Set | Examples reserved for testing performance of our trained model. |. Note that normally, the training dataset will be much larger (e.g. chr1-19),; rather than just a single chromosome. We use just chr1 here to demonstrate how; customized traini",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:4143,validat,validation,4143,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['validat'],['validation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: _highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz""; TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16; ```. ## Download binaries and data. ### Create directories:. ```bash; mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${BIN_DIR}""; mkdir -p ""${DATA_DIR}""; mkdir -p ""${LOG_DIR}""; ```. ### Copy data. ```bash; gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}""; ```. ### Download extra packages. ```bash; sudo apt -y update; sudo apt -y install parallel; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh; bash -x install_nvidia_docker.sh; ```. ## Run make_examples in “training” mode for training and validation sets. Create examples in ""training"" mode (which means these `tensorflow.Example`s will; contain a `label` field). In this tutorial, we create examples on one replicate of HG001 sequenced by; BGISEQ-500 provided on the; [Genome In a Bottle FTP site](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/BGISEQ500/standard_library/readme.txt). In this tutorial, we will split the genome up into the following datasets:. | chrom | Name | Description |; | ----- | --------------------- | -------------------------------------------- |; | chr1 | Training Set | Examples used to train our model. |; | chr21 | Validation / Tune Set | Examples used to evaluate the performance of our model during training.|; | chr20 | Test Set | Examples reserved for testing performance of our trained model. |. Note that normally, the training dataset will be much larger (e.g. chr1-19),; rather than just a single chromosome. We use just chr1 here to demonstrate how; customized traini
",False,"The text contains complete, meaningful sentences in natural language, such as explanations about creating training examples on different chromosomes and splitting the genome into datasets. There are no code snippets or program logs present."
Security,"e ""service account"" so that we can authorize; it. (As you progress in your use of Google Cloud Platform, you will likely find it; useful to create a [Cloud; Organization](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:2420,authenticat,authenticate,2420,docs/deepvariant-gcp-info.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md,1,['authenticat'],['authenticate'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: e ""service account"" so that we can authorize; it. (As you progress in your use of Google Cloud Platform, you will likely find it; useful to create a [Cloud; Organization](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for
",False,
Security,"n/runners/dataflow/); and also [DirectRunner](https://beam.apache.org/documentation/runners/direct/).; Beam can also use other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. Consult the instructions at https://beam.apache.org/get-started/quickstart-py/; if you run into any issues. Then, get the script that performs shuffling:. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. Next, we shuffle the data using DataflowRunner. Before that, please make sure; you enable Dataflow API for your project:; http://console.cloud.google.com/flows/enableapi?apiid=dataflow. To access `gs://` path, make sure you run this in your virtual environment:. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. Shuffle using Dataflow. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. Then, you should be able to see the run on:; https://cons",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:9666,access,access,9666,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['access'],['access'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: n/runners/dataflow/); and also [DirectRunner](https://beam.apache.org/documentation/runners/direct/).; Beam can also use other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. Consult the instructions at https://beam.apache.org/get-started/quickstart-py/; if you run into any issues. Then, get the script that performs shuffling:. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. Next, we shuffle the data using DataflowRunner. Before that, please make sure; you enable Dataflow API for your project:; http://console.cloud.google.com/flows/enableapi?apiid=dataflow. To access `gs://` path, make sure you run this in your virtual environment:. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. Shuffle using Dataflow. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. Then, you should be able to see the run on:; https://cons
",False,"The text contains a series of bash commands and code snippets that describe installation steps for Apache Beam. While it includes some configuration details, it is primarily composed of executable instructions rather than meaningful human-readable content."
Security,"nd `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --acce",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:3068,access,access,3068,docs/deepvariant-gcp-info.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md,1,['access'],['access'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: nd `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --acce
",False,
Security,"ogle Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""co",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:2894,authenticat,authentication,2894,docs/deepvariant-gcp-info.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md,1,['authenticat'],['authentication'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ogle Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""co
",False,
Security,"this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOI",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14178,validat,validation,14178,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['validat'],['validation'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOI
",True,"The content consists primarily of code snippets and logs, including bash commands, configuration settings, and training parameters. There are no meaningful human-readable sentences or descriptive prose."
Testability," ------------- | ---------------- | --------------- |; | INDEL | 29 | 0 | 0 | 1.0 | 1.0 | 1.0 |; | SNP | 683 | 2 | 0 | 0.99708 | 1.0 | 0.998538 |. #### HG004:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 32 | 1 | 1 | 0.969697 | 0.969697 | 0.969697 |; | SNP | 677 | 2 | 0 | 0.997054 | 1.0 | 0.998525 |. * See VCF stats report (for all chromosomes); - [HG002](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG002.output.visual_report.html); - [HG003](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG003.output.visual_report.html); - [HG004](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG004.output.visual_report.html). ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); For bigger datasets (WGS and PACBIO), we used bigger disk size (900G).; This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deeptrio.sh. # WGS; bash inference_deeptrio.sh --model_preset WGS. # WES; bash inference_deeptrio.sh --model_preset WES. # PacBio; bash inference_deeptrio.sh --model_preset PACBIO. ```. Runtime metrics are taken from the resulting log after each stage of; DeepTrio. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 runs produced the same output. [CPU instance with 64 CPUs]: deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics-deeptrio.md:6164,log,log,6164,docs/metrics-deeptrio.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics-deeptrio.md,2,['log'],['log'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  ------------- | ---------------- | --------------- |; | INDEL | 29 | 0 | 0 | 1.0 | 1.0 | 1.0 |; | SNP | 683 | 2 | 0 | 0.99708 | 1.0 | 0.998538 |. #### HG004:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 32 | 1 | 1 | 0.969697 | 0.969697 | 0.969697 |; | SNP | 677 | 2 | 0 | 0.997054 | 1.0 | 0.998525 |. * See VCF stats report (for all chromosomes); - [HG002](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG002.output.visual_report.html); - [HG003](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG003.output.visual_report.html); - [HG004](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG004.output.visual_report.html). ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); For bigger datasets (WGS and PACBIO), we used bigger disk size (900G).; This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deeptrio.sh. # WGS; bash inference_deeptrio.sh --model_preset WGS. # WES; bash inference_deeptrio.sh --model_preset WES. # PacBio; bash inference_deeptrio.sh --model_preset PACBIO. ```. Runtime metrics are taken from the resulting log after each stage of; DeepTrio. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 runs produced the same output. [CPU instance with 64 CPUs]: deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform; 
",False,"The text contains complete, meaningful sentences in natural language, such as instructions on how to reproduce metrics and explanations of the data used."
Testability," --ref=${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; --reads=${PWD}/${BAM} \; --output_vcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; --output_gvcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.g.vcf.gz \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --num_shards=$(nproc); ```. Stage | Time (minutes); -------------------------------- | -----------------; make_examples | 116m37.385s; call_variants | 214m37.055s; postprocess_variants (with gVCF) | 30m59.968s. ### Run hap.py. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; ${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; -f ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -o ${PWD}/happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.h",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:6528,benchmark,benchmark,6528,docs/deepvariant-vg-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  --ref=${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; --reads=${PWD}/${BAM} \; --output_vcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; --output_gvcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.g.vcf.gz \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --num_shards=$(nproc); ```. Stage | Time (minutes); -------------------------------- | -----------------; make_examples | 116m37.385s; call_variants | 214m37.055s; postprocess_variants (with gVCF) | 30m59.968s. ### Run hap.py. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; ${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; -f ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -o ${PWD}/happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.h
",False,
Testability," BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG001. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics T7 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.data-00000-of-00001 > input/weights-51-0.995354.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.index > input/weights-51-0.995354.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf /output/HG001.output.vcf.gz \; --output_gvcf /output/HG001.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-51-0.995354.ckpt; ``",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md:1444,test,testdata,1444,docs/deepvariant-complete-t7-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG001. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics T7 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.data-00000-of-00001 > input/weights-51-0.995354.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.index > input/weights-51-0.995354.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf /output/HG001.output.vcf.gz \; --output_gvcf /output/HG001.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-51-0.995354.ckpt; ``
",False,"The content consists of command lines that download and process genome data, but they are not necessarily code in the sense that requires elimination. The text includes bash commands for file operations, which may be more about workflow automation rather than pure code or logs. There is no presence of syntax error indicators or API documentation. The instructions are descriptive, indicating steps to follow, so they should be retained as explanatory content."
Testability," HG004] -> [HG002]; 222 non-pass records were skipped; Concordance HG002: F:166005/169476 (97.95%) M:166074/168579 (98.51%) F+M:159317/164363 (96.93%); Sample HG002 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 0/188247 (0.00%) records did not conform to expected call ploidy; 176481/188247 (93.75%) records were variant in at least 1 family member and checked for Mendelian constraints; 10169/176481 (5.76%) records had indeterminate consistency status due to incomplete calls; 6610/176481 (3.75%) records contained a violation of Mendelian constraints; ```. ### Benchmark variant calls against 4.2.1 truth set with hap.py. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:9647,benchmark,benchmark,9647,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  HG004] -> [HG002]; 222 non-pass records were skipped; Concordance HG002: F:166005/169476 (97.95%) M:166074/168579 (98.51%) F+M:159317/164363 (96.93%); Sample HG002 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 0/188247 (0.00%) records did not conform to expected call ploidy; 176481/188247 (93.75%) records were variant in at least 1 family member and checked for Mendelian constraints; 10169/176481 (5.76%) records had indeterminate consistency status due to incomplete calls; 6610/176481 (3.75%) records contained a violation of Mendelian constraints; ```. ### Benchmark variant calls against 4.2.1 truth set with hap.py. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.
",True,"The text consists of code snippets and command lines from a bash script. It includes commands for running a Docker container, mounting volumes, and executing a Python script with various parameters. The content is primarily technical instructions rather than meaningful human-readable sentences."
Testability," They would create five resultant gVCF record values with `--gvcf_gq_binsize 5`,; with relevant values of:. ```bash; start | INFO | GQ; ------------------; 1 | END=3 | 8; 4 | END=4 | 27; 5 | END=7 | 47; 8 | END=8 | 45; 9 | END=9 | 33; ```. By synthetically downsampling a 50x coverage whole genome and applying different; GQ binning strategies, we see how the size of the resultant data varies as the; two factors change. The below figure shows the size of output (measured as the; number of records generated relative to the baseline of a 50x whole genome with; `--gvcf_gq_binsize 1`) at different coverage levels, for GQ bins of size 1, 3,; 5, and 10. The value of each bar is written in blue font above it for clarity. ![gVCF size](images/DeepVariant-gvcf-sizes-figure.png?raw=true ""DeepVariant gVCF sizes""). ### Runtime. Despite the creation of many additional records, the running time of; `make_examples` increases minimally when gVCF support is enabled. The; single-threaded `postprocess_variants` program is more adversely affected, with; observed runtimes increasing on the [WGS case study] from ~25 minutes to 5-7; hours depending on genome coverage. ### New option to include MED_DP. Starting in v1.2.0, we added a flag to enable adding MED_DP (median read; coverage seen in the block) in addition to the default MIN_DP (minimum read; coverage seen in the block). To test it, you can follow the steps in [Quick Start], and in the step where; you run the one-step script `/opt/deepvariant/bin/run_deepvariant`, add this; flag:. ```bash; --make_examples_extra_args=""include_med_dp=true""; ```. Then, if you look at your output gVCF, you'll see the additional MED_DP; information, like:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT NA12878; chr20 10000000 . T <*> 0 . END=10000116 GT:GQ:MIN_DP:MED_DP:PL 0/0:50:45:58:0,135,1349; ```. [VCF format]: https://samtools.github.io/hts-specs/VCFv4.3.pdf; [WGS case study]: deepvariant-case-study.md; [Quick Start]: deepvariant-quick-start.md; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:6692,test,test,6692,docs/deepvariant-gvcf-support.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md,1,['test'],['test'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  They would create five resultant gVCF record values with `--gvcf_gq_binsize 5`,; with relevant values of:. ```bash; start | INFO | GQ; ------------------; 1 | END=3 | 8; 4 | END=4 | 27; 5 | END=7 | 47; 8 | END=8 | 45; 9 | END=9 | 33; ```. By synthetically downsampling a 50x coverage whole genome and applying different; GQ binning strategies, we see how the size of the resultant data varies as the; two factors change. The below figure shows the size of output (measured as the; number of records generated relative to the baseline of a 50x whole genome with; `--gvcf_gq_binsize 1`) at different coverage levels, for GQ bins of size 1, 3,; 5, and 10. The value of each bar is written in blue font above it for clarity. ![gVCF size](images/DeepVariant-gvcf-sizes-figure.png?raw=true ""DeepVariant gVCF sizes""). ### Runtime. Despite the creation of many additional records, the running time of; `make_examples` increases minimally when gVCF support is enabled. The; single-threaded `postprocess_variants` program is more adversely affected, with; observed runtimes increasing on the [WGS case study] from ~25 minutes to 5-7; hours depending on genome coverage. ### New option to include MED_DP. Starting in v1.2.0, we added a flag to enable adding MED_DP (median read; coverage seen in the block) in addition to the default MIN_DP (minimum read; coverage seen in the block). To test it, you can follow the steps in [Quick Start], and in the step where; you run the one-step script `/opt/deepvariant/bin/run_deepvariant`, add this; flag:. ```bash; --make_examples_extra_args=""include_med_dp=true""; ```. Then, if you look at your output gVCF, you'll see the additional MED_DP; information, like:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT NA12878; chr20 10000000 . T <*> 0 . END=10000116 GT:GQ:MIN_DP:MED_DP:PL 0/0:50:45:58:0,135,1349; ```. [VCF format]: https://samtools.github.io/hts-specs/VCFv4.3.pdf; [WGS case study]: deepvariant-case-study.md; [Quick Start]: deepvariant-quick-start.md; 
",False,"The text contains sentences that are complete, meaningful, and in natural language discussing concepts and ideas related to the functionality of gVCF records, runtime considerations, and new features. There are no code snippets, logs, API documentation, or other programmatic content present."
Testability," [HG003 + HG004] -> [HG002]; 95 non-pass records were skipped; Concordance HG002: F:137908/139703 (98.72%) M:137988/139909 (98.63%) F+M:134596/137968 (97.56%); Sample HG002 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 0/146013 (0.00%) records did not conform to expected call ploidy; 143704/146013 (98.42%) records were variant in at least 1 family member and checked for Mendelian constraints; 5066/143704 (3.53%) records had indeterminate consistency status due to incomplete calls; 3886/143704 (2.70%) records contained a violation of Mendelian constraints; ```. ### Perform analysis with hap.py against 4.2.1 truth set. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:9366,benchmark,benchmark,9366,docs/deeptrio-wgs-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  [HG003 + HG004] -> [HG002]; 95 non-pass records were skipped; Concordance HG002: F:137908/139703 (98.72%) M:137988/139909 (98.63%) F+M:134596/137968 (97.56%); Sample HG002 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 0/146013 (0.00%) records did not conform to expected call ploidy; 143704/146013 (98.42%) records were variant in at least 1 family member and checked for Mendelian constraints; 5066/143704 (3.53%) records had indeterminate consistency status due to incomplete calls; 3886/143704 (2.70%) records contained a violation of Mendelian constraints; ```. ### Perform analysis with hap.py against 4.2.1 truth set. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.
",True,"The text consists of program logs and API descriptions. It includes command lines with syntax like 'sudo docker run' and parameters, which are typical signs of code or build instructions."
Testability," `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on all chromosomes. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -T /input/idt_capture_novogene.grch38.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 1051 1022 29 1476 13 418 8 3 0.972407 0.987713 0.283198 0.980000 NaN NaN 1.747283 1.859406; INDEL PASS 1051 1022 29 1476 13 418 8 3 0.972407 0.987713 0.283198 0.980000 NaN NaN 1.747283 1.859406; SNP ALL 25279 24987 292 27710 59 2662 34 2 0.988449 0.997645 0.096066 0.993025 2.854703 2.749729 1.623027 1.636078; SNP PASS 25279 24987 292 27710 59 2662 34 2 0.988449 0.997645 0.096066 0.993025 2.854703 2.7",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md:4101,benchmark,benchmark,4101,docs/deepvariant-exome-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on all chromosomes. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -T /input/idt_capture_novogene.grch38.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 1051 1022 29 1476 13 418 8 3 0.972407 0.987713 0.283198 0.980000 NaN NaN 1.747283 1.859406; INDEL PASS 1051 1022 29 1476 13 418 8 3 0.972407 0.987713 0.283198 0.980000 NaN NaN 1.747283 1.859406; SNP ALL 25279 24987 292 27710 59 2662 34 2 0.988449 0.997645 0.096066 0.993025 2.854703 2.749729 1.623027 1.636078; SNP PASS 25279 24987 292 27710 59 2662 34 2 0.988449 0.997645 0.096066 0.993025 2.854703 2.7
",False,
Testability," apt -y update; sudo apt -y install parallel; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh; bash -x install_nvidia_docker.sh; ```. ## Run make_examples in “training” mode for training and validation sets. Create examples in ""training"" mode (which means these `tensorflow.Example`s will; contain a `label` field). In this tutorial, we create examples on one replicate of HG001 sequenced by; BGISEQ-500 provided on the; [Genome In a Bottle FTP site](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/BGISEQ500/standard_library/readme.txt). In this tutorial, we will split the genome up into the following datasets:. | chrom | Name | Description |; | ----- | --------------------- | -------------------------------------------- |; | chr1 | Training Set | Examples used to train our model. |; | chr21 | Validation / Tune Set | Examples used to evaluate the performance of our model during training.|; | chr20 | Test Set | Examples reserved for testing performance of our trained model. |. Note that normally, the training dataset will be much larger (e.g. chr1-19),; rather than just a single chromosome. We use just chr1 here to demonstrate how; customized training works. For the definition of these 3 sets in commonly used machine learning; terminology, please refer to; [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary/). ### Training set. First, to set up, lets pull the docker images. ```bash; sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image.; sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image.; ```. The `make_examples` step doesn't use GPU, so we will not require the GPU-enabled; image. ```bash; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHAR",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:4902,test,testing,4902,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['test'],['testing'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  apt -y update; sudo apt -y install parallel; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh; bash -x install_nvidia_docker.sh; ```. ## Run make_examples in “training” mode for training and validation sets. Create examples in ""training"" mode (which means these `tensorflow.Example`s will; contain a `label` field). In this tutorial, we create examples on one replicate of HG001 sequenced by; BGISEQ-500 provided on the; [Genome In a Bottle FTP site](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/BGISEQ500/standard_library/readme.txt). In this tutorial, we will split the genome up into the following datasets:. | chrom | Name | Description |; | ----- | --------------------- | -------------------------------------------- |; | chr1 | Training Set | Examples used to train our model. |; | chr21 | Validation / Tune Set | Examples used to evaluate the performance of our model during training.|; | chr20 | Test Set | Examples reserved for testing performance of our trained model. |. Note that normally, the training dataset will be much larger (e.g. chr1-19),; rather than just a single chromosome. We use just chr1 here to demonstrate how; customized training works. For the definition of these 3 sets in commonly used machine learning; terminology, please refer to; [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary/). ### Training set. First, to set up, lets pull the docker images. ```bash; sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image.; sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image.; ```. The `make_examples` step doesn't use GPU, so we will not require the GPU-enabled; image. ```bash; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHAR
",False,
Testability," enables RNA-seq data to be; processed efficiently.; * `channels=''` - Resets the channel list to be appropriate for the; RNA-seq model.; * `--intermediate_results_dir` - Outputs results to an intermediate directory. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; sudo docker run \; -v $(pwd):$(pwd) \; -w $(pwd) \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; output/HG005.output.vcf.gz \; -f benchmark/chr20_CDS_3x.benchmark_regions.bed \; -r reference/GRCh38_no_alt_analysis_set.fasta \; -o happy/happy.output \; --engine=vcfeval \; --pass-only \; --target-regions=data/chr20_CDS_3x.bed \; --threads=$(nproc); ```. **Flag summary**. * `-f` - Sets the benchmark regions (regions of interest that we want to; benchmark.); * `-r` - Sets the reference genome.; * `-o` - Specifies the output location.; * `--engine` - Sets the variant comparison engine. See; [hap.py documentation](https://github.com/Illumina/hap.py) for details.; * `--pass-only` - Restricts benchmarking to variants that have passed all; filters.; * `--target-regions` - Restricts analysis to given regions only.; * `--threads` - Level of parallelization to use. **Output:**. The above command should output the following results:. ```; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 9 6 3 11 1 4 1 0 0.666667 0.857143 0.363636 0.75000 NaN NaN 0.800000 1.200000; INDEL PASS 9 6 3 11 1 4 1 0 0.666667 0.857143 0.363636 0.75000 NaN NaN 0.800000 1.200000; SNP ALL 287 275 12 314 6 33 3 2 0.958188 0.978648 0.105096 0.96831 4.125 3.984127 1.141791 1.093333; SNP PASS 287 275 12 314 6 33 3 2 0.958188 0.978648 0.105096 0.96831 4.125 3.984127 1.141791 1.093333; ```; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:9628,benchmark,benchmarking,9628,docs/deepvariant-rnaseq-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md,1,['benchmark'],['benchmarking'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  enables RNA-seq data to be; processed efficiently.; * `channels=''` - Resets the channel list to be appropriate for the; RNA-seq model.; * `--intermediate_results_dir` - Outputs results to an intermediate directory. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; sudo docker run \; -v $(pwd):$(pwd) \; -w $(pwd) \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; output/HG005.output.vcf.gz \; -f benchmark/chr20_CDS_3x.benchmark_regions.bed \; -r reference/GRCh38_no_alt_analysis_set.fasta \; -o happy/happy.output \; --engine=vcfeval \; --pass-only \; --target-regions=data/chr20_CDS_3x.bed \; --threads=$(nproc); ```. **Flag summary**. * `-f` - Sets the benchmark regions (regions of interest that we want to; benchmark.); * `-r` - Sets the reference genome.; * `-o` - Specifies the output location.; * `--engine` - Sets the variant comparison engine. See; [hap.py documentation](https://github.com/Illumina/hap.py) for details.; * `--pass-only` - Restricts benchmarking to variants that have passed all; filters.; * `--target-regions` - Restricts analysis to given regions only.; * `--threads` - Level of parallelization to use. **Output:**. The above command should output the following results:. ```; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 9 6 3 11 1 4 1 0 0.666667 0.857143 0.363636 0.75000 NaN NaN 0.800000 1.200000; INDEL PASS 9 6 3 11 1 4 1 0 0.666667 0.857143 0.363636 0.75000 NaN NaN 0.800000 1.200000; SNP ALL 287 275 12 314 6 33 3 2 0.958188 0.978648 0.105096 0.96831 4.125 3.984127 1.141791 1.093333; SNP PASS 287 275 12 314 6 33 3 2 0.958188 0.978648 0.105096 0.96831 4.125 3.984127 1.141791 1.093333; ```; 
",False,"The text contains complete sentences discussing the functionality and usage of the command, including explanations of flags and outputs."
Testability," file; per task if the examples are sharded. This TSV file can then be visualized using the `runtime_by_region_vis` script,; creating a visual report. ![Sample runtime profile from a WGS run](images/runtime_by_region_wgs.png). Example reports for typical runs:. * [WGS](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_wgs.html); * [WES](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_wes.html); * [PacBio](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_pacbio.html); * [Hybrid](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_hybrid.html). ## How to enable runtime profiling during a DeepVariant run. ### Using the run_deepvariant script. When using the one-step `run_deepvariant` script, supply a `--logging_dir`; directory and set `--runtime_report`. For example, when following the; [quick start](deepvariant-quick-start.md), including `--runtime_report; --logging_dir=/output/logs` would produce the following runtime profiling output; file and visual report:. ```; /output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv; /output/logs/make_examples_runtime_by_region_report.html; ```. ### Running stages individually. If you are using make_examples itself without the run_deepvariant script, supply; make_examples with `--runtime_by_region=` a filename (.tsv). If the examples are; sharded then the runtime profile should be sharded into the same number of; shards, e.g. when using `examples@64` then runtimes could be `runtimes@64.tsv`. Then use the `runtime_by_region_vis` script to create a visual report of the; make_examples runtime by region data. Continuing from the quick start, it looks; like this:. ```bash; BIN_VERSION=""1.6.1"" # Only available in v1.1+.; docker run \; -v ""INPUT_DIR"":""/input"" \; -v ""OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/runtime_by_region_vis \; --input=/output/logs/make_examples_runti",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md:1502,log,logs,1502,docs/runtime-by-region.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md,1,['log'],['logs'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  file; per task if the examples are sharded. This TSV file can then be visualized using the `runtime_by_region_vis` script,; creating a visual report. ![Sample runtime profile from a WGS run](images/runtime_by_region_wgs.png). Example reports for typical runs:. * [WGS](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_wgs.html); * [WES](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_wes.html); * [PacBio](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_pacbio.html); * [Hybrid](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_hybrid.html). ## How to enable runtime profiling during a DeepVariant run. ### Using the run_deepvariant script. When using the one-step `run_deepvariant` script, supply a `--logging_dir`; directory and set `--runtime_report`. For example, when following the; [quick start](deepvariant-quick-start.md), including `--runtime_report; --logging_dir=/output/logs` would produce the following runtime profiling output; file and visual report:. ```; /output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv; /output/logs/make_examples_runtime_by_region_report.html; ```. ### Running stages individually. If you are using make_examples itself without the run_deepvariant script, supply; make_examples with `--runtime_by_region=` a filename (.tsv). If the examples are; sharded then the runtime profile should be sharded into the same number of; shards, e.g. when using `examples@64` then runtimes could be `runtimes@64.tsv`. Then use the `runtime_by_region_vis` script to create a visual report of the; make_examples runtime by region data. Continuing from the quick start, it looks; like this:. ```bash; BIN_VERSION=""1.6.1"" # Only available in v1.1+.; docker run \; -v ""INPUT_DIR"":""/input"" \; -v ""OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/runtime_by_region_vis \; --input=/output/logs/make_examples_runti
",False,
Testability," for each stage separately.; --haploid_contigs=""chrX,chrY"" \ **Optional. Heterozygous variants in these contigs will be re-genotyped as the most likely of reference or homozygous alternates. For a sample with karyotype XY, it should be set to ""chrX,chrY"" for GRCh38 and ""X,Y"" for GRCh37. For a sample with karyotype XX, this should not be used.; --par_regions_bed=""/input/GRCh3X_par.bed"" \ **Optional. If --haploid_contigs is set, then this can be used to provide PAR regions to be excluded from genotype adjustment. Download links to this files are available in this page.; --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.; ```. For details on X,Y support, please see; [DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case; study in; [DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You; can download the PAR bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). To see all flags you can use, run: `docker run; google/deepvariant:""${BIN_VERSION}""`. If you're using GPUs, or want to use Singularity instead, see; [Quick Start](docs/deepvariant-quick-start.md) for more details or see all the; [setup options](#deepvariant_setup) available. For more information, also see:. * [Full documentation list](docs/README.md); * [Detailed usage guide](docs/deepvariant-details.md) with more information on; the input and output file formats and how to work with them.; * [Best practices for multi-sample variant calling with DeepVariant](docs/trio-merge-case-study.md); * [(Advanced) Training tutorial](docs/deepvariant-training-case-study.md); * [DeepVariant's Frequently Asked Questions, FAQ](docs/FAQ.md). ## How to cite. If you're using DeepVariant in your work, please cite:. [A universal SNP and small-indel variant caller using deep neural",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:4917,test,testdata,4917,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  for each stage separately.; --haploid_contigs=""chrX,chrY"" \ **Optional. Heterozygous variants in these contigs will be re-genotyped as the most likely of reference or homozygous alternates. For a sample with karyotype XY, it should be set to ""chrX,chrY"" for GRCh38 and ""X,Y"" for GRCh37. For a sample with karyotype XX, this should not be used.; --par_regions_bed=""/input/GRCh3X_par.bed"" \ **Optional. If --haploid_contigs is set, then this can be used to provide PAR regions to be excluded from genotype adjustment. Download links to this files are available in this page.; --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.; ```. For details on X,Y support, please see; [DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case; study in; [DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You; can download the PAR bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). To see all flags you can use, run: `docker run; google/deepvariant:""${BIN_VERSION}""`. If you're using GPUs, or want to use Singularity instead, see; [Quick Start](docs/deepvariant-quick-start.md) for more details or see all the; [setup options](#deepvariant_setup) available. For more information, also see:. * [Full documentation list](docs/README.md); * [Detailed usage guide](docs/deepvariant-details.md) with more information on; the input and output file formats and how to work with them.; * [Best practices for multi-sample variant calling with DeepVariant](docs/trio-merge-case-study.md); * [(Advanced) Training tutorial](docs/deepvariant-training-case-study.md); * [DeepVariant's Frequently Asked Questions, FAQ](docs/FAQ.md). ## How to cite. If you're using DeepVariant in your work, please cite:. [A universal SNP and small-indel variant caller using deep neural
",True,"The text contains API descriptions, parameter lists, and command outputs which are typical of programmatic content."
Testability," for the; `make_examples` step, which can result in different shape of the output; examples. We will want to shuffle this on Dataflow later, so we copy the data to GCS; bucket first:. ```; gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. NOTE: If you prefer shuffling locally, please take a look at this user-provided; shuffler option:; https://github.com/google/deepvariant/issues/360#issuecomment-1019990366. ### Validation set. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. This took: 5m31.122s. Copy to GCS bucket:. ```bash; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. ## Shuffle each set of examples and generate a data configuration file for each. Shuffling the `tensorflow.Example`s is an important step for training a model.; In our training logic, we shuffle examples globally using a preprocessing step. First, if you have run this step before, and want to rerun it, you might want to; consider cleaning up previous data first to avoid confusion:. ```bash; # (Optional) Clean up existing files.; gsutil -m rm -f ""${OUTPUT_BUCKET}/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; gsutil -m rm -f ""${OUTPUT_BUCKET}/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt""; gsutil rm -f ""${OUTPUT_BUCKET}/example_info.json""; ```. Here we provide ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:7610,log,log,7610,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['log'],['log'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  for the; `make_examples` step, which can result in different shape of the output; examples. We will want to shuffle this on Dataflow later, so we copy the data to GCS; bucket first:. ```; gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. NOTE: If you prefer shuffling locally, please take a look at this user-provided; shuffler option:; https://github.com/google/deepvariant/issues/360#issuecomment-1019990366. ### Validation set. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. This took: 5m31.122s. Copy to GCS bucket:. ```bash; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. ## Shuffle each set of examples and generate a data configuration file for each. Shuffling the `tensorflow.Example`s is an important step for training a model.; In our training logic, we shuffle examples globally using a preprocessing step. First, if you have run this step before, and want to rerun it, you might want to; consider cleaning up previous data first to avoid confusion:. ```bash; # (Optional) Clean up existing files.; gsutil -m rm -f ""${OUTPUT_BUCKET}/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; gsutil -m rm -f ""${OUTPUT_BUCKET}/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt""; gsutil rm -f ""${OUTPUT_BUCKET}/example_info.json""; ```. Here we provide 
",False,
Testability," instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepVariant in one command. Starting from the 0.8 release, we introduced one convenient command that will; run through all 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_D",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:1474,test,test,1474,docs/deepvariant-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md,1,['test'],['test'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepVariant in one command. Starting from the 0.8 release, we introduced one convenient command that will; run through all 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_D
",False,
Testability," it for easy use through the --model_type parameter in; `run_deepvariant.py`. Much of the work we put into DeepVariant is in; experimenting with different approaches, training on more and better data, and; carefully evaluating the models before releasing them. We did the same with this; hybrid model. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:2995,benchmark,benchmark,2995,docs/deepvariant-hybrid-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  it for easy use through the --model_type parameter in; `run_deepvariant.py`. Much of the work we put into DeepVariant is in; experimenting with different approaches, training on more and better data, and; carefully evaluating the models before releasing them. We did the same with this; hybrid model. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003
",False,"The text contains a mix of bash commands and explanatory sentences about the process. While there are code snippets present, they are integrated with meaningful explanations, making the overall content readable by humans. The instructions for setting up the environment, downloading references, and preparing inputs are provided in natural language interspersed with specific commands. This combination results in a text that is both descriptive and actionable without being purely code-based or log-like. Therefore, it should not be filtered out as it meets the criteria of containing meaningful human-readable sentences."
Testability," real WGS trio. Then we; assess the quality of the DeepTrio variant calls with `hap.py`. In addition we; evaluate a mendelian violation rate for a merged VCF. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh3",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:1101,benchmark,benchmarks,1101,docs/deeptrio-wgs-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md,1,['benchmark'],['benchmarks'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  real WGS trio. Then we; assess the quality of the DeepTrio variant calls with `hap.py`. In addition we; evaluate a mendelian violation rate for a merged VCF. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh3
",False,"The content contains a mix of procedural instructions and natural language text. The presence of bash commands, URLs for downloading files, and some technical terms does not constitute purely code-based or programmatic content. There are also complete sentences that discuss the purpose of the study, the use of tools like Docker and hap.py, and the overall workflow. These elements make the content readable as a human-readable document with explanatory text."
Testability," records were variant in at least 1 family member and checked for Mendelian constraints; 5066/143704 (3.53%) records had indeterminate consistency status due to incomplete calls; 3886/143704 (2.70%) records contained a violation of Mendelian constraints; ```. ### Perform analysis with hap.py against 4.2.1 truth set. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TR",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:9674,benchmark,benchmark,9674,docs/deeptrio-wgs-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md,2,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  records were variant in at least 1 family member and checked for Mendelian constraints; 5066/143704 (3.53%) records had indeterminate consistency status due to incomplete calls; 3886/143704 (2.70%) records contained a violation of Mendelian constraints; ```. ### Perform analysis with hap.py against 4.2.1 truth set. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TR
",False,"The text consists of multiple lines of command executions and script runs, including docker commands and pipeline executions. While some sentences are meaningful in explaining the benchmarking process, the majority is code snippets or programmatic steps which are more about system operations rather than human-readable content. Therefore, this text should be eliminated."
Testability," run](images/runtime_by_region_wgs.png). Example reports for typical runs:. * [WGS](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_wgs.html); * [WES](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_wes.html); * [PacBio](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_pacbio.html); * [Hybrid](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_hybrid.html). ## How to enable runtime profiling during a DeepVariant run. ### Using the run_deepvariant script. When using the one-step `run_deepvariant` script, supply a `--logging_dir`; directory and set `--runtime_report`. For example, when following the; [quick start](deepvariant-quick-start.md), including `--runtime_report; --logging_dir=/output/logs` would produce the following runtime profiling output; file and visual report:. ```; /output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv; /output/logs/make_examples_runtime_by_region_report.html; ```. ### Running stages individually. If you are using make_examples itself without the run_deepvariant script, supply; make_examples with `--runtime_by_region=` a filename (.tsv). If the examples are; sharded then the runtime profile should be sharded into the same number of; shards, e.g. when using `examples@64` then runtimes could be `runtimes@64.tsv`. Then use the `runtime_by_region_vis` script to create a visual report of the; make_examples runtime by region data. Continuing from the quick start, it looks; like this:. ```bash; BIN_VERSION=""1.6.1"" # Only available in v1.1+.; docker run \; -v ""INPUT_DIR"":""/input"" \; -v ""OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/runtime_by_region_vis \; --input=/output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv \; --output=/output/logs/make_examples_runtime_by_region/make_examples_runtime_vis.html \; --title=""Quick start runtime profiling""; `",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md:1687,log,logs,1687,docs/runtime-by-region.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md,1,['log'],['logs'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  run](images/runtime_by_region_wgs.png). Example reports for typical runs:. * [WGS](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_wgs.html); * [WES](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_wes.html); * [PacBio](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_pacbio.html); * [Hybrid](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_hybrid.html). ## How to enable runtime profiling during a DeepVariant run. ### Using the run_deepvariant script. When using the one-step `run_deepvariant` script, supply a `--logging_dir`; directory and set `--runtime_report`. For example, when following the; [quick start](deepvariant-quick-start.md), including `--runtime_report; --logging_dir=/output/logs` would produce the following runtime profiling output; file and visual report:. ```; /output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv; /output/logs/make_examples_runtime_by_region_report.html; ```. ### Running stages individually. If you are using make_examples itself without the run_deepvariant script, supply; make_examples with `--runtime_by_region=` a filename (.tsv). If the examples are; sharded then the runtime profile should be sharded into the same number of; shards, e.g. when using `examples@64` then runtimes could be `runtimes@64.tsv`. Then use the `runtime_by_region_vis` script to create a visual report of the; make_examples runtime by region data. Continuing from the quick start, it looks; like this:. ```bash; BIN_VERSION=""1.6.1"" # Only available in v1.1+.; docker run \; -v ""INPUT_DIR"":""/input"" \; -v ""OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/runtime_by_region_vis \; --input=/output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv \; --output=/output/logs/make_examples_runtime_by_region/make_examples_runtime_vis.html \; --title=""Quick start runtime profiling""; `
",False,"The text contains complete, meaningful sentences discussing how to enable runtime profiling in DeepVariant runs, including instructions on using specific scripts and commands. It also includes example usage examples and explanation of the generated files. The content is human-readable prose that provides explanatory information about a tool's functionality."
Testability," same; tasks with Pareto curves leaning to the upper left, indicating that for; tasks than run longer than others, the cause is with a subset of the regions; not with an overall slowdown of all regions.; 4. ""Stage runtimes for each task"": A histogram of how long each stage takes for; the different tasks. Often the `make pileup images` stage will show more; variability here than other stages.; 5. ""Top runtime regions"" and ""Median runtime regions"": This shows some; individual regions to give more context for some of the trends seen in other; charts. Pay attention especially to the differences between the y-axis; limits in these two charts. The long-running regions are often taking; hundreds of times longer than median regions, with the runtime also taken up; by different stages.; 6. ""The longest-running regions that produced no examples"": This profiles some; individual regions that yielded zero output examples. Also look at the; subtitle to see what percentage of the total runtime is taken up by; processing these zero-example regions.; 7. ""Runtime by stage for ..."": When there are more than 5000 regions, there; will be two charts here, one for the bottom 99% of regions and one for the; top 100 regions (both by total runtime). If fewer than 5000 regions, there; will only be one chart showing all the regions. This is similar to the; ""Stage runtimes for each task"" except that regions are shown individually; here instead of being combined into tasks. This shows the spread of runtimes; across regions for the different stages.; 8. ""Trends for ..."": This is in one or two sets of charts by the same logic as; the ""Runtime by stage"" charts. This shows a grid of charts intersecting; counts of reads, candidates, and examples (rows) with the runtime for the; four stages (columns) in seconds. It is common that some of these runtimes; will correlate nicely with the counts. For example, the `write outputs`; runtime is closely tied to the number of examples, which is not surprising.; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md:7150,log,logic,7150,docs/runtime-by-region.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md,1,['log'],['logic'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  same; tasks with Pareto curves leaning to the upper left, indicating that for; tasks than run longer than others, the cause is with a subset of the regions; not with an overall slowdown of all regions.; 4. ""Stage runtimes for each task"": A histogram of how long each stage takes for; the different tasks. Often the `make pileup images` stage will show more; variability here than other stages.; 5. ""Top runtime regions"" and ""Median runtime regions"": This shows some; individual regions to give more context for some of the trends seen in other; charts. Pay attention especially to the differences between the y-axis; limits in these two charts. The long-running regions are often taking; hundreds of times longer than median regions, with the runtime also taken up; by different stages.; 6. ""The longest-running regions that produced no examples"": This profiles some; individual regions that yielded zero output examples. Also look at the; subtitle to see what percentage of the total runtime is taken up by; processing these zero-example regions.; 7. ""Runtime by stage for ..."": When there are more than 5000 regions, there; will be two charts here, one for the bottom 99% of regions and one for the; top 100 regions (both by total runtime). If fewer than 5000 regions, there; will only be one chart showing all the regions. This is similar to the; ""Stage runtimes for each task"" except that regions are shown individually; here instead of being combined into tasks. This shows the spread of runtimes; across regions for the different stages.; 8. ""Trends for ..."": This is in one or two sets of charts by the same logic as; the ""Runtime by stage"" charts. This shows a grid of charts intersecting; counts of reads, candidates, and examples (rows) with the runtime for the; four stages (columns) in seconds. It is common that some of these runtimes; will correlate nicely with the counts. For example, the `write outputs`; runtime is closely tied to the number of examples, which is not surprising.; 
",False,"The provided text contains several detailed explanations about different charts and stages in a process, including descriptions of what each chart shows, how regions are analyzed, and relationships between runtime and other metrics. It uses natural language to explain concepts rather than code snippets or logs."
Testability," sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; -it quay.io/biocontainers/bedtools:2.23.0--h5b5514e_6 \; /bin/bash; ```. ### Extract regions with 3x coverage, and filter out unused contigs. We will restrict our analysis to regions with a minimum of 3x coverage. ```bash; # (Run within the bedtools container); min_coverage=3; gzip -dc data/hg005_coverage.per-base.bed.gz | \; egrep -v 'HLA|decoy|random|alt|chrUn|chrEBV' | \; awk -v OFS=""\t"" -v min_coverage=${min_coverage} '$4 >= min_coverage { print }' | \; bedtools merge -d 1 -c 4 -o mean -i - > data/hg005_3x.bed; ```. ### Intersect coverage with CDS regions. Now we will intersect our 3x bedfile with the CDS bed file:. ```bash; # (Run within the bedtools container); bedtools intersect \; -a data/hg005_3x.bed \; -b data/chr20_CDS.bed > data/chr20_CDS_3x.bed. # We will also intersect this file with confident GIAB regions; bedtools intersect \; -a benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed \; -b data/chr20_CDS_3x.bed > benchmark/chr20_CDS_3x.benchmark_regions.bed; ```. We now have a bed file of CDS regions intersected with 3x coverage regions; called `data/chr20_CDS_3x.bed`. You can exit the docker container now. Type; `exit` and hit enter. ### Download the RNA-seq model. Finally, lets download the RNA-seq model that we will use to call variants. ```bash; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-r",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:5054,benchmark,benchmark,5054,docs/deepvariant-rnaseq-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; -it quay.io/biocontainers/bedtools:2.23.0--h5b5514e_6 \; /bin/bash; ```. ### Extract regions with 3x coverage, and filter out unused contigs. We will restrict our analysis to regions with a minimum of 3x coverage. ```bash; # (Run within the bedtools container); min_coverage=3; gzip -dc data/hg005_coverage.per-base.bed.gz | \; egrep -v 'HLA|decoy|random|alt|chrUn|chrEBV' | \; awk -v OFS=""\t"" -v min_coverage=${min_coverage} '$4 >= min_coverage { print }' | \; bedtools merge -d 1 -c 4 -o mean -i - > data/hg005_3x.bed; ```. ### Intersect coverage with CDS regions. Now we will intersect our 3x bedfile with the CDS bed file:. ```bash; # (Run within the bedtools container); bedtools intersect \; -a data/hg005_3x.bed \; -b data/chr20_CDS.bed > data/chr20_CDS_3x.bed. # We will also intersect this file with confident GIAB regions; bedtools intersect \; -a benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed \; -b data/chr20_CDS_3x.bed > benchmark/chr20_CDS_3x.benchmark_regions.bed; ```. We now have a bed file of CDS regions intersected with 3x coverage regions; called `data/chr20_CDS_3x.bed`. You can exit the docker container now. Type; `exit` and hit enter. ### Download the RNA-seq model. Finally, lets download the RNA-seq model that we will use to call variants. ```bash; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-r
",False,"The text contains multiple bash commands and code snippets, but it also includes explanatory sentences such as 'We now have a bed file...' which are meaningful and in natural language. There is a mix of code and human-readable content."
Testability," test data in a bucket on Google Cloud; Storage. The instructions below show how to download the data through the; corresponding public URLs from these data. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepVariant in one command. Starting from the 0.8 release, we introduced one convenient command that will; run through all 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:1284,test,test,1284,docs/deepvariant-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md,1,['test'],['test'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  test data in a bucket on Google Cloud; Storage. The instructions below show how to download the data through the; corresponding public URLs from these data. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepVariant in one command. Starting from the 0.8 release, we introduced one convenient command that will; run through all 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20
",False,
Testability," the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir""; tensorboard --logdir ${TRAINING_DIR} --port=8080; ```. After it started, I clicked on the “Web Preview” on the top right of the mini; terminal:. ![WebPreview](images/WebPreview.png?raw=true ""Web Preview""). And clicked on ""Preview on port 8080"":. ![PreviewOnPort](images/PreviewOnPort.png?raw=true ""Preview on Port 8080""). Once it starts, you can see many metrics, including accuracy, speed, etc. You; will need to wait for `train` to run for a while before the plots will appear. ### Test the model. Now that we have performed training, we can test the performance of the new; model using our holdout dataset (chr20). The following one-step command can be used to call DeepVariant and run our newly; trained model:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ""${DOCKER_IMAGE}-gpu"" \; run_deepvariant \; --model_type WGS \; --customized_model ""${BEST_CHECKPOINT}"" \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/test_set.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. In v1.4.0, by using `--model_type WGS`, `run_deepvariant` will automatically add; `insert_size` as an extra channel in the `make_examples` step. So we don't need; to add it in `--make_examples_extra_args`. When the `call_variants` step is run, you might see messages like:. ```; E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. You can use `nvidia-smi` to confirm whether the GPUs are used. If so, you can;",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:16513,test,test,16513,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['test'],['test'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir""; tensorboard --logdir ${TRAINING_DIR} --port=8080; ```. After it started, I clicked on the “Web Preview” on the top right of the mini; terminal:. ![WebPreview](images/WebPreview.png?raw=true ""Web Preview""). And clicked on ""Preview on port 8080"":. ![PreviewOnPort](images/PreviewOnPort.png?raw=true ""Preview on Port 8080""). Once it starts, you can see many metrics, including accuracy, speed, etc. You; will need to wait for `train` to run for a while before the plots will appear. ### Test the model. Now that we have performed training, we can test the performance of the new; model using our holdout dataset (chr20). The following one-step command can be used to call DeepVariant and run our newly; trained model:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ""${DOCKER_IMAGE}-gpu"" \; run_deepvariant \; --model_type WGS \; --customized_model ""${BEST_CHECKPOINT}"" \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/test_set.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. In v1.4.0, by using `--model_type WGS`, `run_deepvariant` will automatically add; `insert_size` as an extra channel in the `make_examples` step. So we don't need; to add it in `--make_examples_extra_args`. When the `call_variants` step is run, you might see messages like:. ```; E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. You can use `nvidia-smi` to confirm whether the GPUs are used. If so, you can;
",False,"The text contains a mix of descriptive explanations and code snippets. While there are code examples provided for clarity, they are accompanied by explanatory context that makes them understandable to a human reader. The text also includes instructional steps on how to use certain tools and commands, which adds to its readability. There is no indication that this content should be filtered out as it serves to explain the process rather than just present code or logs."
Testability," this case study, we describe applying DeepVariant to a Complete Genomics T7; sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics T7 HG001 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG001. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics T7 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.data-00000-of-00001 > input/weights-51-0.995354.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.index > input/weights-51-0.995354.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md:1037,benchmark,benchmark,1037,docs/deepvariant-complete-t7-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  this case study, we describe applying DeepVariant to a Complete Genomics T7; sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics T7 HG001 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG001. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics T7 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.data-00000-of-00001 > input/weights-51-0.995354.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.index > input/weights-51-0.995354.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type 
",False,
Testability," to incomplete calls; 6610/176481 (3.75%) records contained a violation of Mendelian constraints; ```. ### Benchmark variant calls against 4.2.1 truth set with hap.py. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:10162,benchmark,benchmark,10162,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  to incomplete calls; 6610/176481 (3.75%) records contained a violation of Mendelian constraints; ```. ### Benchmark variant calls against 4.2.1 truth set with hap.py. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.
",True,"The text consists primarily of command lines and code snippets, including docker commands and script execution instructions. This is more aligned with programmatic API descriptions or build system output rather than meaningful human-readable sentences."
Testability," training works. For the definition of these 3 sets in commonly used machine learning; terminology, please refer to; [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary/). ### Training set. First, to set up, lets pull the docker images. ```bash; sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image.; sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image.; ```. The `make_examples` step doesn't use GPU, so we will not require the GPU-enabled; image. ```bash; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log""; ```. This took `20m14s`. Starting in v1.4.0, we added an extra channel in our WGS setting using the; `--channels ""insert_size""` flag. And, the make_examples step creates; `*.example_info.json` files. For example, you can see it here:. ```; cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json""; ```. ```json; {; ""version"": ""1.6.1"",; ""shape"": [100, 221, 7],; ""channels"": [1, 2, 3, 4, 5, 6, 19]; }; ```. Depending on your data type, you might want to tweak the flags for the; `make_examples` step, which can result in different shape of the output; examples. We will want to shuffle this on Dataflow later, so we copy the data to GCS; bucket first:. ```; gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. NOTE: If you prefer shuffling locally, please take a look at this user-provided; shuffler option:; https://github.com/google/deepvariant/issues/360#issuecomment-1019990366. ### Validation set. ```; ( time s",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:6110,log,log,6110,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['log'],['log'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  training works. For the definition of these 3 sets in commonly used machine learning; terminology, please refer to; [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary/). ### Training set. First, to set up, lets pull the docker images. ```bash; sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image.; sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image.; ```. The `make_examples` step doesn't use GPU, so we will not require the GPU-enabled; image. ```bash; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log""; ```. This took `20m14s`. Starting in v1.4.0, we added an extra channel in our WGS setting using the; `--channels ""insert_size""` flag. And, the make_examples step creates; `*.example_info.json` files. For example, you can see it here:. ```; cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json""; ```. ```json; {; ""version"": ""1.6.1"",; ""shape"": [100, 221, 7],; ""channels"": [1, 2, 3, 4, 5, 6, 19]; }; ```. Depending on your data type, you might want to tweak the flags for the; `make_examples` step, which can result in different shape of the output; examples. We will want to shuffle this on Dataflow later, so we copy the data to GCS; bucket first:. ```; gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. NOTE: If you prefer shuffling locally, please take a look at this user-provided; shuffler option:; https://github.com/google/deepvariant/issues/360#issuecomment-1019990366. ### Validation set. ```; ( time s
",True,"The content consists primarily of code snippets and program logs, including bash commands and script outputs which are typically associated with the execution steps rather than meaningful human-readable text."
Testability," variant calls; with `hap.py`. In addition we evaluate a Mendelian violation rate for a merged; VCF. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmar",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:1233,benchmark,benchmark,1233,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  variant calls; with `hap.py`. In addition we evaluate a Mendelian violation rate for a merged; VCF. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmar
",False,"The text contains a series of commands and steps for downloading data, setting up an environment, installing tools like Docker, and preparing reference files. While there are some technical instructions, the content includes meaningful sentences that explain the purpose and process, rather than consisting solely of code or logs."
Testability," were variant in at least 1 family member and checked for Mendelian constraints; 10169/176481 (5.76%) records had indeterminate consistency status due to incomplete calls; 6610/176481 (3.75%) records contained a violation of Mendelian constraints; ```. ### Benchmark variant calls against 4.2.1 truth set with hap.py. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TR",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:9955,benchmark,benchmark,9955,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,2,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  were variant in at least 1 family member and checked for Mendelian constraints; 10169/176481 (5.76%) records had indeterminate consistency status due to incomplete calls; 6610/176481 (3.75%) records contained a violation of Mendelian constraints; ```. ### Benchmark variant calls against 4.2.1 truth set with hap.py. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TR
",False,
Testability,""" \; -v ""${PWD}:${PWD}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; --reads=${PWD}/${BAM} \; --output_vcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; --output_gvcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.g.vcf.gz \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --num_shards=$(nproc); ```. Stage | Time (minutes); -------------------------------- | -----------------; make_examples | 116m37.385s; call_variants | 214m37.055s; postprocess_variants (with gVCF) | 30m59.968s. ### Run hap.py. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; ${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; -f ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -o ${PWD}/happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Pr",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:6404,benchmark,benchmark,6404,docs/deepvariant-vg-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: "" \; -v ""${PWD}:${PWD}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; --reads=${PWD}/${BAM} \; --output_vcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; --output_gvcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.g.vcf.gz \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --num_shards=$(nproc); ```. Stage | Time (minutes); -------------------------------- | -----------------; make_examples | 116m37.385s; call_variants | 214m37.055s; postprocess_variants (with gVCF) | 30m59.968s. ### Run hap.py. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; ${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; -f ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -o ${PWD}/happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Pr
",False,
Testability,"""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf /output/HG001.output.vcf.gz \; --output_gvcf /output/HG001.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-51-0.995354.ckpt; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG001.output.vcf.gz \; -f /benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 9974 9947 27 21052 9 10750 3 5 0.997293 0.999126 0.510640 0.998209 NaN NaN 1.630447 2.156149; INDEL PASS 9974 9947 27 21052 9 10750 3 5 0.997293 0.999126 0.510640 0.998209 NaN NaN 1.630447 2.156149; SNP ALL 69175 68874 301 85030 44 16068 8 2 0.995649 0.999362 0.188969 0.997502 2.288757 2.084645 1.730097 1.781789; SNP PASS 69175 68874 301 85030 44 16068 8 2 0.995649 0.999362 0.188969 0.997502 2.288757 2.084645 1.730097 1.781789; ```. To summarize:.",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md:2879,benchmark,benchmark,2879,docs/deepvariant-complete-t7-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf /output/HG001.output.vcf.gz \; --output_gvcf /output/HG001.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-51-0.995354.ckpt; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG001.output.vcf.gz \; -f /benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 9974 9947 27 21052 9 10750 3 5 0.997293 0.999126 0.510640 0.998209 NaN NaN 1.630447 2.156149; INDEL PASS 9974 9947 27 21052 9 10750 3 5 0.997293 0.999126 0.510640 0.998209 NaN NaN 1.630447 2.156149; SNP ALL 69175 68874 301 85030 44 16068 8 2 0.995649 0.999362 0.188969 0.997502 2.288757 2.084645 1.730097 1.781789; SNP PASS 69175 68874 301 85030 44 16068 8 2 0.995649 0.999362 0.188969 0.997502 2.288757 2.084645 1.730097 1.781789; ```. To summarize:.
",False,"The content includes benchmarking results and statistics which are human-readable, descriptive, and provide insights into performance metrics."
Testability,"# Calling variants in non-autosomal contigs. For details about the support for haploid contigs, please read; [DeepVariant haploid support](deepvariant-haploid-support.md). In this case study, we describe how to call variants in non-autosomal regions; like X, Y chromosomes. Then we assess the quality of the DeepVariant variant; calls with `hap.py`. The dataset used in this case-study has following attributes:. ```bash; Sample: HG002; Region: ChrX, ChrY; Platform: PacBio; Sample Karyotype: X, Y; ```. ## Prepare environment. In this case study, we will use [Docker](https://docs.docker.com/get-docker/) to; run DeepVariant for variant calling and; [hap.py](https://github.com/illumina/hap.py) for benchmarking. If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/XY-walkthrough"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}/data"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/xy-case-study-testdata; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai. HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata; curl ${HTTPDIR}",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md:700,benchmark,benchmarking,700,docs/deepvariant-xy-calling-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md,1,['benchmark'],['benchmarking'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # Calling variants in non-autosomal contigs. For details about the support for haploid contigs, please read; [DeepVariant haploid support](deepvariant-haploid-support.md). In this case study, we describe how to call variants in non-autosomal regions; like X, Y chromosomes. Then we assess the quality of the DeepVariant variant; calls with `hap.py`. The dataset used in this case-study has following attributes:. ```bash; Sample: HG002; Region: ChrX, ChrY; Platform: PacBio; Sample Karyotype: X, Y; ```. ## Prepare environment. In this case study, we will use [Docker](https://docs.docker.com/get-docker/) to; run DeepVariant for variant calling and; [hap.py](https://github.com/illumina/hap.py) for benchmarking. If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/XY-walkthrough"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}/data"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/xy-case-study-testdata; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai. HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata; curl ${HTTPDIR}
",False,
Testability,"# DeepTrio quick start. This document explains how to quickly start using; [DeepTrio](deeptrio-details.md) to generate variant calls for trio samples. This; tutorial does not cover all possible settings of DeepTrio. It is intended to be; a starting point for using DeepTrio. ## Background. To get started, we've provided a Docker image, and some test data in a bucket on; Google Cloud Storage. The instructions below show how to download the data; through the corresponding public URLs. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepTrio in one command. Although DeepTrio can be built from a source, we provide a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test b",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:346,test,test,346,docs/deeptrio-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md,1,['test'],['test'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # DeepTrio quick start. This document explains how to quickly start using; [DeepTrio](deeptrio-details.md) to generate variant calls for trio samples. This; tutorial does not cover all possible settings of DeepTrio. It is intended to be; a starting point for using DeepTrio. ## Background. To get started, we've provided a Docker image, and some test data in a bucket on; Google Cloud Storage. The instructions below show how to download the data; through the corresponding public URLs. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepTrio in one command. Although DeepTrio can be built from a source, we provide a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test b
",False,"The text contains complete, meaningful sentences in natural language discussing how to quickly start using DeepTrio, requirements, and setup instructions."
Testability,"# DeepVariant Complete Genomics G400 case study. In this case study, we describe applying DeepVariant to a Complete Genomics G400; sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics G400 HG002 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam > input/HG002.complete_g400.V350151728.grch38.chr20.bam. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam.bai > input/HG002.complete_g400.V350151728.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG002. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics G400 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.data-00000-of-00001 > input/weights-60-0.993753.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.index > input/weights-60-0.993753.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/referenc",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md:538,test,testdata,538,docs/deepvariant-complete-g400-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md,2,"['benchmark', 'test']","['benchmark', 'testdata']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # DeepVariant Complete Genomics G400 case study. In this case study, we describe applying DeepVariant to a Complete Genomics G400; sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics G400 HG002 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam > input/HG002.complete_g400.V350151728.grch38.chr20.bam. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam.bai > input/HG002.complete_g400.V350151728.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG002. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics G400 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.data-00000-of-00001 > input/weights-60-0.993753.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.index > input/weights-60-0.993753.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/referenc
",False,"The content consists of a mix of natural language and code. While there are some descriptive sentences, the majority is code snippets for downloading data and running an application, which might be considered programmatic API descriptions or build steps."
Testability,"# DeepVariant Complete Genomics T7 case study. In this case study, we describe applying DeepVariant to a Complete Genomics T7; sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics T7 HG001 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG001. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics T7 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.data-00000-of-00001 > input/weights-51-0.995354.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.index > input/weights-51-0.995354.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md:532,test,testdata,532,docs/deepvariant-complete-t7-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md,2,"['benchmark', 'test']","['benchmark', 'testdata']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # DeepVariant Complete Genomics T7 case study. In this case study, we describe applying DeepVariant to a Complete Genomics T7; sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics T7 HG001 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG001. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics T7 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.data-00000-of-00001 > input/weights-51-0.995354.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.index > input/weights-51-0.995354.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt
",True,"The text consists of command lines and code snippets used to download data and run an application, which are primarily programmatic instructions rather than meaningful human-readable sentences."
Testability,"# DeepVariant RNA-seq Case Study. This case study will demonstrate how to run DeepVariant using the RNA-seq model,; and evaluate the result using `hap.py`. ## Overview. ### Tools. We will use the following tools:. * [Docker](https://docs.docker.com/get-docker/) - Used to run DeepVariant.; * [mosdepth](https://github.com/brentp/mosdepth) - For calculating coverage.; * [bedtools](https://bedtools.readthedocs.io) - Used to intersect bedfiles.; * [hap.py](https://github.com/illumina/hap.py) - Used to evaluate the results.; We will use Docker to run `hap.py`. ### Data. We will use these data in our analysis. Files will be downloaded in subsequent; steps. * HG005 RNA-seq BAM; * Model Checkpoint Files; * GRCh38 Reference + Index; * CDS bedfile (chr20 only); * GIAB benchmark data. ## Prepare Data. ### Setup directories. Lets first create directories to organize files. ```bash; mkdir -p data benchmark reference model output happy; ```. ### Download the GRCh38 Reference. We will be using GRCh38 for this case study. ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:768,benchmark,benchmark,768,docs/deepvariant-rnaseq-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md,2,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # DeepVariant RNA-seq Case Study. This case study will demonstrate how to run DeepVariant using the RNA-seq model,; and evaluate the result using `hap.py`. ## Overview. ### Tools. We will use the following tools:. * [Docker](https://docs.docker.com/get-docker/) - Used to run DeepVariant.; * [mosdepth](https://github.com/brentp/mosdepth) - For calculating coverage.; * [bedtools](https://bedtools.readthedocs.io) - Used to intersect bedfiles.; * [hap.py](https://github.com/illumina/hap.py) - Used to evaluate the results.; We will use Docker to run `hap.py`. ### Data. We will use these data in our analysis. Files will be downloaded in subsequent; steps. * HG005 RNA-seq BAM; * Model Checkpoint Files; * GRCh38 Reference + Index; * CDS bedfile (chr20 only); * GIAB benchmark data. ## Prepare Data. ### Setup directories. Lets first create directories to organize files. ```bash; mkdir -p data benchmark reference model output happy; ```. ### Download the GRCh38 Reference. We will be using GRCh38 for this case study. ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}
",False,"The text contains complete, meaningful sentences in natural language discussing how to run DeepVariant and evaluate results using hap.py. It includes step-by-step instructions for setting up directories, downloading data, and preparing files."
Testability,"# DeepVariant quick start. This is an explanation of how to use DeepVariant. ## Background. To get started, you'll need the DeepVariant programs (and some packages they; depend on), some test data, and of course a place to run them. We've provided a Docker image, and some test data in a bucket on Google Cloud; Storage. The instructions below show how to download the data through the; corresponding public URLs from these data. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepVariant in one command. Starting from the 0.8 release, we introduced one convenient command that will; run through all 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:187,test,test,187,docs/deepvariant-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md,2,['test'],['test'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # DeepVariant quick start. This is an explanation of how to use DeepVariant. ## Background. To get started, you'll need the DeepVariant programs (and some packages they; depend on), some test data, and of course a place to run them. We've provided a Docker image, and some test data in a bucket on Google Cloud; Storage. The instructions below show how to download the data through the; corresponding public URLs from these data. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepVariant in one command. Starting from the 0.8 release, we introduced one convenient command that will; run through all 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; 
",True,"The text contains code snippets, such as bash commands and script instructions, which are primarily used for program execution rather than providing explanatory or descriptive content. It also includes references to specific tools like BWA and Docker, which might be relevant, but the main focus is on step-by-step execution commands rather than a comprehensive explanation."
Testability,"# DeepVariant support for variant calling in chromosome X and Y. ## Case study. A case study on how to use the parameters mentioned here are described in; [DeepVariant X, Y calling case study](deepvariant-xy-calling-case-study.md). ## Haploid calling support. As DeepVariant is a diploid variant caller, it assigns genotypes as {Hom-ref,; Het, Hom-alt} for each candidate allele it observes. For samples with karyotype; XY, the chromosome X and Y are effectively haploid. So, we are introducing two; flags to re-adjust the genotypes in regions that are considered to be haploid; for those samples. You can use `--haploid_contigs` and `--par_regions_bed` parameters to readjust; the genotypes in haploid regions. For samples with XY karyotype, it is expected; that users will set `--haploid_contigs=""chrX,chrY""` for; [GRCh38](https://storage.googleapis.com/deepvariant/case-study-testdata/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa); and `--haploid_contigs=""X,Y""` for; [GRCh37](https://storage.googleapis.com/deepvariant/case-study-testdata/hs37d5.fa).; You can also provide a PAR region bed file with; `--par_regions_bed=""/input/GRCh3X_par.bed""` parameter. The regions in the PAR; bed file will be skipped from genotype readjustment. You can download the PAR; bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). ## How it works. The genotype re-adjustment is implemented in the `postprocess_variants` stage of; DeepVariant. For any variant, that is in the`--haploid_contigs` regions and; **not** in the `--par_regions_bed` regions, the genotype likelihoods of; heterozygous variants are set as 0 and the genotypes are normalized again after; re-adjusting the likelihoods. After that the most-likely genotype is assigned to; the allele which excludes any heterozygous calls. For example, suppose we observe an alternate allele `ALT1`",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-haploid-support.md:879,test,testdata,879,docs/deepvariant-haploid-support.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-haploid-support.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # DeepVariant support for variant calling in chromosome X and Y. ## Case study. A case study on how to use the parameters mentioned here are described in; [DeepVariant X, Y calling case study](deepvariant-xy-calling-case-study.md). ## Haploid calling support. As DeepVariant is a diploid variant caller, it assigns genotypes as {Hom-ref,; Het, Hom-alt} for each candidate allele it observes. For samples with karyotype; XY, the chromosome X and Y are effectively haploid. So, we are introducing two; flags to re-adjust the genotypes in regions that are considered to be haploid; for those samples. You can use `--haploid_contigs` and `--par_regions_bed` parameters to readjust; the genotypes in haploid regions. For samples with XY karyotype, it is expected; that users will set `--haploid_contigs=""chrX,chrY""` for; [GRCh38](https://storage.googleapis.com/deepvariant/case-study-testdata/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa); and `--haploid_contigs=""X,Y""` for; [GRCh37](https://storage.googleapis.com/deepvariant/case-study-testdata/hs37d5.fa).; You can also provide a PAR region bed file with; `--par_regions_bed=""/input/GRCh3X_par.bed""` parameter. The regions in the PAR; bed file will be skipped from genotype readjustment. You can download the PAR; bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). ## How it works. The genotype re-adjustment is implemented in the `postprocess_variants` stage of; DeepVariant. For any variant, that is in the`--haploid_contigs` regions and; **not** in the `--par_regions_bed` regions, the genotype likelihoods of; heterozygous variants are set as 0 and the genotypes are normalized again after; re-adjusting the likelihoods. After that the most-likely genotype is assigned to; the allele which excludes any heterozygous calls. For example, suppose we observe an alternate allele `ALT1`
",False,
Testability,"# DeepVariant whole exome sequencing (WES) case study. Similar to the [case study on whole genome sequencing data], in this; study we describe applying DeepVariant to a real exome sample using a single; machine. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md:895,benchmark,benchmark,895,docs/deepvariant-exome-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md,2,['benchmark'],"['benchmark', 'benchmarks']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # DeepVariant whole exome sequencing (WES) case study. Similar to the [case study on whole genome sequencing data], in this; study we describe applying DeepVariant to a real exome sample using a single; machine. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation
",False,"The content contains a mix of explanatory text, step-by-step instructions, and code snippets. While there are command lines and URLs, these are part of the procedure description and not purely programmatic logs or API docs. The overall text is informative for setting up a case study, explaining tools used, and providing download steps, which are helpful to readers."
Testability,"# DeepVariant whole genome sequencing case study. In this case study, we describe applying DeepVariant to a real WGS sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPD",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:961,benchmark,benchmark,961,docs/deepvariant-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # DeepVariant whole genome sequencing case study. In this case study, we describe applying DeepVariant to a real WGS sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPD
",False,"The provided text contains a detailed case study with meaningful human-readable sentences describing the process of applying DeepVariant and assessing variant calls, including explanatory content about tools and steps. There are no code snippets or program logs present."
Testability,"# DeepVariant with Oxford Nanopore R10.4.1 Duplex reads. In this case study, we describe applying DeepVariant to Oxford Nanopore R10.4.1; duplex reads. Then we assess the quality of the DeepVariant variant calls with; `hap.py`. To make it faster to go over this case study, we run only on chromosome 20. The dataset used in this case-study has following attributes:. ```bash; Sample: HG002; Region: Chr20; Chemistry: ONT R10.4.1 Duplex; Basecaller: Dorado v0.1.1; Coverage: 80x; ```. **Model note:**. * The model is trained with Guppy 6+ ""SUP"" Simplex and Dorado v0.1.1 Duplex; reads. * The model is trained on both Ultra-long and sheared reads with varying read; N50 and coverage. ## Prepare environment. In this case-study, we will use [Docker](https://docs.docker.com/get-docker/) to; run DeepVariant for variant calling and; [hap.py](https://github.com/illumina/hap.py) for benchmarking. If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/ont-case-study-duplex"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input/data""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG002 Duplex chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam > ${INPUT_DIR}/HG002_R",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md:878,benchmark,benchmarking,878,docs/deepvariant-ont-r104-duplex-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md,1,['benchmark'],['benchmarking'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # DeepVariant with Oxford Nanopore R10.4.1 Duplex reads. In this case study, we describe applying DeepVariant to Oxford Nanopore R10.4.1; duplex reads. Then we assess the quality of the DeepVariant variant calls with; `hap.py`. To make it faster to go over this case study, we run only on chromosome 20. The dataset used in this case-study has following attributes:. ```bash; Sample: HG002; Region: Chr20; Chemistry: ONT R10.4.1 Duplex; Basecaller: Dorado v0.1.1; Coverage: 80x; ```. **Model note:**. * The model is trained with Guppy 6+ ""SUP"" Simplex and Dorado v0.1.1 Duplex; reads. * The model is trained on both Ultra-long and sheared reads with varying read; N50 and coverage. ## Prepare environment. In this case-study, we will use [Docker](https://docs.docker.com/get-docker/) to; run DeepVariant for variant calling and; [hap.py](https://github.com/illumina/hap.py) for benchmarking. If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/ont-case-study-duplex"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input/data""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG002 Duplex chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam > ${INPUT_DIR}/HG002_R
",False,"The text contains complete, meaningful sentences in natural language discussing the application of DeepVariant and assessment using hap.py, along with setup instructions for the case study. It includes commands which are likely part of the procedural description but does not solely consist of code or logs. The content is explanatory and human-readable."
Testability,"# DeepVariant with Oxford Nanopore R10.4.1 Simplex reads. In this case study, we describe applying DeepVariant to Oxford Nanopore R10.4.1; simplex reads. Then we assess the quality of the DeepVariant variant calls with; `hap.py`. To make it faster to go over this case study, we run only on chromosome 20. The dataset used in this case-study has following attributes:. ```bash; Sample: HG003; Region: Chr20; Chemistry: ONT R10.4.1; Coverage: 80x; ```. **Model note:**. * The model is trained with Guppy 6+ ""SUP"" Simplex and Dorado v0.1.1 Duplex; reads. * The model is trained on both Ultra-long and sheared reads with varying read; N50 and coverage. ## Prepare environment. In this case-study, we will use [Docker](https://docs.docker.com/get-docker/) to; run DeepVariant for variant calling and; [hap.py](https://github.com/illumina/hap.py) for benchmarking. If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/ont-case-study"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input/data""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG003 Ultra-long chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam; curl ${HTTPDIR}/HG003_R104_sup_merged",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md:846,benchmark,benchmarking,846,docs/deepvariant-ont-r104-simplex-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md,1,['benchmark'],['benchmarking'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # DeepVariant with Oxford Nanopore R10.4.1 Simplex reads. In this case study, we describe applying DeepVariant to Oxford Nanopore R10.4.1; simplex reads. Then we assess the quality of the DeepVariant variant calls with; `hap.py`. To make it faster to go over this case study, we run only on chromosome 20. The dataset used in this case-study has following attributes:. ```bash; Sample: HG003; Region: Chr20; Chemistry: ONT R10.4.1; Coverage: 80x; ```. **Model note:**. * The model is trained with Guppy 6+ ""SUP"" Simplex and Dorado v0.1.1 Duplex; reads. * The model is trained on both Ultra-long and sheared reads with varying read; N50 and coverage. ## Prepare environment. In this case-study, we will use [Docker](https://docs.docker.com/get-docker/) to; run DeepVariant for variant calling and; [hap.py](https://github.com/illumina/hap.py) for benchmarking. If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/ont-case-study"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input/data""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG003 Ultra-long chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam; curl ${HTTPDIR}/HG003_R104_sup_merged
",True,"The provided text is a step-by-step guide for setting up and running DeepVariant, including code snippets, bash commands, and configuration details. This content is more suited for programmatic execution rather than human-readable explanation."
Testability,"# Getting Started with GCP. DeepVariant doesn't require GCP, but if you want to use it, these are some; instructions that we found to be useful when getting started. ## Set up a Google Cloud account. To get started using DeepVariant on Google Cloud Platform (GCP), you first need; to set up an account and a project to contain your cloud resources. * If you do not have an account yet, you should create one at; [cloud.google.com](https://cloud.google.com). You should then [enable; billing for your; account](https://support.google.com/cloud/answer/6288653?hl=en) but note; that if your account is new, [you receive $300 of free; credit](https://cloud.google.com/free/). Once your cloud account is set up,; you should be able to log in to the [Cloud; Console](https://console.cloud.google.com) to view or administer your cloud; resources. * From the Cloud Console, [set up a; project](https://cloud.google.com/resource-manager/docs/creating-managing-projects); to house all of the cloud resources (storage, compute, services) that you; will associate with your use of DeepVariant. For example, if your; organization is AcmeCorp, you might call your project; `acmecorp-deepvariant`. * Finally, please visit the [""Compute Engine"" page on Cloud; Console](https://console.cloud.google.com/compute). You don't need to create; Compute Engine instances at this time, but simply visiting this page will; initialize your compute engine ""service account"" so that we can authorize; it. (As you progress in your use of Google Cloud Platform, you will likely find it; useful to create a [Cloud; Organization](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on you",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:730,log,log,730,docs/deepvariant-gcp-info.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md,1,['log'],['log'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # Getting Started with GCP. DeepVariant doesn't require GCP, but if you want to use it, these are some; instructions that we found to be useful when getting started. ## Set up a Google Cloud account. To get started using DeepVariant on Google Cloud Platform (GCP), you first need; to set up an account and a project to contain your cloud resources. * If you do not have an account yet, you should create one at; [cloud.google.com](https://cloud.google.com). You should then [enable; billing for your; account](https://support.google.com/cloud/answer/6288653?hl=en) but note; that if your account is new, [you receive $300 of free; credit](https://cloud.google.com/free/). Once your cloud account is set up,; you should be able to log in to the [Cloud; Console](https://console.cloud.google.com) to view or administer your cloud; resources. * From the Cloud Console, [set up a; project](https://cloud.google.com/resource-manager/docs/creating-managing-projects); to house all of the cloud resources (storage, compute, services) that you; will associate with your use of DeepVariant. For example, if your; organization is AcmeCorp, you might call your project; `acmecorp-deepvariant`. * Finally, please visit the [""Compute Engine"" page on Cloud; Console](https://console.cloud.google.com/compute). You don't need to create; Compute Engine instances at this time, but simply visiting this page will; initialize your compute engine ""service account"" so that we can authorize; it. (As you progress in your use of Google Cloud Platform, you will likely find it; useful to create a [Cloud; Organization](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on you
",False,"The text provided contains detailed instructions on setting up a Google Cloud account and project, which is explanatory content. It includes step-by-step guides for creating an account, enabling billing, setting up projects, and initializing services like Compute Engine. While the content is technical in nature, it's written in a natural language format without code snippets or logs, making it human-readable and meaningful."
Testability,"## Building DeepTrio from sources. DeepTrio is a part of DeepVariant. DeepVariant comes with scripts to build it; from the source code. For more details please refer to; [""Building DeepVariant from sources""](deepvariant-build-test.md); ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-build-test.md:226,test,test,226,docs/deeptrio-build-test.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-build-test.md,1,['test'],['test'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ## Building DeepTrio from sources. DeepTrio is a part of DeepVariant. DeepVariant comes with scripts to build it; from the source code. For more details please refer to; [""Building DeepVariant from sources""](deepvariant-build-test.md); 
",False,"This text includes sentences that are complete and meaningful in natural language, discussing concepts related to building DeepTrio which is part of DeepVariant. The content is explanatory and written in a human-readable format without any code snippets or logs."
Testability,"## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md:843,test,tests,843,docs/deepvariant-build-test.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md,3,['test'],['tests'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; 
",False,"The text includes commands and shell instructions that are meant to be executed, but it's presented in a way that might be more appropriate for a user guide or installation manual. It also contains descriptive paragraphs about the process of building DeepVariant and preparing machines, which are meaningful sentences."
Testability,"------------------------------------------------------------------------: | -----------; [Docker](docs/deepvariant-quick-start.md) | This is the recommended method.; [Build from source](docs/deepvariant-build-test.md) | DeepVariant comes with scripts to build it on Ubuntu 20.04. To build and run on other Unix-based systems, you will need to modify these scripts.; Prebuilt Binaries | Available at [`gs://deepvariant/`](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you will need a CPU (such as Intel Sandy Bridge) that supports them. You can check the `/proc/cpuinfo` file on your computer, which lists these features under ""flags"". ## Contribution Guidelines. Please [open a pull request](https://github.com/google/deepvariant/compare) if; you wish to contribute to DeepVariant. Note, we have not set up the; infrastructure to merge pull requests externally. If you agree, we will test and; submit the changes internally and mention your contributions in our; [release notes](https://github.com/google/deepvariant/releases). We apologize; for any inconvenience. If you have any difficulty using DeepVariant, feel free to; [open an issue](https://github.com/google/deepvariant/issues/new). If you have; general questions not specific to DeepVariant, we recommend that you post on a; community discussion forum such as [BioStars](https://www.biostars.org/). ## License. [BSD-3-Clause license](LICENSE). ## Acknowledgements. DeepVariant happily makes use of many open source packages. We would like to; specifically call out a few key ones:. * [Boost Graph Library](http://www.boost.org/doc/libs/1_65_1/libs/graph/doc/index.html); * [abseil-cpp](https://github.com/abseil/abseil-cpp) and; [abseil-py](https://github.com/abseil/abseil-py); * [CLIF](https://github.com/google/clif); * [GNU Parallel](https://www.gnu.org/software/parallel/); * [htslib & samtools](http://www.htslib.org/); * [Nucleus](https://github.com/google/nucl",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:11201,test,test,11201,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['test'],['test'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ------------------------------------------------------------------------: | -----------; [Docker](docs/deepvariant-quick-start.md) | This is the recommended method.; [Build from source](docs/deepvariant-build-test.md) | DeepVariant comes with scripts to build it on Ubuntu 20.04. To build and run on other Unix-based systems, you will need to modify these scripts.; Prebuilt Binaries | Available at [`gs://deepvariant/`](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you will need a CPU (such as Intel Sandy Bridge) that supports them. You can check the `/proc/cpuinfo` file on your computer, which lists these features under ""flags"". ## Contribution Guidelines. Please [open a pull request](https://github.com/google/deepvariant/compare) if; you wish to contribute to DeepVariant. Note, we have not set up the; infrastructure to merge pull requests externally. If you agree, we will test and; submit the changes internally and mention your contributions in our; [release notes](https://github.com/google/deepvariant/releases). We apologize; for any inconvenience. If you have any difficulty using DeepVariant, feel free to; [open an issue](https://github.com/google/deepvariant/issues/new). If you have; general questions not specific to DeepVariant, we recommend that you post on a; community discussion forum such as [BioStars](https://www.biostars.org/). ## License. [BSD-3-Clause license](LICENSE). ## Acknowledgements. DeepVariant happily makes use of many open source packages. We would like to; specifically call out a few key ones:. * [Boost Graph Library](http://www.boost.org/doc/libs/1_65_1/libs/graph/doc/index.html); * [abseil-cpp](https://github.com/abseil/abseil-cpp) and; [abseil-py](https://github.com/abseil/abseil-py); * [CLIF](https://github.com/google/clif); * [GNU Parallel](https://www.gnu.org/software/parallel/); * [htslib & samtools](http://www.htslib.org/); * [Nucleus](https://github.com/google/nucl
",True,"The text contains build instructions and configuration details that are more suited for developers rather than general readers. It includes command-line tools, script modifications, and specific system requirements which fall under the category of programmatic documentation."
Testability,"---; name: 'Problem encountered while running DeepVariant'; about: 'Tell us what happened, so we can try to help'; title: ''; labels: ''; assignees: ''. ---. **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**. ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/.github/ISSUE_TEMPLATE/problem_report.md:650,test,test,650,.github/ISSUE_TEMPLATE/problem_report.md,,https://github.com/google/deepvariant/tree/v1.6.1/.github/ISSUE_TEMPLATE/problem_report.md,2,['test'],['test'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ---; name: 'Problem encountered while running DeepVariant'; about: 'Tell us what happened, so we can try to help'; title: ''; labels: ''; assignees: ''. ---. **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**. 
",True,"The content consists of a form template for reporting issues, which includes various structured input fields and instructions for users, rather than containing complete sentences or meaningful human-readable text."
Testability,"-p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --haploid_contigs ""${HAPLOID_CONTIGS}"" \; --par_regions_bed ""${INPUT_DIR}/${PAR_BED}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Benchmark X, Y outputs from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v1.0 of the Genome in a Bottle; small variant benchmarks for HG002_chrXY. ```bash; FTPDIR=https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/chrXY_v1.0/GRCh38/SmallVariant. curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.bed > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.bed; curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz.tbi > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz.tbi. TRUTH_VCF=""HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz""; TRUTH_BED=""HG002_GRCh38_chrXY_smallvar_v1.0.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. REGION=""chrX,chrY""; sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; -f ""${IN",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md:3427,benchmark,benchmark,3427,docs/deepvariant-xy-calling-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --haploid_contigs ""${HAPLOID_CONTIGS}"" \; --par_regions_bed ""${INPUT_DIR}/${PAR_BED}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Benchmark X, Y outputs from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v1.0 of the Genome in a Bottle; small variant benchmarks for HG002_chrXY. ```bash; FTPDIR=https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/chrXY_v1.0/GRCh38/SmallVariant. curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.bed > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.bed; curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz.tbi > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz.tbi. TRUTH_VCF=""HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz""; TRUTH_BED=""HG002_GRCh38_chrXY_smallvar_v1.0.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. REGION=""chrX,chrY""; sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; -f ""${IN
",False,"The content includes bash commands for Docker operations such as pulling and running images. These are code snippets related to system operation, which should be filtered out as they consist primarily of programmatic instructions."
Testability,"-provided; shuffler option:; https://github.com/google/deepvariant/issues/360#issuecomment-1019990366. ### Validation set. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. This took: 5m31.122s. Copy to GCS bucket:. ```bash; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. ## Shuffle each set of examples and generate a data configuration file for each. Shuffling the `tensorflow.Example`s is an important step for training a model.; In our training logic, we shuffle examples globally using a preprocessing step. First, if you have run this step before, and want to rerun it, you might want to; consider cleaning up previous data first to avoid confusion:. ```bash; # (Optional) Clean up existing files.; gsutil -m rm -f ""${OUTPUT_BUCKET}/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; gsutil -m rm -f ""${OUTPUT_BUCKET}/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt""; gsutil rm -f ""${OUTPUT_BUCKET}/example_info.json""; ```. Here we provide examples for running on; [Cloud Dataflow Runner](https://beam.apache.org/documentation/runners/dataflow/); and also [DirectRunner](https://beam.apache.org/documentation/runners/direct/).; Beam can also use other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machi",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:7957,log,logic,7957,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['log'],['logic'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: -provided; shuffler option:; https://github.com/google/deepvariant/issues/360#issuecomment-1019990366. ### Validation set. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. This took: 5m31.122s. Copy to GCS bucket:. ```bash; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. ## Shuffle each set of examples and generate a data configuration file for each. Shuffling the `tensorflow.Example`s is an important step for training a model.; In our training logic, we shuffle examples globally using a preprocessing step. First, if you have run this step before, and want to rerun it, you might want to; consider cleaning up previous data first to avoid confusion:. ```bash; # (Optional) Clean up existing files.; gsutil -m rm -f ""${OUTPUT_BUCKET}/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; gsutil -m rm -f ""${OUTPUT_BUCKET}/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt""; gsutil rm -f ""${OUTPUT_BUCKET}/example_info.json""; ```. Here we provide examples for running on; [Cloud Dataflow Runner](https://beam.apache.org/documentation/runners/dataflow/); and also [DirectRunner](https://beam.apache.org/documentation/runners/direct/).; Beam can also use other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machi
",False,"The text contains meaningful sentences discussing the process of data shuffling and example generation for training a model. It includes explanations about using different runners in Apache Beam and instructions for setting up a virtual environment. The content is human-readable, explanatory, and consists of complete sentences in natural language."
Testability,"-v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG002.complete_g400.V350151728.grch38.chr20.bam \; --output_vcf /output/HG002.output.vcf.gz \; --output_gvcf /output/HG002.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-60-0.993753.ckpt; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11130 126 20925 31 9340 26 4 0.988806 0.997324 0.446356 0.993047 NaN NaN 1.561710 2.049106; INDEL PASS 11256 11130 126 20925 31 9340 26 4 0.988806 0.997324 0.446356 0.993047 NaN NaN 1.561710 2.049106; SNP ALL 71333 70949 384 85736 50 14689 28 6 0.994617 0.999296 0.171328 0.996951 2.314904 2.102286 1.715978 1.753768; SNP PASS 71333 70949 384 85736 50 14689 28 6 0.994617 0.999296 0.171328 0.996951 2.314904 2.102286 1.715978 1.75",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md:2925,benchmark,benchmark,2925,docs/deepvariant-complete-g400-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG002.complete_g400.V350151728.grch38.chr20.bam \; --output_vcf /output/HG002.output.vcf.gz \; --output_gvcf /output/HG002.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-60-0.993753.ckpt; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11130 126 20925 31 9340 26 4 0.988806 0.997324 0.446356 0.993047 NaN NaN 1.561710 2.049106; INDEL PASS 11256 11130 126 20925 31 9340 26 4 0.988806 0.997324 0.446356 0.993047 NaN NaN 1.561710 2.049106; SNP ALL 71333 70949 384 85736 50 14689 28 6 0.994617 0.999296 0.171328 0.996951 2.314904 2.102286 1.715978 1.753768; SNP PASS 71333 70949 384 85736 50 14689 28 6 0.994617 0.999296 0.171328 0.996951 2.314904 2.102286 1.715978 1.75
",True,"The text consists of code snippets and program logs. It includes parameters, commands, and outputs from a script or tool, which are typically not meaningful on their own for human reading."
Testability,". If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/ont-case-study"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input/data""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG003 Ultra-long chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam.bai > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam.bai. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG003_R104_sup_merged.80x.chr20.bam""; THREADS=$(nproc); REGION=""chr20"". # Set up output variable; OUTPUT_VCF=""HG003_UL_R1041_Guppy6_sup_2_GRCh38.chr20.output.vcf.gz""; OUTPUT_GVCF=""HG003_UL_R1041_Guppy6_sup_2_GRCh38.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ONT_R104 \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md:1850,test,testdata,1850,docs/deepvariant-ont-r104-simplex-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: . If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/ont-case-study"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input/data""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG003 Ultra-long chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam.bai > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam.bai. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG003_R104_sup_merged.80x.chr20.bam""; THREADS=$(nproc); REGION=""chr20"". # Set up output variable; OUTPUT_VCF=""HG003_UL_R1041_Guppy6_sup_2_GRCh38.chr20.output.vcf.gz""; OUTPUT_GVCF=""HG003_UL_R1041_Guppy6_sup_2_GRCh38.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ONT_R104 \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}
",True,"The text consists primarily of code snippets and command lines, including `bash` commands, file downloads, directory setups, and script executions. It is more suitable for a programming guide or build instructions rather than human-readable content."
Testability,".; * PacBio HiFi data, see the; [PacBio case study](docs/deeptrio-pacbio-case-study.md). Please also note:. * All DeepTrio models were trained on human data.; * It is possible to use DeepTrio with only 2 samples (child, and one parent).; * External tool [GLnexus](https://github.com/dnanexus-rnd/GLnexus) is used to; merge output VCFs. ## How to run DeepVariant. We recommend using our Docker solution. The command will look like this:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**; --ref=/input/YOUR_REF \; --reads=/input/YOUR_BAM \; --output_vcf=/output/YOUR_OUTPUT_VCF \; --output_gvcf=/output/YOUR_OUTPUT_GVCF \; --num_shards=$(nproc) \ **This will use all your cores to run make_examples. Feel free to change.**; --logging_dir=/output/logs \ **Optional. This saves the log output for each stage separately.; --haploid_contigs=""chrX,chrY"" \ **Optional. Heterozygous variants in these contigs will be re-genotyped as the most likely of reference or homozygous alternates. For a sample with karyotype XY, it should be set to ""chrX,chrY"" for GRCh38 and ""X,Y"" for GRCh37. For a sample with karyotype XX, this should not be used.; --par_regions_bed=""/input/GRCh3X_par.bed"" \ **Optional. If --haploid_contigs is set, then this can be used to provide PAR regions to be excluded from genotype adjustment. Download links to this files are available in this page.; --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.; ```. For details on X,Y support, please see; [DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case; study in; [DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You; can download the PAR bed files from here:; [GRCh38_par.bed](ht",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:3869,log,logs,3869,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['log'],['logs'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: .; * PacBio HiFi data, see the; [PacBio case study](docs/deeptrio-pacbio-case-study.md). Please also note:. * All DeepTrio models were trained on human data.; * It is possible to use DeepTrio with only 2 samples (child, and one parent).; * External tool [GLnexus](https://github.com/dnanexus-rnd/GLnexus) is used to; merge output VCFs. ## How to run DeepVariant. We recommend using our Docker solution. The command will look like this:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**; --ref=/input/YOUR_REF \; --reads=/input/YOUR_BAM \; --output_vcf=/output/YOUR_OUTPUT_VCF \; --output_gvcf=/output/YOUR_OUTPUT_GVCF \; --num_shards=$(nproc) \ **This will use all your cores to run make_examples. Feel free to change.**; --logging_dir=/output/logs \ **Optional. This saves the log output for each stage separately.; --haploid_contigs=""chrX,chrY"" \ **Optional. Heterozygous variants in these contigs will be re-genotyped as the most likely of reference or homozygous alternates. For a sample with karyotype XY, it should be set to ""chrX,chrY"" for GRCh38 and ""X,Y"" for GRCh37. For a sample with karyotype XX, this should not be used.; --par_regions_bed=""/input/GRCh3X_par.bed"" \ **Optional. If --haploid_contigs is set, then this can be used to provide PAR regions to be excluded from genotype adjustment. Download links to this files are available in this page.; --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.; ```. For details on X,Y support, please see; [DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case; study in; [DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You; can download the PAR bed files from here:; [GRCh38_par.bed](ht
",False,"The provided text contains a mix of natural language sentences and code snippets. While there are complete, meaningful sentences discussing concepts like PacBio HiFi data and how to use DeepVariant, the presence of command lines and configuration options (e.g., `--num_shards`, `--logging_dir`) suggests that it's primarily documentation with some technical instructions. However, these elements are part of a larger explanatory context rather than standalone code or logs. Therefore, the text should not be eliminated as it is informative and human-readable."
Testability,".gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. # Extract chr20 CDS regions and convert to bed file.; gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \; awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \; awk '!dup[$0]++' > data/chr20_CDS.bed; ```. ### Download HG005 BAM. We'll use HG005 poly-A selected Illumina RNA-seq reads that are publicly; available. ```bash; HTTPDIR=https://storage.googleapis.com/brain-genomics-public/research/sequencing/grch38/bam/rna/illumina/mrna. curl -L ${HTTPDIR}/hg0",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:2046,benchmark,benchmark,2046,docs/deepvariant-rnaseq-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: .gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. # Extract chr20 CDS regions and convert to bed file.; gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \; awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \; awk '!dup[$0]++' > data/chr20_CDS.bed; ```. ### Download HG005 BAM. We'll use HG005 poly-A selected Illumina RNA-seq reads that are publicly; available. ```bash; HTTPDIR=https://storage.googleapis.com/brain-genomics-public/research/sequencing/grch38/bam/rna/illumina/mrna. curl -L ${HTTPDIR}/hg0
",False,
Testability,".gz files output by DeepVariant during the; make_examples (first) stage. If you followed along with the [quick start guide](deepvariant-quick-start.md); and case studies that used the Docker version, then these files are usually; hidden inside the Docker container. But you can get them exported into the same; output directory where the VCF file appears by adding the following setting in; the `run_deepvariant` command. ```bash; # Add the following to your run_deepvariant command.; --intermediate_results_dir=/output/; ```. Then the make_examples file should appear in the directory docker mounted as; `/output/`. For example, if you followed the; [quick-start documentation](deepvariant-quick-start.md), it looks like this:; `${OUTPUT_DIR}/make_examples.tfrecord-00000-of-00001.gz`. ## Running show_examples. Once you have a make_examples output tfrecord file, then you can run; `show_examples` to see the pileup images inside:. ```bash; # Continuing from the quick start linked above:; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". BIN_VERSION=""1.6.1"" # show_examples is available only in version 1.0.0 and later.; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \; --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \; --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \; --output=/output/pileup \; --num_records=20 \; --curate. # And then your images are here:; ls ""${OUTPUT_DIR}""/pileup*.png; ```. ## Try it with these powerful optional parameters. * Filter to regions? Use e.g. `--regions chr20:1-3000000` or paths to BED or; BEDPE files.; * Filter to records from a VCF? Use `--vcf variants.vcf`. This can be a piece; of a VCF, e.g. grepping a hap.py output VCF for false positives. This is a; powerful way to pick out variants of interest and investigate th",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md:2083,test,testdata,2083,docs/show-examples.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: .gz files output by DeepVariant during the; make_examples (first) stage. If you followed along with the [quick start guide](deepvariant-quick-start.md); and case studies that used the Docker version, then these files are usually; hidden inside the Docker container. But you can get them exported into the same; output directory where the VCF file appears by adding the following setting in; the `run_deepvariant` command. ```bash; # Add the following to your run_deepvariant command.; --intermediate_results_dir=/output/; ```. Then the make_examples file should appear in the directory docker mounted as; `/output/`. For example, if you followed the; [quick-start documentation](deepvariant-quick-start.md), it looks like this:; `${OUTPUT_DIR}/make_examples.tfrecord-00000-of-00001.gz`. ## Running show_examples. Once you have a make_examples output tfrecord file, then you can run; `show_examples` to see the pileup images inside:. ```bash; # Continuing from the quick start linked above:; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". BIN_VERSION=""1.6.1"" # show_examples is available only in version 1.0.0 and later.; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \; --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \; --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \; --output=/output/pileup \; --num_records=20 \; --curate. # And then your images are here:; ls ""${OUTPUT_DIR}""/pileup*.png; ```. ## Try it with these powerful optional parameters. * Filter to regions? Use e.g. `--regions chr20:1-3000000` or paths to BED or; BEDPE files.; * Filter to records from a VCF? Use `--vcf variants.vcf`. This can be a piece; of a VCF, e.g. grepping a hap.py output VCF for false positives. This is a; powerful way to pick out variants of interest and investigate th
",True,"The text contains code snippets, such as command line arguments (`--regions`, `--vcf`), program logs or error messages, API documentation about optional parameters, and build system outputs. It includes bash code examples and commands which are indicative of programming instructions rather than meaningful human-readable content."
Testability,"/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. HTTPDIR=https://storage.googleapis.com/deepvariant/quickstart-testdata. wget -P ${INPUT_DIR} ""${HTTPDIR}""/HG002.chr20.10_10p1mb.bam; curl ""${HTTPDIR}/HG002.chr20.10_10p1mb.bam"" > ""${INPUT_DIR}/HG002.chr20.10_10p1mb.bam""; curl ""${HTTPDIR}/HG002.chr20.10_10p1mb.bam.bai"" > ""${INPUT_DIR}/HG002.chr20.10_10p1mb.bam.bai"". curl ""${HTTPDIR}/HG003.chr20.10_10p1mb.bam"" > ""${INPUT_DIR}/HG003.chr20.10_10p1mb.bam""; curl ""${HTTPDIR}/HG003.chr20.10_10p1mb.bam.bai"" > ""${INPUT_DIR}/HG003.chr20.10_10p1mb.bam.bai"". curl ""${HTTPDIR}/HG004.chr20.10_10p1mb.bam"" > ""${INPUT_DIR}/HG004.chr20.10_10p1mb.bam""; curl ""${HTTPDIR}/HG004.chr20.10_10p1mb.bam.bai"" > ""${INPUT_DIR}/HG004.chr20.10_10p1mb.bam.bai"". FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ""${INPUT_DIR}""/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ""${INPUT_DIR}""/GRCh38_no_alt_analysis_set.fasta.",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:3707,test,testdata,3707,docs/deeptrio-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: /HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. HTTPDIR=https://storage.googleapis.com/deepvariant/quickstart-testdata. wget -P ${INPUT_DIR} ""${HTTPDIR}""/HG002.chr20.10_10p1mb.bam; curl ""${HTTPDIR}/HG002.chr20.10_10p1mb.bam"" > ""${INPUT_DIR}/HG002.chr20.10_10p1mb.bam""; curl ""${HTTPDIR}/HG002.chr20.10_10p1mb.bam.bai"" > ""${INPUT_DIR}/HG002.chr20.10_10p1mb.bam.bai"". curl ""${HTTPDIR}/HG003.chr20.10_10p1mb.bam"" > ""${INPUT_DIR}/HG003.chr20.10_10p1mb.bam""; curl ""${HTTPDIR}/HG003.chr20.10_10p1mb.bam.bai"" > ""${INPUT_DIR}/HG003.chr20.10_10p1mb.bam.bai"". curl ""${HTTPDIR}/HG004.chr20.10_10p1mb.bam"" > ""${INPUT_DIR}/HG004.chr20.10_10p1mb.bam""; curl ""${HTTPDIR}/HG004.chr20.10_10p1mb.bam.bai"" > ""${INPUT_DIR}/HG004.chr20.10_10p1mb.bam.bai"". FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ""${INPUT_DIR}""/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ""${INPUT_DIR}""/GRCh38_no_alt_analysis_set.fasta.
",False,"The text contains a series of shell commands for downloading and processing data, which are code snippets rather than meaningful human-readable sentences. These commands involve file transfers using curl and wget, as well as data manipulation operations. The presence of syntax like variable substitution (e.g., ${INPUT_DIR}) and command-line arguments indicates that this is programmatic content meant for automated processing rather than human interaction. Therefore, it does not meet the criteria for meaningful human-readable sentences."
Testability,"/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11208 48 21239 13 9586 7 4 0.995736 0.998884 0.451340 0.997308 NaN NaN 1.561710 2.047281; INDEL PASS 11256 11208 48 21239 13 9586 7 4 0.995736 0.998884 0.451340 0.997308 NaN NaN 1.561710 2.047281; SNP ALL 71333 71087 246 88976 42 17795 5 4 0.996551 0.999410 0.199998 0.997979 2.3",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:10189,benchmark,benchmark,10189,docs/deeptrio-wgs-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md,2,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: /reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11208 48 21239 13 9586 7 4 0.995736 0.998884 0.451340 0.997308 NaN NaN 1.561710 2.047281; INDEL PASS 11256 11208 48 21239 13 9586 7 4 0.995736 0.998884 0.451340 0.997308 NaN NaN 1.561710 2.047281; SNP ALL 71333 71087 246 88976 42 17795 5 4 0.996551 0.999410 0.199998 0.997979 2.3
",False,"The content contains a benchmarking summary with various metrics, which includes complete sentences discussing the performance of analyses in detail."
Testability,"/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11215 41 23348 85 11580 30 50 0.996357 0.992777 0.495974 0.994564 NaN NaN 1.561710 2.133416; INDEL PASS 11256 11215 41 23348 85 11580 30 50 0.996357 0.992777 0.495974 0.994564 NaN NaN 1.561710 2.133416; SNP ALL 71333 71303 30 108157 20 36757 16 4 0.999579 0.999720 0.339849 0.999",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:10470,benchmark,benchmark,10470,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,2,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: /reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11215 41 23348 85 11580 30 50 0.996357 0.992777 0.495974 0.994564 NaN NaN 1.561710 2.133416; INDEL PASS 11256 11215 41 23348 85 11580 30 50 0.996357 0.992777 0.495974 0.994564 NaN NaN 1.561710 2.133416; SNP ALL 71333 71303 30 108157 20 36757 16 4 0.999579 0.999720 0.339849 0.999
",False,"The text includes a benchmark summary with detailed metrics, which is explanatory and human-readable."
Testability,"0/2 both; become Het (0/x). 1/1 and 3/3 are Hom (x/x). Het - both variants (x/y) includes; all calls with two different alternate alleles, such as 1/2 or 3/5. ### Biallelic base changes. Of all biallelic SNPs, this shows the counts from a particular REF (along the; top labeling the four charts) to a particular ALT (each bar within the charts; labeled at the bottom). See the Ti/Tv section for a brief explanation of why; some of these base changes tend to be more frequent than others. RefCalls and; multi-allelic variants are not included. ### Biallelic Ti/Tv ratio. Transition (Ti) count is the number of biallelic SNPs going from purine->purine; or pyrimidine->pyrimidine, where purines are A and G, pyrimidines are C and T.; Transversions (Tv) are purine->pyrimidine or pyrimidine->purine. Transitions; are biologically more likely to occur than transversions due to the molecular; structure of the bases, so a ratio well above one is desirable. This; [article](https://gatkforums.broadinstitute.org/gatk/discussion/6308/evaluating-the-quality-of-a-variant-callset); on the GATK forums has a good discussion of how to interpret the ratio. These; include all biallelic SNPs, excluding RefCalls. ### Biallelic indel size distribution. The sizes of all biallelic insertions and deletions are shown as histograms. The; top and bottom plots show the same data, just on a linear scale on top and on a; log scale on the bottom. RefCalls and multi-allelic variants are not included. ## Examples. ### WGS case study HG002 (DeepVariant v0.10.0). ![visual report of WGS HG002 case study v0.10.0](images/WGS_HG002.v0.10.0_visual_report.png). ### WES case study HG002 (DeepVariant v0.10.0). ![visual report of WES HG002 case study v0.10.0](images/WES_HG002.v0.10.0_visual_report.png). ### PacBio case study HG002 (DeepVariant v0.10.0). ![visual report of PacBio HG002 case study v0.10.0](images/PacBio_HG002.v0.10.0_visual_report.png). [VCF specification]: https://samtools.github.io/hts-specs/VCFv4.3.pdf; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vcf-stats-report.md:4745,log,log,4745,docs/deepvariant-vcf-stats-report.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vcf-stats-report.md,1,['log'],['log'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: 0/2 both; become Het (0/x). 1/1 and 3/3 are Hom (x/x). Het - both variants (x/y) includes; all calls with two different alternate alleles, such as 1/2 or 3/5. ### Biallelic base changes. Of all biallelic SNPs, this shows the counts from a particular REF (along the; top labeling the four charts) to a particular ALT (each bar within the charts; labeled at the bottom). See the Ti/Tv section for a brief explanation of why; some of these base changes tend to be more frequent than others. RefCalls and; multi-allelic variants are not included. ### Biallelic Ti/Tv ratio. Transition (Ti) count is the number of biallelic SNPs going from purine->purine; or pyrimidine->pyrimidine, where purines are A and G, pyrimidines are C and T.; Transversions (Tv) are purine->pyrimidine or pyrimidine->purine. Transitions; are biologically more likely to occur than transversions due to the molecular; structure of the bases, so a ratio well above one is desirable. This; [article](https://gatkforums.broadinstitute.org/gatk/discussion/6308/evaluating-the-quality-of-a-variant-callset); on the GATK forums has a good discussion of how to interpret the ratio. These; include all biallelic SNPs, excluding RefCalls. ### Biallelic indel size distribution. The sizes of all biallelic insertions and deletions are shown as histograms. The; top and bottom plots show the same data, just on a linear scale on top and on a; log scale on the bottom. RefCalls and multi-allelic variants are not included. ## Examples. ### WGS case study HG002 (DeepVariant v0.10.0). ![visual report of WGS HG002 case study v0.10.0](images/WGS_HG002.v0.10.0_visual_report.png). ### WES case study HG002 (DeepVariant v0.10.0). ![visual report of WES HG002 case study v0.10.0](images/WES_HG002.v0.10.0_visual_report.png). ### PacBio case study HG002 (DeepVariant v0.10.0). ![visual report of PacBio HG002 case study v0.10.0](images/PacBio_HG002.v0.10.0_visual_report.png). [VCF specification]: https://samtools.github.io/hts-specs/VCFv4.3.pdf; 
",False,
Testability,"00001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation, `hap.py` will intersect this BED with the GIAB; confident regions. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_novogene.grch38.bed; ```. ## Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_a",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md:1632,test,testdata,1632,docs/deepvariant-exome-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: 00001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation, `hap.py` will intersect this BED with the GIAB; confident regions. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_novogene.grch38.bed; ```. ## Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_a
",False,"The content contains human-readable sentences discussing the process of downloading datasets and setting up the environment for running an analysis. It includes commands but also explanations, which makes it not purely code or logs."
Testability,"001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. # Extract chr20 CDS regions and convert to bed file.; gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \; awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \; awk '!dup[$0]++' > data/chr20_CDS.bed; ```. ### Download HG005 BAM. We'll use HG005 poly-A selected Illumina RNA-seq reads that are publicly; available. ```bash; HTTPDIR=https://storage.googleapis.com/brain-genomics-public/research/sequencing/grch38/bam/rna/illumina/mrna. curl -L ${HTTPDIR}/hg005_gm26107.mrna.grch38.bam > data/hg005_gm26107.mrna.grch38.bam; curl -L ${HTTPDIR}/hg005_gm26107.mrna.grch38.bam.bai >",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:2165,benchmark,benchmark,2165,docs/deepvariant-rnaseq-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: 001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. # Extract chr20 CDS regions and convert to bed file.; gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \; awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \; awk '!dup[$0]++' > data/chr20_CDS.bed; ```. ### Download HG005 BAM. We'll use HG005 poly-A selected Illumina RNA-seq reads that are publicly; available. ```bash; HTTPDIR=https://storage.googleapis.com/brain-genomics-public/research/sequencing/grch38/bam/rna/illumina/mrna. curl -L ${HTTPDIR}/hg005_gm26107.mrna.grch38.bam > data/hg005_gm26107.mrna.grch38.bam; curl -L ${HTTPDIR}/hg005_gm26107.mrna.grch38.bam.bai >
",False,"The text contains complete, meaningful sentences in natural language such as 'We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle' and includes explanatory content discussing concepts like genome analysis and variant calling benchmarks."
Testability,"08/139703 (98.72%) M:137988/139909 (98.63%) F+M:134596/137968 (97.56%); Sample HG002 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 0/146013 (0.00%) records did not conform to expected call ploidy; 143704/146013 (98.42%) records were variant in at least 1 family member and checked for Mendelian constraints; 5066/143704 (3.53%) records had indeterminate consistency status due to incomplete calls; 3886/143704 (2.70%) records contained a violation of Mendelian constraints; ```. ### Perform analysis with hap.py against 4.2.1 truth set. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.ou",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:9456,benchmark,benchmark,9456,docs/deeptrio-wgs-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: 08/139703 (98.72%) M:137988/139909 (98.63%) F+M:134596/137968 (97.56%); Sample HG002 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 0/146013 (0.00%) records did not conform to expected call ploidy; 143704/146013 (98.42%) records were variant in at least 1 family member and checked for Mendelian constraints; 5066/143704 (3.53%) records had indeterminate consistency status due to incomplete calls; 3886/143704 (2.70%) records contained a violation of Mendelian constraints; ```. ### Perform analysis with hap.py against 4.2.1 truth set. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.ou
",False,"The text contains commands and script instructions for running a specific tool, which may be part of a larger process but doesn't form complete sentences in natural language."
Testability,"1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 PacBio HiFi WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10).; These reads have been aligned to the GRCh38_no_alt_analysis reference using; [pbmm2](https://github.com/PacificBiosciences/pbmm2). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/pacbio-case-study-testdata. curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam > input/HG002.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG002.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam > input/HG003.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG003.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam > input/HG004.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG004.pfda_challenge.grch38.phased.chr20.bam.bai; ```. ## Running DeepTrio with one command. DeepTrio pipeline consists of 4 steps: `make_examples`, `call_variants`,; `postprocess_variants` and `GLnexus merge`. It is possible to run the first; three steps with one command using the `run_deeptrio` script. GLnexus;",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:3199,test,testdata,3199,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: 1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 PacBio HiFi WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10).; These reads have been aligned to the GRCh38_no_alt_analysis reference using; [pbmm2](https://github.com/PacificBiosciences/pbmm2). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/pacbio-case-study-testdata. curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam > input/HG002.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG002.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam > input/HG003.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG003.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam > input/HG004.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG004.pfda_challenge.grch38.phased.chr20.bam.bai; ```. ## Running DeepTrio with one command. DeepTrio pipeline consists of 4 steps: `make_examples`, `call_variants`,; `postprocess_variants` and `GLnexus merge`. It is possible to run the first; three steps with one command using the `run_deeptrio` script. GLnexus;
",False,"The text contains sentences that are complete, meaningful, and in natural language. For example, 'Download HG002, HG003, and HG004 BAM files. We'll use...' is a clear, human-readable sentence explaining the purpose of the files. Additionally, there are descriptive sentences such as 'These reads have been aligned to the GRCh38_no_alt_analysis reference using pbmm2' which provides context about the process. The text also includes instructions for running software and explanations of steps in a pipeline, all of which are meaningful for human readers."
Testability,"2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bam -o HG004.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bai -o HG004.bai; ```. ### Command for downloading the reference file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:4074,test,testdata,4074,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: 2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bam -o HG004.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bai -o HG004.bai; ```. ### Command for downloading the reference file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.
",False,"The text contains multiple command lines that appear to be part of a shell script or command line interface. These commands are for downloading data files, such as BAM and VCF files from specific URLs. Each line starts with a semicolon, which is typical in scripting languages like bash. The content is not primarily human-readable prose but rather a sequence of commands meant for executing specific tasks. While there may be some descriptive text in comments (like 'Command for downloading the reference file.'), the majority of the text is command code. Therefore, it should be considered technical and potentially filterable."
Testability,"38#0#//g"" | samtools sort --threads 10 -m 2G -O BAM > ${BAM}; # Index the BAM.; samtools index -@$(nproc) ${BAM}; ```. The step with `time` above took:. ```; real 73m19.172s; user 178m59.088s; sys 24m36.986s; ```. File size:. ```; $ ls -lh reads.sorted.chrfixed.bam; -rw-rw-r-- 1 pichuan pichuan 40G Nov 2 02:09 reads.sorted.chrfixed.bam; ```. ## Run DeepVariant With `min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true`. Get the same reference we used for; [DeepVariant Case Study](deepvariant-case-study.md). ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; samtools faidx ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; ```. And then, run DeepVariant. (If you want to test on one smaller chromosome first, you can add; `--regions chr20` like what we did in; [DeepVariant Case Study](deepvariant-case-study.md).). ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". time sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; --reads=${PWD}/${BAM} \; --output_vcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; --output_gvcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.g.vcf.gz \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --num_shards=$(nproc); ```. Stage | Time (minutes); -------------------------------- | -----------------; make_examples | 116m37.385s; call_variants | 214m37.055s; postprocess_variants (with gVCF) | 30m59.968s. ### Run hap.py",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:5129,test,test,5129,docs/deepvariant-vg-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md,1,['test'],['test'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: 38#0#//g"" | samtools sort --threads 10 -m 2G -O BAM > ${BAM}; # Index the BAM.; samtools index -@$(nproc) ${BAM}; ```. The step with `time` above took:. ```; real 73m19.172s; user 178m59.088s; sys 24m36.986s; ```. File size:. ```; $ ls -lh reads.sorted.chrfixed.bam; -rw-rw-r-- 1 pichuan pichuan 40G Nov 2 02:09 reads.sorted.chrfixed.bam; ```. ## Run DeepVariant With `min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true`. Get the same reference we used for; [DeepVariant Case Study](deepvariant-case-study.md). ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; samtools faidx ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; ```. And then, run DeepVariant. (If you want to test on one smaller chromosome first, you can add; `--regions chr20` like what we did in; [DeepVariant Case Study](deepvariant-case-study.md).). ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". time sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; --reads=${PWD}/${BAM} \; --output_vcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; --output_gvcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.g.vcf.gz \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --num_shards=$(nproc); ```. Stage | Time (minutes); -------------------------------- | -----------------; make_examples | 116m37.385s; call_variants | 214m37.055s; postprocess_variants (with gVCF) | 30m59.968s. ### Run hap.py
",True,"The content consists primarily of code snippets and programmatic descriptions, including shell commands with parameters, file operations, and command outputs. The text contains configuration settings, API calls, and command lines which are typical indicators for elimination."
Testability,"59_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bam -o HG003.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bai -o HG003.bai; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bam -o HG004.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bai -o HG004.bai; ```. ### Command for downloading the reference file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:3742,test,testdata,3742,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: 59_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bam -o HG003.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bai -o HG003.bai; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bam -o HG004.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bai -o HG004.bai; ```. ### Command for downloading the reference file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh
",True,"The text contains command lines with syntax and parameters typical of a command-line tool like aria2c. The lines include commands with options like -c, -x10, -s10, which are common in command-line interfaces. Additionally, there are file names and paths, which might be part of a script or automated process but do not constitute meaningful human-readable sentences."
Testability,"6 (97.95%) M:166074/168579 (98.51%) F+M:159317/164363 (96.93%); Sample HG002 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 0/188247 (0.00%) records did not conform to expected call ploidy; 176481/188247 (93.75%) records were variant in at least 1 family member and checked for Mendelian constraints; 10169/176481 (5.76%) records had indeterminate consistency status due to incomplete calls; 6610/176481 (3.75%) records contained a violation of Mendelian constraints; ```. ### Benchmark variant calls against 4.2.1 truth set with hap.py. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.ou",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:9737,benchmark,benchmark,9737,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: 6 (97.95%) M:166074/168579 (98.51%) F+M:159317/164363 (96.93%); Sample HG002 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 0/188247 (0.00%) records did not conform to expected call ploidy; 176481/188247 (93.75%) records were variant in at least 1 family member and checked for Mendelian constraints; 10169/176481 (5.76%) records had indeterminate consistency status due to incomplete calls; 6610/176481 (3.75%) records contained a violation of Mendelian constraints; ```. ### Benchmark variant calls against 4.2.1 truth set with hap.py. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.ou
",False,
Testability,":""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG002.complete_g400.V350151728.grch38.chr20.bam \; --output_vcf /output/HG002.output.vcf.gz \; --output_gvcf /output/HG002.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-60-0.993753.ckpt; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11130 126 20925 31 9340 26 4 0.988806 0.997324 0.446356 0.993047 NaN NaN 1.561710 2.049106; INDEL PASS 11256 11130 126 20925 31 9340 26 4 0.988806 0.997324 0.446356 0.993047 NaN NaN 1.561710 2.049106; SNP ALL 71333 70949 384 85736 50 14689 28 6 0.994617 0.999296 0.171328 0.996951 2.314904 2.102286 1.715978 1.753768; SNP PASS 71333 70949 384 85736 50 14689 28 6 0.994617 0.999296 0.171328 0.996951 2.314904 2.102286 1.715978 1.753768; ```. To summarize:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRI",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md:3015,benchmark,benchmark,3015,docs/deepvariant-complete-g400-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: :""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG002.complete_g400.V350151728.grch38.chr20.bam \; --output_vcf /output/HG002.output.vcf.gz \; --output_gvcf /output/HG002.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-60-0.993753.ckpt; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11130 126 20925 31 9340 26 4 0.988806 0.997324 0.446356 0.993047 NaN NaN 1.561710 2.049106; INDEL PASS 11256 11130 126 20925 31 9340 26 4 0.988806 0.997324 0.446356 0.993047 NaN NaN 1.561710 2.049106; SNP ALL 71333 70949 384 85736 50 14689 28 6 0.994617 0.999296 0.171328 0.996951 2.314904 2.102286 1.715978 1.753768; SNP PASS 71333 70949 384 85736 50 14689 28 6 0.994617 0.999296 0.171328 0.996951 2.314904 2.102286 1.715978 1.753768; ```. To summarize:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRI
",False,
Testability,"; We will use Docker to run `hap.py`. ### Data. We will use these data in our analysis. Files will be downloaded in subsequent; steps. * HG005 RNA-seq BAM; * Model Checkpoint Files; * GRCh38 Reference + Index; * CDS bedfile (chr20 only); * GIAB benchmark data. ## Prepare Data. ### Setup directories. Lets first create directories to organize files. ```bash; mkdir -p data benchmark reference model output happy; ```. ### Download the GRCh38 Reference. We will be using GRCh38 for this case study. ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > dat",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:1534,benchmark,benchmarks,1534,docs/deepvariant-rnaseq-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md,1,['benchmark'],['benchmarks'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ; We will use Docker to run `hap.py`. ### Data. We will use these data in our analysis. Files will be downloaded in subsequent; steps. * HG005 RNA-seq BAM; * Model Checkpoint Files; * GRCh38 Reference + Index; * CDS bedfile (chr20 only); * GIAB benchmark data. ## Prepare Data. ### Setup directories. Lets first create directories to organize files. ```bash; mkdir -p data benchmark reference model output happy; ```. ### Download the GRCh38 Reference. We will be using GRCh38 for this case study. ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > dat
",False,
Testability,"<img src=""docs/images/dv_logo.png"" width=50% height=50%>. [![release](https://img.shields.io/badge/release-v1.6.1-green?logo=github)](https://github.com/google/deepvariant/releases); [![announcements](https://img.shields.io/badge/announcements-blue)](https://groups.google.com/d/forum/deepvariant-announcements); [![blog](https://img.shields.io/badge/blog-orange)](https://goo.gl/deepvariant). DeepVariant is a deep learning-based variant caller that takes aligned reads (in; BAM or CRAM format), produces pileup image tensors from them, classifies each; tensor using a convolutional neural network, and finally reports the results in; a standard VCF or gVCF file. DeepVariant supports germline variant-calling in diploid organisms. * NGS (Illumina or Element) data for either a; [whole genome](docs/deepvariant-case-study.md) or; [whole exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and; [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The mod",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:120,log,logo,120,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['log'],['logo'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: <img src=""docs/images/dv_logo.png"" width=50% height=50%>. [![release](https://img.shields.io/badge/release-v1.6.1-green?logo=github)](https://github.com/google/deepvariant/releases); [![announcements](https://img.shields.io/badge/announcements-blue)](https://groups.google.com/d/forum/deepvariant-announcements); [![blog](https://img.shields.io/badge/blog-orange)](https://goo.gl/deepvariant). DeepVariant is a deep learning-based variant caller that takes aligned reads (in; BAM or CRAM format), produces pileup image tensors from them, classifies each; tensor using a convolutional neural network, and finally reports the results in; a standard VCF or gVCF file. DeepVariant supports germline variant-calling in diploid organisms. * NGS (Illumina or Element) data for either a; [whole genome](docs/deepvariant-case-study.md) or; [whole exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and; [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The mod
",False,"The text contains multiple case studies and usage examples for different data types and inputs. It also includes explanatory paragraphs about supported input formats and limitations. There are no code snippets, logs, or API documentation; instead, it is providing user guidance and application scenarios."
Testability,"Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on all chromosomes. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -T /input/idt_capture_novogene.grch38.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 1051 1022 29 1476 13 418 8 3 0.972407 0.987713 0.283198 0.980000 NaN NaN 1.747283 1.859406; INDEL PASS 1051 1022 29 1476 13 418 8 3 0.972407 0.987713 0.283198 0.980000 NaN NaN 1.747283 1.859406; SNP ALL 25279 24987 292 27710 59 2662 34 2 0.988449 0.997645 0.096066 0.993025 2.854703 2.749729 1.623027 1.636078; SNP PASS 25279 24987 292 27710 59 2662 34 2 0.988449 0.997645 0.096066 0.993025 2.854703 2.749729 1.623027 1.636078; ```. [case study on whole genome sequencing data]: deepvariant-c",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md:4191,benchmark,benchmark,4191,docs/deepvariant-exome-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on all chromosomes. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -T /input/idt_capture_novogene.grch38.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 1051 1022 29 1476 13 418 8 3 0.972407 0.987713 0.283198 0.980000 NaN NaN 1.747283 1.859406; INDEL PASS 1051 1022 29 1476 13 418 8 3 0.972407 0.987713 0.283198 0.980000 NaN NaN 1.747283 1.859406; SNP ALL 25279 24987 292 27710 59 2662 34 2 0.988449 0.997645 0.096066 0.993025 2.854703 2.749729 1.623027 1.636078; SNP PASS 25279 24987 292 27710 59 2662 34 2 0.988449 0.997645 0.096066 0.993025 2.854703 2.749729 1.623027 1.636078; ```. [case study on whole genome sequencing data]: deepvariant-c
",True,"The text contains code snippets and program logs. It includes command lines, outputs from scripts, and performance metrics which are primarily for programmatic use rather than human-readable content."
Testability,"CF output as in the [WGS case; study]. However, additional flags must be passed to the `make_examples` and; `postprocess_variants` steps. ### `make_examples`. The `make_examples` program is where the gVCF records are computed. One additional flag is required in `make_examples`, the `--gvcf <filename>`; flag. This specifies an additional output, which is a TFRecord file of Variant; protocol buffers. If running with multiple processes, the sharding applied to; this output filename must be the same as that applied to the `--examples`; output. A concrete example call, using variables defined in the [WGS case study]:. ```bash; GVCF_TFRECORDS=""${OUTPUT_DIR}/HG002.gvcf.tfrecord@${N_SHARDS}.gz"". ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`; ```. NOTE: gVCF outputs are only valid when `make_examples` is run in ""calling"" mode;; if attempted to run in ""training"" mode the program will exit and notify the user; of the error. ### `postprocess_variants`. When run in gVCF mode, the `postprocess_variants` program handles the creation; of the final gVCF file that incorporates both the non-variant records and the; true variants discovered by the previous programs. Two additional flags are required in `postprocess_variants`, the input; `--nonvariant_site_tfrecord_path <filename>` which corresponds to the TFRecord; of Variant protocol buffers created in `make_examples`, and the output; `--gvcf_outfile <filename>` which is the final gVCF output. A concrete example call, using variables defined in the [WGS case study] and in; the above `make_examples` example:. ```bash; OUTPUT_GVCF=""${OUTPUT_DIR}/HG002.output.g.vcf.gz"". ( time python ""${BIN_DIR}""/postprocess_variants.zip \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --o",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:2793,log,log,2793,docs/deepvariant-gvcf-support.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md,1,['log'],['log'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: CF output as in the [WGS case; study]. However, additional flags must be passed to the `make_examples` and; `postprocess_variants` steps. ### `make_examples`. The `make_examples` program is where the gVCF records are computed. One additional flag is required in `make_examples`, the `--gvcf <filename>`; flag. This specifies an additional output, which is a TFRecord file of Variant; protocol buffers. If running with multiple processes, the sharding applied to; this output filename must be the same as that applied to the `--examples`; output. A concrete example call, using variables defined in the [WGS case study]:. ```bash; GVCF_TFRECORDS=""${OUTPUT_DIR}/HG002.gvcf.tfrecord@${N_SHARDS}.gz"". ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`; ```. NOTE: gVCF outputs are only valid when `make_examples` is run in ""calling"" mode;; if attempted to run in ""training"" mode the program will exit and notify the user; of the error. ### `postprocess_variants`. When run in gVCF mode, the `postprocess_variants` program handles the creation; of the final gVCF file that incorporates both the non-variant records and the; true variants discovered by the previous programs. Two additional flags are required in `postprocess_variants`, the input; `--nonvariant_site_tfrecord_path <filename>` which corresponds to the TFRecord; of Variant protocol buffers created in `make_examples`, and the output; `--gvcf_outfile <filename>` which is the final gVCF output. A concrete example call, using variables defined in the [WGS case study] and in; the above `make_examples` example:. ```bash; OUTPUT_GVCF=""${OUTPUT_DIR}/HG002.output.g.vcf.gz"". ( time python ""${BIN_DIR}""/postprocess_variants.zip \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --o
",False,"The provided text contains detailed instructions on how to run specific programs and set up flags for generating gVCF records. It includes concrete examples of bash commands, which are meaningful and explanatory in nature. The text is not primarily code snippets or logs but rather a guide explaining the process step-by-step."
Testability,"ES`, you'll be using a model that is best suited; for Illumina Whole Exome Sequencing data. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on all chromosomes. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -T /input/idt_capture_novogene.grch38.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 1051 1022 29 1476 13 418 8 3 0.972407 0.987713 0.283198 0.980000 NaN NaN 1.747283 1.859406; INDEL PASS 1051 1022 29 1476 13 418 8 3 0.972407 0.987713 0.283198 0.980000 NaN NaN 1.747283 1.859406; SNP ALL 25279 24987 292 27710 59 2662 34 2 0.988449 0.9976",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md:3894,benchmark,benchmark,3894,docs/deepvariant-exome-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md,2,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ES`, you'll be using a model that is best suited; for Illumina Whole Exome Sequencing data. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on all chromosomes. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -T /input/idt_capture_novogene.grch38.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 1051 1022 29 1476 13 418 8 3 0.972407 0.987713 0.283198 0.980000 NaN NaN 1.747283 1.859406; INDEL PASS 1051 1022 29 1476 13 418 8 3 0.972407 0.987713 0.283198 0.980000 NaN NaN 1.747283 1.859406; SNP ALL 25279 24987 292 27710 59 2662 34 2 0.988449 0.9976
",False,
Testability,"ING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir""; tensorboard --logdir ${TRAINING_DIR} --port=8080; ```. After it started, I clicked on the “Web Preview” on the top right of the mini; terminal:. ![WebPreview](images/WebPreview.png?raw=true ""Web Preview""). And clicked on ""Preview on port 8080"":. ![PreviewOnPort](images/PreviewOnPort.png?raw=true ""Preview on Port 8080""). Once it starts, you can see many metrics, including accuracy, speed, etc. You; will need to wait for `train` to run for a while before the plots will appear. ### Test the model. Now that we have performed training, we can test the performance of the new; model using our holdout dataset (chr20). The following one-step command can be used to call DeepVariant and run our newly; trained model:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ""${DOCKER_IMAGE}-gpu"" \; run_deepvariant \; --model_type WGS \; --customized_model ""${BEST_CHECKPOINT}"" \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:15983,log,logdir,15983,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['log'],['logdir'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir""; tensorboard --logdir ${TRAINING_DIR} --port=8080; ```. After it started, I clicked on the “Web Preview” on the top right of the mini; terminal:. ![WebPreview](images/WebPreview.png?raw=true ""Web Preview""). And clicked on ""Preview on port 8080"":. ![PreviewOnPort](images/PreviewOnPort.png?raw=true ""Preview on Port 8080""). Once it starts, you can see many metrics, including accuracy, speed, etc. You; will need to wait for `train` to run for a while before the plots will appear. ### Test the model. Now that we have performed training, we can test the performance of the new; model using our holdout dataset (chr20). The following one-step command can be used to call DeepVariant and run our newly; trained model:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ""${DOCKER_IMAGE}-gpu"" \; run_deepvariant \; --model_type WGS \; --customized_model ""${BEST_CHECKPOINT}"" \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf 
",False,The text contains human-readable sentences explaining the process of training and testing a model with commands provided for user guidance.
Testability,"Passes additional arguments to; make_examples.; * `split_skip_reads=true` - *Important!* This flag is critical for RNA-seq; variant calling to work properly. It enables RNA-seq data to be; processed efficiently.; * `channels=''` - Resets the channel list to be appropriate for the; RNA-seq model.; * `--intermediate_results_dir` - Outputs results to an intermediate directory. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; sudo docker run \; -v $(pwd):$(pwd) \; -w $(pwd) \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; output/HG005.output.vcf.gz \; -f benchmark/chr20_CDS_3x.benchmark_regions.bed \; -r reference/GRCh38_no_alt_analysis_set.fasta \; -o happy/happy.output \; --engine=vcfeval \; --pass-only \; --target-regions=data/chr20_CDS_3x.bed \; --threads=$(nproc); ```. **Flag summary**. * `-f` - Sets the benchmark regions (regions of interest that we want to; benchmark.); * `-r` - Sets the reference genome.; * `-o` - Specifies the output location.; * `--engine` - Sets the variant comparison engine. See; [hap.py documentation](https://github.com/Illumina/hap.py) for details.; * `--pass-only` - Restricts benchmarking to variants that have passed all; filters.; * `--target-regions` - Restricts analysis to given regions only.; * `--threads` - Level of parallelization to use. **Output:**. The above command should output the following results:. ```; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 9 6 3 11 1 4 1 0 0.666667 0.857143 0.363636 0.75000 NaN NaN 0.800000 1.200000; INDEL PASS 9 6 3 11 1 4 1 0 0.666667 0.857143 0.363636 0.75000 NaN NaN 0.800000 1.200000; SNP ALL 287 275 12 314 6 33 3 2 0.958188 0.978648",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:9324,benchmark,benchmark,9324,docs/deepvariant-rnaseq-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md,2,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Passes additional arguments to; make_examples.; * `split_skip_reads=true` - *Important!* This flag is critical for RNA-seq; variant calling to work properly. It enables RNA-seq data to be; processed efficiently.; * `channels=''` - Resets the channel list to be appropriate for the; RNA-seq model.; * `--intermediate_results_dir` - Outputs results to an intermediate directory. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; sudo docker run \; -v $(pwd):$(pwd) \; -w $(pwd) \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; output/HG005.output.vcf.gz \; -f benchmark/chr20_CDS_3x.benchmark_regions.bed \; -r reference/GRCh38_no_alt_analysis_set.fasta \; -o happy/happy.output \; --engine=vcfeval \; --pass-only \; --target-regions=data/chr20_CDS_3x.bed \; --threads=$(nproc); ```. **Flag summary**. * `-f` - Sets the benchmark regions (regions of interest that we want to; benchmark.); * `-r` - Sets the reference genome.; * `-o` - Specifies the output location.; * `--engine` - Sets the variant comparison engine. See; [hap.py documentation](https://github.com/Illumina/hap.py) for details.; * `--pass-only` - Restricts benchmarking to variants that have passed all; filters.; * `--target-regions` - Restricts analysis to given regions only.; * `--threads` - Level of parallelization to use. **Output:**. The above command should output the following results:. ```; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 9 6 3 11 1 4 1 0 0.666667 0.857143 0.363636 0.75000 NaN NaN 0.800000 1.200000; INDEL PASS 9 6 3 11 1 4 1 0 0.666667 0.857143 0.363636 0.75000 NaN NaN 0.800000 1.200000; SNP ALL 287 275 12 314 6 33 3 2 0.958188 0.978648
",False,
Testability,"RCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation, `hap.py` will intersect this BED with the GIAB; confident regions. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_novogene.grch38.bed; ```. ## Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions /input/idt_capture_novogene.grch38.bed \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir /output/intermediate_results_dir; ```. By specifying `--model_type WES`, you'll be using a model that is best suited; for Illumina Whole Exome Sequencing data. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md:2147,test,testdata,2147,docs/deepvariant-exome-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: RCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation, `hap.py` will intersect this BED with the GIAB; confident regions. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_novogene.grch38.bed; ```. ## Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions /input/idt_capture_novogene.grch38.bed \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir /output/intermediate_results_dir; ```. By specifying `--model_type WES`, you'll be using a model that is best suited; for Illumina Whole Exome Sequencing data. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to 
",False,"The text contains multiple curl commands and bash syntax which may be code snippets or script commands. However, it also includes explanatory sentences such as 'By specifying --model_type WES...' which are meaningful and in natural language."
Testability,"Specifies the output variant file.; * `--num_shards` - Sets the number of shards to the number of available; processors (`$(nproc)`). This is used to perform parallelization.; * `--regions` - Restricts analysis to 3x chr20 CDS regions only.; * `--make_examples_extra_args=` - Passes additional arguments to; make_examples.; * `split_skip_reads=true` - *Important!* This flag is critical for RNA-seq; variant calling to work properly. It enables RNA-seq data to be; processed efficiently.; * `channels=''` - Resets the channel list to be appropriate for the; RNA-seq model.; * `--intermediate_results_dir` - Outputs results to an intermediate directory. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; sudo docker run \; -v $(pwd):$(pwd) \; -w $(pwd) \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; output/HG005.output.vcf.gz \; -f benchmark/chr20_CDS_3x.benchmark_regions.bed \; -r reference/GRCh38_no_alt_analysis_set.fasta \; -o happy/happy.output \; --engine=vcfeval \; --pass-only \; --target-regions=data/chr20_CDS_3x.bed \; --threads=$(nproc); ```. **Flag summary**. * `-f` - Sets the benchmark regions (regions of interest that we want to; benchmark.); * `-r` - Sets the reference genome.; * `-o` - Specifies the output location.; * `--engine` - Sets the variant comparison engine. See; [hap.py documentation](https://github.com/Illumina/hap.py) for details.; * `--pass-only` - Restricts benchmarking to variants that have passed all; filters.; * `--target-regions` - Restricts analysis to given regions only.; * `--threads` - Level of parallelization to use. **Output:**. The above command should output the following results:. ```; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUT",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:9064,benchmark,benchmark,9064,docs/deepvariant-rnaseq-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Specifies the output variant file.; * `--num_shards` - Sets the number of shards to the number of available; processors (`$(nproc)`). This is used to perform parallelization.; * `--regions` - Restricts analysis to 3x chr20 CDS regions only.; * `--make_examples_extra_args=` - Passes additional arguments to; make_examples.; * `split_skip_reads=true` - *Important!* This flag is critical for RNA-seq; variant calling to work properly. It enables RNA-seq data to be; processed efficiently.; * `channels=''` - Resets the channel list to be appropriate for the; RNA-seq model.; * `--intermediate_results_dir` - Outputs results to an intermediate directory. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; sudo docker run \; -v $(pwd):$(pwd) \; -w $(pwd) \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; output/HG005.output.vcf.gz \; -f benchmark/chr20_CDS_3x.benchmark_regions.bed \; -r reference/GRCh38_no_alt_analysis_set.fasta \; -o happy/happy.output \; --engine=vcfeval \; --pass-only \; --target-regions=data/chr20_CDS_3x.bed \; --threads=$(nproc); ```. **Flag summary**. * `-f` - Sets the benchmark regions (regions of interest that we want to; benchmark.); * `-r` - Sets the reference genome.; * `-o` - Specifies the output location.; * `--engine` - Sets the variant comparison engine. See; [hap.py documentation](https://github.com/Illumina/hap.py) for details.; * `--pass-only` - Restricts benchmarking to variants that have passed all; filters.; * `--target-regions` - Restricts analysis to given regions only.; * `--threads` - Level of parallelization to use. **Output:**. The above command should output the following results:. ```; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUT
",False,
Testability,"Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. This should create a subdirectory in the current directory containing the actual; data files:. ```bash; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:2030,test,testdata,2030,docs/deepvariant-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. This should create a subdirectory in the current directory containing the actual; data files:. ```bash; 
",False,"The content contains step-by-step instructions and commands for compiling binaries, building Docker images, downloading test data, etc. However, it includes code snippets (like bash commands) and specific file formats (.fasta, .bam), which might be considered programmatic. Yet, the text also provides explanatory information about why each step is necessary, making it more than just a simple code snippet. Therefore, while it contains some technical details, it's still readable as human content discussing processes and requirements."
Testability,"T TC,<*> 50 . . GT:GQ 0/1:50; 1 4390 . C <*> . . END=4390 GT:GQ 0/0:3; ```. ## Creating gVCF output with DeepVariant. The exact same three programs (`make_examples`, `call_variants`, and; `postprocess_variants`) are used when creating gVCF output as in the [WGS case; study]. However, additional flags must be passed to the `make_examples` and; `postprocess_variants` steps. ### `make_examples`. The `make_examples` program is where the gVCF records are computed. One additional flag is required in `make_examples`, the `--gvcf <filename>`; flag. This specifies an additional output, which is a TFRecord file of Variant; protocol buffers. If running with multiple processes, the sharding applied to; this output filename must be the same as that applied to the `--examples`; output. A concrete example call, using variables defined in the [WGS case study]:. ```bash; GVCF_TFRECORDS=""${OUTPUT_DIR}/HG002.gvcf.tfrecord@${N_SHARDS}.gz"". ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`; ```. NOTE: gVCF outputs are only valid when `make_examples` is run in ""calling"" mode;; if attempted to run in ""training"" mode the program will exit and notify the user; of the error. ### `postprocess_variants`. When run in gVCF mode, the `postprocess_variants` program handles the creation; of the final gVCF file that incorporates both the non-variant records and the; true variants discovered by the previous programs. Two additional flags are required in `postprocess_variants`, the input; `--nonvariant_site_tfrecord_path <filename>` which corresponds to the TFRecord; of Variant protocol buffers created in `make_examples`, and the output; `--gvcf_outfile <filename>` which is the final gVCF output. A concrete example call, using variables defined in the",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:2571,log,log,2571,docs/deepvariant-gvcf-support.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md,1,['log'],['log'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: T TC,<*> 50 . . GT:GQ 0/1:50; 1 4390 . C <*> . . END=4390 GT:GQ 0/0:3; ```. ## Creating gVCF output with DeepVariant. The exact same three programs (`make_examples`, `call_variants`, and; `postprocess_variants`) are used when creating gVCF output as in the [WGS case; study]. However, additional flags must be passed to the `make_examples` and; `postprocess_variants` steps. ### `make_examples`. The `make_examples` program is where the gVCF records are computed. One additional flag is required in `make_examples`, the `--gvcf <filename>`; flag. This specifies an additional output, which is a TFRecord file of Variant; protocol buffers. If running with multiple processes, the sharding applied to; this output filename must be the same as that applied to the `--examples`; output. A concrete example call, using variables defined in the [WGS case study]:. ```bash; GVCF_TFRECORDS=""${OUTPUT_DIR}/HG002.gvcf.tfrecord@${N_SHARDS}.gz"". ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`; ```. NOTE: gVCF outputs are only valid when `make_examples` is run in ""calling"" mode;; if attempted to run in ""training"" mode the program will exit and notify the user; of the error. ### `postprocess_variants`. When run in gVCF mode, the `postprocess_variants` program handles the creation; of the final gVCF file that incorporates both the non-variant records and the; true variants discovered by the previous programs. Two additional flags are required in `postprocess_variants`, the input; `--nonvariant_site_tfrecord_path <filename>` which corresponds to the TFRecord; of Variant protocol buffers created in `make_examples`, and the output; `--gvcf_outfile <filename>` which is the final gVCF output. A concrete example call, using variables defined in the
",False,
Testability,This is for lightweight (smoke) tests that we generally want to run; before anything else.; ,MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/deepvariant/environment_tests/README.md:32,test,tests,32,deepvariant/environment_tests/README.md,,https://github.com/google/deepvariant/tree/v1.6.1/deepvariant/environment_tests/README.md,1,['test'],['tests'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: This is for lightweight (smoke) tests that we generally want to run; before anything else.; 
",False,The text contains complete sentences in natural language. It discusses the purpose of lightweight tests.
Testability,"VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:1995,test,test,1995,docs/deeptrio-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md,1,['test'],['test'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}
",False,
Testability,"You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. This should create a subdirectory in the current d",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:1978,test,test,1978,docs/deepvariant-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md,1,['test'],['test'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. This should create a subdirectory in the current d
",False,
Testability,"], which we update with each release of; DeepVariant, for the latest results. You can also see the [Colab example] to see how you can visualize the pileup; images. ## Training data over time. For the models we've released over time, you can find more details about the; training data in; [DeepVariant training data](deepvariant-details-training-data.md). ## CRAM support. As of v0.7, DeepVariant accepts CRAM files as input in addition to BAM files. As of v0.9.0, we changed the default to use the reference file specified by the; `--ref` flag, instead of the path to the original reference in the CRAM file; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 30x whole genome sample in BAM and CRAM format. Filetype | Size (Gb) | Runtime (min); -------- | --------- | -------------; BAM | 66.99 | 79m47.37307s; CRAM | 37.85 | 96m53.477s; Ratio | 56.50% | 121.43%. * BAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bas",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:9803,benchmark,benchmarking,9803,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['benchmark'],['benchmarking'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ], which we update with each release of; DeepVariant, for the latest results. You can also see the [Colab example] to see how you can visualize the pileup; images. ## Training data over time. For the models we've released over time, you can find more details about the; training data in; [DeepVariant training data](deepvariant-details-training-data.md). ## CRAM support. As of v0.7, DeepVariant accepts CRAM files as input in addition to BAM files. As of v0.9.0, we changed the default to use the reference file specified by the; `--ref` flag, instead of the path to the original reference in the CRAM file; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 30x whole genome sample in BAM and CRAM format. Filetype | Size (Gb) | Runtime (min); -------- | --------- | -------------; BAM | 66.99 | 79m47.37307s; CRAM | 37.85 | 96m53.477s; Ratio | 56.50% | 121.43%. * BAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bas
",False,"The text contains complete, meaningful sentences discussing the features of DeepVariant, including CRAM support updates, file formats accepted, performance data, and references to documentation. The content is explanatory and not primarily logs or code."
Testability,"_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:2482,benchmark,benchmark,2482,docs/deeptrio-wgs-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: _benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam
",False,"This text contains commands that are part of a script used to download files from a specific URL. While it includes command syntax, the sentences themselves are instructions meant for executing actions rather than providing explanation or discussion. However, the text is not purely code as it describes the process of downloading data for analysis purposes."
Testability,"_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 PacBio HiFi WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10).; These reads have been aligned to the GRCh38_no_alt_analysis reference using; [pbmm2](https://github.com/PacificBiosciences/pbmm2). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/pacbio-case-study-testdata. curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam > input/HG002.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG002.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam > input/HG003.pfda_challenge.grch38.phased.chr20.bam; cu",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:2551,benchmark,benchmark,2551,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: _benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 PacBio HiFi WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10).; These reads have been aligned to the GRCh38_no_alt_analysis reference using; [pbmm2](https://github.com/PacificBiosciences/pbmm2). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/pacbio-case-study-testdata. curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam > input/HG002.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG002.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam > input/HG003.pfda_challenge.grch38.phased.chr20.bam; cu
",False,
Testability,"_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. By specifying `--model_type ONT_R104`, you'll be using a model that is best; suited for Oxford Nanopore R10.4.1 chemistry Simplex and Duplex reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. ## Benchmark HG003 chr20 output from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. TRUTH_VCF=""HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz""; TRUTH_BED=""HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}""",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md:4026,benchmark,benchmark,4026,docs/deepvariant-ont-r104-simplex-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: _dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. By specifying `--model_type ONT_R104`, you'll be using a model that is best; suited for Oxford Nanopore R10.4.1 chemistry Simplex and Duplex reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. ## Benchmark HG003 chr20 output from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. TRUTH_VCF=""HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz""; TRUTH_BED=""HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}""
",False,"The text contains complete, meaningful sentences in natural language discussing concepts and ideas such as how to download benchmark data, use specific commands to run a Docker container, and explain the functionality of tools. The content is human-readable and not primarily code or logs."
Testability,"_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv; /output/logs/make_examples_runtime_by_region_report.html; ```. ### Running stages individually. If you are using make_examples itself without the run_deepvariant script, supply; make_examples with `--runtime_by_region=` a filename (.tsv). If the examples are; sharded then the runtime profile should be sharded into the same number of; shards, e.g. when using `examples@64` then runtimes could be `runtimes@64.tsv`. Then use the `runtime_by_region_vis` script to create a visual report of the; make_examples runtime by region data. Continuing from the quick start, it looks; like this:. ```bash; BIN_VERSION=""1.6.1"" # Only available in v1.1+.; docker run \; -v ""INPUT_DIR"":""/input"" \; -v ""OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/runtime_by_region_vis \; --input=/output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv \; --output=/output/logs/make_examples_runtime_by_region/make_examples_runtime_vis.html \; --title=""Quick start runtime profiling""; ```. ## Interpreting the runtime report. ### Where the data comes from. `make_examples`, the first stage of DeepVariant, starts from a BAM file and; corresponding reference FASTA file. One 1000 bp region at a time (set by; `--partition_size`), `make_examples` will:. 1. Get reads: Query the bam files for all the reads in the region. Optionally; (`--realign_reads`) do a local assembly of the reads and realign the reads; to the resulting haplotype graph. 2. Find candidates: Catalogue all the putative alternate alleles for all those; reads, and compare the accumulated evidence for each alt against thresholds; (see `--vsc_min*` parameters for the thresholds). 3. Make pileup images: Represent the reads as a; [pileup image tensor](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/).; When `--alt_aligned_pileup` is enabled, those alignments are included in; this ste",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md:2590,log,logs,2590,docs/runtime-by-region.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md,1,['log'],['logs'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: _examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv; /output/logs/make_examples_runtime_by_region_report.html; ```. ### Running stages individually. If you are using make_examples itself without the run_deepvariant script, supply; make_examples with `--runtime_by_region=` a filename (.tsv). If the examples are; sharded then the runtime profile should be sharded into the same number of; shards, e.g. when using `examples@64` then runtimes could be `runtimes@64.tsv`. Then use the `runtime_by_region_vis` script to create a visual report of the; make_examples runtime by region data. Continuing from the quick start, it looks; like this:. ```bash; BIN_VERSION=""1.6.1"" # Only available in v1.1+.; docker run \; -v ""INPUT_DIR"":""/input"" \; -v ""OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/runtime_by_region_vis \; --input=/output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv \; --output=/output/logs/make_examples_runtime_by_region/make_examples_runtime_vis.html \; --title=""Quick start runtime profiling""; ```. ## Interpreting the runtime report. ### Where the data comes from. `make_examples`, the first stage of DeepVariant, starts from a BAM file and; corresponding reference FASTA file. One 1000 bp region at a time (set by; `--partition_size`), `make_examples` will:. 1. Get reads: Query the bam files for all the reads in the region. Optionally; (`--realign_reads`) do a local assembly of the reads and realign the reads; to the resulting haplotype graph. 2. Find candidates: Catalogue all the putative alternate alleles for all those; reads, and compare the accumulated evidence for each alt against thresholds; (see `--vsc_min*` parameters for the thresholds). 3. Make pileup images: Represent the reads as a; [pileup image tensor](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/).; When `--alt_aligned_pileup` is enabled, those alignments are included in; this ste
",False,"The text includes commands and explanations that provide instructions on using specific tools or running processes, which may not be purely human-readable or descriptive without the context. However, it also contains explanatory content about how `make_examples` works, its stages, and interpretations of runtime reports, which are meaningful sentences."
Testability,"` script,; creating a visual report. ![Sample runtime profile from a WGS run](images/runtime_by_region_wgs.png). Example reports for typical runs:. * [WGS](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_wgs.html); * [WES](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_wes.html); * [PacBio](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_pacbio.html); * [Hybrid](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_hybrid.html). ## How to enable runtime profiling during a DeepVariant run. ### Using the run_deepvariant script. When using the one-step `run_deepvariant` script, supply a `--logging_dir`; directory and set `--runtime_report`. For example, when following the; [quick start](deepvariant-quick-start.md), including `--runtime_report; --logging_dir=/output/logs` would produce the following runtime profiling output; file and visual report:. ```; /output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv; /output/logs/make_examples_runtime_by_region_report.html; ```. ### Running stages individually. If you are using make_examples itself without the run_deepvariant script, supply; make_examples with `--runtime_by_region=` a filename (.tsv). If the examples are; sharded then the runtime profile should be sharded into the same number of; shards, e.g. when using `examples@64` then runtimes could be `runtimes@64.tsv`. Then use the `runtime_by_region_vis` script to create a visual report of the; make_examples runtime by region data. Continuing from the quick start, it looks; like this:. ```bash; BIN_VERSION=""1.6.1"" # Only available in v1.1+.; docker run \; -v ""INPUT_DIR"":""/input"" \; -v ""OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/runtime_by_region_vis \; --input=/output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv \; --output=/output/logs/make_examples_runtime_by_region/mak",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md:1600,log,logs,1600,docs/runtime-by-region.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md,1,['log'],['logs'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ` script,; creating a visual report. ![Sample runtime profile from a WGS run](images/runtime_by_region_wgs.png). Example reports for typical runs:. * [WGS](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_wgs.html); * [WES](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_wes.html); * [PacBio](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_pacbio.html); * [Hybrid](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_hybrid.html). ## How to enable runtime profiling during a DeepVariant run. ### Using the run_deepvariant script. When using the one-step `run_deepvariant` script, supply a `--logging_dir`; directory and set `--runtime_report`. For example, when following the; [quick start](deepvariant-quick-start.md), including `--runtime_report; --logging_dir=/output/logs` would produce the following runtime profiling output; file and visual report:. ```; /output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv; /output/logs/make_examples_runtime_by_region_report.html; ```. ### Running stages individually. If you are using make_examples itself without the run_deepvariant script, supply; make_examples with `--runtime_by_region=` a filename (.tsv). If the examples are; sharded then the runtime profile should be sharded into the same number of; shards, e.g. when using `examples@64` then runtimes could be `runtimes@64.tsv`. Then use the `runtime_by_region_vis` script to create a visual report of the; make_examples runtime by region data. Continuing from the quick start, it looks; like this:. ```bash; BIN_VERSION=""1.6.1"" # Only available in v1.1+.; docker run \; -v ""INPUT_DIR"":""/input"" \; -v ""OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/runtime_by_region_vis \; --input=/output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv \; --output=/output/logs/make_examples_runtime_by_region/mak
",False,"The text contains complete sentences in natural language that discuss how to enable runtime profiling during a DeepVariant run. It includes instructions on using specific scripts and commands, which are explanatory and helpful for humans."
Testability,"`--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 10628 10588 40 21099 19 10036 15 3 0.996236 0.998283 0.475662 0.997258 NaN NaN 1.748961 2.318182; INDEL PASS 10628 10588 40 21099 19 10036 15 3 0.996236 0.998283 0.475662 0.997258 NaN NaN 1.748961 2.318182; SNP ALL 70166 69917 249 84796 59 14782 13 3 0.996451 0.999157 0.174324 0.997802 2.296566 2.085786 1.883951 1.920577; SNP PASS 70166 69917 249 84796 59 14782 13 3 0.996451 0.999157 0.174324 0.997802 2.296566 2.085786 1.883951 1.920577; ```; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:4288,benchmark,benchmark,4288,docs/deepvariant-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 10628 10588 40 21099 19 10036 15 3 0.996236 0.998283 0.475662 0.997258 NaN NaN 1.748961 2.318182; INDEL PASS 10628 10588 40 21099 19 10036 15 3 0.996236 0.998283 0.475662 0.997258 NaN NaN 1.748961 2.318182; SNP ALL 70166 69917 249 84796 59 14782 13 3 0.996451 0.999157 0.174324 0.997802 2.296566 2.085786 1.883951 1.920577; SNP PASS 70166 69917 249 84796 59 14782 13 3 0.996451 0.999157 0.174324 0.997802 2.296566 2.085786 1.883951 1.920577; ```; 
",False,"The text contains complete, meaningful sentences in natural language explaining the process and configuration options, such as 'To run the benchmark on chr20...' and 'Based on... different model types...', which are not code snippets or logs. There is also human-readable content discussing how to use commands and flags, which fits into explanatory or descriptive categories."
Testability,"``bash; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.6.1""; mkdir -p deepvariant_output. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant_output/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20; ```. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. ## Benchmark output. ```bash; mkdir -p happy. singularity exec docker://jmcdani20/hap.py:v0.3.12 \; /opt/hap.py/bin/hap.py \; --threads $(nproc) \; -r reference/GRCh38_no_alt_analysis_set.fasta \; -f benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -o happy/giab-comparison.v4.2.first_pass \; --engine=vcfeval \; --pass-only \; -l chr20 \; benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; deepvariant_output/output.vcf.gz; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 10628 10551 77 22590 69 11527 39 29 0.992755 0.993763 0.510270 0.993259 NaN NaN 1.748961 2.275319; INDEL PASS 10628 10551 77 22590 69 11527 39 29 0.992755 0.993763 0.510270 0.993259 NaN NaN 1.748961 2.275319; SNP ALL 70166 70141 25 98780 23 28559 5 11 0.999644 0.999672 0.289117 0.999658 2.296566 1.823452 1.883951 1.913585; SNP PASS 70166 70141 25 98780 23 28559 5 11 0.999644 0.999672 0.289117 0.999658 2.296566 1.823452 1.883951 1.913585; ```; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:3880,benchmark,benchmark,3880,docs/deepvariant-pacbio-model-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md,2,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ``bash; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.6.1""; mkdir -p deepvariant_output. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant_output/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20; ```. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. ## Benchmark output. ```bash; mkdir -p happy. singularity exec docker://jmcdani20/hap.py:v0.3.12 \; /opt/hap.py/bin/hap.py \; --threads $(nproc) \; -r reference/GRCh38_no_alt_analysis_set.fasta \; -f benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -o happy/giab-comparison.v4.2.first_pass \; --engine=vcfeval \; --pass-only \; -l chr20 \; benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; deepvariant_output/output.vcf.gz; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 10628 10551 77 22590 69 11527 39 29 0.992755 0.993763 0.510270 0.993259 NaN NaN 1.748961 2.275319; INDEL PASS 10628 10551 77 22590 69 11527 39 29 0.992755 0.993763 0.510270 0.993259 NaN NaN 1.748961 2.275319; SNP ALL 70166 70141 25 98780 23 28559 5 11 0.999644 0.999672 0.289117 0.999658 2.296566 1.823452 1.883951 1.913585; SNP PASS 70166 70141 25 98780 23 28559 5 11 0.999644 0.999672 0.289117 0.999658 2.296566 1.823452 1.883951 1.913585; ```; 
",False,"The text contains complete, meaningful sentences discussing benchmarking results and metrics."
Testability,"acBio WGS trio. Then we assess the quality of the DeepTrio variant calls; with `hap.py`. In addition we evaluate a Mendelian violation rate for a merged; VCF. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh3",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:1170,benchmark,benchmarks,1170,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,1,['benchmark'],['benchmarks'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: acBio WGS trio. Then we assess the quality of the DeepTrio variant calls; with `hap.py`. In addition we evaluate a Mendelian violation rate for a merged; VCF. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh3
",True,"The content primarily consists of code snippets and shell commands used for downloading and processing data files. The text includes bash commands with syntax like `mkdir -p`, `curl`, and pipes for file operations. Additionally, there are FTP directory paths which are more related to system operations rather than human-readable explanations. There's also no meaningful natural language sentences discussing concepts or ideas; instead, it focuses on step-by-step commands and data processing instructions."
Testability,"achine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf /output/HG001.output.vcf.gz \; --output_gvcf /output/HG001.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-51-0.995354.ckpt; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG001.output.vcf.gz \; -f /benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 9974 9947 27 21052 9 10750 3 5 0.997293 0.999126 0.510640 0.998209 NaN NaN 1.630447 2.156149; INDEL PASS 9974 9947 27 21052 9 10750 3 5 0.997293 0.999126 0.510640 0.998209 NaN NaN 1.630447 2.156149; SNP ALL 69175 68874 301 85030 44 16068 8 2 0.995649 0.999362 0.188969 0.997502 2.288757 2.084645 1.730",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md:2672,benchmark,benchmark,2672,docs/deepvariant-complete-t7-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md,2,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: achine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf /output/HG001.output.vcf.gz \; --output_gvcf /output/HG001.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-51-0.995354.ckpt; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG001.output.vcf.gz \; -f /benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 9974 9947 27 21052 9 10750 3 5 0.997293 0.999126 0.510640 0.998209 NaN NaN 1.630447 2.156149; INDEL PASS 9974 9947 27 21052 9 10750 3 5 0.997293 0.999126 0.510640 0.998209 NaN NaN 1.630447 2.156149; SNP ALL 69175 68874 301 85030 44 16068 8 2 0.995649 0.999362 0.188969 0.997502 2.288757 2.084645 1.730
",True,"The text consists primarily of code snippets and logs, including commands, API descriptions, and benchmark results which are more appropriate for a programmatic context rather than human-readable content."
Testability,"ams used to transform aligned sequencing reads into; variant calls. At the highest level, a user needs to provide three inputs:. 1. A reference genome in [FASTA](https://en.wikipedia.org/wiki/FASTA_format); format and its corresponding; [.fai index file](http://www.htslib.org/doc/faidx.html) generated using the; `samtools faidx` command. 1. An aligned reads file in [BAM](http://genome.sph.umich.edu/wiki/BAM) format; and its corresponding index file (.bai). The reads must be aligned to the; reference genome described above. 1. A model checkpoint for DeepVariant. The output of DeepVariant is a list of all variant calls in; [VCF](https://samtools.github.io/hts-specs/VCFv4.3.pdf) format. DeepVariant is composed of three programs: `make_examples`, `call_variants`, and; `postprocess_variants`. More details about each program are described in detail; in the [Inputs and outputs](#inputs-and-outputs) section. ## Inputs and outputs. ### General notes. * Sharded files are a single logical collection of files with a common naming; convention. For example, we talk about `filename@10` as a single 10-way; sharded file named `filename`. On most filesystems this actually looks like; 10 distinct files `filename-00000-of-00010`, ..., `filename-00009-of-00010`.; DeepVariant can write sharded files using their `filename@10`-style name and; can read sharded files using both that style as well as the glob form, such; as `filename-*` or `filename-*-of-00010`.; * Files with the `.gz` suffix are interpreted as being compressed with gzip; and are read/written accordingly. ### make_examples. `make_examples` consumes reads and the reference genome to create TensorFlow; examples for evaluation with our deep learning models. The tf.Example protos are; written out in TFRecord format. To learn more about tf.Example and TFRecord, see; the; [Using TFRecords and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord); Colab. `make_examples` is a single-threaded program using 1-2 GB of RAM.",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:1055,log,logical,1055,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['log'],['logical'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ams used to transform aligned sequencing reads into; variant calls. At the highest level, a user needs to provide three inputs:. 1. A reference genome in [FASTA](https://en.wikipedia.org/wiki/FASTA_format); format and its corresponding; [.fai index file](http://www.htslib.org/doc/faidx.html) generated using the; `samtools faidx` command. 1. An aligned reads file in [BAM](http://genome.sph.umich.edu/wiki/BAM) format; and its corresponding index file (.bai). The reads must be aligned to the; reference genome described above. 1. A model checkpoint for DeepVariant. The output of DeepVariant is a list of all variant calls in; [VCF](https://samtools.github.io/hts-specs/VCFv4.3.pdf) format. DeepVariant is composed of three programs: `make_examples`, `call_variants`, and; `postprocess_variants`. More details about each program are described in detail; in the [Inputs and outputs](#inputs-and-outputs) section. ## Inputs and outputs. ### General notes. * Sharded files are a single logical collection of files with a common naming; convention. For example, we talk about `filename@10` as a single 10-way; sharded file named `filename`. On most filesystems this actually looks like; 10 distinct files `filename-00000-of-00010`, ..., `filename-00009-of-00010`.; DeepVariant can write sharded files using their `filename@10`-style name and; can read sharded files using both that style as well as the glob form, such; as `filename-*` or `filename-*-of-00010`.; * Files with the `.gz` suffix are interpreted as being compressed with gzip; and are read/written accordingly. ### make_examples. `make_examples` consumes reads and the reference genome to create TensorFlow; examples for evaluation with our deep learning models. The tf.Example protos are; written out in TFRecord format. To learn more about tf.Example and TFRecord, see; the; [Using TFRecords and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord); Colab. `make_examples` is a single-threaded program using 1-2 GB of RAM.
",False,"The text contains meaningful sentences discussing the use of tools like SAMtools and DeepVariant for processing genomics data, including how to prepare inputs and the structure of the programs involved."
Testability,"ase study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai; ```. ## Run DeepVariant on chromosome 20 alignments. ```bash; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.6.1""; mkdir -p deepvariant_output. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:2265,benchmark,benchmark,2265,docs/deepvariant-pacbio-model-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ase study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai; ```. ## Run DeepVariant on chromosome 20 alignments. ```bash; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.6.1""; mkdir -p deepvariant_output. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_
",True,"The text contains multiple code snippets, specifically bash commands and script instructions. These are indicative of programmatic operations and should be filtered out as they do not constitute meaningful human-readable sentences."
Testability,"aseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; following command:. ```bash; gcloud compute ssh ${host} --zone ${zone}; ```. Once you have logged in, set the variables:. ```bash; YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT; OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant""; VERSION=""1.6.1""; DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard""; GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study""; DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input""; BIN_DIR=""${INPUT_DIR}/bin""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_DIR=""${BASE}/output""; LOG_DIR=""${OUTPUT_DIR}/logs""; SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa""; BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam""; BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam""; BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam""; TRUTH_VCF=""",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:2085,log,logged,2085,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['log'],['logged'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: aseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; following command:. ```bash; gcloud compute ssh ${host} --zone ${zone}; ```. Once you have logged in, set the variables:. ```bash; YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT; OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant""; VERSION=""1.6.1""; DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard""; GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study""; DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input""; BIN_DIR=""${INPUT_DIR}/bin""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_DIR=""${BASE}/output""; LOG_DIR=""${OUTPUT_DIR}/logs""; SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa""; BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam""; BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam""; BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam""; TRUTH_VCF=""
",True,"The text contains code snippets and commands, including bash scripts with syntax highlighting, which are typical of programmatic documentation."
Testability,"at we did in; [DeepVariant Case Study](deepvariant-case-study.md).). ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". time sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; --reads=${PWD}/${BAM} \; --output_vcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; --output_gvcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.g.vcf.gz \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --num_shards=$(nproc); ```. Stage | Time (minutes); -------------------------------- | -----------------; make_examples | 116m37.385s; call_variants | 214m37.055s; postprocess_variants (with gVCF) | 30m59.968s. ### Run hap.py. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; ${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; -f ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -o $",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:6210,benchmark,benchmark,6210,docs/deepvariant-vg-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: at we did in; [DeepVariant Case Study](deepvariant-case-study.md).). ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". time sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; --reads=${PWD}/${BAM} \; --output_vcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; --output_gvcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.g.vcf.gz \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --num_shards=$(nproc); ```. Stage | Time (minutes); -------------------------------- | -----------------; make_examples | 116m37.385s; call_variants | 214m37.055s; postprocess_variants (with gVCF) | 30m59.968s. ### Run hap.py. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; ${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; -f ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -o $
",False,"The content provided contains both bash code snippets and explanations of the code, such as creating directories and running docker commands. The text includes meaningful sentences discussing the process of using hap.py for variant calling, along with command lines which are relevant to the context."
Testability,"ately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 10628 10588 40 21099 19 10036 15 3 0.996236 0.998283 0.475662 0.997258 NaN NaN 1.748961 2.318182; INDEL PASS 10628 10588 40 21099 19 10036 15 3 0.996236 0.998283 0.475662 0.997258 NaN NaN 1.748961 2.318182; SNP ALL 70166 69917 249 84796 59 14782 13 3 0.996451 0.999157 0.174324 0.997802 2.296566 2.085786 1.883951 1.920577; SNP PASS 70166 69917 249 84796 59 14782 13 3 0.996451 0.999157 0.174324 0.997802 2.296566 2.085786 1.883951 1.92",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:4198,benchmark,benchmark,4198,docs/deepvariant-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 10628 10588 40 21099 19 10036 15 3 0.996236 0.998283 0.475662 0.997258 NaN NaN 1.748961 2.318182; INDEL PASS 10628 10588 40 21099 19 10036 15 3 0.996236 0.998283 0.475662 0.997258 NaN NaN 1.748961 2.318182; SNP ALL 70166 69917 249 84796 59 14782 13 3 0.996451 0.999157 0.174324 0.997802 2.296566 2.085786 1.883951 1.920577; SNP PASS 70166 69917 249 84796 59 14782 13 3 0.996451 0.999157 0.174324 0.997802 2.296566 2.085786 1.883951 1.92
",False,The content contains meaningful sentences discussing benchmark results and explanations of metrics.
Testability,"benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG002.novase",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:2152,benchmark,benchmark,2152,docs/deeptrio-wgs-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG002.novase
",False,"The content contains multiple `curl` commands used to download files and create directories. However, the text also includes sentences like 'We'll use HG002, HG003, and HG004 Illumina WGS reads publicly available from the PrecisionFDA Truth v2 Challenge' which is meaningful human-readable content. Additionally, there are instructions like '[PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10)' which adds context to the downloaded files. Therefore, while the text primarily consists of commands and file downloads, it also contains explanatory and descriptive information that makes it meaningful for humans."
Testability,"benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 PacBio HiFi WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10).; These reads have been aligned to the GRCh38_no_alt_analysis reference using; [pbmm2](https://github.com/PacificBiosciences/pbmm2). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/pacbio-case-study-testdata. curl ${HTTPDIR}/HG002.pfd",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:2221,benchmark,benchmark,2221,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 PacBio HiFi WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10).; These reads have been aligned to the GRCh38_no_alt_analysis reference using; [pbmm2](https://github.com/PacificBiosciences/pbmm2). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/pacbio-case-study-testdata. curl ${HTTPDIR}/HG002.pfd
",False,"The text contains command lines and file operations, but it also includes explanatory sentences describing the process of downloading and using data. It has a mix of code snippets and descriptive content. The majority of the sentences are not purely code or logs; they explain what is happening in the workflow."
Testability,by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai; ```. ## R,MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:1831,benchmark,benchmark,1831,docs/deepvariant-pacbio-model-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai; ```. ## R
",False,
Testability,"c URLs. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepTrio in one command. Although DeepTrio can be built from a source, we provide a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:1481,test,test,1481,docs/deeptrio-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md,1,['test'],['test'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: c URLs. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepTrio in one command. Although DeepTrio can be built from a source, we provide a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf
",True,"The text contains multiple code snippets, such as the bash commands and Docker image instructions. It also has API documentation and setup instructions which are more programmatic in nature."
Testability,"cbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bai -o HG003.bai; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bam -o HG004.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bai -o HG004.bai; ```. ### Command for downloading the reference file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:3885,test,testdata,3885,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: cbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bai -o HG003.bai; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bam -o HG004.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bai -o HG004.bai; ```. ### Command for downloading the reference file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.
",True,"The text consists of command lines and file download instructions, which are typically associated with programming or automated processes. These commands include parameters like '-o' for output files, 'aria2c' as a command-line tool, directory references in '${DIR}', file URLs from specific servers, and file extensions such as .bai, .bam, etc. These elements indicate that the text is more suited for a script or program rather than human-readable content. The sentences are commands or instructions meant to be executed by a system, not for general reading comprehension."
Testability,"config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14698,test,tested,14698,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['test'],['tested'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud
",False,
Testability,contextlib2; intervaltree; absl-py; mock; numpy; six; protobuf==3.19.4; Pillow==9.5.0; ipython; apache-beam; ,MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/third_party/nucleus/pip_package/egg_files/requires.txt:36,mock,mock,36,third_party/nucleus/pip_package/egg_files/requires.txt,,https://github.com/google/deepvariant/tree/v1.6.1/third_party/nucleus/pip_package/egg_files/requires.txt,1,['mock'],['mock'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: contextlib2; intervaltree; absl-py; mock; numpy; six; protobuf==3.19.4; Pillow==9.5.0; ipython; apache-beam; 
",False,"The text contains a list of packages and their versions, which is typically used in dependency management or setup configurations. This type of information is often found in version control systems, build systems, or configuration files."
Testability,"contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot pipe them into DeepTrio. Duplicate marking may be; performed. In our analyses, there is almost no difference in accuracy with and; without duplicate marking except at lower (<20x) coverages. Finally, we; recommend that you do not perform BQSR. Running BQSR has a small decrease on; accuracy. If you are providing `--regions` or other similar arguments, these should refer; to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. ## Training data. DeepTrio models are trained using the latest publicly avavilable GIAB; benchmarks. You can find more details about the training data for each DeepTrio; model in the; [DeepTrio Training Data document](deeptrio-details-training-data.md). ## DeepVariant dependency. DeepTrio is built on top of DeepVariant and they share most of the components.; Please see [DeepVariant usage guide](deepvariant-details.md) for a full; description of DeepVariant components as well as other consideration for running; DeepVariant pipeline.; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:4583,benchmark,benchmarks,4583,docs/deeptrio-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md,1,['benchmark'],['benchmarks'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot pipe them into DeepTrio. Duplicate marking may be; performed. In our analyses, there is almost no difference in accuracy with and; without duplicate marking except at lower (<20x) coverages. Finally, we; recommend that you do not perform BQSR. Running BQSR has a small decrease on; accuracy. If you are providing `--regions` or other similar arguments, these should refer; to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. ## Training data. DeepTrio models are trained using the latest publicly avavilable GIAB; benchmarks. You can find more details about the training data for each DeepTrio; model in the; [DeepTrio Training Data document](deeptrio-details-training-data.md). ## DeepVariant dependency. DeepTrio is built on top of DeepVariant and they share most of the components.; Please see [DeepVariant usage guide](deepvariant-details.md) for a full; description of DeepVariant components as well as other consideration for running; DeepVariant pipeline.; 
",False,
Testability,"ct ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; following command:. ```bash; gcloud compute ssh ${host} --zone ${zone}; ```. Once you have logged in, set the variables:. ```bash; YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT; OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant""; VERSION=""1.6.1""; DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard""; GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study""; DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input""; BIN_DIR=""${INPUT_DIR}/bin""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_DIR=""${BASE}/output""; LOG_DIR=""${OUTPUT_DIR}/logs""; SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa""; BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam""; BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam""; BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam""; TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz""; TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16; ```. ## Download binaries and data. ### Create directories:. ```bash; mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${BIN_DIR}""; mkdir -p ""${DATA_DIR}""; mkdir -p ""${LOG_DIR}""; ```. ### Copy data. ```bash; gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:2808,log,logs,2808,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['log'],['logs'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ct ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; following command:. ```bash; gcloud compute ssh ${host} --zone ${zone}; ```. Once you have logged in, set the variables:. ```bash; YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT; OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant""; VERSION=""1.6.1""; DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard""; GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study""; DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input""; BIN_DIR=""${INPUT_DIR}/bin""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_DIR=""${BASE}/output""; LOG_DIR=""${OUTPUT_DIR}/logs""; SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa""; BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam""; BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam""; BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam""; TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz""; TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16; ```. ## Download binaries and data. ### Create directories:. ```bash; mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${BIN_DIR}""; mkdir -p ""${DATA_DIR}""; mkdir -p ""${LOG_DIR}""; ```. ### Copy data. ```bash; gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_
",True,"The text consists primarily of code snippets, variable declarations, and file paths. It includes commands in bash syntax which are indicative of programmatic instructions rather than meaningful human-readable sentences."
Testability,dd channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai; ```. ## Run DeepVariant on chromosome 20 alignments. ```bash; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BI,MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:2025,benchmark,benchmark,2025,docs/deepvariant-pacbio-model-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: dd channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai; ```. ## Run DeepVariant on chromosome 20 alignments. ```bash; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BI
",False,"The text contains a mix of code snippets and instructions, but also includes explanatory sentences. For instance, lines like 'mkdir -p reference.' are code-like, while the preceding sentence 'We will be using GRCh38 for this case study.' is human-readable prose. Therefore, it should not be entirely eliminated."
Testability,"dy, we describe applying [DeepTrio](deeptrio-details.md) to a; real PacBio WGS trio. Then we assess the quality of the DeepTrio variant calls; with `hap.py`. In addition we evaluate a Mendelian violation rate for a merged; VCF. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:1086,benchmark,benchmark,1086,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: dy, we describe applying [DeepTrio](deeptrio-details.md) to a; real PacBio WGS trio. Then we assess the quality of the DeepTrio variant calls; with `hap.py`. In addition we evaluate a Mendelian violation rate for a merged; VCF. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_
",False,
Testability,"dy](docs/deeptrio-pacbio-case-study.md). Please also note:. * All DeepTrio models were trained on human data.; * It is possible to use DeepTrio with only 2 samples (child, and one parent).; * External tool [GLnexus](https://github.com/dnanexus-rnd/GLnexus) is used to; merge output VCFs. ## How to run DeepVariant. We recommend using our Docker solution. The command will look like this:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**; --ref=/input/YOUR_REF \; --reads=/input/YOUR_BAM \; --output_vcf=/output/YOUR_OUTPUT_VCF \; --output_gvcf=/output/YOUR_OUTPUT_GVCF \; --num_shards=$(nproc) \ **This will use all your cores to run make_examples. Feel free to change.**; --logging_dir=/output/logs \ **Optional. This saves the log output for each stage separately.; --haploid_contigs=""chrX,chrY"" \ **Optional. Heterozygous variants in these contigs will be re-genotyped as the most likely of reference or homozygous alternates. For a sample with karyotype XY, it should be set to ""chrX,chrY"" for GRCh38 and ""X,Y"" for GRCh37. For a sample with karyotype XX, this should not be used.; --par_regions_bed=""/input/GRCh3X_par.bed"" \ **Optional. If --haploid_contigs is set, then this can be used to provide PAR regions to be excluded from genotype adjustment. Download links to this files are available in this page.; --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.; ```. For details on X,Y support, please see; [DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case; study in; [DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You; can download the PAR bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-stu",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:3903,log,log,3903,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['log'],['log'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: dy](docs/deeptrio-pacbio-case-study.md). Please also note:. * All DeepTrio models were trained on human data.; * It is possible to use DeepTrio with only 2 samples (child, and one parent).; * External tool [GLnexus](https://github.com/dnanexus-rnd/GLnexus) is used to; merge output VCFs. ## How to run DeepVariant. We recommend using our Docker solution. The command will look like this:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**; --ref=/input/YOUR_REF \; --reads=/input/YOUR_BAM \; --output_vcf=/output/YOUR_OUTPUT_VCF \; --output_gvcf=/output/YOUR_OUTPUT_GVCF \; --num_shards=$(nproc) \ **This will use all your cores to run make_examples. Feel free to change.**; --logging_dir=/output/logs \ **Optional. This saves the log output for each stage separately.; --haploid_contigs=""chrX,chrY"" \ **Optional. Heterozygous variants in these contigs will be re-genotyped as the most likely of reference or homozygous alternates. For a sample with karyotype XY, it should be set to ""chrX,chrY"" for GRCh38 and ""X,Y"" for GRCh37. For a sample with karyotype XX, this should not be used.; --par_regions_bed=""/input/GRCh3X_par.bed"" \ **Optional. If --haploid_contigs is set, then this can be used to provide PAR regions to be excluded from genotype adjustment. Download links to this files are available in this page.; --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.; ```. For details on X,Y support, please see; [DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case; study in; [DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You; can download the PAR bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-stu
",False,
Testability,"e a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:1868,test,test,1868,docs/deeptrio-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md,1,['test'],['test'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: e a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark
",False,"The text contains multiple bash code blocks for Docker commands and file download instructions, but also includes explanatory sentences that describe the process of obtaining a Docker image and test data. The code snippets are interspersed with natural language explanations, making the overall content meaningful to human readers without being purely code-based. Therefore, it should not be eliminated."
Testability,"e applying DeepVariant to a real WGS sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.n",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:1085,benchmark,benchmark,1085,docs/deepvariant-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: e applying DeepVariant to a real WGS sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.n
",False,
Testability,"e deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai; ```. ## Run DeepVariant on chromosome 20 alignments. ```bash; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.6.1""; mkdir -p deepvariant_output. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BI",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:2149,benchmark,benchmark,2149,docs/deepvariant-pacbio-model-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: e deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai; ```. ## Run DeepVariant on chromosome 20 alignments. ```bash; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.6.1""; mkdir -p deepvariant_output. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BI
",False,"The content consists of multiple shell scripts and commands for downloading and preparing reference data, which are meaningful human-readable sentences explaining the steps in a data processing pipeline."
Testability,"e reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRC",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:2047,test,testdata,2047,docs/deeptrio-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: e reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRC
",False,
Testability,e study](deepvariant-case-study.md); * [DeepVariant exome case study](deepvariant-exome-case-study.md); * [DeepVariant PacBio case study](deepvariant-pacbio-model-case-study.md); * [DeepVariant ONT R10.4 simplex case study](deepvariant-ont-r104-simplex-case-study.md); [DeepVariant ONT R10.4 duplex case study](deepvariant-ont-r104-duplex-case-study.md); * [DeepVariant hybrid (PacBio and Illumina) case study](deepvariant-hybrid-case-study.md); * [DeepVariant Complete Genomics T7 case study](deepvariant-complete-t7-case-study.md); * [DeepVariant Complete Genomics G400 case study](deepvariant-complete-g400-case-study.md); * [Runtime and accuracy metrics for all DeepVariant models](metrics.md); * [Best practices for multi-sample variant calling](trio-merge-case-study.md); * [Using graph genomes: VG Giraffe + DeepVariant case study](deepvariant-vg-case-study.md). ## Visualization and analysis. * [show_examples: Saving human-readable images from DeepVariant examples](show-examples.md); * [VCF stats report](deepvariant-vcf-stats-report.md); * [Runtime by region for make_examples](runtime-by-region.md). ### Colab notebooks. * [Colab example: visualizing pileup images/tensors](visualizing_examples.ipynb); * [Can you beat DeepVariant?: A look inside the classification task](cybdv_notebook.ipynb); * [Google Developer Codelab: Variant Calling on a Rice genome with DeepVariant](https://codelabs.developers.google.com/codelabs/genomics-deepvariant). ## (Advanced) Training. * [Advanced Case Study: Train a customized SNP and small indel variant caller; for BGISEQ-500 data](deepvariant-training-case-study.md); * [DeepVariant training data](deepvariant-details-training-data.md). ## More details. * [DeepVariant usage guide](deepvariant-details.md); * [Building and testing DeepVariant](deepvariant-build-test.md); * [DeepVariant Genomic VCF (gVCF) support](deepvariant-gvcf-support.md); * [Getting Started with GCP](deepvariant-gcp-info.md) (It is not required to; run DeepVariant on GCP.); ,MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/README.md:1991,test,testing,1991,docs/README.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/README.md,2,['test'],"['test', 'testing']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: e study](deepvariant-case-study.md); * [DeepVariant exome case study](deepvariant-exome-case-study.md); * [DeepVariant PacBio case study](deepvariant-pacbio-model-case-study.md); * [DeepVariant ONT R10.4 simplex case study](deepvariant-ont-r104-simplex-case-study.md); [DeepVariant ONT R10.4 duplex case study](deepvariant-ont-r104-duplex-case-study.md); * [DeepVariant hybrid (PacBio and Illumina) case study](deepvariant-hybrid-case-study.md); * [DeepVariant Complete Genomics T7 case study](deepvariant-complete-t7-case-study.md); * [DeepVariant Complete Genomics G400 case study](deepvariant-complete-g400-case-study.md); * [Runtime and accuracy metrics for all DeepVariant models](metrics.md); * [Best practices for multi-sample variant calling](trio-merge-case-study.md); * [Using graph genomes: VG Giraffe + DeepVariant case study](deepvariant-vg-case-study.md). ## Visualization and analysis. * [show_examples: Saving human-readable images from DeepVariant examples](show-examples.md); * [VCF stats report](deepvariant-vcf-stats-report.md); * [Runtime by region for make_examples](runtime-by-region.md). ### Colab notebooks. * [Colab example: visualizing pileup images/tensors](visualizing_examples.ipynb); * [Can you beat DeepVariant?: A look inside the classification task](cybdv_notebook.ipynb); * [Google Developer Codelab: Variant Calling on a Rice genome with DeepVariant](https://codelabs.developers.google.com/codelabs/genomics-deepvariant). ## (Advanced) Training. * [Advanced Case Study: Train a customized SNP and small indel variant caller; for BGISEQ-500 data](deepvariant-training-case-study.md); * [DeepVariant training data](deepvariant-details-training-data.md). ## More details. * [DeepVariant usage guide](deepvariant-details.md); * [Building and testing DeepVariant](deepvariant-build-test.md); * [DeepVariant Genomic VCF (gVCF) support](deepvariant-gvcf-support.md); * [Getting Started with GCP](deepvariant-gcp-info.md) (It is not required to; run DeepVariant on GCP.); 
",False,"The text contains several markdown files listed, which are case studies and documentation for DeepVariant. These include topics like exome analysis, PacBio integration, ONT R10.4 configurations, hybrid approaches, specific model runtimes, best practices for variant calling, graph genomes, visualization examples, VCF statistics, Colab notebooks, training procedures, usage guides, and build/testing instructions. The content appears to be descriptive and explanatory, discussing concepts related to computational biology and genomic analysis using DeepVariant. There are no code snippets or logs present."
Testability,"e will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPD",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:1997,benchmark,benchmark,1997,docs/deeptrio-wgs-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: e will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPD
",False,
Testability,"e will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 PacBio HiFi WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10).; These reads have been aligned to the GRCh38_no_alt_analysis reference using; [pbmm2](https://github.com/P",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:2066,benchmark,benchmark,2066,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: e will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 PacBio HiFi WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10).; These reads have been aligned to the GRCh38_no_alt_analysis reference using; [pbmm2](https://github.com/P
",False,
Testability,"e; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 30x whole genome sample in BAM and CRAM format. Filetype | Size (Gb) | Runtime (min); -------- | --------- | -------------; BAM | 66.99 | 79m47.37307s; CRAM | 37.85 | 96m53.477s; Ratio | 56.50% | 121.43%. * BAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" samtools; ```. and. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" bcftools; ```. You can read more about samtools and bcftools here: http://www.htslib.org/doc/. ## Commands for requesting machines used in case studies. We report runtime in our case studies documentation. In order to make sure the; results we report are reproducible without too much variation, we provide the; commands we used here to show you what kind of machines we ran the case studies; on. This is NOT the fastest or cheapest configuration. ### Command for a CPU-only mach",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:10410,test,testdata,10410,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: e; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 30x whole genome sample in BAM and CRAM format. Filetype | Size (Gb) | Runtime (min); -------- | --------- | -------------; BAM | 66.99 | 79m47.37307s; CRAM | 37.85 | 96m53.477s; Ratio | 56.50% | 121.43%. * BAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" samtools; ```. and. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" bcftools; ```. You can read more about samtools and bcftools here: http://www.htslib.org/doc/. ## Commands for requesting machines used in case studies. We report runtime in our case studies documentation. In order to make sure the; results we report are reproducible without too much variation, we provide the; commands we used here to show you what kind of machines we ran the case studies; on. This is NOT the fastest or cheapest configuration. ### Command for a CPU-only mach
",True,"The text contains code snippets and program logs, such as command lines with bash syntax (`bash; docker run ...`), file paths, timestamps, and performance metrics which are indicative of programming output or build processes."
Testability,"ecifying `--model_type ONT_R104`, you'll be using a model that is best; suited for Oxford Nanopore R10.4.1 chemistry Simplex and Duplex reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. ## Benchmark HG003 chr20 output from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. TRUTH_VCF=""HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz""; TRUTH_BED=""HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${INPUT_DIR}/${R",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md:4110,benchmark,benchmarks,4110,docs/deepvariant-ont-r104-simplex-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md,1,['benchmark'],['benchmarks'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ecifying `--model_type ONT_R104`, you'll be using a model that is best; suited for Oxford Nanopore R10.4.1 chemistry Simplex and Duplex reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. ## Benchmark HG003 chr20 output from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. TRUTH_VCF=""HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz""; TRUTH_BED=""HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${INPUT_DIR}/${R
",False,"The content includes both bash commands and explanations about the use of those commands. While there are code elements, it also contains descriptive text explaining how to execute the commands and their purpose, which is meaningful for humans. Therefore, this should not be eliminated."
Testability,"eepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. We; have tested and confirmed that this does not affect GPU usage or inference. So; you can continue running DeepVariant without being worried about this message. ## How much GPU memory is needed for the Keras models?. 16GB. In our test, we observe the model occupying 16GB GPU memory. ## Do models from before r1.6.0 work with current inference code?. No. We have moved from Slim to Keras. All models before `1.6.0` were trained in; Slim platform. So they are not compatible with `1.6.0` anymore. ## Can call_variants be run on multiple GPUs?. No. Although possible, we have not implemented the multi-GPU capability in GPU; inference yet. ## Can model_train be run on multiple GPUs?. No. TensorFlow's Estimator API does provide support for running training on; multiple GPUs through the use of a DistributionStrategy. However,; DistributionStrategy cannot be used with exponential moving average (EMA), which; is present in the DeepVariant codebase. ## What is the realigner and how does it work?. From the; [DeepVariant 2018 manuscript](https://www.nature.com/articles/nbt.4235.epdf?author_access_token=q4ZmzqvvcGBqTuKyKgYrQ9RgN0jAjWel9jnR3ZoTv0NuM3saQzpZk8yexjfPUhdFj4zyaA4Yvq0LWBoCYQ4B9vqPuv8e2HHy4vShDgEs8YxI_hLs9ov6Y1f_4fyS7kGZ):. > Mapped reads are",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:9448,test,test,9448,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['test'],['test'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: eepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. We; have tested and confirmed that this does not affect GPU usage or inference. So; you can continue running DeepVariant without being worried about this message. ## How much GPU memory is needed for the Keras models?. 16GB. In our test, we observe the model occupying 16GB GPU memory. ## Do models from before r1.6.0 work with current inference code?. No. We have moved from Slim to Keras. All models before `1.6.0` were trained in; Slim platform. So they are not compatible with `1.6.0` anymore. ## Can call_variants be run on multiple GPUs?. No. Although possible, we have not implemented the multi-GPU capability in GPU; inference yet. ## Can model_train be run on multiple GPUs?. No. TensorFlow's Estimator API does provide support for running training on; multiple GPUs through the use of a DistributionStrategy. However,; DistributionStrategy cannot be used with exponential moving average (EMA), which; is present in the DeepVariant codebase. ## What is the realigner and how does it work?. From the; [DeepVariant 2018 manuscript](https://www.nature.com/articles/nbt.4235.epdf?author_access_token=q4ZmzqvvcGBqTuKyKgYrQ9RgN0jAjWel9jnR3ZoTv0NuM3saQzpZk8yexjfPUhdFj4zyaA4Yvq0LWBoCYQ4B9vqPuv8e2HHy4vShDgEs8YxI_hLs9ov6Y1f_4fyS7kGZ):. > Mapped reads are
",False,
Testability,"eepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --haploid_contigs ""${HAPLOID_CONTIGS}"" \; --par_regions_bed ""${INPUT_DIR}/${PAR_BED}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Benchmark X, Y outputs from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v1.0 of the Genome in a Bottle; small variant benchmarks for HG002_chrXY. ```bash; FTPDIR=https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/chrXY_v1.0/GRCh38/SmallVariant. curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.bed > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.bed; curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz.tbi > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz.tbi. TRUTH_VCF=""HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz""; TRUTH_BED=""HG002_GRCh38_chrXY_smallvar_v1.0.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. REGION=""chrX,chrY""; sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${INPUT_DIR}/${REF}"" \; -o ""$",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md:3509,benchmark,benchmarks,3509,docs/deepvariant-xy-calling-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md,1,['benchmark'],['benchmarks'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: eepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --haploid_contigs ""${HAPLOID_CONTIGS}"" \; --par_regions_bed ""${INPUT_DIR}/${PAR_BED}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Benchmark X, Y outputs from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v1.0 of the Genome in a Bottle; small variant benchmarks for HG002_chrXY. ```bash; FTPDIR=https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/chrXY_v1.0/GRCh38/SmallVariant. curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.bed > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.bed; curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz.tbi > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz.tbi. TRUTH_VCF=""HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz""; TRUTH_BED=""HG002_GRCh38_chrXY_smallvar_v1.0.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. REGION=""chrX,chrY""; sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${INPUT_DIR}/${REF}"" \; -o ""$
",True,"The text contains code snippets and API documentation. It includes command lines, parameters, and script executions which are more programmatic in nature."
Testability,"eference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation, `hap.py` will intersect this BED with the GIAB; confident regions. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_novogene.grch38.bed; ```. ## Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md:1453,benchmark,benchmark,1453,docs/deepvariant-exome-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: eference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation, `hap.py` will intersect this BED with the GIAB; confident regions. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_novogene.grch38.bed; ```. ## Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/
",False,"The content includes several bash commands for downloading data and processing files, which are code snippets. However, the text also contains explanatory sentences describing each step in a natural language format, such as 'In this case study we'll use idt_capture_novogene.grch38.bed as the capture target BED file.' This means the text is not exclusively code or logs but contains meaningful human-readable explanations."
Testability,"eference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepVariant with one command. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can now run DeepVariant with one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:1519,benchmark,benchmark,1519,docs/deepvariant-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: eference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepVariant with one command. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can now run DeepVariant with one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/
",False,"The text includes commands and syntax that might be part of code or scripts, but also contains explanatory sentences about the process of running DeepVariant, making it a mix of both. The primary content is not purely code snippets or logs; instead, there are meaningful explanations interspersed with code. Therefore, while the text has some code elements, it also provides enough human-readable explanation to be considered meaningful and not solely for programmatic use."
Testability,"eference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai; ```. ## Running DeepVariant. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can run DeepVariant with just one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. Here we specify `--regions chr20` to run on just chromosome 20, saving time so; you can run this case study wi",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:3553,benchmark,benchmark,3553,docs/deepvariant-hybrid-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: eference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai; ```. ## Running DeepVariant. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can run DeepVariant with just one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. Here we specify `--regions chr20` to run on just chromosome 20, saving time so; you can run this case study wi
",True,"The text consists primarily of code snippets and commands (curl, mkdir, samtools), which are programmatic and not meaningful human-readable sentences."
Testability,"effectively haploid. So, we are introducing two; flags to re-adjust the genotypes in regions that are considered to be haploid; for those samples. You can use `--haploid_contigs` and `--par_regions_bed` parameters to readjust; the genotypes in haploid regions. For samples with XY karyotype, it is expected; that users will set `--haploid_contigs=""chrX,chrY""` for; [GRCh38](https://storage.googleapis.com/deepvariant/case-study-testdata/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa); and `--haploid_contigs=""X,Y""` for; [GRCh37](https://storage.googleapis.com/deepvariant/case-study-testdata/hs37d5.fa).; You can also provide a PAR region bed file with; `--par_regions_bed=""/input/GRCh3X_par.bed""` parameter. The regions in the PAR; bed file will be skipped from genotype readjustment. You can download the PAR; bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). ## How it works. The genotype re-adjustment is implemented in the `postprocess_variants` stage of; DeepVariant. For any variant, that is in the`--haploid_contigs` regions and; **not** in the `--par_regions_bed` regions, the genotype likelihoods of; heterozygous variants are set as 0 and the genotypes are normalized again after; re-adjusting the likelihoods. After that the most-likely genotype is assigned to; the allele which excludes any heterozygous calls. For example, suppose we observe an alternate allele `ALT1` at a position that we; consider to be haploid. So the observed alleles at that position are:; `Candidates: {REF, ALT1}` The neural network generates likelihoods for the; genotypes for this candidate as such:. ```; Homozygous reference: likelihood(REF,REF); Heterozygous alternate: likelihood(REF,ALT1); Homozygous alternaate: likelihood(ALT1,ALT1); ```. So the likelihood vector looks like: `L={L[(REF, REF)], L[(REF, ALT1)], L[(ALT1,; ALT1)]}` In th",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-haploid-support.md:1455,test,testdata,1455,docs/deepvariant-haploid-support.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-haploid-support.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: effectively haploid. So, we are introducing two; flags to re-adjust the genotypes in regions that are considered to be haploid; for those samples. You can use `--haploid_contigs` and `--par_regions_bed` parameters to readjust; the genotypes in haploid regions. For samples with XY karyotype, it is expected; that users will set `--haploid_contigs=""chrX,chrY""` for; [GRCh38](https://storage.googleapis.com/deepvariant/case-study-testdata/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa); and `--haploid_contigs=""X,Y""` for; [GRCh37](https://storage.googleapis.com/deepvariant/case-study-testdata/hs37d5.fa).; You can also provide a PAR region bed file with; `--par_regions_bed=""/input/GRCh3X_par.bed""` parameter. The regions in the PAR; bed file will be skipped from genotype readjustment. You can download the PAR; bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). ## How it works. The genotype re-adjustment is implemented in the `postprocess_variants` stage of; DeepVariant. For any variant, that is in the`--haploid_contigs` regions and; **not** in the `--par_regions_bed` regions, the genotype likelihoods of; heterozygous variants are set as 0 and the genotypes are normalized again after; re-adjusting the likelihoods. After that the most-likely genotype is assigned to; the allele which excludes any heterozygous calls. For example, suppose we observe an alternate allele `ALT1` at a position that we; consider to be haploid. So the observed alleles at that position are:; `Candidates: {REF, ALT1}` The neural network generates likelihoods for the; genotypes for this candidate as such:. ```; Homozygous reference: likelihood(REF,REF); Heterozygous alternate: likelihood(REF,ALT1); Homozygous alternaate: likelihood(ALT1,ALT1); ```. So the likelihood vector looks like: `L={L[(REF, REF)], L[(REF, ALT1)], L[(ALT1,; ALT1)]}` In th
",False,"The text contains detailed explanations of how genotype re-adjustment is implemented in DeepVariant, including descriptions of parameters and their usage. While there are technical terms, the content is written in a human-readable format discussing concepts and processes."
Testability,"el_type WGS`, you'll be using a model that is best suited; for Illumina Whole Genome Sequencing data. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 10628 10588 40 21099 19 10036 15 3 0.996236 0.998283 0.475662 0.997258 NaN NaN 1.748961 2.318182; INDEL PASS 10628 10588 40 21099 19 10036 15 3 0.996236 0.998283 0.475662 0.997258 NaN NaN 1.748961 2.318182; SNP ALL 70166 69917 249 84796 59 14782 13 3 0.996451 0.999157 0.174324 0.997802",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:3991,benchmark,benchmark,3991,docs/deepvariant-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md,2,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: el_type WGS`, you'll be using a model that is best suited; for Illumina Whole Genome Sequencing data. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 10628 10588 40 21099 19 10036 15 3 0.996236 0.998283 0.475662 0.997258 NaN NaN 1.748961 2.318182; INDEL PASS 10628 10588 40 21099 19 10036 15 3 0.996236 0.998283 0.475662 0.997258 NaN NaN 1.748961 2.318182; SNP ALL 70166 69917 249 84796 59 14782 13 3 0.996451 0.999157 0.174324 0.997802
",False,"The text contains detailed benchmarking summary and statistics which are in the form of tables and numerical data. However, it is presented as a human-readable report rather than code or logs. The content includes explanations such as 'Benchmarking Summary' and descriptions of each metric like 'METRIC.Recall', etc., which are part of an analysis rather than programmatic output."
Testability,"enchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepTrio with one command. DeepTrio pipeline consists of 4 steps: `make_examples`, `call_variants`,; `postp",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:2988,test,testdata,2988,docs/deeptrio-wgs-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: enchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepTrio with one command. DeepTrio pipeline consists of 4 steps: `make_examples`, `call_variants`,; `postp
",True,"The text contains code snippets and command lines used for downloading files, which are primarily logs or API documentation. It also includes bash commands and file operations that are typical of programmatic instructions rather than meaningful human-readable content."
Testability,"er interactively to execute a series of; commands. Run the following command to launch a bedtools container. ```bash; sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; -it quay.io/biocontainers/bedtools:2.23.0--h5b5514e_6 \; /bin/bash; ```. ### Extract regions with 3x coverage, and filter out unused contigs. We will restrict our analysis to regions with a minimum of 3x coverage. ```bash; # (Run within the bedtools container); min_coverage=3; gzip -dc data/hg005_coverage.per-base.bed.gz | \; egrep -v 'HLA|decoy|random|alt|chrUn|chrEBV' | \; awk -v OFS=""\t"" -v min_coverage=${min_coverage} '$4 >= min_coverage { print }' | \; bedtools merge -d 1 -c 4 -o mean -i - > data/hg005_3x.bed; ```. ### Intersect coverage with CDS regions. Now we will intersect our 3x bedfile with the CDS bed file:. ```bash; # (Run within the bedtools container); bedtools intersect \; -a data/hg005_3x.bed \; -b data/chr20_CDS.bed > data/chr20_CDS_3x.bed. # We will also intersect this file with confident GIAB regions; bedtools intersect \; -a benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed \; -b data/chr20_CDS_3x.bed > benchmark/chr20_CDS_3x.benchmark_regions.bed; ```. We now have a bed file of CDS regions intersected with 3x coverage regions; called `data/chr20_CDS_3x.bed`. You can exit the docker container now. Type; `exit` and hit enter. ### Download the RNA-seq model. Finally, lets download the RNA-seq model that we will use to call variants. ```bash; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:4975,benchmark,benchmark,4975,docs/deepvariant-rnaseq-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: er interactively to execute a series of; commands. Run the following command to launch a bedtools container. ```bash; sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; -it quay.io/biocontainers/bedtools:2.23.0--h5b5514e_6 \; /bin/bash; ```. ### Extract regions with 3x coverage, and filter out unused contigs. We will restrict our analysis to regions with a minimum of 3x coverage. ```bash; # (Run within the bedtools container); min_coverage=3; gzip -dc data/hg005_coverage.per-base.bed.gz | \; egrep -v 'HLA|decoy|random|alt|chrUn|chrEBV' | \; awk -v OFS=""\t"" -v min_coverage=${min_coverage} '$4 >= min_coverage { print }' | \; bedtools merge -d 1 -c 4 -o mean -i - > data/hg005_3x.bed; ```. ### Intersect coverage with CDS regions. Now we will intersect our 3x bedfile with the CDS bed file:. ```bash; # (Run within the bedtools container); bedtools intersect \; -a data/hg005_3x.bed \; -b data/chr20_CDS.bed > data/chr20_CDS_3x.bed. # We will also intersect this file with confident GIAB regions; bedtools intersect \; -a benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed \; -b data/chr20_CDS_3x.bed > benchmark/chr20_CDS_3x.benchmark_regions.bed; ```. We now have a bed file of CDS regions intersected with 3x coverage regions; called `data/chr20_CDS_3x.bed`. You can exit the docker container now. Type; `exit` and hit enter. ### Download the RNA-seq model. Finally, lets download the RNA-seq model that we will use to call variants. ```bash; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.
",True,"The text contains multiple code snippets using bash and bedtools commands. These are programmatic instructions meant for executing shell scripts and pipeline steps, likely part of a data analysis workflow. There is no meaningful human-readable content beyond the procedural steps."
Testability,"examples | 116m37.385s; call_variants | 214m37.055s; postprocess_variants (with gVCF) | 30m59.968s. ### Run hap.py. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; ${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; -f ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -o ${PWD}/happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 504501 502199 2302 960061 1526 434935 906 371 0.995437 0.997094 0.453029 0.996265 NaN NaN 1.489759 1.952023; INDEL PASS 504501 502199 2302 960061 1526 434935 906 371 0.995437 0.997094 0.453029 0.996265 NaN NaN 1.489759 1.952023; SNP ALL 3327496 3316515 10981 3858659 5550 534709 2104 475 0.996700 0.998330 0.138574 0.997514 2.102576 1.970783 1.535137 1.436586; SNP PASS 3327496 3316515 10981 3858659 5550 534709 2104 475 0.996700 0.998330 0.138574 0.997514 2.102576 1.970783 1.535137 1.436586; ```. | Type | TRUT",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:7069,benchmark,benchmark,7069,docs/deepvariant-vg-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: examples | 116m37.385s; call_variants | 214m37.055s; postprocess_variants (with gVCF) | 30m59.968s. ### Run hap.py. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; ${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; -f ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -o ${PWD}/happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 504501 502199 2302 960061 1526 434935 906 371 0.995437 0.997094 0.453029 0.996265 NaN NaN 1.489759 1.952023; INDEL PASS 504501 502199 2302 960061 1526 434935 906 371 0.995437 0.997094 0.453029 0.996265 NaN NaN 1.489759 1.952023; SNP ALL 3327496 3316515 10981 3858659 5550 534709 2104 475 0.996700 0.998330 0.138574 0.997514 2.102576 1.970783 1.535137 1.436586; SNP PASS 3327496 3316515 10981 3858659 5550 534709 2104 475 0.996700 0.998330 0.138574 0.997514 2.102576 1.970783 1.535137 1.436586; ```. | Type | TRUT
",False,"The text contains human-readable sentences, including benchmark summaries and data analysis results which are presented in a structured format with meaningful content."
Testability,"ference sequence.; * `--reads` - Specifies the input bam file.; * `--output_vcf` - Specifies the output variant file.; * `--num_shards` - Sets the number of shards to the number of available; processors (`$(nproc)`). This is used to perform parallelization.; * `--regions` - Restricts analysis to 3x chr20 CDS regions only.; * `--make_examples_extra_args=` - Passes additional arguments to; make_examples.; * `split_skip_reads=true` - *Important!* This flag is critical for RNA-seq; variant calling to work properly. It enables RNA-seq data to be; processed efficiently.; * `channels=''` - Resets the channel list to be appropriate for the; RNA-seq model.; * `--intermediate_results_dir` - Outputs results to an intermediate directory. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; sudo docker run \; -v $(pwd):$(pwd) \; -w $(pwd) \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; output/HG005.output.vcf.gz \; -f benchmark/chr20_CDS_3x.benchmark_regions.bed \; -r reference/GRCh38_no_alt_analysis_set.fasta \; -o happy/happy.output \; --engine=vcfeval \; --pass-only \; --target-regions=data/chr20_CDS_3x.bed \; --threads=$(nproc); ```. **Flag summary**. * `-f` - Sets the benchmark regions (regions of interest that we want to; benchmark.); * `-r` - Sets the reference genome.; * `-o` - Specifies the output location.; * `--engine` - Sets the variant comparison engine. See; [hap.py documentation](https://github.com/Illumina/hap.py) for details.; * `--pass-only` - Restricts benchmarking to variants that have passed all; filters.; * `--target-regions` - Restricts analysis to given regions only.; * `--threads` - Level of parallelization to use. **Output:**. The above command should output the following results:. ```; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:8976,benchmark,benchmark,8976,docs/deepvariant-rnaseq-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ference sequence.; * `--reads` - Specifies the input bam file.; * `--output_vcf` - Specifies the output variant file.; * `--num_shards` - Sets the number of shards to the number of available; processors (`$(nproc)`). This is used to perform parallelization.; * `--regions` - Restricts analysis to 3x chr20 CDS regions only.; * `--make_examples_extra_args=` - Passes additional arguments to; make_examples.; * `split_skip_reads=true` - *Important!* This flag is critical for RNA-seq; variant calling to work properly. It enables RNA-seq data to be; processed efficiently.; * `channels=''` - Resets the channel list to be appropriate for the; RNA-seq model.; * `--intermediate_results_dir` - Outputs results to an intermediate directory. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; sudo docker run \; -v $(pwd):$(pwd) \; -w $(pwd) \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; output/HG005.output.vcf.gz \; -f benchmark/chr20_CDS_3x.benchmark_regions.bed \; -r reference/GRCh38_no_alt_analysis_set.fasta \; -o happy/happy.output \; --engine=vcfeval \; --pass-only \; --target-regions=data/chr20_CDS_3x.bed \; --threads=$(nproc); ```. **Flag summary**. * `-f` - Sets the benchmark regions (regions of interest that we want to; benchmark.); * `-r` - Sets the reference genome.; * `-o` - Specifies the output location.; * `--engine` - Sets the variant comparison engine. See; [hap.py documentation](https://github.com/Illumina/hap.py) for details.; * `--pass-only` - Restricts benchmarking to variants that have passed all; filters.; * `--target-regions` - Restricts analysis to given regions only.; * `--threads` - Level of parallelization to use. **Output:**. The above command should output the following results:. ```; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision
",False,
Testability,"fferent model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. To see the pileup images visually, check out [show_examples](show-examples.md). For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). Just make sure to use `--model_type; HYBRID_PACBIO_ILLUMINA` when running on combined PacBio and Illumina data. ## Benchmark with hap.py. See [hap.py](https://github.com/illumina/hap.py) documentation for more details; on the parameters and outputs. ```bash; mkdir -p happy. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 10628 10602 26 23385 63 12212 10 51 0.997554 0.994361 0.522215 0.995955 NaN NaN 1.748961 2.721448; INDEL PASS 10628 10602 26 23385 63 12212 10 51 0.997554 0.994361 0.522215 0.995955 NaN NaN 1.748961 2.721448; SNP ALL 70166 70138 28 105564 43 35354 16 16 0.999601 0.999388 0.334906 0.999",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:6547,benchmark,benchmark,6547,docs/deepvariant-hybrid-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md,2,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: fferent model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. To see the pileup images visually, check out [show_examples](show-examples.md). For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). Just make sure to use `--model_type; HYBRID_PACBIO_ILLUMINA` when running on combined PacBio and Illumina data. ## Benchmark with hap.py. See [hap.py](https://github.com/illumina/hap.py) documentation for more details; on the parameters and outputs. ```bash; mkdir -p happy. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 10628 10602 26 23385 63 12212 10 51 0.997554 0.994361 0.522215 0.995955 NaN NaN 1.748961 2.721448; INDEL PASS 10628 10602 26 23385 63 12212 10 51 0.997554 0.994361 0.522215 0.995955 NaN NaN 1.748961 2.721448; SNP ALL 70166 70138 28 105564 43 35354 16 16 0.999601 0.999388 0.334906 0.999
",False,
Testability,"for each candidate allele it observes. For samples with karyotype; XY, the chromosome X and Y are effectively haploid. So, we are introducing two; flags to re-adjust the genotypes in regions that are considered to be haploid; for those samples. You can use `--haploid_contigs` and `--par_regions_bed` parameters to readjust; the genotypes in haploid regions. For samples with XY karyotype, it is expected; that users will set `--haploid_contigs=""chrX,chrY""` for; [GRCh38](https://storage.googleapis.com/deepvariant/case-study-testdata/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa); and `--haploid_contigs=""X,Y""` for; [GRCh37](https://storage.googleapis.com/deepvariant/case-study-testdata/hs37d5.fa).; You can also provide a PAR region bed file with; `--par_regions_bed=""/input/GRCh3X_par.bed""` parameter. The regions in the PAR; bed file will be skipped from genotype readjustment. You can download the PAR; bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). ## How it works. The genotype re-adjustment is implemented in the `postprocess_variants` stage of; DeepVariant. For any variant, that is in the`--haploid_contigs` regions and; **not** in the `--par_regions_bed` regions, the genotype likelihoods of; heterozygous variants are set as 0 and the genotypes are normalized again after; re-adjusting the likelihoods. After that the most-likely genotype is assigned to; the allele which excludes any heterozygous calls. For example, suppose we observe an alternate allele `ALT1` at a position that we; consider to be haploid. So the observed alleles at that position are:; `Candidates: {REF, ALT1}` The neural network generates likelihoods for the; genotypes for this candidate as such:. ```; Homozygous reference: likelihood(REF,REF); Heterozygous alternate: likelihood(REF,ALT1); Homozygous alternaate: likelihood(ALT1,ALT1); ```",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-haploid-support.md:1357,test,testdata,1357,docs/deepvariant-haploid-support.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-haploid-support.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: for each candidate allele it observes. For samples with karyotype; XY, the chromosome X and Y are effectively haploid. So, we are introducing two; flags to re-adjust the genotypes in regions that are considered to be haploid; for those samples. You can use `--haploid_contigs` and `--par_regions_bed` parameters to readjust; the genotypes in haploid regions. For samples with XY karyotype, it is expected; that users will set `--haploid_contigs=""chrX,chrY""` for; [GRCh38](https://storage.googleapis.com/deepvariant/case-study-testdata/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa); and `--haploid_contigs=""X,Y""` for; [GRCh37](https://storage.googleapis.com/deepvariant/case-study-testdata/hs37d5.fa).; You can also provide a PAR region bed file with; `--par_regions_bed=""/input/GRCh3X_par.bed""` parameter. The regions in the PAR; bed file will be skipped from genotype readjustment. You can download the PAR; bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). ## How it works. The genotype re-adjustment is implemented in the `postprocess_variants` stage of; DeepVariant. For any variant, that is in the`--haploid_contigs` regions and; **not** in the `--par_regions_bed` regions, the genotype likelihoods of; heterozygous variants are set as 0 and the genotypes are normalized again after; re-adjusting the likelihoods. After that the most-likely genotype is assigned to; the allele which excludes any heterozygous calls. For example, suppose we observe an alternate allele `ALT1` at a position that we; consider to be haploid. So the observed alleles at that position are:; `Candidates: {REF, ALT1}` The neural network generates likelihoods for the; genotypes for this candidate as such:. ```; Homozygous reference: likelihood(REF,REF); Heterozygous alternate: likelihood(REF,ALT1); Homozygous alternaate: likelihood(ALT1,ALT1); ```
",False,"This text contains detailed explanations about how the genotype re-adjustment works in DeepVariant. It includes steps and descriptions that are easily understandable for humans without requiring programming knowledge. The sentences explain the process clearly, making it accessible to readers who may not be technical experts."
Testability,"h38 Reference + Index; * CDS bedfile (chr20 only); * GIAB benchmark data. ## Prepare Data. ### Setup directories. Lets first create directories to organize files. ```bash; mkdir -p data benchmark reference model output happy; ```. ### Download the GRCh38 Reference. We will be using GRCh38 for this case study. ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. # Extract chr20 CDS regions and convert to bed file.; gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \; awk -v OFS='\t' '$1 == ""chr20"" && $3 ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:1660,benchmark,benchmarks,1660,docs/deepvariant-rnaseq-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md,1,['benchmark'],['benchmarks'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: h38 Reference + Index; * CDS bedfile (chr20 only); * GIAB benchmark data. ## Prepare Data. ### Setup directories. Lets first create directories to organize files. ```bash; mkdir -p data benchmark reference model output happy; ```. ### Download the GRCh38 Reference. We will be using GRCh38 for this case study. ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. # Extract chr20 CDS regions and convert to bed file.; gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \; awk -v OFS='\t' '$1 == ""chr20"" && $3 
",False,"The text contains detailed step-by-step instructions for setting up directories, downloading and processing reference data, including GRCh38 and GIAB benchmarks. It also includes code snippets for bash commands which are part of the process but doesn't consist solely of code or logs."
Testability,"h38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmar",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:1736,benchmark,benchmark,1736,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,2,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: h38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmar
",False,
Testability,"h38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai; ```. ## Running DeepVariant. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can run DeepVariant with just one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. Here we specify `--regions chr20` to run on just chromosome 20, saving time so; you can run this case study within about half an hour (tested on 64 CPUs). ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --mode",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:3916,test,testdata,3916,docs/deepvariant-hybrid-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: h38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai; ```. ## Running DeepVariant. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can run DeepVariant with just one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. Here we specify `--regions chr20` to run on just chromosome 20, saving time so; you can run this case study within about half an hour (tested on 64 CPUs). ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --mode
",False,
Testability,"hat you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure; ```. NOTE: To create an instance *without GPU*, simply omit the last line from the; command. Check that the instance has been created and started:. ```shell; gcloud compute instances list; ```. which should produce output like:. ```; NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS; [USER]-deepvariant-quickstart us-west1-b n1-standard-8 10.138.0.4 35.185.203.59 RUNNING; ```. Then connect to your instance via SSH:. ```shell; gcloud compute ssh --zone us-west1-b ""${USER}-deepvariant-quickstart""; ```. You should land at a shell prompt in your new instance!. NOTE: All of these steps can also be completed from the Cloud Console, if you; prefer. Consult [this; guide](https://cloud.google.com/compute/docs/quickstart-linux), but be sure to; choose Ubuntu 20.04 as your image, as DeepVariant has not been tested on other; Linux distributions. For more information about getting started with Compute Engine, see:. * [Compute Engine instance creation in `gcloud`; manual](https://cloud.google.com/sdk/gcloud/reference/compute/instances/create); * [Reference to machine; sizes/types](https://cloud.google.com/compute/docs/machine-types); ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:4997,test,tested,4997,docs/deepvariant-gcp-info.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md,1,['test'],['tested'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: hat you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure; ```. NOTE: To create an instance *without GPU*, simply omit the last line from the; command. Check that the instance has been created and started:. ```shell; gcloud compute instances list; ```. which should produce output like:. ```; NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS; [USER]-deepvariant-quickstart us-west1-b n1-standard-8 10.138.0.4 35.185.203.59 RUNNING; ```. Then connect to your instance via SSH:. ```shell; gcloud compute ssh --zone us-west1-b ""${USER}-deepvariant-quickstart""; ```. You should land at a shell prompt in your new instance!. NOTE: All of these steps can also be completed from the Cloud Console, if you; prefer. Consult [this; guide](https://cloud.google.com/compute/docs/quickstart-linux), but be sure to; choose Ubuntu 20.04 as your image, as DeepVariant has not been tested on other; Linux distributions. For more information about getting started with Compute Engine, see:. * [Compute Engine instance creation in `gcloud`; manual](https://cloud.google.com/sdk/gcloud/reference/compute/instances/create); * [Reference to machine; sizes/types](https://cloud.google.com/compute/docs/machine-types); 
",False,"The content includes step-by-step instructions for creating a Google Cloud instance, including code snippets and commands. While there are some descriptive text, the primary focus is on providing actionable code, which may be considered more technical than human-readable prose."
Testability,"hat your home directory is first.; ls $HOME; BIN_VERSION=""1.6.1""; sudo docker run \; -v ""${HOME}"":""${HOME}"" \; google/deepvariant:""${BIN_VERSION}"" \; ls $HOME; ```. ## How do I run multi-sample calling?. Since the DeepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. We; have tested and confirmed that this does not affect GPU usage or inference. So; you can continue running DeepVariant without being worried about this message. ## How much GPU memory is needed for the Keras models?. 16GB. In our test, we observe the model occupying 16GB GPU memory. ## Do models from before r1.6.0 work with current inference code?. No. We have moved from Slim to Keras. All models before `1.6.0` were trained in; Slim platform. So they are not compatible with `1.6.0` anymore. ## Can call_variants be run on multiple GPUs?. No. Although possible, we have not implemented the multi-GPU capability in GPU; inference yet. ## Can model_train be run on multiple GPUs?. No. TensorFlow's Estimator API does provide support for running training on; multiple GPUs through the use of a DistributionStrategy. However,; DistributionStrategy cannot be used with exponential moving average (EMA), which; is present in the DeepVariant codebase. ## What is the realigner and how does it work?. From the; [DeepVariant 2018 manuscript](",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:9225,test,tested,9225,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['test'],['tested'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: hat your home directory is first.; ls $HOME; BIN_VERSION=""1.6.1""; sudo docker run \; -v ""${HOME}"":""${HOME}"" \; google/deepvariant:""${BIN_VERSION}"" \; ls $HOME; ```. ## How do I run multi-sample calling?. Since the DeepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. We; have tested and confirmed that this does not affect GPU usage or inference. So; you can continue running DeepVariant without being worried about this message. ## How much GPU memory is needed for the Keras models?. 16GB. In our test, we observe the model occupying 16GB GPU memory. ## Do models from before r1.6.0 work with current inference code?. No. We have moved from Slim to Keras. All models before `1.6.0` were trained in; Slim platform. So they are not compatible with `1.6.0` anymore. ## Can call_variants be run on multiple GPUs?. No. Although possible, we have not implemented the multi-GPU capability in GPU; inference yet. ## Can model_train be run on multiple GPUs?. No. TensorFlow's Estimator API does provide support for running training on; multiple GPUs through the use of a DistributionStrategy. However,; DistributionStrategy cannot be used with exponential moving average (EMA), which; is present in the DeepVariant codebase. ## What is the realigner and how does it work?. From the; [DeepVariant 2018 manuscript](
",True,"The text contains API documentation or specifications such as parameter lists and return types. For example, the command line options are described in detail, indicating a programmatic description rather than meaningful human-readable sentences."
Testability,"hromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics T7 HG001 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG001. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics T7 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.data-00000-of-00001 > input/weights-51-0.995354.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.index > input/weights-51-0.995354.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf /output/HG001.output.vcf.gz \; --output_gvcf /output/HG001.output.g.vcf.gz \; --num",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md:1262,benchmark,benchmark,1262,docs/deepvariant-complete-t7-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: hromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics T7 HG001 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG001. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics T7 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.data-00000-of-00001 > input/weights-51-0.995354.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.index > input/weights-51-0.995354.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf /output/HG001.output.vcf.gz \; --output_gvcf /output/HG001.output.g.vcf.gz \; --num
",True,"The text contains command lines and code snippets, such as bash commands and curl commands. These are typically found in program logs or API documentation."
Testability,"https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam > input/HG002.complete_g400.V350151728.grch38.chr20.bam. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam.bai > input/HG002.complete_g400.V350151728.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG002. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics G400 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.data-00000-of-00001 > input/weights-60-0.993753.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.index > input/weights-60-0.993753.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG002.complete_g400.V350151728.grch38.chr20.bam \; --output_vcf /output/HG002.output.vcf.gz \; --output_gvcf /output/HG002.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-60-0.993753.ckpt; `",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md:1489,test,testdata,1489,docs/deepvariant-complete-g400-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam > input/HG002.complete_g400.V350151728.grch38.chr20.bam. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam.bai > input/HG002.complete_g400.V350151728.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG002. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics G400 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.data-00000-of-00001 > input/weights-60-0.993753.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.index > input/weights-60-0.993753.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG002.complete_g400.V350151728.grch38.chr20.bam \; --output_vcf /output/HG002.output.vcf.gz \; --output_gvcf /output/HG002.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-60-0.993753.ckpt; `
",False,"The text contains command lines and file operations, but it also includes explanatory sentences like 'Download Genome in a Bottle Benchmarks for HG002' which is meaningful and in natural language."
Testability,"hub.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation, `hap.py` will intersect this BED with the GIAB; confident regions. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_novogene.grch38.bed; ```. ## Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_di",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md:1337,benchmark,benchmark,1337,docs/deepvariant-exome-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: hub.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation, `hap.py` will intersect this BED with the GIAB; confident regions. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_novogene.grch38.bed; ```. ## Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_di
",False,
Testability,"hub.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepVariant with one command. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can now run DeepVariant with one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. ```bash; mkd",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:1403,benchmark,benchmark,1403,docs/deepvariant-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: hub.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepVariant with one command. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can now run DeepVariant with one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. ```bash; mkd
",False,"The text contains complete, meaningful sentences in natural language discussing the process of running DeepVariant and downloading necessary references, which are relevant for human-readable content."
Testability,"hub.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai; ```. ## Running DeepVariant. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can run DeepVariant with just one command using the; `run_deepvariant` script. ### Running on a CPU-only mac",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:3437,benchmark,benchmark,3437,docs/deepvariant-hybrid-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: hub.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai; ```. ## Running DeepVariant. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can run DeepVariant with just one command using the; `run_deepvariant` script. ### Running on a CPU-only mac
",True,"The text consists of code snippets, specifically bash commands for file downloads and data processing, which are indicative of programmatic content rather than meaningful human-readable sentences."
Testability,"ially for indels; * Vastly faster with reduced memory usage. We have made a number of improvements to the methodology as well. The biggest; change was to move away from RGB-encoded (3-channel) pileup images and instead; represent the aligned read data using a multi-channel tensor data layout. We; currently represent the data as a 6-channel raw tensor in which we encode:. * The read base (A, C, G, T); * The base's quality score; * The read's mapping quality score; * The read's strand (positive or negative); * Does the read support the allele being evaluated?; * Does the base match the reference genome at this position?. These are all readily derived from the information found in the BAM file; encoding of each read. Additional modeling changes were to move to the inception-v3 architecture and to; train on many more independent sequencing replicates of the ground truth; training samples, including 50% downsampled versions of each of those read sets.; In our testing this allowed the model to better generalize to other data types. In the end these changes reduced our error rate by more than 50% on the held out; evaluation sample (NA24385 / HG002) as compared to our results in the; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results/):. DeepVariant April 2016 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 4175 | 2839 | 0.987882 | 0.991728 | 0.989802; SNP | 1689 | 832 | 0.999447 | 0.999728 | 0.999587. DeepVariant December 2017 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 2384 | 1811 | 0.993081 | 0.994954 | 0.994017; SNP | 735 | 363 | 0.999759 | 0.999881 | 0.999820. See the [whole genome case study], which we update with each release of; DeepVariant, for the latest results. You can also see the [Colab example] to see how you can visualize the pileup; imag",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:7928,test,testing,7928,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['test'],['testing'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ially for indels; * Vastly faster with reduced memory usage. We have made a number of improvements to the methodology as well. The biggest; change was to move away from RGB-encoded (3-channel) pileup images and instead; represent the aligned read data using a multi-channel tensor data layout. We; currently represent the data as a 6-channel raw tensor in which we encode:. * The read base (A, C, G, T); * The base's quality score; * The read's mapping quality score; * The read's strand (positive or negative); * Does the read support the allele being evaluated?; * Does the base match the reference genome at this position?. These are all readily derived from the information found in the BAM file; encoding of each read. Additional modeling changes were to move to the inception-v3 architecture and to; train on many more independent sequencing replicates of the ground truth; training samples, including 50% downsampled versions of each of those read sets.; In our testing this allowed the model to better generalize to other data types. In the end these changes reduced our error rate by more than 50% on the held out; evaluation sample (NA24385 / HG002) as compared to our results in the; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results/):. DeepVariant April 2016 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 4175 | 2839 | 0.987882 | 0.991728 | 0.989802; SNP | 1689 | 832 | 0.999447 | 0.999728 | 0.999587. DeepVariant December 2017 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 2384 | 1811 | 0.993081 | 0.994954 | 0.994017; SNP | 735 | 363 | 0.999759 | 0.999881 | 0.999820. See the [whole genome case study], which we update with each release of; DeepVariant, for the latest results. You can also see the [Colab example] to see how you can visualize the pileup; imag
",False,"The text contains complete sentences that discuss improvements made to the methodology and specific changes in data representation and modeling. It includes both technical descriptions and analysis of results, which are meaningful for human readers."
Testability,"ian constraints; ```. ### Perform analysis with hap.py against 4.2.1 truth set. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 112",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:9971,benchmark,benchmark,9971,docs/deeptrio-wgs-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ian constraints; ```. ### Perform analysis with hap.py against 4.2.1 truth set. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 112
",True,"The text contains code snippets and program logs. The lines with bash commands, docker run instructions, and Hap.py execution are indicative of programmatic content that should be filtered out."
Testability,"iant calling in chromosome X and Y. ## Case study. A case study on how to use the parameters mentioned here are described in; [DeepVariant X, Y calling case study](deepvariant-xy-calling-case-study.md). ## Haploid calling support. As DeepVariant is a diploid variant caller, it assigns genotypes as {Hom-ref,; Het, Hom-alt} for each candidate allele it observes. For samples with karyotype; XY, the chromosome X and Y are effectively haploid. So, we are introducing two; flags to re-adjust the genotypes in regions that are considered to be haploid; for those samples. You can use `--haploid_contigs` and `--par_regions_bed` parameters to readjust; the genotypes in haploid regions. For samples with XY karyotype, it is expected; that users will set `--haploid_contigs=""chrX,chrY""` for; [GRCh38](https://storage.googleapis.com/deepvariant/case-study-testdata/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa); and `--haploid_contigs=""X,Y""` for; [GRCh37](https://storage.googleapis.com/deepvariant/case-study-testdata/hs37d5.fa).; You can also provide a PAR region bed file with; `--par_regions_bed=""/input/GRCh3X_par.bed""` parameter. The regions in the PAR; bed file will be skipped from genotype readjustment. You can download the PAR; bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). ## How it works. The genotype re-adjustment is implemented in the `postprocess_variants` stage of; DeepVariant. For any variant, that is in the`--haploid_contigs` regions and; **not** in the `--par_regions_bed` regions, the genotype likelihoods of; heterozygous variants are set as 0 and the genotypes are normalized again after; re-adjusting the likelihoods. After that the most-likely genotype is assigned to; the allele which excludes any heterozygous calls. For example, suppose we observe an alternate allele `ALT1` at a position that we; consi",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-haploid-support.md:1035,test,testdata,1035,docs/deepvariant-haploid-support.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-haploid-support.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: iant calling in chromosome X and Y. ## Case study. A case study on how to use the parameters mentioned here are described in; [DeepVariant X, Y calling case study](deepvariant-xy-calling-case-study.md). ## Haploid calling support. As DeepVariant is a diploid variant caller, it assigns genotypes as {Hom-ref,; Het, Hom-alt} for each candidate allele it observes. For samples with karyotype; XY, the chromosome X and Y are effectively haploid. So, we are introducing two; flags to re-adjust the genotypes in regions that are considered to be haploid; for those samples. You can use `--haploid_contigs` and `--par_regions_bed` parameters to readjust; the genotypes in haploid regions. For samples with XY karyotype, it is expected; that users will set `--haploid_contigs=""chrX,chrY""` for; [GRCh38](https://storage.googleapis.com/deepvariant/case-study-testdata/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa); and `--haploid_contigs=""X,Y""` for; [GRCh37](https://storage.googleapis.com/deepvariant/case-study-testdata/hs37d5.fa).; You can also provide a PAR region bed file with; `--par_regions_bed=""/input/GRCh3X_par.bed""` parameter. The regions in the PAR; bed file will be skipped from genotype readjustment. You can download the PAR; bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). ## How it works. The genotype re-adjustment is implemented in the `postprocess_variants` stage of; DeepVariant. For any variant, that is in the`--haploid_contigs` regions and; **not** in the `--par_regions_bed` regions, the genotype likelihoods of; heterozygous variants are set as 0 and the genotypes are normalized again after; re-adjusting the likelihoods. After that the most-likely genotype is assigned to; the allele which excludes any heterozygous calls. For example, suppose we observe an alternate allele `ALT1` at a position that we; consi
",False,"The text contains detailed explanations about the use of parameters and implementation steps in DeepVariant for haploid calling support. It includes instructions on how to adjust genotypes, usage of specific flags, and links to resources. While there are some technical details, it is written in a way that explains concepts clearly and provides guidance, making it human-readable."
Testability,"iant.input_rtg_output.txt; ```. As a result we should get the following output:. ```bash; Checking: /output/HG002_trio_merged.vcf.gz; Family: [HG003 + HG004] -> [HG002]; 222 non-pass records were skipped; Concordance HG002: F:166005/169476 (97.95%) M:166074/168579 (98.51%) F+M:159317/164363 (96.93%); Sample HG002 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 0/188247 (0.00%) records did not conform to expected call ploidy; 176481/188247 (93.75%) records were variant in at least 1 family member and checked for Mendelian constraints; 10169/176481 (5.76%) records had indeterminate consistency status due to incomplete calls; 6610/176481 (3.75%) records contained a violation of Mendelian constraints; ```. ### Benchmark variant calls against 4.2.1 truth set with hap.py. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/i",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:9440,benchmark,benchmark,9440,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,2,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: iant.input_rtg_output.txt; ```. As a result we should get the following output:. ```bash; Checking: /output/HG002_trio_merged.vcf.gz; Family: [HG003 + HG004] -> [HG002]; 222 non-pass records were skipped; Concordance HG002: F:166005/169476 (97.95%) M:166074/168579 (98.51%) F+M:159317/164363 (96.93%); Sample HG002 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 0/188247 (0.00%) records did not conform to expected call ploidy; 176481/188247 (93.75%) records were variant in at least 1 family member and checked for Mendelian constraints; 10169/176481 (5.76%) records had indeterminate consistency status due to incomplete calls; 6610/176481 (3.75%) records contained a violation of Mendelian constraints; ```. ### Benchmark variant calls against 4.2.1 truth set with hap.py. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/i
",True,"The text consists of command lines and output logs from a script execution, including Docker commands, file paths, and pipeline outputs. This is primarily log data rather than meaningful human-readable sentences."
Testability,"ibe applying DeepVariant to a Complete Genomics G400; sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics G400 HG002 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam > input/HG002.complete_g400.V350151728.grch38.chr20.bam. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam.bai > input/HG002.complete_g400.V350151728.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG002. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics G400 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.data-00000-of-00001 > input/weights-60-0.993753.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.index > input/weights-60-0.993753.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvar",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md:1065,benchmark,benchmark,1065,docs/deepvariant-complete-g400-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ibe applying DeepVariant to a Complete Genomics G400; sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics G400 HG002 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam > input/HG002.complete_g400.V350151728.grch38.chr20.bam. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam.bai > input/HG002.complete_g400.V350151728.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG002. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics G400 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.data-00000-of-00001 > input/weights-60-0.993753.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.index > input/weights-60-0.993753.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvar
",False,
Testability,"ified by the; `--ref` flag, instead of the path to the original reference in the CRAM file; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 30x whole genome sample in BAM and CRAM format. Filetype | Size (Gb) | Runtime (min); -------- | --------- | -------------; BAM | 66.99 | 79m47.37307s; CRAM | 37.85 | 96m53.477s; Ratio | 56.50% | 121.43%. * BAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" samtools; ```. and. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" bcftools; ```. You can read more about samtools and bcftools here: http://www.htslib.org/doc/. ## Commands for requesting machines used in case studies. We report runtime in our case studies documentation. In order to make sure the; results we report are reproducible without too much variation, we provide the; commands we used here to show you what kind of machines we ran the case studie",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:10318,test,testdata,10318,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ified by the; `--ref` flag, instead of the path to the original reference in the CRAM file; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 30x whole genome sample in BAM and CRAM format. Filetype | Size (Gb) | Runtime (min); -------- | --------- | -------------; BAM | 66.99 | 79m47.37307s; CRAM | 37.85 | 96m53.477s; Ratio | 56.50% | 121.43%. * BAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" samtools; ```. and. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" bcftools; ```. You can read more about samtools and bcftools here: http://www.htslib.org/doc/. ## Commands for requesting machines used in case studies. We report runtime in our case studies documentation. In order to make sure the; results we report are reproducible without too much variation, we provide the; commands we used here to show you what kind of machines we ran the case studie
",False,
Testability,"ing case study. In this case study, we describe applying DeepVariant to a real WGS sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.de",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:1045,benchmark,benchmarks,1045,docs/deepvariant-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md,1,['benchmark'],['benchmarks'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ing case study. In this case study, we describe applying DeepVariant to a real WGS sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.de
",False,"The text contains complete, meaningful sentences in natural language discussing concepts and steps in a case study."
Testability,"ique counted k-mers : 2753735220; Total no. of k-mers : 103092565745; Total no. of reads : 838385300; Total no. of super-k-mers : 9929565346. real 24m11.431s; user 142m37.817s; sys 8m14.566s; ```. Run `giraffe`` on the graph, haplotype index, kmers and reads:. ```bash; ${DATA_DIR}/vg paths \; -x ${DATA_DIR}/hprc-v1.1-mc-grch38.gbz \; -L -Q GRCh38 > ${DATA_DIR}/GRCh38.path_list.txt; ```. ```bash; time ${DATA_DIR}/vg giraffe --progress \; --read-group ""ID:1 LB:lib1 SM:HG003 PL:illumina PU:unit1"" \; --sample ""HG003"" \; -o BAM --ref-paths ${DATA_DIR}/GRCh38.path_list.txt \; -P -L 3000 \; -f ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R1.fastq.gz \; -f ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R2.fastq.gz \; -Z ${DATA_DIR}/hprc-v1.1-mc-grch38.gbz \; --kff-name ${DATA_DIR}/HG003.fq.kff \; --haplotype-name ${DATA_DIR}/hprc-v1.1-mc-grch38.hapl \; -t $(nproc) > reads.unsorted.bam; ```. NOTE: No need to sort this yet, because we'll need to sort it in the next step. ## Runtime. On my machine, the last few lines of the log showed:. ```; Mapped 838385300 reads across 64 threads in 14093.4 seconds with 3.25431 additional single-threaded seconds.; Mapping speed: 929.496 reads per second per thread; Used 896175 CPU-seconds (including output).; Achieved 935.515 reads per CPU-second (including output); Memory footprint: 61.0703 GB. real 283m10.368s; user 15260m35.845s; sys 214m57.882s; ```. File size:. ```; $ ls -lh reads.unsorted.bam; -rw-rw-r-- 1 pichuan pichuan 69G Nov 1 23:56 reads.unsorted.bam; ```. Then, clean up contig names, and sort:. ```bash; INBAM=reads.unsorted.bam; BAM=reads.sorted.chrfixed.bam; time samtools view -h $INBAM | sed -e ""s/GRCh38#0#//g"" | samtools sort --threads 10 -m 2G -O BAM > ${BAM}; # Index the BAM.; samtools index -@$(nproc) ${BAM}; ```. The step with `time` above took:. ```; real 73m19.172s; user 178m59.088s; sys 24m36.986s; ```. File size:. ```; $ ls -lh reads.sorted.chrfixed.bam; -rw-rw-r-- 1 pichuan pichuan 40G Nov 2 02:09 reads.sorted.chrfixed.bam; ```. #",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:3549,log,log,3549,docs/deepvariant-vg-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md,1,['log'],['log'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ique counted k-mers : 2753735220; Total no. of k-mers : 103092565745; Total no. of reads : 838385300; Total no. of super-k-mers : 9929565346. real 24m11.431s; user 142m37.817s; sys 8m14.566s; ```. Run `giraffe`` on the graph, haplotype index, kmers and reads:. ```bash; ${DATA_DIR}/vg paths \; -x ${DATA_DIR}/hprc-v1.1-mc-grch38.gbz \; -L -Q GRCh38 > ${DATA_DIR}/GRCh38.path_list.txt; ```. ```bash; time ${DATA_DIR}/vg giraffe --progress \; --read-group ""ID:1 LB:lib1 SM:HG003 PL:illumina PU:unit1"" \; --sample ""HG003"" \; -o BAM --ref-paths ${DATA_DIR}/GRCh38.path_list.txt \; -P -L 3000 \; -f ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R1.fastq.gz \; -f ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R2.fastq.gz \; -Z ${DATA_DIR}/hprc-v1.1-mc-grch38.gbz \; --kff-name ${DATA_DIR}/HG003.fq.kff \; --haplotype-name ${DATA_DIR}/hprc-v1.1-mc-grch38.hapl \; -t $(nproc) > reads.unsorted.bam; ```. NOTE: No need to sort this yet, because we'll need to sort it in the next step. ## Runtime. On my machine, the last few lines of the log showed:. ```; Mapped 838385300 reads across 64 threads in 14093.4 seconds with 3.25431 additional single-threaded seconds.; Mapping speed: 929.496 reads per second per thread; Used 896175 CPU-seconds (including output).; Achieved 935.515 reads per CPU-second (including output); Memory footprint: 61.0703 GB. real 283m10.368s; user 15260m35.845s; sys 214m57.882s; ```. File size:. ```; $ ls -lh reads.unsorted.bam; -rw-rw-r-- 1 pichuan pichuan 69G Nov 1 23:56 reads.unsorted.bam; ```. Then, clean up contig names, and sort:. ```bash; INBAM=reads.unsorted.bam; BAM=reads.sorted.chrfixed.bam; time samtools view -h $INBAM | sed -e ""s/GRCh38#0#//g"" | samtools sort --threads 10 -m 2G -O BAM > ${BAM}; # Index the BAM.; samtools index -@$(nproc) ${BAM}; ```. The step with `time` above took:. ```; real 73m19.172s; user 178m59.088s; sys 24m36.986s; ```. File size:. ```; $ ls -lh reads.sorted.chrfixed.bam; -rw-rw-r-- 1 pichuan pichuan 40G Nov 2 02:09 reads.sorted.chrfixed.bam; ```. #
",False,"The text contains a mixture of natural language explanations and command lines. While there are code snippets, the majority of the content includes descriptive text about the process, runtime details, and file sizes which are meaningful for humans. The presence of commands does not dominate the text, and it's supplemented with enough explanatory information to be considered human-readable."
Testability,"ithub.com/illumina/hap.py) - Used to evaluate the results.; We will use Docker to run `hap.py`. ### Data. We will use these data in our analysis. Files will be downloaded in subsequent; steps. * HG005 RNA-seq BAM; * Model Checkpoint Files; * GRCh38 Reference + Index; * CDS bedfile (chr20 only); * GIAB benchmark data. ## Prepare Data. ### Setup directories. Lets first create directories to organize files. ```bash; mkdir -p data benchmark reference model output happy; ```. ### Download the GRCh38 Reference. We will be using GRCh38 for this case study. ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_h",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:1450,benchmark,benchmark,1450,docs/deepvariant-rnaseq-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ithub.com/illumina/hap.py) - Used to evaluate the results.; We will use Docker to run `hap.py`. ### Data. We will use these data in our analysis. Files will be downloaded in subsequent; steps. * HG005 RNA-seq BAM; * Model Checkpoint Files; * GRCh38 Reference + Index; * CDS bedfile (chr20 only); * GIAB benchmark data. ## Prepare Data. ### Setup directories. Lets first create directories to organize files. ```bash; mkdir -p data benchmark reference model output happy; ```. ### Download the GRCh38 Reference. We will be using GRCh38 for this case study. ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_h
",True,"The content primarily consists of code snippets, such as bash commands for downloading and processing data, which are typical of programmatic instructions rather than human-readable prose. The text includes directory setup, data download steps using curl and ftp URLs, and specific file extraction commands, all of which are indicative of automation or script usage. Additionally, the presence of code formatting (e.g., `bash;` followed by command lines) further supports this categorization as primarily technical in nature rather than explanatory or descriptive content meant for human reading."
Testability,"ity of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics T7 HG001 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG001. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics T7 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.data-00000-of-00001 > input/weights-51-0.995354.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.index > input/weights-51-0.995354.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG001.complete_t7.E100030471QC960.",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md:1146,benchmark,benchmark,1146,docs/deepvariant-complete-t7-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ity of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics T7 HG001 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG001. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics T7 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.data-00000-of-00001 > input/weights-51-0.995354.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.index > input/weights-51-0.995354.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG001.complete_t7.E100030471QC960.
",False,"The text contains multiple bash commands and script snippets, which are code examples. However, it also includes explanatory sentences such as 'To make it faster to run over this case study, we run only on chromosome 20' and 'For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md).', which are meaningful and in natural language. The text thus contains a mix of code and explanatory content."
Testability,"l ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:1834,benchmark,benchmark,1834,docs/deeptrio-wgs-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: l ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2
",False,
Testability,"l ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 PacBio HiFi WGS reads publicly available from the; [PrecisionFDA Truth",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:1903,benchmark,benchmark,1903,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: l ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 PacBio HiFi WGS reads publicly available from the; [PrecisionFDA Truth
",False,"The content includes commands for downloading data from FTP servers and setting up benchmarking files. While this may involve technical steps, it is not purely code or logs. It contains explanatory text about benchmarking Genome in a Bottle variant calls and instructions that are more operational in nature but not strictly code snippets or logs."
Testability,"ll 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.ch",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:1851,test,test,1851,docs/deepvariant-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md,1,['test'],['test'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ll 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.ch
",False,"The text contains step-by-step instructions and explanations which are meaningful sentences in natural language. It includes human-readable content discussing the process of using Docker to compile binaries, downloading test data, and preparing input files for analysis."
Testability,"ll be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.v",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:1584,benchmark,benchmark,1584,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,2,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ll be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.v
",False,"The text contains complete sentences explaining the use of Genome in a Bottle benchmarks for variant calling, including directory setups and data downloading commands."
Testability,"lumina/hap.py) for benchmarking. If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/XY-walkthrough"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}/data"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/xy-case-study-testdata; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai. HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata; curl ${HTTPDIR}/GRCh38_PAR.bed > ${INPUT_DIR}/GRCh38_PAR.bed. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG002.pfda_challenge.grch38.chrXY.bam""; THREADS=$(nproc); REGION=""chrX chrY""; HAPLOID_CONTIGS=""chrX,chrY""; PAR_BED=""GRCh38_PAR.bed"". # Set up output variable; OUTPUT_VCF=""HG002_pacbio_hifi.chrXY.output.vcf.gz""; OUTPUT_GVCF=""HG002_pacbio_hifi.chrXY.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". sudo docker ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md:1680,test,testdata,1680,docs/deepvariant-xy-calling-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: lumina/hap.py) for benchmarking. If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/XY-walkthrough"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}/data"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/xy-case-study-testdata; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai. HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata; curl ${HTTPDIR}/GRCh38_PAR.bed > ${INPUT_DIR}/GRCh38_PAR.bed. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG002.pfda_challenge.grch38.chrXY.bam""; THREADS=$(nproc); REGION=""chrX chrY""; HAPLOID_CONTIGS=""chrX,chrY""; PAR_BED=""GRCh38_PAR.bed"". # Set up output variable; OUTPUT_VCF=""HG002_pacbio_hifi.chrXY.output.vcf.gz""; OUTPUT_GVCF=""HG002_pacbio_hifi.chrXY.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". sudo docker 
",False,"The text contains a mix of code snippets, commands, and setup instructions that are necessary for executing a workflow. While there is some procedural content, it includes direct human-readable explanations interspersed with specific commands, which together provide a comprehensive guide for setting up the environment and running the application. The text explains why one might choose Singularity over Docker for GPU usage and provides step-by-step instructions on directory setup and file downloads. It also outlines variable assignments and necessary intermediate directories, all of which are useful for someone executing this workflow. However, the presence of code blocks (like bash commands) might make it feel more technical, but the overall content serves to inform and guide users without being purely programmatic or log-like. There is an informative tone aimed at human readers trying to understand how to run DeepVariant."
Testability,"mark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:2637,benchmark,benchmark,2637,docs/deeptrio-wgs-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: mark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input
",True,"The text consists primarily of code snippets and command lines, including 'curl' commands and file transfers. There are no meaningful human-readable sentences or explanatory content."
Testability,"mark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 PacBio HiFi WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10).; These reads have been aligned to the GRCh38_no_alt_analysis reference using; [pbmm2](https://github.com/PacificBiosciences/pbmm2). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/pacbio-case-study-testdata. curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam > input/HG002.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG002.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam > input/HG003.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG003.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG004.pfda_challen",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:2706,benchmark,benchmark,2706,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: mark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 PacBio HiFi WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10).; These reads have been aligned to the GRCh38_no_alt_analysis reference using; [pbmm2](https://github.com/PacificBiosciences/pbmm2). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/pacbio-case-study-testdata. curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam > input/HG002.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG002.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam > input/HG003.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG003.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG004.pfda_challen
",True,"The provided text is a script with commands and file operations, which includes code snippets, curl commands, directory creation, and file downloads. This content does not contain meaningful human-readable sentences or explanatory prose; instead, it consists primarily of programmatic actions intended for execution."
Testability,"mark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11208 48 21239 13 9586 7 4 0.995736 0.998884 0.451340 0.997308 NaN NaN 1.561710 2.047281; INDEL PASS 11256 11208 48 21239 13 9586 7 4 0.995736 0.998884 0.451340 0.997308 NaN NaN 1.561710 2.047281; SNP ALL 71333 71087 246 88976 42 17795 5 4 0.996551 0.999410 0.199998 0.997979 2.314904 2.029984 1.715978 1.716560; SNP PASS 71333 71087 246 88976 42 17795 5 4 0.996551 0.999410 0.199998 0.997979 2.314904 2.029984 1.715978 1.716560. Benchmarking Summary for HG003:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL Q",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:10486,benchmark,benchmark,10486,docs/deeptrio-wgs-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: mark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11208 48 21239 13 9586 7 4 0.995736 0.998884 0.451340 0.997308 NaN NaN 1.561710 2.047281; INDEL PASS 11256 11208 48 21239 13 9586 7 4 0.995736 0.998884 0.451340 0.997308 NaN NaN 1.561710 2.047281; SNP ALL 71333 71087 246 88976 42 17795 5 4 0.996551 0.999410 0.199998 0.997979 2.314904 2.029984 1.715978 1.716560; SNP PASS 71333 71087 246 88976 42 17795 5 4 0.996551 0.999410 0.199998 0.997979 2.314904 2.029984 1.715978 1.716560. Benchmarking Summary for HG003:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL Q
",False,"The content contains natural language sentences explaining benchmark results and metrics, such as 'Benchmarking Summary for HG002' followed by detailed tables of metrics. These are meaningful human-readable sentences."
Testability,"mark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11215 41 23348 85 11580 30 50 0.996357 0.992777 0.495974 0.994564 NaN NaN 1.561710 2.133416; INDEL PASS 11256 11215 41 23348 85 11580 30 50 0.996357 0.992777 0.495974 0.994564 NaN NaN 1.561710 2.133416; SNP ALL 71333 71303 30 108157 20 36757 16 4 0.999579 0.999720 0.339849 0.999650 2.314904 1.745105 1.715978 1.773270; SNP PASS 71333 71303 30 108157 20 36757 16 4 0.999579 0.999720 0.339849 0.999650 2.314904 1.745105 1.715978 1.773270. Benchmarking Summary for HG003:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:10767,benchmark,benchmark,10767,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: mark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11215 41 23348 85 11580 30 50 0.996357 0.992777 0.495974 0.994564 NaN NaN 1.561710 2.133416; INDEL PASS 11256 11215 41 23348 85 11580 30 50 0.996357 0.992777 0.495974 0.994564 NaN NaN 1.561710 2.133416; SNP ALL 71333 71303 30 108157 20 36757 16 4 0.999579 0.999720 0.339849 0.999650 2.314904 1.745105 1.715978 1.773270; SNP PASS 71333 71303 30 108157 20 36757 16 4 0.999579 0.999720 0.339849 0.999650 2.314904 1.745105 1.715978 1.773270. Benchmarking Summary for HG003:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY
",False,"The text contains benchmark summaries with meaningful data, including statistics and explanations of metrics. It is written in natural language and provides insights into performance."
Testability,"mats; (like SAM and VCF) designed for painless integration with the; [TensorFlow](https://www.tensorflow.org/) machine learning framework. Nucleus; was built with DeepVariant in mind and open-sourced separately so it can be used; by anyone in the genomics research community for other projects. See this blog; post on; [Using Nucleus and TensorFlow for DNA Sequencing Error Correction](https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/). ## DeepVariant Setup. ### Prerequisites. * Unix-like operating system (cannot run on Windows); * Python 3.8. ### Official Solutions. Below are the official solutions provided by the; [Genomics team in Google Health](https://health.google/health-research/). Name | Description; :-------------------------------------------------------------------------------------------------: | -----------; [Docker](docs/deepvariant-quick-start.md) | This is the recommended method.; [Build from source](docs/deepvariant-build-test.md) | DeepVariant comes with scripts to build it on Ubuntu 20.04. To build and run on other Unix-based systems, you will need to modify these scripts.; Prebuilt Binaries | Available at [`gs://deepvariant/`](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you will need a CPU (such as Intel Sandy Bridge) that supports them. You can check the `/proc/cpuinfo` file on your computer, which lists these features under ""flags"". ## Contribution Guidelines. Please [open a pull request](https://github.com/google/deepvariant/compare) if; you wish to contribute to DeepVariant. Note, we have not set up the; infrastructure to merge pull requests externally. If you agree, we will test and; submit the changes internally and mention your contributions in our; [release notes](https://github.com/google/deepvariant/releases). We apologize; for any inconvenience. If you have any difficulty using DeepVariant, feel ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:10453,test,test,10453,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['test'],['test'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: mats; (like SAM and VCF) designed for painless integration with the; [TensorFlow](https://www.tensorflow.org/) machine learning framework. Nucleus; was built with DeepVariant in mind and open-sourced separately so it can be used; by anyone in the genomics research community for other projects. See this blog; post on; [Using Nucleus and TensorFlow for DNA Sequencing Error Correction](https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/). ## DeepVariant Setup. ### Prerequisites. * Unix-like operating system (cannot run on Windows); * Python 3.8. ### Official Solutions. Below are the official solutions provided by the; [Genomics team in Google Health](https://health.google/health-research/). Name | Description; :-------------------------------------------------------------------------------------------------: | -----------; [Docker](docs/deepvariant-quick-start.md) | This is the recommended method.; [Build from source](docs/deepvariant-build-test.md) | DeepVariant comes with scripts to build it on Ubuntu 20.04. To build and run on other Unix-based systems, you will need to modify these scripts.; Prebuilt Binaries | Available at [`gs://deepvariant/`](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you will need a CPU (such as Intel Sandy Bridge) that supports them. You can check the `/proc/cpuinfo` file on your computer, which lists these features under ""flags"". ## Contribution Guidelines. Please [open a pull request](https://github.com/google/deepvariant/compare) if; you wish to contribute to DeepVariant. Note, we have not set up the; infrastructure to merge pull requests externally. If you agree, we will test and; submit the changes internally and mention your contributions in our; [release notes](https://github.com/google/deepvariant/releases). We apologize; for any inconvenience. If you have any difficulty using DeepVariant, feel 
",True,"The text consists primarily of programmatic API documentation and setup instructions which include terms like 'prerequisites', 'official solutions', 'docker', 'build from source', etc., along with parameter lists and descriptions. This content is typically found in API docs or setup guides rather than human-readable narratives."
Testability,ment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HT,MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:1707,benchmark,benchmark,1707,docs/deepvariant-pacbio-model-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HT
",False,"The text contains a mixture of bash scripts and descriptive paragraphs. The bash code is relevant for setting up the environment and downloading necessary data for the case study, which provides operational steps rather than mere logs or code snippets. Additionally, the text includes explanatory content about what is being done, such as 'We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle' and instructions on how to download references, which are meaningful sentences aiding in understanding the process. Therefore, while there are code snippets present, they are accompanied by enough descriptive text to be considered human-readable."
Testability,"min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; --output_gvcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.g.vcf.gz \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --num_shards=$(nproc); ```. Stage | Time (minutes); -------------------------------- | -----------------; make_examples | 116m37.385s; call_variants | 214m37.055s; postprocess_variants (with gVCF) | 30m59.968s. ### Run hap.py. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; ${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; -f ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -o ${PWD}/happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 504501 502199 2302 960061 1526 434935 906 371 0.995437 0.997094 0.453029 0.996265 NaN NaN 1.",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:6644,benchmark,benchmark,6644,docs/deepvariant-vg-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; --output_gvcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.g.vcf.gz \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --num_shards=$(nproc); ```. Stage | Time (minutes); -------------------------------- | -----------------; make_examples | 116m37.385s; call_variants | 214m37.055s; postprocess_variants (with gVCF) | 30m59.968s. ### Run hap.py. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; ${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; -f ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -o ${PWD}/happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 504501 502199 2302 960061 1526 434935 906 371 0.995437 0.997094 0.453029 0.996265 NaN NaN 1.
",False,
Testability,"nchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai; ```. ## Running DeepVariant. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can run DeepVariant with just one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. Here we specify `--regions chr20` to run on just chromosome 20, saving time so; you can run this case study within about half an hour (tested on 64 CPUs). ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ""HYBRID_PACBIO_ILLUMINA"" \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir; ```. By specifying `--model_type HYBRID_PACBIO_ILLUMINA`, you'll be using a model; that is best suited for (and trained on) the combination of PacBio Hifi long; reads and Illumina short reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:4591,test,tested,4591,docs/deepvariant-hybrid-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md,1,['test'],['tested'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: nchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai; ```. ## Running DeepVariant. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can run DeepVariant with just one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. Here we specify `--regions chr20` to run on just chromosome 20, saving time so; you can run this case study within about half an hour (tested on 64 CPUs). ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ""HYBRID_PACBIO_ILLUMINA"" \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir; ```. By specifying `--model_type HYBRID_PACBIO_ILLUMINA`, you'll be using a model; that is best suited for (and trained on) the combination of PacBio Hifi long; reads and Illumina short reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to
",False,
Testability,nd samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG0,MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:1791,benchmark,benchmarks,1791,docs/deepvariant-pacbio-model-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md,1,['benchmark'],['benchmarks'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: nd samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG0
",False,"The text contains multiple bash code snippets, which are code samples and should be filtered out. However, the text also includes meaningful sentences such as 'We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle' that provide context and explanation. The majority of the content is code-based but there are enough natural language explanations to warrant retention."
Testability,"ng case study. In this case study, we describe applying DeepTrio to a real WGS trio. Then we; assess the quality of the DeepTrio variant calls with `hap.py`. In addition we; evaluate a mendelian violation rate for a merged VCF. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:1017,benchmark,benchmark,1017,docs/deeptrio-wgs-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ng case study. In this case study, we describe applying DeepTrio to a real WGS trio. Then we; assess the quality of the DeepTrio variant calls with `hap.py`. In addition we; evaluate a mendelian violation rate for a merged VCF. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_
",True,"The text consists primarily of code snippets and command lines (e.g., `curl`, `gunzip`, `mkdir -p`), which are typical indicators of programmatic content that should be filtered out."
Testability,"nment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics G400 HG002 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam > input/HG002.complete_g400.V350151728.grch38.chr20.bam. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam.bai > input/HG002.complete_g400.V350151728.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG002. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics G400 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.data-00000-of-00001 > input/weights-60-0.993753.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.index > input/weights-60-0.993753.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG002.complete_g400.V350151728.grch38.chr20.bam \; --output_vcf /output/HG002.output.vcf.gz \; --output_gvcf /output/HG002.output.g.vcf.gz \; --",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md:1305,benchmark,benchmark,1305,docs/deepvariant-complete-g400-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: nment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics G400 HG002 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam > input/HG002.complete_g400.V350151728.grch38.chr20.bam. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam.bai > input/HG002.complete_g400.V350151728.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG002. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics G400 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.data-00000-of-00001 > input/weights-60-0.993753.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.index > input/weights-60-0.993753.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG002.complete_g400.V350151728.grch38.chr20.bam \; --output_vcf /output/HG002.output.vcf.gz \; --output_gvcf /output/HG002.output.g.vcf.gz \; --
",False,
Testability,"nt, for the latest results. You can also see the [Colab example] to see how you can visualize the pileup; images. ## Training data over time. For the models we've released over time, you can find more details about the; training data in; [DeepVariant training data](deepvariant-details-training-data.md). ## CRAM support. As of v0.7, DeepVariant accepts CRAM files as input in addition to BAM files. As of v0.9.0, we changed the default to use the reference file specified by the; `--ref` flag, instead of the path to the original reference in the CRAM file; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 30x whole genome sample in BAM and CRAM format. Filetype | Size (Gb) | Runtime (min); -------- | --------- | -------------; BAM | 66.99 | 79m47.37307s; CRAM | 37.85 | 96m53.477s; Ratio | 56.50% | 121.43%. * BAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}""",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:9844,benchmark,benchmarks,9844,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['benchmark'],['benchmarks'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: nt, for the latest results. You can also see the [Colab example] to see how you can visualize the pileup; images. ## Training data over time. For the models we've released over time, you can find more details about the; training data in; [DeepVariant training data](deepvariant-details-training-data.md). ## CRAM support. As of v0.7, DeepVariant accepts CRAM files as input in addition to BAM files. As of v0.9.0, we changed the default to use the reference file specified by the; `--ref` flag, instead of the path to the original reference in the CRAM file; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 30x whole genome sample in BAM and CRAM format. Filetype | Size (Gb) | Runtime (min); -------- | --------- | -------------; BAM | 66.99 | 79m47.37307s; CRAM | 37.85 | 96m53.477s; Ratio | 56.50% | 121.43%. * BAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}""
",False,"The text contains meaningful sentences discussing the features of DeepVariant, including CRAM support updates, performance benchmarks between BAM and CRAM formats, and references to specific commands and tools. It also provides configuration details for running jobs in Google Cloud, which is explanatory content useful for users. There are no code snippets or logs present. The text includes links to documentation, which is a common practice in human-readable explanations."
Testability,"nter_behavior=true,normalize_reads=true"" \; --num_shards=$(nproc); ```. Stage | Time (minutes); -------------------------------- | -----------------; make_examples | 116m37.385s; call_variants | 214m37.055s; postprocess_variants (with gVCF) | 30m59.968s. ### Run hap.py. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; ${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; -f ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -o ${PWD}/happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 504501 502199 2302 960061 1526 434935 906 371 0.995437 0.997094 0.453029 0.996265 NaN NaN 1.489759 1.952023; INDEL PASS 504501 502199 2302 960061 1526 434935 906 371 0.995437 0.997094 0.453029 0.996265 NaN NaN 1.489759 1.952023; SNP ALL 3327496 3316515 10981 3858659 5550 534709 2104 475 0.996700 0.998330 0.138574 0.997514 2.102576 1.970783 1.535137 1.436",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:6912,benchmark,benchmark,6912,docs/deepvariant-vg-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: nter_behavior=true,normalize_reads=true"" \; --num_shards=$(nproc); ```. Stage | Time (minutes); -------------------------------- | -----------------; make_examples | 116m37.385s; call_variants | 214m37.055s; postprocess_variants (with gVCF) | 30m59.968s. ### Run hap.py. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; ${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; -f ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -o ${PWD}/happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 504501 502199 2302 960061 1526 434935 906 371 0.995437 0.997094 0.453029 0.996265 NaN NaN 1.489759 1.952023; INDEL PASS 504501 502199 2302 960061 1526 434935 906 371 0.995437 0.997094 0.453029 0.996265 NaN NaN 1.489759 1.952023; SNP ALL 3327496 3316515 10981 3858659 5550 534709 2104 475 0.996700 0.998330 0.138574 0.997514 2.102576 1.970783 1.535137 1.436
",False,
Testability,"o variant calls with `hap.py`. In addition we; evaluate a mendelian violation rate for a merged VCF. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmar",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:1164,benchmark,benchmark,1164,docs/deeptrio-wgs-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: o variant calls with `hap.py`. In addition we; evaluate a mendelian violation rate for a merged VCF. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmar
",True,"The content consists primarily of code snippets and shell commands, including curl commands for downloading files, directory structures (mkdir -p), and bash script syntax. There are also file paths and URLs which are indicative of programmatic operations rather than meaningful human-readable text. The text describes a procedure for preparing an environment and downloading reference data, which is more aligned with system administration or programming tasks rather than explanatory content."
Testability,"ols. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noin",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:1424,benchmark,benchmark,1424,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,2,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ols. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noin
",True,"The provided text consists primarily of command line instructions for downloading and processing reference genomes and benchmarks, which is more appropriate for a programmatic context rather than human-readable content."
Testability,"otify the user; of the error. ### `postprocess_variants`. When run in gVCF mode, the `postprocess_variants` program handles the creation; of the final gVCF file that incorporates both the non-variant records and the; true variants discovered by the previous programs. Two additional flags are required in `postprocess_variants`, the input; `--nonvariant_site_tfrecord_path <filename>` which corresponds to the TFRecord; of Variant protocol buffers created in `make_examples`, and the output; `--gvcf_outfile <filename>` which is the final gVCF output. A concrete example call, using variables defined in the [WGS case study] and in; the above `make_examples` example:. ```bash; OUTPUT_GVCF=""${OUTPUT_DIR}/HG002.output.g.vcf.gz"". ( time python ""${BIN_DIR}""/postprocess_variants.zip \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${OUTPUT_VCF}"" \; --nonvariant_site_tfrecord_path ""${GVCF_TFRECORDS}"" \; --gvcf_outfile ""${OUTPUT_GVCF}""; ) >""${LOG_DIR}/postprocess_variants.log"" 2>&1; ```. ## Storage and runtime considerations. The number of non-variant records created when running DeepVariant in gVCF; depends highly on the sequencing depth of the input sample. This is because the; gVCF records at adjacent sites are merged when the genotype qualities are equal,; and we limit the possible genotype quality seen to be at most 50. For; deeply-sequenced individuals (e.g. 30-50x coverage), many sites hit the GQ=50; cap and are merged into few records. Samples with lower sequencing depth have; more sites within the dynamic range of the binomial model used to estimate; non-variant site genotype quality, and thus more records are created. To mitigate this effect, the `make_examples` program has a flag; `--gvcf_gq_binsize <int>`. This flag allows the merging of adjacent records that; all have GQ values within a bin of the given size, and for each record emits the; minimum GQ value seen within the bin. For example, setting `--gvcf_gq_binsize 5` has the effect that adjacent ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:3951,log,log,3951,docs/deepvariant-gvcf-support.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md,1,['log'],['log'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: otify the user; of the error. ### `postprocess_variants`. When run in gVCF mode, the `postprocess_variants` program handles the creation; of the final gVCF file that incorporates both the non-variant records and the; true variants discovered by the previous programs. Two additional flags are required in `postprocess_variants`, the input; `--nonvariant_site_tfrecord_path <filename>` which corresponds to the TFRecord; of Variant protocol buffers created in `make_examples`, and the output; `--gvcf_outfile <filename>` which is the final gVCF output. A concrete example call, using variables defined in the [WGS case study] and in; the above `make_examples` example:. ```bash; OUTPUT_GVCF=""${OUTPUT_DIR}/HG002.output.g.vcf.gz"". ( time python ""${BIN_DIR}""/postprocess_variants.zip \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${OUTPUT_VCF}"" \; --nonvariant_site_tfrecord_path ""${GVCF_TFRECORDS}"" \; --gvcf_outfile ""${OUTPUT_GVCF}""; ) >""${LOG_DIR}/postprocess_variants.log"" 2>&1; ```. ## Storage and runtime considerations. The number of non-variant records created when running DeepVariant in gVCF; depends highly on the sequencing depth of the input sample. This is because the; gVCF records at adjacent sites are merged when the genotype qualities are equal,; and we limit the possible genotype quality seen to be at most 50. For; deeply-sequenced individuals (e.g. 30-50x coverage), many sites hit the GQ=50; cap and are merged into few records. Samples with lower sequencing depth have; more sites within the dynamic range of the binomial model used to estimate; non-variant site genotype quality, and thus more records are created. To mitigate this effect, the `make_examples` program has a flag; `--gvcf_gq_binsize <int>`. This flag allows the merging of adjacent records that; all have GQ values within a bin of the given size, and for each record emits the; minimum GQ value seen within the bin. For example, setting `--gvcf_gq_binsize 5` has the effect that adjacent 
",False,"The text provided includes a bash command with code syntax and detailed explanations about how DeepVariant processes gVCF files, which might be more technical than general prose. However, it also contains natural language explanations about storage and runtime considerations in the form of sentences like 'This is because the gVCF records at adjacent sites are merged...' which are meaningful and not purely code or logs."
Testability,"ownload the GRCh38 Reference. We will be using GRCh38 for this case study. ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. # Extract chr20 CDS regions and convert to bed file.; gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \; awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \; awk '!dup[$0]++' > data/chr20_CDS.bed; ```. ### Download HG005 BAM. We'll use HG005 poly-A selected Illumina RNA-seq reads that are publicly; available. ```bash; HTTPDIR=https://stora",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:1934,benchmark,benchmark,1934,docs/deepvariant-rnaseq-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ownload the GRCh38 Reference. We will be using GRCh38 for this case study. ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. # Extract chr20 CDS regions and convert to bed file.; gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \; awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \; awk '!dup[$0]++' > data/chr20_CDS.bed; ```. ### Download HG005 BAM. We'll use HG005 poly-A selected Illumina RNA-seq reads that are publicly; available. ```bash; HTTPDIR=https://stora
",False,"The text provided contains multiple bash code snippets, which are programmatic instructions. However, it also includes explanatory sentences discussing the downloading and processing of genomic data for a case study. The content is a mix of human-readable explanations and code logs. Despite the presence of code, the primary content is informative and intended to guide the user through a process, rather than solely consisting of logs or API documentation."
Testability,"p input and output directory data; INPUT_DIR=""${BASE}/input""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}/data"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/xy-case-study-testdata; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai. HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata; curl ${HTTPDIR}/GRCh38_PAR.bed > ${INPUT_DIR}/GRCh38_PAR.bed. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG002.pfda_challenge.grch38.chrXY.bam""; THREADS=$(nproc); REGION=""chrX chrY""; HAPLOID_CONTIGS=""chrX,chrY""; PAR_BED=""GRCh38_PAR.bed"". # Set up output variable; OUTPUT_VCF=""HG002_pacbio_hifi.chrXY.output.vcf.gz""; OUTPUT_GVCF=""HG002_pacbio_hifi.chrXY.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --outpu",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md:1976,test,testdata,1976,docs/deepvariant-xy-calling-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: p input and output directory data; INPUT_DIR=""${BASE}/input""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}/data"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/xy-case-study-testdata; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai. HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata; curl ${HTTPDIR}/GRCh38_PAR.bed > ${INPUT_DIR}/GRCh38_PAR.bed. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG002.pfda_challenge.grch38.chrXY.bam""; THREADS=$(nproc); REGION=""chrX chrY""; HAPLOID_CONTIGS=""chrX,chrY""; PAR_BED=""GRCh38_PAR.bed"". # Set up output variable; OUTPUT_VCF=""HG002_pacbio_hifi.chrXY.output.vcf.gz""; OUTPUT_GVCF=""HG002_pacbio_hifi.chrXY.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --outpu
",True,"The text consists primarily of code snippets and commands, including shell script syntax, variable assignments, file operations, and API calls. These elements are typical of programmatic instructions rather than meaningful human-readable content."
Testability,"pe | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 503014 | 1487 | 2767 | 0.997053 | 0.994781 | 0.995916 |; | SNP | 3323624 | 3871 | 2273 | 0.998837 | 0.999317 | 0.999077 |. [See VCF stats report.](https://storage.googleapis.com/deepvariant/visual_reports/DeepVariant/1.6.1/HYBRID/deepvariant.output.visual_report.html). ## Inspect outputs that produced the metrics above. The DeepVariant VCFs, gVCFs, and hap.py evaluation outputs are available at:. ```; gs://deepvariant/case-study-outputs; ```. You can also inspect them in a web browser here:; https://42basepairs.com/browse/gs/deepvariant/case-study-outputs. ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; # Get the script.; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deepvariant.sh. # WGS; bash inference_deepvariant.sh --model_preset WGS. # WES; bash inference_deepvariant.sh --model_preset WES. # PacBio; bash inference_deepvariant.sh --model_preset PACBIO. # ONT_R104; bash inference_deepvariant.sh --model_preset ONT_R104. # Hybrid; bash inference_deepvariant.sh --model_preset HYBRID_PACBIO_ILLUMINA; ```. Runtime metrics are taken from the resulting log after each stage of; DeepVariant. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 runs produced the same output. [CPU instance with 64 CPUs]: deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md:5023,log,log,5023,docs/metrics.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md,2,['log'],['log'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: pe | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 503014 | 1487 | 2767 | 0.997053 | 0.994781 | 0.995916 |; | SNP | 3323624 | 3871 | 2273 | 0.998837 | 0.999317 | 0.999077 |. [See VCF stats report.](https://storage.googleapis.com/deepvariant/visual_reports/DeepVariant/1.6.1/HYBRID/deepvariant.output.visual_report.html). ## Inspect outputs that produced the metrics above. The DeepVariant VCFs, gVCFs, and hap.py evaluation outputs are available at:. ```; gs://deepvariant/case-study-outputs; ```. You can also inspect them in a web browser here:; https://42basepairs.com/browse/gs/deepvariant/case-study-outputs. ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; # Get the script.; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deepvariant.sh. # WGS; bash inference_deepvariant.sh --model_preset WGS. # WES; bash inference_deepvariant.sh --model_preset WES. # PacBio; bash inference_deepvariant.sh --model_preset PACBIO. # ONT_R104; bash inference_deepvariant.sh --model_preset ONT_R104. # Hybrid; bash inference_deepvariant.sh --model_preset HYBRID_PACBIO_ILLUMINA; ```. Runtime metrics are taken from the resulting log after each stage of; DeepVariant. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 runs produced the same output. [CPU instance with 64 CPUs]: deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. 
",False,"The text contains detailed explanations about metrics, their sources, and instructions on how to reproduce them. It includes natural language sentences discussing the data processing steps and tools used."
Testability,"pecifying `--model_type ONT_R104`, you'll be using a model that is best; suited for Oxford Nanopore R10.4.1 chemistry Simplex and Duplex reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true` to; the command above to figure out what flags you need in each step. Based on the; different model types, different flags are needed in the `make_examples` step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. ## Benchmark HG002 chr20 output from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002. ```bash; FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. TRUTH_VCF=""HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz""; TRUTH_BED=""HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${INPUT_DIR}/${REF}",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md:4314,benchmark,benchmarks,4314,docs/deepvariant-ont-r104-duplex-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md,1,['benchmark'],['benchmarks'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: pecifying `--model_type ONT_R104`, you'll be using a model that is best; suited for Oxford Nanopore R10.4.1 chemistry Simplex and Duplex reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true` to; the command above to figure out what flags you need in each step. Based on the; different model types, different flags are needed in the `make_examples` step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. ## Benchmark HG002 chr20 output from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002. ```bash; FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. TRUTH_VCF=""HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz""; TRUTH_BED=""HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${INPUT_DIR}/${REF}
",False,"The text contains complete, meaningful sentences discussing concepts and providing instructions for using specific tools. It includes explanatory content about model selection and benchmarking processes."
Testability,"rectory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. To see the pileup images visually, check out [show_examples](show-examples.md). For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). Just make sure to use `--model_type; HYBRID_PACBIO_ILLUMINA` when running on combined PacBio and Illumina data. ## Benchmark with hap.py. See [hap.py](https://github.com/illumina/hap.py) documentation for more details; on the parameters and outputs. ```bash; mkdir -p happy. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 10628 10602 26 23385 63 12212 10 51 0.997554 0.994361 0.522215 0.995955 NaN NaN 1.748961 2.721448; INDEL PASS 10628 10602 26 23385 63 12212 10 51 0.997554 0.994361 0.522215 0.995955 NaN NaN 1.748961 2.721448; SNP ALL 70166 70138 28 105564 43 35354 16 16 0.999601 0.999388 0.334906 0.999494 2.296566 1.812971 1.883951 2.187440; SNP PASS 70166 70138 28 105564 43 35354 16 16 0.999601 0.999388 0.334906 0.999494 2.296566 1.812971 1.883951 2.187440; ```. Notice that F1 scores are above 0.999 for SNPs and above 0.995 for indels!",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:6844,benchmark,benchmark,6844,docs/deepvariant-hybrid-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: rectory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. To see the pileup images visually, check out [show_examples](show-examples.md). For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). Just make sure to use `--model_type; HYBRID_PACBIO_ILLUMINA` when running on combined PacBio and Illumina data. ## Benchmark with hap.py. See [hap.py](https://github.com/illumina/hap.py) documentation for more details; on the parameters and outputs. ```bash; mkdir -p happy. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 10628 10602 26 23385 63 12212 10 51 0.997554 0.994361 0.522215 0.995955 NaN NaN 1.748961 2.721448; INDEL PASS 10628 10602 26 23385 63 12212 10 51 0.997554 0.994361 0.522215 0.995955 NaN NaN 1.748961 2.721448; SNP ALL 70166 70138 28 105564 43 35354 16 16 0.999601 0.999388 0.334906 0.999494 2.296566 1.812971 1.883951 2.187440; SNP PASS 70166 70138 28 105564 43 35354 16 16 0.999601 0.999388 0.334906 0.999494 2.296566 1.812971 1.883951 2.187440; ```. Notice that F1 scores are above 0.999 for SNPs and above 0.995 for indels!
",False,"The text includes benchmark results, which are complete and meaningful sentences describing performance metrics. The content is human-readable, providing insights into the system's effectiveness through detailed statistics."
Testability,"rio. ## Background. To get started, we've provided a Docker image, and some test data in a bucket on; Google Cloud Storage. The instructions below show how to download the data; through the corresponding public URLs. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepTrio in one command. Although DeepTrio can be built from a source, we provide a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:1282,test,test,1282,docs/deeptrio-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md,1,['test'],['test'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: rio. ## Background. To get started, we've provided a Docker image, and some test data in a bucket on; Google Cloud Storage. The instructions below show how to download the data; through the corresponding public URLs. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepTrio in one command. Although DeepTrio can be built from a source, we provide a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_
",True,"The content consists primarily of code snippets and programmatic instructions, including shell commands and Docker setup, which are typically indicative of technical documentation or build instructions rather than meaningful human-readable sentences."
Testability,"rk.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11208 48 21239 13 9586 7 4 0.995736 0.998884 0.451340 0.997308 NaN NaN 1.561710 2.047281; INDEL PASS 11256 11208 48 21239 13 9586 7 4 0.995736 0.998884 0.451340 0.997308 NaN NaN 1.561710 2.047281; SNP ALL 71333 71087 246 88976 42 17795 5 4 0.996551 0.999410 0.199998 0.997979 2.314904 2.029984 1.715978 1.716560; SNP PASS 71333 71087 246 88976 42 17795 5 4 0.996551 0.999410 0.199998 0.997979 2.314904 2.029984 1.715978 1.716560.",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:10396,benchmark,benchmark,10396,docs/deeptrio-wgs-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: rk.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11208 48 21239 13 9586 7 4 0.995736 0.998884 0.451340 0.997308 NaN NaN 1.561710 2.047281; INDEL PASS 11256 11208 48 21239 13 9586 7 4 0.995736 0.998884 0.451340 0.997308 NaN NaN 1.561710 2.047281; SNP ALL 71333 71087 246 88976 42 17795 5 4 0.996551 0.999410 0.199998 0.997979 2.314904 2.029984 1.715978 1.716560; SNP PASS 71333 71087 246 88976 42 17795 5 4 0.996551 0.999410 0.199998 0.997979 2.314904 2.029984 1.715978 1.716560.
",True,"The text consists primarily of code snippets and program output, including command lines and benchmarking statistics that are typical of logs or API documentation."
Testability,"rk.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11215 41 23348 85 11580 30 50 0.996357 0.992777 0.495974 0.994564 NaN NaN 1.561710 2.133416; INDEL PASS 11256 11215 41 23348 85 11580 30 50 0.996357 0.992777 0.495974 0.994564 NaN NaN 1.561710 2.133416; SNP ALL 71333 71303 30 108157 20 36757 16 4 0.999579 0.999720 0.339849 0.999650 2.314904 1.745105 1.715978 1.773270; SNP PASS 71333 71303 30 108157 20 36757 16 4 0.999579 0.999720 0.339849 0.999650 2.314904 1.745105 1.715978 1",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:10677,benchmark,benchmark,10677,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: rk.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11215 41 23348 85 11580 30 50 0.996357 0.992777 0.495974 0.994564 NaN NaN 1.561710 2.133416; INDEL PASS 11256 11215 41 23348 85 11580 30 50 0.996357 0.992777 0.495974 0.994564 NaN NaN 1.561710 2.133416; SNP ALL 71333 71303 30 108157 20 36757 16 4 0.999579 0.999720 0.339849 0.999650 2.314904 1.745105 1.715978 1.773270; SNP PASS 71333 71303 30 108157 20 36757 16 4 0.999579 0.999720 0.339849 0.999650 2.314904 1.745105 1.715978 1
",True,"The text consists primarily of code snippets and program logs. The command lines and output from a script are present, including Docker commands, file paths, and arguments which indicate programming context. Additionally, the presence of metrics like 'INDEL', 'SNP', 'TP', 'FN' suggests that this is related to a software analysis or processing pipeline, likely involving VCF files and genotyping data. There are no complete sentences or explanatory content; instead, it appears to be a log output from a script."
Testability,"rk_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:2319,benchmark,benchmark,2319,docs/deeptrio-wgs-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: rk_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup
",True,"The text consists of command lines and file downloads, which are programmatic API descriptions used to automate data retrieval. It includes syntax like variable substitution (`${HTTPDIR}`) and shell commands, typical of configuration files or build systems."
Testability,"rk_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 PacBio HiFi WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10).; These reads have been aligned to the GRCh38_no_alt_analysis reference using; [pbmm2](https://github.com/PacificBiosciences/pbmm2). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/pacbio-case-study-testdata. curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam > input/HG002.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG00",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:2388,benchmark,benchmark,2388,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: rk_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 PacBio HiFi WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10).; These reads have been aligned to the GRCh38_no_alt_analysis reference using; [pbmm2](https://github.com/PacificBiosciences/pbmm2). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/pacbio-case-study-testdata. curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam > input/HG002.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG00
",False,"The content contains both code snippets and descriptive text. The code is in bash, which includes commands for downloading files and setting up directories. However, there are also sentences explaining the purpose of the data, such as 'These reads have been aligned to the GRCh38_no_alt_analysis reference using pbmm2'. This text is meaningful and discusses concepts in natural language, thus it should not be eliminated."
Testability,"rridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRA",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:13932,log,log,13932,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['log'],['log'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: rridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRA
",False,"The text contains both bash code snippets and explanatory content. While there are code blocks, they are accompanied by descriptions that make them human-readable and not purely code or logs. For example, 'You can lower `--config.tune_every_steps` to perform evaluation more frequently' is a meaningful sentence explaining the configuration parameter. The presence of such sentences means it should be kept."
Testability,"rt.md), including `--runtime_report; --logging_dir=/output/logs` would produce the following runtime profiling output; file and visual report:. ```; /output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv; /output/logs/make_examples_runtime_by_region_report.html; ```. ### Running stages individually. If you are using make_examples itself without the run_deepvariant script, supply; make_examples with `--runtime_by_region=` a filename (.tsv). If the examples are; sharded then the runtime profile should be sharded into the same number of; shards, e.g. when using `examples@64` then runtimes could be `runtimes@64.tsv`. Then use the `runtime_by_region_vis` script to create a visual report of the; make_examples runtime by region data. Continuing from the quick start, it looks; like this:. ```bash; BIN_VERSION=""1.6.1"" # Only available in v1.1+.; docker run \; -v ""INPUT_DIR"":""/input"" \; -v ""OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/runtime_by_region_vis \; --input=/output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv \; --output=/output/logs/make_examples_runtime_by_region/make_examples_runtime_vis.html \; --title=""Quick start runtime profiling""; ```. ## Interpreting the runtime report. ### Where the data comes from. `make_examples`, the first stage of DeepVariant, starts from a BAM file and; corresponding reference FASTA file. One 1000 bp region at a time (set by; `--partition_size`), `make_examples` will:. 1. Get reads: Query the bam files for all the reads in the region. Optionally; (`--realign_reads`) do a local assembly of the reads and realign the reads; to the resulting haplotype graph. 2. Find candidates: Catalogue all the putative alternate alleles for all those; reads, and compare the accumulated evidence for each alt against thresholds; (see `--vsc_min*` parameters for the thresholds). 3. Make pileup images: Represent the reads as a; [pileup image tensor](https://",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md:2492,log,logs,2492,docs/runtime-by-region.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md,1,['log'],['logs'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: rt.md), including `--runtime_report; --logging_dir=/output/logs` would produce the following runtime profiling output; file and visual report:. ```; /output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv; /output/logs/make_examples_runtime_by_region_report.html; ```. ### Running stages individually. If you are using make_examples itself without the run_deepvariant script, supply; make_examples with `--runtime_by_region=` a filename (.tsv). If the examples are; sharded then the runtime profile should be sharded into the same number of; shards, e.g. when using `examples@64` then runtimes could be `runtimes@64.tsv`. Then use the `runtime_by_region_vis` script to create a visual report of the; make_examples runtime by region data. Continuing from the quick start, it looks; like this:. ```bash; BIN_VERSION=""1.6.1"" # Only available in v1.1+.; docker run \; -v ""INPUT_DIR"":""/input"" \; -v ""OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/runtime_by_region_vis \; --input=/output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv \; --output=/output/logs/make_examples_runtime_by_region/make_examples_runtime_vis.html \; --title=""Quick start runtime profiling""; ```. ## Interpreting the runtime report. ### Where the data comes from. `make_examples`, the first stage of DeepVariant, starts from a BAM file and; corresponding reference FASTA file. One 1000 bp region at a time (set by; `--partition_size`), `make_examples` will:. 1. Get reads: Query the bam files for all the reads in the region. Optionally; (`--realign_reads`) do a local assembly of the reads and realign the reads; to the resulting haplotype graph. 2. Find candidates: Catalogue all the putative alternate alleles for all those; reads, and compare the accumulated evidence for each alt against thresholds; (see `--vsc_min*` parameters for the thresholds). 3. Make pileup images: Represent the reads as a; [pileup image tensor](https://
",False,"The content contains complete, meaningful sentences in natural language discussing concepts and explaining processes related to DeepVariant's runtime profiling."
Testability,"s_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. By specifying `--model_type ONT_R104`, you'll be using a model that is best; suited for Oxford Nanopore R10.4.1 chemistry Simplex and Duplex reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true` to; the command above to figure out what flags you need in each step. Based on the; different model types, different flags are needed in the `make_examples` step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. ## Benchmark HG002 chr20 output from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002. ```bash; FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. TRUTH_VCF=""HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz""; TRUTH_BED=""HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \;",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md:4230,benchmark,benchmark,4230,docs/deepvariant-ont-r104-duplex-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: s_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. By specifying `--model_type ONT_R104`, you'll be using a model that is best; suited for Oxford Nanopore R10.4.1 chemistry Simplex and Duplex reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true` to; the command above to figure out what flags you need in each step. Based on the; different model types, different flags are needed in the `make_examples` step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. ## Benchmark HG002 chr20 output from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002. ```bash; FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. TRUTH_VCF=""HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz""; TRUTH_BED=""HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \;
",False,"The text includes bash commands and some script setup, but it also contains explanatory sentences about using specific models and steps in the workflow. It has a mix of code snippets and descriptive content."
Testability,"se; studies; ([DeepTrio whole genome sequencing case study](deeptrio-wgs-case-study.md) and; [Using DeepTrio for small variant calling from the trio sequenced with PacBio; HiFi](deeptrio-pacbio-case-study.md)), and in the manuscript,; [""Accurate, scalable cohort variant calls using DeepVariant and GLnexus""](https://www.biorxiv.org/content/10.1101/2020.02.10.942086v2). Please note that DeepTrio can be run with a `run_deeptrio.py` script that; automates all DeepTrio steps and thus greatly simplifies the inference pipeline.; The details of using this script can be found in the section below as well as in; the DeepTrio case studies. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we; recommend running these providing only the parent who contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot pipe them into DeepTrio. Duplicate marking may be; performed. In our analyses, there is almost no difference in accuracy with and; without duplicate marking except at lower (<20x) coverages. Finally, we; recommend that you do not perform BQSR. Running BQSR has a small decrease on; accuracy. If you are providing `--regions` or other similar arguments, these should refer; to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--r",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:3295,test,test,3295,docs/deeptrio-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md,1,['test'],['test'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: se; studies; ([DeepTrio whole genome sequencing case study](deeptrio-wgs-case-study.md) and; [Using DeepTrio for small variant calling from the trio sequenced with PacBio; HiFi](deeptrio-pacbio-case-study.md)), and in the manuscript,; [""Accurate, scalable cohort variant calls using DeepVariant and GLnexus""](https://www.biorxiv.org/content/10.1101/2020.02.10.942086v2). Please note that DeepTrio can be run with a `run_deeptrio.py` script that; automates all DeepTrio steps and thus greatly simplifies the inference pipeline.; The details of using this script can be found in the section below as well as in; the DeepTrio case studies. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we; recommend running these providing only the parent who contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot pipe them into DeepTrio. Duplicate marking may be; performed. In our analyses, there is almost no difference in accuracy with and; without duplicate marking except at lower (<20x) coverages. Finally, we; recommend that you do not perform BQSR. Running BQSR has a small decrease on; accuracy. If you are providing `--regions` or other similar arguments, these should refer; to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--r
",False,
Testability,"straints; ```. ### Benchmark variant calls against 4.2.1 truth set with hap.py. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 112",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:10252,benchmark,benchmark,10252,docs/deeptrio-pacbio-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: straints; ```. ### Benchmark variant calls against 4.2.1 truth set with hap.py. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 112
",False,The text includes natural language sentences such as 'Benchmarking Summary for HG002:' which is a human-readable explanation of results.
Testability,"t/deepvariant.input_rtg_output.txt; ```. As a result we should get the following output:. ```bash; Checking: /output/HG002_trio_merged.vcf.gz; Family: [HG003 + HG004] -> [HG002]; 95 non-pass records were skipped; Concordance HG002: F:137908/139703 (98.72%) M:137988/139909 (98.63%) F+M:134596/137968 (97.56%); Sample HG002 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 0/146013 (0.00%) records did not conform to expected call ploidy; 143704/146013 (98.42%) records were variant in at least 1 family member and checked for Mendelian constraints; 5066/143704 (3.53%) records had indeterminate consistency status due to incomplete calls; 3886/143704 (2.70%) records contained a violation of Mendelian constraints; ```. ### Perform analysis with hap.py against 4.2.1 truth set. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/i",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:9159,benchmark,benchmark,9159,docs/deeptrio-wgs-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md,2,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: t/deepvariant.input_rtg_output.txt; ```. As a result we should get the following output:. ```bash; Checking: /output/HG002_trio_merged.vcf.gz; Family: [HG003 + HG004] -> [HG002]; 95 non-pass records were skipped; Concordance HG002: F:137908/139703 (98.72%) M:137988/139909 (98.63%) F+M:134596/137968 (97.56%); Sample HG002 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 0/146013 (0.00%) records did not conform to expected call ploidy; 143704/146013 (98.42%) records were variant in at least 1 family member and checked for Mendelian constraints; 5066/143704 (3.53%) records had indeterminate consistency status due to incomplete calls; 3886/143704 (2.70%) records contained a violation of Mendelian constraints; ```. ### Perform analysis with hap.py against 4.2.1 truth set. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/i
",False,"The provided text contains log-like entries that describe the process of running Hap.py and the results obtained, such as counts and percentages. While it does include some natural language sentences, the majority consists of commands, code snippets, and technical details which are more suited for programmatic handling rather than human-readable prose."
Testability,"t_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepVariant with one command. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can now run DeepVariant with one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:1840,test,testdata,1840,docs/deepvariant-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: t_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepVariant with one command. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can now run DeepVariant with one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20
",True,"The content consists of command lines and script calls (e.g., 'mkdir -p', 'curl', 'docker run'), which are programmatic instructions rather than meaningful human-readable text. Additionally, it includes API references like parameter lists (e.g., '--ref /reference/...') which are part of the configuration or build process."
Testability,"termediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. To see the pileup images visually, check out [show_examples](show-examples.md). For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). Just make sure to use `--model_type; HYBRID_PACBIO_ILLUMINA` when running on combined PacBio and Illumina data. ## Benchmark with hap.py. See [hap.py](https://github.com/illumina/hap.py) documentation for more details; on the parameters and outputs. ```bash; mkdir -p happy. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 10628 10602 26 23385 63 12212 10 51 0.997554 0.994361 0.522215 0.995955 NaN NaN 1.748961 2.721448; INDEL PASS 10628 10602 26 23385 63 12212 10 51 0.997554 0.994361 0.522215 0.995955 NaN NaN 1.748961 2.721448; SNP ALL 70166 70138 28 105564 43 35354 16 16 0.999601 0.999388 0.334906 0.999494 2.296566 1.812971 1.883951 2.187440; SNP PASS 70166 70138 28 105564 43 35354 16 16 0.999601 0.999388 0.334906 0.999494 2.296566 1.812971 1.883951 ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:6754,benchmark,benchmark,6754,docs/deepvariant-hybrid-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: termediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. To see the pileup images visually, check out [show_examples](show-examples.md). For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). Just make sure to use `--model_type; HYBRID_PACBIO_ILLUMINA` when running on combined PacBio and Illumina data. ## Benchmark with hap.py. See [hap.py](https://github.com/illumina/hap.py) documentation for more details; on the parameters and outputs. ```bash; mkdir -p happy. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 10628 10602 26 23385 63 12212 10 51 0.997554 0.994361 0.522215 0.995955 NaN NaN 1.748961 2.721448; INDEL PASS 10628 10602 26 23385 63 12212 10 51 0.997554 0.994361 0.522215 0.995955 NaN NaN 1.748961 2.721448; SNP ALL 70166 70138 28 105564 43 35354 16 16 0.999601 0.999388 0.334906 0.999494 2.296566 1.812971 1.883951 2.187440; SNP PASS 70166 70138 28 105564 43 35354 16 16 0.999601 0.999388 0.334906 0.999494 2.296566 1.812971 1.883951 
",False,
Testability,"th `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics G400 HG002 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam > input/HG002.complete_g400.V350151728.grch38.chr20.bam. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam.bai > input/HG002.complete_g400.V350151728.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG002. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics G400 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.data-00000-of-00001 > input/weights-60-0.993753.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.index > input/weights-60-0.993753.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG002.complete_g400.V3501517",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md:1189,benchmark,benchmark,1189,docs/deepvariant-complete-g400-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: th `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics G400 HG002 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam > input/HG002.complete_g400.V350151728.grch38.chr20.bam. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam.bai > input/HG002.complete_g400.V350151728.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG002. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics G400 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.data-00000-of-00001 > input/weights-60-0.993753.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.index > input/weights-60-0.993753.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG002.complete_g400.V3501517
",False,
Testability,"these contigs will be re-genotyped as the most likely of reference or homozygous alternates. For a sample with karyotype XY, it should be set to ""chrX,chrY"" for GRCh38 and ""X,Y"" for GRCh37. For a sample with karyotype XX, this should not be used.; --par_regions_bed=""/input/GRCh3X_par.bed"" \ **Optional. If --haploid_contigs is set, then this can be used to provide PAR regions to be excluded from genotype adjustment. Download links to this files are available in this page.; --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.; ```. For details on X,Y support, please see; [DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case; study in; [DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You; can download the PAR bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). To see all flags you can use, run: `docker run; google/deepvariant:""${BIN_VERSION}""`. If you're using GPUs, or want to use Singularity instead, see; [Quick Start](docs/deepvariant-quick-start.md) for more details or see all the; [setup options](#deepvariant_setup) available. For more information, also see:. * [Full documentation list](docs/README.md); * [Detailed usage guide](docs/deepvariant-details.md) with more information on; the input and output file formats and how to work with them.; * [Best practices for multi-sample variant calling with DeepVariant](docs/trio-merge-case-study.md); * [(Advanced) Training tutorial](docs/deepvariant-training-case-study.md); * [DeepVariant's Frequently Asked Questions, FAQ](docs/FAQ.md). ## How to cite. If you're using DeepVariant in your work, please cite:. [A universal SNP and small-indel variant caller using deep neural networks. *Nature Biotechnology* 36, 983–987 (2018).](https://rdcu.be/7Dhl) <br/>; Ryan Poplin, P",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:5015,test,testdata,5015,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: these contigs will be re-genotyped as the most likely of reference or homozygous alternates. For a sample with karyotype XY, it should be set to ""chrX,chrY"" for GRCh38 and ""X,Y"" for GRCh37. For a sample with karyotype XX, this should not be used.; --par_regions_bed=""/input/GRCh3X_par.bed"" \ **Optional. If --haploid_contigs is set, then this can be used to provide PAR regions to be excluded from genotype adjustment. Download links to this files are available in this page.; --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.; ```. For details on X,Y support, please see; [DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case; study in; [DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You; can download the PAR bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). To see all flags you can use, run: `docker run; google/deepvariant:""${BIN_VERSION}""`. If you're using GPUs, or want to use Singularity instead, see; [Quick Start](docs/deepvariant-quick-start.md) for more details or see all the; [setup options](#deepvariant_setup) available. For more information, also see:. * [Full documentation list](docs/README.md); * [Detailed usage guide](docs/deepvariant-details.md) with more information on; the input and output file formats and how to work with them.; * [Best practices for multi-sample variant calling with DeepVariant](docs/trio-merge-case-study.md); * [(Advanced) Training tutorial](docs/deepvariant-training-case-study.md); * [DeepVariant's Frequently Asked Questions, FAQ](docs/FAQ.md). ## How to cite. If you're using DeepVariant in your work, please cite:. [A universal SNP and small-indel variant caller using deep neural networks. *Nature Biotechnology* 36, 983–987 (2018).](https://rdcu.be/7Dhl) <br/>; Ryan Poplin, P
",True,"The text contains API documentation and parameter lists, such as --haploid_contigs, --par_regions_bed, and download links, which are typical of programmatic descriptions that should be filtered out."
Testability,"to DeepVariant is in; experimenting with different approaches, training on more and better data, and; carefully evaluating the models before releasing them. We did the same with this; hybrid model. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:3119,benchmark,benchmark,3119,docs/deepvariant-hybrid-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: to DeepVariant is in; experimenting with different approaches, training on more and better data, and; carefully evaluating the models before releasing them. We did the same with this; hybrid model. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.
",True,"The text contains code snippets and API documentation, including command lines and file downloads which are typical of programmatic content that should be filtered."
Testability,"tus due to incomplete calls; 3886/143704 (2.70%) records contained a violation of Mendelian constraints; ```. ### Perform analysis with hap.py against 4.2.1 truth set. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:9881,benchmark,benchmark,9881,docs/deeptrio-wgs-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: tus due to incomplete calls; 3886/143704 (2.70%) records contained a violation of Mendelian constraints; ```. ### Perform analysis with hap.py against 4.2.1 truth set. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.
",True,"The text consists primarily of code snippets, logs, and API documentation, including docker commands, bash scripts, and programmatic arguments. It lacks meaningful human-readable sentences."
Testability,"un_deepvariant.py`. Much of the work we put into DeepVariant is in; experimenting with different approaches, training on more and better data, and; carefully evaluating the models before releasing them. We did the same with this; hybrid model. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:3079,benchmark,benchmarks,3079,docs/deepvariant-hybrid-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md,1,['benchmark'],['benchmarks'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: un_deepvariant.py`. Much of the work we put into DeepVariant is in; experimenting with different approaches, training on more and better data, and; carefully evaluating the models before releasing them. We did the same with this; hybrid model. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${
",False,"The text contains a mix of bash commands, file downloads, and some descriptive content. While there are code snippets present, the majority of the text is explanatory and describes the steps taken in the workflow. The presence of URLs and file names suggests it might be related to configuration or setup instructions rather than pure code or logs. Additionally, the text includes sentences that provide context about the process (e.g., 'much of the work... before releasing them'), which are meaningful for human readers."
Testability,"urself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. This should create a subdirectory in the current directory containing the actual; data files:. ```bash; ls -1 ${INPUT_DIR}; ```. outputting:. ```; NA12878_S1.chr20.10_10p1mb.",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:2110,test,testdata,2110,docs/deepvariant-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: urself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. This should create a subdirectory in the current directory containing the actual; data files:. ```bash; ls -1 ${INPUT_DIR}; ```. outputting:. ```; NA12878_S1.chr20.10_10p1mb.
",False,
Testability,"vironment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation, `hap.py` will intersect this BED with the GIAB; confident regions. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_n",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md:1213,benchmark,benchmark,1213,docs/deepvariant-exome-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: vironment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation, `hap.py` will intersect this BED with the GIAB; confident regions. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_n
",False,
Testability,"vironment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepVariant with one command. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You c",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:1279,benchmark,benchmark,1279,docs/deepvariant-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: vironment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepVariant with one command. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You c
",False,"The text contains commands and code snippets that are used to download data and run the DeepVariant pipeline. However, there are also sentences like 'DeepVariant pipeline consists of 3 steps: make_examples, call_variants, and postprocess_variants' which are meaningful and explanatory. Additionally, it discusses concepts such as the use of GRCh38, Genome in a Bottle benchmarks, and HG003 Illumina reads, which provide context about the case study."
Testability,"vironment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai; ```. ## Running DeepVariant. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:3313,benchmark,benchmark,3313,docs/deepvariant-hybrid-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: vironment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai; ```. ## Running DeepVariant. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess
",False,
Testability,"want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/ont-case-study-duplex"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input/data""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG002 Duplex chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam > ${INPUT_DIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam; curl ${HTTPDIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam.bai > ${INPUT_DIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam.bai. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam""; THREADS=$(nproc); REGION=""chr20"". # Set up output variable; OUTPUT_VCF=""HG002_R1041_Duplex_Dorado_v0.1.1_GRCh38.chr20.output.vcf.gz""; OUTPUT_GVCF=""HG002_R1041_Duplex_Dorado_v0.1.1_GRCh38.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md:1885,test,testdata,1885,docs/deepvariant-ont-r104-duplex-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md,1,['test'],['testdata'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/ont-case-study-duplex"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input/data""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG002 Duplex chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam > ${INPUT_DIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam; curl ${HTTPDIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam.bai > ${INPUT_DIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam.bai. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam""; THREADS=$(nproc); REGION=""chr20"". # Set up output variable; OUTPUT_VCF=""HG002_R1041_Duplex_Dorado_v0.1.1_GRCh38.chr20.output.vcf.gz""; OUTPUT_GVCF=""HG002_R1041_Duplex_Dorado_v0.1.1_GRCh38.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_
",True,"The provided text is a mix of code snippets and configuration commands. It includes bash scripts with variable assignments, file downloads using curl, directory creation, and script execution instructions. While there are some sentences that could be considered explanatory, the majority of the content is focused on specific programming steps and command lines, which fall under programmatic API descriptions or build system output."
Testability,"whole exome sequencing (WES) case study. Similar to the [case study on whole genome sequencing data], in this; study we describe applying DeepVariant to a real exome sample using a single; machine. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation, `hap.py` wil",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md:1019,benchmark,benchmark,1019,docs/deepvariant-exome-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: whole exome sequencing (WES) case study. Similar to the [case study on whole genome sequencing data], in this; study we describe applying DeepVariant to a real exome sample using a single; machine. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation, `hap.py` wil
",False,"The text contains a detailed guide on setting up and running DeepVariant for whole exome sequencing, including code snippets for downloading references, benchmarks, and data files. While it includes some bash commands, the majority of the content is explanatory and procedural, with meaningful sentences discussing the process and tools used."
Testability,"xit` and hit enter. ### Download the RNA-seq model. Finally, lets download the RNA-seq model that we will use to call variants. ```bash; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta; ```. ### Directory Structure. After you have run the steps above, your directory structure should look like; this:. ```; .; ├── benchmark; │   ├── chr20_CDS_3x.benchmark_regions.bed; │   ├── HG005_GRCh38_1_22_v4.2.1_benchmark.bed; │   ├── HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; │   └── HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ├── data; │   ├── chr20_CDS_3x.bed; │   ├── chr20_CDS.bed; │   ├── gencode.v41.basic.annotation.gff3.gz; │   ├── hg005_3x.bed; │   ├── hg005_coverage.mosdepth.global.dist.txt; │   ├── hg005_coverage.mosdepth.summary.txt; │   ├── hg005_coverage.per-base.bed.gz; │   ├── hg005_coverage.per-base.bed.gz.csi; │   ├── hg005_gm26107.mrna.grch38.bam; │   └── hg005_gm26107.mrna.grch38.bam.bai; ├── happy; ├── model; │   ├── model.ckpt.data-00000-of-00001; │   ├── model.ckpt.index; │   └── model.ckpt.meta; ├── output; └── reference; ├── GRCh38_no_alt_analysis_set.fasta; └── GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Running DeepVariant RNA-seq on a CPU-only machine. The command below will run the DeepVariant RNA-seq model and produce an output; VCF (`output/out.vcf.gz`). ```bash; BIN_VERSION=""1.4.0"". ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:6247,benchmark,benchmark,6247,docs/deepvariant-rnaseq-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: xit` and hit enter. ### Download the RNA-seq model. Finally, lets download the RNA-seq model that we will use to call variants. ```bash; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta; ```. ### Directory Structure. After you have run the steps above, your directory structure should look like; this:. ```; .; ├── benchmark; │   ├── chr20_CDS_3x.benchmark_regions.bed; │   ├── HG005_GRCh38_1_22_v4.2.1_benchmark.bed; │   ├── HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; │   └── HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ├── data; │   ├── chr20_CDS_3x.bed; │   ├── chr20_CDS.bed; │   ├── gencode.v41.basic.annotation.gff3.gz; │   ├── hg005_3x.bed; │   ├── hg005_coverage.mosdepth.global.dist.txt; │   ├── hg005_coverage.mosdepth.summary.txt; │   ├── hg005_coverage.per-base.bed.gz; │   ├── hg005_coverage.per-base.bed.gz.csi; │   ├── hg005_gm26107.mrna.grch38.bam; │   └── hg005_gm26107.mrna.grch38.bam.bai; ├── happy; ├── model; │   ├── model.ckpt.data-00000-of-00001; │   ├── model.ckpt.index; │   └── model.ckpt.meta; ├── output; └── reference; ├── GRCh38_no_alt_analysis_set.fasta; └── GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Running DeepVariant RNA-seq on a CPU-only machine. The command below will run the DeepVariant RNA-seq model and produce an output; VCF (`output/out.vcf.gz`). ```bash; BIN_VERSION=""1.4.0"". 
",False,"The content contains natural language sentences discussing the process of downloading an RNA-seq model, directory structure, and running DeepVariant on a CPU, along with commands in bash. It provides both instructions for actions and explanatory text about the expected output."
Testability,"y machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG002.complete_g400.V350151728.grch38.chr20.bam \; --output_vcf /output/HG002.output.vcf.gz \; --output_gvcf /output/HG002.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-60-0.993753.ckpt; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11130 126 20925 31 9340 26 4 0.988806 0.997324 0.446356 0.993047 NaN NaN 1.561710 2.049106; INDEL PASS 11256 11130 126 20925 31 9340 26 4 0.988806 0.997324 0.446356 0.993047 NaN NaN 1.561710 2.049106; SNP ALL 71333 70949 384 85736 50 14689 28 6 0.994617 0.999296 0.171328 0.996951",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md:2718,benchmark,benchmark,2718,docs/deepvariant-complete-g400-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md,2,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: y machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG002.complete_g400.V350151728.grch38.chr20.bam \; --output_vcf /output/HG002.output.vcf.gz \; --output_gvcf /output/HG002.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-60-0.993753.ckpt; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11130 126 20925 31 9340 26 4 0.988806 0.997324 0.446356 0.993047 NaN NaN 1.561710 2.049106; INDEL PASS 11256 11130 126 20925 31 9340 26 4 0.988806 0.997324 0.446356 0.993047 NaN NaN 1.561710 2.049106; SNP ALL 71333 70949 384 85736 50 14689 28 6 0.994617 0.999296 0.171328 0.996951
",False,"The content consists of benchmark results which are meaningful and in human-readable format, including statistics like Recall, Precision, F1 Score etc. These metrics provide information on the performance of a system or tool, making them valuable for analysis and understanding."
Testability,"{BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf /output/HG001.output.vcf.gz \; --output_gvcf /output/HG001.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-51-0.995354.ckpt; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG001.output.vcf.gz \; -f /benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 9974 9947 27 21052 9 10750 3 5 0.997293 0.999126 0.510640 0.998209 NaN NaN 1.630447 2.156149; INDEL PASS 9974 9947 27 21052 9 10750 3 5 0.997293 0.999126 0.510640 0.998209 NaN NaN 1.630447 2.156149; SNP ALL 69175 68874 301 85030 44 16068 8 2 0.995649 0.999362 0.188969 0.997502 2.288757 2.084645 1.730097 1.781789; SNP PASS 69175 68874 301 85030 44 16068 8 2 0.995649 0.999362 0.188969 0.997502 2.288757 2.084645 1.730097 1.781789; ```. To summarize:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_S",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md:2969,benchmark,benchmark,2969,docs/deepvariant-complete-t7-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md,1,['benchmark'],['benchmark'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: {BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf /output/HG001.output.vcf.gz \; --output_gvcf /output/HG001.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-51-0.995354.ckpt; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG001.output.vcf.gz \; -f /benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 9974 9947 27 21052 9 10750 3 5 0.997293 0.999126 0.510640 0.998209 NaN NaN 1.630447 2.156149; INDEL PASS 9974 9947 27 21052 9 10750 3 5 0.997293 0.999126 0.510640 0.998209 NaN NaN 1.630447 2.156149; SNP ALL 69175 68874 301 85030 44 16068 8 2 0.995649 0.999362 0.188969 0.997502 2.288757 2.084645 1.730097 1.781789; SNP PASS 69175 68874 301 85030 44 16068 8 2 0.995649 0.999362 0.188969 0.997502 2.288757 2.084645 1.730097 1.781789; ```. To summarize:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_S
",True,"The text consists primarily of program logs or error messages, including timestamps and metrics like Recall, Precision, F1 Score which are typical in evaluation reports. There is no meaningful human-readable sentences beyond these computational results."
Usability," adjustment. Download links to this files are available in this page.; --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.; ```. For details on X,Y support, please see; [DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case; study in; [DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You; can download the PAR bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). To see all flags you can use, run: `docker run; google/deepvariant:""${BIN_VERSION}""`. If you're using GPUs, or want to use Singularity instead, see; [Quick Start](docs/deepvariant-quick-start.md) for more details or see all the; [setup options](#deepvariant_setup) available. For more information, also see:. * [Full documentation list](docs/README.md); * [Detailed usage guide](docs/deepvariant-details.md) with more information on; the input and output file formats and how to work with them.; * [Best practices for multi-sample variant calling with DeepVariant](docs/trio-merge-case-study.md); * [(Advanced) Training tutorial](docs/deepvariant-training-case-study.md); * [DeepVariant's Frequently Asked Questions, FAQ](docs/FAQ.md). ## How to cite. If you're using DeepVariant in your work, please cite:. [A universal SNP and small-indel variant caller using deep neural networks. *Nature Biotechnology* 36, 983–987 (2018).](https://rdcu.be/7Dhl) <br/>; Ryan Poplin, Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas Colthurst, Alexander Ku, Dan Newburger, Jojo Dijamco, Nam Nguyen, Pegah T. Afshar, Sam S. Gross, Lizzie Dorfman, Cory Y. McLean, and Mark A. DePristo.<br/>; doi: https://doi.org/10.1038/nbt.4235. Additionally, if you are generating multi-sample calls using our; [DeepVariant and GLnexus Best Practices](docs/trio-merge-case-study.md), please; cite:. [",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:5413,guid,guide,5413,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['guid'],['guide'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  adjustment. Download links to this files are available in this page.; --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.; ```. For details on X,Y support, please see; [DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case; study in; [DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You; can download the PAR bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). To see all flags you can use, run: `docker run; google/deepvariant:""${BIN_VERSION}""`. If you're using GPUs, or want to use Singularity instead, see; [Quick Start](docs/deepvariant-quick-start.md) for more details or see all the; [setup options](#deepvariant_setup) available. For more information, also see:. * [Full documentation list](docs/README.md); * [Detailed usage guide](docs/deepvariant-details.md) with more information on; the input and output file formats and how to work with them.; * [Best practices for multi-sample variant calling with DeepVariant](docs/trio-merge-case-study.md); * [(Advanced) Training tutorial](docs/deepvariant-training-case-study.md); * [DeepVariant's Frequently Asked Questions, FAQ](docs/FAQ.md). ## How to cite. If you're using DeepVariant in your work, please cite:. [A universal SNP and small-indel variant caller using deep neural networks. *Nature Biotechnology* 36, 983–987 (2018).](https://rdcu.be/7Dhl) <br/>; Ryan Poplin, Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas Colthurst, Alexander Ku, Dan Newburger, Jojo Dijamco, Nam Nguyen, Pegah T. Afshar, Sam S. Gross, Lizzie Dorfman, Cory Y. McLean, and Mark A. DePristo.<br/>; doi: https://doi.org/10.1038/nbt.4235. Additionally, if you are generating multi-sample calls using our; [DeepVariant and GLnexus Best Practices](docs/trio-merge-case-study.md), please; cite:. [
",False,"The text contains complete sentences in natural language discussing concepts and providing information on usage, documentation, and best practices for DeepVariant. It includes links to resources and explanations of how to use the tool, which are meaningful human-readable content."
Usability," get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure; ```. NOTE: To create an instance *without GPU*, simply omit the last line from the; command. Check that the instance has been created and started:. ```shell; gcloud compute instances list; ```. which should produce output like:. ```; NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS; [USER]-deepvariant-quickstart us-west1-b n1-standard-8 10.138.0.4 35.185.203.59 RUNNING; ```. Then connect to your instance via SSH:. ```shell; gcloud compute ssh --zone us-west1-b ""${USER}-deepvariant-quickstart""; ```. You should land at a shell prompt in your new instance!. NOTE: All of these steps can also be completed from the Cloud Console, if you; prefer. Consult [this; guide](https://cloud.google.com/compute/docs/quickstart-linux), but be sure to; choose Ubuntu 20.04 as your image, as DeepVariant has not been tested on other; Linux distributions. For more information about getting started with Compute Engine, see:. * [Compute Engine instance creation in `gcloud`; manual](https://cloud.google.com/sdk/gcloud/reference/compute/instan",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:4222,simpl,simply,4222,docs/deepvariant-gcp-info.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md,1,['simpl'],['simply'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure; ```. NOTE: To create an instance *without GPU*, simply omit the last line from the; command. Check that the instance has been created and started:. ```shell; gcloud compute instances list; ```. which should produce output like:. ```; NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS; [USER]-deepvariant-quickstart us-west1-b n1-standard-8 10.138.0.4 35.185.203.59 RUNNING; ```. Then connect to your instance via SSH:. ```shell; gcloud compute ssh --zone us-west1-b ""${USER}-deepvariant-quickstart""; ```. You should land at a shell prompt in your new instance!. NOTE: All of these steps can also be completed from the Cloud Console, if you; prefer. Consult [this; guide](https://cloud.google.com/compute/docs/quickstart-linux), but be sure to; choose Ubuntu 20.04 as your image, as DeepVariant has not been tested on other; Linux distributions. For more information about getting started with Compute Engine, see:. * [Compute Engine instance creation in `gcloud`; manual](https://cloud.google.com/sdk/gcloud/reference/compute/instan
",False,"The text contains a series of step-by-step instructions for configuring and setting up an instance on Google Cloud Compute Engine. It includes commands for creating instances, managing quotas, allocating resources like CPUs and GPUs, specifying machine types, disk sizes, zones, and scopes. These are primarily command-line instructions and code snippets aimed at users who are familiar with gcloud CLI. The text does not contain meaningful sentences in natural language but instead provides programmatic steps and configurations."
Usability," in their `SM`; header tag, which is usually derived from a command-line flag to the read; aligner. If your BAM files don't have unique `SM` tags (and if it's not feasible; to adjust the alignment pipeline), add the `--sample_name=XYZ` flag to; `run_deepvariant` to override the sample name written into the gVCF file header. ## Merge the trio samples using GLnexus. ### Run GLnexus to merge 3 gVCFs. And then run GLnexus with this config:. ```; sudo docker pull quay.io/mlin/glnexus:v1.2.7. time sudo docker run \; -v ""${DIR}"":""/data"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWES \; --bed ""/data/${CAPTURE_BED}"" \; /data/HG004.g.vcf.gz /data/HG003.g.vcf.gz /data/HG002.g.vcf.gz \; | sudo docker run -i google/deepvariant:${VERSION} bcftools view - \; | sudo docker run -i google/deepvariant:${VERSION} bgzip -c \; > ${DIR}/deepvariant.cohort.vcf.gz; ```. When we ran on this WES trio, it took only about 13 seconds. For more details on; performance, see; [GLnexus performance guide](https://github.com/dnanexus-rnd/GLnexus/wiki/Performance). For a WGS cohort, we recommend using `--config DeepVariantWGS` instead of; `DeepVariantWES`. Another preset `DeepVariant_unfiltered` is available in; `glnexus:v1.2.7` or later versions for merging DeepVariant gVCFs with no QC; filters or genotype revision (see; [GitHub issue #326](https://github.com/google/deepvariant/issues/326) for a; potential use case). The details of these presets can be found; [here](../deepvariant/cohort_best_practice). ## Annotate the merged VCF with Mendelian discordance information using RTG Tools. Create an SDF template from our reference file:. ```; sudo docker run \; -v ""${DIR}"":""/data"" \; realtimegenomics/rtg-tools format \; -o /data/hs37d5.sdf /data/hs37d5.fa; ```. Create a PED file `$DIR/trio.ped` that looks like this (with the sample name; of the trio):. ```; FILE=""${DIR}/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/ind-id/pat-id/mat-id: 0=unknown; #se",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:8278,guid,guide,8278,docs/trio-merge-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md,1,['guid'],['guide'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  in their `SM`; header tag, which is usually derived from a command-line flag to the read; aligner. If your BAM files don't have unique `SM` tags (and if it's not feasible; to adjust the alignment pipeline), add the `--sample_name=XYZ` flag to; `run_deepvariant` to override the sample name written into the gVCF file header. ## Merge the trio samples using GLnexus. ### Run GLnexus to merge 3 gVCFs. And then run GLnexus with this config:. ```; sudo docker pull quay.io/mlin/glnexus:v1.2.7. time sudo docker run \; -v ""${DIR}"":""/data"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWES \; --bed ""/data/${CAPTURE_BED}"" \; /data/HG004.g.vcf.gz /data/HG003.g.vcf.gz /data/HG002.g.vcf.gz \; | sudo docker run -i google/deepvariant:${VERSION} bcftools view - \; | sudo docker run -i google/deepvariant:${VERSION} bgzip -c \; > ${DIR}/deepvariant.cohort.vcf.gz; ```. When we ran on this WES trio, it took only about 13 seconds. For more details on; performance, see; [GLnexus performance guide](https://github.com/dnanexus-rnd/GLnexus/wiki/Performance). For a WGS cohort, we recommend using `--config DeepVariantWGS` instead of; `DeepVariantWES`. Another preset `DeepVariant_unfiltered` is available in; `glnexus:v1.2.7` or later versions for merging DeepVariant gVCFs with no QC; filters or genotype revision (see; [GitHub issue #326](https://github.com/google/deepvariant/issues/326) for a; potential use case). The details of these presets can be found; [here](../deepvariant/cohort_best_practice). ## Annotate the merged VCF with Mendelian discordance information using RTG Tools. Create an SDF template from our reference file:. ```; sudo docker run \; -v ""${DIR}"":""/data"" \; realtimegenomics/rtg-tools format \; -o /data/hs37d5.sdf /data/hs37d5.fa; ```. Create a PED file `$DIR/trio.ped` that looks like this (with the sample name; of the trio):. ```; FILE=""${DIR}/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/ind-id/pat-id/mat-id: 0=unknown; #se
",True,"The text consists primarily of code snippets, commands, and program logs. It includes syntax like semicolons and commands such as 'sudo docker run', which are indicative of code execution. Additionally, the presence of configuration files and build system output is evident, as well as version control metadata or comments related to GitHub issues and performance guides."
Usability," or other similar arguments these should; refer to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and; `confident_regions` arguments should point to VCF and BED files containing the; true variants and regions where we are confident in our calls (i.e., calls; within these regions and not in the truth_vcf are considered false positives).; These should be bgzipped and tabix indexed and be on a reference consistent with; the one provided with the `--ref` argument. ### call_variants. `call_variants` consumes TFRecord file(s) of tf.Examples protos created; by `make_examples` and a deep learning model checkpoint and evaluates the model; on each example in the input TFRecord. The output here is a TFRecord of; CallVariantsOutput protos. `call_variants` doesn't directly support sharding its; outputs, but accepts a glob or shard-pattern for its inputs. `call_variants` uses around 4 GB per process and uses TensorFlow for evaluation.; When evaluating a model in CPU mode, TensorFlow can make use of multiple cores,; but scaling is sub-linear. In other words, `call_variants` on a 64 core machine; is less than 8x faster than running on a 8 core machine. When using a GPU, `call_variants` is both faster, more efficient, and needs; fewer CPUs. Based on a small number of experiments, currently the most efficient; configuration for `call_variants` on a GPU instance is 4-8 CPUs and 1 GPU.; Compared to our setting in the [whole genome case study], we noticed a 2.5x; speedup on the call_variants step using a single P100 GPU and 8 CPUs. Note that; currently `call_variants` can only use one GPU at most. ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:4868,learn,learning,4868,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['learn'],['learning'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  or other similar arguments these should; refer to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and; `confident_regions` arguments should point to VCF and BED files containing the; true variants and regions where we are confident in our calls (i.e., calls; within these regions and not in the truth_vcf are considered false positives).; These should be bgzipped and tabix indexed and be on a reference consistent with; the one provided with the `--ref` argument. ### call_variants. `call_variants` consumes TFRecord file(s) of tf.Examples protos created; by `make_examples` and a deep learning model checkpoint and evaluates the model; on each example in the input TFRecord. The output here is a TFRecord of; CallVariantsOutput protos. `call_variants` doesn't directly support sharding its; outputs, but accepts a glob or shard-pattern for its inputs. `call_variants` uses around 4 GB per process and uses TensorFlow for evaluation.; When evaluating a model in CPU mode, TensorFlow can make use of multiple cores,; but scaling is sub-linear. In other words, `call_variants` on a 64 core machine; is less than 8x faster than running on a 8 core machine. When using a GPU, `call_variants` is both faster, more efficient, and needs; fewer CPUs. Based on a small number of experiments, currently the most efficient; configuration for `call_variants` on a GPU instance is 4-8 CPUs and 1 GPU.; Compared to our setting in the [whole genome case study], we noticed a 2.5x; speedup on the call_variants step using a single P100 GPU and 8 CPUs. Note that; currently `call_variants` can only use one GPU at most. 
",False,
Usability," you should create one at; [cloud.google.com](https://cloud.google.com). You should then [enable; billing for your; account](https://support.google.com/cloud/answer/6288653?hl=en) but note; that if your account is new, [you receive $300 of free; credit](https://cloud.google.com/free/). Once your cloud account is set up,; you should be able to log in to the [Cloud; Console](https://console.cloud.google.com) to view or administer your cloud; resources. * From the Cloud Console, [set up a; project](https://cloud.google.com/resource-manager/docs/creating-managing-projects); to house all of the cloud resources (storage, compute, services) that you; will associate with your use of DeepVariant. For example, if your; organization is AcmeCorp, you might call your project; `acmecorp-deepvariant`. * Finally, please visit the [""Compute Engine"" page on Cloud; Console](https://console.cloud.google.com/compute). You don't need to create; Compute Engine instances at this time, but simply visiting this page will; initialize your compute engine ""service account"" so that we can authorize; it. (As you progress in your use of Google Cloud Platform, you will likely find it; useful to create a [Cloud; Organization](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation proc",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:1365,simpl,simply,1365,docs/deepvariant-gcp-info.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md,1,['simpl'],['simply'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  you should create one at; [cloud.google.com](https://cloud.google.com). You should then [enable; billing for your; account](https://support.google.com/cloud/answer/6288653?hl=en) but note; that if your account is new, [you receive $300 of free; credit](https://cloud.google.com/free/). Once your cloud account is set up,; you should be able to log in to the [Cloud; Console](https://console.cloud.google.com) to view or administer your cloud; resources. * From the Cloud Console, [set up a; project](https://cloud.google.com/resource-manager/docs/creating-managing-projects); to house all of the cloud resources (storage, compute, services) that you; will associate with your use of DeepVariant. For example, if your; organization is AcmeCorp, you might call your project; `acmecorp-deepvariant`. * Finally, please visit the [""Compute Engine"" page on Cloud; Console](https://console.cloud.google.com/compute). You don't need to create; Compute Engine instances at this time, but simply visiting this page will; initialize your compute engine ""service account"" so that we can authorize; it. (As you progress in your use of Google Cloud Platform, you will likely find it; useful to create a [Cloud; Organization](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation proc
",True,"The content primarily consists of code snippets and API documentation. It includes URLs for navigation, steps to set up projects, and mentions of command-line tools (`gcloud`, `gsutil`). While it does contain some explanatory text, the majority is programmatic in nature and should be filtered out."
Usability," your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure; ```. NOTE: To create an instance *without GPU*, simply omit the last line from the; command. Check that the instance has been created and started:. ```shell; gcloud compute instances list; ```. which should produce output like:. ```; NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS; [USER]-deepvariant-quickstart us-west1-b n1-standard-8 10.138.0.4 35.185.203.59 RUNNING; ```. Then connect to your instance via SSH:. ```shell",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:3571,guid,guide,3571,docs/deepvariant-gcp-info.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md,1,['guid'],['guide'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content:  your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure; ```. NOTE: To create an instance *without GPU*, simply omit the last line from the; command. Check that the instance has been created and started:. ```shell; gcloud compute instances list; ```. which should produce output like:. ```; NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS; [USER]-deepvariant-quickstart us-west1-b n1-standard-8 10.138.0.4 35.185.203.59 RUNNING; ```. Then connect to your instance via SSH:. ```shell
",False,
Usability,"# Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data. DeepVariant is an analysis pipeline that uses a deep neural network to call; genetic variants from next-generation DNA sequencing (NGS) data. While; DeepVariant is highly accurate for; [many types of NGS data](https://rdcu.be/7Dhl), some users may be interested in; training custom deep learning models that have been optimized for very specific; data. This case study describes one way to train such a custom model using a GPU, in; this case for BGISEQ-500 data. Please note that there is not yet a production-grade training pipeline. This is; just one example of how to train a custom model, and is neither the fastest nor; the cheapest possible configuration. The resulting model also does not represent; the greatest achievable accuracy for BGISEQ-500 data. ## High level summary of result. We demonstrated that by training on 1 replicate of BGISEQ-500 whole genome data; (everything except for chromosome 20-22), we can significantly improve the; accuracy comparing to the WGS model as a baseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; followi",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:385,learn,learning,385,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['learn'],['learning'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data. DeepVariant is an analysis pipeline that uses a deep neural network to call; genetic variants from next-generation DNA sequencing (NGS) data. While; DeepVariant is highly accurate for; [many types of NGS data](https://rdcu.be/7Dhl), some users may be interested in; training custom deep learning models that have been optimized for very specific; data. This case study describes one way to train such a custom model using a GPU, in; this case for BGISEQ-500 data. Please note that there is not yet a production-grade training pipeline. This is; just one example of how to train a custom model, and is neither the fastest nor; the cheapest possible configuration. The resulting model also does not represent; the greatest achievable accuracy for BGISEQ-500 data. ## High level summary of result. We demonstrated that by training on 1 replicate of BGISEQ-500 whole genome data; (everything except for chromosome 20-22), we can significantly improve the; accuracy comparing to the WGS model as a baseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; followi
",False,"The text contains complete sentences in natural language discussing the process of training a custom variant caller for specific data types, including detailed explanations about accuracy improvements and hardware requirements. It also includes code examples which are necessary for instruction but does not consist solely of code or logs."
Usability,"# DeepVariant with Oxford Nanopore R10.4.1 Simplex reads. In this case study, we describe applying DeepVariant to Oxford Nanopore R10.4.1; simplex reads. Then we assess the quality of the DeepVariant variant calls with; `hap.py`. To make it faster to go over this case study, we run only on chromosome 20. The dataset used in this case-study has following attributes:. ```bash; Sample: HG003; Region: Chr20; Chemistry: ONT R10.4.1; Coverage: 80x; ```. **Model note:**. * The model is trained with Guppy 6+ ""SUP"" Simplex and Dorado v0.1.1 Duplex; reads. * The model is trained on both Ultra-long and sheared reads with varying read; N50 and coverage. ## Prepare environment. In this case-study, we will use [Docker](https://docs.docker.com/get-docker/) to; run DeepVariant for variant calling and; [hap.py](https://github.com/illumina/hap.py) for benchmarking. If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/ont-case-study"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input/data""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG003 Ultra-long chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam; curl ${HTTPDIR}/HG003_R104_sup_merged",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md:139,simpl,simplex,139,docs/deepvariant-ont-r104-simplex-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md,1,['simpl'],['simplex'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # DeepVariant with Oxford Nanopore R10.4.1 Simplex reads. In this case study, we describe applying DeepVariant to Oxford Nanopore R10.4.1; simplex reads. Then we assess the quality of the DeepVariant variant calls with; `hap.py`. To make it faster to go over this case study, we run only on chromosome 20. The dataset used in this case-study has following attributes:. ```bash; Sample: HG003; Region: Chr20; Chemistry: ONT R10.4.1; Coverage: 80x; ```. **Model note:**. * The model is trained with Guppy 6+ ""SUP"" Simplex and Dorado v0.1.1 Duplex; reads. * The model is trained on both Ultra-long and sheared reads with varying read; N50 and coverage. ## Prepare environment. In this case-study, we will use [Docker](https://docs.docker.com/get-docker/) to; run DeepVariant for variant calling and; [hap.py](https://github.com/illumina/hap.py) for benchmarking. If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/ont-case-study"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input/data""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG003 Ultra-long chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam; curl ${HTTPDIR}/HG003_R104_sup_merged
",True,"The text consists primarily of programmatic instructions and script snippets, including command line calls to download files and set up directories. This is more appropriate for a configuration file or build system output rather than meaningful human-readable content."
Usability,# Documentation. * [DeepVariant release notes](https://github.com/google/deepvariant/releases). ## Quick start and Case studies. * [DeepVariant quick start](deepvariant-quick-start.md); * [DeepVariant whole genome case study](deepvariant-case-study.md); * [DeepVariant exome case study](deepvariant-exome-case-study.md); * [DeepVariant PacBio case study](deepvariant-pacbio-model-case-study.md); * [DeepVariant ONT R10.4 simplex case study](deepvariant-ont-r104-simplex-case-study.md); [DeepVariant ONT R10.4 duplex case study](deepvariant-ont-r104-duplex-case-study.md); * [DeepVariant hybrid (PacBio and Illumina) case study](deepvariant-hybrid-case-study.md); * [DeepVariant Complete Genomics T7 case study](deepvariant-complete-t7-case-study.md); * [DeepVariant Complete Genomics G400 case study](deepvariant-complete-g400-case-study.md); * [Runtime and accuracy metrics for all DeepVariant models](metrics.md); * [Best practices for multi-sample variant calling](trio-merge-case-study.md); * [Using graph genomes: VG Giraffe + DeepVariant case study](deepvariant-vg-case-study.md). ## Visualization and analysis. * [show_examples: Saving human-readable images from DeepVariant examples](show-examples.md); * [VCF stats report](deepvariant-vcf-stats-report.md); * [Runtime by region for make_examples](runtime-by-region.md). ### Colab notebooks. * [Colab example: visualizing pileup images/tensors](visualizing_examples.ipynb); * [Can you beat DeepVariant?: A look inside the classification task](cybdv_notebook.ipynb); * [Google Developer Codelab: Variant Calling on a Rice genome with DeepVariant](https://codelabs.developers.google.com/codelabs/genomics-deepvariant). ## (Advanced) Training. * [Advanced Case Study: Train a customized SNP and small indel variant caller; for BGISEQ-500 data](deepvariant-training-case-study.md); * [DeepVariant training data](deepvariant-details-training-data.md). ## More details. * [DeepVariant usage guide](deepvariant-details.md); * [Building and testing De,MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/README.md:421,simpl,simplex,421,docs/README.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/README.md,2,['simpl'],"['simplex', 'simplex-case-study']","You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # Documentation. * [DeepVariant release notes](https://github.com/google/deepvariant/releases). ## Quick start and Case studies. * [DeepVariant quick start](deepvariant-quick-start.md); * [DeepVariant whole genome case study](deepvariant-case-study.md); * [DeepVariant exome case study](deepvariant-exome-case-study.md); * [DeepVariant PacBio case study](deepvariant-pacbio-model-case-study.md); * [DeepVariant ONT R10.4 simplex case study](deepvariant-ont-r104-simplex-case-study.md); [DeepVariant ONT R10.4 duplex case study](deepvariant-ont-r104-duplex-case-study.md); * [DeepVariant hybrid (PacBio and Illumina) case study](deepvariant-hybrid-case-study.md); * [DeepVariant Complete Genomics T7 case study](deepvariant-complete-t7-case-study.md); * [DeepVariant Complete Genomics G400 case study](deepvariant-complete-g400-case-study.md); * [Runtime and accuracy metrics for all DeepVariant models](metrics.md); * [Best practices for multi-sample variant calling](trio-merge-case-study.md); * [Using graph genomes: VG Giraffe + DeepVariant case study](deepvariant-vg-case-study.md). ## Visualization and analysis. * [show_examples: Saving human-readable images from DeepVariant examples](show-examples.md); * [VCF stats report](deepvariant-vcf-stats-report.md); * [Runtime by region for make_examples](runtime-by-region.md). ### Colab notebooks. * [Colab example: visualizing pileup images/tensors](visualizing_examples.ipynb); * [Can you beat DeepVariant?: A look inside the classification task](cybdv_notebook.ipynb); * [Google Developer Codelab: Variant Calling on a Rice genome with DeepVariant](https://codelabs.developers.google.com/codelabs/genomics-deepvariant). ## (Advanced) Training. * [Advanced Case Study: Train a customized SNP and small indel variant caller; for BGISEQ-500 data](deepvariant-training-case-study.md); * [DeepVariant training data](deepvariant-details-training-data.md). ## More details. * [DeepVariant usage guide](deepvariant-details.md); * [Building and testing De
",False,"The text contains links to various documentation resources, case studies, quick starts, and Colab notebooks. While it may have some technical content, the majority consists of descriptive and explanatory information meant for humans to understand and use DeepVariant. There are no code snippets, logs, or API specifications that would typically be eliminated."
Usability,"# Using graph genomes: VG Giraffe + DeepVariant case study; ---. This is an example to run `vg giraffe`, so we can go from FASTQs --> BAM. For simplicity and consistency, we run the following with a; [Google Cloud instance with 64 cores](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). I added more disks because 300G is not enough for the example below. I changed; it to `--boot-disk-size ""1000""`. ## Install softwares that will be used later. ```bash; sudo apt update -y; sudo apt-get -y install aria2 docker.io samtools; ```. ## Download input FASTQ files. ```bash; DATA_DIR=${PWD}/data; mkdir -p ${DATA_DIR}; gcloud storage cp gs://brain-genomics-public/research/sequencing/fastq/novaseq/wgs_pcr_free/35x/HG003.novaseq.pcr-free.35x.R?.fastq.gz ${DATA_DIR}/; ```. ## Download VG files. Get binaries `vg` 1.51.0 and `kmc`:. ```bash; wget https://github.com/refresh-bio/KMC/releases/download/v3.2.2/KMC3.2.2.linux.x64.tar.gz; tar zxf KMC3.2.2.linux.x64.tar.gz bin/kmc; mv bin/kmc ${DATA_DIR}/; wget https://github.com/vgteam/vg/releases/download/v1.51.0/vg -O ${DATA_DIR}/vg; chmod +x ${DATA_DIR}/vg ${DATA_DIR}/kmc; ```. Get the graph (.gbz) and haplotype index (.hapl).; I used `aria2c` to download these files. You can use other approaches as well. ```bash; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.gbz; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.hapl; ```. ## Run `vg giraffe` with one command to get from FASTQs to BAM. Put the paths name into a file named HG003.fq.paths:. ```bash; cat > HG003.fq.paths <<- EOM; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R1.fastq.gz; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R2.fastq.gz; EOM; ```. Run `kmc`` on this file. I used -t$(nproc) to use all cores, and $TMPDIR ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:143,simpl,simplicity,143,docs/deepvariant-vg-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md,1,['simpl'],['simplicity'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # Using graph genomes: VG Giraffe + DeepVariant case study; ---. This is an example to run `vg giraffe`, so we can go from FASTQs --> BAM. For simplicity and consistency, we run the following with a; [Google Cloud instance with 64 cores](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). I added more disks because 300G is not enough for the example below. I changed; it to `--boot-disk-size ""1000""`. ## Install softwares that will be used later. ```bash; sudo apt update -y; sudo apt-get -y install aria2 docker.io samtools; ```. ## Download input FASTQ files. ```bash; DATA_DIR=${PWD}/data; mkdir -p ${DATA_DIR}; gcloud storage cp gs://brain-genomics-public/research/sequencing/fastq/novaseq/wgs_pcr_free/35x/HG003.novaseq.pcr-free.35x.R?.fastq.gz ${DATA_DIR}/; ```. ## Download VG files. Get binaries `vg` 1.51.0 and `kmc`:. ```bash; wget https://github.com/refresh-bio/KMC/releases/download/v3.2.2/KMC3.2.2.linux.x64.tar.gz; tar zxf KMC3.2.2.linux.x64.tar.gz bin/kmc; mv bin/kmc ${DATA_DIR}/; wget https://github.com/vgteam/vg/releases/download/v1.51.0/vg -O ${DATA_DIR}/vg; chmod +x ${DATA_DIR}/vg ${DATA_DIR}/kmc; ```. Get the graph (.gbz) and haplotype index (.hapl).; I used `aria2c` to download these files. You can use other approaches as well. ```bash; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.gbz; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.hapl; ```. ## Run `vg giraffe` with one command to get from FASTQs to BAM. Put the paths name into a file named HG003.fq.paths:. ```bash; cat > HG003.fq.paths <<- EOM; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R1.fastq.gz; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R2.fastq.gz; EOM; ```. Run `kmc`` on this file. I used -t$(nproc) to use all cores, and $TMPDIR 
",False,"The text contains detailed step-by-step instructions for installing software, downloading data, and running specific commands to process FASTQ files into BAM format using tools like vg giraffe and kmc. While the text includes code snippets and command lines, it also provides explanatory context about each step and why certain configurations are chosen. The content is structured as complete sentences discussing how to perform computational tasks, which meets the criteria for keeping human-readable text."
Usability,"# show_examples: Saving human-readable images from DeepVariant examples. This is a short guide to using the show_examples tool to view the pileup images; used within DeepVariant and save them as PNG image files. This tool is; particularly useful when you want to try to understand how a candidate variant; of interest was represented when it was passed into the neural network. ![An example pileup image](images/example_1.4.0.png). This example was generated with the data from the; [quick start guide](deepvariant-quick-start.md) and the example commands below. For more information on the pileup images and how to read them, please see the; [""Looking through DeepVariant's Eyes"" blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). The `show_examples` tool is introduced in DeepVariant 1.0.0, so it is not; available in older versions, but it will work with make_examples output files; from older versions of DeepVariant. ## Finding the make_examples output tfrecord files. First, find the make_examples.tfrecord.gz files output by DeepVariant during the; make_examples (first) stage. If you followed along with the [quick start guide](deepvariant-quick-start.md); and case studies that used the Docker version, then these files are usually; hidden inside the Docker container. But you can get them exported into the same; output directory where the VCF file appears by adding the following setting in; the `run_deepvariant` command. ```bash; # Add the following to your run_deepvariant command.; --intermediate_results_dir=/output/; ```. Then the make_examples file should appear in the directory docker mounted as; `/output/`. For example, if you followed the; [quick-start documentation](deepvariant-quick-start.md), it looks like this:; `${OUTPUT_DIR}/make_examples.tfrecord-00000-of-00001.gz`. ## Running show_examples. Once you have a make_examples output tfrecord file, then you can run; `show_examples` to see the pileup images inside:. ```ba",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md:89,guid,guide,89,docs/show-examples.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md,2,['guid'],['guide'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: # show_examples: Saving human-readable images from DeepVariant examples. This is a short guide to using the show_examples tool to view the pileup images; used within DeepVariant and save them as PNG image files. This tool is; particularly useful when you want to try to understand how a candidate variant; of interest was represented when it was passed into the neural network. ![An example pileup image](images/example_1.4.0.png). This example was generated with the data from the; [quick start guide](deepvariant-quick-start.md) and the example commands below. For more information on the pileup images and how to read them, please see the; [""Looking through DeepVariant's Eyes"" blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). The `show_examples` tool is introduced in DeepVariant 1.0.0, so it is not; available in older versions, but it will work with make_examples output files; from older versions of DeepVariant. ## Finding the make_examples output tfrecord files. First, find the make_examples.tfrecord.gz files output by DeepVariant during the; make_examples (first) stage. If you followed along with the [quick start guide](deepvariant-quick-start.md); and case studies that used the Docker version, then these files are usually; hidden inside the Docker container. But you can get them exported into the same; output directory where the VCF file appears by adding the following setting in; the `run_deepvariant` command. ```bash; # Add the following to your run_deepvariant command.; --intermediate_results_dir=/output/; ```. Then the make_examples file should appear in the directory docker mounted as; `/output/`. For example, if you followed the; [quick-start documentation](deepvariant-quick-start.md), it looks like this:; `${OUTPUT_DIR}/make_examples.tfrecord-00000-of-00001.gz`. ## Running show_examples. Once you have a make_examples output tfrecord file, then you can run; `show_examples` to see the pileup images inside:. ```ba
",False,"The text consists of multiple paragraphs explaining how to use the 'show_examples' tool in DeepVariant. It includes instructions on finding and running the tool, along with links to additional resources for further information. The content is explanatory and written in natural language, making it human-readable and meaningful."
Usability,"-------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 29 | 0 | 0 | 1.0 | 1.0 | 1.0 |; | SNP | 683 | 2 | 0 | 0.99708 | 1.0 | 0.998538 |. #### HG004:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 32 | 1 | 1 | 0.969697 | 0.969697 | 0.969697 |; | SNP | 677 | 2 | 0 | 0.997054 | 1.0 | 0.998525 |. * See VCF stats report (for all chromosomes); - [HG002](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG002.output.visual_report.html); - [HG003](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG003.output.visual_report.html); - [HG004](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG004.output.visual_report.html). ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); For bigger datasets (WGS and PACBIO), we used bigger disk size (900G).; This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deeptrio.sh. # WGS; bash inference_deeptrio.sh --model_preset WGS. # WES; bash inference_deeptrio.sh --model_preset WES. # PacBio; bash inference_deeptrio.sh --model_preset PACBIO. ```. Runtime metrics are taken from the resulting log after each stage of; DeepTrio. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 runs produced the same output. [CPU instance with 64 CPUs]: deepvariant-details.md#command-for-a-cpu-only-mac",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics-deeptrio.md:5850,simpl,simplicity,5850,docs/metrics-deeptrio.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics-deeptrio.md,1,['simpl'],['simplicity'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 29 | 0 | 0 | 1.0 | 1.0 | 1.0 |; | SNP | 683 | 2 | 0 | 0.99708 | 1.0 | 0.998538 |. #### HG004:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 32 | 1 | 1 | 0.969697 | 0.969697 | 0.969697 |; | SNP | 677 | 2 | 0 | 0.997054 | 1.0 | 0.998525 |. * See VCF stats report (for all chromosomes); - [HG002](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG002.output.visual_report.html); - [HG003](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG003.output.visual_report.html); - [HG004](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG004.output.visual_report.html). ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); For bigger datasets (WGS and PACBIO), we used bigger disk size (900G).; This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deeptrio.sh. # WGS; bash inference_deeptrio.sh --model_preset WGS. # WES; bash inference_deeptrio.sh --model_preset WES. # PacBio; bash inference_deeptrio.sh --model_preset PACBIO. ```. Runtime metrics are taken from the resulting log after each stage of; DeepTrio. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 runs produced the same output. [CPU instance with 64 CPUs]: deepvariant-details.md#command-for-a-cpu-only-mac
",False,"The text contains explanatory and descriptive content, including how to reproduce metrics, details about configurations, and usage instructions."
Usability,"---; name: 'Problem encountered while running DeepVariant'; about: 'Tell us what happened, so we can try to help'; title: ''; labels: ''; assignees: ''. ---. **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**. ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/.github/ISSUE_TEMPLATE/problem_report.md:280,clear,clear,280,.github/ISSUE_TEMPLATE/problem_report.md,,https://github.com/google/deepvariant/tree/v1.6.1/.github/ISSUE_TEMPLATE/problem_report.md,1,['clear'],['clear'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ---; name: 'Problem encountered while running DeepVariant'; about: 'Tell us what happened, so we can try to help'; title: ''; labels: ''; assignees: ''. ---. **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**. 
",False,
Usability,"<img src=""docs/images/dv_logo.png"" width=50% height=50%>. [![release](https://img.shields.io/badge/release-v1.6.1-green?logo=github)](https://github.com/google/deepvariant/releases); [![announcements](https://img.shields.io/badge/announcements-blue)](https://groups.google.com/d/forum/deepvariant-announcements); [![blog](https://img.shields.io/badge/blog-orange)](https://goo.gl/deepvariant). DeepVariant is a deep learning-based variant caller that takes aligned reads (in; BAM or CRAM format), produces pileup image tensors from them, classifies each; tensor using a convolutional neural network, and finally reports the results in; a standard VCF or gVCF file. DeepVariant supports germline variant-calling in diploid organisms. * NGS (Illumina or Element) data for either a; [whole genome](docs/deepvariant-case-study.md) or; [whole exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and; [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The mod",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:416,learn,learning-based,416,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['learn'],['learning-based'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: <img src=""docs/images/dv_logo.png"" width=50% height=50%>. [![release](https://img.shields.io/badge/release-v1.6.1-green?logo=github)](https://github.com/google/deepvariant/releases); [![announcements](https://img.shields.io/badge/announcements-blue)](https://groups.google.com/d/forum/deepvariant-announcements); [![blog](https://img.shields.io/badge/blog-orange)](https://goo.gl/deepvariant). DeepVariant is a deep learning-based variant caller that takes aligned reads (in; BAM or CRAM format), produces pileup image tensors from them, classifies each; tensor using a convolutional neural network, and finally reports the results in; a standard VCF or gVCF file. DeepVariant supports germline variant-calling in diploid organisms. * NGS (Illumina or Element) data for either a; [whole genome](docs/deepvariant-case-study.md) or; [whole exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and; [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The mod
",False,"The text contains human-readable sentences discussing the capabilities and use cases of DeepVariant in various genomic data contexts. It includes links to case studies but the main content is descriptive and explanatory, making it meaningful for human readers."
Usability,"GS).; * Illumina whole exome data (WES).; * PacBio HiFi whole genome data (PacBio WGS). ## Running DeepTrio. The easiest and recommended way to run DeepTrio is using; `google/deepvariant:deeptrio-latest` docker image. Please refer to the; [quick start guide](deeptrio-quick-start.md) for more details on how to run; DeepTrio using docker. Merging VCFs can be done using; [GLnexus](https://github.com/dnanexus-rnd/GLnexus) which has been optimized for; use with DeepVariant gVCFs. The process is described in the DeepTrio case; studies; ([DeepTrio whole genome sequencing case study](deeptrio-wgs-case-study.md) and; [Using DeepTrio for small variant calling from the trio sequenced with PacBio; HiFi](deeptrio-pacbio-case-study.md)), and in the manuscript,; [""Accurate, scalable cohort variant calls using DeepVariant and GLnexus""](https://www.biorxiv.org/content/10.1101/2020.02.10.942086v2). Please note that DeepTrio can be run with a `run_deeptrio.py` script that; automates all DeepTrio steps and thus greatly simplifies the inference pipeline.; The details of using this script can be found in the section below as well as in; the DeepTrio case studies. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we; recommend running these providing only the parent who contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot p",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:2754,simpl,simplifies,2754,docs/deeptrio-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md,1,['simpl'],['simplifies'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: GS).; * Illumina whole exome data (WES).; * PacBio HiFi whole genome data (PacBio WGS). ## Running DeepTrio. The easiest and recommended way to run DeepTrio is using; `google/deepvariant:deeptrio-latest` docker image. Please refer to the; [quick start guide](deeptrio-quick-start.md) for more details on how to run; DeepTrio using docker. Merging VCFs can be done using; [GLnexus](https://github.com/dnanexus-rnd/GLnexus) which has been optimized for; use with DeepVariant gVCFs. The process is described in the DeepTrio case; studies; ([DeepTrio whole genome sequencing case study](deeptrio-wgs-case-study.md) and; [Using DeepTrio for small variant calling from the trio sequenced with PacBio; HiFi](deeptrio-pacbio-case-study.md)), and in the manuscript,; [""Accurate, scalable cohort variant calls using DeepVariant and GLnexus""](https://www.biorxiv.org/content/10.1101/2020.02.10.942086v2). Please note that DeepTrio can be run with a `run_deeptrio.py` script that; automates all DeepTrio steps and thus greatly simplifies the inference pipeline.; The details of using this script can be found in the section below as well as in; the DeepTrio case studies. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we; recommend running these providing only the parent who contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot p
",False,"The text contains complete, meaningful sentences in natural language discussing concepts and ideas related to DeepTrio usage, including explanations of how to run the tool, input assumptions, and recommended practices. There are no code snippets, logs, API documentation, or other programmatic elements present."
Usability,"aligned to the reference genome; described above. The output of DeepTrio is a set of variants in; [VCF](https://samtools.github.io/hts-specs/VCFv4.3.pdf) format representing the; child and one or two parents. Similar to DeepVariant, DeepTrio is composed of three stages: `make_examples`,; `call_variants`, and `postprocess_variants`. Some of the components (; `call_variants`, `postprocess_variants`) are shared with DeepVariant, and; `make_examples` is specialized for DeepTrio. More details about each program are; described in detail in the; [Inputs and outputs](deepvariant-details.md#inputs-and-outputs) section of the; DeepVariant documentation. DeepTrio comes with three models for different types of input data:. * Illumina whole genome data (WGS).; * Illumina whole exome data (WES).; * PacBio HiFi whole genome data (PacBio WGS). ## Running DeepTrio. The easiest and recommended way to run DeepTrio is using; `google/deepvariant:deeptrio-latest` docker image. Please refer to the; [quick start guide](deeptrio-quick-start.md) for more details on how to run; DeepTrio using docker. Merging VCFs can be done using; [GLnexus](https://github.com/dnanexus-rnd/GLnexus) which has been optimized for; use with DeepVariant gVCFs. The process is described in the DeepTrio case; studies; ([DeepTrio whole genome sequencing case study](deeptrio-wgs-case-study.md) and; [Using DeepTrio for small variant calling from the trio sequenced with PacBio; HiFi](deeptrio-pacbio-case-study.md)), and in the manuscript,; [""Accurate, scalable cohort variant calls using DeepVariant and GLnexus""](https://www.biorxiv.org/content/10.1101/2020.02.10.942086v2). Please note that DeepTrio can be run with a `run_deeptrio.py` script that; automates all DeepTrio steps and thus greatly simplifies the inference pipeline.; The details of using this script can be found in the section below as well as in; the DeepTrio case studies. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we; recommend",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:1991,guid,guide,1991,docs/deeptrio-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md,1,['guid'],['guide'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: aligned to the reference genome; described above. The output of DeepTrio is a set of variants in; [VCF](https://samtools.github.io/hts-specs/VCFv4.3.pdf) format representing the; child and one or two parents. Similar to DeepVariant, DeepTrio is composed of three stages: `make_examples`,; `call_variants`, and `postprocess_variants`. Some of the components (; `call_variants`, `postprocess_variants`) are shared with DeepVariant, and; `make_examples` is specialized for DeepTrio. More details about each program are; described in detail in the; [Inputs and outputs](deepvariant-details.md#inputs-and-outputs) section of the; DeepVariant documentation. DeepTrio comes with three models for different types of input data:. * Illumina whole genome data (WGS).; * Illumina whole exome data (WES).; * PacBio HiFi whole genome data (PacBio WGS). ## Running DeepTrio. The easiest and recommended way to run DeepTrio is using; `google/deepvariant:deeptrio-latest` docker image. Please refer to the; [quick start guide](deeptrio-quick-start.md) for more details on how to run; DeepTrio using docker. Merging VCFs can be done using; [GLnexus](https://github.com/dnanexus-rnd/GLnexus) which has been optimized for; use with DeepVariant gVCFs. The process is described in the DeepTrio case; studies; ([DeepTrio whole genome sequencing case study](deeptrio-wgs-case-study.md) and; [Using DeepTrio for small variant calling from the trio sequenced with PacBio; HiFi](deeptrio-pacbio-case-study.md)), and in the manuscript,; [""Accurate, scalable cohort variant calls using DeepVariant and GLnexus""](https://www.biorxiv.org/content/10.1101/2020.02.10.942086v2). Please note that DeepTrio can be run with a `run_deeptrio.py` script that; automates all DeepTrio steps and thus greatly simplifies the inference pipeline.; The details of using this script can be found in the section below as well as in; the DeepTrio case studies. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we; recommend
",False,"The text contains complete, meaningful sentences in natural language discussing concepts related to the use and functionality of DeepTrio, such as how it runs using Docker images, its components like make_examples, call_variants, postprocess_variants, and its models for different data types. It also provides resources like case studies and a reference paper, which are all examples of human-readable content that explain the tool's usage and purpose."
Usability,"contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot pipe them into DeepTrio. Duplicate marking may be; performed. In our analyses, there is almost no difference in accuracy with and; without duplicate marking except at lower (<20x) coverages. Finally, we; recommend that you do not perform BQSR. Running BQSR has a small decrease on; accuracy. If you are providing `--regions` or other similar arguments, these should refer; to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. ## Training data. DeepTrio models are trained using the latest publicly avavilable GIAB; benchmarks. You can find more details about the training data for each DeepTrio; model in the; [DeepTrio Training Data document](deeptrio-details-training-data.md). ## DeepVariant dependency. DeepTrio is built on top of DeepVariant and they share most of the components.; Please see [DeepVariant usage guide](deepvariant-details.md) for a full; description of DeepVariant components as well as other consideration for running; DeepVariant pipeline.; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:4885,guid,guide,4885,docs/deeptrio-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md,1,['guid'],['guide'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot pipe them into DeepTrio. Duplicate marking may be; performed. In our analyses, there is almost no difference in accuracy with and; without duplicate marking except at lower (<20x) coverages. Finally, we; recommend that you do not perform BQSR. Running BQSR has a small decrease on; accuracy. If you are providing `--regions` or other similar arguments, these should refer; to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. ## Training data. DeepTrio models are trained using the latest publicly avavilable GIAB; benchmarks. You can find more details about the training data for each DeepTrio; model in the; [DeepTrio Training Data document](deeptrio-details-training-data.md). ## DeepVariant dependency. DeepTrio is built on top of DeepVariant and they share most of the components.; Please see [DeepVariant usage guide](deepvariant-details.md) for a full; description of DeepVariant components as well as other consideration for running; DeepVariant pipeline.; 
",False,"The text includes sentences that are complete and meaningful, discussing concepts related to the use of DeepTrio in bioinformatics. There are no code snippets or logs present. The content is explanatory and descriptive, suitable for a human-readable document."
Usability,"d** - See [metrics](docs/metrics.md) for the runtime of all supported; datatypes on a 64-core CPU-only machine</sup>. Multiple options for; acceleration exist.; * **Usage options** - DeepVariant can be run via Docker or binaries, using; both on-premise hardware or in the cloud, with support for hardware; accelerators like GPUs and TPUs. <a name=""myfootnote1"">(1)</a>: Time estimates do not include mapping. ## How DeepVariant works. ![Stages in DeepVariant](docs/images/inference_flow_diagram.svg). For more information on the pileup images and how to read them, please see the; [""Looking through DeepVariant's Eyes"" blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant relies on [Nucleus](https://github.com/google/nucleus), a library of; Python and C++ code for reading and writing data in common genomics file formats; (like SAM and VCF) designed for painless integration with the; [TensorFlow](https://www.tensorflow.org/) machine learning framework. Nucleus; was built with DeepVariant in mind and open-sourced separately so it can be used; by anyone in the genomics research community for other projects. See this blog; post on; [Using Nucleus and TensorFlow for DNA Sequencing Error Correction](https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/). ## DeepVariant Setup. ### Prerequisites. * Unix-like operating system (cannot run on Windows); * Python 3.8. ### Official Solutions. Below are the official solutions provided by the; [Genomics team in Google Health](https://health.google/health-research/). Name | Description; :-------------------------------------------------------------------------------------------------: | -----------; [Docker](docs/deepvariant-quick-start.md) | This is the recommended method.; [Build from source](docs/deepvariant-build-test.md) | DeepVariant comes with scripts to build it on Ubuntu 20.04. To build and run on other Unix-",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:9552,learn,learning,9552,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['learn'],['learning'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: d** - See [metrics](docs/metrics.md) for the runtime of all supported; datatypes on a 64-core CPU-only machine</sup>. Multiple options for; acceleration exist.; * **Usage options** - DeepVariant can be run via Docker or binaries, using; both on-premise hardware or in the cloud, with support for hardware; accelerators like GPUs and TPUs. <a name=""myfootnote1"">(1)</a>: Time estimates do not include mapping. ## How DeepVariant works. ![Stages in DeepVariant](docs/images/inference_flow_diagram.svg). For more information on the pileup images and how to read them, please see the; [""Looking through DeepVariant's Eyes"" blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant relies on [Nucleus](https://github.com/google/nucleus), a library of; Python and C++ code for reading and writing data in common genomics file formats; (like SAM and VCF) designed for painless integration with the; [TensorFlow](https://www.tensorflow.org/) machine learning framework. Nucleus; was built with DeepVariant in mind and open-sourced separately so it can be used; by anyone in the genomics research community for other projects. See this blog; post on; [Using Nucleus and TensorFlow for DNA Sequencing Error Correction](https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/). ## DeepVariant Setup. ### Prerequisites. * Unix-like operating system (cannot run on Windows); * Python 3.8. ### Official Solutions. Below are the official solutions provided by the; [Genomics team in Google Health](https://health.google/health-research/). Name | Description; :-------------------------------------------------------------------------------------------------: | -----------; [Docker](docs/deepvariant-quick-start.md) | This is the recommended method.; [Build from source](docs/deepvariant-build-test.md) | DeepVariant comes with scripts to build it on Ubuntu 20.04. To build and run on other Unix-
",False,"The text contains a mix of natural language sentences discussing the functionality and setup of DeepVariant, along with some technical terms and links, but overall maintains meaningful human-readable content. It includes explanatory paragraphs about how DeepVariant works, its prerequisites, official solutions, and references to related materials. The structure is narrative in nature rather than presenting code snippets or logs. Therefore, it should not be filtered out."
Usability,"d; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 30x whole genome sample in BAM and CRAM format. Filetype | Size (Gb) | Runtime (min); -------- | --------- | -------------; BAM | 66.99 | 79m47.37307s; CRAM | 37.85 | 96m53.477s; Ratio | 56.50% | 121.43%. * BAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" samtools; ```. and. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" bcftools; ```. You can read more about samtools and bcftools here: http://www.htslib.org/doc/. ## Commands for requesting machines used in case studies. We report runtime in our case studies documentation. In order to make sure the; results we report are reproducible without too much variation, we provide the; commands we used here to show you what kind of machines we ran the case studies; on. This is NOT the fastest or cheapest configuration. ### Command for a CPU-only machine on Google Cloud Platform. We used a 64-core (vCPU) machine with 240GiB of memory and no GPU, on the Google; Cloud Platform. Specifying the CPU platform also allows us to report the runtime; more consistently. ```shell; gcloud compute instances create ""${USER}-cpu""",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:10659,feedback,feedback,10659,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['feedback'],['feedback'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: d; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 30x whole genome sample in BAM and CRAM format. Filetype | Size (Gb) | Runtime (min); -------- | --------- | -------------; BAM | 66.99 | 79m47.37307s; CRAM | 37.85 | 96m53.477s; Ratio | 56.50% | 121.43%. * BAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" samtools; ```. and. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" bcftools; ```. You can read more about samtools and bcftools here: http://www.htslib.org/doc/. ## Commands for requesting machines used in case studies. We report runtime in our case studies documentation. In order to make sure the; results we report are reproducible without too much variation, we provide the; commands we used here to show you what kind of machines we ran the case studies; on. This is NOT the fastest or cheapest configuration. ### Command for a CPU-only machine on Google Cloud Platform. We used a 64-core (vCPU) machine with 240GiB of memory and no GPU, on the Google; Cloud Platform. Specifying the CPU platform also allows us to report the runtime; more consistently. ```shell; gcloud compute instances create ""${USER}-cpu""
",True,"The text contains code snippets (e.g., bash commands and shell scripts), logs or performance metrics (file sizes, runtimes), and API documentation links which are more programmatic in nature. The content also includes tables and references to specific tools and their usage, which aligns with a programming or technical documentation style rather than a human-readable narrative."
Usability,"e a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:1913,guid,guide,1913,docs/deeptrio-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md,1,['guid'],['guide'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: e a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark
",False,"The content consists of step-by-step instructions for downloading test data, including code snippets and commands. However, the text also contains meaningful sentences discussing how to obtain necessary input files and prepare data for analysis. While there is a significant portion that is code-like, the overall context is explanatory and instructive. Therefore, it should not be entirely filtered out."
Usability,e study](deepvariant-case-study.md); * [DeepVariant exome case study](deepvariant-exome-case-study.md); * [DeepVariant PacBio case study](deepvariant-pacbio-model-case-study.md); * [DeepVariant ONT R10.4 simplex case study](deepvariant-ont-r104-simplex-case-study.md); [DeepVariant ONT R10.4 duplex case study](deepvariant-ont-r104-duplex-case-study.md); * [DeepVariant hybrid (PacBio and Illumina) case study](deepvariant-hybrid-case-study.md); * [DeepVariant Complete Genomics T7 case study](deepvariant-complete-t7-case-study.md); * [DeepVariant Complete Genomics G400 case study](deepvariant-complete-g400-case-study.md); * [Runtime and accuracy metrics for all DeepVariant models](metrics.md); * [Best practices for multi-sample variant calling](trio-merge-case-study.md); * [Using graph genomes: VG Giraffe + DeepVariant case study](deepvariant-vg-case-study.md). ## Visualization and analysis. * [show_examples: Saving human-readable images from DeepVariant examples](show-examples.md); * [VCF stats report](deepvariant-vcf-stats-report.md); * [Runtime by region for make_examples](runtime-by-region.md). ### Colab notebooks. * [Colab example: visualizing pileup images/tensors](visualizing_examples.ipynb); * [Can you beat DeepVariant?: A look inside the classification task](cybdv_notebook.ipynb); * [Google Developer Codelab: Variant Calling on a Rice genome with DeepVariant](https://codelabs.developers.google.com/codelabs/genomics-deepvariant). ## (Advanced) Training. * [Advanced Case Study: Train a customized SNP and small indel variant caller; for BGISEQ-500 data](deepvariant-training-case-study.md); * [DeepVariant training data](deepvariant-details-training-data.md). ## More details. * [DeepVariant usage guide](deepvariant-details.md); * [Building and testing DeepVariant](deepvariant-build-test.md); * [DeepVariant Genomic VCF (gVCF) support](deepvariant-gvcf-support.md); * [Getting Started with GCP](deepvariant-gcp-info.md) (It is not required to; run DeepVariant on GCP.); ,MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/README.md:1943,guid,guide,1943,docs/README.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/README.md,1,['guid'],['guide'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: e study](deepvariant-case-study.md); * [DeepVariant exome case study](deepvariant-exome-case-study.md); * [DeepVariant PacBio case study](deepvariant-pacbio-model-case-study.md); * [DeepVariant ONT R10.4 simplex case study](deepvariant-ont-r104-simplex-case-study.md); [DeepVariant ONT R10.4 duplex case study](deepvariant-ont-r104-duplex-case-study.md); * [DeepVariant hybrid (PacBio and Illumina) case study](deepvariant-hybrid-case-study.md); * [DeepVariant Complete Genomics T7 case study](deepvariant-complete-t7-case-study.md); * [DeepVariant Complete Genomics G400 case study](deepvariant-complete-g400-case-study.md); * [Runtime and accuracy metrics for all DeepVariant models](metrics.md); * [Best practices for multi-sample variant calling](trio-merge-case-study.md); * [Using graph genomes: VG Giraffe + DeepVariant case study](deepvariant-vg-case-study.md). ## Visualization and analysis. * [show_examples: Saving human-readable images from DeepVariant examples](show-examples.md); * [VCF stats report](deepvariant-vcf-stats-report.md); * [Runtime by region for make_examples](runtime-by-region.md). ### Colab notebooks. * [Colab example: visualizing pileup images/tensors](visualizing_examples.ipynb); * [Can you beat DeepVariant?: A look inside the classification task](cybdv_notebook.ipynb); * [Google Developer Codelab: Variant Calling on a Rice genome with DeepVariant](https://codelabs.developers.google.com/codelabs/genomics-deepvariant). ## (Advanced) Training. * [Advanced Case Study: Train a customized SNP and small indel variant caller; for BGISEQ-500 data](deepvariant-training-case-study.md); * [DeepVariant training data](deepvariant-details-training-data.md). ## More details. * [DeepVariant usage guide](deepvariant-details.md); * [Building and testing DeepVariant](deepvariant-build-test.md); * [DeepVariant Genomic VCF (gVCF) support](deepvariant-gvcf-support.md); * [Getting Started with GCP](deepvariant-gcp-info.md) (It is not required to; run DeepVariant on GCP.); 
",True,"The text consists of file names and references, which are primarily programmatic or related to documentation rather than containing meaningful human-readable sentences."
Usability,"es on one replicate of HG001 sequenced by; BGISEQ-500 provided on the; [Genome In a Bottle FTP site](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/BGISEQ500/standard_library/readme.txt). In this tutorial, we will split the genome up into the following datasets:. | chrom | Name | Description |; | ----- | --------------------- | -------------------------------------------- |; | chr1 | Training Set | Examples used to train our model. |; | chr21 | Validation / Tune Set | Examples used to evaluate the performance of our model during training.|; | chr20 | Test Set | Examples reserved for testing performance of our trained model. |. Note that normally, the training dataset will be much larger (e.g. chr1-19),; rather than just a single chromosome. We use just chr1 here to demonstrate how; customized training works. For the definition of these 3 sets in commonly used machine learning; terminology, please refer to; [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary/). ### Training set. First, to set up, lets pull the docker images. ```bash; sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image.; sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image.; ```. The `make_examples` step doesn't use GPU, so we will not require the GPU-enabled; image. ```bash; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log""; ```. This took `20m14s`. Starting in v1.4.0, we added an extra channel in our WGS setting using the; `--channels ""insert_size""` flag. And, the make_examples step creates; `*.example_info",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:5298,learn,learning,5298,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['learn'],['learning'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: es on one replicate of HG001 sequenced by; BGISEQ-500 provided on the; [Genome In a Bottle FTP site](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/BGISEQ500/standard_library/readme.txt). In this tutorial, we will split the genome up into the following datasets:. | chrom | Name | Description |; | ----- | --------------------- | -------------------------------------------- |; | chr1 | Training Set | Examples used to train our model. |; | chr21 | Validation / Tune Set | Examples used to evaluate the performance of our model during training.|; | chr20 | Test Set | Examples reserved for testing performance of our trained model. |. Note that normally, the training dataset will be much larger (e.g. chr1-19),; rather than just a single chromosome. We use just chr1 here to demonstrate how; customized training works. For the definition of these 3 sets in commonly used machine learning; terminology, please refer to; [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary/). ### Training set. First, to set up, lets pull the docker images. ```bash; sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image.; sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image.; ```. The `make_examples` step doesn't use GPU, so we will not require the GPU-enabled; image. ```bash; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log""; ```. This took `20m14s`. Starting in v1.4.0, we added an extra channel in our WGS setting using the; `--channels ""insert_size""` flag. And, the make_examples step creates; `*.example_info
",False,
Usability,"f# DeepVariant usage guide. ## Overview. DeepVariant is a set of programs used to transform aligned sequencing reads into; variant calls. At the highest level, a user needs to provide three inputs:. 1. A reference genome in [FASTA](https://en.wikipedia.org/wiki/FASTA_format); format and its corresponding; [.fai index file](http://www.htslib.org/doc/faidx.html) generated using the; `samtools faidx` command. 1. An aligned reads file in [BAM](http://genome.sph.umich.edu/wiki/BAM) format; and its corresponding index file (.bai). The reads must be aligned to the; reference genome described above. 1. A model checkpoint for DeepVariant. The output of DeepVariant is a list of all variant calls in; [VCF](https://samtools.github.io/hts-specs/VCFv4.3.pdf) format. DeepVariant is composed of three programs: `make_examples`, `call_variants`, and; `postprocess_variants`. More details about each program are described in detail; in the [Inputs and outputs](#inputs-and-outputs) section. ## Inputs and outputs. ### General notes. * Sharded files are a single logical collection of files with a common naming; convention. For example, we talk about `filename@10` as a single 10-way; sharded file named `filename`. On most filesystems this actually looks like; 10 distinct files `filename-00000-of-00010`, ..., `filename-00009-of-00010`.; DeepVariant can write sharded files using their `filename@10`-style name and; can read sharded files using both that style as well as the glob form, such; as `filename-*` or `filename-*-of-00010`.; * Files with the `.gz` suffix are interpreted as being compressed with gzip; and are read/written accordingly. ### make_examples. `make_examples` consumes reads and the reference genome to create TensorFlow; examples for evaluation with our deep learning models. The tf.Example protos are; written out in TFRecord format. To learn more about tf.Example and TFRecord, see; the; [Using TFRecords and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord); Co",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:21,guid,guide,21,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['guid'],['guide'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: f# DeepVariant usage guide. ## Overview. DeepVariant is a set of programs used to transform aligned sequencing reads into; variant calls. At the highest level, a user needs to provide three inputs:. 1. A reference genome in [FASTA](https://en.wikipedia.org/wiki/FASTA_format); format and its corresponding; [.fai index file](http://www.htslib.org/doc/faidx.html) generated using the; `samtools faidx` command. 1. An aligned reads file in [BAM](http://genome.sph.umich.edu/wiki/BAM) format; and its corresponding index file (.bai). The reads must be aligned to the; reference genome described above. 1. A model checkpoint for DeepVariant. The output of DeepVariant is a list of all variant calls in; [VCF](https://samtools.github.io/hts-specs/VCFv4.3.pdf) format. DeepVariant is composed of three programs: `make_examples`, `call_variants`, and; `postprocess_variants`. More details about each program are described in detail; in the [Inputs and outputs](#inputs-and-outputs) section. ## Inputs and outputs. ### General notes. * Sharded files are a single logical collection of files with a common naming; convention. For example, we talk about `filename@10` as a single 10-way; sharded file named `filename`. On most filesystems this actually looks like; 10 distinct files `filename-00000-of-00010`, ..., `filename-00009-of-00010`.; DeepVariant can write sharded files using their `filename@10`-style name and; can read sharded files using both that style as well as the glob form, such; as `filename-*` or `filename-*-of-00010`.; * Files with the `.gz` suffix are interpreted as being compressed with gzip; and are read/written accordingly. ### make_examples. `make_examples` consumes reads and the reference genome to create TensorFlow; examples for evaluation with our deep learning models. The tf.Example protos are; written out in TFRecord format. To learn more about tf.Example and TFRecord, see; the; [Using TFRecords and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord); Co
",False,"The text provided is a guide for using DeepVariant, which includes detailed steps, inputs, outputs, and explanations. It uses natural language to describe the process and does not contain code snippets or logs."
Usability,"hat you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure; ```. NOTE: To create an instance *without GPU*, simply omit the last line from the; command. Check that the instance has been created and started:. ```shell; gcloud compute instances list; ```. which should produce output like:. ```; NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS; [USER]-deepvariant-quickstart us-west1-b n1-standard-8 10.138.0.4 35.185.203.59 RUNNING; ```. Then connect to your instance via SSH:. ```shell; gcloud compute ssh --zone us-west1-b ""${USER}-deepvariant-quickstart""; ```. You should land at a shell prompt in your new instance!. NOTE: All of these steps can also be completed from the Cloud Console, if you; prefer. Consult [this; guide](https://cloud.google.com/compute/docs/quickstart-linux), but be sure to; choose Ubuntu 20.04 as your image, as DeepVariant has not been tested on other; Linux distributions. For more information about getting started with Compute Engine, see:. * [Compute Engine instance creation in `gcloud`; manual](https://cloud.google.com/sdk/gcloud/reference/compute/instances/create); * [Reference to machine; sizes/types](https://cloud.google.com/compute/docs/machine-types); ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:4854,guid,guide,4854,docs/deepvariant-gcp-info.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md,1,['guid'],['guide'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: hat you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure; ```. NOTE: To create an instance *without GPU*, simply omit the last line from the; command. Check that the instance has been created and started:. ```shell; gcloud compute instances list; ```. which should produce output like:. ```; NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS; [USER]-deepvariant-quickstart us-west1-b n1-standard-8 10.138.0.4 35.185.203.59 RUNNING; ```. Then connect to your instance via SSH:. ```shell; gcloud compute ssh --zone us-west1-b ""${USER}-deepvariant-quickstart""; ```. You should land at a shell prompt in your new instance!. NOTE: All of these steps can also be completed from the Cloud Console, if you; prefer. Consult [this; guide](https://cloud.google.com/compute/docs/quickstart-linux), but be sure to; choose Ubuntu 20.04 as your image, as DeepVariant has not been tested on other; Linux distributions. For more information about getting started with Compute Engine, see:. * [Compute Engine instance creation in `gcloud`; manual](https://cloud.google.com/sdk/gcloud/reference/compute/instances/create); * [Reference to machine; sizes/types](https://cloud.google.com/compute/docs/machine-types); 
",False,"The text contains detailed, human-readable instructions for setting up a Google Cloud Compute Engine instance, including step-by-step commands and explanations. This is explanatory content that provides value to users without requiring programming knowledge or code analysis."
Usability,"iant and save them as PNG image files. This tool is; particularly useful when you want to try to understand how a candidate variant; of interest was represented when it was passed into the neural network. ![An example pileup image](images/example_1.4.0.png). This example was generated with the data from the; [quick start guide](deepvariant-quick-start.md) and the example commands below. For more information on the pileup images and how to read them, please see the; [""Looking through DeepVariant's Eyes"" blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). The `show_examples` tool is introduced in DeepVariant 1.0.0, so it is not; available in older versions, but it will work with make_examples output files; from older versions of DeepVariant. ## Finding the make_examples output tfrecord files. First, find the make_examples.tfrecord.gz files output by DeepVariant during the; make_examples (first) stage. If you followed along with the [quick start guide](deepvariant-quick-start.md); and case studies that used the Docker version, then these files are usually; hidden inside the Docker container. But you can get them exported into the same; output directory where the VCF file appears by adding the following setting in; the `run_deepvariant` command. ```bash; # Add the following to your run_deepvariant command.; --intermediate_results_dir=/output/; ```. Then the make_examples file should appear in the directory docker mounted as; `/output/`. For example, if you followed the; [quick-start documentation](deepvariant-quick-start.md), it looks like this:; `${OUTPUT_DIR}/make_examples.tfrecord-00000-of-00001.gz`. ## Running show_examples. Once you have a make_examples output tfrecord file, then you can run; `show_examples` to see the pileup images inside:. ```bash; # Continuing from the quick start linked above:; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". BIN_VERSION=""1.6.1"" # show_examples is a",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md:1180,guid,guide,1180,docs/show-examples.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md,1,['guid'],['guide'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: iant and save them as PNG image files. This tool is; particularly useful when you want to try to understand how a candidate variant; of interest was represented when it was passed into the neural network. ![An example pileup image](images/example_1.4.0.png). This example was generated with the data from the; [quick start guide](deepvariant-quick-start.md) and the example commands below. For more information on the pileup images and how to read them, please see the; [""Looking through DeepVariant's Eyes"" blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). The `show_examples` tool is introduced in DeepVariant 1.0.0, so it is not; available in older versions, but it will work with make_examples output files; from older versions of DeepVariant. ## Finding the make_examples output tfrecord files. First, find the make_examples.tfrecord.gz files output by DeepVariant during the; make_examples (first) stage. If you followed along with the [quick start guide](deepvariant-quick-start.md); and case studies that used the Docker version, then these files are usually; hidden inside the Docker container. But you can get them exported into the same; output directory where the VCF file appears by adding the following setting in; the `run_deepvariant` command. ```bash; # Add the following to your run_deepvariant command.; --intermediate_results_dir=/output/; ```. Then the make_examples file should appear in the directory docker mounted as; `/output/`. For example, if you followed the; [quick-start documentation](deepvariant-quick-start.md), it looks like this:; `${OUTPUT_DIR}/make_examples.tfrecord-00000-of-00001.gz`. ## Running show_examples. Once you have a make_examples output tfrecord file, then you can run; `show_examples` to see the pileup images inside:. ```bash; # Continuing from the quick start linked above:; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". BIN_VERSION=""1.6.1"" # show_examples is a
",False,"The provided text contains complete, meaningful sentences in natural language discussing the usage and functionality of tools related to DeepVariant. It includes explanatory content and human-readable prose without code snippets or logs."
Usability,"iant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The models included with DeepVariant are only trained on human data. For; other organisms, see the; [blog post on non-human variant-calling](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/); for some possible pitfalls and how to handle them. ## DeepTrio. DeepTrio is a deep learning-based trio variant caller built on top of; DeepVariant. DeepTrio extends DeepVariant's functionality, allowing it to; utilize the power of neural networks to predict genomic variants in trios or; duos. See [this page](docs/deeptrio-details.md) for more details and; instructions on how to run DeepTrio. DeepTrio supports germline variant-calling in diploid organisms for the; following types of input data:. * NGS (Illumina) data for either; [whole genome](docs/deeptrio-wgs-case-study.md) or whole exome.; * PacBio HiFi data, see the; [PacBio case study](docs/deeptrio-pacbio-case-study.md). Please also note:. * All DeepTrio models were trained on human data.; * It is possible to use DeepTrio with only 2 samples (child, and one parent).; * External tool [GLnexus](https://github.com/dnanexus-rnd/GLnexus) is used to; merge output VCFs. ## How to run DeepVariant. We recommend using our Docker solution. The command will look like this:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:2352,learn,learning-based,2352,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['learn'],['learning-based'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: iant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The models included with DeepVariant are only trained on human data. For; other organisms, see the; [blog post on non-human variant-calling](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/); for some possible pitfalls and how to handle them. ## DeepTrio. DeepTrio is a deep learning-based trio variant caller built on top of; DeepVariant. DeepTrio extends DeepVariant's functionality, allowing it to; utilize the power of neural networks to predict genomic variants in trios or; duos. See [this page](docs/deeptrio-details.md) for more details and; instructions on how to run DeepTrio. DeepTrio supports germline variant-calling in diploid organisms for the; following types of input data:. * NGS (Illumina) data for either; [whole genome](docs/deeptrio-wgs-case-study.md) or whole exome.; * PacBio HiFi data, see the; [PacBio case study](docs/deeptrio-pacbio-case-study.md). Please also note:. * All DeepTrio models were trained on human data.; * It is possible to use DeepTrio with only 2 samples (child, and one parent).; * External tool [GLnexus](https://github.com/dnanexus-rnd/GLnexus) is used to; merge output VCFs. ## How to run DeepVariant. We recommend using our Docker solution. The command will look like this:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; 
",False,"The text contains meaningful sentences that discuss the functionality and usage of DeepTrio and DeepVariant tools in genomics, including their use cases, configurations, and limitations. It also includes links to documentation and setup instructions, which are explanatory and helpful for users."
Usability,"ies. In the case of rice,; there was good evidence of high accuracy. You can see; [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant).; However, these rice genomes were diploid and with a similar variant density of; humans. DeepVariant is currently written to be a diploid variant caller. So if the plant; species you are working with is polyploid, it is not yet clear how DeepVariant; will perform. That is because even with re-training, DeepVariant can only; produce variant calls that are homozygous alternate, heterozygous, or homozygous; reference, which don't have much meaning in a tetraploid genome, for example. ## Can I use DeepVariant on other non-human species?. See this; [blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). ## How do I build/run DeepVariant?. In general, we recommend running DeepVariant using Docker for the simplest; setup. If you are building from source because you want to experiment with; changes to the codebase, we still recommend Docker. You can clone the; DeepVariant repo, modify the source code, and build a Docker image with your; changes using the provided Dockerfile. ## Why can't it find one of the input files? E.g., ""Could not open"". This often happens because the way Docker works, input and output directories; have to be mounted and then files are referred to by their mounted location,; which can be confusing. To check that files are visible inside the Docker; container, you can `ls` inside the container. For example, using the setup shown; in the README and looking inside the `/input` volume:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; ls /input; ```. Mounting directories with Docker can be confusing. One trick to make this; simpler is to set both sides ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:7172,simpl,simplest,7172,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['simpl'],['simplest'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ies. In the case of rice,; there was good evidence of high accuracy. You can see; [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant).; However, these rice genomes were diploid and with a similar variant density of; humans. DeepVariant is currently written to be a diploid variant caller. So if the plant; species you are working with is polyploid, it is not yet clear how DeepVariant; will perform. That is because even with re-training, DeepVariant can only; produce variant calls that are homozygous alternate, heterozygous, or homozygous; reference, which don't have much meaning in a tetraploid genome, for example. ## Can I use DeepVariant on other non-human species?. See this; [blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). ## How do I build/run DeepVariant?. In general, we recommend running DeepVariant using Docker for the simplest; setup. If you are building from source because you want to experiment with; changes to the codebase, we still recommend Docker. You can clone the; DeepVariant repo, modify the source code, and build a Docker image with your; changes using the provided Dockerfile. ## Why can't it find one of the input files? E.g., ""Could not open"". This often happens because the way Docker works, input and output directories; have to be mounted and then files are referred to by their mounted location,; which can be confusing. To check that files are visible inside the Docker; container, you can `ls` inside the container. For example, using the setup shown; in the README and looking inside the `/input` volume:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; ls /input; ```. Mounting directories with Docker can be confusing. One trick to make this; simpler is to set both sides 
",False,
Usability,"in general; cases because the realigner improves accuracy overall. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. See the ""What is the realigner and how does it work?"" section; for instructions. **Missing variants where a candidate is generated:**. If a candidate is made, but is called as reference (either 0/0 or ./.) it means; that the neural network processed the genomic region, but based on all of its; learned experience from training data, it decided the highest probability for; the position was as non-variant. Some of the reasons that DeepVariant may; suspect a false positive are: strand-bias in reads, low mapping quality in; reads, low base quality in reads, and overall low coverage. In addition, there is another pattern that causes DeepVariant to suspect variant; positions which can initially seem counterintuitive to human observers. This; occurs when a dense set of variants appears on one haplotype while the other; haplotype is fully reference, and humans often perceive this as missing a; clearly heterozygous position. DeepVariant seems to have learned that this; signature often indicates a region which is a segmental duplication, copy number; variant, or structural variant where multiple copies of similar genomic regions; are mapping to the same reference location. In this case, it may be worthwhile; to inspect the region to see if it has elevated coverage, and whether you can; identify more than 2 haplotypes present by overlapping the reads. If you can, it; suggests that the region may have a copy number variation. Some analysis of this; was presented at AGBT as a poster; “[Uncaptured segmental duplication creates artifacts in workflows using GRCh37](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096)”. This pattern of undercalling positions at high variant density may affect; variant-dense non-human species (those with a variant density of >1 in 40; positions). For an analysis of this",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:2635,clear,clearly,2635,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['clear'],['clearly'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: in general; cases because the realigner improves accuracy overall. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. See the ""What is the realigner and how does it work?"" section; for instructions. **Missing variants where a candidate is generated:**. If a candidate is made, but is called as reference (either 0/0 or ./.) it means; that the neural network processed the genomic region, but based on all of its; learned experience from training data, it decided the highest probability for; the position was as non-variant. Some of the reasons that DeepVariant may; suspect a false positive are: strand-bias in reads, low mapping quality in; reads, low base quality in reads, and overall low coverage. In addition, there is another pattern that causes DeepVariant to suspect variant; positions which can initially seem counterintuitive to human observers. This; occurs when a dense set of variants appears on one haplotype while the other; haplotype is fully reference, and humans often perceive this as missing a; clearly heterozygous position. DeepVariant seems to have learned that this; signature often indicates a region which is a segmental duplication, copy number; variant, or structural variant where multiple copies of similar genomic regions; are mapping to the same reference location. In this case, it may be worthwhile; to inspect the region to see if it has elevated coverage, and whether you can; identify more than 2 haplotypes present by overlapping the reads. If you can, it; suggests that the region may have a copy number variation. Some analysis of this; was presented at AGBT as a poster; “[Uncaptured segmental duplication creates artifacts in workflows using GRCh37](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096)”. This pattern of undercalling positions at high variant density may affect; variant-dense non-human species (those with a variant density of >1 in 40; positions). For an analysis of this
",False,"The text contains complete, meaningful sentences discussing the functionality and behavior of DeepVariant. It includes explanations about potential errors and how to interpret results, written in a human-readable format."
Usability,"ll 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.ch",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:1896,guid,guide,1896,docs/deepvariant-quick-start.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md,1,['guid'],['guide'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ll 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.ch
",False,
Usability,"lls. Any entries without a DP are ignored. ### Quality score. This is the QUAL column of the VCF file. See the [VCF specification] for how; this is calculated, but in short, a high QUAL score indicates a low probability; that the call shown in ALT is wrong. This chart shows quality scores from all; rows of the VCF file, including RefCalls. ### Genotype quality. These numbers come from the GQ sub-column, listed in the FORMAT column of the; VCF file. An important distinction here is that if the variant-caller is very; sure that the variant is there, but not sure if it is heterozygous or; homozygous, the QUAL score can be very high, but the genotype quality score (GQ); can be low. GQ is on a Phred scale, calculated as -10*log10(probability that the; genotype is wrong). Any entries without a GQ are ignored. ### Variant allele frequency for all genotypes. The histograms show the variant allele frequency (VAF) distributions for; different genotypes. Black guiding lines are shown to indicate the theoretical; VAF for the main genotypes. For example heterozygous variants should have about; as many variant-supporting as reference-supporting reads, for a VAF of 0.5. The; reference calls will not usually show a VAF as low as 0 because otherwise they; wouldn’t have been flagged as candidates in the first place. The genotypes are; based on the GT sub-column and consolidated. For example, 0/1 and 0/2 both; become Het (0/x). 1/1 and 3/3 are Hom (x/x). Het - both variants (x/y) includes; all calls with two different alternate alleles, such as 1/2 or 3/5. ### Biallelic base changes. Of all biallelic SNPs, this shows the counts from a particular REF (along the; top labeling the four charts) to a particular ALT (each bar within the charts; labeled at the bottom). See the Ti/Tv section for a brief explanation of why; some of these base changes tend to be more frequent than others. RefCalls and; multi-allelic variants are not included. ### Biallelic Ti/Tv ratio. Transition (Ti) count is t",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vcf-stats-report.md:2902,guid,guiding,2902,docs/deepvariant-vcf-stats-report.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vcf-stats-report.md,1,['guid'],['guiding'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: lls. Any entries without a DP are ignored. ### Quality score. This is the QUAL column of the VCF file. See the [VCF specification] for how; this is calculated, but in short, a high QUAL score indicates a low probability; that the call shown in ALT is wrong. This chart shows quality scores from all; rows of the VCF file, including RefCalls. ### Genotype quality. These numbers come from the GQ sub-column, listed in the FORMAT column of the; VCF file. An important distinction here is that if the variant-caller is very; sure that the variant is there, but not sure if it is heterozygous or; homozygous, the QUAL score can be very high, but the genotype quality score (GQ); can be low. GQ is on a Phred scale, calculated as -10*log10(probability that the; genotype is wrong). Any entries without a GQ are ignored. ### Variant allele frequency for all genotypes. The histograms show the variant allele frequency (VAF) distributions for; different genotypes. Black guiding lines are shown to indicate the theoretical; VAF for the main genotypes. For example heterozygous variants should have about; as many variant-supporting as reference-supporting reads, for a VAF of 0.5. The; reference calls will not usually show a VAF as low as 0 because otherwise they; wouldn’t have been flagged as candidates in the first place. The genotypes are; based on the GT sub-column and consolidated. For example, 0/1 and 0/2 both; become Het (0/x). 1/1 and 3/3 are Hom (x/x). Het - both variants (x/y) includes; all calls with two different alternate alleles, such as 1/2 or 3/5. ### Biallelic base changes. Of all biallelic SNPs, this shows the counts from a particular REF (along the; top labeling the four charts) to a particular ALT (each bar within the charts; labeled at the bottom). See the Ti/Tv section for a brief explanation of why; some of these base changes tend to be more frequent than others. RefCalls and; multi-allelic variants are not included. ### Biallelic Ti/Tv ratio. Transition (Ti) count is t
",False,
Usability,"nd `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --acce",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:3054,simpl,simple,3054,docs/deepvariant-gcp-info.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md,1,['simpl'],['simple'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: nd `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --acce
",False,"The text contains a mix of installation instructions and descriptive content about using Google Cloud tools. It includes code snippets within backticks but also has explanatory paragraphs about setting up instances and verifying authentication. The content is meant to guide users through the setup process, including steps with commands, which are common in API documentation, yet it's accompanied by enough human-readable explanation to be considered meaningful. While primarily focused on commands, the presence of sentences explaining the purpose of each step means it should not be entirely filtered out."
Usability,"ns these `tensorflow.Example`s will; contain a `label` field). In this tutorial, we create examples on one replicate of HG001 sequenced by; BGISEQ-500 provided on the; [Genome In a Bottle FTP site](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/BGISEQ500/standard_library/readme.txt). In this tutorial, we will split the genome up into the following datasets:. | chrom | Name | Description |; | ----- | --------------------- | -------------------------------------------- |; | chr1 | Training Set | Examples used to train our model. |; | chr21 | Validation / Tune Set | Examples used to evaluate the performance of our model during training.|; | chr20 | Test Set | Examples reserved for testing performance of our trained model. |. Note that normally, the training dataset will be much larger (e.g. chr1-19),; rather than just a single chromosome. We use just chr1 here to demonstrate how; customized training works. For the definition of these 3 sets in commonly used machine learning; terminology, please refer to; [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary/). ### Training set. First, to set up, lets pull the docker images. ```bash; sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image.; sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image.; ```. The `make_examples` step doesn't use GPU, so we will not require the GPU-enabled; image. ```bash; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log""; ```. This took `20m14s`. Starting in v1.4.0, we added an extra channel in our WGS setting",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:5192,learn,learning,5192,docs/deepvariant-training-case-study.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md,1,['learn'],['learning'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ns these `tensorflow.Example`s will; contain a `label` field). In this tutorial, we create examples on one replicate of HG001 sequenced by; BGISEQ-500 provided on the; [Genome In a Bottle FTP site](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/BGISEQ500/standard_library/readme.txt). In this tutorial, we will split the genome up into the following datasets:. | chrom | Name | Description |; | ----- | --------------------- | -------------------------------------------- |; | chr1 | Training Set | Examples used to train our model. |; | chr21 | Validation / Tune Set | Examples used to evaluate the performance of our model during training.|; | chr20 | Test Set | Examples reserved for testing performance of our trained model. |. Note that normally, the training dataset will be much larger (e.g. chr1-19),; rather than just a single chromosome. We use just chr1 here to demonstrate how; customized training works. For the definition of these 3 sets in commonly used machine learning; terminology, please refer to; [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary/). ### Training set. First, to set up, lets pull the docker images. ```bash; sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image.; sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image.; ```. The `make_examples` step doesn't use GPU, so we will not require the GPU-enabled; image. ```bash; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log""; ```. This took `20m14s`. Starting in v1.4.0, we added an extra channel in our WGS setting
",False,
Usability,"r for the simplest; setup. If you are building from source because you want to experiment with; changes to the codebase, we still recommend Docker. You can clone the; DeepVariant repo, modify the source code, and build a Docker image with your; changes using the provided Dockerfile. ## Why can't it find one of the input files? E.g., ""Could not open"". This often happens because the way Docker works, input and output directories; have to be mounted and then files are referred to by their mounted location,; which can be confusing. To check that files are visible inside the Docker; container, you can `ls` inside the container. For example, using the setup shown; in the README and looking inside the `/input` volume:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; ls /input; ```. Mounting directories with Docker can be confusing. One trick to make this; simpler is to set both sides as your `$HOME`, so the paths are the same inside; and outside the Docker container. ```; echo $HOME # see what your home directory is first.; ls $HOME; BIN_VERSION=""1.6.1""; sudo docker run \; -v ""${HOME}"":""${HOME}"" \; google/deepvariant:""${BIN_VERSION}"" \; ls $HOME; ```. ## How do I run multi-sample calling?. Since the DeepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: C",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:8118,simpl,simpler,8118,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['simpl'],['simpler'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: r for the simplest; setup. If you are building from source because you want to experiment with; changes to the codebase, we still recommend Docker. You can clone the; DeepVariant repo, modify the source code, and build a Docker image with your; changes using the provided Dockerfile. ## Why can't it find one of the input files? E.g., ""Could not open"". This often happens because the way Docker works, input and output directories; have to be mounted and then files are referred to by their mounted location,; which can be confusing. To check that files are visible inside the Docker; container, you can `ls` inside the container. For example, using the setup shown; in the README and looking inside the `/input` volume:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; ls /input; ```. Mounting directories with Docker can be confusing. One trick to make this; simpler is to set both sides as your `$HOME`, so the paths are the same inside; and outside the Docker container. ```; echo $HOME # see what your home directory is first.; ls $HOME; BIN_VERSION=""1.6.1""; sudo docker run \; -v ""${HOME}"":""${HOME}"" \; google/deepvariant:""${BIN_VERSION}"" \; ls $HOME; ```. ## How do I run multi-sample calling?. Since the DeepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: C
",False,
Usability,"se sites, you may add the; following parameters, here shown with their defaults:. ```; --make_examples_extra_args=""vsc_min_count_snps=2,vsc_min_fraction_snps=0.12,vsc_min_count_indels=2,vsc_min_fraction_indels=0.06""; ```. It is sometimes also the case that realignment of the reads within DeepVariant; changes or reduces the evidence supporting the variant. To check for this, try; using the `--norealign_reads` flag to turn off realignment temporarily. Note; that we don't recommend turning off the realigner for Illumina data in general; cases because the realigner improves accuracy overall. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. See the ""What is the realigner and how does it work?"" section; for instructions. **Missing variants where a candidate is generated:**. If a candidate is made, but is called as reference (either 0/0 or ./.) it means; that the neural network processed the genomic region, but based on all of its; learned experience from training data, it decided the highest probability for; the position was as non-variant. Some of the reasons that DeepVariant may; suspect a false positive are: strand-bias in reads, low mapping quality in; reads, low base quality in reads, and overall low coverage. In addition, there is another pattern that causes DeepVariant to suspect variant; positions which can initially seem counterintuitive to human observers. This; occurs when a dense set of variants appears on one haplotype while the other; haplotype is fully reference, and humans often perceive this as missing a; clearly heterozygous position. DeepVariant seems to have learned that this; signature often indicates a region which is a segmental duplication, copy number; variant, or structural variant where multiple copies of similar genomic regions; are mapping to the same reference location. In this case, it may be worthwhile; to inspect the region to see if it has elevated coverage, and whether you can; identify ",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:2032,learn,learned,2032,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['learn'],['learned'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: se sites, you may add the; following parameters, here shown with their defaults:. ```; --make_examples_extra_args=""vsc_min_count_snps=2,vsc_min_fraction_snps=0.12,vsc_min_count_indels=2,vsc_min_fraction_indels=0.06""; ```. It is sometimes also the case that realignment of the reads within DeepVariant; changes or reduces the evidence supporting the variant. To check for this, try; using the `--norealign_reads` flag to turn off realignment temporarily. Note; that we don't recommend turning off the realigner for Illumina data in general; cases because the realigner improves accuracy overall. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. See the ""What is the realigner and how does it work?"" section; for instructions. **Missing variants where a candidate is generated:**. If a candidate is made, but is called as reference (either 0/0 or ./.) it means; that the neural network processed the genomic region, but based on all of its; learned experience from training data, it decided the highest probability for; the position was as non-variant. Some of the reasons that DeepVariant may; suspect a false positive are: strand-bias in reads, low mapping quality in; reads, low base quality in reads, and overall low coverage. In addition, there is another pattern that causes DeepVariant to suspect variant; positions which can initially seem counterintuitive to human observers. This; occurs when a dense set of variants appears on one haplotype while the other; haplotype is fully reference, and humans often perceive this as missing a; clearly heterozygous position. DeepVariant seems to have learned that this; signature often indicates a region which is a segmental duplication, copy number; variant, or structural variant where multiple copies of similar genomic regions; are mapping to the same reference location. In this case, it may be worthwhile; to inspect the region to see if it has elevated coverage, and whether you can; identify 
",False,"The text contains complete sentences discussing how DeepVariant processes variants and potential issues, which are meaningful for humans."
Usability,"section; for instructions. **Missing variants where a candidate is generated:**. If a candidate is made, but is called as reference (either 0/0 or ./.) it means; that the neural network processed the genomic region, but based on all of its; learned experience from training data, it decided the highest probability for; the position was as non-variant. Some of the reasons that DeepVariant may; suspect a false positive are: strand-bias in reads, low mapping quality in; reads, low base quality in reads, and overall low coverage. In addition, there is another pattern that causes DeepVariant to suspect variant; positions which can initially seem counterintuitive to human observers. This; occurs when a dense set of variants appears on one haplotype while the other; haplotype is fully reference, and humans often perceive this as missing a; clearly heterozygous position. DeepVariant seems to have learned that this; signature often indicates a region which is a segmental duplication, copy number; variant, or structural variant where multiple copies of similar genomic regions; are mapping to the same reference location. In this case, it may be worthwhile; to inspect the region to see if it has elevated coverage, and whether you can; identify more than 2 haplotypes present by overlapping the reads. If you can, it; suggests that the region may have a copy number variation. Some analysis of this; was presented at AGBT as a poster; “[Uncaptured segmental duplication creates artifacts in workflows using GRCh37](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096)”. This pattern of undercalling positions at high variant density may affect; variant-dense non-human species (those with a variant density of >1 in 40; positions). For an analysis of this, please see our blog; “[Improved non-human variant calling using species-specific DeepVariant models](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepva",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:2692,learn,learned,2692,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['learn'],['learned'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: section; for instructions. **Missing variants where a candidate is generated:**. If a candidate is made, but is called as reference (either 0/0 or ./.) it means; that the neural network processed the genomic region, but based on all of its; learned experience from training data, it decided the highest probability for; the position was as non-variant. Some of the reasons that DeepVariant may; suspect a false positive are: strand-bias in reads, low mapping quality in; reads, low base quality in reads, and overall low coverage. In addition, there is another pattern that causes DeepVariant to suspect variant; positions which can initially seem counterintuitive to human observers. This; occurs when a dense set of variants appears on one haplotype while the other; haplotype is fully reference, and humans often perceive this as missing a; clearly heterozygous position. DeepVariant seems to have learned that this; signature often indicates a region which is a segmental duplication, copy number; variant, or structural variant where multiple copies of similar genomic regions; are mapping to the same reference location. In this case, it may be worthwhile; to inspect the region to see if it has elevated coverage, and whether you can; identify more than 2 haplotypes present by overlapping the reads. If you can, it; suggests that the region may have a copy number variation. Some analysis of this; was presented at AGBT as a poster; “[Uncaptured segmental duplication creates artifacts in workflows using GRCh37](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096)”. This pattern of undercalling positions at high variant density may affect; variant-dense non-human species (those with a variant density of >1 in 40; positions). For an analysis of this, please see our blog; “[Improved non-human variant calling using species-specific DeepVariant models](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepva
",False,"The content contains meaningful, explanatory text about the behavior of a variant calling tool and its analysis."
Usability,"t/releases); [![announcements](https://img.shields.io/badge/announcements-blue)](https://groups.google.com/d/forum/deepvariant-announcements); [![blog](https://img.shields.io/badge/blog-orange)](https://goo.gl/deepvariant). DeepVariant is a deep learning-based variant caller that takes aligned reads (in; BAM or CRAM format), produces pileup image tensors from them, classifies each; tensor using a convolutional neural network, and finally reports the results in; a standard VCF or gVCF file. DeepVariant supports germline variant-calling in diploid organisms. * NGS (Illumina or Element) data for either a; [whole genome](docs/deepvariant-case-study.md) or; [whole exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and; [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The models included with DeepVariant are only trained on human data. For; other organisms, see the; [blog post on non-human variant-calling](https://google.github.io/deepvariant",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/README.md:1185,simpl,simplex-case-study,1185,README.md,,https://github.com/google/deepvariant/tree/v1.6.1/README.md,1,['simpl'],['simplex-case-study'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: t/releases); [![announcements](https://img.shields.io/badge/announcements-blue)](https://groups.google.com/d/forum/deepvariant-announcements); [![blog](https://img.shields.io/badge/blog-orange)](https://goo.gl/deepvariant). DeepVariant is a deep learning-based variant caller that takes aligned reads (in; BAM or CRAM format), produces pileup image tensors from them, classifies each; tensor using a convolutional neural network, and finally reports the results in; a standard VCF or gVCF file. DeepVariant supports germline variant-calling in diploid organisms. * NGS (Illumina or Element) data for either a; [whole genome](docs/deepvariant-case-study.md) or; [whole exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and; [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The models included with DeepVariant are only trained on human data. For; other organisms, see the; [blog post on non-human variant-calling](https://google.github.io/deepvariant
",False,
Usability,"ts`. More details about each program are described in detail; in the [Inputs and outputs](#inputs-and-outputs) section. ## Inputs and outputs. ### General notes. * Sharded files are a single logical collection of files with a common naming; convention. For example, we talk about `filename@10` as a single 10-way; sharded file named `filename`. On most filesystems this actually looks like; 10 distinct files `filename-00000-of-00010`, ..., `filename-00009-of-00010`.; DeepVariant can write sharded files using their `filename@10`-style name and; can read sharded files using both that style as well as the glob form, such; as `filename-*` or `filename-*-of-00010`.; * Files with the `.gz` suffix are interpreted as being compressed with gzip; and are read/written accordingly. ### make_examples. `make_examples` consumes reads and the reference genome to create TensorFlow; examples for evaluation with our deep learning models. The tf.Example protos are; written out in TFRecord format. To learn more about tf.Example and TFRecord, see; the; [Using TFRecords and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord); Colab. `make_examples` is a single-threaded program using 1-2 GB of RAM. Since the; process of generating examples is embarrassingly parallel across the genome,; `make_examples` supports sharding of its input and output via the `--task`; argument with a sharded output specification. For example, if the output is; specified as `--examples examples.tfrecord@10.gz` and `--task 0`, the input to; the program will be 10% of the regions and the output will be written to; `examples.tfrecord-00000-of-00010.gz`. #### Input assumptions. `make_examples` requires its input files to satisfy a few basic requirements to; be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be; indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible""; versi",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:1856,learn,learn,1856,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['learn'],['learn'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ts`. More details about each program are described in detail; in the [Inputs and outputs](#inputs-and-outputs) section. ## Inputs and outputs. ### General notes. * Sharded files are a single logical collection of files with a common naming; convention. For example, we talk about `filename@10` as a single 10-way; sharded file named `filename`. On most filesystems this actually looks like; 10 distinct files `filename-00000-of-00010`, ..., `filename-00009-of-00010`.; DeepVariant can write sharded files using their `filename@10`-style name and; can read sharded files using both that style as well as the glob form, such; as `filename-*` or `filename-*-of-00010`.; * Files with the `.gz` suffix are interpreted as being compressed with gzip; and are read/written accordingly. ### make_examples. `make_examples` consumes reads and the reference genome to create TensorFlow; examples for evaluation with our deep learning models. The tf.Example protos are; written out in TFRecord format. To learn more about tf.Example and TFRecord, see; the; [Using TFRecords and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord); Colab. `make_examples` is a single-threaded program using 1-2 GB of RAM. Since the; process of generating examples is embarrassingly parallel across the genome,; `make_examples` supports sharding of its input and output via the `--task`; argument with a sharded output specification. For example, if the output is; specified as `--examples examples.tfrecord@10.gz` and `--task 0`, the input to; the program will be 10% of the regions and the output will be written to; `examples.tfrecord-00000-of-00010.gz`. #### Input assumptions. `make_examples` requires its input files to satisfy a few basic requirements to; be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be; indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible""; versi
",False,The text contains complete sentences in natural language discussing concepts like sharded files and input assumptions for the make_examples program.
Usability,"ub.io/hts-specs/VCFv4.3.pdf) format. DeepVariant is composed of three programs: `make_examples`, `call_variants`, and; `postprocess_variants`. More details about each program are described in detail; in the [Inputs and outputs](#inputs-and-outputs) section. ## Inputs and outputs. ### General notes. * Sharded files are a single logical collection of files with a common naming; convention. For example, we talk about `filename@10` as a single 10-way; sharded file named `filename`. On most filesystems this actually looks like; 10 distinct files `filename-00000-of-00010`, ..., `filename-00009-of-00010`.; DeepVariant can write sharded files using their `filename@10`-style name and; can read sharded files using both that style as well as the glob form, such; as `filename-*` or `filename-*-of-00010`.; * Files with the `.gz` suffix are interpreted as being compressed with gzip; and are read/written accordingly. ### make_examples. `make_examples` consumes reads and the reference genome to create TensorFlow; examples for evaluation with our deep learning models. The tf.Example protos are; written out in TFRecord format. To learn more about tf.Example and TFRecord, see; the; [Using TFRecords and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord); Colab. `make_examples` is a single-threaded program using 1-2 GB of RAM. Since the; process of generating examples is embarrassingly parallel across the genome,; `make_examples` supports sharding of its input and output via the `--task`; argument with a sharded output specification. For example, if the output is; specified as `--examples examples.tfrecord@10.gz` and `--task 0`, the input to; the program will be 10% of the regions and the output will be written to; `examples.tfrecord-00000-of-00010.gz`. #### Input assumptions. `make_examples` requires its input files to satisfy a few basic requirements to; be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be; indexed and c",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:1777,learn,learning,1777,docs/deepvariant-details.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md,1,['learn'],['learning'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ub.io/hts-specs/VCFv4.3.pdf) format. DeepVariant is composed of three programs: `make_examples`, `call_variants`, and; `postprocess_variants`. More details about each program are described in detail; in the [Inputs and outputs](#inputs-and-outputs) section. ## Inputs and outputs. ### General notes. * Sharded files are a single logical collection of files with a common naming; convention. For example, we talk about `filename@10` as a single 10-way; sharded file named `filename`. On most filesystems this actually looks like; 10 distinct files `filename-00000-of-00010`, ..., `filename-00009-of-00010`.; DeepVariant can write sharded files using their `filename@10`-style name and; can read sharded files using both that style as well as the glob form, such; as `filename-*` or `filename-*-of-00010`.; * Files with the `.gz` suffix are interpreted as being compressed with gzip; and are read/written accordingly. ### make_examples. `make_examples` consumes reads and the reference genome to create TensorFlow; examples for evaluation with our deep learning models. The tf.Example protos are; written out in TFRecord format. To learn more about tf.Example and TFRecord, see; the; [Using TFRecords and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord); Colab. `make_examples` is a single-threaded program using 1-2 GB of RAM. Since the; process of generating examples is embarrassingly parallel across the genome,; `make_examples` supports sharding of its input and output via the `--task`; argument with a sharded output specification. For example, if the output is; specified as `--examples examples.tfrecord@10.gz` and `--task 0`, the input to; the program will be 10% of the regions and the output will be written to; `examples.tfrecord-00000-of-00010.gz`. #### Input assumptions. `make_examples` requires its input files to satisfy a few basic requirements to; be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be; indexed and c
",False,
Usability,"urs. ### Accuracy. Evaluating on HG003 (all chromosomes, using NIST v4.2.1 truth), which was held; out while training the hybrid model. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 503014 | 1487 | 2767 | 0.997053 | 0.994781 | 0.995916 |; | SNP | 3323624 | 3871 | 2273 | 0.998837 | 0.999317 | 0.999077 |. [See VCF stats report.](https://storage.googleapis.com/deepvariant/visual_reports/DeepVariant/1.6.1/HYBRID/deepvariant.output.visual_report.html). ## Inspect outputs that produced the metrics above. The DeepVariant VCFs, gVCFs, and hap.py evaluation outputs are available at:. ```; gs://deepvariant/case-study-outputs; ```. You can also inspect them in a web browser here:; https://42basepairs.com/browse/gs/deepvariant/case-study-outputs. ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; # Get the script.; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deepvariant.sh. # WGS; bash inference_deepvariant.sh --model_preset WGS. # WES; bash inference_deepvariant.sh --model_preset WES. # PacBio; bash inference_deepvariant.sh --model_preset PACBIO. # ONT_R104; bash inference_deepvariant.sh --model_preset ONT_R104. # Hybrid; bash inference_deepvariant.sh --model_preset HYBRID_PACBIO_ILLUMINA; ```. Runtime metrics are taken from the resulting log after each stage of; DeepVariant. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 r",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md:4781,simpl,simplicity,4781,docs/metrics.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md,1,['simpl'],['simplicity'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: urs. ### Accuracy. Evaluating on HG003 (all chromosomes, using NIST v4.2.1 truth), which was held; out while training the hybrid model. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 503014 | 1487 | 2767 | 0.997053 | 0.994781 | 0.995916 |; | SNP | 3323624 | 3871 | 2273 | 0.998837 | 0.999317 | 0.999077 |. [See VCF stats report.](https://storage.googleapis.com/deepvariant/visual_reports/DeepVariant/1.6.1/HYBRID/deepvariant.output.visual_report.html). ## Inspect outputs that produced the metrics above. The DeepVariant VCFs, gVCFs, and hap.py evaluation outputs are available at:. ```; gs://deepvariant/case-study-outputs; ```. You can also inspect them in a web browser here:; https://42basepairs.com/browse/gs/deepvariant/case-study-outputs. ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; # Get the script.; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deepvariant.sh. # WGS; bash inference_deepvariant.sh --model_preset WGS. # WES; bash inference_deepvariant.sh --model_preset WES. # PacBio; bash inference_deepvariant.sh --model_preset PACBIO. # ONT_R104; bash inference_deepvariant.sh --model_preset ONT_R104. # Hybrid; bash inference_deepvariant.sh --model_preset HYBRID_PACBIO_ILLUMINA; ```. Runtime metrics are taken from the resulting log after each stage of; DeepVariant. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 r
",True,"The text consists primarily of code snippets, API documentation, and program logs. It includes command lines, script executions, and statistical outputs that are more appropriate for developers than general readers."
Usability,"ype implementation for somatic calling, which can take a tumor and normal; BAM and call subclonal variants. However, we don't yet have enough confidence in; the available truth sets, and that they come from a diverse enough sampling of; cancers with mutational profiles, for us to be certain in releasing something of; high quality. We're watching developments in the area of these truth sets and; hope to be able to further develop the somatic caller in the future. ## Can I use DeepVariant on plant genomes?. DeepVariant has previously been applied to plant species. In the case of rice,; there was good evidence of high accuracy. You can see; [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant).; However, these rice genomes were diploid and with a similar variant density of; humans. DeepVariant is currently written to be a diploid variant caller. So if the plant; species you are working with is polyploid, it is not yet clear how DeepVariant; will perform. That is because even with re-training, DeepVariant can only; produce variant calls that are homozygous alternate, heterozygous, or homozygous; reference, which don't have much meaning in a tetraploid genome, for example. ## Can I use DeepVariant on other non-human species?. See this; [blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). ## How do I build/run DeepVariant?. In general, we recommend running DeepVariant using Docker for the simplest; setup. If you are building from source because you want to experiment with; changes to the codebase, we still recommend Docker. You can clone the; DeepVariant repo, modify the source code, and build a Docker image with your; changes using the provided Dockerfile. ## Why can't it find one of the input files? E.g., ""Could not open"". This often happens because the way Docker works, input and output di",MatchSource.DOCS,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:6602,clear,clear,6602,docs/FAQ.md,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md,1,['clear'],['clear'],"You are an expert in analyzing and categorizing text content. Your task is to evaluate whether the given text contains meaningful human-readable sentences or if it consists primarily of logs, code samples, or programmatic API description that should be filtered out.

For each input text, analyze it and determine:
1. Whether it should be eliminated (true/false)
2. The reason for elimination (if applicable)

Evaluation criteria:
- Eliminate text that consists primarily of:
  * Code snippets or samples (marked by syntax, keywords like ""if/else"", brackets, etc.)
  * Program logs or error messages (timestamps, error codes, stack traces)
  * API documentation or specifications (parameter lists, return types)
  * Configuration files or build system output
  * Version control metadata or comments
  * Compiler/interpreter output or warnings
- Keep text that contains:
  * Complete, meaningful sentences in natural language
  * Explanatory or descriptive content
  * Human-written prose discussing concepts or ideas

Content: ype implementation for somatic calling, which can take a tumor and normal; BAM and call subclonal variants. However, we don't yet have enough confidence in; the available truth sets, and that they come from a diverse enough sampling of; cancers with mutational profiles, for us to be certain in releasing something of; high quality. We're watching developments in the area of these truth sets and; hope to be able to further develop the somatic caller in the future. ## Can I use DeepVariant on plant genomes?. DeepVariant has previously been applied to plant species. In the case of rice,; there was good evidence of high accuracy. You can see; [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant).; However, these rice genomes were diploid and with a similar variant density of; humans. DeepVariant is currently written to be a diploid variant caller. So if the plant; species you are working with is polyploid, it is not yet clear how DeepVariant; will perform. That is because even with re-training, DeepVariant can only; produce variant calls that are homozygous alternate, heterozygous, or homozygous; reference, which don't have much meaning in a tetraploid genome, for example. ## Can I use DeepVariant on other non-human species?. See this; [blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). ## How do I build/run DeepVariant?. In general, we recommend running DeepVariant using Docker for the simplest; setup. If you are building from source because you want to experiment with; changes to the codebase, we still recommend Docker. You can clone the; DeepVariant repo, modify the source code, and build a Docker image with your; changes using the provided Dockerfile. ## Why can't it find one of the input files? E.g., ""Could not open"". This often happens because the way Docker works, input and output di
",False,"The text contains multiple complete sentences that are in natural language discussing concepts and ideas related to using DeepVariant for variant calling, including questions and answers about its applicability and setup. It also includes references to blog posts which provide further information. The content is explanatory and human-written, without any code snippets or program logs."
