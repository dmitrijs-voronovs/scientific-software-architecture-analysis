id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/1#issuecomment-348330972:0,Testability,TEST,TEST,0,TEST 2,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/1#issuecomment-348330972
https://github.com/google/deepvariant/issues/2#issuecomment-349070609:220,Deployability,install,install,220,"Hi Arkanosis,; thanks for your question!; It is not mandatory to run DeepVariant on Google Cloud. We use Google Cloud Platform as an example in our quick start and case study to show how you can get a machine easily and install DeepVariant correctly. But you should be able to use DeepVariant outside Google Cloud as well.; However, note that the current instructions are for Ubuntu 16. If you're on different OS or different versions of Ubuntu, it is possible you need to make some changes to install things correctly. Please feel free to let us know if you encounter any issues.; Thanks!; -pichuan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/2#issuecomment-349070609
https://github.com/google/deepvariant/issues/2#issuecomment-349070609:494,Deployability,install,install,494,"Hi Arkanosis,; thanks for your question!; It is not mandatory to run DeepVariant on Google Cloud. We use Google Cloud Platform as an example in our quick start and case study to show how you can get a machine easily and install DeepVariant correctly. But you should be able to use DeepVariant outside Google Cloud as well.; However, note that the current instructions are for Ubuntu 16. If you're on different OS or different versions of Ubuntu, it is possible you need to make some changes to install things correctly. Please feel free to let us know if you encounter any issues.; Thanks!; -pichuan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/2#issuecomment-349070609
https://github.com/google/deepvariant/issues/3#issuecomment-350808367:38,Performance,optimiz,optimized,38,"Just following up on this, is the CNN optimized solely for human variant calling or should it deliver similar success on non-model organisms such as bacteria or protists?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/3#issuecomment-350808367
https://github.com/google/deepvariant/issues/3#issuecomment-350808977:84,Testability,test,tested,84,Our experience is the model works well across a variety of species. But we have not tested it on bacteria or other haploid organisms so we'd love to hear about any results you get there.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/3#issuecomment-350808977
https://github.com/google/deepvariant/pull/4#issuecomment-349494695:302,Deployability,patch,patch,302,"Hi cclaus,. Thank you so much for this pull request. At the moment we cannot accept external contributions (see https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md) as we don't yet have a mechanism setup to resync changes from github back into our codebase within Google. We are happy to patch this internally so it'll appear in the next release. Are you ok with that?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/4#issuecomment-349494695
https://github.com/google/deepvariant/pull/4#issuecomment-349494695:352,Deployability,release,release,352,"Hi cclaus,. Thank you so much for this pull request. At the moment we cannot accept external contributions (see https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md) as we don't yet have a mechanism setup to resync changes from github back into our codebase within Google. We are happy to patch this internally so it'll appear in the next release. Are you ok with that?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/4#issuecomment-349494695
https://github.com/google/deepvariant/pull/4#issuecomment-349500262:60,Deployability,release,release,60,The change is in internally and will appear in the next OSS release. Thanks again!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/4#issuecomment-349500262
https://github.com/google/deepvariant/issues/5#issuecomment-349829021:95,Deployability,install,install,95,Unfortunately it's not clear from your post what might be going wrong here. Is this on a clean install of Ubuntu 16? We'd recommend starting there first to make sure everything is working and then moving to whatever environment you are running on.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5#issuecomment-349829021
https://github.com/google/deepvariant/issues/5#issuecomment-349829021:23,Usability,clear,clear,23,Unfortunately it's not clear from your post what might be going wrong here. Is this on a clean install of Ubuntu 16? We'd recommend starting there first to make sure everything is working and then moving to whatever environment you are running on.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5#issuecomment-349829021
https://github.com/google/deepvariant/issues/5#issuecomment-349832933:99,Availability,error,error,99,"I think you are getting the wrong ""gsutil"". There is an Ubuntu package with that name and matching error messages, but what you need is the ""Google Cloud SDK"" (see https://cloud.google.com/sdk/downloads).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5#issuecomment-349832933
https://github.com/google/deepvariant/issues/5#issuecomment-349832933:193,Availability,down,downloads,193,"I think you are getting the wrong ""gsutil"". There is an Ubuntu package with that name and matching error messages, but what you need is the ""Google Cloud SDK"" (see https://cloud.google.com/sdk/downloads).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5#issuecomment-349832933
https://github.com/google/deepvariant/issues/5#issuecomment-349832933:105,Integrability,message,messages,105,"I think you are getting the wrong ""gsutil"". There is an Ubuntu package with that name and matching error messages, but what you need is the ""Google Cloud SDK"" (see https://cloud.google.com/sdk/downloads).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5#issuecomment-349832933
https://github.com/google/deepvariant/issues/5#issuecomment-350478903:83,Testability,Test,Test,83,We added a note about needing the `gsutil` from Google Cloud SDK to our [Build and Test guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md). Let us know if you are still having issues.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5#issuecomment-350478903
https://github.com/google/deepvariant/issues/5#issuecomment-350478903:166,Testability,test,test,166,We added a note about needing the `gsutil` from Google Cloud SDK to our [Build and Test guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md). Let us know if you are still having issues.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5#issuecomment-350478903
https://github.com/google/deepvariant/issues/5#issuecomment-350478903:88,Usability,guid,guide,88,We added a note about needing the `gsutil` from Google Cloud SDK to our [Build and Test guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md). Let us know if you are still having issues.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/5#issuecomment-350478903
https://github.com/google/deepvariant/issues/6#issuecomment-350159832:108,Security,access,access,108,"Hi,; currently the instructions are written for Ubuntu 16. I've not tried it on CentOS before.; If you have access to a Ubuntu 16 machine, can you give it a try and see if you're still seen the same issue? I can also try running on CentOS and report back, but that might take a while for me to do.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-350159832
https://github.com/google/deepvariant/issues/6#issuecomment-350168328:155,Testability,test,test,155,"@huangl07 Does `./build-prereq.sh` complete successfully for you, as noted [here](https://github.com/google/deepvariant/blob/master/docs/deepvariant-build-test.md)?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-350168328
https://github.com/google/deepvariant/issues/6#issuecomment-372376833:175,Testability,test,test,175,@ardoli You will need to use Ubuntu 14 or 16 - see the following: . https://github.com/google/deepvariant/blob/59738f0ca91df3757d754e7ce6507f614816fd1c/docs/deepvariant-build-test.md,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-372376833
https://github.com/google/deepvariant/issues/6#issuecomment-372381243:240,Deployability,install,installation,240,"Thank's for the answer, I saw it, but all servers in the genomic research center I work are on CentOS-7 and Ubuntu is not an option. So I can't use Ubuntu, and of course, no way to send human genomic data on the cloud. Maybe a more generic installation procedure, not stick on ubuntu, could be a good idea, at least for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-372381243
https://github.com/google/deepvariant/issues/6#issuecomment-372439840:273,Deployability,install,install,273,"I worked around that by creating a custom Docker image (the provided one didn't work for me) and then converting it to a Singularity image. And it worked on CentOS 7. I was going to share the recipe when I have the time. Of course, you'd need to convince your sysadmins to install Singularity for you but that should be possible because Singularity was designed with HPC in mind.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-372439840
https://github.com/google/deepvariant/issues/6#issuecomment-372515026:386,Deployability,pipeline,pipeline,386,"@ardoli, @ink1's suggestion is great, and there are two more:. * You can try [Udocker](https://blog.utar.co/blog/udocker) so that root privileges are not required for a Docker image.; * Tweak the DeepVariant source code to make it compile in user-space for your environment. It's a bit hairy, but can be done. The plus side is that you'll see DeepVariant is a pretty simple and modular pipeline, which you can tweak for your preferred analysis. Deep neural networks can be applied to many areas in the -Omics space besides just variant analysis.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-372515026
https://github.com/google/deepvariant/issues/6#issuecomment-372515026:367,Usability,simpl,simple,367,"@ardoli, @ink1's suggestion is great, and there are two more:. * You can try [Udocker](https://blog.utar.co/blog/udocker) so that root privileges are not required for a Docker image.; * Tweak the DeepVariant source code to make it compile in user-space for your environment. It's a bit hairy, but can be done. The plus side is that you'll see DeepVariant is a pretty simple and modular pipeline, which you can tweak for your preferred analysis. Deep neural networks can be applied to many areas in the -Omics space besides just variant analysis.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-372515026
https://github.com/google/deepvariant/issues/6#issuecomment-372590216:38,Deployability,install,installed,38,"Sound a good idea, and Singularity is installed on ours servers. I will try it, of course if in the meantime you had the time to share your recipe it will be great. If my Singularity image is ok, I will see to share it if their no licence restriction. Thank’s for this answer. Le 12 mars 2018 à 20:47, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I worked around that by creating a custom Docker image (the provided one didn't work for me) and then converting it to a Singularity image. And it worked on CentOS 7. I was going to share the recipe when I have the time. Of course, you'd need to convince your sysadmins to install Singularity for you but that should be possible because Singularity was designed with HPC in mind. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372439840>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNFAqxV5xX0KGKeEba8fJkH-EKIclks5tdtDqgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-372590216
https://github.com/google/deepvariant/issues/6#issuecomment-372590216:651,Deployability,install,install,651,"Sound a good idea, and Singularity is installed on ours servers. I will try it, of course if in the meantime you had the time to share your recipe it will be great. If my Singularity image is ok, I will see to share it if their no licence restriction. Thank’s for this answer. Le 12 mars 2018 à 20:47, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I worked around that by creating a custom Docker image (the provided one didn't work for me) and then converting it to a Singularity image. And it worked on CentOS 7. I was going to share the recipe when I have the time. Of course, you'd need to convince your sysadmins to install Singularity for you but that should be possible because Singularity was designed with HPC in mind. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372439840>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNFAqxV5xX0KGKeEba8fJkH-EKIclks5tdtDqgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-372590216
https://github.com/google/deepvariant/issues/6#issuecomment-372644789:396,Availability,down,downloads,396,I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:; https://github.com/ink1/deepvariant/releases/tag/v0.5.2a; The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-372644789
https://github.com/google/deepvariant/issues/6#issuecomment-372644789:139,Deployability,release,releases,139,I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:; https://github.com/ink1/deepvariant/releases/tag/v0.5.2a; The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-372644789
https://github.com/google/deepvariant/issues/6#issuecomment-372644789:172,Deployability,release,release,172,I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:; https://github.com/ink1/deepvariant/releases/tag/v0.5.2a; The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-372644789
https://github.com/google/deepvariant/issues/6#issuecomment-372647046:592,Availability,down,downloads,592,"Thank’s ink1. I will try your singularity image, I have just finish build my own docker image. Le 13 mars 2018 à 13:13, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:; https://github.com/ink1/deepvariant/releases/tag/v0.5.2a; The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372644789>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNJ8iEzMzNBFgr1bgmaUyF6v9pt7Nks5td7fxgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-372647046
https://github.com/google/deepvariant/issues/6#issuecomment-372647046:335,Deployability,release,releases,335,"Thank’s ink1. I will try your singularity image, I have just finish build my own docker image. Le 13 mars 2018 à 13:13, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:; https://github.com/ink1/deepvariant/releases/tag/v0.5.2a; The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372644789>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNJ8iEzMzNBFgr1bgmaUyF6v9pt7Nks5td7fxgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-372647046
https://github.com/google/deepvariant/issues/6#issuecomment-372647046:368,Deployability,release,release,368,"Thank’s ink1. I will try your singularity image, I have just finish build my own docker image. Le 13 mars 2018 à 13:13, ink1 <notifications@github.com<mailto:notifications@github.com>> a écrit :. I created a build script and a singularity image updating to 0.5.2 in the processes; here is my fork:; https://github.com/ink1/deepvariant/releases/tag/v0.5.2a; The binary release is the singularity image. My original image was quite large because I built everything inside of the container. Instead I resorted to the original Dockerfile which relies on pre-built zip files which my build script downloads. Still the image is 2 GB but then it should just run I presume. The changes I had to do to the Dockerfile are probably optional - my docker version does not like the lines with ARG. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/6#issuecomment-372644789>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AfxQNJ8iEzMzNBFgr1bgmaUyF6v9pt7Nks5td7fxgaJpZM4Q6ZDg>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-372647046
https://github.com/google/deepvariant/issues/6#issuecomment-372953552:75,Modifiability,portab,portable,75,"Building a deepvariant Singularity image is indeed really quite simple and portable. I did it and test it on CentOS7 and MacOS X and it run in both case with deepvariant quick-test data. I will post the complete ""how-to"" when I'll have a couple of minutes.; Thank's ink1 for the idea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-372953552
https://github.com/google/deepvariant/issues/6#issuecomment-372953552:98,Testability,test,test,98,"Building a deepvariant Singularity image is indeed really quite simple and portable. I did it and test it on CentOS7 and MacOS X and it run in both case with deepvariant quick-test data. I will post the complete ""how-to"" when I'll have a couple of minutes.; Thank's ink1 for the idea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-372953552
https://github.com/google/deepvariant/issues/6#issuecomment-372953552:176,Testability,test,test,176,"Building a deepvariant Singularity image is indeed really quite simple and portable. I did it and test it on CentOS7 and MacOS X and it run in both case with deepvariant quick-test data. I will post the complete ""how-to"" when I'll have a couple of minutes.; Thank's ink1 for the idea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-372953552
https://github.com/google/deepvariant/issues/6#issuecomment-372953552:64,Usability,simpl,simple,64,"Building a deepvariant Singularity image is indeed really quite simple and portable. I did it and test it on CentOS7 and MacOS X and it run in both case with deepvariant quick-test data. I will post the complete ""how-to"" when I'll have a couple of minutes.; Thank's ink1 for the idea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-372953552
https://github.com/google/deepvariant/issues/6#issuecomment-440897520:53,Availability,ERROR,ERROR,53,"I see the same issue, but with Ubuntu-16. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1: //deepvariant/protos:deepvariant_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1 1 input file(s) do not exist. This is running on IBM Power 8 in a docker container, so I built pyclif from source. However, the location of pyclif isn't the same as what is in build-prereq.sh, so I commented that section out because `which pyclif` works. Would appreciate any help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-440897520
https://github.com/google/deepvariant/issues/6#issuecomment-440897520:239,Availability,ERROR,ERROR,239,"I see the same issue, but with Ubuntu-16. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1: //deepvariant/protos:deepvariant_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1 1 input file(s) do not exist. This is running on IBM Power 8 in a docker container, so I built pyclif from source. However, the location of pyclif isn't the same as what is in build-prereq.sh, so I commented that section out because `which pyclif` works. Would appreciate any help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-440897520
https://github.com/google/deepvariant/issues/6#issuecomment-440897520:363,Energy Efficiency,Power,Power,363,"I see the same issue, but with Ubuntu-16. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1: //deepvariant/protos:deepvariant_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1 1 input file(s) do not exist. This is running on IBM Power 8 in a docker container, so I built pyclif from source. However, the location of pyclif isn't the same as what is in build-prereq.sh, so I commented that section out because `which pyclif` works. Would appreciate any help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/6#issuecomment-440897520
https://github.com/google/deepvariant/issues/7#issuecomment-350520174:61,Security,authenticat,authentication,61,"From the log it seems like there's something to do with your authentication to the Google Cloud Platform. But usually this shouldn't fail even if you didn't set that up. Can you give us a bit more description about your machine? (OS version, if you're running on cloud, where and what type of machine, etc)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7#issuecomment-350520174
https://github.com/google/deepvariant/issues/7#issuecomment-350520174:9,Testability,log,log,9,"From the log it seems like there's something to do with your authentication to the Google Cloud Platform. But usually this shouldn't fail even if you didn't set that up. Can you give us a bit more description about your machine? (OS version, if you're running on cloud, where and what type of machine, etc)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7#issuecomment-350520174
https://github.com/google/deepvariant/issues/7#issuecomment-350527164:152,Testability,log,login,152,@TuBieJun Try first resetting your credentials on your machine to be yours via either of the following commands:. `gcloud beta auth application-default login`. or. `gcloud auth login`,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7#issuecomment-350527164
https://github.com/google/deepvariant/issues/7#issuecomment-350527164:177,Testability,log,login,177,@TuBieJun Try first resetting your credentials on your machine to be yours via either of the following commands:. `gcloud beta auth application-default login`. or. `gcloud auth login`,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7#issuecomment-350527164
https://github.com/google/deepvariant/issues/7#issuecomment-350528251:20,Availability,error,error,20,"hi i have check the error. The error reason is I can't connect the GCP, the make_samples.py need to run ; 'htslib_gcp_oauth.init()', I remove this and install htslib in local. So build_and_test pass all. ; ![image](https://user-images.githubusercontent.com/15261087/33802638-2bf6bb2c-ddb6-11e7-963e-950660e357ff.png). ![image](https://user-images.githubusercontent.com/15261087/33802647-66a7728e-ddb6-11e7-9279-80c1c79a37e0.png). thanks everyone~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7#issuecomment-350528251
https://github.com/google/deepvariant/issues/7#issuecomment-350528251:31,Availability,error,error,31,"hi i have check the error. The error reason is I can't connect the GCP, the make_samples.py need to run ; 'htslib_gcp_oauth.init()', I remove this and install htslib in local. So build_and_test pass all. ; ![image](https://user-images.githubusercontent.com/15261087/33802638-2bf6bb2c-ddb6-11e7-963e-950660e357ff.png). ![image](https://user-images.githubusercontent.com/15261087/33802647-66a7728e-ddb6-11e7-9279-80c1c79a37e0.png). thanks everyone~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7#issuecomment-350528251
https://github.com/google/deepvariant/issues/7#issuecomment-350528251:151,Deployability,install,install,151,"hi i have check the error. The error reason is I can't connect the GCP, the make_samples.py need to run ; 'htslib_gcp_oauth.init()', I remove this and install htslib in local. So build_and_test pass all. ; ![image](https://user-images.githubusercontent.com/15261087/33802638-2bf6bb2c-ddb6-11e7-963e-950660e357ff.png). ![image](https://user-images.githubusercontent.com/15261087/33802647-66a7728e-ddb6-11e7-9279-80c1c79a37e0.png). thanks everyone~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7#issuecomment-350528251
https://github.com/google/deepvariant/issues/7#issuecomment-351080961:28,Integrability,depend,dependency,28,"We've got a fix for the GCP dependency internally, and will sync it to the OSS github version on our next push. Thanks for the report.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/7#issuecomment-351080961
https://github.com/google/deepvariant/issues/8#issuecomment-350520082:63,Availability,avail,availability,63,"See: https://github.com/google/deepvariant/blob/r0.4/README.md#availability; ""Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant)""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/8#issuecomment-350520082
https://github.com/google/deepvariant/issues/8#issuecomment-350520082:101,Availability,avail,available,101,"See: https://github.com/google/deepvariant/blob/r0.4/README.md#availability; ""Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant)""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/8#issuecomment-350520082
https://github.com/google/deepvariant/issues/9#issuecomment-350774284:339,Availability,avail,available,339,Thanks much for suggesting this. I'm actively working on preparing a DeepVariant conda recipe for bioconda (https://github.com/bioconda/bioconda-recipes) along with the Google team. This might take a few days to get sorted out as there are a few pieces like bazel that appear to need updating in conda but I can report back here when it's available.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-350774284
https://github.com/google/deepvariant/issues/9#issuecomment-351577610:132,Deployability,install,install,132,@depristo thank you for taking the time to help me out. . When I was trying to build DeepVariant on Ubuntu 16.04 it required that I install Google Cloud SDK because it needed the 'gsutil' command. Before installing Google Cloud SDK I started reading the 'deepvariant-quick-start.md' file and it talked a lot about using Google Cloud SDK and setting up a Google Cloud account and 'enable billing for you account'. . Based on your comment I am assuming that Google Cloud Platform is not needed and we can simply ignore 'gsutil' once the build process is complete?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-351577610
https://github.com/google/deepvariant/issues/9#issuecomment-351577610:204,Deployability,install,installing,204,@depristo thank you for taking the time to help me out. . When I was trying to build DeepVariant on Ubuntu 16.04 it required that I install Google Cloud SDK because it needed the 'gsutil' command. Before installing Google Cloud SDK I started reading the 'deepvariant-quick-start.md' file and it talked a lot about using Google Cloud SDK and setting up a Google Cloud account and 'enable billing for you account'. . Based on your comment I am assuming that Google Cloud Platform is not needed and we can simply ignore 'gsutil' once the build process is complete?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-351577610
https://github.com/google/deepvariant/issues/9#issuecomment-351577610:503,Usability,simpl,simply,503,@depristo thank you for taking the time to help me out. . When I was trying to build DeepVariant on Ubuntu 16.04 it required that I install Google Cloud SDK because it needed the 'gsutil' command. Before installing Google Cloud SDK I started reading the 'deepvariant-quick-start.md' file and it talked a lot about using Google Cloud SDK and setting up a Google Cloud account and 'enable billing for you account'. . Based on your comment I am assuming that Google Cloud Platform is not needed and we can simply ignore 'gsutil' once the build process is complete?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-351577610
https://github.com/google/deepvariant/issues/9#issuecomment-351582326:117,Availability,down,download,117,"@depristo after rereading the 'deepvariant-build-test.md' file I am starting to think that 'gsutil' is being used to download the relevant files from GCP. Anyways, I was able to successfully build it and will start fiddling with it. Thank you for this awesome software!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-351582326
https://github.com/google/deepvariant/issues/9#issuecomment-351582326:49,Testability,test,test,49,"@depristo after rereading the 'deepvariant-build-test.md' file I am starting to think that 'gsutil' is being used to download the relevant files from GCP. Anyways, I was able to successfully build it and will start fiddling with it. Thank you for this awesome software!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-351582326
https://github.com/google/deepvariant/issues/9#issuecomment-352087449:19,Deployability,update,updated,19,"@MediciPrime We've updated the [quickstart guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md) based on your feedback to make it more clear that setting up a Cloud account and enabling billing isn't required to run DeepVariant. Take the new wording in that doc out for a spin and let us know what you think. Cheers,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-352087449
https://github.com/google/deepvariant/issues/9#issuecomment-352087449:43,Usability,guid,guide,43,"@MediciPrime We've updated the [quickstart guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md) based on your feedback to make it more clear that setting up a Cloud account and enabling billing isn't required to run DeepVariant. Take the new wording in that doc out for a spin and let us know what you think. Cheers,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-352087449
https://github.com/google/deepvariant/issues/9#issuecomment-352087449:145,Usability,feedback,feedback,145,"@MediciPrime We've updated the [quickstart guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md) based on your feedback to make it more clear that setting up a Cloud account and enabling billing isn't required to run DeepVariant. Take the new wording in that doc out for a spin and let us know what you think. Cheers,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-352087449
https://github.com/google/deepvariant/issues/9#issuecomment-352087449:170,Usability,clear,clear,170,"@MediciPrime We've updated the [quickstart guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md) based on your feedback to make it more clear that setting up a Cloud account and enabling billing isn't required to run DeepVariant. Take the new wording in that doc out for a spin and let us know what you think. Cheers,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-352087449
https://github.com/google/deepvariant/issues/9#issuecomment-354748344:39,Availability,avail,available,39,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:; ```; conda install -c conda-forge -c bioconda deepvariant; ```; It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-354748344
https://github.com/google/deepvariant/issues/9#issuecomment-354748344:471,Availability,avail,available,471,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:; ```; conda install -c conda-forge -c bioconda deepvariant; ```; It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-354748344
https://github.com/google/deepvariant/issues/9#issuecomment-354748344:91,Deployability,install,install,91,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:; ```; conda install -c conda-forge -c bioconda deepvariant; ```; It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-354748344
https://github.com/google/deepvariant/issues/9#issuecomment-354748344:117,Deployability,install,install,117,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:; ```; conda install -c conda-forge -c bioconda deepvariant; ```; It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-354748344
https://github.com/google/deepvariant/issues/9#issuecomment-354748344:708,Deployability,install,installing,708,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:; ```; conda install -c conda-forge -c bioconda deepvariant; ```; It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-354748344
https://github.com/google/deepvariant/issues/9#issuecomment-354748344:182,Integrability,wrap,wrapper,182,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:; ```; conda install -c conda-forge -c bioconda deepvariant; ```; It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-354748344
https://github.com/google/deepvariant/issues/9#issuecomment-354748344:311,Integrability,wrap,wrapping,311,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:; ```; conda install -c conda-forge -c bioconda deepvariant; ```; It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-354748344
https://github.com/google/deepvariant/issues/9#issuecomment-354748344:656,Modifiability,portab,portability,656,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:; ```; conda install -c conda-forge -c bioconda deepvariant; ```; It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-354748344
https://github.com/google/deepvariant/issues/9#issuecomment-354748344:452,Security,expose,expose,452,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:; ```; conda install -c conda-forge -c bioconda deepvariant; ```; It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-354748344
https://github.com/google/deepvariant/issues/9#issuecomment-354748344:759,Usability,feedback,feedback,759,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:; ```; conda install -c conda-forge -c bioconda deepvariant; ```; It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/9#issuecomment-354748344
https://github.com/google/deepvariant/issues/10#issuecomment-350781969:123,Modifiability,config,config,123,"Hi,; can you tell me where you got your inception_v3.ckpt* model files?; And, can you paste the content of your test_train.config.txt file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10#issuecomment-350781969
https://github.com/google/deepvariant/issues/10#issuecomment-350909874:369,Availability,down,downloaded,369,"Sorry, i used model_train.zip really, just pasted the incorrect command to issue. ; This is my correct command:; ```; python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt ""/leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt"" --start_from_checkpoint inception_v3.ckpt; ```; The inception_v3.ckpt is downloaded from https://github.com/tensorflow/models/tree/master/research/slim#Data ; This is my config file:; ```; name: ""test-training-dataset""; tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz""; num_examples: 1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10#issuecomment-350909874
https://github.com/google/deepvariant/issues/10#issuecomment-350909874:284,Modifiability,config,config,284,"Sorry, i used model_train.zip really, just pasted the incorrect command to issue. ; This is my correct command:; ```; python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt ""/leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt"" --start_from_checkpoint inception_v3.ckpt; ```; The inception_v3.ckpt is downloaded from https://github.com/tensorflow/models/tree/master/research/slim#Data ; This is my config file:; ```; name: ""test-training-dataset""; tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz""; num_examples: 1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10#issuecomment-350909874
https://github.com/google/deepvariant/issues/10#issuecomment-350909874:466,Modifiability,config,config,466,"Sorry, i used model_train.zip really, just pasted the incorrect command to issue. ; This is my correct command:; ```; python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt ""/leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt"" --start_from_checkpoint inception_v3.ckpt; ```; The inception_v3.ckpt is downloaded from https://github.com/tensorflow/models/tree/master/research/slim#Data ; This is my config file:; ```; name: ""test-training-dataset""; tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz""; num_examples: 1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10#issuecomment-350909874
https://github.com/google/deepvariant/issues/10#issuecomment-350909874:492,Testability,test,test-training-dataset,492,"Sorry, i used model_train.zip really, just pasted the incorrect command to issue. ; This is my correct command:; ```; python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt ""/leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt"" --start_from_checkpoint inception_v3.ckpt; ```; The inception_v3.ckpt is downloaded from https://github.com/tensorflow/models/tree/master/research/slim#Data ; This is my config file:; ```; name: ""test-training-dataset""; tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz""; num_examples: 1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10#issuecomment-350909874
https://github.com/google/deepvariant/issues/10#issuecomment-350952138:338,Modifiability,config,config,338,"I changed the ""num_examples"" form 1 to 2 only,not add any tfrecord.gz file, then it start run, but it is really slow, it have run two hours stilly. So is this situation normal? The tfrecord.gz file is about 2.4M size. ![image](https://user-images.githubusercontent.com/15261087/33869274-d250d54c-df42-11e7-9d37-a6cb401e4cdc.png). Here my config.txt:; ```; name: ""test-training-dataset""; tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz""; num_examples: 2; ```; This is my command; ```; python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt /leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt --start_from_checkpoint inception_v3.ckpt; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10#issuecomment-350952138
https://github.com/google/deepvariant/issues/10#issuecomment-350952138:705,Modifiability,config,config,705,"I changed the ""num_examples"" form 1 to 2 only,not add any tfrecord.gz file, then it start run, but it is really slow, it have run two hours stilly. So is this situation normal? The tfrecord.gz file is about 2.4M size. ![image](https://user-images.githubusercontent.com/15261087/33869274-d250d54c-df42-11e7-9d37-a6cb401e4cdc.png). Here my config.txt:; ```; name: ""test-training-dataset""; tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz""; num_examples: 2; ```; This is my command; ```; python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt /leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt --start_from_checkpoint inception_v3.ckpt; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10#issuecomment-350952138
https://github.com/google/deepvariant/issues/10#issuecomment-350952138:363,Testability,test,test-training-dataset,363,"I changed the ""num_examples"" form 1 to 2 only,not add any tfrecord.gz file, then it start run, but it is really slow, it have run two hours stilly. So is this situation normal? The tfrecord.gz file is about 2.4M size. ![image](https://user-images.githubusercontent.com/15261087/33869274-d250d54c-df42-11e7-9d37-a6cb401e4cdc.png). Here my config.txt:; ```; name: ""test-training-dataset""; tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz""; num_examples: 2; ```; This is my command; ```; python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt /leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt --start_from_checkpoint inception_v3.ckpt; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10#issuecomment-350952138
https://github.com/google/deepvariant/issues/10#issuecomment-351130156:610,Usability,learn,learning,610,"If I remember correctly, wildcards like * and ? should work. We can probably improve the comment there.; But concatenating everything together works too. You can directly cat all `*tfrecord.gz` into another big all.tfrecord.gz file. I would suggest trying wildcard first though. In terms of how to set num_examples: for now if you know roughly how many examples you have (for example, I can't remember if make_examples print out that information), you can just set a rough number. It's only being used here: ; https://github.com/google/deepvariant/blob/r0.4/deepvariant/model_train.py#L211; It does affect the learning rate decay, but it doesn't have to be exact.; I'll see if I can come back with a better example to count examples later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10#issuecomment-351130156
https://github.com/google/deepvariant/issues/10#issuecomment-361769858:0,Deployability,Update,Update,0,"Update:; if you look at https://github.com/google/deepvariant/blob/r0.5/docs/visualizing_examples.ipynb; the code in read_tfrecords is an example of how you can read a tfrecord file, and count examples if you like.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/10#issuecomment-361769858
https://github.com/google/deepvariant/issues/11#issuecomment-350780716:227,Deployability,update,update,227,"Hi Tomasz,; this issue might be the same as an earlier one. See this comment for a temporary solution:; https://github.com/google/deepvariant/issues/7#issuecomment-350528251. We are currently working on a fix, and will post an update to this github issue when it's fully fixed. Meanwhile, please let us know if you're having problems with the temporary fix.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/11#issuecomment-350780716
https://github.com/google/deepvariant/issues/12#issuecomment-351020472:201,Modifiability,extend,extend,201,"@pgrosu I am actually reckoning some sort of include order issue related to clif, which I am not quite familiar with. To my understanding, it is quite normal when defining new types, one would like to extend the defs in limits.h (really bad...). In my case I think it is telling me at the end of my include path list there is a include_next which leads to no where, I am not sure if what I need is a modified derivative for headers. ```; /root/opt/clif/clang/lib/clang/5.0.0/include/limits.h; /root/clif_backend/llvm/tools/clang/lib/Headers/limits.h; /root/clif_backend/build_matcher/lib/clang/5.0.0/include/limits.h; /usr/include/c++/v1/support/ibm/limits.h; /usr/include/c++/4.8/tr1/limits.h; /usr/include/c++/5/tr1/limits.h; /usr/include/limits.h; /usr/include/linux/limits.h; /usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/limits.h; /usr/lib/gcc/x86_64-linux-gnu/5/include-fixed/limits.h; /usr/src/linux-headers-4.4.0-103/arch/arm/include/asm/limits.h; /usr/src/linux-headers-4.4.0-103/include/uapi/linux/limits.h; ```. ```; includes = [; include_htslib,; ""."",; ""/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/"",; ]; ```. I actually modified the chunk in deepvariant/third_party/htslib.BUILD, to specifically include a particular location **which contains the type python needs and without the include_next loophole**, but still doesnot work, maybe the syntax was wrong, the build_and_test actually issued a warning saying . `WARNING: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/htslib/BUILD.bazel:216:16: in includes attribute of cc_library rule @htslib//:htslib: ignoring invalid absolute path '/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed'`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351020472
https://github.com/google/deepvariant/issues/12#issuecomment-351020472:1448,Performance,cache,cache,1448,"@pgrosu I am actually reckoning some sort of include order issue related to clif, which I am not quite familiar with. To my understanding, it is quite normal when defining new types, one would like to extend the defs in limits.h (really bad...). In my case I think it is telling me at the end of my include path list there is a include_next which leads to no where, I am not sure if what I need is a modified derivative for headers. ```; /root/opt/clif/clang/lib/clang/5.0.0/include/limits.h; /root/clif_backend/llvm/tools/clang/lib/Headers/limits.h; /root/clif_backend/build_matcher/lib/clang/5.0.0/include/limits.h; /usr/include/c++/v1/support/ibm/limits.h; /usr/include/c++/4.8/tr1/limits.h; /usr/include/c++/5/tr1/limits.h; /usr/include/limits.h; /usr/include/linux/limits.h; /usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/limits.h; /usr/lib/gcc/x86_64-linux-gnu/5/include-fixed/limits.h; /usr/src/linux-headers-4.4.0-103/arch/arm/include/asm/limits.h; /usr/src/linux-headers-4.4.0-103/include/uapi/linux/limits.h; ```. ```; includes = [; include_htslib,; ""."",; ""/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/"",; ]; ```. I actually modified the chunk in deepvariant/third_party/htslib.BUILD, to specifically include a particular location **which contains the type python needs and without the include_next loophole**, but still doesnot work, maybe the syntax was wrong, the build_and_test actually issued a warning saying . `WARNING: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/htslib/BUILD.bazel:216:16: in includes attribute of cc_library rule @htslib//:htslib: ignoring invalid absolute path '/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed'`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351020472
https://github.com/google/deepvariant/issues/12#issuecomment-351081960:54,Availability,error,errors,54,"Having personally fought through all sorts of similar errors when we were preparing the OSS release, I know how painful this is. Before diving into this, maybe you can tell me what you are trying to do here. Are you saying that you can't run build_and_tesh.sh without modification, and you are trying to overcome some issue that's not itemized here? Or are you trying to do something else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351081960
https://github.com/google/deepvariant/issues/12#issuecomment-351081960:92,Deployability,release,release,92,"Having personally fought through all sorts of similar errors when we were preparing the OSS release, I know how painful this is. Before diving into this, maybe you can tell me what you are trying to do here. Are you saying that you can't run build_and_tesh.sh without modification, and you are trying to overcome some issue that's not itemized here? Or are you trying to do something else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351081960
https://github.com/google/deepvariant/issues/12#issuecomment-351084133:183,Deployability,install,installed,183,"I do have one recommendation for you though; try building CLIF on your machine before running ./build-prereqs.sh. That script only uses our prebuilt CLIF binary if CLIF isn't already installed on your machine. It's possible that your system has local upgrades to ubuntu 16 that are confusing our pre-built binary (we are waiting for an official CLIF binary distribution, so in the meantime we are stuck with this less-than-ideal solution). The installation is pretty painless (https://github.com/google/clif#installation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351084133
https://github.com/google/deepvariant/issues/12#issuecomment-351084133:251,Deployability,upgrade,upgrades,251,"I do have one recommendation for you though; try building CLIF on your machine before running ./build-prereqs.sh. That script only uses our prebuilt CLIF binary if CLIF isn't already installed on your machine. It's possible that your system has local upgrades to ubuntu 16 that are confusing our pre-built binary (we are waiting for an official CLIF binary distribution, so in the meantime we are stuck with this less-than-ideal solution). The installation is pretty painless (https://github.com/google/clif#installation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351084133
https://github.com/google/deepvariant/issues/12#issuecomment-351084133:444,Deployability,install,installation,444,"I do have one recommendation for you though; try building CLIF on your machine before running ./build-prereqs.sh. That script only uses our prebuilt CLIF binary if CLIF isn't already installed on your machine. It's possible that your system has local upgrades to ubuntu 16 that are confusing our pre-built binary (we are waiting for an official CLIF binary distribution, so in the meantime we are stuck with this less-than-ideal solution). The installation is pretty painless (https://github.com/google/clif#installation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351084133
https://github.com/google/deepvariant/issues/12#issuecomment-351084133:508,Deployability,install,installation,508,"I do have one recommendation for you though; try building CLIF on your machine before running ./build-prereqs.sh. That script only uses our prebuilt CLIF binary if CLIF isn't already installed on your machine. It's possible that your system has local upgrades to ubuntu 16 that are confusing our pre-built binary (we are waiting for an official CLIF binary distribution, so in the meantime we are stuck with this less-than-ideal solution). The installation is pretty painless (https://github.com/google/clif#installation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351084133
https://github.com/google/deepvariant/issues/12#issuecomment-351255520:298,Deployability,Install,Install,298,"@depristo Thanks a lot for the clarification. ; In my case, I am trying to have a standalone version to test with, without GCP at this stage.; ; In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, ; since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:; ```; export DV_PLATFORM=""ubuntu-16""; cd ..; git clone https://github.com/google/clif ; cd clif; ./INSTALL.sh; python setup.py install; sudo ldconfig # Reload shared libraries.; ```; To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ?. `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351255520
https://github.com/google/deepvariant/issues/12#issuecomment-351255520:558,Deployability,INSTALL,INSTALL,558,"@depristo Thanks a lot for the clarification. ; In my case, I am trying to have a standalone version to test with, without GCP at this stage.; ; In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, ; since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:; ```; export DV_PLATFORM=""ubuntu-16""; cd ..; git clone https://github.com/google/clif ; cd clif; ./INSTALL.sh; python setup.py install; sudo ldconfig # Reload shared libraries.; ```; To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ?. `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351255520
https://github.com/google/deepvariant/issues/12#issuecomment-351255520:586,Deployability,install,install,586,"@depristo Thanks a lot for the clarification. ; In my case, I am trying to have a standalone version to test with, without GCP at this stage.; ; In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, ; since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:; ```; export DV_PLATFORM=""ubuntu-16""; cd ..; git clone https://github.com/google/clif ; cd clif; ./INSTALL.sh; python setup.py install; sudo ldconfig # Reload shared libraries.; ```; To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ?. `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351255520
https://github.com/google/deepvariant/issues/12#issuecomment-351255520:104,Testability,test,test,104,"@depristo Thanks a lot for the clarification. ; In my case, I am trying to have a standalone version to test with, without GCP at this stage.; ; In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, ; since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:; ```; export DV_PLATFORM=""ubuntu-16""; cd ..; git clone https://github.com/google/clif ; cd clif; ./INSTALL.sh; python setup.py install; sudo ldconfig # Reload shared libraries.; ```; To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ?. `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351255520
https://github.com/google/deepvariant/issues/12#issuecomment-351255520:922,Testability,test,testing,922,"@depristo Thanks a lot for the clarification. ; In my case, I am trying to have a standalone version to test with, without GCP at this stage.; ; In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, ; since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:; ```; export DV_PLATFORM=""ubuntu-16""; cd ..; git clone https://github.com/google/clif ; cd clif; ./INSTALL.sh; python setup.py install; sudo ldconfig # Reload shared libraries.; ```; To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ?. `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351255520
https://github.com/google/deepvariant/issues/12#issuecomment-351260272:375,Availability,echo,echo,375,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```; # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351260272
https://github.com/google/deepvariant/issues/12#issuecomment-351260272:527,Availability,down,download,527,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```; # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351260272
https://github.com/google/deepvariant/issues/12#issuecomment-351260272:742,Availability,echo,echo,742,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```; # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351260272
https://github.com/google/deepvariant/issues/12#issuecomment-351260272:935,Availability,echo,echo,935,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```; # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351260272
https://github.com/google/deepvariant/issues/12#issuecomment-351260272:1139,Availability,echo,echo,1139,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```; # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351260272
https://github.com/google/deepvariant/issues/12#issuecomment-351260272:168,Deployability,Install,Install,168,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```; # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351260272
https://github.com/google/deepvariant/issues/12#issuecomment-351260272:205,Deployability,install,install,205,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```; # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351260272
https://github.com/google/deepvariant/issues/12#issuecomment-351260272:256,Deployability,install,install,256,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```; # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351260272
https://github.com/google/deepvariant/issues/12#issuecomment-351260272:310,Deployability,install,install,310,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```; # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351260272
https://github.com/google/deepvariant/issues/12#issuecomment-351260272:350,Deployability,install,install,350,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```; # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351260272
https://github.com/google/deepvariant/issues/12#issuecomment-351260272:420,Deployability,install,install,420,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```; # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351260272
https://github.com/google/deepvariant/issues/12#issuecomment-351260272:518,Deployability,release,releases,518,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```; # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351260272
https://github.com/google/deepvariant/issues/12#issuecomment-351260272:714,Deployability,install,install,714,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```; # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351260272
https://github.com/google/deepvariant/issues/12#issuecomment-351260272:855,Deployability,INSTALL,INSTALL,855,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```; # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351260272
https://github.com/google/deepvariant/issues/12#issuecomment-351260272:891,Deployability,INSTALL,INSTALL,891,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```; # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351260272
https://github.com/google/deepvariant/issues/12#issuecomment-351260272:922,Deployability,INSTALL,INSTALL,922,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```; # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351260272
https://github.com/google/deepvariant/issues/12#issuecomment-351260272:1324,Deployability,INSTALL,INSTALL,1324,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```; # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351260272
https://github.com/google/deepvariant/issues/12#issuecomment-351260272:1354,Deployability,install,install,1354,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```; # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351260272
https://github.com/google/deepvariant/issues/12#issuecomment-351260272:1622,Deployability,install,install,1622,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```; # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351260272
https://github.com/google/deepvariant/issues/12#issuecomment-351260272:290,Modifiability,config,config,290,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```; # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351260272
https://github.com/google/deepvariant/issues/12#issuecomment-351260272:648,Modifiability,config,configure,648,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```; # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351260272
https://github.com/google/deepvariant/issues/12#issuecomment-351271203:339,Deployability,install,installation,339,"@depristo In fact, without touching any setup sh scripts, that same build_and_test attempt failed with a missing header of this prepend:; `--prepend clif/python/types.h`; That is why I ended up modifying `clif.bzl` in` third_party` to include the absolute path of this include, not sure if this is the culprit. I will try to move the clif installation to /usr/local to give it a shot, also with this new 0.4.1 release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351271203
https://github.com/google/deepvariant/issues/12#issuecomment-351271203:410,Deployability,release,release,410,"@depristo In fact, without touching any setup sh scripts, that same build_and_test attempt failed with a missing header of this prepend:; `--prepend clif/python/types.h`; That is why I ended up modifying `clif.bzl` in` third_party` to include the absolute path of this include, not sure if this is the culprit. I will try to move the clif installation to /usr/local to give it a shot, also with this new 0.4.1 release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351271203
https://github.com/google/deepvariant/issues/12#issuecomment-351271748:134,Modifiability,sandbox,sandboxing,134,"Yes, you can't do that with bazel - it doesn't allow you to do absolute path operations like that in general due to their approach to sandboxing / hermetic builds. I suspect moving CLIF to the expected location may fix it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351271748
https://github.com/google/deepvariant/issues/12#issuecomment-351271748:134,Testability,sandbox,sandboxing,134,"Yes, you can't do that with bazel - it doesn't allow you to do absolute path operations like that in general due to their approach to sandboxing / hermetic builds. I suspect moving CLIF to the expected location may fix it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351271748
https://github.com/google/deepvariant/issues/12#issuecomment-351350738:411,Deployability,install,installation,411,"@depristo The porting to /usr/local works, thanks a lot. With 0.4.0 build_and_test did complain about the htslib_gcp_oauth.init issue, which is now fixed in 0.4.1. ```; (18:20:33) INFO: Elapsed time: 2.092s, Critical Path: 0.03s; (18:20:33) INFO: Build completed successfully, 2 total actions; ```; Case closed, cheers. . oops, just to append a comment, by exporting an include variable with a non `/usr/local` installation of `clif` doesnot help, `pyclif` still cant find it. This was tested before touching `clif.bzl`. This `clif` thing must have its own include logic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351350738
https://github.com/google/deepvariant/issues/12#issuecomment-351350738:378,Modifiability,variab,variable,378,"@depristo The porting to /usr/local works, thanks a lot. With 0.4.0 build_and_test did complain about the htslib_gcp_oauth.init issue, which is now fixed in 0.4.1. ```; (18:20:33) INFO: Elapsed time: 2.092s, Critical Path: 0.03s; (18:20:33) INFO: Build completed successfully, 2 total actions; ```; Case closed, cheers. . oops, just to append a comment, by exporting an include variable with a non `/usr/local` installation of `clif` doesnot help, `pyclif` still cant find it. This was tested before touching `clif.bzl`. This `clif` thing must have its own include logic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351350738
https://github.com/google/deepvariant/issues/12#issuecomment-351350738:486,Testability,test,tested,486,"@depristo The porting to /usr/local works, thanks a lot. With 0.4.0 build_and_test did complain about the htslib_gcp_oauth.init issue, which is now fixed in 0.4.1. ```; (18:20:33) INFO: Elapsed time: 2.092s, Critical Path: 0.03s; (18:20:33) INFO: Build completed successfully, 2 total actions; ```; Case closed, cheers. . oops, just to append a comment, by exporting an include variable with a non `/usr/local` installation of `clif` doesnot help, `pyclif` still cant find it. This was tested before touching `clif.bzl`. This `clif` thing must have its own include logic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351350738
https://github.com/google/deepvariant/issues/12#issuecomment-351350738:565,Testability,log,logic,565,"@depristo The porting to /usr/local works, thanks a lot. With 0.4.0 build_and_test did complain about the htslib_gcp_oauth.init issue, which is now fixed in 0.4.1. ```; (18:20:33) INFO: Elapsed time: 2.092s, Critical Path: 0.03s; (18:20:33) INFO: Build completed successfully, 2 total actions; ```; Case closed, cheers. . oops, just to append a comment, by exporting an include variable with a non `/usr/local` installation of `clif` doesnot help, `pyclif` still cant find it. This was tested before touching `clif.bzl`. This `clif` thing must have its own include logic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/12#issuecomment-351350738
https://github.com/google/deepvariant/issues/13#issuecomment-351172185:725,Deployability,release,release,725,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13#issuecomment-351172185
https://github.com/google/deepvariant/issues/13#issuecomment-351172185:3,Integrability,depend,depends,3,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13#issuecomment-351172185
https://github.com/google/deepvariant/issues/13#issuecomment-351172185:623,Modifiability,extend,extending,623,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13#issuecomment-351172185
https://github.com/google/deepvariant/issues/13#issuecomment-351172185:839,Testability,log,logic,839,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13#issuecomment-351172185
https://github.com/google/deepvariant/issues/13#issuecomment-351172185:36,Usability,simpl,simplest,36,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13#issuecomment-351172185
https://github.com/google/deepvariant/issues/13#issuecomment-351172185:170,Usability,simpl,simplistic,170,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13#issuecomment-351172185
https://github.com/google/deepvariant/issues/13#issuecomment-351172185:480,Usability,usab,usable,480,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/13#issuecomment-351172185
https://github.com/google/deepvariant/issues/14#issuecomment-351176839:157,Availability,avail,available,157,"We are pretty certain this is due to the same bug noted in https://github.com/google/deepvariant/issues/7. We have a fix already internally and will make it available as soon as possible. In the meantime, if you *need* this to work you can follow the same trick of commenting out the call to `token = cloud_utils.oauth2_token()` and replacing it with `token = None`. Sorry about that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/14#issuecomment-351176839
https://github.com/google/deepvariant/issues/15#issuecomment-351194795:124,Performance,perform,performing,124,"That's a good question. We use this BED file to provide to the calling programs where possible so that we don't use compute performing calling in regions we won't be evaluating on. You are correct, doing the intersection in hap.py will work just fine in terms of the final results. Here is the non-intersected BED file: . *edited by adding file attachment; [agilent_sureselect_human_all_exon_v5_b37_targets.bed.gz](https://github.com/google/deepvariant/files/3875984/agilent_sureselect_human_all_exon_v5_b37_targets.bed.gz)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/15#issuecomment-351194795
https://github.com/google/deepvariant/issues/15#issuecomment-351203489:92,Deployability,update,update,92,Thanks Andrew! We've put this on our internal buganizer component for DeepVariant and we'll update the case study to use this.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/15#issuecomment-351203489
https://github.com/google/deepvariant/issues/16#issuecomment-351298307:111,Performance,perform,performed,111,"If you could describe your environment in more detail (which OS, CPU), and the full sequence of operations you performed (copy and paste the text from the terminal), it will help us to diagnose the problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16#issuecomment-351298307
https://github.com/google/deepvariant/issues/16#issuecomment-351322489:506,Availability,echo,echo,506,"Thank you for your reply @scott7z. CPU and OS infomation:; `uname -a`; Linux 6562232b4f47 4.4.0-57-generic #78-Ubuntu SMP Fri Dec 9 23:50:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux ; `head -n 1 /etc/issue`; Ubuntu 16.04.1 LTS \n \l. First, when I ran a container for deepvariant and executed the make_examples command; `./make_examples`; Nothing happened; `python make_examples.zip`; Still nothing happened. `unzip make_examples.zip`; `cd runfiles/genomics/deepvariant` ; #Add deepvariant to PYTHONPATH ; `echo ""export PYTHONPATH=\$PYTHONPATH:/opt/deepvariant/bin/runfiles/genomics"" >> /root/.bashrc` `source /root/.bashrc` ; `python make_examples.py`; Then I got; ""Illegal instruction (core dumped)"". This are dockerfile, run-prereq.sh and setting.sh, for reference.; [Dockerfile.zip](https://github.com/google/deepvariant/files/1554745/Dockerfile.zip)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16#issuecomment-351322489
https://github.com/google/deepvariant/issues/16#issuecomment-351583972:374,Availability,down,downloaded,374,"This directory here contains `dockerfile` and other files that need to be `ADD` to the image; ![image](https://user-images.githubusercontent.com/25972546/33970607-df14b3b2-e0ae-11e7-8aea-0509ae0c9c61.png); `docker build -f dockerfile -t deepvariant_1214 .`; `Successfully built b829b45a9401`. This directory contains the deepvariant `models` and `quickstart-testdata` files downloaded from [https://console.cloud.google.com/storage/browser/deepvariant](url); ![image](https://user-images.githubusercontent.com/25972546/33970996-1a4e378a-e0b1-11e7-8852-9b63cb477f47.png); `docker run --name deepvariant_test_1214 -v /***/bioinfo/wangwd/workflow/deepvariant/data:/data -it b829b45a9401 /bin/bash`; Then I `cd /opt/deepvariant/bin/`; ![image](https://user-images.githubusercontent.com/25972546/33971209-4789bc8c-e0b2-11e7-833c-95f440712240.png); `./make_examples \; --mode calling \; --ref /data/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; --reads /data/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --examples /data/quickstart-output/examples.tfrecord.gz`; When I finished executing this command, I did not get any result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16#issuecomment-351583972
https://github.com/google/deepvariant/issues/16#issuecomment-351583972:358,Testability,test,testdata,358,"This directory here contains `dockerfile` and other files that need to be `ADD` to the image; ![image](https://user-images.githubusercontent.com/25972546/33970607-df14b3b2-e0ae-11e7-8aea-0509ae0c9c61.png); `docker build -f dockerfile -t deepvariant_1214 .`; `Successfully built b829b45a9401`. This directory contains the deepvariant `models` and `quickstart-testdata` files downloaded from [https://console.cloud.google.com/storage/browser/deepvariant](url); ![image](https://user-images.githubusercontent.com/25972546/33970996-1a4e378a-e0b1-11e7-8852-9b63cb477f47.png); `docker run --name deepvariant_test_1214 -v /***/bioinfo/wangwd/workflow/deepvariant/data:/data -it b829b45a9401 /bin/bash`; Then I `cd /opt/deepvariant/bin/`; ![image](https://user-images.githubusercontent.com/25972546/33971209-4789bc8c-e0b2-11e7-833c-95f440712240.png); `./make_examples \; --mode calling \; --ref /data/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; --reads /data/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --examples /data/quickstart-output/examples.tfrecord.gz`; When I finished executing this command, I did not get any result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16#issuecomment-351583972
https://github.com/google/deepvariant/issues/16#issuecomment-351583972:904,Testability,test,testdata,904,"This directory here contains `dockerfile` and other files that need to be `ADD` to the image; ![image](https://user-images.githubusercontent.com/25972546/33970607-df14b3b2-e0ae-11e7-8aea-0509ae0c9c61.png); `docker build -f dockerfile -t deepvariant_1214 .`; `Successfully built b829b45a9401`. This directory contains the deepvariant `models` and `quickstart-testdata` files downloaded from [https://console.cloud.google.com/storage/browser/deepvariant](url); ![image](https://user-images.githubusercontent.com/25972546/33970996-1a4e378a-e0b1-11e7-8852-9b63cb477f47.png); `docker run --name deepvariant_test_1214 -v /***/bioinfo/wangwd/workflow/deepvariant/data:/data -it b829b45a9401 /bin/bash`; Then I `cd /opt/deepvariant/bin/`; ![image](https://user-images.githubusercontent.com/25972546/33971209-4789bc8c-e0b2-11e7-833c-95f440712240.png); `./make_examples \; --mode calling \; --ref /data/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; --reads /data/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --examples /data/quickstart-output/examples.tfrecord.gz`; When I finished executing this command, I did not get any result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16#issuecomment-351583972
https://github.com/google/deepvariant/issues/16#issuecomment-351583972:972,Testability,test,testdata,972,"This directory here contains `dockerfile` and other files that need to be `ADD` to the image; ![image](https://user-images.githubusercontent.com/25972546/33970607-df14b3b2-e0ae-11e7-8aea-0509ae0c9c61.png); `docker build -f dockerfile -t deepvariant_1214 .`; `Successfully built b829b45a9401`. This directory contains the deepvariant `models` and `quickstart-testdata` files downloaded from [https://console.cloud.google.com/storage/browser/deepvariant](url); ![image](https://user-images.githubusercontent.com/25972546/33970996-1a4e378a-e0b1-11e7-8852-9b63cb477f47.png); `docker run --name deepvariant_test_1214 -v /***/bioinfo/wangwd/workflow/deepvariant/data:/data -it b829b45a9401 /bin/bash`; Then I `cd /opt/deepvariant/bin/`; ![image](https://user-images.githubusercontent.com/25972546/33971209-4789bc8c-e0b2-11e7-833c-95f440712240.png); `./make_examples \; --mode calling \; --ref /data/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; --reads /data/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --examples /data/quickstart-output/examples.tfrecord.gz`; When I finished executing this command, I did not get any result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16#issuecomment-351583972
https://github.com/google/deepvariant/issues/16#issuecomment-351757223:15,Deployability,update,updates,15,"Thanks for the updates. It looks like you are building a customized docker image. Before we can debug that, could you please confirm whether you can:. - Run quickstart on your machine (without using docker)? https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md; - Run DeepVariant using our provided docker image from gcr.io/deepvariant-docker/deepvariant:0.4.1? https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-docker.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16#issuecomment-351757223
https://github.com/google/deepvariant/issues/16#issuecomment-351842327:200,Integrability,message,message,200,"Also, if you can show us the output of; cat /proc/cpuinfo; that will help. It's possible the pre-compiled binaries require a newer CPU than you are using,; which could explain the illegal instruction message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16#issuecomment-351842327
https://github.com/google/deepvariant/issues/16#issuecomment-351888260:147,Deployability,install,installed,147,Cloud Platform CPU Information：; ![image](https://user-images.githubusercontent.com/25972546/34021423-c8d9a4a6-e174-11e7-8a3d-a34bf1acd12d.png). I installed a Ubuntu 16.04.3 virtual machine on my laptop and it works with or without docker @arostamianfar ; Laptop CPU information：; ![image](https://user-images.githubusercontent.com/25972546/34021793-a91f9614-e176-11e7-98ad-b3c948578613.png); ![image](https://user-images.githubusercontent.com/25972546/34021800-b6b94d10-e176-11e7-8110-623c2d7a73c4.png),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16#issuecomment-351888260
https://github.com/google/deepvariant/issues/16#issuecomment-352893787:688,Availability,avail,available,688,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system.; The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,; The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture.; So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16#issuecomment-352893787
https://github.com/google/deepvariant/issues/16#issuecomment-352893787:365,Modifiability,variab,variable,365,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system.; The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,; The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture.; So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16#issuecomment-352893787
https://github.com/google/deepvariant/issues/16#issuecomment-352893787:272,Performance,tune,tuned,272,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system.; The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,; The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture.; So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16#issuecomment-352893787
https://github.com/google/deepvariant/issues/16#issuecomment-352893787:609,Safety,detect,detect,609,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system.; The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,; The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture.; So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16#issuecomment-352893787
https://github.com/google/deepvariant/issues/16#issuecomment-352945209:201,Testability,test,test,201,Thanks for your reminder @scott7z ; I ignored what you mentioned.; But for now I do not plan to make any further changes.; I've converted to a platform that supports AVX2 and completed a 'quick-start' test.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/16#issuecomment-352945209
https://github.com/google/deepvariant/issues/17#issuecomment-352085708:352,Availability,down,download,352,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud.; One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:; https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17#issuecomment-352085708
https://github.com/google/deepvariant/issues/17#issuecomment-352085708:118,Deployability,install,install,118,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud.; One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:; https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17#issuecomment-352085708
https://github.com/google/deepvariant/issues/17#issuecomment-352085708:278,Security,access,access,278,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud.; One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:; https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17#issuecomment-352085708
https://github.com/google/deepvariant/issues/17#issuecomment-352085708:595,Testability,test,test,595,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud.; One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:; https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17#issuecomment-352085708
https://github.com/google/deepvariant/issues/17#issuecomment-352085708:836,Testability,test,test-data,836,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud.; One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:; https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17#issuecomment-352085708
https://github.com/google/deepvariant/issues/17#issuecomment-352605040:54,Availability,down,down,54,"I'm going to close this issue, since it seems to boil down to either a network issue or a problem installing gsutil.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17#issuecomment-352605040
https://github.com/google/deepvariant/issues/17#issuecomment-352605040:98,Deployability,install,installing,98,"I'm going to close this issue, since it seems to boil down to either a network issue or a problem installing gsutil.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/17#issuecomment-352605040
https://github.com/google/deepvariant/pull/18#issuecomment-353062531:805,Security,authoriz,authorized,805,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify. Thanks. ---. - If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; - If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.; - In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/18#issuecomment-353062531
https://github.com/google/deepvariant/pull/18#issuecomment-353062531:930,Security,authoriz,authorized,930,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify. Thanks. ---. - If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; - If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.; - In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/18#issuecomment-353062531
https://github.com/google/deepvariant/pull/18#issuecomment-353062531:1094,Security,authoriz,authorized,1094,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify. Thanks. ---. - If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; - If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.; - In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/18#issuecomment-353062531
https://github.com/google/deepvariant/pull/18#issuecomment-353673281:300,Deployability,patch,patch,300,"Hi Brad,. Thank you so much for this pull request. At the moment we cannot accept external contributions (see https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md) as we don't yet have a mechanism setup to resync changes from github back into our codebase within Google. We are happy to patch this internally so it'll appear in the next release. Are you ok with that?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/18#issuecomment-353673281
https://github.com/google/deepvariant/pull/18#issuecomment-353673281:350,Deployability,release,release,350,"Hi Brad,. Thank you so much for this pull request. At the moment we cannot accept external contributions (see https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md) as we don't yet have a mechanism setup to resync changes from github back into our codebase within Google. We are happy to patch this internally so it'll appear in the next release. Are you ok with that?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/18#issuecomment-353673281
https://github.com/google/deepvariant/pull/18#issuecomment-353720479:169,Availability,avail,available,169,Patching it internally and releasing it works great with me. I appreciate y'all considering this and pushing it through on the backend. Thank you for making DeepVariant available and all the helpful support.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/18#issuecomment-353720479
https://github.com/google/deepvariant/pull/18#issuecomment-353720479:0,Deployability,Patch,Patching,0,Patching it internally and releasing it works great with me. I appreciate y'all considering this and pushing it through on the backend. Thank you for making DeepVariant available and all the helpful support.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/18#issuecomment-353720479
https://github.com/google/deepvariant/pull/18#issuecomment-355332563:39,Deployability,release,release,39,This is in and will go out in the next release. Thank you so much Brad!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/18#issuecomment-355332563
https://github.com/google/deepvariant/issues/19#issuecomment-353388370:19,Availability,down,downgrading,19,"I can confirm that downgrading bazel to 0.8.1 as suggested in that comment worked for me.; Here's what I did to install an older version of bazel:; ```; BAZEL_VERSION=0.8.1; wget https://github.com/bazelbuild/bazel/releases/download/""${BAZEL_VERSION}""/bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh; chmod +x bazel-*.sh; ./bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh --user; ```; Give this temporary solution a try and let me know if it works. We'll still need to fix this in a more principled way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353388370
https://github.com/google/deepvariant/issues/19#issuecomment-353388370:224,Availability,down,download,224,"I can confirm that downgrading bazel to 0.8.1 as suggested in that comment worked for me.; Here's what I did to install an older version of bazel:; ```; BAZEL_VERSION=0.8.1; wget https://github.com/bazelbuild/bazel/releases/download/""${BAZEL_VERSION}""/bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh; chmod +x bazel-*.sh; ./bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh --user; ```; Give this temporary solution a try and let me know if it works. We'll still need to fix this in a more principled way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353388370
https://github.com/google/deepvariant/issues/19#issuecomment-353388370:112,Deployability,install,install,112,"I can confirm that downgrading bazel to 0.8.1 as suggested in that comment worked for me.; Here's what I did to install an older version of bazel:; ```; BAZEL_VERSION=0.8.1; wget https://github.com/bazelbuild/bazel/releases/download/""${BAZEL_VERSION}""/bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh; chmod +x bazel-*.sh; ./bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh --user; ```; Give this temporary solution a try and let me know if it works. We'll still need to fix this in a more principled way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353388370
https://github.com/google/deepvariant/issues/19#issuecomment-353388370:215,Deployability,release,releases,215,"I can confirm that downgrading bazel to 0.8.1 as suggested in that comment worked for me.; Here's what I did to install an older version of bazel:; ```; BAZEL_VERSION=0.8.1; wget https://github.com/bazelbuild/bazel/releases/download/""${BAZEL_VERSION}""/bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh; chmod +x bazel-*.sh; ./bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh --user; ```; Give this temporary solution a try and let me know if it works. We'll still need to fix this in a more principled way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353388370
https://github.com/google/deepvariant/issues/19#issuecomment-353388370:277,Deployability,install,installer-linux-,277,"I can confirm that downgrading bazel to 0.8.1 as suggested in that comment worked for me.; Here's what I did to install an older version of bazel:; ```; BAZEL_VERSION=0.8.1; wget https://github.com/bazelbuild/bazel/releases/download/""${BAZEL_VERSION}""/bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh; chmod +x bazel-*.sh; ./bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh --user; ```; Give this temporary solution a try and let me know if it works. We'll still need to fix this in a more principled way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353388370
https://github.com/google/deepvariant/issues/19#issuecomment-353388370:352,Deployability,install,installer-linux-,352,"I can confirm that downgrading bazel to 0.8.1 as suggested in that comment worked for me.; Here's what I did to install an older version of bazel:; ```; BAZEL_VERSION=0.8.1; wget https://github.com/bazelbuild/bazel/releases/download/""${BAZEL_VERSION}""/bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh; chmod +x bazel-*.sh; ./bazel-""${BAZEL_VERSION}""-installer-linux-x86_64.sh --user; ```; Give this temporary solution a try and let me know if it works. We'll still need to fix this in a more principled way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353388370
https://github.com/google/deepvariant/issues/19#issuecomment-353393763:21,Deployability,install,install,21,"It might not hurt to install a [Jenkins server](https://jenkins-ci.org/) - similar how the protobuf team does it - for continuous integration, as it also plays well with Github PRs and alleviates these simple headaches.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353393763
https://github.com/google/deepvariant/issues/19#issuecomment-353393763:119,Deployability,continuous,continuous,119,"It might not hurt to install a [Jenkins server](https://jenkins-ci.org/) - similar how the protobuf team does it - for continuous integration, as it also plays well with Github PRs and alleviates these simple headaches.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353393763
https://github.com/google/deepvariant/issues/19#issuecomment-353393763:130,Deployability,integrat,integration,130,"It might not hurt to install a [Jenkins server](https://jenkins-ci.org/) - similar how the protobuf team does it - for continuous integration, as it also plays well with Github PRs and alleviates these simple headaches.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353393763
https://github.com/google/deepvariant/issues/19#issuecomment-353393763:130,Integrability,integrat,integration,130,"It might not hurt to install a [Jenkins server](https://jenkins-ci.org/) - similar how the protobuf team does it - for continuous integration, as it also plays well with Github PRs and alleviates these simple headaches.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353393763
https://github.com/google/deepvariant/issues/19#issuecomment-353393763:202,Usability,simpl,simple,202,"It might not hurt to install a [Jenkins server](https://jenkins-ci.org/) - similar how the protobuf team does it - for continuous integration, as it also plays well with Github PRs and alleviates these simple headaches.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353393763
https://github.com/google/deepvariant/issues/19#issuecomment-353466661:0,Deployability,Install,Installing,0,Installing bazel 0.8.1 fixed the issue for me. It might be worth updating `build-prereq.sh` to install a specific version for bazel. Thanks a lot for the help,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353466661
https://github.com/google/deepvariant/issues/19#issuecomment-353466661:95,Deployability,install,install,95,Installing bazel 0.8.1 fixed the issue for me. It might be worth updating `build-prereq.sh` to install a specific version for bazel. Thanks a lot for the help,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353466661
https://github.com/google/deepvariant/issues/19#issuecomment-353481304:105,Modifiability,evolve,evolved,105,"The root cause here is that we are cloning a specific version of Tensorflow, while it (and other things) evolved to deal with the Bazel change. Normally, though, we don't want to just clone from TF HEAD because that can break us in other ways. So a fix is to change settings.sh to use a more recent commit, like; export DV_TENSORFLOW_GIT_SHA=""97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a""; (That compiles, but is otherwise untested.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353481304
https://github.com/google/deepvariant/issues/19#issuecomment-353510712:222,Deployability,release,release,222,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash; $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'; 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a; 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2; $; ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353510712
https://github.com/google/deepvariant/issues/19#issuecomment-353510712:310,Integrability,depend,dependencies,310,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash; $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'; 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a; 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2; $; ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353510712
https://github.com/google/deepvariant/issues/19#issuecomment-353510712:427,Integrability,depend,dependencies,427,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash; $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'; 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a; 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2; $; ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353510712
https://github.com/google/deepvariant/issues/19#issuecomment-353510712:932,Integrability,depend,dependency,932,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash; $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'; 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a; 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2; $; ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353510712
https://github.com/google/deepvariant/issues/19#issuecomment-353510712:340,Modifiability,evolve,evolve,340,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash; $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'; 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a; 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2; $; ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353510712
https://github.com/google/deepvariant/issues/19#issuecomment-353510712:667,Testability,log,log,667,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash; $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'; 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a; 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2; $; ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353510712
https://github.com/google/deepvariant/issues/19#issuecomment-353510712:367,Usability,simpl,simplify,367,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash; $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'; 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a; 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2; $; ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/19#issuecomment-353510712
https://github.com/google/deepvariant/issues/21#issuecomment-353701703:192,Availability,avail,available,192,"Hi Paul, thanks for mentioning this issue.; I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant).; These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for.; Would it be ok for you to build DeepVariant for your CPU by following [Building and testing; DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353701703
https://github.com/google/deepvariant/issues/21#issuecomment-353701703:94,Deployability,update,update,94,"Hi Paul, thanks for mentioning this issue.; I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant).; These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for.; Would it be ok for you to build DeepVariant for your CPU by following [Building and testing; DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353701703
https://github.com/google/deepvariant/issues/21#issuecomment-353701703:384,Integrability,Bridg,Bridge,384,"Hi Paul, thanks for mentioning this issue.; I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant).; These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for.; Would it be ok for you to build DeepVariant for your CPU by following [Building and testing; DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353701703
https://github.com/google/deepvariant/issues/21#issuecomment-353701703:677,Performance,optimiz,optimization,677,"Hi Paul, thanks for mentioning this issue.; I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant).; These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for.; Would it be ok for you to build DeepVariant for your CPU by following [Building and testing; DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353701703
https://github.com/google/deepvariant/issues/21#issuecomment-353701703:877,Testability,test,testing,877,"Hi Paul, thanks for mentioning this issue.; I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant).; These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for.; Would it be ok for you to build DeepVariant for your CPU by following [Building and testing; DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353701703
https://github.com/google/deepvariant/issues/21#issuecomment-353701703:922,Testability,test,test,922,"Hi Paul, thanks for mentioning this issue.; I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant).; These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for.; Would it be ok for you to build DeepVariant for your CPU by following [Building and testing; DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353701703
https://github.com/google/deepvariant/issues/21#issuecomment-353701703:753,Usability,clear,clear,753,"Hi Paul, thanks for mentioning this issue.; I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant).; These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for.; Would it be ok for you to build DeepVariant for your CPU by following [Building and testing; DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353701703
https://github.com/google/deepvariant/issues/21#issuecomment-353712933:463,Deployability,release,release-notes,463,"Hi Pichuan,. That's a great idea to add it to the README, as it's probably the first thing users see. For those who might miss noticing its importance in the README, it probably would not hurt adding an assert statement to the GCS zip-specific `make_examples.py`, for the appropriate flags in `/proc/cpuinfo` with a gentle commented termination. I also just noticed it with a search here as well:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-release-notes.md#040. Thank you for the offer regarding the customized binaries, but it was just a few minor changes and I got working now. I was just mentioning it in case others might run into that issue, and would wonder why it exited. Thanks,; `p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353712933
https://github.com/google/deepvariant/issues/21#issuecomment-353712933:203,Testability,assert,assert,203,"Hi Pichuan,. That's a great idea to add it to the README, as it's probably the first thing users see. For those who might miss noticing its importance in the README, it probably would not hurt adding an assert statement to the GCS zip-specific `make_examples.py`, for the appropriate flags in `/proc/cpuinfo` with a gentle commented termination. I also just noticed it with a search here as well:. https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-release-notes.md#040. Thank you for the offer regarding the customized binaries, but it was just a few minor changes and I got working now. I was just mentioning it in case others might run into that issue, and would wonder why it exited. Thanks,; `p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353712933
https://github.com/google/deepvariant/issues/21#issuecomment-353730864:98,Availability,error,error,98,Thanks Paul.; I added an internal bug to track your suggestion. DeepVariant still has quite a lot error messages that have room for improvement. Thanks for reporting this to us so we can itemize them and improve over time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353730864
https://github.com/google/deepvariant/issues/21#issuecomment-353730864:104,Integrability,message,messages,104,Thanks Paul.; I added an internal bug to track your suggestion. DeepVariant still has quite a lot error messages that have room for improvement. Thanks for reporting this to us so we can itemize them and improve over time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353730864
https://github.com/google/deepvariant/issues/21#issuecomment-353849339:2275,Availability,down,down,2275,"e more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_set_name: ""Sample_Diag-excap51-HG002-EEogPU""; }; end: 1115835; reference_name: ""1""; start: 1115834; ...; ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339
https://github.com/google/deepvariant/issues/21#issuecomment-353849339:2287,Availability,avail,available,2287,"e more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_set_name: ""Sample_Diag-excap51-HG002-EEogPU""; }; end: 1115835; reference_name: ""1""; start: 1115834; ...; ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339
https://github.com/google/deepvariant/issues/21#issuecomment-353849339:3184,Availability,down,down,3184,"e more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_set_name: ""Sample_Diag-excap51-HG002-EEogPU""; }; end: 1115835; reference_name: ""1""; start: 1115834; ...; ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339
https://github.com/google/deepvariant/issues/21#issuecomment-353849339:1309,Deployability,pipeline,pipelines,1309,"d much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```; genotype_probabilities: 0.9999428988; genotype_probabilities: 1.8287e-05; genotype_probabilities: 3.88142e-05; ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_set_name: ""Sample_Diag-excap51-HG002-EEogPU""; }; end: 1115835; reference_name: ""1""; start: 1115834; ...; ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339
https://github.com/google/deepvariant/issues/21#issuecomment-353849339:2498,Deployability,pipeline,pipelines,2498,"e more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_set_name: ""Sample_Diag-excap51-HG002-EEogPU""; }; end: 1115835; reference_name: ""1""; start: 1115834; ...; ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339
https://github.com/google/deepvariant/issues/21#issuecomment-353849339:2760,Deployability,integrat,integrated,2760,"e more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_set_name: ""Sample_Diag-excap51-HG002-EEogPU""; }; end: 1115835; reference_name: ""1""; start: 1115834; ...; ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339
https://github.com/google/deepvariant/issues/21#issuecomment-353849339:2940,Deployability,pipeline,pipelines,2940,"e more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_set_name: ""Sample_Diag-excap51-HG002-EEogPU""; }; end: 1115835; reference_name: ""1""; start: 1115834; ...; ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339
https://github.com/google/deepvariant/issues/21#issuecomment-353849339:2964,Deployability,integrat,integration,2964,"e more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_set_name: ""Sample_Diag-excap51-HG002-EEogPU""; }; end: 1115835; reference_name: ""1""; start: 1115834; ...; ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339
https://github.com/google/deepvariant/issues/21#issuecomment-353849339:229,Integrability,wrap,wrapper,229,"Hi Pichuan,. It's a team effort :) Time is on our side as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```; genotype_probabilities: 0.9999428988; genotype_probabilities: 1.8287e-05; genotype_probabilities: 3.88142e-05; ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339
https://github.com/google/deepvariant/issues/21#issuecomment-353849339:1748,Integrability,message,message,1748,"abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_set_name: ""Sample_Diag-excap51-HG002-EEogPU""; }; end: 1115835; reference_name: ""1""; start: 1115834; ...; ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339
https://github.com/google/deepvariant/issues/21#issuecomment-353849339:2760,Integrability,integrat,integrated,2760,"e more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_set_name: ""Sample_Diag-excap51-HG002-EEogPU""; }; end: 1115835; reference_name: ""1""; start: 1115834; ...; ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339
https://github.com/google/deepvariant/issues/21#issuecomment-353849339:2964,Integrability,integrat,integration,2964,"e more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_set_name: ""Sample_Diag-excap51-HG002-EEogPU""; }; end: 1115835; reference_name: ""1""; start: 1115834; ...; ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339
https://github.com/google/deepvariant/issues/21#issuecomment-353849339:2168,Modifiability,flexible,flexible,2168,"re changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_set_name: ""Sample_Diag-excap51-HG002-EEogPU""; }; end: 1115835; reference_name: ""1""; start: 1115834; ...; ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339
https://github.com/google/deepvariant/issues/21#issuecomment-353849339:3097,Modifiability,refactor,refactoring,3097,"e more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_set_name: ""Sample_Diag-excap51-HG002-EEogPU""; }; end: 1115835; reference_name: ""1""; start: 1115834; ...; ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339
https://github.com/google/deepvariant/issues/21#issuecomment-353849339:2568,Performance,cache,cached,2568,"e more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_set_name: ""Sample_Diag-excap51-HG002-EEogPU""; }; end: 1115835; reference_name: ""1""; start: 1115834; ...; ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339
https://github.com/google/deepvariant/issues/21#issuecomment-353849339:512,Safety,predict,predicted,512,"Hi Pichuan,. It's a team effort :) Time is on our side as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```; genotype_probabilities: 0.9999428988; genotype_probabilities: 1.8287e-05; genotype_probabilities: 3.88142e-05; ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339
https://github.com/google/deepvariant/issues/21#issuecomment-353849339:1812,Security,validat,validation,1812,"abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_set_name: ""Sample_Diag-excap51-HG002-EEogPU""; }; end: 1115835; reference_name: ""1""; start: 1115834; ...; ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339
https://github.com/google/deepvariant/issues/21#issuecomment-355389148:341,Deployability,release,releases,341,I'm closing this issue because we aren't likely to provide prebuilt binaries *without* AVX instructions. One reason is that the AVX instructions are critical to efficiently evaluate our deep learning model. Another is that TensorFlow itself will soon provide prebuilt binaries with AVX instructions (https://github.com/tensorflow/tensorflow/releases). . Users who need to run DeepVariant on pre-AVX instruction chipsets should build DeepVariant from sources.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355389148
https://github.com/google/deepvariant/issues/21#issuecomment-355389148:161,Energy Efficiency,efficient,efficiently,161,I'm closing this issue because we aren't likely to provide prebuilt binaries *without* AVX instructions. One reason is that the AVX instructions are critical to efficiently evaluate our deep learning model. Another is that TensorFlow itself will soon provide prebuilt binaries with AVX instructions (https://github.com/tensorflow/tensorflow/releases). . Users who need to run DeepVariant on pre-AVX instruction chipsets should build DeepVariant from sources.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355389148
https://github.com/google/deepvariant/issues/21#issuecomment-355389148:191,Usability,learn,learning,191,I'm closing this issue because we aren't likely to provide prebuilt binaries *without* AVX instructions. One reason is that the AVX instructions are critical to efficiently evaluate our deep learning model. Another is that TensorFlow itself will soon provide prebuilt binaries with AVX instructions (https://github.com/tensorflow/tensorflow/releases). . Users who need to run DeepVariant on pre-AVX instruction chipsets should build DeepVariant from sources.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355389148
https://github.com/google/deepvariant/issues/21#issuecomment-355440557:1251,Deployability,pipeline,pipeline,1251,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | ; | ------------ | ----------- | ------------- | ------------- | ------------- | ; | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | ; | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | ; | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | ; | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355440557
https://github.com/google/deepvariant/issues/21#issuecomment-355440557:1451,Deployability,pipeline,pipeline,1451,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | ; | ------------ | ----------- | ------------- | ------------- | ------------- | ; | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | ; | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | ; | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | ; | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355440557
https://github.com/google/deepvariant/issues/21#issuecomment-355440557:1276,Modifiability,plug-in,plug-in,1276,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | ; | ------------ | ----------- | ------------- | ------------- | ------------- | ; | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | ; | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | ; | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | ; | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355440557
https://github.com/google/deepvariant/issues/21#issuecomment-355440557:1411,Modifiability,flexible,flexible,1411,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | ; | ------------ | ----------- | ------------- | ------------- | ------------- | ; | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | ; | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | ; | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | ; | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355440557
https://github.com/google/deepvariant/issues/21#issuecomment-355440557:273,Performance,optimiz,optimization,273,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | ; | ------------ | ----------- | ------------- | ------------- | ------------- | ; | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | ; | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | ; | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | ; | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355440557
https://github.com/google/deepvariant/issues/21#issuecomment-355440557:385,Performance,optimiz,optimizations,385,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | ; | ------------ | ----------- | ------------- | ------------- | ------------- | ; | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | ; | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | ; | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | ; | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355440557
https://github.com/google/deepvariant/issues/21#issuecomment-355440557:595,Performance,perform,performance,595,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | ; | ------------ | ----------- | ------------- | ------------- | ------------- | ; | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | ; | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | ; | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | ; | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355440557
https://github.com/google/deepvariant/issues/21#issuecomment-355440557:647,Performance,optimiz,optimizations,647,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | ; | ------------ | ----------- | ------------- | ------------- | ------------- | ; | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | ; | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | ; | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | ; | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355440557
https://github.com/google/deepvariant/issues/21#issuecomment-355440557:765,Performance,Optimiz,Optimization,765,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | ; | ------------ | ----------- | ------------- | ------------- | ------------- | ; | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | ; | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | ; | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | ; | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355440557
https://github.com/google/deepvariant/issues/21#issuecomment-355441693:380,Deployability,configurat,configuration,380,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355441693
https://github.com/google/deepvariant/issues/21#issuecomment-355441693:535,Deployability,deploy,deployments,535,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355441693
https://github.com/google/deepvariant/issues/21#issuecomment-355441693:380,Modifiability,config,configuration,380,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355441693
https://github.com/google/deepvariant/issues/21#issuecomment-355441693:321,Performance,optimiz,optimized,321,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355441693
https://github.com/google/deepvariant/issues/21#issuecomment-355441693:459,Performance,perform,performance,459,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355441693
https://github.com/google/deepvariant/issues/21#issuecomment-355441693:585,Performance,optimiz,optimized,585,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355441693
https://github.com/google/deepvariant/issues/21#issuecomment-355441693:625,Performance,perform,performance,625,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355441693
https://github.com/google/deepvariant/issues/21#issuecomment-355441693:782,Performance,load,load,782,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355441693
https://github.com/google/deepvariant/issues/21#issuecomment-355441693:801,Performance,perform,performant,801,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355441693
https://github.com/google/deepvariant/issues/21#issuecomment-355441693:1053,Performance,optimiz,optimization,1053,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355441693
https://github.com/google/deepvariant/issues/21#issuecomment-355441693:1160,Performance,perform,performance,1160,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-355441693
https://github.com/google/deepvariant/issues/21#issuecomment-488573130:179,Energy Efficiency,power,power-of-building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-,179,Adding an acknowledgement that this discussion eventually resulted in improvements incorporated to DeepVariant described in the TensorFlow blog (https://medium.com/tensorflow/the-power-of-building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-c8f0acb62344). Mark's proposal for the method to incorporate AVX improvements is a quite accurate description of what was implemented.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-488573130
https://github.com/google/deepvariant/issues/21#issuecomment-488573130:257,Performance,optimiz,optimizations-,257,Adding an acknowledgement that this discussion eventually resulted in improvements incorporated to DeepVariant described in the TensorFlow blog (https://medium.com/tensorflow/the-power-of-building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-c8f0acb62344). Mark's proposal for the method to incorporate AVX improvements is a quite accurate description of what was implemented.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-488573130
https://github.com/google/deepvariant/issues/21#issuecomment-489377484:479,Performance,optimiz,optimizations-on-modern-intel-architecture,479,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl; * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-489377484
https://github.com/google/deepvariant/issues/21#issuecomment-489377484:537,Performance,throughput,throughput,537,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl; * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-489377484
https://github.com/google/deepvariant/issues/21#issuecomment-489377484:624,Performance,optimiz,optimizations,624,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl; * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-489377484
https://github.com/google/deepvariant/issues/21#issuecomment-489377484:683,Performance,optimiz,optimization,683,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl; * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-489377484
https://github.com/google/deepvariant/issues/21#issuecomment-489377484:718,Performance,perform,performs,718,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl; * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-489377484
https://github.com/google/deepvariant/issues/21#issuecomment-489377484:1037,Performance,optimiz,optimizations,1037,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl; * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-489377484
https://github.com/google/deepvariant/issues/21#issuecomment-489377484:1547,Performance,optimiz,optimizations,1547,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl; * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-489377484
https://github.com/google/deepvariant/issues/21#issuecomment-489377484:1083,Security,access,access,1083,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl; * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-489377484
https://github.com/google/deepvariant/issues/21#issuecomment-489377484:317,Testability,test,test,317,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl; * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-489377484
https://github.com/google/deepvariant/issues/21#issuecomment-489377484:892,Usability,guid,guide-instruction-set-specific-dispatching-on-intel-architectures,892,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl; * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/21#issuecomment-489377484
https://github.com/google/deepvariant/issues/22#issuecomment-353700244:82,Availability,error,error,82,"What OS & version are you running? (i.e., what is the output of uname -a ?). This error frequently shows up when using something other than Ubuntu 16. -Thomas C. On Fri, Dec 22, 2017 at 8:50 PM, Zihua Liu <notifications@github.com> wrote:. > When I run make-example.zip, the error shows up... Please fix it for me ;-; > ); >; > ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version; > `GLIBCXX_3.4.21' not found (required by /tmp/Bazel.runfiles_YSzuwd/; > runfiles/protobuf_archive/python/google/protob; > uf/pyext/_message.so); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/22>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AWascfGSdQRRp6h7WGfqGo2Co-d5DjRhks5tDFxxgaJpZM4RLiml>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22#issuecomment-353700244
https://github.com/google/deepvariant/issues/22#issuecomment-353700244:275,Availability,error,error,275,"What OS & version are you running? (i.e., what is the output of uname -a ?). This error frequently shows up when using something other than Ubuntu 16. -Thomas C. On Fri, Dec 22, 2017 at 8:50 PM, Zihua Liu <notifications@github.com> wrote:. > When I run make-example.zip, the error shows up... Please fix it for me ;-; > ); >; > ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version; > `GLIBCXX_3.4.21' not found (required by /tmp/Bazel.runfiles_YSzuwd/; > runfiles/protobuf_archive/python/google/protob; > uf/pyext/_message.so); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/22>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AWascfGSdQRRp6h7WGfqGo2Co-d5DjRhks5tDFxxgaJpZM4RLiml>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22#issuecomment-353700244
https://github.com/google/deepvariant/issues/22#issuecomment-353703390:96,Modifiability,config,configuring,96,"Yeah, that's a Debian instance. You will need an Ubuntu 16 instance. There are instructions for configuring an Ubuntu 16 instance in; https://github.com/google/deepvariant/blob/master/docs/deepvariant-case-study.md. The relevant command is. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \; --boot-disk-device-name ""deepvariant-casestudy"" \; --zone ""us-west1-b"". On Fri, Dec 22, 2017 at 10:01 PM, Zihua Liu <notifications@github.com>; wrote:. > Hi, Thomas; > The output of uname -a is; >; > Linux cs-6000-devshell-vm-d5fc574d-7d85-4a60-b2ee-5248f8d10e41; > 3.16.0-4-amd64 #1 <https://github.com/google/deepvariant/issues/1> SMP; > Debian 3.16.51-2 (2017-12-03) x86_64 GNU/Linux; >; > I run it on Google Cloud Platform; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/22#issuecomment-353703108>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AWascUbKdMeaUed4ob1t06DxAKeuXaIbks5tDGzxgaJpZM4RLiml>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/22#issuecomment-353703390
https://github.com/google/deepvariant/issues/23#issuecomment-353986978:246,Availability,down,down,246,"Hi @yexiao2016z, that's very exciting to hear. The best place to start is https://github.com/google/deepvariant/blob/r0.4/deepvariant/pileup_image.py#L244 which is the python function build_pileup() that we use to create the pileup tensor. Going down the function call stack will show you everything needed to build the tensors. There's a high-performance C++ piece of code that actually constructs the tensor from a pile of reads here as https://github.com/google/deepvariant/blob/r0.4/deepvariant/pileup_image_native.cc that will ultimately be called by pileup_image.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/23#issuecomment-353986978
https://github.com/google/deepvariant/issues/23#issuecomment-353986978:344,Performance,perform,performance,344,"Hi @yexiao2016z, that's very exciting to hear. The best place to start is https://github.com/google/deepvariant/blob/r0.4/deepvariant/pileup_image.py#L244 which is the python function build_pileup() that we use to create the pileup tensor. Going down the function call stack will show you everything needed to build the tensors. There's a high-performance C++ piece of code that actually constructs the tensor from a pile of reads here as https://github.com/google/deepvariant/blob/r0.4/deepvariant/pileup_image_native.cc that will ultimately be called by pileup_image.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/23#issuecomment-353986978
https://github.com/google/deepvariant/issues/24#issuecomment-354129126:59,Availability,error,error,59,Did you build yourself from scratch? We've never seen this error before. Can you confirm that you can run the prebuilt binary on this machine? It's possible that TensorFlow and ABSL have updated their code on github in a way that's breaking our build.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/24#issuecomment-354129126
https://github.com/google/deepvariant/issues/24#issuecomment-354129126:187,Deployability,update,updated,187,Did you build yourself from scratch? We've never seen this error before. Can you confirm that you can run the prebuilt binary on this machine? It's possible that TensorFlow and ABSL have updated their code on github in a way that's breaking our build.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/24#issuecomment-354129126
https://github.com/google/deepvariant/issues/24#issuecomment-355395157:27,Deployability,update,update,27,Closing as there's been no update to this thread in 9 days. Please reopen if necessary.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/24#issuecomment-355395157
https://github.com/google/deepvariant/issues/25#issuecomment-354157667:400,Deployability,install,install,400,"We haven't tried yet to make DeepVariant accessible via pip or in a virtualenv, so unfortunately we don't have any advice for you on this topic. I'm not even sure its possible to get it to work, but if you do decide to work it through please let us know how you do it and we can consider adding official support for working inside of a virtualenv. . Our current recommendations, if you don't want to install DeepVariant dependencies on your machine, is to either (1) run on a cloud instance or (2) run it in a docker container. We build an official docker container for DeepVariant and you can build your own here: https://github.com/google/deepvariant/tree/master/deepvariant/docker. @arostamianfar I don't see any docs on how to access the prebuilt DeepVariant docker image. Could we add a link to it here and add it to the general docs as well?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25#issuecomment-354157667
https://github.com/google/deepvariant/issues/25#issuecomment-354157667:420,Integrability,depend,dependencies,420,"We haven't tried yet to make DeepVariant accessible via pip or in a virtualenv, so unfortunately we don't have any advice for you on this topic. I'm not even sure its possible to get it to work, but if you do decide to work it through please let us know how you do it and we can consider adding official support for working inside of a virtualenv. . Our current recommendations, if you don't want to install DeepVariant dependencies on your machine, is to either (1) run on a cloud instance or (2) run it in a docker container. We build an official docker container for DeepVariant and you can build your own here: https://github.com/google/deepvariant/tree/master/deepvariant/docker. @arostamianfar I don't see any docs on how to access the prebuilt DeepVariant docker image. Could we add a link to it here and add it to the general docs as well?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25#issuecomment-354157667
https://github.com/google/deepvariant/issues/25#issuecomment-354157667:41,Security,access,accessible,41,"We haven't tried yet to make DeepVariant accessible via pip or in a virtualenv, so unfortunately we don't have any advice for you on this topic. I'm not even sure its possible to get it to work, but if you do decide to work it through please let us know how you do it and we can consider adding official support for working inside of a virtualenv. . Our current recommendations, if you don't want to install DeepVariant dependencies on your machine, is to either (1) run on a cloud instance or (2) run it in a docker container. We build an official docker container for DeepVariant and you can build your own here: https://github.com/google/deepvariant/tree/master/deepvariant/docker. @arostamianfar I don't see any docs on how to access the prebuilt DeepVariant docker image. Could we add a link to it here and add it to the general docs as well?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25#issuecomment-354157667
https://github.com/google/deepvariant/issues/25#issuecomment-354157667:731,Security,access,access,731,"We haven't tried yet to make DeepVariant accessible via pip or in a virtualenv, so unfortunately we don't have any advice for you on this topic. I'm not even sure its possible to get it to work, but if you do decide to work it through please let us know how you do it and we can consider adding official support for working inside of a virtualenv. . Our current recommendations, if you don't want to install DeepVariant dependencies on your machine, is to either (1) run on a cloud instance or (2) run it in a docker container. We build an official docker container for DeepVariant and you can build your own here: https://github.com/google/deepvariant/tree/master/deepvariant/docker. @arostamianfar I don't see any docs on how to access the prebuilt DeepVariant docker image. Could we add a link to it here and add it to the general docs as well?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25#issuecomment-354157667
https://github.com/google/deepvariant/issues/25#issuecomment-355050387:32,Deployability,update,update,32,@depristo RE docker docs: I can update the README in https://github.com/google/deepvariant/tree/master/deepvariant/docker to include a link to https://github.com/google/deepvariant/tree/master/docs/deepvariant-docker.md (note that this README is included inside the prebuild image as well). I think that page has all the instructions needed to use our prebuilt image.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/25#issuecomment-355050387
https://github.com/google/deepvariant/issues/27#issuecomment-355032456:11,Availability,error,error,11,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error?. P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-355032456
https://github.com/google/deepvariant/issues/27#issuecomment-355032456:577,Availability,error,error,577,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error?. P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-355032456
https://github.com/google/deepvariant/issues/27#issuecomment-355032456:613,Availability,error,error,613,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error?. P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-355032456
https://github.com/google/deepvariant/issues/27#issuecomment-355032456:688,Availability,error,errors,688,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error?. P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-355032456
https://github.com/google/deepvariant/issues/27#issuecomment-355032456:740,Availability,error,error,740,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error?. P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-355032456
https://github.com/google/deepvariant/issues/27#issuecomment-355032456:324,Deployability,pipeline,pipeline,324,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error?. P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-355032456
https://github.com/google/deepvariant/issues/27#issuecomment-355032456:746,Integrability,message,message,746,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error?. P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-355032456
https://github.com/google/deepvariant/issues/27#issuecomment-355032456:63,Testability,test,test-shan,63,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error?. P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-355032456
https://github.com/google/deepvariant/issues/27#issuecomment-355032456:708,Testability,log,logs,708,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error?. P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-355032456
https://github.com/google/deepvariant/issues/27#issuecomment-355036534:285,Energy Efficiency,reduce,reduce,285,"yeap, it's caused by empty shards. I was able to reproduce this by using 64 shards with the quickstart test data. @depristo should I file a separate issue for this as it's not really a docker issue?. @chenshan03: thanks for the report. As a workaround until this bug is fixed, you may reduce the number of shards to avoid having empty ones.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-355036534
https://github.com/google/deepvariant/issues/27#issuecomment-355036534:316,Safety,avoid,avoid,316,"yeap, it's caused by empty shards. I was able to reproduce this by using 64 shards with the quickstart test data. @depristo should I file a separate issue for this as it's not really a docker issue?. @chenshan03: thanks for the report. As a workaround until this bug is fixed, you may reduce the number of shards to avoid having empty ones.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-355036534
https://github.com/google/deepvariant/issues/27#issuecomment-355036534:103,Testability,test,test,103,"yeap, it's caused by empty shards. I was able to reproduce this by using 64 shards with the quickstart test data. @depristo should I file a separate issue for this as it's not really a docker issue?. @chenshan03: thanks for the report. As a workaround until this bug is fixed, you may reduce the number of shards to avoid having empty ones.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-355036534
https://github.com/google/deepvariant/issues/27#issuecomment-355805026:441,Availability,error,error,441,"Hi Mark and Asha,; here's what I believe the current status is:; (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug.; (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:; The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-355805026
https://github.com/google/deepvariant/issues/27#issuecomment-355805026:490,Availability,error,error,490,"Hi Mark and Asha,; here's what I believe the current status is:; (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug.; (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:; The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-355805026
https://github.com/google/deepvariant/issues/27#issuecomment-355805026:853,Availability,error,error,853,"Hi Mark and Asha,; here's what I believe the current status is:; (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug.; (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:; The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-355805026
https://github.com/google/deepvariant/issues/27#issuecomment-355805026:447,Integrability,message,message,447,"Hi Mark and Asha,; here's what I believe the current status is:; (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug.; (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:; The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-355805026
https://github.com/google/deepvariant/issues/27#issuecomment-355805026:496,Integrability,message,message,496,"Hi Mark and Asha,; here's what I believe the current status is:; (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug.; (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:; The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-355805026
https://github.com/google/deepvariant/issues/27#issuecomment-355805026:560,Testability,test,test-shan,560,"Hi Mark and Asha,; here's what I believe the current status is:; (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug.; (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:; The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-355805026
https://github.com/google/deepvariant/issues/27#issuecomment-355996061:350,Deployability,configurat,configuration,350,"yes, I think this is a real bug that still exists.; Due to the distributed nature of the cloud process, some machines may get shards that are all empty. Also, we actually only supply one of the shards to each process, so (1) doesn't really apply (there is no 'next shard').; You can reproduce this by adding ""--shards 64"" to the quickstart test data configuration in https://cloud.google.com/genomics/deepvariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-355996061
https://github.com/google/deepvariant/issues/27#issuecomment-355996061:350,Modifiability,config,configuration,350,"yes, I think this is a real bug that still exists.; Due to the distributed nature of the cloud process, some machines may get shards that are all empty. Also, we actually only supply one of the shards to each process, so (1) doesn't really apply (there is no 'next shard').; You can reproduce this by adding ""--shards 64"" to the quickstart test data configuration in https://cloud.google.com/genomics/deepvariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-355996061
https://github.com/google/deepvariant/issues/27#issuecomment-355996061:340,Testability,test,test,340,"yes, I think this is a real bug that still exists.; Due to the distributed nature of the cloud process, some machines may get shards that are all empty. Also, we actually only supply one of the shards to each process, so (1) doesn't really apply (there is no 'next shard').; You can reproduce this by adding ""--shards 64"" to the quickstart test data configuration in https://cloud.google.com/genomics/deepvariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-355996061
https://github.com/google/deepvariant/issues/27#issuecomment-364553446:45,Deployability,release,release,45,This has been fixed by the DeepVariant 0.5.1 release that just came out a few minutes ago. Thank you for raising attention to this issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-364553446
https://github.com/google/deepvariant/issues/27#issuecomment-364627307:43,Deployability,release,release,43,"Hi Cory (@cmclean),. Thank you for the new release, but if we look at the new timings with the `0.5.1` release, they seem to have gotten longer than with the previous version:. [Commit v0.5.1](https://github.com/google/deepvariant/commit/38c17ed0f8b8b03da8daf68d24226c4c1957e116). #### Timings: Whole Genome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![whole-genome-case-study-timing](https://user-images.githubusercontent.com/6555937/36058742-9f1916de-0df6-11e8-9431-a19a677612ef.png). #### Timings: Exome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![exome-case-study-timings](https://user-images.githubusercontent.com/6555937/36058748-bff057a0-0df6-11e8-9941-dc89084572b8.png). What is the cause of the additional delay in version `0.5.1` as compared to the previous one?. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-364627307
https://github.com/google/deepvariant/issues/27#issuecomment-364627307:103,Deployability,release,release,103,"Hi Cory (@cmclean),. Thank you for the new release, but if we look at the new timings with the `0.5.1` release, they seem to have gotten longer than with the previous version:. [Commit v0.5.1](https://github.com/google/deepvariant/commit/38c17ed0f8b8b03da8daf68d24226c4c1957e116). #### Timings: Whole Genome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![whole-genome-case-study-timing](https://user-images.githubusercontent.com/6555937/36058742-9f1916de-0df6-11e8-9431-a19a677612ef.png). #### Timings: Exome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![exome-case-study-timings](https://user-images.githubusercontent.com/6555937/36058748-bff057a0-0df6-11e8-9941-dc89084572b8.png). What is the cause of the additional delay in version `0.5.1` as compared to the previous one?. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-364627307
https://github.com/google/deepvariant/issues/27#issuecomment-364627307:345,Energy Efficiency,green,green,345,"Hi Cory (@cmclean),. Thank you for the new release, but if we look at the new timings with the `0.5.1` release, they seem to have gotten longer than with the previous version:. [Commit v0.5.1](https://github.com/google/deepvariant/commit/38c17ed0f8b8b03da8daf68d24226c4c1957e116). #### Timings: Whole Genome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![whole-genome-case-study-timing](https://user-images.githubusercontent.com/6555937/36058742-9f1916de-0df6-11e8-9431-a19a677612ef.png). #### Timings: Exome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![exome-case-study-timings](https://user-images.githubusercontent.com/6555937/36058748-bff057a0-0df6-11e8-9941-dc89084572b8.png). What is the cause of the additional delay in version `0.5.1` as compared to the previous one?. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-364627307
https://github.com/google/deepvariant/issues/27#issuecomment-364627307:548,Energy Efficiency,green,green,548,"Hi Cory (@cmclean),. Thank you for the new release, but if we look at the new timings with the `0.5.1` release, they seem to have gotten longer than with the previous version:. [Commit v0.5.1](https://github.com/google/deepvariant/commit/38c17ed0f8b8b03da8daf68d24226c4c1957e116). #### Timings: Whole Genome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![whole-genome-case-study-timing](https://user-images.githubusercontent.com/6555937/36058742-9f1916de-0df6-11e8-9431-a19a677612ef.png). #### Timings: Exome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![exome-case-study-timings](https://user-images.githubusercontent.com/6555937/36058748-bff057a0-0df6-11e8-9941-dc89084572b8.png). What is the cause of the additional delay in version `0.5.1` as compared to the previous one?. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-364627307
https://github.com/google/deepvariant/issues/27#issuecomment-385593624:142,Deployability,release,released,142,"Hi all,; it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:; https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:; https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9; (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-385593624
https://github.com/google/deepvariant/issues/27#issuecomment-385593624:240,Deployability,release,releases,240,"Hi all,; it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:; https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:; https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9; (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-385593624
https://github.com/google/deepvariant/issues/27#issuecomment-385593624:524,Deployability,release,release,524,"Hi all,; it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:; https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:; https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9; (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-385593624
https://github.com/google/deepvariant/issues/27#issuecomment-385593624:347,Safety,detect,detected,347,"Hi all,; it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:; https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:; https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9; (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-385593624
https://github.com/google/deepvariant/issues/27#issuecomment-385593624:459,Testability,test,test,459,"Hi all,; it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:; https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:; https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9; (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-385593624
https://github.com/google/deepvariant/issues/27#issuecomment-385593624:508,Testability,test,test,508,"Hi all,; it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:; https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:; https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9; (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-385593624
https://github.com/google/deepvariant/issues/27#issuecomment-385593624:550,Testability,test,tested,550,"Hi all,; it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:; https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:; https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9; (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/27#issuecomment-385593624
https://github.com/google/deepvariant/issues/28#issuecomment-354390647:40,Availability,error,error,40,A final update. We've just improved the error messages for these cases in the internal version. The next release of DeepVariant should have this fix.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/28#issuecomment-354390647
https://github.com/google/deepvariant/issues/28#issuecomment-354390647:8,Deployability,update,update,8,A final update. We've just improved the error messages for these cases in the internal version. The next release of DeepVariant should have this fix.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/28#issuecomment-354390647
https://github.com/google/deepvariant/issues/28#issuecomment-354390647:105,Deployability,release,release,105,A final update. We've just improved the error messages for these cases in the internal version. The next release of DeepVariant should have this fix.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/28#issuecomment-354390647
https://github.com/google/deepvariant/issues/28#issuecomment-354390647:46,Integrability,message,messages,46,A final update. We've just improved the error messages for these cases in the internal version. The next release of DeepVariant should have this fix.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/28#issuecomment-354390647
https://github.com/google/deepvariant/issues/29#issuecomment-355455875:408,Availability,avail,available,408,Björn -- I'm agreed. I tried to look into building clif but it was too intense (https://github.com/google/clif#building) and had to give up. Right now the pre-built version assumes unpacking into `/usr` so is also not an option for a conda package. If you have time to investigate and think you can tackle that would be great. I've already gotten bazel up to date so should be able to try building with clif available.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-355455875
https://github.com/google/deepvariant/issues/29#issuecomment-355459571:5,Availability,ping,pinged,5,I've pinged the @mrovner and @gpshead about CLIF. I'm hopeful they'll chime in here.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-355459571
https://github.com/google/deepvariant/issues/29#issuecomment-385118050:0,Deployability,Update,Update,0,"Update (1) on numpy version:. Turns out getting back to numpy 1.12 is harder than I thought, because TensorFlow 1.4 requires numpy 1.14. When I try to revert the prereq script to numpy 1.12, I keep getting this message:; ""tensorflow 1.4.1 has requirement numpy>=1.12.1, but you'll have numpy 1.12.0 which is incompatible."". which makes me uncomfortable to pin numpy back at 1.12. I did try changing the numpy version to 1.12, and build with build_release_binaries.sh. It seems to build, and call_variants step (which is the main step that uses Tensorflow) seems to run with similar speed. The hap.py numbers are exactly the same.; However, it seems like bad practice to ignore that warning message that shows up in red.; Thoughts?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385118050
https://github.com/google/deepvariant/issues/29#issuecomment-385118050:211,Integrability,message,message,211,"Update (1) on numpy version:. Turns out getting back to numpy 1.12 is harder than I thought, because TensorFlow 1.4 requires numpy 1.14. When I try to revert the prereq script to numpy 1.12, I keep getting this message:; ""tensorflow 1.4.1 has requirement numpy>=1.12.1, but you'll have numpy 1.12.0 which is incompatible."". which makes me uncomfortable to pin numpy back at 1.12. I did try changing the numpy version to 1.12, and build with build_release_binaries.sh. It seems to build, and call_variants step (which is the main step that uses Tensorflow) seems to run with similar speed. The hap.py numbers are exactly the same.; However, it seems like bad practice to ignore that warning message that shows up in red.; Thoughts?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385118050
https://github.com/google/deepvariant/issues/29#issuecomment-385118050:690,Integrability,message,message,690,"Update (1) on numpy version:. Turns out getting back to numpy 1.12 is harder than I thought, because TensorFlow 1.4 requires numpy 1.14. When I try to revert the prereq script to numpy 1.12, I keep getting this message:; ""tensorflow 1.4.1 has requirement numpy>=1.12.1, but you'll have numpy 1.12.0 which is incompatible."". which makes me uncomfortable to pin numpy back at 1.12. I did try changing the numpy version to 1.12, and build with build_release_binaries.sh. It seems to build, and call_variants step (which is the main step that uses Tensorflow) seems to run with similar speed. The hap.py numbers are exactly the same.; However, it seems like bad practice to ignore that warning message that shows up in red.; Thoughts?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385118050
https://github.com/google/deepvariant/issues/29#issuecomment-385125512:142,Deployability,update,update,142,"Pi-Chuan -- thanks so much for looking at this.; Paul -- thank you for digging into the bioconda history on the numpy pin. I suspect we could update numpy in bioconda but no one has yet taken that on. Given that and the issues with tensorflow pinning, I'd advise just sticking with 1.14 and we can work on the dependency issues in bioconda. Apologies, I hadn't meant to cause a lot of work and didn't realize about the pinning preventing this. If we can get older glibc compatible binaries that'll cover most of the issues and we can work around the numpy problems by installing in an isolated conda environment for now. Thanks again for looking into the numpy and glibc work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385125512
https://github.com/google/deepvariant/issues/29#issuecomment-385125512:568,Deployability,install,installing,568,"Pi-Chuan -- thanks so much for looking at this.; Paul -- thank you for digging into the bioconda history on the numpy pin. I suspect we could update numpy in bioconda but no one has yet taken that on. Given that and the issues with tensorflow pinning, I'd advise just sticking with 1.14 and we can work on the dependency issues in bioconda. Apologies, I hadn't meant to cause a lot of work and didn't realize about the pinning preventing this. If we can get older glibc compatible binaries that'll cover most of the issues and we can work around the numpy problems by installing in an isolated conda environment for now. Thanks again for looking into the numpy and glibc work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385125512
https://github.com/google/deepvariant/issues/29#issuecomment-385125512:310,Integrability,depend,dependency,310,"Pi-Chuan -- thanks so much for looking at this.; Paul -- thank you for digging into the bioconda history on the numpy pin. I suspect we could update numpy in bioconda but no one has yet taken that on. Given that and the issues with tensorflow pinning, I'd advise just sticking with 1.14 and we can work on the dependency issues in bioconda. Apologies, I hadn't meant to cause a lot of work and didn't realize about the pinning preventing this. If we can get older glibc compatible binaries that'll cover most of the issues and we can work around the numpy problems by installing in an isolated conda environment for now. Thanks again for looking into the numpy and glibc work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385125512
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:2899,Availability,echo,echo,2899,"e when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:3051,Availability,down,download,3051,"e when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:3266,Availability,echo,echo,3266,"e when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:3459,Availability,echo,echo,3459,"e when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:3663,Availability,echo,echo,3663,"e when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:8,Deployability,update,update,8,"Another update on CLIF dependency:; @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```; # Builds OSS CLIF binary for DeepVariant.; #; # This script should be run on a cloud VM. Known to work on some versions of; # Linux OS.; #; # OSS CLIF takes a very long time to build (10+ minutes) since it needs to; # compile parts of clang and LLVM. To save this build time, we use this script; # to build CLIF, install it in /usr/local/clif, and then packages up; # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz; # called oss_clif.latest.tgz.; #; # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:196,Deployability,release,release,196,"Another update on CLIF dependency:; @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```; # Builds OSS CLIF binary for DeepVariant.; #; # This script should be run on a cloud VM. Known to work on some versions of; # Linux OS.; #; # OSS CLIF takes a very long time to build (10+ minutes) since it needs to; # compile parts of clang and LLVM. To save this build time, we use this script; # to build CLIF, install it in /usr/local/clif, and then packages up; # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz; # called oss_clif.latest.tgz.; #; # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:267,Deployability,release,released,267,"Another update on CLIF dependency:; @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```; # Builds OSS CLIF binary for DeepVariant.; #; # This script should be run on a cloud VM. Known to work on some versions of; # Linux OS.; #; # OSS CLIF takes a very long time to build (10+ minutes) since it needs to; # compile parts of clang and LLVM. To save this build time, we use this script; # to build CLIF, install it in /usr/local/clif, and then packages up; # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz; # called oss_clif.latest.tgz.; #; # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:1257,Deployability,install,install,1257,"ut a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```; # Builds OSS CLIF binary for DeepVariant.; #; # This script should be run on a cloud VM. Known to work on some versions of; # Linux OS.; #; # OSS CLIF takes a very long time to build (10+ minutes) since it needs to; # compile parts of clang and LLVM. To save this build time, we use this script; # to build CLIF, install it in /usr/local/clif, and then packages up; # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz; # called oss_clif.latest.tgz.; #; # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # Fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:1538,Deployability,release,released,1538,"id start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```; # Builds OSS CLIF binary for DeepVariant.; #; # This script should be run on a cloud VM. Known to work on some versions of; # Linux OS.; #; # OSS CLIF takes a very long time to build (10+ minutes) since it needs to; # compile parts of clang and LLVM. To save this build time, we use this script; # to build CLIF, install it in /usr/local/clif, and then packages up; # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz; # called oss_clif.latest.tgz.; #; # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:1711,Deployability,update,update,1711,"e our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```; # Builds OSS CLIF binary for DeepVariant.; #; # This script should be run on a cloud VM. Known to work on some versions of; # Linux OS.; #; # OSS CLIF takes a very long time to build (10+ minutes) since it needs to; # compile parts of clang and LLVM. To save this build time, we use this script; # to build CLIF, install it in /usr/local/clif, and then packages up; # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz; # called oss_clif.latest.tgz.; #; # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:1918,Deployability,install,installation,1918,"`; # Builds OSS CLIF binary for DeepVariant.; #; # This script should be run on a cloud VM. Known to work on some versions of; # Linux OS.; #; # OSS CLIF takes a very long time to build (10+ minutes) since it needs to; # compile parts of clang and LLVM. To save this build time, we use this script; # to build CLIF, install it in /usr/local/clif, and then packages up; # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz; # called oss_clif.latest.tgz.; #; # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-ge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:2160,Deployability,install,install,2160," # to build CLIF, install it in /usr/local/clif, and then packages up; # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz; # called oss_clif.latest.tgz.; #; # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:2194,Deployability,install,install,2194," # to build CLIF, install it in /usr/local/clif, and then packages up; # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz; # called oss_clif.latest.tgz.; #; # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:2316,Deployability,install,install,2316," # to build CLIF, install it in /usr/local/clif, and then packages up; # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz; # called oss_clif.latest.tgz.; #; # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:2351,Deployability,install,install,2351," # to build CLIF, install it in /usr/local/clif, and then packages up; # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz; # called oss_clif.latest.tgz.; #; # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:2494,Deployability,install,install,2494,"d-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo ===",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:2529,Deployability,install,install,2529,"under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:2692,Deployability,Install,Install,2692,"ry, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:2729,Deployability,install,install,2729,"e when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:2780,Deployability,install,install,2780,"e when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:2834,Deployability,install,install,2834,"e when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:2874,Deployability,install,install,2874,"e when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:2944,Deployability,install,install,2944,"e when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:3042,Deployability,release,releases,3042,"e when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:3238,Deployability,install,install,3238,"e when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:3379,Deployability,INSTALL,INSTALL,3379,"e when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:3415,Deployability,INSTALL,INSTALL,3415,"e when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:3446,Deployability,INSTALL,INSTALL,3446,"e when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:23,Integrability,depend,dependency,23,"Another update on CLIF dependency:; @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```; # Builds OSS CLIF binary for DeepVariant.; #; # This script should be run on a cloud VM. Known to work on some versions of; # Linux OS.; #; # OSS CLIF takes a very long time to build (10+ minutes) since it needs to; # compile parts of clang and LLVM. To save this build time, we use this script; # to build CLIF, install it in /usr/local/clif, and then packages up; # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz; # called oss_clif.latest.tgz.; #; # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:2814,Modifiability,config,config,2814,"e when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:3172,Modifiability,config,configure,3172,"e when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385130636:647,Usability,resume,resumed,647,"Another update on CLIF dependency:; @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```; # Builds OSS CLIF binary for DeepVariant.; #; # This script should be run on a cloud VM. Known to work on some versions of; # Linux OS.; #; # OSS CLIF takes a very long time to build (10+ minutes) since it needs to; # compile parts of clang and LLVM. To save this build time, we use this script; # to build CLIF, install it in /usr/local/clif, and then packages up; # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz; # called oss_clif.latest.tgz.; #; # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636
https://github.com/google/deepvariant/issues/29#issuecomment-385485505:349,Availability,down,downside,349,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385485505
https://github.com/google/deepvariant/issues/29#issuecomment-385485505:189,Deployability,install,install,189,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385485505
https://github.com/google/deepvariant/issues/29#issuecomment-385485505:249,Energy Efficiency,reduce,reduced,249,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385485505
https://github.com/google/deepvariant/issues/29#issuecomment-385485505:202,Integrability,depend,dependency,202,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385485505
https://github.com/google/deepvariant/issues/29#issuecomment-385485505:561,Integrability,synchroniz,synchronize,561,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385485505
https://github.com/google/deepvariant/issues/29#issuecomment-385485505:82,Safety,avoid,avoid,82,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385485505
https://github.com/google/deepvariant/issues/29#issuecomment-385485505:223,Testability,test,test,223,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385485505
https://github.com/google/deepvariant/issues/29#issuecomment-385500259:198,Deployability,install,installable,198,"Pi-Chuan -- sorry, that's right, we would want to build on CentOS6 to be compatible with bioconda. They have a restricted build environment for portability so we'd need to have all the dependencies installable by bioconda (rather than system packages). I had looked at this earlier and realized all the pre-requisites so got afraid of tackling it. It's definitely a help to have that information but I think would still take a bit of work to port over.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385500259
https://github.com/google/deepvariant/issues/29#issuecomment-385500259:185,Integrability,depend,dependencies,185,"Pi-Chuan -- sorry, that's right, we would want to build on CentOS6 to be compatible with bioconda. They have a restricted build environment for portability so we'd need to have all the dependencies installable by bioconda (rather than system packages). I had looked at this earlier and realized all the pre-requisites so got afraid of tackling it. It's definitely a help to have that information but I think would still take a bit of work to port over.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385500259
https://github.com/google/deepvariant/issues/29#issuecomment-385500259:144,Modifiability,portab,portability,144,"Pi-Chuan -- sorry, that's right, we would want to build on CentOS6 to be compatible with bioconda. They have a restricted build environment for portability so we'd need to have all the dependencies installable by bioconda (rather than system packages). I had looked at this earlier and realized all the pre-requisites so got afraid of tackling it. It's definitely a help to have that information but I think would still take a bit of work to port over.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385500259
https://github.com/google/deepvariant/issues/29#issuecomment-385593875:0,Deployability,Update,Update,0,Update:; the build_clif_package.sh (the same in https://github.com/google/deepvariant/issues/29#issuecomment-385130636) is now in included in v0.6.1:; https://github.com/google/deepvariant/blob/r0.6/tools/build_clif_package.sh; https://github.com/google/deepvariant/releases/tag/v0.6.1. I haven't looked more into the CentOS6 build. I'll send another update when I make progress on that.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385593875
https://github.com/google/deepvariant/issues/29#issuecomment-385593875:266,Deployability,release,releases,266,Update:; the build_clif_package.sh (the same in https://github.com/google/deepvariant/issues/29#issuecomment-385130636) is now in included in v0.6.1:; https://github.com/google/deepvariant/blob/r0.6/tools/build_clif_package.sh; https://github.com/google/deepvariant/releases/tag/v0.6.1. I haven't looked more into the CentOS6 build. I'll send another update when I make progress on that.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385593875
https://github.com/google/deepvariant/issues/29#issuecomment-385593875:351,Deployability,update,update,351,Update:; the build_clif_package.sh (the same in https://github.com/google/deepvariant/issues/29#issuecomment-385130636) is now in included in v0.6.1:; https://github.com/google/deepvariant/blob/r0.6/tools/build_clif_package.sh; https://github.com/google/deepvariant/releases/tag/v0.6.1. I haven't looked more into the CentOS6 build. I'll send another update when I make progress on that.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385593875
https://github.com/google/deepvariant/issues/29#issuecomment-385864674:1601,Availability,error,error,1601,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:; ```; # Get a machine; gcloud beta compute instances create ""${USER}-centos6"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-6"" --image-project ""centos-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \; --zone ""us-west1-b"". # ssh into it; gcloud compute ssh ${USER}-centos6 --zone us-west1-b; ```. ```; ##### On the GCE instance #####; # Install Python 2.7; sudo yum install -y centos-release-SCL; sudo yum install -y python27; source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/; (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""); sudo ldconfig # Reload shared libraries.; ```; (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:; ```; $ /usr/local/clif/bin/pyclif; usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]; [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]; [--prepend PREPEND] [--include_paths INCLUDE_PATHS]; [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]; [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]; input_filename; pyclif: error: too few arguments; ```. Please let me know once you have a chance to try it.; CentOS 6 is tricky. It feels like everything is old :(; Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385864674
https://github.com/google/deepvariant/issues/29#issuecomment-385864674:23,Deployability,update,update,23,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:; ```; # Get a machine; gcloud beta compute instances create ""${USER}-centos6"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-6"" --image-project ""centos-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \; --zone ""us-west1-b"". # ssh into it; gcloud compute ssh ${USER}-centos6 --zone us-west1-b; ```. ```; ##### On the GCE instance #####; # Install Python 2.7; sudo yum install -y centos-release-SCL; sudo yum install -y python27; source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/; (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""); sudo ldconfig # Reload shared libraries.; ```; (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:; ```; $ /usr/local/clif/bin/pyclif; usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]; [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]; [--prepend PREPEND] [--include_paths INCLUDE_PATHS]; [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]; [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]; input_filename; pyclif: error: too few arguments; ```. Please let me know once you have a chance to try it.; CentOS 6 is tricky. It feels like everything is old :(; Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385864674
https://github.com/google/deepvariant/issues/29#issuecomment-385864674:708,Deployability,Install,Install,708,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:; ```; # Get a machine; gcloud beta compute instances create ""${USER}-centos6"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-6"" --image-project ""centos-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \; --zone ""us-west1-b"". # ssh into it; gcloud compute ssh ${USER}-centos6 --zone us-west1-b; ```. ```; ##### On the GCE instance #####; # Install Python 2.7; sudo yum install -y centos-release-SCL; sudo yum install -y python27; source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/; (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""); sudo ldconfig # Reload shared libraries.; ```; (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:; ```; $ /usr/local/clif/bin/pyclif; usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]; [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]; [--prepend PREPEND] [--include_paths INCLUDE_PATHS]; [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]; [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]; input_filename; pyclif: error: too few arguments; ```. Please let me know once you have a chance to try it.; CentOS 6 is tricky. It feels like everything is old :(; Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385864674
https://github.com/google/deepvariant/issues/29#issuecomment-385864674:737,Deployability,install,install,737,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:; ```; # Get a machine; gcloud beta compute instances create ""${USER}-centos6"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-6"" --image-project ""centos-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \; --zone ""us-west1-b"". # ssh into it; gcloud compute ssh ${USER}-centos6 --zone us-west1-b; ```. ```; ##### On the GCE instance #####; # Install Python 2.7; sudo yum install -y centos-release-SCL; sudo yum install -y python27; source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/; (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""); sudo ldconfig # Reload shared libraries.; ```; (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:; ```; $ /usr/local/clif/bin/pyclif; usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]; [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]; [--prepend PREPEND] [--include_paths INCLUDE_PATHS]; [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]; [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]; input_filename; pyclif: error: too few arguments; ```. Please let me know once you have a chance to try it.; CentOS 6 is tricky. It feels like everything is old :(; Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385864674
https://github.com/google/deepvariant/issues/29#issuecomment-385864674:755,Deployability,release,release-SCL,755,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:; ```; # Get a machine; gcloud beta compute instances create ""${USER}-centos6"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-6"" --image-project ""centos-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \; --zone ""us-west1-b"". # ssh into it; gcloud compute ssh ${USER}-centos6 --zone us-west1-b; ```. ```; ##### On the GCE instance #####; # Install Python 2.7; sudo yum install -y centos-release-SCL; sudo yum install -y python27; source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/; (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""); sudo ldconfig # Reload shared libraries.; ```; (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:; ```; $ /usr/local/clif/bin/pyclif; usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]; [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]; [--prepend PREPEND] [--include_paths INCLUDE_PATHS]; [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]; [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]; input_filename; pyclif: error: too few arguments; ```. Please let me know once you have a chance to try it.; CentOS 6 is tricky. It feels like everything is old :(; Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385864674
https://github.com/google/deepvariant/issues/29#issuecomment-385864674:777,Deployability,install,install,777,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:; ```; # Get a machine; gcloud beta compute instances create ""${USER}-centos6"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-6"" --image-project ""centos-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \; --zone ""us-west1-b"". # ssh into it; gcloud compute ssh ${USER}-centos6 --zone us-west1-b; ```. ```; ##### On the GCE instance #####; # Install Python 2.7; sudo yum install -y centos-release-SCL; sudo yum install -y python27; source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/; (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""); sudo ldconfig # Reload shared libraries.; ```; (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:; ```; $ /usr/local/clif/bin/pyclif; usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]; [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]; [--prepend PREPEND] [--include_paths INCLUDE_PATHS]; [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]; [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]; input_filename; pyclif: error: too few arguments; ```. Please let me know once you have a chance to try it.; CentOS 6 is tricky. It feels like everything is old :(; Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385864674
https://github.com/google/deepvariant/issues/29#issuecomment-385864674:128,Usability,usab,usable,128,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:; ```; # Get a machine; gcloud beta compute instances create ""${USER}-centos6"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-6"" --image-project ""centos-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \; --zone ""us-west1-b"". # ssh into it; gcloud compute ssh ${USER}-centos6 --zone us-west1-b; ```. ```; ##### On the GCE instance #####; # Install Python 2.7; sudo yum install -y centos-release-SCL; sudo yum install -y python27; source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/; (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""); sudo ldconfig # Reload shared libraries.; ```; (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:; ```; $ /usr/local/clif/bin/pyclif; usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]; [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]; [--prepend PREPEND] [--include_paths INCLUDE_PATHS]; [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]; [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]; input_filename; pyclif: error: too few arguments; ```. Please let me know once you have a chance to try it.; CentOS 6 is tricky. It feels like everything is old :(; Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385864674
https://github.com/google/deepvariant/issues/29#issuecomment-385868054:546,Availability,echo,echo,546,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:; ```; $ ldd --version; ldd (GNU libc) 2.12; Copyright (C) 2010 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; Written by Roland McGrath and Ulrich Drepper.; ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead.; And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:; ```; $ /usr/local/bin/bazel version; Extracting Bazel installation...; Build label: 0.11.0- (@non-git); Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar; Build time: Wed Nov 5 12:47:48 +50302 (1525237217268); Build timestamp: 1525237217268; Build timestamp as int: 1525237217268; ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385868054
https://github.com/google/deepvariant/issues/29#issuecomment-385868054:802,Deployability,install,installation,802,"And, another thing I did is build bazel 0.11.0 with the older GLIBC. On my CentOS 6 GCE instance:; ```; $ ldd --version; ldd (GNU libc) 2.12; Copyright (C) 2010 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; Written by Roland McGrath and Ulrich Drepper.; ```. I basically followed https://gist.github.com/truatpasteurdotfr/d541cd279b9f7bf38ce967aa3743dfcb , but use bazel version 0.11.0 instead.; And in the `echo 'cd /tmp/bazel-0.4.5-dist && bash ./compile.sh && cp output/bazel /usr/local/bin' | scl enable devtoolset-3 bash` command I had to add `sudo` to the cp command. After this, I have a bazel 0.11.0:; ```; $ /usr/local/bin/bazel version; Extracting Bazel installation...; Build label: 0.11.0- (@non-git); Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar; Build time: Wed Nov 5 12:47:48 +50302 (1525237217268); Build timestamp: 1525237217268; Build timestamp as int: 1525237217268; ```. I haven't tried building with it, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385868054
https://github.com/google/deepvariant/issues/29#issuecomment-385874525:1042,Availability,error,error,1042,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385874525
https://github.com/google/deepvariant/issues/29#issuecomment-385874525:217,Deployability,configurat,configurations,217,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385874525
https://github.com/google/deepvariant/issues/29#issuecomment-385874525:995,Deployability,configurat,configurations,995,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385874525
https://github.com/google/deepvariant/issues/29#issuecomment-385874525:1048,Integrability,message,messages,1048,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385874525
https://github.com/google/deepvariant/issues/29#issuecomment-385874525:217,Modifiability,config,configurations,217,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385874525
https://github.com/google/deepvariant/issues/29#issuecomment-385874525:995,Modifiability,config,configurations,995,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385874525
https://github.com/google/deepvariant/issues/29#issuecomment-385874525:153,Testability,test,test,153,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385874525
https://github.com/google/deepvariant/issues/29#issuecomment-385874525:863,Testability,test,tested,863,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-385874525
https://github.com/google/deepvariant/issues/29#issuecomment-386074270:793,Availability,error,error,793,"Pi-Chuan;; Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:; ```; $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python; /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory; ```; and the python libraries included symlink to the system wide ones you built against:; ```; lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py; ```; I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386074270
https://github.com/google/deepvariant/issues/29#issuecomment-386074270:27,Deployability,update,update,27,"Pi-Chuan;; Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:; ```; $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python; /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory; ```; and the python libraries included symlink to the system wide ones you built against:; ```; lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py; ```; I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386074270
https://github.com/google/deepvariant/issues/29#issuecomment-386074270:72,Deployability,update,updated,72,"Pi-Chuan;; Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:; ```; $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python; /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory; ```; and the python libraries included symlink to the system wide ones you built against:; ```; lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py; ```; I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386074270
https://github.com/google/deepvariant/issues/29#issuecomment-386074270:424,Deployability,install,install,424,"Pi-Chuan;; Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:; ```; $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python; /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory; ```; and the python libraries included symlink to the system wide ones you built against:; ```; lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py; ```; I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386074270
https://github.com/google/deepvariant/issues/29#issuecomment-386074270:1317,Deployability,install,install,1317,"Pi-Chuan;; Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:; ```; $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python; /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory; ```; and the python libraries included symlink to the system wide ones you built against:; ```; lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py; ```; I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386074270
https://github.com/google/deepvariant/issues/29#issuecomment-386074270:144,Integrability,depend,dependency,144,"Pi-Chuan;; Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:; ```; $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python; /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory; ```; and the python libraries included symlink to the system wide ones you built against:; ```; lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py; ```; I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386074270
https://github.com/google/deepvariant/issues/29#issuecomment-386074270:805,Performance,load,loading,805,"Pi-Chuan;; Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:; ```; $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python; /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory; ```; and the python libraries included symlink to the system wide ones you built against:; ```; lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py; ```; I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386074270
https://github.com/google/deepvariant/issues/29#issuecomment-386075552:85,Deployability,Install,Install,85,@chapmanb Thanks for giving it a try. ; Before I built I did something like:; ```; # Install Python 2.7; sudo yum install -y centos-release-SCL; sudo yum install -y python27; source /opt/rh/python27/enable; ```. I think starting from there it just assumes python is in /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can make it recognize python at any path. ; Is there a convention that people use to build something so that they can point to other Python locations?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386075552
https://github.com/google/deepvariant/issues/29#issuecomment-386075552:114,Deployability,install,install,114,@chapmanb Thanks for giving it a try. ; Before I built I did something like:; ```; # Install Python 2.7; sudo yum install -y centos-release-SCL; sudo yum install -y python27; source /opt/rh/python27/enable; ```. I think starting from there it just assumes python is in /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can make it recognize python at any path. ; Is there a convention that people use to build something so that they can point to other Python locations?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386075552
https://github.com/google/deepvariant/issues/29#issuecomment-386075552:132,Deployability,release,release-SCL,132,@chapmanb Thanks for giving it a try. ; Before I built I did something like:; ```; # Install Python 2.7; sudo yum install -y centos-release-SCL; sudo yum install -y python27; source /opt/rh/python27/enable; ```. I think starting from there it just assumes python is in /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can make it recognize python at any path. ; Is there a convention that people use to build something so that they can point to other Python locations?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386075552
https://github.com/google/deepvariant/issues/29#issuecomment-386075552:154,Deployability,install,install,154,@chapmanb Thanks for giving it a try. ; Before I built I did something like:; ```; # Install Python 2.7; sudo yum install -y centos-release-SCL; sudo yum install -y python27; source /opt/rh/python27/enable; ```. I think starting from there it just assumes python is in /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can make it recognize python at any path. ; Is there a convention that people use to build something so that they can point to other Python locations?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386075552
https://github.com/google/deepvariant/issues/29#issuecomment-386079018:44,Deployability,INSTALL,INSTALL,44,"I might be completely irrelevant but CLIF's INSTALL.sh usage is ""Usage: $0; [python interpreter]"" ie. it might take a Python of user choice. On Wed, May 2, 2018 at 11:28 AM Pi-Chuan Chang <notifications@github.com>; wrote:. > @chapmanb <https://github.com/chapmanb> Thanks for giving it a try.; > Before I built I did something like:; >; > # Install Python 2.7; > sudo yum install -y centos-release-SCL; > sudo yum install -y python27; > source /opt/rh/python27/enable; >; > I think starting from there it just assumes python is in; > /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can; > make it recognize python at any path.; > Is there a convention that people use to build something so that they can; > point to other Python locations?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/29#issuecomment-386075552>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABQZ2jCil_vF0-yLBB_EZkN8Q9RZQEByks5tufrOgaJpZM4RQhCy>; > .; >. -- ; Thanks,; --Mike",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386079018
https://github.com/google/deepvariant/issues/29#issuecomment-386079018:342,Deployability,Install,Install,342,"I might be completely irrelevant but CLIF's INSTALL.sh usage is ""Usage: $0; [python interpreter]"" ie. it might take a Python of user choice. On Wed, May 2, 2018 at 11:28 AM Pi-Chuan Chang <notifications@github.com>; wrote:. > @chapmanb <https://github.com/chapmanb> Thanks for giving it a try.; > Before I built I did something like:; >; > # Install Python 2.7; > sudo yum install -y centos-release-SCL; > sudo yum install -y python27; > source /opt/rh/python27/enable; >; > I think starting from there it just assumes python is in; > /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can; > make it recognize python at any path.; > Is there a convention that people use to build something so that they can; > point to other Python locations?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/29#issuecomment-386075552>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABQZ2jCil_vF0-yLBB_EZkN8Q9RZQEByks5tufrOgaJpZM4RQhCy>; > .; >. -- ; Thanks,; --Mike",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386079018
https://github.com/google/deepvariant/issues/29#issuecomment-386079018:373,Deployability,install,install,373,"I might be completely irrelevant but CLIF's INSTALL.sh usage is ""Usage: $0; [python interpreter]"" ie. it might take a Python of user choice. On Wed, May 2, 2018 at 11:28 AM Pi-Chuan Chang <notifications@github.com>; wrote:. > @chapmanb <https://github.com/chapmanb> Thanks for giving it a try.; > Before I built I did something like:; >; > # Install Python 2.7; > sudo yum install -y centos-release-SCL; > sudo yum install -y python27; > source /opt/rh/python27/enable; >; > I think starting from there it just assumes python is in; > /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can; > make it recognize python at any path.; > Is there a convention that people use to build something so that they can; > point to other Python locations?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/29#issuecomment-386075552>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABQZ2jCil_vF0-yLBB_EZkN8Q9RZQEByks5tufrOgaJpZM4RQhCy>; > .; >. -- ; Thanks,; --Mike",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386079018
https://github.com/google/deepvariant/issues/29#issuecomment-386079018:391,Deployability,release,release-SCL,391,"I might be completely irrelevant but CLIF's INSTALL.sh usage is ""Usage: $0; [python interpreter]"" ie. it might take a Python of user choice. On Wed, May 2, 2018 at 11:28 AM Pi-Chuan Chang <notifications@github.com>; wrote:. > @chapmanb <https://github.com/chapmanb> Thanks for giving it a try.; > Before I built I did something like:; >; > # Install Python 2.7; > sudo yum install -y centos-release-SCL; > sudo yum install -y python27; > source /opt/rh/python27/enable; >; > I think starting from there it just assumes python is in; > /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can; > make it recognize python at any path.; > Is there a convention that people use to build something so that they can; > point to other Python locations?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/29#issuecomment-386075552>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABQZ2jCil_vF0-yLBB_EZkN8Q9RZQEByks5tufrOgaJpZM4RQhCy>; > .; >. -- ; Thanks,; --Mike",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386079018
https://github.com/google/deepvariant/issues/29#issuecomment-386079018:415,Deployability,install,install,415,"I might be completely irrelevant but CLIF's INSTALL.sh usage is ""Usage: $0; [python interpreter]"" ie. it might take a Python of user choice. On Wed, May 2, 2018 at 11:28 AM Pi-Chuan Chang <notifications@github.com>; wrote:. > @chapmanb <https://github.com/chapmanb> Thanks for giving it a try.; > Before I built I did something like:; >; > # Install Python 2.7; > sudo yum install -y centos-release-SCL; > sudo yum install -y python27; > source /opt/rh/python27/enable; >; > I think starting from there it just assumes python is in; > /opt/rh/python27/root/usr/bin/python. I'll take a look and see if I can; > make it recognize python at any path.; > Is there a convention that people use to build something so that they can; > point to other Python locations?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/29#issuecomment-386075552>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABQZ2jCil_vF0-yLBB_EZkN8Q9RZQEByks5tufrOgaJpZM4RQhCy>; > .; >. -- ; Thanks,; --Mike",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386079018
https://github.com/google/deepvariant/issues/29#issuecomment-386149042:36,Deployability,INSTALL,INSTALL,36,"Correct - specifying the Python for INSTALL is the Python for building CLIF.; Is has _no_ connection to the user Python (they even can be Py2 and Py3 in; any combination).; When using CLIF the default will be the same _version_ (2 or 3) for; generating Python extension modules source code as the build Python was but; even that is controlled with (presence or absence of) --py3 flag for CLIF; tool. On Wed, May 2, 2018 at 1:17 PM Pi-Chuan Chang <notifications@github.com>; wrote:. > @mrovner <https://github.com/mrovner> Thanks Mike. That is at the time of; > building, correct? If I choose with a python interpreter, will the user; > (Brad) need to also have python at the same location?; > I already built one here for CentOS6:; > gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz; > But it seems like @chapmanb <https://github.com/chapmanb> is having; > trouble using it.; > Ideally we'll be able to specify the location differently at run time than; > the one at build time. Do you think that's possible?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/29#issuecomment-386107005>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABQZ2gYshEz7tDrorULEWcrruKM5LrLYks5tuhQ8gaJpZM4RQhCy>; > .; >. -- ; Thanks,; --Mike",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386149042
https://github.com/google/deepvariant/issues/29#issuecomment-386250002:228,Availability,error,error,228,"Pi-Chuan and Mike;; Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:; ```; (17:56:01) INFO: Found 1 target...; (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; Target //deepvariant:binaries failed to build; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; ```; which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386250002
https://github.com/google/deepvariant/issues/29#issuecomment-386250002:410,Availability,ERROR,ERROR,410,"Pi-Chuan and Mike;; Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:; ```; (17:56:01) INFO: Found 1 target...; (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; Target //deepvariant:binaries failed to build; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; ```; which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386250002
https://github.com/google/deepvariant/issues/29#issuecomment-386250002:480,Availability,ERROR,ERROR,480,"Pi-Chuan and Mike;; Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:; ```; (17:56:01) INFO: Found 1 target...; (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; Target //deepvariant:binaries failed to build; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; ```; which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386250002
https://github.com/google/deepvariant/issues/29#issuecomment-386250002:763,Availability,ERROR,ERROR,763,"Pi-Chuan and Mike;; Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:; ```; (17:56:01) INFO: Found 1 target...; (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; Target //deepvariant:binaries failed to build; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; ```; which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386250002
https://github.com/google/deepvariant/issues/29#issuecomment-386250002:1179,Availability,down,download,1179,"Pi-Chuan and Mike;; Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:; ```; (17:56:01) INFO: Found 1 target...; (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; Target //deepvariant:binaries failed to build; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; ```; which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386250002
https://github.com/google/deepvariant/issues/29#issuecomment-386250002:1009,Deployability,install,installed,1009,"Pi-Chuan and Mike;; Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:; ```; (17:56:01) INFO: Found 1 target...; (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; Target //deepvariant:binaries failed to build; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; ```; which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386250002
https://github.com/google/deepvariant/issues/29#issuecomment-386250002:1048,Deployability,install,installing,1048,"Pi-Chuan and Mike;; Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:; ```; (17:56:01) INFO: Found 1 target...; (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; Target //deepvariant:binaries failed to build; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; ```; which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386250002
https://github.com/google/deepvariant/issues/29#issuecomment-386250002:1308,Deployability,install,install,1308,"Pi-Chuan and Mike;; Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:; ```; (17:56:01) INFO: Found 1 target...; (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; Target //deepvariant:binaries failed to build; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; ```; which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386250002
https://github.com/google/deepvariant/issues/29#issuecomment-386250002:1105,Modifiability,sandbox,sandboxed,1105,"Pi-Chuan and Mike;; Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:; ```; (17:56:01) INFO: Found 1 target...; (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; Target //deepvariant:binaries failed to build; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; ```; which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386250002
https://github.com/google/deepvariant/issues/29#issuecomment-386250002:1105,Testability,sandbox,sandboxed,1105,"Pi-Chuan and Mike;; Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:; ```; (17:56:01) INFO: Found 1 target...; (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; Target //deepvariant:binaries failed to build; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; ```; which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386250002
https://github.com/google/deepvariant/issues/29#issuecomment-386327937:832,Availability,error,error,832,"I'm guessing that that is an effect of Python PIP trying to be helpful.; CLIF provides two Python programs/tools (pyclif and pyclif_proto) which are; Python. PIP (with setup.py) creates tiny launchers for them for user; convenience, but encode build Python path and eg. --py3 option into those; launchers.; When user environment for CLIF use is different from the build environment; those launchers are not correct anymore and needs to be removed/regenerated; or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>; wrote:. > Pi-Chuan and Mike;; > Thanks for all this background and help. I'm trying to fit this into the; > conda recipe bazel build for DeepVariant but am not sure how to take; > advantage of using the local anaconda python in that context. The error I'm; > seeing is that bazel can't find pyclif_proto:; >; > (17:56:01) INFO: Found 1 target...; > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; > Target //deepvariant:binaries failed to build; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; >; > which I thought was triggered by the difficulty running pyclif without; > having the local python installed. It could also be due to not installing; > is in /usr/local/bin since I have to remain sandboxed in the work; > directory, but I did adjust the PATH to include the download location.; >; > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either; > understanding how to handle a root install of the pre-build pyclif or; > tweaking to use the lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386327937
https://github.com/google/deepvariant/issues/29#issuecomment-386327937:1021,Availability,ERROR,ERROR,1021,"es two Python programs/tools (pyclif and pyclif_proto) which are; Python. PIP (with setup.py) creates tiny launchers for them for user; convenience, but encode build Python path and eg. --py3 option into those; launchers.; When user environment for CLIF use is different from the build environment; those launchers are not correct anymore and needs to be removed/regenerated; or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>; wrote:. > Pi-Chuan and Mike;; > Thanks for all this background and help. I'm trying to fit this into the; > conda recipe bazel build for DeepVariant but am not sure how to take; > advantage of using the local anaconda python in that context. The error I'm; > seeing is that bazel can't find pyclif_proto:; >; > (17:56:01) INFO: Found 1 target...; > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; > Target //deepvariant:binaries failed to build; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; >; > which I thought was triggered by the difficulty running pyclif without; > having the local python installed. It could also be due to not installing; > is in /usr/local/bin since I have to remain sandboxed in the work; > directory, but I did adjust the PATH to include the download location.; >; > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either; > understanding how to handle a root install of the pre-build pyclif or; > tweaking to use the local python would be helpful. Alternatively, if you; > can already build DeepVariant ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386327937
https://github.com/google/deepvariant/issues/29#issuecomment-386327937:1093,Availability,ERROR,ERROR,1093,"es two Python programs/tools (pyclif and pyclif_proto) which are; Python. PIP (with setup.py) creates tiny launchers for them for user; convenience, but encode build Python path and eg. --py3 option into those; launchers.; When user environment for CLIF use is different from the build environment; those launchers are not correct anymore and needs to be removed/regenerated; or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>; wrote:. > Pi-Chuan and Mike;; > Thanks for all this background and help. I'm trying to fit this into the; > conda recipe bazel build for DeepVariant but am not sure how to take; > advantage of using the local anaconda python in that context. The error I'm; > seeing is that bazel can't find pyclif_proto:; >; > (17:56:01) INFO: Found 1 target...; > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; > Target //deepvariant:binaries failed to build; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; >; > which I thought was triggered by the difficulty running pyclif without; > having the local python installed. It could also be due to not installing; > is in /usr/local/bin since I have to remain sandboxed in the work; > directory, but I did adjust the PATH to include the download location.; >; > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either; > understanding how to handle a root install of the pre-build pyclif or; > tweaking to use the local python would be helpful. Alternatively, if you; > can already build DeepVariant ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386327937
https://github.com/google/deepvariant/issues/29#issuecomment-386327937:1380,Availability,ERROR,ERROR,1380,"n user environment for CLIF use is different from the build environment; those launchers are not correct anymore and needs to be removed/regenerated; or otherwise ""fixed"" to reflect different conditions. On Thu, May 3, 2018 at 3:17 AM Brad Chapman <notifications@github.com>; wrote:. > Pi-Chuan and Mike;; > Thanks for all this background and help. I'm trying to fit this into the; > conda recipe bazel build for DeepVariant but am not sure how to take; > advantage of using the local anaconda python in that context. The error I'm; > seeing is that bazel can't find pyclif_proto:; >; > (17:56:01) INFO: Found 1 target...; > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; > Target //deepvariant:binaries failed to build; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; >; > which I thought was triggered by the difficulty running pyclif without; > having the local python installed. It could also be due to not installing; > is in /usr/local/bin since I have to remain sandboxed in the work; > directory, but I did adjust the PATH to include the download location.; >; > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either; > understanding how to handle a root install of the pre-build pyclif or; > tweaking to use the local python would be helpful. Alternatively, if you; > can already build DeepVariant on a CentOS6 system yourself I could use the; > pre-build binaries the way we're doing now, just with the build against an; > older glibc. Thanks again for the help with this.; >; > —; > You are receiving this because you were",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386327937
https://github.com/google/deepvariant/issues/29#issuecomment-386327937:1803,Availability,down,download,1803,"ike;; > Thanks for all this background and help. I'm trying to fit this into the; > conda recipe bazel build for DeepVariant but am not sure how to take; > advantage of using the local anaconda python in that context. The error I'm; > seeing is that bazel can't find pyclif_proto:; >; > (17:56:01) INFO: Found 1 target...; > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; > Target //deepvariant:binaries failed to build; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; >; > which I thought was triggered by the difficulty running pyclif without; > having the local python installed. It could also be due to not installing; > is in /usr/local/bin since I have to remain sandboxed in the work; > directory, but I did adjust the PATH to include the download location.; >; > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either; > understanding how to handle a root install of the pre-build pyclif or; > tweaking to use the local python would be helpful. Alternatively, if you; > can already build DeepVariant on a CentOS6 system yourself I could use the; > pre-build binaries the way we're doing now, just with the build against an; > older glibc. Thanks again for the help with this.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABQZ2kaD2Oo0vlzfw55tL9A65ZhknIu-ks5tutlEgaJpZM4RQhCy>; > .; >. -- ; Thanks,; --Mike",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386327937
https://github.com/google/deepvariant/issues/29#issuecomment-386327937:1629,Deployability,install,installed,1629,"ad Chapman <notifications@github.com>; wrote:. > Pi-Chuan and Mike;; > Thanks for all this background and help. I'm trying to fit this into the; > conda recipe bazel build for DeepVariant but am not sure how to take; > advantage of using the local anaconda python in that context. The error I'm; > seeing is that bazel can't find pyclif_proto:; >; > (17:56:01) INFO: Found 1 target...; > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; > Target //deepvariant:binaries failed to build; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; >; > which I thought was triggered by the difficulty running pyclif without; > having the local python installed. It could also be due to not installing; > is in /usr/local/bin since I have to remain sandboxed in the work; > directory, but I did adjust the PATH to include the download location.; >; > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either; > understanding how to handle a root install of the pre-build pyclif or; > tweaking to use the local python would be helpful. Alternatively, if you; > can already build DeepVariant on a CentOS6 system yourself I could use the; > pre-build binaries the way we're doing now, just with the build against an; > older glibc. Thanks again for the help with this.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABQZ2kaD2Oo0vlzfw55t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386327937
https://github.com/google/deepvariant/issues/29#issuecomment-386327937:1668,Deployability,install,installing,1668,"ike;; > Thanks for all this background and help. I'm trying to fit this into the; > conda recipe bazel build for DeepVariant but am not sure how to take; > advantage of using the local anaconda python in that context. The error I'm; > seeing is that bazel can't find pyclif_proto:; >; > (17:56:01) INFO: Found 1 target...; > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; > Target //deepvariant:binaries failed to build; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; >; > which I thought was triggered by the difficulty running pyclif without; > having the local python installed. It could also be due to not installing; > is in /usr/local/bin since I have to remain sandboxed in the work; > directory, but I did adjust the PATH to include the download location.; >; > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either; > understanding how to handle a root install of the pre-build pyclif or; > tweaking to use the local python would be helpful. Alternatively, if you; > can already build DeepVariant on a CentOS6 system yourself I could use the; > pre-build binaries the way we're doing now, just with the build against an; > older glibc. Thanks again for the help with this.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABQZ2kaD2Oo0vlzfw55tL9A65ZhknIu-ks5tutlEgaJpZM4RQhCy>; > .; >. -- ; Thanks,; --Mike",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386327937
https://github.com/google/deepvariant/issues/29#issuecomment-386327937:1941,Deployability,install,install,1941,"ike;; > Thanks for all this background and help. I'm trying to fit this into the; > conda recipe bazel build for DeepVariant but am not sure how to take; > advantage of using the local anaconda python in that context. The error I'm; > seeing is that bazel can't find pyclif_proto:; >; > (17:56:01) INFO: Found 1 target...; > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; > Target //deepvariant:binaries failed to build; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; >; > which I thought was triggered by the difficulty running pyclif without; > having the local python installed. It could also be due to not installing; > is in /usr/local/bin since I have to remain sandboxed in the work; > directory, but I did adjust the PATH to include the download location.; >; > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either; > understanding how to handle a root install of the pre-build pyclif or; > tweaking to use the local python would be helpful. Alternatively, if you; > can already build DeepVariant on a CentOS6 system yourself I could use the; > pre-build binaries the way we're doing now, just with the build against an; > older glibc. Thanks again for the help with this.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABQZ2kaD2Oo0vlzfw55tL9A65ZhknIu-ks5tutlEgaJpZM4RQhCy>; > .; >. -- ; Thanks,; --Mike",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386327937
https://github.com/google/deepvariant/issues/29#issuecomment-386327937:1726,Modifiability,sandbox,sandboxed,1726,"ike;; > Thanks for all this background and help. I'm trying to fit this into the; > conda recipe bazel build for DeepVariant but am not sure how to take; > advantage of using the local anaconda python in that context. The error I'm; > seeing is that bazel can't find pyclif_proto:; >; > (17:56:01) INFO: Found 1 target...; > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; > Target //deepvariant:binaries failed to build; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; >; > which I thought was triggered by the difficulty running pyclif without; > having the local python installed. It could also be due to not installing; > is in /usr/local/bin since I have to remain sandboxed in the work; > directory, but I did adjust the PATH to include the download location.; >; > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either; > understanding how to handle a root install of the pre-build pyclif or; > tweaking to use the local python would be helpful. Alternatively, if you; > can already build DeepVariant on a CentOS6 system yourself I could use the; > pre-build binaries the way we're doing now, just with the build against an; > older glibc. Thanks again for the help with this.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABQZ2kaD2Oo0vlzfw55tL9A65ZhknIu-ks5tutlEgaJpZM4RQhCy>; > .; >. -- ; Thanks,; --Mike",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386327937
https://github.com/google/deepvariant/issues/29#issuecomment-386327937:1726,Testability,sandbox,sandboxed,1726,"ike;; > Thanks for all this background and help. I'm trying to fit this into the; > conda recipe bazel build for DeepVariant but am not sure how to take; > advantage of using the local anaconda python in that context. The error I'm; > seeing is that bazel can't find pyclif_proto:; >; > (17:56:01) INFO: Found 1 target...; > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; > Target //deepvariant:binaries failed to build; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; >; > which I thought was triggered by the difficulty running pyclif without; > having the local python installed. It could also be due to not installing; > is in /usr/local/bin since I have to remain sandboxed in the work; > directory, but I did adjust the PATH to include the download location.; >; > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either; > understanding how to handle a root install of the pre-build pyclif or; > tweaking to use the local python would be helpful. Alternatively, if you; > can already build DeepVariant on a CentOS6 system yourself I could use the; > pre-build binaries the way we're doing now, just with the build against an; > older glibc. Thanks again for the help with this.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABQZ2kaD2Oo0vlzfw55tL9A65ZhknIu-ks5tutlEgaJpZM4RQhCy>; > .; >. -- ; Thanks,; --Mike",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386327937
https://github.com/google/deepvariant/issues/29#issuecomment-386513685:377,Availability,error,error,377,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:; ```; sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto; ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:; ```; (06:15:00) INFO: Found 80 targets and 33 test targets...; (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command; (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \; exec env - \; PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/local/bin/python2.7 \; PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386513685
https://github.com/google/deepvariant/issues/29#issuecomment-386513685:459,Availability,ERROR,ERROR,459,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:; ```; sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto; ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:; ```; (06:15:00) INFO: Found 80 targets and 33 test targets...; (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command; (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \; exec env - \; PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/local/bin/python2.7 \; PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386513685
https://github.com/google/deepvariant/issues/29#issuecomment-386513685:643,Availability,error,error,643,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:; ```; sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto; ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:; ```; (06:15:00) INFO: Found 80 targets and 33 test targets...; (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command; (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \; exec env - \; PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/local/bin/python2.7 \; PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386513685
https://github.com/google/deepvariant/issues/29#issuecomment-386513685:2331,Availability,error,error,2331,"and; (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \; exec env - \; PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/local/bin/python2.7 \; PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nsync/internal/common.c -o bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.o); cc1plus: error: unrecognized command line option ""-std=c++11""; cc1plus: warning: unrecognized command line option ""-Wno-maybe-uninitialized""; cc1plus: warning: unrecognized command line option ""-Wno-free-nonheap-object""; (06:15:00) INFO: Elapsed time: 0.386s, Critical Path: 0.05s; (06:15:00) FAILED: Build did NOT complete successfully; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386513685
https://github.com/google/deepvariant/issues/29#issuecomment-386513685:481,Performance,cache,cache,481,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:; ```; sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto; ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:; ```; (06:15:00) INFO: Found 80 targets and 33 test targets...; (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command; (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \; exec env - \; PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/local/bin/python2.7 \; PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386513685
https://github.com/google/deepvariant/issues/29#issuecomment-386513685:687,Performance,cache,cache,687,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:; ```; sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto; ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:; ```; (06:15:00) INFO: Found 80 targets and 33 test targets...; (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command; (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \; exec env - \; PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/local/bin/python2.7 \; PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386513685
https://github.com/google/deepvariant/issues/29#issuecomment-386513685:431,Testability,test,test,431,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:; ```; sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto; ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:; ```; (06:15:00) INFO: Found 80 targets and 33 test targets...; (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command; (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \; exec env - \; PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/local/bin/python2.7 \; PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386513685
https://github.com/google/deepvariant/issues/29#issuecomment-386517018:105,Availability,error,error,105,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:; ```; ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so); ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6.; I did some search and found some old thread that could be relevant:; https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386517018
https://github.com/google/deepvariant/issues/29#issuecomment-386517018:645,Availability,down,down,645,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:; ```; ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so); ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6.; I did some search and found some old thread that could be relevant:; https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386517018
https://github.com/google/deepvariant/issues/29#issuecomment-386517018:16,Deployability,install,install,16,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:; ```; ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so); ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6.; I did some search and found some old thread that could be relevant:; https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386517018
https://github.com/google/deepvariant/issues/29#issuecomment-386517018:54,Deployability,install,install,54,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:; ```; ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so); ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6.; I did some search and found some old thread that could be relevant:; https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386517018
https://github.com/google/deepvariant/issues/29#issuecomment-386517018:559,Deployability,install,install,559,"After trying to install a few things that I failed to install before, linking a few paths, I got to this error that concerns me:; ```; ImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so); ```. It's possible that TensorFlow itself requires a newer version of GLIBC than what's on CentOS 6.; I did some search and found some old thread that could be relevant:; https://github.com/tensorflow/tensorflow/issues/527. @chapmanb Is it possible at all to install this on a different OS? This is getting to a point that I'm worried I'm going down a path with no good ending in sight..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386517018
https://github.com/google/deepvariant/issues/29#issuecomment-386644191:833,Deployability,install,installing,833,"Pi-Chuan;; Thanks for following up with the additional work on this. For the pyclif requirement, it sounds like you're hitting the same problem as me: I'm not sure where to put it so that bazel can find it. Do you understand what locations it looks in and if we can tweak them? I'm not able to put it into `/usr/local` like you did, since we're confined to our work directory. Can I stick it in an arbitrary location and let bazel know?. For tensorflow, it seems like that was built on system with a more recent glibc than on CentOS6. It is a pain to have the full dependency try be compatible with older glibc. This is part of what makes conda nice, is that you're guaranteed to have this (well, as long as the dependency exists). It looks from that thread you linked that the conda package for tensorflow is all good on CentOS6 if installing from there for your build is doable. Thanks again for helping tackle this; I look forward to working on actual fun things instead of compiling and porting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386644191
https://github.com/google/deepvariant/issues/29#issuecomment-386644191:565,Integrability,depend,dependency,565,"Pi-Chuan;; Thanks for following up with the additional work on this. For the pyclif requirement, it sounds like you're hitting the same problem as me: I'm not sure where to put it so that bazel can find it. Do you understand what locations it looks in and if we can tweak them? I'm not able to put it into `/usr/local` like you did, since we're confined to our work directory. Can I stick it in an arbitrary location and let bazel know?. For tensorflow, it seems like that was built on system with a more recent glibc than on CentOS6. It is a pain to have the full dependency try be compatible with older glibc. This is part of what makes conda nice, is that you're guaranteed to have this (well, as long as the dependency exists). It looks from that thread you linked that the conda package for tensorflow is all good on CentOS6 if installing from there for your build is doable. Thanks again for helping tackle this; I look forward to working on actual fun things instead of compiling and porting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386644191
https://github.com/google/deepvariant/issues/29#issuecomment-386644191:712,Integrability,depend,dependency,712,"Pi-Chuan;; Thanks for following up with the additional work on this. For the pyclif requirement, it sounds like you're hitting the same problem as me: I'm not sure where to put it so that bazel can find it. Do you understand what locations it looks in and if we can tweak them? I'm not able to put it into `/usr/local` like you did, since we're confined to our work directory. Can I stick it in an arbitrary location and let bazel know?. For tensorflow, it seems like that was built on system with a more recent glibc than on CentOS6. It is a pain to have the full dependency try be compatible with older glibc. This is part of what makes conda nice, is that you're guaranteed to have this (well, as long as the dependency exists). It looks from that thread you linked that the conda package for tensorflow is all good on CentOS6 if installing from there for your build is doable. Thanks again for helping tackle this; I look forward to working on actual fun things instead of compiling and porting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386644191
https://github.com/google/deepvariant/issues/29#issuecomment-386752411:671,Availability,error,error,671,"1) @chapmanb : I think this is what you're looking for! @depristo pointed it out me, and I felt dumb for not thinking just to edit the WORKSPACE (and instead I linked the file to the ""right place"" instead).; In the `WORKSPACE` file of DeepVariant, you can see this at the bottom.; I tried changing the path:; ```; new_local_repository(; name = ""clif"",; build_file = ""third_party/clif.BUILD"",; path = ""/home/pichuan"",; ); ```. And I make sure the two files are there:; ```; $ ls /home/pichuan/clif/bin/; pyclif pyclif_proto; ```; After this change, it seems to run past the part where it can't find clif! Basically the `missing input file '@clif//:clif/bin/pyclif_proto'` error was no longer there after this change. 2) You're right -- I just tried installing TensorFlow with `conda install tensorflow` on CentOS6. It's so easy and smooth. That's great. However, I'm not sure which directory I should point to as a replacement for the pointer in our WORKSPACE file:; ```; # Import tensorflow. Note path.; local_repository(; name = ""org_tensorflow"",; path = ""../tensorflow"",; ); ```; So I'm currently block on that. Maybe you'll have better luck once you get past 1). Please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386752411
https://github.com/google/deepvariant/issues/29#issuecomment-386752411:748,Deployability,install,installing,748,"1) @chapmanb : I think this is what you're looking for! @depristo pointed it out me, and I felt dumb for not thinking just to edit the WORKSPACE (and instead I linked the file to the ""right place"" instead).; In the `WORKSPACE` file of DeepVariant, you can see this at the bottom.; I tried changing the path:; ```; new_local_repository(; name = ""clif"",; build_file = ""third_party/clif.BUILD"",; path = ""/home/pichuan"",; ); ```. And I make sure the two files are there:; ```; $ ls /home/pichuan/clif/bin/; pyclif pyclif_proto; ```; After this change, it seems to run past the part where it can't find clif! Basically the `missing input file '@clif//:clif/bin/pyclif_proto'` error was no longer there after this change. 2) You're right -- I just tried installing TensorFlow with `conda install tensorflow` on CentOS6. It's so easy and smooth. That's great. However, I'm not sure which directory I should point to as a replacement for the pointer in our WORKSPACE file:; ```; # Import tensorflow. Note path.; local_repository(; name = ""org_tensorflow"",; path = ""../tensorflow"",; ); ```; So I'm currently block on that. Maybe you'll have better luck once you get past 1). Please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386752411
https://github.com/google/deepvariant/issues/29#issuecomment-386752411:782,Deployability,install,install,782,"1) @chapmanb : I think this is what you're looking for! @depristo pointed it out me, and I felt dumb for not thinking just to edit the WORKSPACE (and instead I linked the file to the ""right place"" instead).; In the `WORKSPACE` file of DeepVariant, you can see this at the bottom.; I tried changing the path:; ```; new_local_repository(; name = ""clif"",; build_file = ""third_party/clif.BUILD"",; path = ""/home/pichuan"",; ); ```. And I make sure the two files are there:; ```; $ ls /home/pichuan/clif/bin/; pyclif pyclif_proto; ```; After this change, it seems to run past the part where it can't find clif! Basically the `missing input file '@clif//:clif/bin/pyclif_proto'` error was no longer there after this change. 2) You're right -- I just tried installing TensorFlow with `conda install tensorflow` on CentOS6. It's so easy and smooth. That's great. However, I'm not sure which directory I should point to as a replacement for the pointer in our WORKSPACE file:; ```; # Import tensorflow. Note path.; local_repository(; name = ""org_tensorflow"",; path = ""../tensorflow"",; ); ```; So I'm currently block on that. Maybe you'll have better luck once you get past 1). Please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386752411
https://github.com/google/deepvariant/issues/29#issuecomment-386869866:229,Availability,avail,available,229,"Pi-Chuan;; Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:; ```; sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD; ```; but bazel is too smart and won't let us continue with non-bazel defined references:; ```; (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root.; ```; So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386869866
https://github.com/google/deepvariant/issues/29#issuecomment-386869866:383,Availability,error,errors,383,"Pi-Chuan;; Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:; ```; sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD; ```; but bazel is too smart and won't let us continue with non-bazel defined references:; ```; (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root.; ```; So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386869866
https://github.com/google/deepvariant/issues/29#issuecomment-386869866:573,Availability,error,error,573,"Pi-Chuan;; Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:; ```; sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD; ```; but bazel is too smart and won't let us continue with non-bazel defined references:; ```; (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root.; ```; So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386869866
https://github.com/google/deepvariant/issues/29#issuecomment-386869866:589,Availability,error,error,589,"Pi-Chuan;; Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:; ```; sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD; ```; but bazel is too smart and won't let us continue with non-bazel defined references:; ```; (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root.; ```; So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386869866
https://github.com/google/deepvariant/issues/29#issuecomment-386869866:757,Availability,ERROR,ERROR,757,"Pi-Chuan;; Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:; ```; sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD; ```; but bazel is too smart and won't let us continue with non-bazel defined references:; ```; (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root.; ```; So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386869866
https://github.com/google/deepvariant/issues/29#issuecomment-386869866:1543,Deployability,install,installing,1543,"Pi-Chuan;; Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:; ```; sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD; ```; but bazel is too smart and won't let us continue with non-bazel defined references:; ```; (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root.; ```; So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386869866
https://github.com/google/deepvariant/issues/29#issuecomment-386869866:1523,Integrability,depend,dependencies,1523,"Pi-Chuan;; Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:; ```; sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD; ```; but bazel is too smart and won't let us continue with non-bazel defined references:; ```; (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root.; ```; So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386869866
https://github.com/google/deepvariant/issues/29#issuecomment-386869866:777,Performance,cache,cache,777,"Pi-Chuan;; Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:; ```; sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD; ```; but bazel is too smart and won't let us continue with non-bazel defined references:; ```; (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root.; ```; So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-386869866
https://github.com/google/deepvariant/issues/29#issuecomment-387179220:192,Availability,error,errors,192,"@chapmanb ; I'm actually pretty new to this whole build thing myself. And I'm not really that familiar with bazel myself.; Do you have some instructions on how to reproduce all the way to the errors you hit? Having that will be useful for me to try to figure this out. . And, I'll try to see if I find some bazel experts internally to look at your questions as well. Maybe this is a very trivial question for people who have seen it before...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-387179220
https://github.com/google/deepvariant/issues/29#issuecomment-387445867:252,Integrability,depend,dependencies,252,"Pi-Chuan;; Thanks for checking around to see if we can get a bazel expert involved. I think that would be the best way forward, as I'm just hacking around and don't have a strong understanding of how best to attack this. It's a general issue where the dependencies are present in a non-system directory and how to inject that into a build. I'm trying to build this inside of bioconda. If you want to get it setup there are instructions here: https://bioconda.github.io/contributing.html and I could share the current recipe I'm working from. Although I don't want to make you wade into a new build system and get familiar with that if we can get more high level bazel advice and sort through improving the DeepVariant build process to handle this case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-387445867
https://github.com/google/deepvariant/issues/29#issuecomment-387445867:314,Integrability,inject,inject,314,"Pi-Chuan;; Thanks for checking around to see if we can get a bazel expert involved. I think that would be the best way forward, as I'm just hacking around and don't have a strong understanding of how best to attack this. It's a general issue where the dependencies are present in a non-system directory and how to inject that into a build. I'm trying to build this inside of bioconda. If you want to get it setup there are instructions here: https://bioconda.github.io/contributing.html and I could share the current recipe I'm working from. Although I don't want to make you wade into a new build system and get familiar with that if we can get more high level bazel advice and sort through improving the DeepVariant build process to handle this case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-387445867
https://github.com/google/deepvariant/issues/29#issuecomment-387445867:208,Security,attack,attack,208,"Pi-Chuan;; Thanks for checking around to see if we can get a bazel expert involved. I think that would be the best way forward, as I'm just hacking around and don't have a strong understanding of how best to attack this. It's a general issue where the dependencies are present in a non-system directory and how to inject that into a build. I'm trying to build this inside of bioconda. If you want to get it setup there are instructions here: https://bioconda.github.io/contributing.html and I could share the current recipe I'm working from. Although I don't want to make you wade into a new build system and get familiar with that if we can get more high level bazel advice and sort through improving the DeepVariant build process to handle this case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-387445867
https://github.com/google/deepvariant/issues/29#issuecomment-387445867:314,Security,inject,inject,314,"Pi-Chuan;; Thanks for checking around to see if we can get a bazel expert involved. I think that would be the best way forward, as I'm just hacking around and don't have a strong understanding of how best to attack this. It's a general issue where the dependencies are present in a non-system directory and how to inject that into a build. I'm trying to build this inside of bioconda. If you want to get it setup there are instructions here: https://bioconda.github.io/contributing.html and I could share the current recipe I'm working from. Although I don't want to make you wade into a new build system and get familiar with that if we can get more high level bazel advice and sort through improving the DeepVariant build process to handle this case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-387445867
https://github.com/google/deepvariant/issues/29#issuecomment-390375949:313,Deployability,release,release,313,"+1 for work being done. Thanks!. I cannot use the binaries:. ```; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmpdata/Bazel.runfiles_52e5Mr/runfiles/com_google_deepvariant/third_party/nucleus/io/python/../../../../_solib_k8/libexternal_Shtslib_Slibhtslib.so); ```. On CentOS Linux release 7.2.1511 (Core), HPC cluster if that makes a difference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-390375949
https://github.com/google/deepvariant/issues/29#issuecomment-390984830:116,Integrability,depend,dependencies,116,"Jillian;; Thanks for offering to help with this. Unfortunately we're still stuck on building with bazel using conda dependencies. I have a build that works around the clif issue, but don't know how to modify the DeepVariant bazel build infrastructure so that we can use an arbitrary prefix. Right not it assumes libraries are present in `/usr` so fails to pick up the anaconda zlib and friends. We've been trying to identify someone who knows bazel to help walk us through how to change the DeepVariant build to support this. Until we can get a native CentOS6 build it unfortunately will have issues on CentOS due to compiling against more recent glibc on Ubuntu in the pre-built binaries. If you have any bazel expertise or want to dig into this, that would be much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-390984830
https://github.com/google/deepvariant/issues/29#issuecomment-391324483:290,Safety,detect,detecting,290,"Jillian;; Awesome, thanks so much. Here is a branch with where we're at right now:. https://github.com/chapmanb/bioconda-recipes/tree/deepvariant-compile/recipes/deepvariant. Lots of hacking in there to reference the conda python with pyclif but that works and then should get stuck on not detecting zlib during the htslib compile. Let me know if you have any questions and thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/29#issuecomment-391324483
https://github.com/google/deepvariant/issues/30#issuecomment-355031587:146,Deployability,install,installing,146,"Thanks @machomachopadre for your report. Just so I'm 100% clear, you've got python 2.7 and 3.5 on the machine, and our build-prereqs.sh script is installing some packages into python 3.5 and some into 2.7? I don't think we've been clear before about this, but DeepVariant is intended for python 2.7 only, as we've never tested it using python3. . Can you confirm that you can install DeepVariant on a clean Ubuntu 16 instance in the cloud?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/30#issuecomment-355031587
https://github.com/google/deepvariant/issues/30#issuecomment-355031587:376,Deployability,install,install,376,"Thanks @machomachopadre for your report. Just so I'm 100% clear, you've got python 2.7 and 3.5 on the machine, and our build-prereqs.sh script is installing some packages into python 3.5 and some into 2.7? I don't think we've been clear before about this, but DeepVariant is intended for python 2.7 only, as we've never tested it using python3. . Can you confirm that you can install DeepVariant on a clean Ubuntu 16 instance in the cloud?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/30#issuecomment-355031587
https://github.com/google/deepvariant/issues/30#issuecomment-355031587:320,Testability,test,tested,320,"Thanks @machomachopadre for your report. Just so I'm 100% clear, you've got python 2.7 and 3.5 on the machine, and our build-prereqs.sh script is installing some packages into python 3.5 and some into 2.7? I don't think we've been clear before about this, but DeepVariant is intended for python 2.7 only, as we've never tested it using python3. . Can you confirm that you can install DeepVariant on a clean Ubuntu 16 instance in the cloud?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/30#issuecomment-355031587
https://github.com/google/deepvariant/issues/30#issuecomment-355031587:58,Usability,clear,clear,58,"Thanks @machomachopadre for your report. Just so I'm 100% clear, you've got python 2.7 and 3.5 on the machine, and our build-prereqs.sh script is installing some packages into python 3.5 and some into 2.7? I don't think we've been clear before about this, but DeepVariant is intended for python 2.7 only, as we've never tested it using python3. . Can you confirm that you can install DeepVariant on a clean Ubuntu 16 instance in the cloud?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/30#issuecomment-355031587
https://github.com/google/deepvariant/issues/30#issuecomment-355031587:231,Usability,clear,clear,231,"Thanks @machomachopadre for your report. Just so I'm 100% clear, you've got python 2.7 and 3.5 on the machine, and our build-prereqs.sh script is installing some packages into python 3.5 and some into 2.7? I don't think we've been clear before about this, but DeepVariant is intended for python 2.7 only, as we've never tested it using python3. . Can you confirm that you can install DeepVariant on a clean Ubuntu 16 instance in the cloud?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/30#issuecomment-355031587
https://github.com/google/deepvariant/issues/30#issuecomment-355047368:197,Deployability,install,installs,197,"@depristo This might be happening because I'd aliased the ""python3"" command as ""python"", which is supposed to be for python 2.7 by default. Any idea as to how I could reset everything such that it installs only for python 2.7 (I've already removed the alias)?; ; EDIT: The closing and reopening of the issue was a mistake, please ignore.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/30#issuecomment-355047368
https://github.com/google/deepvariant/issues/32#issuecomment-355348946:11,Usability,clear,clear,11,"To be 100% clear, are you saying you booted a clean ubuntu 16 instance and it failed to build there? Or is this on an already customized machine?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32#issuecomment-355348946
https://github.com/google/deepvariant/issues/32#issuecomment-355386945:37,Deployability,install,installed,37,It is customised. And Python 2.7 was installed using Anaconda.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32#issuecomment-355386945
https://github.com/google/deepvariant/issues/32#issuecomment-355522771:575,Availability,error,error,575,"To make it clearer, I put the path structure here.; ```; /deepvariant/core/; cloud_utils_test.py; math.py; ...; ```; And in `cloud_utils_test.py`:; ```; """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import; from __future__ import division; from __future__ import print_function. import httplib; ...; ```; Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. ; But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:; ```; >>> import httplib; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>; import mimetools; File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>; import tempfile; File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>; from random import Random as _Random; File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>; from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil; ***File ""math.py"", line 79, in <module>***; import numpy as np; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import add_newdocs; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>; from numpy.lib import add_newdoc; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>; from .type_check import *; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>; import numpy.core.numeric as _nx; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>; from numpy.testing.nosetester impo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32#issuecomment-355522771
https://github.com/google/deepvariant/issues/32#issuecomment-355522771:671,Availability,error,error,671,"To make it clearer, I put the path structure here.; ```; /deepvariant/core/; cloud_utils_test.py; math.py; ...; ```; And in `cloud_utils_test.py`:; ```; """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import; from __future__ import division; from __future__ import print_function. import httplib; ...; ```; Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. ; But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:; ```; >>> import httplib; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>; import mimetools; File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>; import tempfile; File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>; from random import Random as _Random; File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>; from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil; ***File ""math.py"", line 79, in <module>***; import numpy as np; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import add_newdocs; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>; from numpy.lib import add_newdoc; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>; from .type_check import *; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>; import numpy.core.numeric as _nx; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>; from numpy.testing.nosetester impo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32#issuecomment-355522771
https://github.com/google/deepvariant/issues/32#issuecomment-355522771:2546,Performance,load,load,2546,"ttplib; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>; import mimetools; File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>; import tempfile; File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>; from random import Random as _Random; File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>; from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil; ***File ""math.py"", line 79, in <module>***; import numpy as np; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import add_newdocs; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>; from numpy.lib import add_newdoc; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>; from .type_check import *; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>; import numpy.core.numeric as _nx; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>; from numpy.testing.nosetester import _numpy_tester; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>; from . import decorators as dec; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>; from .utils import SkipTest, assert_warns; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>; from tempfile import mkdtemp, mkstemp; ImportError: cannot import name mkdtemp; >>> ; ```; As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module.; On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32#issuecomment-355522771
https://github.com/google/deepvariant/issues/32#issuecomment-355522771:156,Testability,Test,Tests,156,"To make it clearer, I put the path structure here.; ```; /deepvariant/core/; cloud_utils_test.py; math.py; ...; ```; And in `cloud_utils_test.py`:; ```; """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import; from __future__ import division; from __future__ import print_function. import httplib; ...; ```; Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. ; But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:; ```; >>> import httplib; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>; import mimetools; File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>; import tempfile; File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>; from random import Random as _Random; File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>; from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil; ***File ""math.py"", line 79, in <module>***; import numpy as np; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import add_newdocs; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>; from numpy.lib import add_newdoc; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>; from .type_check import *; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>; import numpy.core.numeric as _nx; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>; from numpy.testing.nosetester impo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32#issuecomment-355522771
https://github.com/google/deepvariant/issues/32#issuecomment-355522771:585,Testability,test,test,585,"To make it clearer, I put the path structure here.; ```; /deepvariant/core/; cloud_utils_test.py; math.py; ...; ```; And in `cloud_utils_test.py`:; ```; """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import; from __future__ import division; from __future__ import print_function. import httplib; ...; ```; Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. ; But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:; ```; >>> import httplib; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>; import mimetools; File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>; import tempfile; File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>; from random import Random as _Random; File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>; from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil; ***File ""math.py"", line 79, in <module>***; import numpy as np; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import add_newdocs; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>; from numpy.lib import add_newdoc; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>; from .type_check import *; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>; import numpy.core.numeric as _nx; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>; from numpy.testing.nosetester impo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32#issuecomment-355522771
https://github.com/google/deepvariant/issues/32#issuecomment-355522771:1192,Testability,log,log,1192,"_future__ import absolute_import; from __future__ import division; from __future__ import print_function. import httplib; ...; ```; Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. ; But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:; ```; >>> import httplib; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>; import mimetools; File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>; import tempfile; File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>; from random import Random as _Random; File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>; from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil; ***File ""math.py"", line 79, in <module>***; import numpy as np; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import add_newdocs; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>; from numpy.lib import add_newdoc; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>; from .type_check import *; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>; import numpy.core.numeric as _nx; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>; from numpy.testing.nosetester import _numpy_tester; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>; from . import decorators as dec; File ""xx/anaconda/envs/Python27/lib/python2.7/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32#issuecomment-355522771
https://github.com/google/deepvariant/issues/32#issuecomment-355522771:1978,Testability,test,testing,1978,"ttplib; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>; import mimetools; File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>; import tempfile; File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>; from random import Random as _Random; File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>; from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil; ***File ""math.py"", line 79, in <module>***; import numpy as np; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import add_newdocs; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>; from numpy.lib import add_newdoc; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>; from .type_check import *; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>; import numpy.core.numeric as _nx; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>; from numpy.testing.nosetester import _numpy_tester; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>; from . import decorators as dec; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>; from .utils import SkipTest, assert_warns; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>; from tempfile import mkdtemp, mkstemp; ImportError: cannot import name mkdtemp; >>> ; ```; As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module.; On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32#issuecomment-355522771
https://github.com/google/deepvariant/issues/32#issuecomment-355522771:2085,Testability,test,testing,2085,"ttplib; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>; import mimetools; File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>; import tempfile; File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>; from random import Random as _Random; File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>; from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil; ***File ""math.py"", line 79, in <module>***; import numpy as np; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import add_newdocs; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>; from numpy.lib import add_newdoc; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>; from .type_check import *; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>; import numpy.core.numeric as _nx; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>; from numpy.testing.nosetester import _numpy_tester; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>; from . import decorators as dec; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>; from .utils import SkipTest, assert_warns; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>; from tempfile import mkdtemp, mkstemp; ImportError: cannot import name mkdtemp; >>> ; ```; As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module.; On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32#issuecomment-355522771
https://github.com/google/deepvariant/issues/32#issuecomment-355522771:2228,Testability,test,testing,2228,"ttplib; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>; import mimetools; File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>; import tempfile; File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>; from random import Random as _Random; File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>; from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil; ***File ""math.py"", line 79, in <module>***; import numpy as np; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import add_newdocs; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>; from numpy.lib import add_newdoc; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>; from .type_check import *; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>; import numpy.core.numeric as _nx; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>; from numpy.testing.nosetester import _numpy_tester; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>; from . import decorators as dec; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>; from .utils import SkipTest, assert_warns; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>; from tempfile import mkdtemp, mkstemp; ImportError: cannot import name mkdtemp; >>> ; ```; As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module.; On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32#issuecomment-355522771
https://github.com/google/deepvariant/issues/32#issuecomment-355522771:2383,Testability,test,testing,2383,"ttplib; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>; import mimetools; File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>; import tempfile; File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>; from random import Random as _Random; File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>; from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil; ***File ""math.py"", line 79, in <module>***; import numpy as np; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import add_newdocs; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>; from numpy.lib import add_newdoc; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>; from .type_check import *; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>; import numpy.core.numeric as _nx; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>; from numpy.testing.nosetester import _numpy_tester; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>; from . import decorators as dec; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>; from .utils import SkipTest, assert_warns; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>; from tempfile import mkdtemp, mkstemp; ImportError: cannot import name mkdtemp; >>> ; ```; As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module.; On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32#issuecomment-355522771
https://github.com/google/deepvariant/issues/32#issuecomment-355522771:11,Usability,clear,clearer,11,"To make it clearer, I put the path structure here.; ```; /deepvariant/core/; cloud_utils_test.py; math.py; ...; ```; And in `cloud_utils_test.py`:; ```; """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import; from __future__ import division; from __future__ import print_function. import httplib; ...; ```; Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. ; But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:; ```; >>> import httplib; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>; import mimetools; File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>; import tempfile; File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>; from random import Random as _Random; File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>; from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil; ***File ""math.py"", line 79, in <module>***; import numpy as np; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import add_newdocs; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>; from numpy.lib import add_newdoc; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>; from .type_check import *; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>; import numpy.core.numeric as _nx; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>; from numpy.testing.nosetester impo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32#issuecomment-355522771
https://github.com/google/deepvariant/issues/32#issuecomment-355522771:606,Usability,simpl,simply,606,"To make it clearer, I put the path structure here.; ```; /deepvariant/core/; cloud_utils_test.py; math.py; ...; ```; And in `cloud_utils_test.py`:; ```; """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import; from __future__ import division; from __future__ import print_function. import httplib; ...; ```; Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. ; But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:; ```; >>> import httplib; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>; import mimetools; File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>; import tempfile; File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>; from random import Random as _Random; File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>; from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil; ***File ""math.py"", line 79, in <module>***; import numpy as np; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import add_newdocs; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>; from numpy.lib import add_newdoc; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>; from .type_check import *; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>; import numpy.core.numeric as _nx; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>; from numpy.testing.nosetester impo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32#issuecomment-355522771
https://github.com/google/deepvariant/issues/32#issuecomment-355654713:156,Deployability,update,update,156,I agree this is a problem. We have an internal bug tracking this. Probably we will just rename deepvariant/core/math.py to core_math.py or equivalent. I'll update this bug when the change is in internally and it'll show up in the next push of deepvariant to github. Note you can workaround this issue just like https://github.com/notoraptor/deepvariant/commit/15c2deb211672a8ba32c1cbe609d81e1a2b0fb74,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32#issuecomment-355654713
https://github.com/google/deepvariant/issues/32#issuecomment-356119694:166,Deployability,update,update,166,"A fix is in in google, renaming math.py to genomics_math.py, which should fix the problem. The next major push of functional changes to DeepVariant will include this update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/32#issuecomment-356119694
https://github.com/google/deepvariant/issues/33#issuecomment-355423493:197,Deployability,pipeline,pipelines,197,"There isn't an option to provide multiple BAM files in a single request since each request needs a separate staging directory and needs to be processed independently. However, you may run multiple pipelines at the same time provided you have enough quota in your project (IPs, CPU, GPU (if applicable), disk). If you are running a substantial (>1000) number of jobs, then please also increase the genomics API quota. See https://cloud.google.com/compute/quotas for details on how to adjust your quota.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/33#issuecomment-355423493
https://github.com/google/deepvariant/issues/33#issuecomment-355563693:13,Energy Efficiency,charge,charge,13,So it should charge same amount of money right (compare running one by one with in parallel)?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/33#issuecomment-355563693
https://github.com/google/deepvariant/pull/34#issuecomment-356449021:805,Security,authoriz,authorized,805,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify. Thanks. ---. - If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; - If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.; - In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/34#issuecomment-356449021
https://github.com/google/deepvariant/pull/34#issuecomment-356449021:930,Security,authoriz,authorized,930,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify. Thanks. ---. - If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; - If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.; - In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/34#issuecomment-356449021
https://github.com/google/deepvariant/pull/34#issuecomment-356449021:1094,Security,authoriz,authorized,1094,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify. Thanks. ---. - If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; - If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.; - In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/34#issuecomment-356449021
https://github.com/google/deepvariant/issues/35#issuecomment-356807166:122,Deployability,release,release,122,"We have a CL going into our internal systems at Google, which we will cherry-pick in r0.4 or wait until the upcoming r0.5 release, depending on timing. Here's the text, though:. `make_examples` requires its input files to satisfy a few basic requirements to; be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be; indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible""; version of the genome reference provided as the `--ref`. By compatible here we; mean the BAM and FASTA share at least a reasonable set of common contigs, as; DeepVariant will only process contigs shared by both the BAM and reference. As; an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you; provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only; process variants on the shared contigs, effectively excluding the hs37d5 contig; present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you; cannot pipe it into DeepVariant. We currently recommend that the BAM be; duplicate marked, but it's unclear if this is even necessary. Finally, it's not; necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should; refer to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and; `confident_regions` arguments should point to VCF and BED files containing the; true variants and regions where we are confide",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35#issuecomment-356807166
https://github.com/google/deepvariant/issues/35#issuecomment-356807166:131,Integrability,depend,depending,131,"We have a CL going into our internal systems at Google, which we will cherry-pick in r0.4 or wait until the upcoming r0.5 release, depending on timing. Here's the text, though:. `make_examples` requires its input files to satisfy a few basic requirements to; be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be; indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible""; version of the genome reference provided as the `--ref`. By compatible here we; mean the BAM and FASTA share at least a reasonable set of common contigs, as; DeepVariant will only process contigs shared by both the BAM and reference. As; an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you; provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only; process variants on the shared contigs, effectively excluding the hs37d5 contig; present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you; cannot pipe it into DeepVariant. We currently recommend that the BAM be; duplicate marked, but it's unclear if this is even necessary. Finally, it's not; necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should; refer to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and; `confident_regions` arguments should point to VCF and BED files containing the; true variants and regions where we are confide",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/35#issuecomment-356807166
https://github.com/google/deepvariant/issues/37#issuecomment-357087121:63,Deployability,release,release,63,Thanks @jjfarrell. We've added a bug and will fix for the next release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37#issuecomment-357087121
https://github.com/google/deepvariant/issues/37#issuecomment-360202139:54,Deployability,release,release,54,Nice! This is fixed now and will go out with the next release of DeepVariant. Thanks for the great suggestion.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37#issuecomment-360202139
https://github.com/google/deepvariant/issues/37#issuecomment-529713917:375,Deployability,release,release,375,"@rpoplin @depristo Is there a specific reason to exclude all of the super contigs in GRCh38 from deepvariant analysis? I agree with removing the HLA, EBV, M/MT and decoys, but removing the placed and unplaced contigs doesn't seem justified. Other callers do work on these regions so it doesn't seem like they are truly ""Common problematic contigs on GRCh38"" as stated in the release notes for v0.5.0. Does it affect training or calling if they were included? I'd suggest the user should be able to provide the list of regions to exclude or include as its taken a bit of work to sort out why deepvariant is not making calls on all contigs in the provided BAM files like a number of other callers we are testing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37#issuecomment-529713917
https://github.com/google/deepvariant/issues/37#issuecomment-529713917:702,Testability,test,testing,702,"@rpoplin @depristo Is there a specific reason to exclude all of the super contigs in GRCh38 from deepvariant analysis? I agree with removing the HLA, EBV, M/MT and decoys, but removing the placed and unplaced contigs doesn't seem justified. Other callers do work on these regions so it doesn't seem like they are truly ""Common problematic contigs on GRCh38"" as stated in the release notes for v0.5.0. Does it affect training or calling if they were included? I'd suggest the user should be able to provide the list of regions to exclude or include as its taken a bit of work to sort out why deepvariant is not making calls on all contigs in the provided BAM files like a number of other callers we are testing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37#issuecomment-529713917
https://github.com/google/deepvariant/issues/37#issuecomment-531149691:694,Deployability,release,release,694,"Hi @jjfarrell . I hadn't encountered users who meaningfully used the unplaced, but non-decoy contigs. The exclude list is current coded into DeepVariant and not modifiable. However, I agree that it would be reasonable to assess adding those in. The main reason a decoy or unplaced contig should be excluded is if it aggregates a large amount of incorrectly mapped coverage. It makes sense for us to investigate how much the placed and unplaced contigs are affected by that and decide based on those results. Training will not be affected, because these contigs do not have truth label variants that I am aware of. The model will be the same. We'll consider this a potential change for the next release, which will hopefully be in the next month or two. I hope that timing will be adequate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37#issuecomment-531149691
https://github.com/google/deepvariant/issues/37#issuecomment-1673451441:317,Performance,perform,performed,317,"Dear @ESDeutekom,. There are a few things to unpack here. The goal is to make sure that the BAM file and reference FASTA file match in their contigs and bases to ensure correct variants get called between the two, and if local realignment is required that the regions match appropriately. Thus, a couple of steps get performed:. $`1)`$ So in order to be sure there is proper consensus between the two, a first pass over the reference gets processed to remove contigs based on an internal list of excluded contigs found in the following file:. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. This is done in the reference first, because most likely these would not exist in most BAM files. $`2)`$ Now given the remaining contigs from the reference, these are used to find common contigs that exist in the BAM file as well, to ensure there is proper overlap. All of this (including $`Step \; 1`$ above) is performed by the [`_ensure_consistent_contigs` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L345-L383). The two steps summarize the statement you noticed in the documentation. Now the only excluding option I see is for [`exclude_regions`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L94-L100), though the rest of the above excluded contigs are hard-coded as [`exclude_contigs=exclude_contigs.EXCLUDED_HUMAN_CONTIGS`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L349). To exclude a region, that would come as a space-separated list of regions, which can be region literals (e.g., `chr20:10-20`) or paths to BED/BEDPE files. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37#issuecomment-1673451441
https://github.com/google/deepvariant/issues/37#issuecomment-1673451441:936,Performance,perform,performed,936,"Dear @ESDeutekom,. There are a few things to unpack here. The goal is to make sure that the BAM file and reference FASTA file match in their contigs and bases to ensure correct variants get called between the two, and if local realignment is required that the regions match appropriately. Thus, a couple of steps get performed:. $`1)`$ So in order to be sure there is proper consensus between the two, a first pass over the reference gets processed to remove contigs based on an internal list of excluded contigs found in the following file:. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. This is done in the reference first, because most likely these would not exist in most BAM files. $`2)`$ Now given the remaining contigs from the reference, these are used to find common contigs that exist in the BAM file as well, to ensure there is proper overlap. All of this (including $`Step \; 1`$ above) is performed by the [`_ensure_consistent_contigs` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L345-L383). The two steps summarize the statement you noticed in the documentation. Now the only excluding option I see is for [`exclude_regions`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L94-L100), though the rest of the above excluded contigs are hard-coded as [`exclude_contigs=exclude_contigs.EXCLUDED_HUMAN_CONTIGS`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L349). To exclude a region, that would come as a space-separated list of regions, which can be region literals (e.g., `chr20:10-20`) or paths to BED/BEDPE files. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37#issuecomment-1673451441
https://github.com/google/deepvariant/issues/37#issuecomment-1675079201:275,Availability,Mask,Masked-reference-genomes,275,"Hi Eva, . Usually such comparisons are rare, as Kishwar suggested through Heng's article. If you are curious about such a comparison, below is a table generated on GIAB samples through [GATK's HaplotypeCaller](https://gatk.broadinstitute.org/hc/en-us/articles/17295731870235-Masked-reference-genomes) on GRCh38 and GRCh38 with masked alt regions:. ![image](https://github.com/google/deepvariant/assets/6555937/18be1f0e-6c1f-4c65-ae76-51661af20282). Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37#issuecomment-1675079201
https://github.com/google/deepvariant/issues/37#issuecomment-1675079201:327,Availability,mask,masked,327,"Hi Eva, . Usually such comparisons are rare, as Kishwar suggested through Heng's article. If you are curious about such a comparison, below is a table generated on GIAB samples through [GATK's HaplotypeCaller](https://gatk.broadinstitute.org/hc/en-us/articles/17295731870235-Masked-reference-genomes) on GRCh38 and GRCh38 with masked alt regions:. ![image](https://github.com/google/deepvariant/assets/6555937/18be1f0e-6c1f-4c65-ae76-51661af20282). Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/37#issuecomment-1675079201
https://github.com/google/deepvariant/issues/38#issuecomment-357025416:149,Modifiability,extend,extend,149,"We are 80% of the way there on CRAM support. Under the hood DeepVariant uses htslib to read it's reads datasets, which supports cram. But we need to extend the IO systems to pass in the reference genome to read a CRAM file, and that's not possible right now. We are hopeful that the community would extend DeepVariant to handle CRAM files, so please feel free to contribute a pull request with the functionality if you decide to add it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38#issuecomment-357025416
https://github.com/google/deepvariant/issues/38#issuecomment-357025416:299,Modifiability,extend,extend,299,"We are 80% of the way there on CRAM support. Under the hood DeepVariant uses htslib to read it's reads datasets, which supports cram. But we need to extend the IO systems to pass in the reference genome to read a CRAM file, and that's not possible right now. We are hopeful that the community would extend DeepVariant to handle CRAM files, so please feel free to contribute a pull request with the functionality if you decide to add it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38#issuecomment-357025416
https://github.com/google/deepvariant/issues/38#issuecomment-372143466:819,Availability,down,downloading,819,"Samtools use this approach to pass the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**; One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta; export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s; export REF_CACHE=/some_dir/cache/%2s/%2s/%s; REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38#issuecomment-372143466
https://github.com/google/deepvariant/issues/38#issuecomment-372143466:1836,Availability,down,downloaded,1836," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**; One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta; export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s; export REF_CACHE=/some_dir/cache/%2s/%2s/%s; REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38#issuecomment-372143466
https://github.com/google/deepvariant/issues/38#issuecomment-372143466:1939,Availability,down,downloads,1939," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**; One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta; export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s; export REF_CACHE=/some_dir/cache/%2s/%2s/%s; REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38#issuecomment-372143466
https://github.com/google/deepvariant/issues/38#issuecomment-372143466:582,Modifiability,variab,variable,582,"Samtools use this approach to pass the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**; One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta; export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s; export REF_CACHE=/some_dir/cache/%2s/%2s/%s; REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38#issuecomment-372143466
https://github.com/google/deepvariant/issues/38#issuecomment-372143466:1798,Modifiability,variab,variable,1798," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**; One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta; export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s; export REF_CACHE=/some_dir/cache/%2s/%2s/%s; REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38#issuecomment-372143466
https://github.com/google/deepvariant/issues/38#issuecomment-372143466:894,Performance,cache,cache,894,"Samtools use this approach to pass the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**; One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta; export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s; export REF_CACHE=/some_dir/cache/%2s/%2s/%s; REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38#issuecomment-372143466
https://github.com/google/deepvariant/issues/38#issuecomment-372143466:1125,Performance,cache,cache,1125," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**; One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta; export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s; export REF_CACHE=/some_dir/cache/%2s/%2s/%s; REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38#issuecomment-372143466
https://github.com/google/deepvariant/issues/38#issuecomment-372143466:1170,Performance,cache,cache,1170," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**; One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta; export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s; export REF_CACHE=/some_dir/cache/%2s/%2s/%s; REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38#issuecomment-372143466
https://github.com/google/deepvariant/issues/38#issuecomment-372143466:1252,Performance,cache,cache,1252," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**; One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta; export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s; export REF_CACHE=/some_dir/cache/%2s/%2s/%s; REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38#issuecomment-372143466
https://github.com/google/deepvariant/issues/38#issuecomment-372143466:1497,Performance,cache,cached,1497," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**; One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta; export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s; export REF_CACHE=/some_dir/cache/%2s/%2s/%s; REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38#issuecomment-372143466
https://github.com/google/deepvariant/issues/38#issuecomment-372143466:1715,Safety,avoid,avoid,1715," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**; One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta; export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s; export REF_CACHE=/some_dir/cache/%2s/%2s/%s; REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38#issuecomment-372143466
https://github.com/google/deepvariant/issues/38#issuecomment-372143466:1922,Safety,avoid,avoid,1922," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**; One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta; export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s; export REF_CACHE=/some_dir/cache/%2s/%2s/%s; REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/38#issuecomment-372143466
https://github.com/google/deepvariant/issues/39#issuecomment-358230941:135,Availability,error,error,135,No luck either. I figured it seems that the Xeon CPU on my machine does not support AVX (according to /proc/cpuinfo). Some informative error message would be nice in such a situation. Couldn't the docker container be built without assuming avx presence? It would be slower I guess but easier to explore the tool.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39#issuecomment-358230941
https://github.com/google/deepvariant/issues/39#issuecomment-358230941:141,Integrability,message,message,141,No luck either. I figured it seems that the Xeon CPU on my machine does not support AVX (according to /proc/cpuinfo). Some informative error message would be nice in such a situation. Couldn't the docker container be built without assuming avx presence? It would be slower I guess but easier to explore the tool.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39#issuecomment-358230941
https://github.com/google/deepvariant/issues/39#issuecomment-358337849:34,Availability,error,error,34,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39#issuecomment-358337849
https://github.com/google/deepvariant/issues/39#issuecomment-358337849:40,Integrability,message,message,40,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39#issuecomment-358337849
https://github.com/google/deepvariant/issues/39#issuecomment-358337849:249,Performance,optimiz,optimization,249,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39#issuecomment-358337849
https://github.com/google/deepvariant/issues/39#issuecomment-358337849:311,Performance,optimiz,optimized,311,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39#issuecomment-358337849
https://github.com/google/deepvariant/issues/39#issuecomment-358337849:360,Performance,optimiz,optimized-os,360,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/39#issuecomment-358337849
https://github.com/google/deepvariant/issues/40#issuecomment-358413895:219,Availability,avail,available,219,"Hi @avilella,. That's a great question. We did some limited experiments with DeepVariant on lower coverage samples, but not at the 2.5x-5x range directly (see attached image using an earlier version of DeepVariant than available on GitHub). . Typically at such a low depth you need to follow a joint calling strategy like the 1000 Genomes project in order to get accurate allele discovery across many samples. If you are trying to do single sample calling from low coverage, despite the relatively low quality of calls you'll get due to the low coverage, you can certainly use DeepVariant. You really don't need to do anything different than for a deep WGS sample, so just follow the case study example command lines. . There are some options in make_examples.py to manipulate the thresholds for generating candidate variant calls, but I'm not sure tweaking those will materially change the results. We'd be interested in hearing about your experiences with such low coverage samples if you do decide to try it out. ![screen shot 2018-01-17 at 11 14 01 am](https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40#issuecomment-358413895
https://github.com/google/deepvariant/issues/40#issuecomment-358416665:490,Availability,avail,available,490,"Thanks for the quick reply Mark. How about joint calling hundreds (100-200); of low coverage WGS? Would that make a difference to DeepVariant?. On Wed, Jan 17, 2018 at 7:29 PM, Mark DePristo <notifications@github.com>; wrote:. > Hi @avilella <https://github.com/avilella>,; >; > That's a great question. We done some limited experiments with DeepVariant; > on lower coverage samples, but not at the 2.5x-5x range directly (see; > attached image using an earlier version of DeepVariant than available on; > GitHub).; >; > Typically at such a low depth you need to follow a joint calling strategy; > like the 1000 Genomes project in order to get accurate allele discovery; > across many samples. If you are trying to do single sample calling from low; > coverage, despite the relatively low quality of calls you'll get due to the; > low coverage, you can certainly use DeepVariant. You really don't need to; > do anything different than for a deep WGS sample, so just follow the case; > study example command lines.; >; > There are some options in make_examples.py to manipulate the thresholds; > for generating candidate variant calls, but I'm not sure tweaking those; > will materially change the results. We'd be interested in hearing about; > your experiences with such low coverage samples if you do decide to try it; > out.; >; > [image: screen shot 2018-01-17 at 11 14 01 am]; > <https://user-images.githubusercontent.com/2250400/35062234-632d9068-fb78-11e7-84eb-66062ba79a43.png>; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/40#issuecomment-358413895>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAJpN-cW-EdQ649oZlMZPkDj6H6ILPLXks5tLkiqgaJpZM4Rh0y3>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/40#issuecomment-358416665
https://github.com/google/deepvariant/issues/41#issuecomment-360677087:104,Availability,error,error,104,"Hi,; can you tell us the operating system you're on? Did the step with run-prereq.sh finish without any error messages?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-360677087
https://github.com/google/deepvariant/issues/41#issuecomment-360677087:110,Integrability,message,messages,110,"Hi,; can you tell us the operating system you're on? Did the step with run-prereq.sh finish without any error messages?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-360677087
https://github.com/google/deepvariant/issues/41#issuecomment-360832980:140,Testability,test,test,140,I confirmed again that `cd bin; sudo ./run-prereq.sh` exits with `$?` set to 0. Then I did another attempt with `make-examples.zip` and the test data but the result is still the same ImportError.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-360832980
https://github.com/google/deepvariant/issues/41#issuecomment-360833579:86,Availability,error,error,86,"Interestingly, if I run `cd bin; ./run-prereq.sh` (without `sudo`) I get a permission error:; ```; ========== [Fri Jan 26 11:13:59 EST 2018] Stage 'Install TensorFlow pip package' starting ; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel ; Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl...; OSError: Operation not permitted. ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-360833579
https://github.com/google/deepvariant/issues/41#issuecomment-360833579:148,Deployability,Install,Install,148,"Interestingly, if I run `cd bin; ./run-prereq.sh` (without `sudo`) I get a permission error:; ```; ========== [Fri Jan 26 11:13:59 EST 2018] Stage 'Install TensorFlow pip package' starting ; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel ; Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl...; OSError: Operation not permitted. ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-360833579
https://github.com/google/deepvariant/issues/41#issuecomment-360833579:191,Deployability,Install,Installing,191,"Interestingly, if I run `cd bin; ./run-prereq.sh` (without `sudo`) I get a permission error:; ```; ========== [Fri Jan 26 11:13:59 EST 2018] Stage 'Install TensorFlow pip package' starting ; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel ; Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl...; OSError: Operation not permitted. ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-360833579
https://github.com/google/deepvariant/issues/41#issuecomment-360833579:224,Performance,optimiz,optimized,224,"Interestingly, if I run `cd bin; ./run-prereq.sh` (without `sudo`) I get a permission error:; ```; ========== [Fri Jan 26 11:13:59 EST 2018] Stage 'Install TensorFlow pip package' starting ; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel ; Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl...; OSError: Operation not permitted. ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-360833579
https://github.com/google/deepvariant/issues/41#issuecomment-360945398:271,Availability,error,error,271,"Hi Attila,. So your account would need `sudo` privileges, which you probably already figured out. The `run-prereq.sh` itself [contains calls](https://github.com/google/deepvariant/blob/r0.4/run-prereq.sh) using `sudo`, which is why you are getting the `not permitted` OS error. If you curious to see the trace of the script to determine where the issue might be arising from, just run it like this and that will provide you with an exact control-flow:. `sudo bash -x ./run-prereq.sh`. So these suggestions are just for curiosity purposes, and Pi-Chuan's recommendation is the correct one since it has been validated on Ubuntu 16.04. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-360945398
https://github.com/google/deepvariant/issues/41#issuecomment-360945398:606,Security,validat,validated,606,"Hi Attila,. So your account would need `sudo` privileges, which you probably already figured out. The `run-prereq.sh` itself [contains calls](https://github.com/google/deepvariant/blob/r0.4/run-prereq.sh) using `sudo`, which is why you are getting the `not permitted` OS error. If you curious to see the trace of the script to determine where the issue might be arising from, just run it like this and that will provide you with an exact control-flow:. `sudo bash -x ./run-prereq.sh`. So these suggestions are just for curiosity purposes, and Pi-Chuan's recommendation is the correct one since it has been validated on Ubuntu 16.04. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-360945398
https://github.com/google/deepvariant/issues/41#issuecomment-361015565:455,Deployability,install,installation,455,"Hi Paul,. thanks for the explanation. In the meantime I found out that Debian 9 stretch uses openssl v1.1 whereas Ubuntu 16.04 uses v1.0 and perhaps that's why I run into the issue. I symlinked `ln -s libcrypto.so.1.1.0 libcrypto.so.1.0.0` and this hack solved the issue. But then other issues emerged on Debian 9, which made me try to build everything from source. However, the large number of dependencies prevented me from succeeding. Anyhow, I'll try installation on Ubuntu 16.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-361015565
https://github.com/google/deepvariant/issues/41#issuecomment-361015565:395,Integrability,depend,dependencies,395,"Hi Paul,. thanks for the explanation. In the meantime I found out that Debian 9 stretch uses openssl v1.1 whereas Ubuntu 16.04 uses v1.0 and perhaps that's why I run into the issue. I symlinked `ln -s libcrypto.so.1.1.0 libcrypto.so.1.0.0` and this hack solved the issue. But then other issues emerged on Debian 9, which made me try to build everything from source. However, the large number of dependencies prevented me from succeeding. Anyhow, I'll try installation on Ubuntu 16.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-361015565
https://github.com/google/deepvariant/issues/41#issuecomment-361132599:68,Integrability,depend,dependencies,68,"Hi Attila,. Yeah it takes about a week to fully uncover how all the dependencies work together, which can be fun if you have the time. The side-benefit is that then you'll discover that DeepVariant is just a basic collections of tools (and data-structures) with many possibilities to tweak, simplify and expand on - which can be even more fun to play with :). ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-361132599
https://github.com/google/deepvariant/issues/41#issuecomment-361132599:291,Usability,simpl,simplify,291,"Hi Attila,. Yeah it takes about a week to fully uncover how all the dependencies work together, which can be fun if you have the time. The side-benefit is that then you'll discover that DeepVariant is just a basic collections of tools (and data-structures) with many possibilities to tweak, simplify and expand on - which can be even more fun to play with :). ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-361132599
https://github.com/google/deepvariant/issues/41#issuecomment-361436703:113,Integrability,depend,dependencies,113,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-361436703
https://github.com/google/deepvariant/issues/41#issuecomment-361436703:320,Performance,optimiz,optimizations,320,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-361436703
https://github.com/google/deepvariant/issues/41#issuecomment-361436703:299,Usability,simpl,simplifications,299,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-361436703
https://github.com/google/deepvariant/issues/41#issuecomment-361455478:326,Usability,simpl,simple,326,"Hi Mark (@depristo),. There is a theme of elegance with the current implementation that I tend to appreciate, though I agree that streamlining it for support/growth is rich with opportunities to explore. Let me know if you would like to work on it together - or just bounce off ideas - as some could be low-hanging fruit with simple remedies. Best,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-361455478
https://github.com/google/deepvariant/issues/41#issuecomment-466661657:0,Availability,down,downloading,0,downloading http://security.debian.org/debian-security/pool/updates/main/o/openssl/libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb followed by; ```; sudo dpkg --install libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb; ```; seems to work on WSL debian 9,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-466661657
https://github.com/google/deepvariant/issues/41#issuecomment-466661657:60,Deployability,update,updates,60,downloading http://security.debian.org/debian-security/pool/updates/main/o/openssl/libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb followed by; ```; sudo dpkg --install libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb; ```; seems to work on WSL debian 9,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-466661657
https://github.com/google/deepvariant/issues/41#issuecomment-466661657:152,Deployability,install,install,152,downloading http://security.debian.org/debian-security/pool/updates/main/o/openssl/libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb followed by; ```; sudo dpkg --install libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb; ```; seems to work on WSL debian 9,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-466661657
https://github.com/google/deepvariant/issues/41#issuecomment-466661657:19,Security,secur,security,19,downloading http://security.debian.org/debian-security/pool/updates/main/o/openssl/libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb followed by; ```; sudo dpkg --install libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb; ```; seems to work on WSL debian 9,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-466661657
https://github.com/google/deepvariant/issues/41#issuecomment-466661657:46,Security,secur,security,46,downloading http://security.debian.org/debian-security/pool/updates/main/o/openssl/libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb followed by; ```; sudo dpkg --install libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb; ```; seems to work on WSL debian 9,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/41#issuecomment-466661657
https://github.com/google/deepvariant/issues/42#issuecomment-360510853:183,Deployability,configurat,configuration,183,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```; config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \; allow_soft_placement=True, device_count = {'CPU': 1}); session = tf.Session(config=config); ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42#issuecomment-360510853
https://github.com/google/deepvariant/issues/42#issuecomment-360510853:183,Modifiability,config,configuration,183,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```; config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \; allow_soft_placement=True, device_count = {'CPU': 1}); session = tf.Session(config=config); ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42#issuecomment-360510853
https://github.com/google/deepvariant/issues/42#issuecomment-360510853:281,Modifiability,config,config,281,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```; config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \; allow_soft_placement=True, device_count = {'CPU': 1}); session = tf.Session(config=config); ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42#issuecomment-360510853
https://github.com/google/deepvariant/issues/42#issuecomment-360510853:293,Modifiability,Config,ConfigProto,293,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```; config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \; allow_soft_placement=True, device_count = {'CPU': 1}); session = tf.Session(config=config); ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42#issuecomment-360510853
https://github.com/google/deepvariant/issues/42#issuecomment-360510853:448,Modifiability,config,config,448,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```; config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \; allow_soft_placement=True, device_count = {'CPU': 1}); session = tf.Session(config=config); ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42#issuecomment-360510853
https://github.com/google/deepvariant/issues/42#issuecomment-360510853:455,Modifiability,config,config,455,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```; config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \; allow_soft_placement=True, device_count = {'CPU': 1}); session = tf.Session(config=config); ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42#issuecomment-360510853
https://github.com/google/deepvariant/issues/42#issuecomment-360510853:155,Performance,perform,performed,155,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```; config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \; allow_soft_placement=True, device_count = {'CPU': 1}); session = tf.Session(config=config); ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42#issuecomment-360510853
https://github.com/google/deepvariant/issues/42#issuecomment-360789056:26,Testability,test,testing,26,"Thank you! Although while testing I will continue to use Docker, changing the code might become important to me 👍",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/42#issuecomment-360789056
https://github.com/google/deepvariant/issues/43#issuecomment-360999837:16,Availability,error,error,16,"The command and error attached here in full, just in case that is needed to debug the problem.; [build_and_test_errors.txt](https://github.com/google/deepvariant/files/1670326/build_and_test_errors.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/43#issuecomment-360999837
https://github.com/google/deepvariant/issues/43#issuecomment-361020869:46,Availability,error,errors,46,"sorry my solution only fixed the first set of errors, some others persist and I am not sure how to resolve them. maybe these message help the cognoscenti figure out what is wrong, system is ubuntu 16.04, python2.7. [build_and_test_errors_2.txt](https://github.com/google/deepvariant/files/1670605/build_and_test_errors_2.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/43#issuecomment-361020869
https://github.com/google/deepvariant/issues/43#issuecomment-361020869:125,Integrability,message,message,125,"sorry my solution only fixed the first set of errors, some others persist and I am not sure how to resolve them. maybe these message help the cognoscenti figure out what is wrong, system is ubuntu 16.04, python2.7. [build_and_test_errors_2.txt](https://github.com/google/deepvariant/files/1670605/build_and_test_errors_2.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/43#issuecomment-361020869
https://github.com/google/deepvariant/issues/45#issuecomment-363913008:362,Performance,perform,performance,362,"At present we do not have a specific recommendation for joint genotyping DeepVariant gVCFs. Given the accurate genotype likelihood calibration of single-sample DeepVariant calls it may be better to simply merge calls without computing genotype posteriors based on population allelic frequencies and then altering the genotypes. We are actively investigating the performance of different methods to be able to provide a set of best practices. That said, the gVCF outputs of DeepVariant are syntactically and semantically equivalent to those produced by other tools like GATK, so can be used by any existing joint genotyping tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45#issuecomment-363913008
https://github.com/google/deepvariant/issues/45#issuecomment-363913008:198,Usability,simpl,simply,198,"At present we do not have a specific recommendation for joint genotyping DeepVariant gVCFs. Given the accurate genotype likelihood calibration of single-sample DeepVariant calls it may be better to simply merge calls without computing genotype posteriors based on population allelic frequencies and then altering the genotypes. We are actively investigating the performance of different methods to be able to provide a set of best practices. That said, the gVCF outputs of DeepVariant are syntactically and semantically equivalent to those produced by other tools like GATK, so can be used by any existing joint genotyping tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45#issuecomment-363913008
https://github.com/google/deepvariant/issues/45#issuecomment-481414559:88,Availability,error,error,88,"Hi there!. I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"".; Do you have any other recommendation? Or did you tested your gvcf in GATK?. This is the error that I got using GATK:; A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45#issuecomment-481414559
https://github.com/google/deepvariant/issues/45#issuecomment-481414559:209,Availability,error,error,209,"Hi there!. I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"".; Do you have any other recommendation? Or did you tested your gvcf in GATK?. This is the error that I got using GATK:; A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45#issuecomment-481414559
https://github.com/google/deepvariant/issues/45#issuecomment-481414559:246,Availability,ERROR,ERROR,246,"Hi there!. I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"".; Do you have any other recommendation? Or did you tested your gvcf in GATK?. This is the error that I got using GATK:; A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45#issuecomment-481414559
https://github.com/google/deepvariant/issues/45#issuecomment-481414559:170,Testability,test,tested,170,"Hi there!. I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"".; Do you have any other recommendation? Or did you tested your gvcf in GATK?. This is the error that I got using GATK:; A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45#issuecomment-481414559
https://github.com/google/deepvariant/issues/45#issuecomment-482355184:680,Deployability,update,update,680,"Hi @jaqueytw . To combine DeepVariant gVCFs together, we currently recommend using [GLnexus](https://github.com/dnanexus-rnd/GLnexus) with the --config deepvariant flag. This should combine DeepVariant gVCFs in a valid manner without filtering or re-genotyping. We have used GLnexus in this way in some of our early cohort investigations (for example in [this blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) on a mosquito pedigree). We are currently in the process of more fully evaluating the optimal ways to use GLnexus to merge DeepVariant calls. As this functionality matures, we will update our recommendations. Finally, to clarify our recommendations regarding GATK-based methods to combine gVCFs. Although these may run, in these sense that the gVCF format used by DeepVariant can be successfully used, we do not currently recommend their use to combine DeepVariant gVCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45#issuecomment-482355184
https://github.com/google/deepvariant/issues/45#issuecomment-482355184:145,Modifiability,config,config,145,"Hi @jaqueytw . To combine DeepVariant gVCFs together, we currently recommend using [GLnexus](https://github.com/dnanexus-rnd/GLnexus) with the --config deepvariant flag. This should combine DeepVariant gVCFs in a valid manner without filtering or re-genotyping. We have used GLnexus in this way in some of our early cohort investigations (for example in [this blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) on a mosquito pedigree). We are currently in the process of more fully evaluating the optimal ways to use GLnexus to merge DeepVariant calls. As this functionality matures, we will update our recommendations. Finally, to clarify our recommendations regarding GATK-based methods to combine gVCFs. Although these may run, in these sense that the gVCF format used by DeepVariant can be successfully used, we do not currently recommend their use to combine DeepVariant gVCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45#issuecomment-482355184
https://github.com/google/deepvariant/issues/45#issuecomment-553659703:27,Deployability,update,update,27,"Hi @llllaaaa . I wanted to update this issue with recent developments for cohort merging. With the DeepVariant v0.9 release, we recommend [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md). We encourage you or other users interested in multi-sample calling to follow these recommendations to combine multiple DeepVariant gVCFs. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45#issuecomment-553659703
https://github.com/google/deepvariant/issues/45#issuecomment-553659703:116,Deployability,release,release,116,"Hi @llllaaaa . I wanted to update this issue with recent developments for cohort merging. With the DeepVariant v0.9 release, we recommend [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md). We encourage you or other users interested in multi-sample calling to follow these recommendations to combine multiple DeepVariant gVCFs. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/45#issuecomment-553659703
https://github.com/google/deepvariant/issues/46#issuecomment-363256889:492,Availability,mainten,maintenance-policy,492,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant; ./build-prereq.sh; ./build_release_binaries.sh; ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md); I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --confident_regions ""${TRUTH_BED}"" \; --truth_variants ""${TRUTH_VCF}"" \; --examples ""${EXAMPLES}"" \; --regions ""20:10,000,000-12,000,000"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1; ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:; ```; > cat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-363256889
https://github.com/google/deepvariant/issues/46#issuecomment-363256889:534,Availability,failure,failure,534,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant; ./build-prereq.sh; ./build_release_binaries.sh; ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md); I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --confident_regions ""${TRUTH_BED}"" \; --truth_variants ""${TRUTH_VCF}"" \; --examples ""${EXAMPLES}"" \; --regions ""20:10,000,000-12,000,000"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1; ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:; ```; > cat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-363256889
https://github.com/google/deepvariant/issues/46#issuecomment-363256889:551,Availability,down,downloaded,551,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant; ./build-prereq.sh; ./build_release_binaries.sh; ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md); I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --confident_regions ""${TRUTH_BED}"" \; --truth_variants ""${TRUTH_VCF}"" \; --examples ""${EXAMPLES}"" \; --regions ""20:10,000,000-12,000,000"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1; ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:; ```; > cat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-363256889
https://github.com/google/deepvariant/issues/46#issuecomment-363256889:810,Availability,down,downloaded,810,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant; ./build-prereq.sh; ./build_release_binaries.sh; ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md); I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --confident_regions ""${TRUTH_BED}"" \; --truth_variants ""${TRUTH_VCF}"" \; --examples ""${EXAMPLES}"" \; --regions ""20:10,000,000-12,000,000"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1; ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:; ```; > cat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-363256889
https://github.com/google/deepvariant/issues/46#issuecomment-363256889:1147,Energy Efficiency,adapt,adapted,1147,"you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant; ./build-prereq.sh; ./build_release_binaries.sh; ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md); I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --confident_regions ""${TRUTH_BED}"" \; --truth_variants ""${TRUTH_VCF}"" \; --examples ""${EXAMPLES}"" \; --regions ""20:10,000,000-12,000,000"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1; ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:; ```; > cat output/data.pbtxt ; name: ""my-training-dataset""; tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz""; num_examples: 150; ```. Then I la",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-363256889
https://github.com/google/deepvariant/issues/46#issuecomment-363256889:844,Modifiability,variab,variables,844,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant; ./build-prereq.sh; ./build_release_binaries.sh; ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md); I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --confident_regions ""${TRUTH_BED}"" \; --truth_variants ""${TRUTH_VCF}"" \; --examples ""${EXAMPLES}"" \; --regions ""20:10,000,000-12,000,000"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1; ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:; ```; > cat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-363256889
https://github.com/google/deepvariant/issues/46#issuecomment-363256889:1147,Modifiability,adapt,adapted,1147,"you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant; ./build-prereq.sh; ./build_release_binaries.sh; ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md); I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --confident_regions ""${TRUTH_BED}"" \; --truth_variants ""${TRUTH_VCF}"" \; --examples ""${EXAMPLES}"" \; --regions ""20:10,000,000-12,000,000"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1; ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:; ```; > cat output/data.pbtxt ; name: ""my-training-dataset""; tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz""; num_examples: 150; ```. Then I la",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-363256889
https://github.com/google/deepvariant/issues/46#issuecomment-363256889:1839,Modifiability,config,config,1839," type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant; ./build-prereq.sh; ./build_release_binaries.sh; ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md); I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --confident_regions ""${TRUTH_BED}"" \; --truth_variants ""${TRUTH_VCF}"" \; --examples ""${EXAMPLES}"" \; --regions ""20:10,000,000-12,000,000"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1; ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:; ```; > cat output/data.pbtxt ; name: ""my-training-dataset""; tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz""; num_examples: 150; ```. Then I launched a training job with model_train.zip:. ```; python ""${BIN_DIR}""/model_train.zip \; --dataset_config_pbtxt output/data.pbtxt \; --start_from_checkpoint """" \; --batch_size 16 \; --alsologtostderr; ```. This is quick overview, but definitely let me know if you have any questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-363256889
https://github.com/google/deepvariant/issues/46#issuecomment-363256889:622,Performance,optimiz,optimization,622,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant; ./build-prereq.sh; ./build_release_binaries.sh; ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md); I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --confident_regions ""${TRUTH_BED}"" \; --truth_variants ""${TRUTH_VCF}"" \; --examples ""${EXAMPLES}"" \; --regions ""20:10,000,000-12,000,000"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1; ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:; ```; > cat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-363256889
https://github.com/google/deepvariant/issues/46#issuecomment-363256889:1363,Testability,log,log,1363,"oject ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant; ./build-prereq.sh; ./build_release_binaries.sh; ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md); I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --confident_regions ""${TRUTH_BED}"" \; --truth_variants ""${TRUTH_VCF}"" \; --examples ""${EXAMPLES}"" \; --regions ""20:10,000,000-12,000,000"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1; ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:; ```; > cat output/data.pbtxt ; name: ""my-training-dataset""; tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz""; num_examples: 150; ```. Then I launched a training job with model_train.zip:. ```; python ""${BIN_DIR}""/model_train.zip \; --dataset_config_pbtxt output/data.pbtxt \; --start_from_checkpoint """" \; --batch_size 16",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-363256889
https://github.com/google/deepvariant/issues/46#issuecomment-363256889:1669,Testability,log,log,1669," type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant; ./build-prereq.sh; ./build_release_binaries.sh; ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md); I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --confident_regions ""${TRUTH_BED}"" \; --truth_variants ""${TRUTH_VCF}"" \; --examples ""${EXAMPLES}"" \; --regions ""20:10,000,000-12,000,000"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1; ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:; ```; > cat output/data.pbtxt ; name: ""my-training-dataset""; tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz""; num_examples: 150; ```. Then I launched a training job with model_train.zip:. ```; python ""${BIN_DIR}""/model_train.zip \; --dataset_config_pbtxt output/data.pbtxt \; --start_from_checkpoint """" \; --batch_size 16 \; --alsologtostderr; ```. This is quick overview, but definitely let me know if you have any questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-363256889
https://github.com/google/deepvariant/issues/46#issuecomment-364502198:63,Availability,checkpoint,checkpoint,63,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants; ```; root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log; WARNING: Logging before flag parsing goes to stderr.; I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib; 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt; 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants; model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint; [self.n_classes_model_variable]); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-364502198
https://github.com/google/deepvariant/issues/46#issuecomment-364502198:2199,Availability,Checkpoint,CheckpointReader,2199,"ompiled to use: AVX2 FMA; I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt; 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants; model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint; [self.n_classes_model_variable]); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_shapes; reader = tf.train.NewCheckpointReader(checkpoint_path); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 150, in NewCheckpointReader; return CheckpointReader(compat.as_bytes(filepattern), status); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__; c_api.TF_GetCode(self.status.status)); tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?; ; real 0m5.561s; user 0m6.116s; sys 0m0.810s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-364502198
https://github.com/google/deepvariant/issues/46#issuecomment-364502198:527,Security,access,access,527,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants; ```; root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log; WARNING: Logging before flag parsing goes to stderr.; I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib; 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt; 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants; model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint; [self.n_classes_model_variable]); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-364502198
https://github.com/google/deepvariant/issues/46#issuecomment-364502198:343,Testability,log,logs,343,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants; ```; root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log; WARNING: Logging before flag parsing goes to stderr.; I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib; 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt; 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants; model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint; [self.n_classes_model_variable]); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-364502198
https://github.com/google/deepvariant/issues/46#issuecomment-364502198:367,Testability,log,log,367,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants; ```; root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log; WARNING: Logging before flag parsing goes to stderr.; I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib; 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt; 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants; model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint; [self.n_classes_model_variable]); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-364502198
https://github.com/google/deepvariant/issues/46#issuecomment-364502198:381,Testability,Log,Logging,381,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants; ```; root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log; WARNING: Logging before flag parsing goes to stderr.; I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib; 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt; 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants; model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint; [self.n_classes_model_variable]); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-364502198
https://github.com/google/deepvariant/issues/46#issuecomment-364533837:282,Availability,down,down,282,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:; wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz; tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add; "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt; modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-364533837
https://github.com/google/deepvariant/issues/46#issuecomment-364533837:345,Availability,down,download,345,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:; wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz; tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add; "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt; modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-364533837
https://github.com/google/deepvariant/issues/46#issuecomment-364533837:369,Availability,checkpoint,checkpoint,369,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:; wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz; tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add; "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt; modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-364533837
https://github.com/google/deepvariant/issues/46#issuecomment-364533837:394,Availability,down,download,394,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:; wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz; tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add; "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt; modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-364533837
https://github.com/google/deepvariant/issues/46#issuecomment-364533837:673,Availability,checkpoint,checkpoint,673,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:; wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz; tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add; "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt; modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-364533837
https://github.com/google/deepvariant/issues/46#issuecomment-364533837:723,Availability,Checkpoint,Checkpoint,723,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:; wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz; tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add; "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt; modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-364533837
https://github.com/google/deepvariant/issues/46#issuecomment-364533837:895,Availability,checkpoint,checkpoint,895,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:; wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz; tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add; "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt; modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-364533837
https://github.com/google/deepvariant/issues/46#issuecomment-364533837:608,Integrability,message,messages,608,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:; wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz; tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add; "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt; modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-364533837
https://github.com/google/deepvariant/issues/46#issuecomment-364533837:843,Performance,load,loaded,843,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:; wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz; tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add; "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt; modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-364533837
https://github.com/google/deepvariant/issues/46#issuecomment-364533837:906,Performance,load,loading,906,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:; wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz; tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add; "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt; modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-364533837
https://github.com/google/deepvariant/issues/46#issuecomment-364535704:55,Availability,checkpoint,checkpoints,55,"It's also possible to start training from our released checkpoints, if you want, so you can either start from scratch, start from imagenet, or start from a DeepVariant released model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-364535704
https://github.com/google/deepvariant/issues/46#issuecomment-364535704:46,Deployability,release,released,46,"It's also possible to start training from our released checkpoints, if you want, so you can either start from scratch, start from imagenet, or start from a DeepVariant released model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-364535704
https://github.com/google/deepvariant/issues/46#issuecomment-364535704:168,Deployability,release,released,168,"It's also possible to start training from our released checkpoints, if you want, so you can either start from scratch, start from imagenet, or start from a DeepVariant released model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-364535704
https://github.com/google/deepvariant/issues/46#issuecomment-364545868:22,Availability,checkpoint,checkpoint,22,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-364545868
https://github.com/google/deepvariant/issues/46#issuecomment-364545868:123,Availability,checkpoint,checkpoint,123,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-364545868
https://github.com/google/deepvariant/issues/46#issuecomment-364545868:410,Availability,checkpoint,checkpoint,410,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-364545868
https://github.com/google/deepvariant/issues/46#issuecomment-364545868:10,Performance,load,loading,10,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-364545868
https://github.com/google/deepvariant/issues/46#issuecomment-364545868:499,Testability,log,log,499,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/46#issuecomment-364545868
https://github.com/google/deepvariant/issues/47#issuecomment-363221616:394,Testability,test,test,394,"I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh; ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --regions ""chr20:10,000,000-10,010,000"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-363221616
https://github.com/google/deepvariant/issues/47#issuecomment-363221616:427,Testability,test,testdata,427,"I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh; ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --regions ""chr20:10,000,000-10,010,000"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-363221616
https://github.com/google/deepvariant/issues/47#issuecomment-363230217:141,Availability,avail,available,141,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Monday, February 5, 2018 3:08 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh; ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip; --mode calling; --ref ""${REF}""; --reads ""${BAM}""; --regions ""chr20:10,000,000-10,010,000""; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-363230217
https://github.com/google/deepvariant/issues/47#issuecomment-363230217:1996,Availability,error,error-free,1996,"@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh; ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip; --mode calling; --ref ""${REF}""; --reads ""${BAM}""; --regions ""chr20:10,000,000-10,010,000""; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-363230217
https://github.com/google/deepvariant/issues/47#issuecomment-363230217:2174,Availability,error,errors,2174,"@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh; ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip; --mode calling; --ref ""${REF}""; --reads ""${BAM}""; --regions ""chr20:10,000,000-10,010,000""; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-363230217
https://github.com/google/deepvariant/issues/47#issuecomment-363230217:91,Deployability,install,installed,91,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Monday, February 5, 2018 3:08 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh; ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip; --mode calling; --ref ""${REF}""; --reads ""${BAM}""; --regions ""chr20:10,000,000-10,010,000""; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-363230217
https://github.com/google/deepvariant/issues/47#issuecomment-363230217:272,Deployability,install,install,272,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Monday, February 5, 2018 3:08 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh; ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip; --mode calling; --ref ""${REF}""; --reads ""${BAM}""; --regions ""chr20:10,000,000-10,010,000""; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-363230217
https://github.com/google/deepvariant/issues/47#issuecomment-363230217:162,Integrability,depend,depends,162,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Monday, February 5, 2018 3:08 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh; ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip; --mode calling; --ref ""${REF}""; --reads ""${BAM}""; --regions ""chr20:10,000,000-10,010,000""; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-363230217
https://github.com/google/deepvariant/issues/47#issuecomment-363230217:1622,Integrability,message,message,1622,"@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh; ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip; --mode calling; --ref ""${REF}""; --reads ""${BAM}""; --regions ""chr20:10,000,000-10,010,000""; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-363230217
https://github.com/google/deepvariant/issues/47#issuecomment-363230217:2218,Integrability,message,message,2218,"@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh; ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip; --mode calling; --ref ""${REF}""; --reads ""${BAM}""; --regions ""chr20:10,000,000-10,010,000""; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-363230217
https://github.com/google/deepvariant/issues/47#issuecomment-363230217:1639,Security,confidential,confidential,1639,"@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh; ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip; --mode calling; --ref ""${REF}""; --reads ""${BAM}""; --regions ""chr20:10,000,000-10,010,000""; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-363230217
https://github.com/google/deepvariant/issues/47#issuecomment-363230217:1985,Security,secur,secured,1985,"@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh; ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip; --mode calling; --ref ""${REF}""; --reads ""${BAM}""; --regions ""chr20:10,000,000-10,010,000""; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-363230217
https://github.com/google/deepvariant/issues/47#issuecomment-363230217:246,Testability,test,tests,246,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Monday, February 5, 2018 3:08 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh; ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip; --mode calling; --ref ""${REF}""; --reads ""${BAM}""; --regions ""chr20:10,000,000-10,010,000""; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-363230217
https://github.com/google/deepvariant/issues/47#issuecomment-363230217:611,Testability,test,test,611,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Monday, February 5, 2018 3:08 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh; ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip; --mode calling; --ref ""${REF}""; --reads ""${BAM}""; --regions ""chr20:10,000,000-10,010,000""; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-363230217
https://github.com/google/deepvariant/issues/47#issuecomment-363230217:1040,Testability,test,test,1040,".0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Monday, February 5, 2018 3:08 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh; ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip; --mode calling; --ref ""${REF}""; --reads ""${BAM}""; --regions ""chr20:10,000,000-10,010,000""; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-363230217
https://github.com/google/deepvariant/issues/47#issuecomment-363230217:1073,Testability,test,testdata,1073,".0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Monday, February 5, 2018 3:08 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh; ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip; --mode calling; --ref ""${REF}""; --reads ""${BAM}""; --regions ""chr20:10,000,000-10,010,000""; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-363230217
https://github.com/google/deepvariant/issues/47#issuecomment-363252951:101,Deployability,install,install,101,Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with ; sudo -H pip install --upgrade 'tensorflow-gpu==1.4',MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-363252951
https://github.com/google/deepvariant/issues/47#issuecomment-363252951:204,Deployability,install,install,204,Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with ; sudo -H pip install --upgrade 'tensorflow-gpu==1.4',MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-363252951
https://github.com/google/deepvariant/issues/47#issuecomment-363252951:214,Deployability,upgrade,upgrade,214,Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with ; sudo -H pip install --upgrade 'tensorflow-gpu==1.4',MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-363252951
https://github.com/google/deepvariant/issues/47#issuecomment-364172639:1596,Availability,error,error-free,1596,"CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)?. Thanks,; Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Monday, February 5, 2018 5:10 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with; sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-364172639
https://github.com/google/deepvariant/issues/47#issuecomment-364172639:1774,Availability,error,errors,1774,"CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)?. Thanks,; Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Monday, February 5, 2018 5:10 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with; sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-364172639
https://github.com/google/deepvariant/issues/47#issuecomment-364172639:769,Deployability,install,install,769,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)?. Thanks,; Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Monday, February 5, 2018 5:10 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with; sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-364172639
https://github.com/google/deepvariant/issues/47#issuecomment-364172639:871,Deployability,install,install,871,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)?. Thanks,; Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Monday, February 5, 2018 5:10 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with; sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-364172639
https://github.com/google/deepvariant/issues/47#issuecomment-364172639:881,Deployability,upgrade,upgrade,881,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)?. Thanks,; Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Monday, February 5, 2018 5:10 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with; sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-364172639
https://github.com/google/deepvariant/issues/47#issuecomment-364172639:1222,Integrability,message,message,1222,"CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)?. Thanks,; Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Monday, February 5, 2018 5:10 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with; sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-364172639
https://github.com/google/deepvariant/issues/47#issuecomment-364172639:1818,Integrability,message,message,1818,"CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)?. Thanks,; Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Monday, February 5, 2018 5:10 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with; sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-364172639
https://github.com/google/deepvariant/issues/47#issuecomment-364172639:1239,Security,confidential,confidential,1239,"CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)?. Thanks,; Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Monday, February 5, 2018 5:10 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with; sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-364172639
https://github.com/google/deepvariant/issues/47#issuecomment-364172639:1585,Security,secur,secured,1585,"CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)?. Thanks,; Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Monday, February 5, 2018 5:10 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with; sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-364172639
https://github.com/google/deepvariant/issues/47#issuecomment-364172639:633,Testability,test,test,633,"Hello Ryan,. I was able to build successfully using CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)?. Thanks,; Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Monday, February 5, 2018 5:10 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with; sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-364172639
https://github.com/google/deepvariant/issues/47#issuecomment-364524990:356,Safety,predict,predict,356,"Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling specifically.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-364524990
https://github.com/google/deepvariant/issues/47#issuecomment-364650047:1754,Availability,error,error-free,1754," Best,. Brad Thomas. ________________________________; From: Ryan Poplin <notifications@github.com>; Sent: Friday, February 9, 2018 12:54 PM; To: google/deepvariant; Cc: Brad Thomas; Author; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-364650047
https://github.com/google/deepvariant/issues/47#issuecomment-364650047:1932,Availability,error,errors,1932," Best,. Brad Thomas. ________________________________; From: Ryan Poplin <notifications@github.com>; Sent: Friday, February 9, 2018 12:54 PM; To: google/deepvariant; Cc: Brad Thomas; Author; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-364650047
https://github.com/google/deepvariant/issues/47#issuecomment-364650047:1380,Integrability,message,message,1380," Best,. Brad Thomas. ________________________________; From: Ryan Poplin <notifications@github.com>; Sent: Friday, February 9, 2018 12:54 PM; To: google/deepvariant; Cc: Brad Thomas; Author; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-364650047
https://github.com/google/deepvariant/issues/47#issuecomment-364650047:1976,Integrability,message,message,1976," Best,. Brad Thomas. ________________________________; From: Ryan Poplin <notifications@github.com>; Sent: Friday, February 9, 2018 12:54 PM; To: google/deepvariant; Cc: Brad Thomas; Author; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-364650047
https://github.com/google/deepvariant/issues/47#issuecomment-364650047:846,Safety,predict,predict,846,"Hello Ryan,. Thanks for the information. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar?. Best,. Brad Thomas. ________________________________; From: Ryan Poplin <notifications@github.com>; Sent: Friday, February 9, 2018 12:54 PM; To: google/deepvariant; Cc: Brad Thomas; Author; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-364650047
https://github.com/google/deepvariant/issues/47#issuecomment-364650047:1397,Security,confidential,confidential,1397," Best,. Brad Thomas. ________________________________; From: Ryan Poplin <notifications@github.com>; Sent: Friday, February 9, 2018 12:54 PM; To: google/deepvariant; Cc: Brad Thomas; Author; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-364650047
https://github.com/google/deepvariant/issues/47#issuecomment-364650047:1743,Security,secur,secured,1743," Best,. Brad Thomas. ________________________________; From: Ryan Poplin <notifications@github.com>; Sent: Friday, February 9, 2018 12:54 PM; To: google/deepvariant; Cc: Brad Thomas; Author; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-364650047
https://github.com/google/deepvariant/issues/47#issuecomment-364650047:455,Testability,test,test,455,"Hello Ryan,. Thanks for the information. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar?. Best,. Brad Thomas. ________________________________; From: Ryan Poplin <notifications@github.com>; Sent: Friday, February 9, 2018 12:54 PM; To: google/deepvariant; Cc: Brad Thomas; Author; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-364650047
https://github.com/google/deepvariant/issues/47#issuecomment-365324431:1540,Availability,error,error-free,1540,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Tuesday, February 13, 2018 10:28 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-365324431
https://github.com/google/deepvariant/issues/47#issuecomment-365324431:1718,Availability,error,errors,1718,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Tuesday, February 13, 2018 10:28 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-365324431
https://github.com/google/deepvariant/issues/47#issuecomment-365324431:1166,Integrability,message,message,1166,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Tuesday, February 13, 2018 10:28 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-365324431
https://github.com/google/deepvariant/issues/47#issuecomment-365324431:1762,Integrability,message,message,1762,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Tuesday, February 13, 2018 10:28 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-365324431
https://github.com/google/deepvariant/issues/47#issuecomment-365324431:1183,Security,confidential,confidential,1183,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Tuesday, February 13, 2018 10:28 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-365324431
https://github.com/google/deepvariant/issues/47#issuecomment-365324431:1529,Security,secur,secured,1529,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Tuesday, February 13, 2018 10:28 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-365324431
https://github.com/google/deepvariant/issues/47#issuecomment-365324431:432,Testability,test,test,432,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Tuesday, February 13, 2018 10:28 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/47#issuecomment-365324431
https://github.com/google/deepvariant/issues/49#issuecomment-366745899:158,Deployability,configurat,configuration,158,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49#issuecomment-366745899
https://github.com/google/deepvariant/issues/49#issuecomment-366745899:450,Energy Efficiency,schedul,scheduler,450,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49#issuecomment-366745899
https://github.com/google/deepvariant/issues/49#issuecomment-366745899:158,Modifiability,config,configuration,158,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49#issuecomment-366745899
https://github.com/google/deepvariant/issues/49#issuecomment-366745899:384,Modifiability,config,config,384,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49#issuecomment-366745899
https://github.com/google/deepvariant/issues/49#issuecomment-366745899:424,Modifiability,config,configure-the-default-cfs-scheduler,424,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49#issuecomment-366745899
https://github.com/google/deepvariant/issues/49#issuecomment-366745899:56,Usability,simpl,simplest,56,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49#issuecomment-366745899
https://github.com/google/deepvariant/issues/49#issuecomment-366748047:294,Deployability,configurat,configuration,294,"Is it possible to merge the tfrecords files though?. On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,; >; > Since your WDL workflow is using Docker, the simplest approach is to; > include a Docker-specific argument for --cpuset-cpus, or change the; > Session configuration which I've detailed at, the following location:; >; > #42 (comment); > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>; >; > For information regarding the --cpuset-cpus here's a reference:; >; > https://docs.docker.com/config/containers/resource_; > constraints/#configure-the-default-cfs-scheduler; >; > There are many ways to change DeepVariant, but I think this will will get; > you the quickest results for the issue you're facing.; >; > Hope it helps,; > Paul; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49#issuecomment-366748047
https://github.com/google/deepvariant/issues/49#issuecomment-366748047:622,Energy Efficiency,schedul,scheduler,622,"Is it possible to merge the tfrecords files though?. On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,; >; > Since your WDL workflow is using Docker, the simplest approach is to; > include a Docker-specific argument for --cpuset-cpus, or change the; > Session configuration which I've detailed at, the following location:; >; > #42 (comment); > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>; >; > For information regarding the --cpuset-cpus here's a reference:; >; > https://docs.docker.com/config/containers/resource_; > constraints/#configure-the-default-cfs-scheduler; >; > There are many ways to change DeepVariant, but I think this will will get; > you the quickest results for the issue you're facing.; >; > Hope it helps,; > Paul; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49#issuecomment-366748047
https://github.com/google/deepvariant/issues/49#issuecomment-366748047:294,Modifiability,config,configuration,294,"Is it possible to merge the tfrecords files though?. On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,; >; > Since your WDL workflow is using Docker, the simplest approach is to; > include a Docker-specific argument for --cpuset-cpus, or change the; > Session configuration which I've detailed at, the following location:; >; > #42 (comment); > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>; >; > For information regarding the --cpuset-cpus here's a reference:; >; > https://docs.docker.com/config/containers/resource_; > constraints/#configure-the-default-cfs-scheduler; >; > There are many ways to change DeepVariant, but I think this will will get; > you the quickest results for the issue you're facing.; >; > Hope it helps,; > Paul; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49#issuecomment-366748047
https://github.com/google/deepvariant/issues/49#issuecomment-366748047:552,Modifiability,config,config,552,"Is it possible to merge the tfrecords files though?. On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,; >; > Since your WDL workflow is using Docker, the simplest approach is to; > include a Docker-specific argument for --cpuset-cpus, or change the; > Session configuration which I've detailed at, the following location:; >; > #42 (comment); > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>; >; > For information regarding the --cpuset-cpus here's a reference:; >; > https://docs.docker.com/config/containers/resource_; > constraints/#configure-the-default-cfs-scheduler; >; > There are many ways to change DeepVariant, but I think this will will get; > you the quickest results for the issue you're facing.; >; > Hope it helps,; > Paul; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49#issuecomment-366748047
https://github.com/google/deepvariant/issues/49#issuecomment-366748047:596,Modifiability,config,configure-the-default-cfs-scheduler,596,"Is it possible to merge the tfrecords files though?. On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,; >; > Since your WDL workflow is using Docker, the simplest approach is to; > include a Docker-specific argument for --cpuset-cpus, or change the; > Session configuration which I've detailed at, the following location:; >; > #42 (comment); > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>; >; > For information regarding the --cpuset-cpus here's a reference:; >; > https://docs.docker.com/config/containers/resource_; > constraints/#configure-the-default-cfs-scheduler; >; > There are many ways to change DeepVariant, but I think this will will get; > you the quickest results for the issue you're facing.; >; > Hope it helps,; > Paul; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49#issuecomment-366748047
https://github.com/google/deepvariant/issues/49#issuecomment-366748047:188,Usability,simpl,simplest,188,"Is it possible to merge the tfrecords files though?. On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,; >; > Since your WDL workflow is using Docker, the simplest approach is to; > include a Docker-specific argument for --cpuset-cpus, or change the; > Session configuration which I've detailed at, the following location:; >; > #42 (comment); > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>; >; > For information regarding the --cpuset-cpus here's a reference:; >; > https://docs.docker.com/config/containers/resource_; > constraints/#configure-the-default-cfs-scheduler; >; > There are many ways to change DeepVariant, but I think this will will get; > you the quickest results for the issue you're facing.; >; > Hope it helps,; > Paul; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49#issuecomment-366748047
https://github.com/google/deepvariant/issues/49#issuecomment-366848143:22,Usability,simpl,simply,22,"If you're looking for simply merging the tfrecord files (without having to touch Python code) to one, you can actually just concatenate tfrecord files together.; Something like:; `cat shard.*.tfrecord > merged.tfrecord`. or you can also concatenate zipped tfrecord files:; `cat shard.*.tfrecord.gz > merged.tfrecord.gz`; will work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/49#issuecomment-366848143
https://github.com/google/deepvariant/issues/52#issuecomment-370805635:179,Usability,simpl,simple,179,Can you produce a small snippet of your BAM file and an associated command line that reproduces the issue and share it with us? We'd be happy to debug but it'd be great to have a simple example that causes the problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-370805635
https://github.com/google/deepvariant/issues/52#issuecomment-371096675:17,Availability,down,downloaded,17,"The Bam file was downloaded from. ```; https://www.encodeproject.org/files/ENCFF528VXT/@@download/ENCFF528VXT.bam; ```. Then, since it was missing the @RG line, I added it manually just to test using picard:; ```; java -jar /picard.jar AddOrReplaceReadGroups I=ENCFF528VXT.bam O=ENCFF528VXT.bam RGID=4 RGLB=lib1 RGPL=illumina RGPU=unit1 RGSM=20; ```; The output of runninng; ```; samtools view -H ENCFF528VXT.bam; ```; is the following :; ```; @HD VN:1.5 SO:coordinate; @SQ SN:chr1 LN:249250621; @SQ SN:chr2 LN:243199373; @SQ SN:chr3 LN:198022430; @SQ SN:chr4 LN:191154276; @SQ SN:chr5 LN:180915260; @SQ SN:chr6 LN:171115067; @SQ SN:chr7 LN:159138663; @SQ SN:chr8 LN:146364022; @SQ SN:chr9 LN:141213431; @SQ SN:chr10 LN:135534747; @SQ SN:chr11 LN:135006516; @SQ SN:chr12 LN:133851895; @SQ SN:chr13 LN:115169878; @SQ SN:chr14 LN:107349540; @SQ SN:chr15 LN:102531392; @SQ SN:chr16 LN:90354753; @SQ SN:chr17 LN:81195210; @SQ SN:chr18 LN:78077248; @SQ SN:chr19 LN:59128983; @SQ SN:chr20 LN:63025520; @SQ SN:chr21 LN:48129895; @SQ SN:chr22 LN:51304566; @SQ SN:chrX LN:155270560; @SQ SN:chrY LN:59373566; @SQ SN:chrM LN:16571; @RG ID:4 LB:lib1 PL:illumina SM:20 PU:unit1; @PG ID:bwa PN:bwa VN:0.7.10-r789 CL:/usr/local/bin/bwa0.7.10 sampe -P reference_files/male.hg19.fa.gz ENCFF182MTO.sai ENCFF949NMY.sai ENCFF182MTO.fastq.gz ENCFF949NMY.fastq.gz; @PG ID:MarkDuplicates PN:MarkDuplicates VN:1.92() CL:net.sf.picard.sam.MarkDuplicates INPUT=[ENCFF182MTOENCFF949NMY.raw.srt.filt.srt.bam] OUTPUT=ENCFF182MTOENCFF949NMY.raw.srt.dupmark.bam METRICS_FILE=ENCFF182MTOENCFF949NMY.raw.srt.dup.qc REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371096675
https://github.com/google/deepvariant/issues/52#issuecomment-371096675:89,Availability,down,download,89,"The Bam file was downloaded from. ```; https://www.encodeproject.org/files/ENCFF528VXT/@@download/ENCFF528VXT.bam; ```. Then, since it was missing the @RG line, I added it manually just to test using picard:; ```; java -jar /picard.jar AddOrReplaceReadGroups I=ENCFF528VXT.bam O=ENCFF528VXT.bam RGID=4 RGLB=lib1 RGPL=illumina RGPU=unit1 RGSM=20; ```; The output of runninng; ```; samtools view -H ENCFF528VXT.bam; ```; is the following :; ```; @HD VN:1.5 SO:coordinate; @SQ SN:chr1 LN:249250621; @SQ SN:chr2 LN:243199373; @SQ SN:chr3 LN:198022430; @SQ SN:chr4 LN:191154276; @SQ SN:chr5 LN:180915260; @SQ SN:chr6 LN:171115067; @SQ SN:chr7 LN:159138663; @SQ SN:chr8 LN:146364022; @SQ SN:chr9 LN:141213431; @SQ SN:chr10 LN:135534747; @SQ SN:chr11 LN:135006516; @SQ SN:chr12 LN:133851895; @SQ SN:chr13 LN:115169878; @SQ SN:chr14 LN:107349540; @SQ SN:chr15 LN:102531392; @SQ SN:chr16 LN:90354753; @SQ SN:chr17 LN:81195210; @SQ SN:chr18 LN:78077248; @SQ SN:chr19 LN:59128983; @SQ SN:chr20 LN:63025520; @SQ SN:chr21 LN:48129895; @SQ SN:chr22 LN:51304566; @SQ SN:chrX LN:155270560; @SQ SN:chrY LN:59373566; @SQ SN:chrM LN:16571; @RG ID:4 LB:lib1 PL:illumina SM:20 PU:unit1; @PG ID:bwa PN:bwa VN:0.7.10-r789 CL:/usr/local/bin/bwa0.7.10 sampe -P reference_files/male.hg19.fa.gz ENCFF182MTO.sai ENCFF949NMY.sai ENCFF182MTO.fastq.gz ENCFF949NMY.fastq.gz; @PG ID:MarkDuplicates PN:MarkDuplicates VN:1.92() CL:net.sf.picard.sam.MarkDuplicates INPUT=[ENCFF182MTOENCFF949NMY.raw.srt.filt.srt.bam] OUTPUT=ENCFF182MTOENCFF949NMY.raw.srt.dupmark.bam METRICS_FILE=ENCFF182MTOENCFF949NMY.raw.srt.dup.qc REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371096675
https://github.com/google/deepvariant/issues/52#issuecomment-371096675:2986,Availability,checkpoint,checkpoint,2986,"DS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false; ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : ; s3://dv-testfiles/hg19.fa; s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :; ```; samtools index ENCFF528VXT.bam; samtools faidx hg19.fa; bgzip -c -i hg19.fa > hg19.fa.gz; samtools faidx ""hg19.fa.gz""; ```. Then I ran in the docker container you provide :; ```; mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```; /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt; ```. and here the error output:; ```; WARNING: Logging before flag parsing goes to stderr.; W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants; examples_filename, example_format)); ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to reru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371096675
https://github.com/google/deepvariant/issues/52#issuecomment-371096675:3038,Availability,error,error,3038,"aded the modified file on this public s3 bucket so you can have a look on it directly from here : ; s3://dv-testfiles/hg19.fa; s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :; ```; samtools index ENCFF528VXT.bam; samtools faidx hg19.fa; bgzip -c -i hg19.fa > hg19.fa.gz; samtools faidx ""hg19.fa.gz""; ```. Then I ran in the docker container you provide :; ```; mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```; /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt; ```. and here the error output:; ```; WARNING: Logging before flag parsing goes to stderr.; W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants; examples_filename, example_format)); ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_examples to genenerate the examples again.; ```. Thanks a lot,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371096675
https://github.com/google/deepvariant/issues/52#issuecomment-371096675:3248,Security,access,accessible,3248," modified file on this public s3 bucket so you can have a look on it directly from here : ; s3://dv-testfiles/hg19.fa; s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :; ```; samtools index ENCFF528VXT.bam; samtools faidx hg19.fa; bgzip -c -i hg19.fa > hg19.fa.gz; samtools faidx ""hg19.fa.gz""; ```. Then I ran in the docker container you provide :; ```; mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```; /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt; ```. and here the error output:; ```; WARNING: Logging before flag parsing goes to stderr.; W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants; examples_filename, example_format)); ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_examples to genenerate the examples again.; ```. Thanks a lot, ; Luisa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371096675
https://github.com/google/deepvariant/issues/52#issuecomment-371096675:189,Testability,test,test,189,"The Bam file was downloaded from. ```; https://www.encodeproject.org/files/ENCFF528VXT/@@download/ENCFF528VXT.bam; ```. Then, since it was missing the @RG line, I added it manually just to test using picard:; ```; java -jar /picard.jar AddOrReplaceReadGroups I=ENCFF528VXT.bam O=ENCFF528VXT.bam RGID=4 RGLB=lib1 RGPL=illumina RGPU=unit1 RGSM=20; ```; The output of runninng; ```; samtools view -H ENCFF528VXT.bam; ```; is the following :; ```; @HD VN:1.5 SO:coordinate; @SQ SN:chr1 LN:249250621; @SQ SN:chr2 LN:243199373; @SQ SN:chr3 LN:198022430; @SQ SN:chr4 LN:191154276; @SQ SN:chr5 LN:180915260; @SQ SN:chr6 LN:171115067; @SQ SN:chr7 LN:159138663; @SQ SN:chr8 LN:146364022; @SQ SN:chr9 LN:141213431; @SQ SN:chr10 LN:135534747; @SQ SN:chr11 LN:135006516; @SQ SN:chr12 LN:133851895; @SQ SN:chr13 LN:115169878; @SQ SN:chr14 LN:107349540; @SQ SN:chr15 LN:102531392; @SQ SN:chr16 LN:90354753; @SQ SN:chr17 LN:81195210; @SQ SN:chr18 LN:78077248; @SQ SN:chr19 LN:59128983; @SQ SN:chr20 LN:63025520; @SQ SN:chr21 LN:48129895; @SQ SN:chr22 LN:51304566; @SQ SN:chrX LN:155270560; @SQ SN:chrY LN:59373566; @SQ SN:chrM LN:16571; @RG ID:4 LB:lib1 PL:illumina SM:20 PU:unit1; @PG ID:bwa PN:bwa VN:0.7.10-r789 CL:/usr/local/bin/bwa0.7.10 sampe -P reference_files/male.hg19.fa.gz ENCFF182MTO.sai ENCFF949NMY.sai ENCFF182MTO.fastq.gz ENCFF949NMY.fastq.gz; @PG ID:MarkDuplicates PN:MarkDuplicates VN:1.92() CL:net.sf.picard.sam.MarkDuplicates INPUT=[ENCFF182MTOENCFF949NMY.raw.srt.filt.srt.bam] OUTPUT=ENCFF182MTOENCFF949NMY.raw.srt.dupmark.bam METRICS_FILE=ENCFF182MTOENCFF949NMY.raw.srt.dup.qc REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371096675
https://github.com/google/deepvariant/issues/52#issuecomment-371096675:2175,Testability,test,testfiles,2175,"ID:4 LB:lib1 PL:illumina SM:20 PU:unit1; @PG ID:bwa PN:bwa VN:0.7.10-r789 CL:/usr/local/bin/bwa0.7.10 sampe -P reference_files/male.hg19.fa.gz ENCFF182MTO.sai ENCFF949NMY.sai ENCFF182MTO.fastq.gz ENCFF949NMY.fastq.gz; @PG ID:MarkDuplicates PN:MarkDuplicates VN:1.92() CL:net.sf.picard.sam.MarkDuplicates INPUT=[ENCFF182MTOENCFF949NMY.raw.srt.filt.srt.bam] OUTPUT=ENCFF182MTOENCFF949NMY.raw.srt.dupmark.bam METRICS_FILE=ENCFF182MTOENCFF949NMY.raw.srt.dup.qc REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false; ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : ; s3://dv-testfiles/hg19.fa; s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :; ```; samtools index ENCFF528VXT.bam; samtools faidx hg19.fa; bgzip -c -i hg19.fa > hg19.fa.gz; samtools faidx ""hg19.fa.gz""; ```. Then I ran in the docker container you provide :; ```; mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```; /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt; ```. and here the error output:; ```; WARNING: Logging before flag parsing goes to stderr.; W0307 09:54:5",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371096675
https://github.com/google/deepvariant/issues/52#issuecomment-371096675:2202,Testability,test,testfiles,2202,"local/bin/bwa0.7.10 sampe -P reference_files/male.hg19.fa.gz ENCFF182MTO.sai ENCFF949NMY.sai ENCFF182MTO.fastq.gz ENCFF949NMY.fastq.gz; @PG ID:MarkDuplicates PN:MarkDuplicates VN:1.92() CL:net.sf.picard.sam.MarkDuplicates INPUT=[ENCFF182MTOENCFF949NMY.raw.srt.filt.srt.bam] OUTPUT=ENCFF182MTOENCFF949NMY.raw.srt.dupmark.bam METRICS_FILE=ENCFF182MTOENCFF949NMY.raw.srt.dup.qc REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false; ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : ; s3://dv-testfiles/hg19.fa; s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :; ```; samtools index ENCFF528VXT.bam; samtools faidx hg19.fa; bgzip -c -i hg19.fa > hg19.fa.gz; samtools faidx ""hg19.fa.gz""; ```. Then I ran in the docker container you provide :; ```; mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```; /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt; ```. and here the error output:; ```; WARNING: Logging before flag parsing goes to stderr.; W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371096675
https://github.com/google/deepvariant/issues/52#issuecomment-371096675:3067,Testability,Log,Logging,3067,"aded the modified file on this public s3 bucket so you can have a look on it directly from here : ; s3://dv-testfiles/hg19.fa; s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :; ```; samtools index ENCFF528VXT.bam; samtools faidx hg19.fa; bgzip -c -i hg19.fa > hg19.fa.gz; samtools faidx ""hg19.fa.gz""; ```. Then I ran in the docker container you provide :; ```; mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```; /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt; ```. and here the error output:; ```; WARNING: Logging before flag parsing goes to stderr.; W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants; examples_filename, example_format)); ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_examples to genenerate the examples again.; ```. Thanks a lot,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371096675
https://github.com/google/deepvariant/issues/52#issuecomment-371184018:465,Availability,ERROR,ERROR,465,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; HTTP request sent, awaiting response... 403 Forbidden; 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371184018
https://github.com/google/deepvariant/issues/52#issuecomment-371184018:69,Testability,test,testfiles,69,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; HTTP request sent, awaiting response... 403 Forbidden; 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371184018
https://github.com/google/deepvariant/issues/52#issuecomment-371184018:147,Testability,test,testfiles,147,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; HTTP request sent, awaiting response... 403 Forbidden; 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371184018
https://github.com/google/deepvariant/issues/52#issuecomment-371184018:204,Testability,test,testfiles,204,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; HTTP request sent, awaiting response... 403 Forbidden; 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371184018
https://github.com/google/deepvariant/issues/52#issuecomment-371184018:235,Testability,test,testfiles,235,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; HTTP request sent, awaiting response... 403 Forbidden; 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371184018
https://github.com/google/deepvariant/issues/52#issuecomment-371184018:298,Testability,test,testfiles,298,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; HTTP request sent, awaiting response... 403 Forbidden; 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371184018
https://github.com/google/deepvariant/issues/52#issuecomment-371184018:329,Testability,test,testfiles,329,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; HTTP request sent, awaiting response... 403 Forbidden; 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371184018
https://github.com/google/deepvariant/issues/52#issuecomment-371186430:628,Availability,ERROR,ERROR,628,"Thank a lot for your answer. Now the are public for real!. Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:; >; > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 07:28:01-- http://dv-testfiles.s3.; > amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)...; > 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80...; > connected.; > HTTP request sent, awaiting response... 403 Forbidden; > 2018-03-07 07:28:01 ERROR 403: Forbidden.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371186430
https://github.com/google/deepvariant/issues/52#issuecomment-371186430:212,Testability,test,testfiles,212,"Thank a lot for your answer. Now the are public for real!. Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:; >; > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 07:28:01-- http://dv-testfiles.s3.; > amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)...; > 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80...; > connected.; > HTTP request sent, awaiting response... 403 Forbidden; > 2018-03-07 07:28:01 ERROR 403: Forbidden.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371186430
https://github.com/google/deepvariant/issues/52#issuecomment-371186430:292,Testability,test,testfiles,292,"Thank a lot for your answer. Now the are public for real!. Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:; >; > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 07:28:01-- http://dv-testfiles.s3.; > amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)...; > 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80...; > connected.; > HTTP request sent, awaiting response... 403 Forbidden; > 2018-03-07 07:28:01 ERROR 403: Forbidden.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371186430
https://github.com/google/deepvariant/issues/52#issuecomment-371186430:355,Testability,test,testfiles,355,"Thank a lot for your answer. Now the are public for real!. Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:; >; > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 07:28:01-- http://dv-testfiles.s3.; > amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)...; > 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80...; > connected.; > HTTP request sent, awaiting response... 403 Forbidden; > 2018-03-07 07:28:01 ERROR 403: Forbidden.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371186430
https://github.com/google/deepvariant/issues/52#issuecomment-371186430:386,Testability,test,testfiles,386,"Thank a lot for your answer. Now the are public for real!. Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:; >; > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 07:28:01-- http://dv-testfiles.s3.; > amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)...; > 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80...; > connected.; > HTTP request sent, awaiting response... 403 Forbidden; > 2018-03-07 07:28:01 ERROR 403: Forbidden.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371186430
https://github.com/google/deepvariant/issues/52#issuecomment-371186430:454,Testability,test,testfiles,454,"Thank a lot for your answer. Now the are public for real!. Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:; >; > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 07:28:01-- http://dv-testfiles.s3.; > amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)...; > 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80...; > connected.; > HTTP request sent, awaiting response... 403 Forbidden; > 2018-03-07 07:28:01 ERROR 403: Forbidden.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371186430
https://github.com/google/deepvariant/issues/52#issuecomment-371186430:485,Testability,test,testfiles,485,"Thank a lot for your answer. Now the are public for real!. Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:; >; > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 07:28:01-- http://dv-testfiles.s3.; > amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)...; > 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80...; > connected.; > HTTP request sent, awaiting response... 403 Forbidden; > 2018-03-07 07:28:01 ERROR 403: Forbidden.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371186430
https://github.com/google/deepvariant/issues/52#issuecomment-371238076:238,Deployability,upgrade,upgrade,238,"This looks like a duplicate of https://github.com/google/deepvariant/issues/27 -- some of your data shards are empty (no examples were created in the shard), which was not being handled properly and was fixed in DeepVariant 0.5.1. If you upgrade to use that release version does the problem remain?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371238076
https://github.com/google/deepvariant/issues/52#issuecomment-371238076:258,Deployability,release,release,258,"This looks like a duplicate of https://github.com/google/deepvariant/issues/27 -- some of your data shards are empty (no examples were created in the shard), which was not being handled properly and was fixed in DeepVariant 0.5.1. If you upgrade to use that release version does the problem remain?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371238076
https://github.com/google/deepvariant/issues/52#issuecomment-371288942:153,Availability,error,error,153,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:; ```; gcr.io/deepvariant-docker/deepvariant:0.5.1; ```. I get this error instead:; ```; WARNING: Logging before flag parsing goes to stderr.; W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants; predictions = model.create(images, 3, is_training=False)['Predictions']; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3; depth_multiplier=depth_multiplier); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base; net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args; return func(*args, **current_args); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution; input_rank); ValueError: ('Co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371288942
https://github.com/google/deepvariant/issues/52#issuecomment-371288942:2075,Availability,error,error,2075,"``; gcr.io/deepvariant-docker/deepvariant:0.5.1; ```. I get this error instead:; ```; WARNING: Logging before flag parsing goes to stderr.; W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants; predictions = model.create(images, 3, is_training=False)['Predictions']; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3; depth_multiplier=depth_multiplier); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base; net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args; return func(*args, **current_args); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution; input_rank); ValueError: ('Convolution not supported for input with rank', 1); ```; Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371288942
https://github.com/google/deepvariant/issues/52#issuecomment-371288942:1912,Modifiability,layers,layers,1912,"``; gcr.io/deepvariant-docker/deepvariant:0.5.1; ```. I get this error instead:; ```; WARNING: Logging before flag parsing goes to stderr.; W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants; predictions = model.create(images, 3, is_training=False)['Predictions']; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3; depth_multiplier=depth_multiplier); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base; net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args; return func(*args, **current_args); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution; input_rank); ValueError: ('Convolution not supported for input with rank', 1); ```; Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371288942
https://github.com/google/deepvariant/issues/52#issuecomment-371288942:1926,Modifiability,layers,layers,1926,"``; gcr.io/deepvariant-docker/deepvariant:0.5.1; ```. I get this error instead:; ```; WARNING: Logging before flag parsing goes to stderr.; W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants; predictions = model.create(images, 3, is_training=False)['Predictions']; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3; depth_multiplier=depth_multiplier); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base; net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args; return func(*args, **current_args); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution; input_rank); ValueError: ('Convolution not supported for input with rank', 1); ```; Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371288942
https://github.com/google/deepvariant/issues/52#issuecomment-371288942:1933,Modifiability,layers,layers,1933,"``; gcr.io/deepvariant-docker/deepvariant:0.5.1; ```. I get this error instead:; ```; WARNING: Logging before flag parsing goes to stderr.; W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants; predictions = model.create(images, 3, is_training=False)['Predictions']; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3; depth_multiplier=depth_multiplier); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base; net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args; return func(*args, **current_args); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution; input_rank); ValueError: ('Convolution not supported for input with rank', 1); ```; Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371288942
https://github.com/google/deepvariant/issues/52#issuecomment-371288942:1102,Safety,predict,predictions,1102,"``; gcr.io/deepvariant-docker/deepvariant:0.5.1; ```. I get this error instead:; ```; WARNING: Logging before flag parsing goes to stderr.; W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants; predictions = model.create(images, 3, is_training=False)['Predictions']; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3; depth_multiplier=depth_multiplier); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base; net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args; return func(*args, **current_args); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution; input_rank); ValueError: ('Convolution not supported for input with rank', 1); ```; Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371288942
https://github.com/google/deepvariant/issues/52#issuecomment-371288942:1160,Safety,Predict,Predictions,1160,"``; gcr.io/deepvariant-docker/deepvariant:0.5.1; ```. I get this error instead:; ```; WARNING: Logging before flag parsing goes to stderr.; W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants; predictions = model.create(images, 3, is_training=False)['Predictions']; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3; depth_multiplier=depth_multiplier); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base; net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args; return func(*args, **current_args); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution; input_rank); ValueError: ('Convolution not supported for input with rank', 1); ```; Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371288942
https://github.com/google/deepvariant/issues/52#issuecomment-371288942:364,Security,access,accessible,364,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:; ```; gcr.io/deepvariant-docker/deepvariant:0.5.1; ```. I get this error instead:; ```; WARNING: Logging before flag parsing goes to stderr.; W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants; predictions = model.create(images, 3, is_training=False)['Predictions']; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3; depth_multiplier=depth_multiplier); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base; net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args; return func(*args, **current_args); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution; input_rank); ValueError: ('Co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371288942
https://github.com/google/deepvariant/issues/52#issuecomment-371288942:183,Testability,Log,Logging,183,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:; ```; gcr.io/deepvariant-docker/deepvariant:0.5.1; ```. I get this error instead:; ```; WARNING: Logging before flag parsing goes to stderr.; W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants; predictions = model.create(images, 3, is_training=False)['Predictions']; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3; depth_multiplier=depth_multiplier); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base; net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args; return func(*args, **current_args); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution; input_rank); ValueError: ('Co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371288942
https://github.com/google/deepvariant/issues/52#issuecomment-371293506:1128,Availability,error,error,1128,"oticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```; paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; HTTP request sent, awaiting response... 200 OK; Length: 357342653 (341M) [binary/octet-stream]; Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; Traceback (most recent call last):; File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506
https://github.com/google/deepvariant/issues/52#issuecomment-371293506:2090,Performance,cache,cache,2090,"46 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; Traceback (most recent call last):; File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options; with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:; File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506
https://github.com/google/deepvariant/issues/52#issuecomment-371293506:2467,Performance,cache,cache,2467,"HONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; Traceback (most recent call last):; File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options; with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:; File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader; random_seed=random_seed)); ValueError: Not found: No index found for /home/paul/data/luisa/ENCFF528VXT.bam; paul@gubuntu:~/deepvariant/bazel-bin$; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506
https://github.com/google/deepvariant/issues/52#issuecomment-371293506:2736,Performance,cache,cache,2736,"HONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; Traceback (most recent call last):; File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options; with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:; File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader; random_seed=random_seed)); ValueError: Not found: No index found for /home/paul/data/luisa/ENCFF528VXT.bam; paul@gubuntu:~/deepvariant/bazel-bin$; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506
https://github.com/google/deepvariant/issues/52#issuecomment-371293506:3021,Performance,cache,cache,3021,"HONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; Traceback (most recent call last):; File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options; with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:; File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader; random_seed=random_seed)); ValueError: Not found: No index found for /home/paul/data/luisa/ENCFF528VXT.bam; paul@gubuntu:~/deepvariant/bazel-bin$; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506
https://github.com/google/deepvariant/issues/52#issuecomment-371293506:1752,Safety,Timeout,Timeout,1752,"m 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; Traceback (most recent call last):; File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506
https://github.com/google/deepvariant/issues/52#issuecomment-371293506:1939,Security,access,accessible,1939,"42653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; Traceback (most recent call last):; File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_op",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506
https://github.com/google/deepvariant/issues/52#issuecomment-371293506:70,Testability,test,test,70,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```; paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; HTTP request sent, awaiting response... 200 OK; Length: 357342653 (341M) [binary/octet-stream]; Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506
https://github.com/google/deepvariant/issues/52#issuecomment-371293506:311,Testability,test,testfiles,311,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```; paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; HTTP request sent, awaiting response... 200 OK; Length: 357342653 (341M) [binary/octet-stream]; Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506
https://github.com/google/deepvariant/issues/52#issuecomment-371293506:389,Testability,test,testfiles,389,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```; paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; HTTP request sent, awaiting response... 200 OK; Length: 357342653 (341M) [binary/octet-stream]; Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506
https://github.com/google/deepvariant/issues/52#issuecomment-371293506:446,Testability,test,testfiles,446,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```; paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; HTTP request sent, awaiting response... 200 OK; Length: 357342653 (341M) [binary/octet-stream]; Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506
https://github.com/google/deepvariant/issues/52#issuecomment-371293506:477,Testability,test,testfiles,477,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```; paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; HTTP request sent, awaiting response... 200 OK; Length: 357342653 (341M) [binary/octet-stream]; Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506
https://github.com/google/deepvariant/issues/52#issuecomment-371293506:540,Testability,test,testfiles,540,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```; paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; HTTP request sent, awaiting response... 200 OK; Length: 357342653 (341M) [binary/octet-stream]; Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506
https://github.com/google/deepvariant/issues/52#issuecomment-371293506:571,Testability,test,testfiles,571,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```; paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; HTTP request sent, awaiting response... 200 OK; Length: 357342653 (341M) [binary/octet-stream]; Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506
https://github.com/google/deepvariant/issues/52#issuecomment-371293506:1653,Testability,Log,Logging,1653," sent, awaiting response... 200 OK; Length: 357342653 (341M) [binary/octet-stream]; Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; Traceback (most recent call last):; File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 10",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506
https://github.com/google/deepvariant/issues/52#issuecomment-371293506:63,Usability,simpl,simple,63,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```; paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; HTTP request sent, awaiting response... 200 OK; Length: 357342653 (341M) [binary/octet-stream]; Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506
https://github.com/google/deepvariant/issues/52#issuecomment-371306075:274,Availability,error,error,274,"Indeed the file was truncated, sorry about that. I am still testing locally; with other even smaller files ( like : wget; http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam; which is public and smaller and not truncated ) and I get the same exact; error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test; > to see if there is an issue with Luisa's BAM file, and noticed that I; > cannot even create an index - which would naturally make even the; > prerequisite make_examples not complete properly:; >; > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; > HTTP request sent, awaiting response... 200 OK; > Length: 357342653 (341M) [binary/octet-stream]; > Saving to: âENCFF528VXT.bamâ; >; > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s; >; > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]; >; > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
https://github.com/google/deepvariant/issues/52#issuecomment-371306075:1589,Availability,error,error,1589,"e_examples not complete properly:; >; > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; > HTTP request sent, awaiting response... 200 OK; > Length: 357342653 (341M) [binary/octet-stream]; > Saving to: âENCFF528VXT.bamâ; >; > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s; >; > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]; >; > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; > WARNING: Logging before flag parsing goes to stderr.; > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > Traceback (most recent call last):; > File ""/home/paul/.cache/bazel/_bazel_p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
https://github.com/google/deepvariant/issues/52#issuecomment-371306075:2573,Performance,cache,cache,2573,"ndex: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; > WARNING: Logging before flag parsing goes to stderr.; > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > Traceback (most recent call last):; > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; > tf.app.run(); > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; > _sys.exit(main(_sys.argv[:1] + flags_passthrough)); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; > options = default_options(add_flags=True, flags_obj=FLAGS); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options; > with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:; > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
https://github.com/google/deepvariant/issues/52#issuecomment-371306075:2958,Performance,cache,cache,2958,"ta/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; > WARNING: Logging before flag parsing goes to stderr.; > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > Traceback (most recent call last):; > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; > tf.app.run(); > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; > _sys.exit(main(_sys.argv[:1] + flags_passthrough)); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; > options = default_options(add_flags=True, flags_obj=FLAGS); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options; > with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:; > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader; > random_seed=random_seed)); > ValueError: Not found: No index found for /home/paul/data/luisa/ENCFF528VXT.bam; > paul@gubuntu:~/deepvariant/bazel-bin$; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/go",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
https://github.com/google/deepvariant/issues/52#issuecomment-371306075:3231,Performance,cache,cache,3231,"92 client.py:1004] Timeout attempting to reach GCE metadata service.; > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > Traceback (most recent call last):; > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; > tf.app.run(); > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; > _sys.exit(main(_sys.argv[:1] + flags_passthrough)); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; > options = default_options(add_flags=True, flags_obj=FLAGS); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options; > with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:; > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader; > random_seed=random_seed)); > ValueError: Not found: No index found for /home/paul/data/luisa/ENCFF528VXT.bam; > paul@gubuntu:~/deepvariant/bazel-bin$; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/52#issuecomment-371293506>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AWD1QdO0gBW0VvSmC7tBatJdKNAzBhQlks5tcFLhgaJpZM4SejU_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
https://github.com/google/deepvariant/issues/52#issuecomment-371306075:3520,Performance,cache,cache,3520,"92 client.py:1004] Timeout attempting to reach GCE metadata service.; > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > Traceback (most recent call last):; > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; > tf.app.run(); > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; > _sys.exit(main(_sys.argv[:1] + flags_passthrough)); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; > options = default_options(add_flags=True, flags_obj=FLAGS); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options; > with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:; > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader; > random_seed=random_seed)); > ValueError: Not found: No index found for /home/paul/data/luisa/ENCFF528VXT.bam; > paul@gubuntu:~/deepvariant/bazel-bin$; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/52#issuecomment-371293506>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AWD1QdO0gBW0VvSmC7tBatJdKNAzBhQlks5tcFLhgaJpZM4SejU_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
https://github.com/google/deepvariant/issues/52#issuecomment-371306075:2227,Safety,Timeout,Timeout,2227,"====================================>] 340.79M 1.26MB/s in 5m 17s; >; > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]; >; > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; > WARNING: Logging before flag parsing goes to stderr.; > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > Traceback (most recent call last):; > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; > tf.app.run(); > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; > _sys.exit(main(_sys.argv[:1] + flags_passthrough)); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; > options = default_options(add_flags=True, flags_obj=FLAGS); > File ""/home/paul/.cache/bazel/_baz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
https://github.com/google/deepvariant/issues/52#issuecomment-371306075:2416,Security,access,accessible,2416,"l@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; > WARNING: Logging before flag parsing goes to stderr.; > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > Traceback (most recent call last):; > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; > tf.app.run(); > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; > _sys.exit(main(_sys.argv[:1] + flags_passthrough)); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; > options = default_options(add_flags=True, flags_obj=FLAGS); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
https://github.com/google/deepvariant/issues/52#issuecomment-371306075:60,Testability,test,testing,60,"Indeed the file was truncated, sorry about that. I am still testing locally; with other even smaller files ( like : wget; http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam; which is public and smaller and not truncated ) and I get the same exact; error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test; > to see if there is an issue with Luisa's BAM file, and noticed that I; > cannot even create an index - which would naturally make even the; > prerequisite make_examples not complete properly:; >; > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; > HTTP request sent, awaiting response... 200 OK; > Length: 357342653 (341M) [binary/octet-stream]; > Saving to: âENCFF528VXT.bamâ; >; > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s; >; > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]; >; > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
https://github.com/google/deepvariant/issues/52#issuecomment-371306075:132,Testability,test,testfiles,132,"Indeed the file was truncated, sorry about that. I am still testing locally; with other even smaller files ( like : wget; http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam; which is public and smaller and not truncated ) and I get the same exact; error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test; > to see if there is an issue with Luisa's BAM file, and noticed that I; > cannot even create an index - which would naturally make even the; > prerequisite make_examples not complete properly:; >; > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; > HTTP request sent, awaiting response... 200 OK; > Length: 357342653 (341M) [binary/octet-stream]; > Saving to: âENCFF528VXT.bamâ; >; > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s; >; > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]; >; > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
https://github.com/google/deepvariant/issues/52#issuecomment-371306075:426,Testability,test,test,426,"Indeed the file was truncated, sorry about that. I am still testing locally; with other even smaller files ( like : wget; http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam; which is public and smaller and not truncated ) and I get the same exact; error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test; > to see if there is an issue with Luisa's BAM file, and noticed that I; > cannot even create an index - which would naturally make even the; > prerequisite make_examples not complete properly:; >; > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; > HTTP request sent, awaiting response... 200 OK; > Length: 357342653 (341M) [binary/octet-stream]; > Saving to: âENCFF528VXT.bamâ; >; > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s; >; > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]; >; > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
https://github.com/google/deepvariant/issues/52#issuecomment-371306075:674,Testability,test,testfiles,674,"Indeed the file was truncated, sorry about that. I am still testing locally; with other even smaller files ( like : wget; http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam; which is public and smaller and not truncated ) and I get the same exact; error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test; > to see if there is an issue with Luisa's BAM file, and noticed that I; > cannot even create an index - which would naturally make even the; > prerequisite make_examples not complete properly:; >; > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; > HTTP request sent, awaiting response... 200 OK; > Length: 357342653 (341M) [binary/octet-stream]; > Saving to: âENCFF528VXT.bamâ; >; > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s; >; > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]; >; > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
https://github.com/google/deepvariant/issues/52#issuecomment-371306075:744,Testability,test,testfiles,744,"Indeed the file was truncated, sorry about that. I am still testing locally; with other even smaller files ( like : wget; http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam; which is public and smaller and not truncated ) and I get the same exact; error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test; > to see if there is an issue with Luisa's BAM file, and noticed that I; > cannot even create an index - which would naturally make even the; > prerequisite make_examples not complete properly:; >; > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; > HTTP request sent, awaiting response... 200 OK; > Length: 357342653 (341M) [binary/octet-stream]; > Saving to: âENCFF528VXT.bamâ; >; > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s; >; > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]; >; > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
https://github.com/google/deepvariant/issues/52#issuecomment-371306075:821,Testability,test,testfiles,821,"Indeed the file was truncated, sorry about that. I am still testing locally; with other even smaller files ( like : wget; http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam; which is public and smaller and not truncated ) and I get the same exact; error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test; > to see if there is an issue with Luisa's BAM file, and noticed that I; > cannot even create an index - which would naturally make even the; > prerequisite make_examples not complete properly:; >; > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; > HTTP request sent, awaiting response... 200 OK; > Length: 357342653 (341M) [binary/octet-stream]; > Saving to: âENCFF528VXT.bamâ; >; > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s; >; > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]; >; > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
https://github.com/google/deepvariant/issues/52#issuecomment-371306075:880,Testability,test,testfiles,880,"Indeed the file was truncated, sorry about that. I am still testing locally; with other even smaller files ( like : wget; http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam; which is public and smaller and not truncated ) and I get the same exact; error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test; > to see if there is an issue with Luisa's BAM file, and noticed that I; > cannot even create an index - which would naturally make even the; > prerequisite make_examples not complete properly:; >; > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; > HTTP request sent, awaiting response... 200 OK; > Length: 357342653 (341M) [binary/octet-stream]; > Saving to: âENCFF528VXT.bamâ; >; > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s; >; > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]; >; > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
https://github.com/google/deepvariant/issues/52#issuecomment-371306075:911,Testability,test,testfiles,911,"Indeed the file was truncated, sorry about that. I am still testing locally; with other even smaller files ( like : wget; http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam; which is public and smaller and not truncated ) and I get the same exact; error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test; > to see if there is an issue with Luisa's BAM file, and noticed that I; > cannot even create an index - which would naturally make even the; > prerequisite make_examples not complete properly:; >; > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; > HTTP request sent, awaiting response... 200 OK; > Length: 357342653 (341M) [binary/octet-stream]; > Saving to: âENCFF528VXT.bamâ; >; > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s; >; > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]; >; > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
https://github.com/google/deepvariant/issues/52#issuecomment-371306075:976,Testability,test,testfiles,976,"Indeed the file was truncated, sorry about that. I am still testing locally; with other even smaller files ( like : wget; http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam; which is public and smaller and not truncated ) and I get the same exact; error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test; > to see if there is an issue with Luisa's BAM file, and noticed that I; > cannot even create an index - which would naturally make even the; > prerequisite make_examples not complete properly:; >; > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; > HTTP request sent, awaiting response... 200 OK; > Length: 357342653 (341M) [binary/octet-stream]; > Saving to: âENCFF528VXT.bamâ; >; > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s; >; > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]; >; > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
https://github.com/google/deepvariant/issues/52#issuecomment-371306075:1007,Testability,test,testfiles,1007,"he file was truncated, sorry about that. I am still testing locally; with other even smaller files ( like : wget; http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam; which is public and smaller and not truncated ) and I get the same exact; error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test; > to see if there is an issue with Luisa's BAM file, and noticed that I; > cannot even create an index - which would naturally make even the; > prerequisite make_examples not complete properly:; >; > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; > HTTP request sent, awaiting response... 200 OK; > Length: 357342653 (341M) [binary/octet-stream]; > Saving to: âENCFF528VXT.bamâ; >; > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s; >; > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]; >; > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /h",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
https://github.com/google/deepvariant/issues/52#issuecomment-371306075:2126,Testability,Log,Logging,2126,"; > Length: 357342653 (341M) [binary/octet-stream]; > Saving to: âENCFF528VXT.bamâ; >; > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s; >; > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]; >; > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; > WARNING: Logging before flag parsing goes to stderr.; > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > Traceback (most recent call last):; > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; > tf.app.run(); > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; > _sys.exit(main(_sys.argv[:1] + flags_passthrough)); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
https://github.com/google/deepvariant/issues/52#issuecomment-371306075:419,Usability,simpl,simple,419,"Indeed the file was truncated, sorry about that. I am still testing locally; with other even smaller files ( like : wget; http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam; which is public and smaller and not truncated ) and I get the same exact; error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test; > to see if there is an issue with Luisa's BAM file, and noticed that I; > cannot even create an index - which would naturally make even the; > prerequisite make_examples not complete properly:; >; > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; > HTTP request sent, awaiting response... 200 OK; > Length: 357342653 (341M) [binary/octet-stream]; > Saving to: âENCFF528VXT.bamâ; >; > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s; >; > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]; >; > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075
https://github.com/google/deepvariant/pull/55#issuecomment-371302921:260,Testability,log,login,260,"We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s). If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)? If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.; In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed. <!-- need_author_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/55#issuecomment-371302921
https://github.com/google/deepvariant/pull/56#issuecomment-371308069:260,Testability,log,login,260,"We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s). If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)? If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.; In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed. <!-- need_author_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/56#issuecomment-371308069
https://github.com/google/deepvariant/pull/57#issuecomment-371614660:154,Availability,avail,available,154,"Hi Cory (@cmclean),. Thank you for the updated version, and when you get a chance could you please update the binaries as there is no `0.5.2` version yet available:. ```Bash; $ gsutil ls -l gs://deepvariant/binaries/DeepVariant/; gs://deepvariant/binaries/DeepVariant/0.4.0/; gs://deepvariant/binaries/DeepVariant/0.4.1/; gs://deepvariant/binaries/DeepVariant/0.5.0/; gs://deepvariant/binaries/DeepVariant/0.5.1/; $; ```. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57#issuecomment-371614660
https://github.com/google/deepvariant/pull/57#issuecomment-371614660:39,Deployability,update,updated,39,"Hi Cory (@cmclean),. Thank you for the updated version, and when you get a chance could you please update the binaries as there is no `0.5.2` version yet available:. ```Bash; $ gsutil ls -l gs://deepvariant/binaries/DeepVariant/; gs://deepvariant/binaries/DeepVariant/0.4.0/; gs://deepvariant/binaries/DeepVariant/0.4.1/; gs://deepvariant/binaries/DeepVariant/0.5.0/; gs://deepvariant/binaries/DeepVariant/0.5.1/; $; ```. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57#issuecomment-371614660
https://github.com/google/deepvariant/pull/57#issuecomment-371614660:99,Deployability,update,update,99,"Hi Cory (@cmclean),. Thank you for the updated version, and when you get a chance could you please update the binaries as there is no `0.5.2` version yet available:. ```Bash; $ gsutil ls -l gs://deepvariant/binaries/DeepVariant/; gs://deepvariant/binaries/DeepVariant/0.4.0/; gs://deepvariant/binaries/DeepVariant/0.4.1/; gs://deepvariant/binaries/DeepVariant/0.5.0/; gs://deepvariant/binaries/DeepVariant/0.5.1/; $; ```. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57#issuecomment-371614660
https://github.com/google/deepvariant/pull/57#issuecomment-371628818:40,Availability,avail,available,40,"Thanks Paul, the 0.5.2 binaries are now available at the above bucket. Note that we also create a 'deepvariant.zip' asset with each tagged release that includes the binaries and models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57#issuecomment-371628818
https://github.com/google/deepvariant/pull/57#issuecomment-371628818:139,Deployability,release,release,139,"Thanks Paul, the 0.5.2 binaries are now available at the above bucket. Note that we also create a 'deepvariant.zip' asset with each tagged release that includes the binaries and models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57#issuecomment-371628818
https://github.com/google/deepvariant/pull/57#issuecomment-371635934:447,Availability,down,download-the-deepvariant-binaries-and-install-prerequisites,447,"Very awesome - forgot about that - thanks Cory! Just realized that the docs point to the old version (i.e. [Exome CS prelims](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-exome-case-study.md#preliminaries), [Whole Genome prelims](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md#preliminaries), [Quick Start prereqs](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-quick-start.md#download-the-deepvariant-binaries-and-install-prerequisites), etc.) in case new users git-clone. thx,; `p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57#issuecomment-371635934
https://github.com/google/deepvariant/pull/57#issuecomment-371635934:485,Deployability,install,install-prerequisites,485,"Very awesome - forgot about that - thanks Cory! Just realized that the docs point to the old version (i.e. [Exome CS prelims](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-exome-case-study.md#preliminaries), [Whole Genome prelims](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md#preliminaries), [Quick Start prereqs](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-quick-start.md#download-the-deepvariant-binaries-and-install-prerequisites), etc.) in case new users git-clone. thx,; `p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/57#issuecomment-371635934
https://github.com/google/deepvariant/issues/60#issuecomment-375321687:4,Availability,error,error,4,The error suggests that you don't have enough CPU quota in the us-central region. You need at least 16 CPUs for the quickstart job. Please see https://cloud.google.com/compute/quotas for how to request more quota.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/60#issuecomment-375321687
https://github.com/google/deepvariant/issues/61#issuecomment-375468423:158,Testability,test,testdata,158,"Hi @aderzelle ,. Since you're using `zsh` as a shell just include single-quotes like this when you use wildcards:. `gsutil -m cp 'gs://deepvariant/quickstart-testdata/*' input/`. Or you could try `bash` as a shell :). Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/61#issuecomment-375468423
https://github.com/google/deepvariant/issues/62#issuecomment-379107194:23,Deployability,release,released,23,"Hi KBT59,; because the released models are trained with the default height of the pileup images, by just changing `--pileup_image_height` at inference time won't really give you better results. Currently DeepVariant is a germline variant caller, so it's not designed to call variants with 1% frequency.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-379107194
https://github.com/google/deepvariant/issues/62#issuecomment-379110341:371,Usability,simpl,simply,371,"Hi again,; I didn't read carefully so I missed that you said you want to __train__ a model.; If you want to get `make_examples` to create more candidates, the other flags you need to consider are: `vsc_min_count_snps`, `vsc_min_count_indels`, `vsc_min_fraction_snps`, `vsc_min_fraction_indels`. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-379110341
https://github.com/google/deepvariant/issues/62#issuecomment-379857500:1957,Availability,error,error-free,1957,"ad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-379857500
https://github.com/google/deepvariant/issues/62#issuecomment-379857500:2135,Availability,error,errors,2135,"ad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-379857500
https://github.com/google/deepvariant/issues/62#issuecomment-379857500:1583,Integrability,message,message,1583,"ad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-379857500
https://github.com/google/deepvariant/issues/62#issuecomment-379857500:2179,Integrability,message,message,2179,"ad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-379857500
https://github.com/google/deepvariant/issues/62#issuecomment-379857500:701,Safety,safe,safe,701,"Thanks, I’m giving that a try today. vsc_min_count_snps, vsc_min_count_indels are already small numbers (2), so I changed only the fraction flags from their defaults, 0.12, to 0.01 which the fraction I want. Is that reasonable?. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Thursday, April 5, 2018 6:56 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercept",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-379857500
https://github.com/google/deepvariant/issues/62#issuecomment-379857500:1600,Security,confidential,confidential,1600,"ad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-379857500
https://github.com/google/deepvariant/issues/62#issuecomment-379857500:1946,Security,secur,secured,1946,"ad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-379857500
https://github.com/google/deepvariant/issues/62#issuecomment-379857500:1064,Usability,simpl,simply,1064,"t_indels are already small numbers (2), so I changed only the fraction flags from their defaults, 0.12, to 0.01 which the fraction I want. Is that reasonable?. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Thursday, April 5, 2018 6:56 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-379857500
https://github.com/google/deepvariant/issues/62#issuecomment-380158183:358,Availability,error,error,358,"Hello,. With make_examples I believe I have made examples I can use in model_train. The 64 files are named like this: 5PRR-RD_S86.examples.tfrecord-00000-of-00064. My protobuffer file contains this:. name: ""my-training-dataset""; tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord""; num_examples: 64. When I run model_train I see this error:. ValueError: Cannot find matching files with the pattern ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". How should I specify the tfrecord_path to get model_train to use the files?. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Thursday, April 5, 2018 6:56 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual name",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380158183
https://github.com/google/deepvariant/issues/62#issuecomment-380158183:2289,Availability,error,error-free,2289,"ad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380158183
https://github.com/google/deepvariant/issues/62#issuecomment-380158183:2467,Availability,error,errors,2467,"ad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380158183
https://github.com/google/deepvariant/issues/62#issuecomment-380158183:1915,Integrability,message,message,1915,"ad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380158183
https://github.com/google/deepvariant/issues/62#issuecomment-380158183:2511,Integrability,message,message,2511,"ad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380158183
https://github.com/google/deepvariant/issues/62#issuecomment-380158183:1033,Safety,safe,safe,1033,"Hello,. With make_examples I believe I have made examples I can use in model_train. The 64 files are named like this: 5PRR-RD_S86.examples.tfrecord-00000-of-00064. My protobuffer file contains this:. name: ""my-training-dataset""; tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord""; num_examples: 64. When I run model_train I see this error:. ValueError: Cannot find matching files with the pattern ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". How should I specify the tfrecord_path to get model_train to use the files?. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Thursday, April 5, 2018 6:56 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual name",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380158183
https://github.com/google/deepvariant/issues/62#issuecomment-380158183:1932,Security,confidential,confidential,1932,"ad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380158183
https://github.com/google/deepvariant/issues/62#issuecomment-380158183:2278,Security,secur,secured,2278,"ad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380158183
https://github.com/google/deepvariant/issues/62#issuecomment-380158183:1396,Usability,simpl,simply,1396,"les with the pattern ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". How should I specify the tfrecord_path to get model_train to use the files?. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Thursday, April 5, 2018 6:56 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380158183
https://github.com/google/deepvariant/issues/62#issuecomment-380197853:46,Availability,error,error,46,"OK – that proceeded further, I think. Now the error is; ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to?. Thanks,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Tuesday, April 10, 2018 1:04 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:; tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380197853
https://github.com/google/deepvariant/issues/62#issuecomment-380197853:1603,Availability,error,error-free,1603,"ueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to?. Thanks,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Tuesday, April 10, 2018 1:04 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:; tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380197853
https://github.com/google/deepvariant/issues/62#issuecomment-380197853:1781,Availability,error,errors,1781,"ueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to?. Thanks,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Tuesday, April 10, 2018 1:04 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:; tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380197853
https://github.com/google/deepvariant/issues/62#issuecomment-380197853:1229,Integrability,message,message,1229,"ueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to?. Thanks,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Tuesday, April 10, 2018 1:04 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:; tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380197853
https://github.com/google/deepvariant/issues/62#issuecomment-380197853:1825,Integrability,message,message,1825,"ueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to?. Thanks,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Tuesday, April 10, 2018 1:04 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:; tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380197853
https://github.com/google/deepvariant/issues/62#issuecomment-380197853:799,Safety,safe,safe,799,"OK – that proceeded further, I think. Now the error is; ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to?. Thanks,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Tuesday, April 10, 2018 1:04 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:; tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380197853
https://github.com/google/deepvariant/issues/62#issuecomment-380197853:1246,Security,confidential,confidential,1246,"ueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to?. Thanks,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Tuesday, April 10, 2018 1:04 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:; tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380197853
https://github.com/google/deepvariant/issues/62#issuecomment-380197853:1592,Security,secur,secured,1592,"ueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to?. Thanks,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Tuesday, April 10, 2018 1:04 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:; tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380197853
https://github.com/google/deepvariant/issues/62#issuecomment-380197853:143,Testability,Log,Logits,143,"OK – that proceeded further, I think. Now the error is; ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to?. Thanks,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Tuesday, April 10, 2018 1:04 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:; tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380197853
https://github.com/google/deepvariant/issues/62#issuecomment-380935943:31,Availability,error,error,31,"Hi,; From a quick look of your error, it doesn't look like anything I've ever; encountered before. If you could potentially set up a reproducible setting; that I can very quickly run, I can see if I can try it out and tell you; what might could have gone wrong. We don't currently have a tutorial for; training, unfortunately. And to be honest, even if we do, it probably; wouldn't specifically cover this error case. (from my phone). On Tue, Apr 10, 2018, 11:16 AM KBT59 <notifications@github.com> wrote:. > OK – that proceeded further, I think. Now the error is; > ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for; > 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes:; > [64,27,1,3]; >; >; > I hate to keep bothering people about this. Is there documentation on all; > of this that I can refer to?; >; >; > Thanks,; > Brad Thomas; >; >; > From: Pi-Chuan Chang [mailto:notifications@github.com]; > Sent: Tuesday, April 10, 2018 1:04 PM; > To: google/deepvariant <deepvariant@noreply.github.com>; > Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <; > author@noreply.github.com>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380935943
https://github.com/google/deepvariant/issues/62#issuecomment-380935943:406,Availability,error,error,406,"Hi,; From a quick look of your error, it doesn't look like anything I've ever; encountered before. If you could potentially set up a reproducible setting; that I can very quickly run, I can see if I can try it out and tell you; what might could have gone wrong. We don't currently have a tutorial for; training, unfortunately. And to be honest, even if we do, it probably; wouldn't specifically cover this error case. (from my phone). On Tue, Apr 10, 2018, 11:16 AM KBT59 <notifications@github.com> wrote:. > OK – that proceeded further, I think. Now the error is; > ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for; > 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes:; > [64,27,1,3]; >; >; > I hate to keep bothering people about this. Is there documentation on all; > of this that I can refer to?; >; >; > Thanks,; > Brad Thomas; >; >; > From: Pi-Chuan Chang [mailto:notifications@github.com]; > Sent: Tuesday, April 10, 2018 1:04 PM; > To: google/deepvariant <deepvariant@noreply.github.com>; > Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <; > author@noreply.github.com>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380935943
https://github.com/google/deepvariant/issues/62#issuecomment-380935943:555,Availability,error,error,555,"Hi,; From a quick look of your error, it doesn't look like anything I've ever; encountered before. If you could potentially set up a reproducible setting; that I can very quickly run, I can see if I can try it out and tell you; what might could have gone wrong. We don't currently have a tutorial for; training, unfortunately. And to be honest, even if we do, it probably; wouldn't specifically cover this error case. (from my phone). On Tue, Apr 10, 2018, 11:16 AM KBT59 <notifications@github.com> wrote:. > OK – that proceeded further, I think. Now the error is; > ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for; > 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes:; > [64,27,1,3]; >; >; > I hate to keep bothering people about this. Is there documentation on all; > of this that I can refer to?; >; >; > Thanks,; > Brad Thomas; >; >; > From: Pi-Chuan Chang [mailto:notifications@github.com]; > Sent: Tuesday, April 10, 2018 1:04 PM; > To: google/deepvariant <deepvariant@noreply.github.com>; > Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <; > author@noreply.github.com>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380935943
https://github.com/google/deepvariant/issues/62#issuecomment-380935943:2227,Availability,error,error-free,2227,"eply.github.com>; > Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <; > author@noreply.github.com>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distribute or copy this e-mail. Please notify the sender; > immediately by e-mail if you have received this e-mail by mistake and; > delete this e-mail from your system. E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-380197853>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAczBalfAA5qelNx5damo_mTuPg7r4UJks5tnPbigaJpZM4TIm9R>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380935943
https://github.com/google/deepvariant/issues/62#issuecomment-380935943:2411,Availability,error,errors,2411,"eply.github.com>; > Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <; > author@noreply.github.com>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distribute or copy this e-mail. Please notify the sender; > immediately by e-mail if you have received this e-mail by mistake and; > delete this e-mail from your system. E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-380197853>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAczBalfAA5qelNx5damo_mTuPg7r4UJks5tnPbigaJpZM4TIm9R>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380935943
https://github.com/google/deepvariant/issues/62#issuecomment-380935943:1838,Integrability,message,message,1838,"; >; >; > From: Pi-Chuan Chang [mailto:notifications@github.com]; > Sent: Tuesday, April 10, 2018 1:04 PM; > To: google/deepvariant <deepvariant@noreply.github.com>; > Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <; > author@noreply.github.com>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distribute or copy this e-mail. Please notify the sender; > immediately by e-mail if you have received this e-mail by mistake and; > delete this e-mail from your system. E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380935943
https://github.com/google/deepvariant/issues/62#issuecomment-380935943:2458,Integrability,message,message,2458,"eply.github.com>; > Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <; > author@noreply.github.com>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distribute or copy this e-mail. Please notify the sender; > immediately by e-mail if you have received this e-mail by mistake and; > delete this e-mail from your system. E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-380197853>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAczBalfAA5qelNx5damo_mTuPg7r4UJks5tnPbigaJpZM4TIm9R>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380935943
https://github.com/google/deepvariant/issues/62#issuecomment-380935943:1368,Safety,safe,safe,1368,"nately. And to be honest, even if we do, it probably; wouldn't specifically cover this error case. (from my phone). On Tue, Apr 10, 2018, 11:16 AM KBT59 <notifications@github.com> wrote:. > OK – that proceeded further, I think. Now the error is; > ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for; > 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes:; > [64,27,1,3]; >; >; > I hate to keep bothering people about this. Is there documentation on all; > of this that I can refer to?; >; >; > Thanks,; > Brad Thomas; >; >; > From: Pi-Chuan Chang [mailto:notifications@github.com]; > Sent: Tuesday, April 10, 2018 1:04 PM; > To: google/deepvariant <deepvariant@noreply.github.com>; > Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <; > author@noreply.github.com>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distribute or copy this e-mail. Please notify the sender; > immediately by e-mail if you have received this e-mail by mistake and; > delete this e-mail from your system. E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380935943
https://github.com/google/deepvariant/issues/62#issuecomment-380935943:1855,Security,confidential,confidential,1855,"; >; >; > From: Pi-Chuan Chang [mailto:notifications@github.com]; > Sent: Tuesday, April 10, 2018 1:04 PM; > To: google/deepvariant <deepvariant@noreply.github.com>; > Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <; > author@noreply.github.com>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distribute or copy this e-mail. Please notify the sender; > immediately by e-mail if you have received this e-mail by mistake and; > delete this e-mail from your system. E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380935943
https://github.com/google/deepvariant/issues/62#issuecomment-380935943:2216,Security,secur,secured,2216,"eply.github.com>; > Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <; > author@noreply.github.com>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distribute or copy this e-mail. Please notify the sender; > immediately by e-mail if you have received this e-mail by mistake and; > delete this e-mail from your system. E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-380197853>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAczBalfAA5qelNx5damo_mTuPg7r4UJks5tnPbigaJpZM4TIm9R>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380935943
https://github.com/google/deepvariant/issues/62#issuecomment-380935943:657,Testability,Log,Logits,657,"Hi,; From a quick look of your error, it doesn't look like anything I've ever; encountered before. If you could potentially set up a reproducible setting; that I can very quickly run, I can see if I can try it out and tell you; what might could have gone wrong. We don't currently have a tutorial for; training, unfortunately. And to be honest, even if we do, it probably; wouldn't specifically cover this error case. (from my phone). On Tue, Apr 10, 2018, 11:16 AM KBT59 <notifications@github.com> wrote:. > OK – that proceeded further, I think. Now the error is; > ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for; > 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes:; > [64,27,1,3]; >; >; > I hate to keep bothering people about this. Is there documentation on all; > of this that I can refer to?; >; >; > Thanks,; > Brad Thomas; >; >; > From: Pi-Chuan Chang [mailto:notifications@github.com]; > Sent: Tuesday, April 10, 2018 1:04 PM; > To: google/deepvariant <deepvariant@noreply.github.com>; > Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <; > author@noreply.github.com>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-380935943
https://github.com/google/deepvariant/issues/62#issuecomment-381164890:693,Availability,error,error,693,"Hello,. Unfortunately the data I’m using are restricted by Federal regulations and also are proprietary. Apart from sharing data, what can I provide that might help figure this out?. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Thursday, April 12, 2018 3:34 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; From a quick look of your error, it doesn't look like anything I've ever; encountered before. If you could potentially set up a reproducible setting; that I can very quickly run, I can see if I can try it out and tell you; what might could have gone wrong. We don't currently have a tutorial for; training, unfortunately. And to be honest, even if we do, it probably; wouldn't specifically cover this error case. (from my phone). On Tue, Apr 10, 2018, 11:16 AM KBT59 <notifications@github.com<mailto:notifications@github.com>> wrote:. > OK – that proceeded further, I think. Now the error is; > ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for; > 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes:; > [64,27,1,3]; >; >; > I hate to keep bothering people about this. Is there documentation on all; > of this that I can refer to?; >; >; > Thanks,; > Brad Thomas; >; >; > From: Pi-Chuan Chang [mailto:notifications@github.com]; > Sent: Tuesday, April 10, 2018 1:04 PM; > To: google/deepvariant <deepvariant@noreply.github.com<mailto:deepvariant@noreply.github.com>>; > Cc: Brad Thomas <brad.thomas@neogenomics.com<mailto:brad.thomas@neogenomics.com>>; Author <; > author@noreply.github.com<mailto:author@noreply.github.com>>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890
https://github.com/google/deepvariant/issues/62#issuecomment-381164890:1068,Availability,error,error,1068,"m using are restricted by Federal regulations and also are proprietary. Apart from sharing data, what can I provide that might help figure this out?. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Thursday, April 12, 2018 3:34 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; From a quick look of your error, it doesn't look like anything I've ever; encountered before. If you could potentially set up a reproducible setting; that I can very quickly run, I can see if I can try it out and tell you; what might could have gone wrong. We don't currently have a tutorial for; training, unfortunately. And to be honest, even if we do, it probably; wouldn't specifically cover this error case. (from my phone). On Tue, Apr 10, 2018, 11:16 AM KBT59 <notifications@github.com<mailto:notifications@github.com>> wrote:. > OK – that proceeded further, I think. Now the error is; > ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for; > 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes:; > [64,27,1,3]; >; >; > I hate to keep bothering people about this. Is there documentation on all; > of this that I can refer to?; >; >; > Thanks,; > Brad Thomas; >; >; > From: Pi-Chuan Chang [mailto:notifications@github.com]; > Sent: Tuesday, April 10, 2018 1:04 PM; > To: google/deepvariant <deepvariant@noreply.github.com<mailto:deepvariant@noreply.github.com>>; > Cc: Brad Thomas <brad.thomas@neogenomics.com<mailto:brad.thomas@neogenomics.com>>; Author <; > author@noreply.github.com<mailto:author@noreply.github.com>>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email origi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890
https://github.com/google/deepvariant/issues/62#issuecomment-381164890:1250,Availability,error,error,1250,"homas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; From a quick look of your error, it doesn't look like anything I've ever; encountered before. If you could potentially set up a reproducible setting; that I can very quickly run, I can see if I can try it out and tell you; what might could have gone wrong. We don't currently have a tutorial for; training, unfortunately. And to be honest, even if we do, it probably; wouldn't specifically cover this error case. (from my phone). On Tue, Apr 10, 2018, 11:16 AM KBT59 <notifications@github.com<mailto:notifications@github.com>> wrote:. > OK – that proceeded further, I think. Now the error is; > ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for; > 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes:; > [64,27,1,3]; >; >; > I hate to keep bothering people about this. Is there documentation on all; > of this that I can refer to?; >; >; > Thanks,; > Brad Thomas; >; >; > From: Pi-Chuan Chang [mailto:notifications@github.com]; > Sent: Tuesday, April 10, 2018 1:04 PM; > To: google/deepvariant <deepvariant@noreply.github.com<mailto:deepvariant@noreply.github.com>>; > Cc: Brad Thomas <brad.thomas@neogenomics.com<mailto:brad.thomas@neogenomics.com>>; Author <; > author@noreply.github.com<mailto:author@noreply.github.com>>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890
https://github.com/google/deepvariant/issues/62#issuecomment-381164890:3031,Availability,error,error-free,3031,"ization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distribute or copy this e-mail. Please notify the sender; > immediately by e-mail if you have received this e-mail by mistake and; > delete this e-mail from your system. E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-380197853>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAczBalfAA5qelNx5damo_mTuPg7r4UJks5tnPbigaJpZM4TIm9R>; > .; >. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380935943>, or mute the thread<https://github.com/notifications/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890
https://github.com/google/deepvariant/issues/62#issuecomment-381164890:3215,Availability,error,errors,3215,"delAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distribute or copy this e-mail. Please notify the sender; > immediately by e-mail if you have received this e-mail by mistake and; > delete this e-mail from your system. E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-380197853>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAczBalfAA5qelNx5damo_mTuPg7r4UJks5tnPbigaJpZM4TIm9R>; > .; >. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380935943>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWTCcHHVi1NrDCRWylTEadlDsGGAks5tn7pQgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890
https://github.com/google/deepvariant/issues/62#issuecomment-381164890:4514,Availability,error,error-free,4514,"E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-380197853>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAczBalfAA5qelNx5damo_mTuPg7r4UJks5tnPbigaJpZM4TIm9R>; > .; >. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380935943>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWTCcHHVi1NrDCRWylTEadlDsGGAks5tn7pQgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890
https://github.com/google/deepvariant/issues/62#issuecomment-381164890:4692,Availability,error,errors,4692,"E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-380197853>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAczBalfAA5qelNx5damo_mTuPg7r4UJks5tnPbigaJpZM4TIm9R>; > .; >. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380935943>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWTCcHHVi1NrDCRWylTEadlDsGGAks5tn7pQgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890
https://github.com/google/deepvariant/issues/62#issuecomment-381164890:2642,Integrability,message,message,2642,"To: google/deepvariant <deepvariant@noreply.github.com<mailto:deepvariant@noreply.github.com>>; > Cc: Brad Thomas <brad.thomas@neogenomics.com<mailto:brad.thomas@neogenomics.com>>; Author <; > author@noreply.github.com<mailto:author@noreply.github.com>>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distribute or copy this e-mail. Please notify the sender; > immediately by e-mail if you have received this e-mail by mistake and; > delete this e-mail from your system. E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890
https://github.com/google/deepvariant/issues/62#issuecomment-381164890:3262,Integrability,message,message,3262,"delAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distribute or copy this e-mail. Please notify the sender; > immediately by e-mail if you have received this e-mail by mistake and; > delete this e-mail from your system. E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-380197853>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAczBalfAA5qelNx5damo_mTuPg7r4UJks5tnPbigaJpZM4TIm9R>; > .; >. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380935943>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWTCcHHVi1NrDCRWylTEadlDsGGAks5tn7pQgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890
https://github.com/google/deepvariant/issues/62#issuecomment-381164890:4140,Integrability,message,message,4140,"E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-380197853>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAczBalfAA5qelNx5damo_mTuPg7r4UJks5tnPbigaJpZM4TIm9R>; > .; >. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380935943>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWTCcHHVi1NrDCRWylTEadlDsGGAks5tn7pQgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890
https://github.com/google/deepvariant/issues/62#issuecomment-381164890:4736,Integrability,message,message,4736,"E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-380197853>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAczBalfAA5qelNx5damo_mTuPg7r4UJks5tnPbigaJpZM4TIm9R>; > .; >. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380935943>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWTCcHHVi1NrDCRWylTEadlDsGGAks5tn7pQgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890
https://github.com/google/deepvariant/issues/62#issuecomment-381164890:656,Safety,safe,safe,656,"Hello,. Unfortunately the data I’m using are restricted by Federal regulations and also are proprietary. Apart from sharing data, what can I provide that might help figure this out?. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Thursday, April 12, 2018 3:34 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; From a quick look of your error, it doesn't look like anything I've ever; encountered before. If you could potentially set up a reproducible setting; that I can very quickly run, I can see if I can try it out and tell you; what might could have gone wrong. We don't currently have a tutorial for; training, unfortunately. And to be honest, even if we do, it probably; wouldn't specifically cover this error case. (from my phone). On Tue, Apr 10, 2018, 11:16 AM KBT59 <notifications@github.com<mailto:notifications@github.com>> wrote:. > OK – that proceeded further, I think. Now the error is; > ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for; > 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes:; > [64,27,1,3]; >; >; > I hate to keep bothering people about this. Is there documentation on all; > of this that I can refer to?; >; >; > Thanks,; > Brad Thomas; >; >; > From: Pi-Chuan Chang [mailto:notifications@github.com]; > Sent: Tuesday, April 10, 2018 1:04 PM; > To: google/deepvariant <deepvariant@noreply.github.com<mailto:deepvariant@noreply.github.com>>; > Cc: Brad Thomas <brad.thomas@neogenomics.com<mailto:brad.thomas@neogenomics.com>>; Author <; > author@noreply.github.com<mailto:author@noreply.github.com>>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890
https://github.com/google/deepvariant/issues/62#issuecomment-381164890:2172,Safety,safe,safe,2172,"6 AM KBT59 <notifications@github.com<mailto:notifications@github.com>> wrote:. > OK – that proceeded further, I think. Now the error is; > ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for; > 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes:; > [64,27,1,3]; >; >; > I hate to keep bothering people about this. Is there documentation on all; > of this that I can refer to?; >; >; > Thanks,; > Brad Thomas; >; >; > From: Pi-Chuan Chang [mailto:notifications@github.com]; > Sent: Tuesday, April 10, 2018 1:04 PM; > To: google/deepvariant <deepvariant@noreply.github.com<mailto:deepvariant@noreply.github.com>>; > Cc: Brad Thomas <brad.thomas@neogenomics.com<mailto:brad.thomas@neogenomics.com>>; Author <; > author@noreply.github.com<mailto:author@noreply.github.com>>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distribute or copy this e-mail. Please notify the sender; > immediately by e-mail if you have received this e-mail by mistake and; > delete this e-mail from your system. E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890
https://github.com/google/deepvariant/issues/62#issuecomment-381164890:2659,Security,confidential,confidential,2659,"To: google/deepvariant <deepvariant@noreply.github.com<mailto:deepvariant@noreply.github.com>>; > Cc: Brad Thomas <brad.thomas@neogenomics.com<mailto:brad.thomas@neogenomics.com>>; Author <; > author@noreply.github.com<mailto:author@noreply.github.com>>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distribute or copy this e-mail. Please notify the sender; > immediately by e-mail if you have received this e-mail by mistake and; > delete this e-mail from your system. E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890
https://github.com/google/deepvariant/issues/62#issuecomment-381164890:3020,Security,secur,secured,3020,"ization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distribute or copy this e-mail. Please notify the sender; > immediately by e-mail if you have received this e-mail by mistake and; > delete this e-mail from your system. E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-380197853>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAczBalfAA5qelNx5damo_mTuPg7r4UJks5tnPbigaJpZM4TIm9R>; > .; >. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380935943>, or mute the thread<https://github.com/notifications/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890
https://github.com/google/deepvariant/issues/62#issuecomment-381164890:4157,Security,confidential,confidential,4157,"E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-380197853>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAczBalfAA5qelNx5damo_mTuPg7r4UJks5tnPbigaJpZM4TIm9R>; > .; >. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380935943>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWTCcHHVi1NrDCRWylTEadlDsGGAks5tn7pQgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890
https://github.com/google/deepvariant/issues/62#issuecomment-381164890:4503,Security,secur,secured,4503,"E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-380197853>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAczBalfAA5qelNx5damo_mTuPg7r4UJks5tnPbigaJpZM4TIm9R>; > .; >. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380935943>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWTCcHHVi1NrDCRWylTEadlDsGGAks5tn7pQgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890
https://github.com/google/deepvariant/issues/62#issuecomment-381164890:1352,Testability,Log,Logits,1352,"homas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; From a quick look of your error, it doesn't look like anything I've ever; encountered before. If you could potentially set up a reproducible setting; that I can very quickly run, I can see if I can try it out and tell you; what might could have gone wrong. We don't currently have a tutorial for; training, unfortunately. And to be honest, even if we do, it probably; wouldn't specifically cover this error case. (from my phone). On Tue, Apr 10, 2018, 11:16 AM KBT59 <notifications@github.com<mailto:notifications@github.com>> wrote:. > OK – that proceeded further, I think. Now the error is; > ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for; > 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes:; > [64,27,1,3]; >; >; > I hate to keep bothering people about this. Is there documentation on all; > of this that I can refer to?; >; >; > Thanks,; > Brad Thomas; >; >; > From: Pi-Chuan Chang [mailto:notifications@github.com]; > Sent: Tuesday, April 10, 2018 1:04 PM; > To: google/deepvariant <deepvariant@noreply.github.com<mailto:deepvariant@noreply.github.com>>; > Cc: Brad Thomas <brad.thomas@neogenomics.com<mailto:brad.thomas@neogenomics.com>>; Author <; > author@noreply.github.com<mailto:author@noreply.github.com>>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890
https://github.com/google/deepvariant/issues/62#issuecomment-381167653:785,Availability,error,error,785,"Hi,; originally I was thinking a small/synthetic dataset could subsampled from your data. I actually don't want the full data anyway (that wouldn't really be a small thing I can try). But I understand if you can't even subsample from your real data.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381167653
https://github.com/google/deepvariant/issues/62#issuecomment-381167653:975,Availability,error,error,975,"Hi,; originally I was thinking a small/synthetic dataset could subsampled from your data. I actually don't want the full data anyway (that wouldn't really be a small thing I can try). But I understand if you can't even subsample from your real data.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381167653
https://github.com/google/deepvariant/issues/62#issuecomment-381167653:596,Testability,log,logic,596,"Hi,; originally I was thinking a small/synthetic dataset could subsampled from your data. I actually don't want the full data anyway (that wouldn't really be a small thing I can try). But I understand if you can't even subsample from your real data.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381167653
https://github.com/google/deepvariant/issues/62#issuecomment-381209312:1372,Availability,error,error,1372,"b.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; originally I was thinking a small/synthetic dataset could subsampled from your data. I actually don't want the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381167653>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqSFWVfTRnfVvHyl6ecMC-XCahjurks5toMDJgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or conta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381209312
https://github.com/google/deepvariant/issues/62#issuecomment-381209312:1562,Availability,error,error,1562,"ss you recognize the sender and know the content is safe. Hi,; originally I was thinking a small/synthetic dataset could subsampled from your data. I actually don't want the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381167653>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqSFWVfTRnfVvHyl6ecMC-XCahjurks5toMDJgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verificat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381209312
https://github.com/google/deepvariant/issues/62#issuecomment-381209312:2253,Availability,error,error-free,2253,"ant the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381167653>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqSFWVfTRnfVvHyl6ecMC-XCahjurks5toMDJgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381209312
https://github.com/google/deepvariant/issues/62#issuecomment-381209312:2431,Availability,error,errors,2431,"ant the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381167653>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqSFWVfTRnfVvHyl6ecMC-XCahjurks5toMDJgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381209312
https://github.com/google/deepvariant/issues/62#issuecomment-381209312:1879,Integrability,message,message,1879,"ant the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381167653>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqSFWVfTRnfVvHyl6ecMC-XCahjurks5toMDJgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381209312
https://github.com/google/deepvariant/issues/62#issuecomment-381209312:2475,Integrability,message,message,2475,"ant the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381167653>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqSFWVfTRnfVvHyl6ecMC-XCahjurks5toMDJgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381209312
https://github.com/google/deepvariant/issues/62#issuecomment-381209312:595,Safety,safe,safe,595,"I’m generating a set from the GIAB exome data as you described. I’ll see what happens with it when I try to train with it. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Friday, April 13, 2018 10:14 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; originally I was thinking a small/synthetic dataset could subsampled from your data. I actually don't want the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381167653>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqSFWVfTRnfVvHyl6ecMC-XCahjurks5toMDJgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addresse",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381209312
https://github.com/google/deepvariant/issues/62#issuecomment-381209312:1896,Security,confidential,confidential,1896,"ant the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381167653>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqSFWVfTRnfVvHyl6ecMC-XCahjurks5toMDJgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381209312
https://github.com/google/deepvariant/issues/62#issuecomment-381209312:2242,Security,secur,secured,2242,"ant the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381167653>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqSFWVfTRnfVvHyl6ecMC-XCahjurks5toMDJgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381209312
https://github.com/google/deepvariant/issues/62#issuecomment-381209312:1183,Testability,log,logic,1183," Sent: Friday, April 13, 2018 10:14 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; originally I was thinking a small/synthetic dataset could subsampled from your data. I actually don't want the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381167653>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqSFWVfTRnfVvHyl6ecMC-XCahjurks5toMDJgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your syst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381209312
https://github.com/google/deepvariant/issues/62#issuecomment-381247700:1774,Availability,error,error,1774,"b.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; originally I was thinking a small/synthetic dataset could subsampled from your data. I actually don't want the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381167653>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqSFWVfTRnfVvHyl6ecMC-XCahjurks5toMDJgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or conta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381247700
https://github.com/google/deepvariant/issues/62#issuecomment-381247700:1964,Availability,error,error,1964,"ss you recognize the sender and know the content is safe. Hi,; originally I was thinking a small/synthetic dataset could subsampled from your data. I actually don't want the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381167653>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqSFWVfTRnfVvHyl6ecMC-XCahjurks5toMDJgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verificat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381247700
https://github.com/google/deepvariant/issues/62#issuecomment-381247700:2655,Availability,error,error-free,2655,"ant the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381167653>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqSFWVfTRnfVvHyl6ecMC-XCahjurks5toMDJgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381247700
https://github.com/google/deepvariant/issues/62#issuecomment-381247700:2833,Availability,error,errors,2833,"ant the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381167653>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqSFWVfTRnfVvHyl6ecMC-XCahjurks5toMDJgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381247700
https://github.com/google/deepvariant/issues/62#issuecomment-381247700:2281,Integrability,message,message,2281,"ant the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381167653>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqSFWVfTRnfVvHyl6ecMC-XCahjurks5toMDJgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381247700
https://github.com/google/deepvariant/issues/62#issuecomment-381247700:2877,Integrability,message,message,2877,"ant the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381167653>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqSFWVfTRnfVvHyl6ecMC-XCahjurks5toMDJgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381247700
https://github.com/google/deepvariant/issues/62#issuecomment-381247700:997,Safety,safe,safe,997,"Hello,. I did make 64 examples from the GIAB exome data mentioned on the github site. I encountered the same problem I mentioned. I’ve attached an archive, bundle.zip that has important files. The file nohup.out shows what was returned when I ran model_train from the command line. Examples were made using the shell script in the bundle: testModeExamples.sh. I’ve included the two python scripts I’ve altered for my deep sequencing project. I appreciate your help. Let me know if there is more I should provide. Thank you,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Friday, April 13, 2018 10:14 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; originally I was thinking a small/synthetic dataset could subsampled from your data. I actually don't want the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this beca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381247700
https://github.com/google/deepvariant/issues/62#issuecomment-381247700:2298,Security,confidential,confidential,2298,"ant the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381167653>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqSFWVfTRnfVvHyl6ecMC-XCahjurks5toMDJgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381247700
https://github.com/google/deepvariant/issues/62#issuecomment-381247700:2644,Security,secur,secured,2644,"ant the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381167653>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqSFWVfTRnfVvHyl6ecMC-XCahjurks5toMDJgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381247700
https://github.com/google/deepvariant/issues/62#issuecomment-381247700:339,Testability,test,testModeExamples,339,"Hello,. I did make 64 examples from the GIAB exome data mentioned on the github site. I encountered the same problem I mentioned. I’ve attached an archive, bundle.zip that has important files. The file nohup.out shows what was returned when I ran model_train from the command line. Examples were made using the shell script in the bundle: testModeExamples.sh. I’ve included the two python scripts I’ve altered for my deep sequencing project. I appreciate your help. Let me know if there is more I should provide. Thank you,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Friday, April 13, 2018 10:14 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; originally I was thinking a small/synthetic dataset could subsampled from your data. I actually don't want the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this beca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381247700
https://github.com/google/deepvariant/issues/62#issuecomment-381247700:1585,Testability,log,logic,1585," Sent: Friday, April 13, 2018 10:14 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; originally I was thinking a small/synthetic dataset could subsampled from your data. I actually don't want the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381167653>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqSFWVfTRnfVvHyl6ecMC-XCahjurks5toMDJgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your syst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381247700
https://github.com/google/deepvariant/issues/62#issuecomment-381621757:1220,Availability,error,error-free,1220,"Here is the zip file. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Saturday, April 14, 2018 12:09 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; I'm not seeing the zip file. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381304103>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqT_omHfFPRVmhBNx0mJ-jQQyMRMXks5toYR4gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381621757
https://github.com/google/deepvariant/issues/62#issuecomment-381621757:1398,Availability,error,errors,1398,"Here is the zip file. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Saturday, April 14, 2018 12:09 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; I'm not seeing the zip file. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381304103>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqT_omHfFPRVmhBNx0mJ-jQQyMRMXks5toYR4gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381621757
https://github.com/google/deepvariant/issues/62#issuecomment-381621757:846,Integrability,message,message,846,"Here is the zip file. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Saturday, April 14, 2018 12:09 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; I'm not seeing the zip file. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381304103>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqT_omHfFPRVmhBNx0mJ-jQQyMRMXks5toYR4gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381621757
https://github.com/google/deepvariant/issues/62#issuecomment-381621757:1442,Integrability,message,message,1442,"Here is the zip file. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Saturday, April 14, 2018 12:09 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; I'm not seeing the zip file. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381304103>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqT_omHfFPRVmhBNx0mJ-jQQyMRMXks5toYR4gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381621757
https://github.com/google/deepvariant/issues/62#issuecomment-381621757:496,Safety,safe,safe,496,"Here is the zip file. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Saturday, April 14, 2018 12:09 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; I'm not seeing the zip file. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381304103>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqT_omHfFPRVmhBNx0mJ-jQQyMRMXks5toYR4gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381621757
https://github.com/google/deepvariant/issues/62#issuecomment-381621757:863,Security,confidential,confidential,863,"Here is the zip file. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Saturday, April 14, 2018 12:09 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; I'm not seeing the zip file. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381304103>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqT_omHfFPRVmhBNx0mJ-jQQyMRMXks5toYR4gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381621757
https://github.com/google/deepvariant/issues/62#issuecomment-381621757:1209,Security,secur,secured,1209,"Here is the zip file. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Saturday, April 14, 2018 12:09 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; I'm not seeing the zip file. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381304103>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqT_omHfFPRVmhBNx0mJ-jQQyMRMXks5toYR4gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-381621757
https://github.com/google/deepvariant/issues/62#issuecomment-385701252:83,Availability,error,error,83,"Hello,. Did you receive the attachment I resent on 4/16? Also, any thoughts on the error I was seeing?. Thank you and best regards,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Saturday, April 14, 2018 12:09 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; I'm not seeing the zip file. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381304103>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqT_omHfFPRVmhBNx0mJ-jQQyMRMXks5toYR4gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-385701252
https://github.com/google/deepvariant/issues/62#issuecomment-385701252:1331,Availability,error,error-free,1331,"Hello,. Did you receive the attachment I resent on 4/16? Also, any thoughts on the error I was seeing?. Thank you and best regards,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Saturday, April 14, 2018 12:09 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; I'm not seeing the zip file. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381304103>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqT_omHfFPRVmhBNx0mJ-jQQyMRMXks5toYR4gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-385701252
https://github.com/google/deepvariant/issues/62#issuecomment-385701252:1509,Availability,error,errors,1509,"Hello,. Did you receive the attachment I resent on 4/16? Also, any thoughts on the error I was seeing?. Thank you and best regards,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Saturday, April 14, 2018 12:09 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; I'm not seeing the zip file. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381304103>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqT_omHfFPRVmhBNx0mJ-jQQyMRMXks5toYR4gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-385701252
https://github.com/google/deepvariant/issues/62#issuecomment-385701252:957,Integrability,message,message,957,"Hello,. Did you receive the attachment I resent on 4/16? Also, any thoughts on the error I was seeing?. Thank you and best regards,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Saturday, April 14, 2018 12:09 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; I'm not seeing the zip file. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381304103>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqT_omHfFPRVmhBNx0mJ-jQQyMRMXks5toYR4gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-385701252
https://github.com/google/deepvariant/issues/62#issuecomment-385701252:1553,Integrability,message,message,1553,"Hello,. Did you receive the attachment I resent on 4/16? Also, any thoughts on the error I was seeing?. Thank you and best regards,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Saturday, April 14, 2018 12:09 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; I'm not seeing the zip file. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381304103>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqT_omHfFPRVmhBNx0mJ-jQQyMRMXks5toYR4gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-385701252
https://github.com/google/deepvariant/issues/62#issuecomment-385701252:607,Safety,safe,safe,607,"Hello,. Did you receive the attachment I resent on 4/16? Also, any thoughts on the error I was seeing?. Thank you and best regards,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Saturday, April 14, 2018 12:09 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; I'm not seeing the zip file. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381304103>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqT_omHfFPRVmhBNx0mJ-jQQyMRMXks5toYR4gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-385701252
https://github.com/google/deepvariant/issues/62#issuecomment-385701252:974,Security,confidential,confidential,974,"Hello,. Did you receive the attachment I resent on 4/16? Also, any thoughts on the error I was seeing?. Thank you and best regards,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Saturday, April 14, 2018 12:09 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; I'm not seeing the zip file. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381304103>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqT_omHfFPRVmhBNx0mJ-jQQyMRMXks5toYR4gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-385701252
https://github.com/google/deepvariant/issues/62#issuecomment-385701252:1320,Security,secur,secured,1320,"Hello,. Did you receive the attachment I resent on 4/16? Also, any thoughts on the error I was seeing?. Thank you and best regards,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Saturday, April 14, 2018 12:09 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; I'm not seeing the zip file. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381304103>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqT_omHfFPRVmhBNx0mJ-jQQyMRMXks5toYR4gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-385701252
https://github.com/google/deepvariant/issues/62#issuecomment-385706487:1388,Availability,error,error-free,1388,"Frustrating. We are blocked from using Google Drive or DropBox. I will send the file from home. Thanks,; Brad Thomas. From: Paul Grosu [mailto:notifications@github.com]; Sent: Tuesday, May 1, 2018 10:50 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi Brad,. Sometimes smtp (email) servers block zip files. Just put it on Google Drive or DropBox and share the link to it. ~p. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-385705660>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWj7l8lWX5469lRFaED45lcY1l0Kks5tuIQogaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-385706487
https://github.com/google/deepvariant/issues/62#issuecomment-385706487:1566,Availability,error,errors,1566,"Frustrating. We are blocked from using Google Drive or DropBox. I will send the file from home. Thanks,; Brad Thomas. From: Paul Grosu [mailto:notifications@github.com]; Sent: Tuesday, May 1, 2018 10:50 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi Brad,. Sometimes smtp (email) servers block zip files. Just put it on Google Drive or DropBox and share the link to it. ~p. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-385705660>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWj7l8lWX5469lRFaED45lcY1l0Kks5tuIQogaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-385706487
https://github.com/google/deepvariant/issues/62#issuecomment-385706487:1014,Integrability,message,message,1014,"Frustrating. We are blocked from using Google Drive or DropBox. I will send the file from home. Thanks,; Brad Thomas. From: Paul Grosu [mailto:notifications@github.com]; Sent: Tuesday, May 1, 2018 10:50 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi Brad,. Sometimes smtp (email) servers block zip files. Just put it on Google Drive or DropBox and share the link to it. ~p. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-385705660>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWj7l8lWX5469lRFaED45lcY1l0Kks5tuIQogaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-385706487
https://github.com/google/deepvariant/issues/62#issuecomment-385706487:1610,Integrability,message,message,1610,"Frustrating. We are blocked from using Google Drive or DropBox. I will send the file from home. Thanks,; Brad Thomas. From: Paul Grosu [mailto:notifications@github.com]; Sent: Tuesday, May 1, 2018 10:50 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi Brad,. Sometimes smtp (email) servers block zip files. Just put it on Google Drive or DropBox and share the link to it. ~p. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-385705660>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWj7l8lWX5469lRFaED45lcY1l0Kks5tuIQogaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-385706487
https://github.com/google/deepvariant/issues/62#issuecomment-385706487:571,Safety,safe,safe,571,"Frustrating. We are blocked from using Google Drive or DropBox. I will send the file from home. Thanks,; Brad Thomas. From: Paul Grosu [mailto:notifications@github.com]; Sent: Tuesday, May 1, 2018 10:50 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi Brad,. Sometimes smtp (email) servers block zip files. Just put it on Google Drive or DropBox and share the link to it. ~p. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-385705660>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWj7l8lWX5469lRFaED45lcY1l0Kks5tuIQogaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-385706487
https://github.com/google/deepvariant/issues/62#issuecomment-385706487:1031,Security,confidential,confidential,1031,"Frustrating. We are blocked from using Google Drive or DropBox. I will send the file from home. Thanks,; Brad Thomas. From: Paul Grosu [mailto:notifications@github.com]; Sent: Tuesday, May 1, 2018 10:50 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi Brad,. Sometimes smtp (email) servers block zip files. Just put it on Google Drive or DropBox and share the link to it. ~p. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-385705660>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWj7l8lWX5469lRFaED45lcY1l0Kks5tuIQogaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-385706487
https://github.com/google/deepvariant/issues/62#issuecomment-385706487:1377,Security,secur,secured,1377,"Frustrating. We are blocked from using Google Drive or DropBox. I will send the file from home. Thanks,; Brad Thomas. From: Paul Grosu [mailto:notifications@github.com]; Sent: Tuesday, May 1, 2018 10:50 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi Brad,. Sometimes smtp (email) servers block zip files. Just put it on Google Drive or DropBox and share the link to it. ~p. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-385705660>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWj7l8lWX5469lRFaED45lcY1l0Kks5tuIQogaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-385706487
https://github.com/google/deepvariant/issues/62#issuecomment-386026449:63,Availability,ping,ping,63,"Hi,; I'll take a look. Give me a few days. Please feel free to ping back if you don't hear from me by end of this week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-386026449
https://github.com/google/deepvariant/issues/62#issuecomment-388450895:55,Availability,error,error,55,Update:; I can confirm that I'm able to reproduce your error. We're working on a fix. Stay tuned!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-388450895
https://github.com/google/deepvariant/issues/62#issuecomment-388450895:0,Deployability,Update,Update,0,Update:; I can confirm that I'm able to reproduce your error. We're working on a fix. Stay tuned!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-388450895
https://github.com/google/deepvariant/issues/62#issuecomment-388450895:91,Performance,tune,tuned,91,Update:; I can confirm that I'm able to reproduce your error. We're working on a fix. Stay tuned!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-388450895
https://github.com/google/deepvariant/issues/62#issuecomment-389567694:303,Safety,avoid,avoid,303,"I've figured out what's going on here and have some good news and bad news. . First, the bad news is that setting the height to 2000 isn't going to work in the short run. This is a limitation coming from inception_v3 itself. At such large image sizes, we would have to run with spatial_squeeze=False to avoid this exception. By doing so we'd essentially end up with a ""tile"" of deepvariant predictions every 64 rows in the image, and then have to pool them together somehow, which makes sense in the general object detection case but not for us in DeepVariant. . The good news is that the maximum supported depth is 362. So you can get a lot more information into your images than the default 100 value. Give 362 a try and let us know if that works. . I should point out that we use a reservoir sampler to create these images. So a height of 362 means you'll get a random sampling of 362 - 5 [for the reference] reads from your very deep sequencing. It's not ideal if you want to detect things occurring in only 1 or 2 reads, but you get a reasonable number of reads if you are looking for things >1% or so frequency in the reads. Hope that helps!. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-389567694
https://github.com/google/deepvariant/issues/62#issuecomment-389567694:390,Safety,predict,predictions,390,"I've figured out what's going on here and have some good news and bad news. . First, the bad news is that setting the height to 2000 isn't going to work in the short run. This is a limitation coming from inception_v3 itself. At such large image sizes, we would have to run with spatial_squeeze=False to avoid this exception. By doing so we'd essentially end up with a ""tile"" of deepvariant predictions every 64 rows in the image, and then have to pool them together somehow, which makes sense in the general object detection case but not for us in DeepVariant. . The good news is that the maximum supported depth is 362. So you can get a lot more information into your images than the default 100 value. Give 362 a try and let us know if that works. . I should point out that we use a reservoir sampler to create these images. So a height of 362 means you'll get a random sampling of 362 - 5 [for the reference] reads from your very deep sequencing. It's not ideal if you want to detect things occurring in only 1 or 2 reads, but you get a reasonable number of reads if you are looking for things >1% or so frequency in the reads. Hope that helps!. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-389567694
https://github.com/google/deepvariant/issues/62#issuecomment-389567694:515,Safety,detect,detection,515,"I've figured out what's going on here and have some good news and bad news. . First, the bad news is that setting the height to 2000 isn't going to work in the short run. This is a limitation coming from inception_v3 itself. At such large image sizes, we would have to run with spatial_squeeze=False to avoid this exception. By doing so we'd essentially end up with a ""tile"" of deepvariant predictions every 64 rows in the image, and then have to pool them together somehow, which makes sense in the general object detection case but not for us in DeepVariant. . The good news is that the maximum supported depth is 362. So you can get a lot more information into your images than the default 100 value. Give 362 a try and let us know if that works. . I should point out that we use a reservoir sampler to create these images. So a height of 362 means you'll get a random sampling of 362 - 5 [for the reference] reads from your very deep sequencing. It's not ideal if you want to detect things occurring in only 1 or 2 reads, but you get a reasonable number of reads if you are looking for things >1% or so frequency in the reads. Hope that helps!. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-389567694
https://github.com/google/deepvariant/issues/62#issuecomment-389567694:980,Safety,detect,detect,980,"I've figured out what's going on here and have some good news and bad news. . First, the bad news is that setting the height to 2000 isn't going to work in the short run. This is a limitation coming from inception_v3 itself. At such large image sizes, we would have to run with spatial_squeeze=False to avoid this exception. By doing so we'd essentially end up with a ""tile"" of deepvariant predictions every 64 rows in the image, and then have to pool them together somehow, which makes sense in the general object detection case but not for us in DeepVariant. . The good news is that the maximum supported depth is 362. So you can get a lot more information into your images than the default 100 value. Give 362 a try and let us know if that works. . I should point out that we use a reservoir sampler to create these images. So a height of 362 means you'll get a random sampling of 362 - 5 [for the reference] reads from your very deep sequencing. It's not ideal if you want to detect things occurring in only 1 or 2 reads, but you get a reasonable number of reads if you are looking for things >1% or so frequency in the reads. Hope that helps!. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/62#issuecomment-389567694
https://github.com/google/deepvariant/issues/63#issuecomment-379300704:518,Deployability,release,released,518,"Hi,; [Quick Start](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-quick-start.md) provides a quick example of how you identify SNPs and Indels in a small region. [Case Study](https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md) gives you a full example of how to identify SNPs and Indels in a whole genome on a single machine. We don't currently have detailed documentation on how to train your own model. I recommend you start with running DeepVariant using the models we released.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/63#issuecomment-379300704
https://github.com/google/deepvariant/issues/64#issuecomment-380240790:115,Safety,predict,predict,115,"Yes, the truth_variants have to have genotypes, as these are used to compute the labels that DeepVariant trains to predict. It's not sufficient to just have sites. . As a general comment, we are aware of the need for more information about how to train DeepVariant, which we hope to produce in a reasonable timeframe.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/64#issuecomment-380240790
https://github.com/google/deepvariant/issues/64#issuecomment-380632313:28,Deployability,release,release,28,Thank you! Yes - the newest release 0.6.0 has fixed this issue and my truth VCF now works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/64#issuecomment-380632313
https://github.com/google/deepvariant/issues/66#issuecomment-403636246:48,Deployability,install,installation,48,"I'm not completely sure about your setting. Our installation guide currently is done on Ubuntu 16. ; In your case, it seems like you're unable to install python-wheel? I think that's outside the scope of DeepVariant support. A few possible ways to get unstuck : maybe you can see whether you can skip installing python-wheel, and see what you actually need to install to proceed to the next step. I don't think we can be of more help on this issue. I'm closing this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/66#issuecomment-403636246
https://github.com/google/deepvariant/issues/66#issuecomment-403636246:146,Deployability,install,install,146,"I'm not completely sure about your setting. Our installation guide currently is done on Ubuntu 16. ; In your case, it seems like you're unable to install python-wheel? I think that's outside the scope of DeepVariant support. A few possible ways to get unstuck : maybe you can see whether you can skip installing python-wheel, and see what you actually need to install to proceed to the next step. I don't think we can be of more help on this issue. I'm closing this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/66#issuecomment-403636246
https://github.com/google/deepvariant/issues/66#issuecomment-403636246:301,Deployability,install,installing,301,"I'm not completely sure about your setting. Our installation guide currently is done on Ubuntu 16. ; In your case, it seems like you're unable to install python-wheel? I think that's outside the scope of DeepVariant support. A few possible ways to get unstuck : maybe you can see whether you can skip installing python-wheel, and see what you actually need to install to proceed to the next step. I don't think we can be of more help on this issue. I'm closing this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/66#issuecomment-403636246
https://github.com/google/deepvariant/issues/66#issuecomment-403636246:360,Deployability,install,install,360,"I'm not completely sure about your setting. Our installation guide currently is done on Ubuntu 16. ; In your case, it seems like you're unable to install python-wheel? I think that's outside the scope of DeepVariant support. A few possible ways to get unstuck : maybe you can see whether you can skip installing python-wheel, and see what you actually need to install to proceed to the next step. I don't think we can be of more help on this issue. I'm closing this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/66#issuecomment-403636246
https://github.com/google/deepvariant/issues/66#issuecomment-403636246:61,Usability,guid,guide,61,"I'm not completely sure about your setting. Our installation guide currently is done on Ubuntu 16. ; In your case, it seems like you're unable to install python-wheel? I think that's outside the scope of DeepVariant support. A few possible ways to get unstuck : maybe you can see whether you can skip installing python-wheel, and see what you actually need to install to proceed to the next step. I don't think we can be of more help on this issue. I'm closing this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/66#issuecomment-403636246
https://github.com/google/deepvariant/issues/67#issuecomment-383258475:759,Testability,test,testdata,759,"Hi,; we don't currently have a good tutorial for training mode yet. But a quick answer here - your understanding for the ""labels"" isn't quite correct.; 0/0, 0/1, or 1/1 you're referring to is the representation in VCFs, which is HOM-REF, HET, and HOM-ALT.; However, that's not how the ""labels"" are represented in the training examples. If you want to see examples of how each of the ""training examples"" that go into training, please look at this:; https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb; Specifically, this data here was generated with `make_examples` in training mode:; ```; # This tfrecord comes from HG002 PFDA data. I ran make_examples in training mode so we also have the labels.; src = 'gs://deepvariant/datalab-testdata/make_examples_datalab.tfrecord.gz'; ```. The ""label"" that represent the 3 classes are actually 0, 1, or 2, which represents HOM-REF, HET, and HOM-ALT. You can see the `get_label` function in the notebook. The representation (0/0, 0/1, 1/1) you're seeing in the final VCF is after `postprocess_variants` step, where we properly write out the common VCF format from the intermediate representation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383258475
https://github.com/google/deepvariant/issues/67#issuecomment-383347052:1228,Deployability,Pipeline,Pipeline,1228,"Hi,. I see, that actually makes a lot more sense, thank you for helping clear that up. As for the training, I understand that there isn't really any official documentation on adding classes, however I was wondering if I could be pointed towards some of the files/functions in charge of calculating the result of the CNN, to see what needs to be changed for adding additional classes. Another option I was considering that is easier to perform, however is much more computationally heavy, is to train a new model from scratch for each of the classes I wish to add, where I would modify the VCF file given to the 'make_examples' script to make the labels be : 0/0 = undefined, 0/1 = yes, 0/2 = no, and to do this for every new class I wish to add. So I would run the vanilla DeepVariant on my input BAM, then do further runs with each model to further categorize variants that would fall within each of the classes I am examining (can possibly even process the initial output VCF such that the additional runs will only run for within those regions that were initially classified as a variant). Is such an approach possible? If all that is being done is converting each input into 6 channels and have it run through the Inception Pipeline then I believe my approach should be possible. I guess my question specifically is, is there anything within the DeepVariant pipeline that will prevent me from training it to identify completely new classes instead of the default HOM-REF, HET, and HOM-ALT?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383347052
https://github.com/google/deepvariant/issues/67#issuecomment-383347052:1362,Deployability,pipeline,pipeline,1362,"Hi,. I see, that actually makes a lot more sense, thank you for helping clear that up. As for the training, I understand that there isn't really any official documentation on adding classes, however I was wondering if I could be pointed towards some of the files/functions in charge of calculating the result of the CNN, to see what needs to be changed for adding additional classes. Another option I was considering that is easier to perform, however is much more computationally heavy, is to train a new model from scratch for each of the classes I wish to add, where I would modify the VCF file given to the 'make_examples' script to make the labels be : 0/0 = undefined, 0/1 = yes, 0/2 = no, and to do this for every new class I wish to add. So I would run the vanilla DeepVariant on my input BAM, then do further runs with each model to further categorize variants that would fall within each of the classes I am examining (can possibly even process the initial output VCF such that the additional runs will only run for within those regions that were initially classified as a variant). Is such an approach possible? If all that is being done is converting each input into 6 channels and have it run through the Inception Pipeline then I believe my approach should be possible. I guess my question specifically is, is there anything within the DeepVariant pipeline that will prevent me from training it to identify completely new classes instead of the default HOM-REF, HET, and HOM-ALT?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383347052
https://github.com/google/deepvariant/issues/67#issuecomment-383347052:276,Energy Efficiency,charge,charge,276,"Hi,. I see, that actually makes a lot more sense, thank you for helping clear that up. As for the training, I understand that there isn't really any official documentation on adding classes, however I was wondering if I could be pointed towards some of the files/functions in charge of calculating the result of the CNN, to see what needs to be changed for adding additional classes. Another option I was considering that is easier to perform, however is much more computationally heavy, is to train a new model from scratch for each of the classes I wish to add, where I would modify the VCF file given to the 'make_examples' script to make the labels be : 0/0 = undefined, 0/1 = yes, 0/2 = no, and to do this for every new class I wish to add. So I would run the vanilla DeepVariant on my input BAM, then do further runs with each model to further categorize variants that would fall within each of the classes I am examining (can possibly even process the initial output VCF such that the additional runs will only run for within those regions that were initially classified as a variant). Is such an approach possible? If all that is being done is converting each input into 6 channels and have it run through the Inception Pipeline then I believe my approach should be possible. I guess my question specifically is, is there anything within the DeepVariant pipeline that will prevent me from training it to identify completely new classes instead of the default HOM-REF, HET, and HOM-ALT?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383347052
https://github.com/google/deepvariant/issues/67#issuecomment-383347052:435,Performance,perform,perform,435,"Hi,. I see, that actually makes a lot more sense, thank you for helping clear that up. As for the training, I understand that there isn't really any official documentation on adding classes, however I was wondering if I could be pointed towards some of the files/functions in charge of calculating the result of the CNN, to see what needs to be changed for adding additional classes. Another option I was considering that is easier to perform, however is much more computationally heavy, is to train a new model from scratch for each of the classes I wish to add, where I would modify the VCF file given to the 'make_examples' script to make the labels be : 0/0 = undefined, 0/1 = yes, 0/2 = no, and to do this for every new class I wish to add. So I would run the vanilla DeepVariant on my input BAM, then do further runs with each model to further categorize variants that would fall within each of the classes I am examining (can possibly even process the initial output VCF such that the additional runs will only run for within those regions that were initially classified as a variant). Is such an approach possible? If all that is being done is converting each input into 6 channels and have it run through the Inception Pipeline then I believe my approach should be possible. I guess my question specifically is, is there anything within the DeepVariant pipeline that will prevent me from training it to identify completely new classes instead of the default HOM-REF, HET, and HOM-ALT?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383347052
https://github.com/google/deepvariant/issues/67#issuecomment-383347052:72,Usability,clear,clear,72,"Hi,. I see, that actually makes a lot more sense, thank you for helping clear that up. As for the training, I understand that there isn't really any official documentation on adding classes, however I was wondering if I could be pointed towards some of the files/functions in charge of calculating the result of the CNN, to see what needs to be changed for adding additional classes. Another option I was considering that is easier to perform, however is much more computationally heavy, is to train a new model from scratch for each of the classes I wish to add, where I would modify the VCF file given to the 'make_examples' script to make the labels be : 0/0 = undefined, 0/1 = yes, 0/2 = no, and to do this for every new class I wish to add. So I would run the vanilla DeepVariant on my input BAM, then do further runs with each model to further categorize variants that would fall within each of the classes I am examining (can possibly even process the initial output VCF such that the additional runs will only run for within those regions that were initially classified as a variant). Is such an approach possible? If all that is being done is converting each input into 6 channels and have it run through the Inception Pipeline then I believe my approach should be possible. I guess my question specifically is, is there anything within the DeepVariant pipeline that will prevent me from training it to identify completely new classes instead of the default HOM-REF, HET, and HOM-ALT?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383347052
https://github.com/google/deepvariant/issues/67#issuecomment-383764665:623,Availability,down,down,623,"Hi masgouri@, . Thanks for the excellent question and for sharing that you've been having good experiences with DeepVariant. We are always interested in user stories so if you feel like sharing more about your experiences with DeepVariant please send them our way. There are a few separate issues here; let me try to unpack them. -- There are some engineering constraints in DeepVariant that assume we train only 3 classes. One example is https://github.com/google/deepvariant/search?utf8=%E2%9C%93&q=DEFAULT_NUM_CLASSES&type= but there may be others. If you want to change the number of classes you'll need to track these down and generalize them. If you do that please send us a CL and we'll make sure to incorporate the changes into the master codebase here. Once you do that it should be possible to train as with as many labels as you like. -- You'll need to write your own labeler algorithm to provide a label for each training variant. These classes are in deepvariant/labeler. There are two (a PositionalLabeler and a HaplotypeLabeler) so you can use these as templates to hook up your own labeling algorithm. At the core these just need to return an integer label (0 ... N) for each example. Once you get that going it should be simple to label your own examples. Technically the labels can be anything. For example, if you emitted the reference base value as 0 for A, 1 for T, etc I'm pretty sure the machinery can learn to call the reference base from the image with perfect accuracy. -- The machinery of training really doesn't know anything about NGS data. It just takes the examples and tries to learn to predict the label. Once you get your labels in and the machinery doesn't expect just 3 it should be possible to train anything you want. You can use standard evaluation machinery to decide if your DeepVariant training run is successfully learning something. -- You'll likely need to write your own version of postprocess_variants (or extend it in some way) to bridge from the predic",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383764665
https://github.com/google/deepvariant/issues/67#issuecomment-383764665:1979,Integrability,bridg,bridge,1979,"et me try to unpack them. -- There are some engineering constraints in DeepVariant that assume we train only 3 classes. One example is https://github.com/google/deepvariant/search?utf8=%E2%9C%93&q=DEFAULT_NUM_CLASSES&type= but there may be others. If you want to change the number of classes you'll need to track these down and generalize them. If you do that please send us a CL and we'll make sure to incorporate the changes into the master codebase here. Once you do that it should be possible to train as with as many labels as you like. -- You'll need to write your own labeler algorithm to provide a label for each training variant. These classes are in deepvariant/labeler. There are two (a PositionalLabeler and a HaplotypeLabeler) so you can use these as templates to hook up your own labeling algorithm. At the core these just need to return an integer label (0 ... N) for each example. Once you get that going it should be simple to label your own examples. Technically the labels can be anything. For example, if you emitted the reference base value as 0 for A, 1 for T, etc I'm pretty sure the machinery can learn to call the reference base from the image with perfect accuracy. -- The machinery of training really doesn't know anything about NGS data. It just takes the examples and tries to learn to predict the label. Once you get your labels in and the machinery doesn't expect just 3 it should be possible to train anything you want. You can use standard evaluation machinery to decide if your DeepVariant training run is successfully learning something. -- You'll likely need to write your own version of postprocess_variants (or extend it in some way) to bridge from the predicted labels and the information you want to export in the VCF (or other file format). In germline DeepVariant we learn to predict 0, 1, and 2 which mean hom-ref, het, and hom-alt, respectively, which is encoded in postprocess_variants. You'll need your own equivalent to this code. Hope this helps!. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383764665
https://github.com/google/deepvariant/issues/67#issuecomment-383764665:1953,Modifiability,extend,extend,1953,"et me try to unpack them. -- There are some engineering constraints in DeepVariant that assume we train only 3 classes. One example is https://github.com/google/deepvariant/search?utf8=%E2%9C%93&q=DEFAULT_NUM_CLASSES&type= but there may be others. If you want to change the number of classes you'll need to track these down and generalize them. If you do that please send us a CL and we'll make sure to incorporate the changes into the master codebase here. Once you do that it should be possible to train as with as many labels as you like. -- You'll need to write your own labeler algorithm to provide a label for each training variant. These classes are in deepvariant/labeler. There are two (a PositionalLabeler and a HaplotypeLabeler) so you can use these as templates to hook up your own labeling algorithm. At the core these just need to return an integer label (0 ... N) for each example. Once you get that going it should be simple to label your own examples. Technically the labels can be anything. For example, if you emitted the reference base value as 0 for A, 1 for T, etc I'm pretty sure the machinery can learn to call the reference base from the image with perfect accuracy. -- The machinery of training really doesn't know anything about NGS data. It just takes the examples and tries to learn to predict the label. Once you get your labels in and the machinery doesn't expect just 3 it should be possible to train anything you want. You can use standard evaluation machinery to decide if your DeepVariant training run is successfully learning something. -- You'll likely need to write your own version of postprocess_variants (or extend it in some way) to bridge from the predicted labels and the information you want to export in the VCF (or other file format). In germline DeepVariant we learn to predict 0, 1, and 2 which mean hom-ref, het, and hom-alt, respectively, which is encoded in postprocess_variants. You'll need your own equivalent to this code. Hope this helps!. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383764665
https://github.com/google/deepvariant/issues/67#issuecomment-383764665:1619,Safety,predict,predict,1619,"et me try to unpack them. -- There are some engineering constraints in DeepVariant that assume we train only 3 classes. One example is https://github.com/google/deepvariant/search?utf8=%E2%9C%93&q=DEFAULT_NUM_CLASSES&type= but there may be others. If you want to change the number of classes you'll need to track these down and generalize them. If you do that please send us a CL and we'll make sure to incorporate the changes into the master codebase here. Once you do that it should be possible to train as with as many labels as you like. -- You'll need to write your own labeler algorithm to provide a label for each training variant. These classes are in deepvariant/labeler. There are two (a PositionalLabeler and a HaplotypeLabeler) so you can use these as templates to hook up your own labeling algorithm. At the core these just need to return an integer label (0 ... N) for each example. Once you get that going it should be simple to label your own examples. Technically the labels can be anything. For example, if you emitted the reference base value as 0 for A, 1 for T, etc I'm pretty sure the machinery can learn to call the reference base from the image with perfect accuracy. -- The machinery of training really doesn't know anything about NGS data. It just takes the examples and tries to learn to predict the label. Once you get your labels in and the machinery doesn't expect just 3 it should be possible to train anything you want. You can use standard evaluation machinery to decide if your DeepVariant training run is successfully learning something. -- You'll likely need to write your own version of postprocess_variants (or extend it in some way) to bridge from the predicted labels and the information you want to export in the VCF (or other file format). In germline DeepVariant we learn to predict 0, 1, and 2 which mean hom-ref, het, and hom-alt, respectively, which is encoded in postprocess_variants. You'll need your own equivalent to this code. Hope this helps!. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383764665
https://github.com/google/deepvariant/issues/67#issuecomment-383764665:1995,Safety,predict,predicted,1995,"et me try to unpack them. -- There are some engineering constraints in DeepVariant that assume we train only 3 classes. One example is https://github.com/google/deepvariant/search?utf8=%E2%9C%93&q=DEFAULT_NUM_CLASSES&type= but there may be others. If you want to change the number of classes you'll need to track these down and generalize them. If you do that please send us a CL and we'll make sure to incorporate the changes into the master codebase here. Once you do that it should be possible to train as with as many labels as you like. -- You'll need to write your own labeler algorithm to provide a label for each training variant. These classes are in deepvariant/labeler. There are two (a PositionalLabeler and a HaplotypeLabeler) so you can use these as templates to hook up your own labeling algorithm. At the core these just need to return an integer label (0 ... N) for each example. Once you get that going it should be simple to label your own examples. Technically the labels can be anything. For example, if you emitted the reference base value as 0 for A, 1 for T, etc I'm pretty sure the machinery can learn to call the reference base from the image with perfect accuracy. -- The machinery of training really doesn't know anything about NGS data. It just takes the examples and tries to learn to predict the label. Once you get your labels in and the machinery doesn't expect just 3 it should be possible to train anything you want. You can use standard evaluation machinery to decide if your DeepVariant training run is successfully learning something. -- You'll likely need to write your own version of postprocess_variants (or extend it in some way) to bridge from the predicted labels and the information you want to export in the VCF (or other file format). In germline DeepVariant we learn to predict 0, 1, and 2 which mean hom-ref, het, and hom-alt, respectively, which is encoded in postprocess_variants. You'll need your own equivalent to this code. Hope this helps!. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383764665
https://github.com/google/deepvariant/issues/67#issuecomment-383764665:2122,Safety,predict,predict,2122,"et me try to unpack them. -- There are some engineering constraints in DeepVariant that assume we train only 3 classes. One example is https://github.com/google/deepvariant/search?utf8=%E2%9C%93&q=DEFAULT_NUM_CLASSES&type= but there may be others. If you want to change the number of classes you'll need to track these down and generalize them. If you do that please send us a CL and we'll make sure to incorporate the changes into the master codebase here. Once you do that it should be possible to train as with as many labels as you like. -- You'll need to write your own labeler algorithm to provide a label for each training variant. These classes are in deepvariant/labeler. There are two (a PositionalLabeler and a HaplotypeLabeler) so you can use these as templates to hook up your own labeling algorithm. At the core these just need to return an integer label (0 ... N) for each example. Once you get that going it should be simple to label your own examples. Technically the labels can be anything. For example, if you emitted the reference base value as 0 for A, 1 for T, etc I'm pretty sure the machinery can learn to call the reference base from the image with perfect accuracy. -- The machinery of training really doesn't know anything about NGS data. It just takes the examples and tries to learn to predict the label. Once you get your labels in and the machinery doesn't expect just 3 it should be possible to train anything you want. You can use standard evaluation machinery to decide if your DeepVariant training run is successfully learning something. -- You'll likely need to write your own version of postprocess_variants (or extend it in some way) to bridge from the predicted labels and the information you want to export in the VCF (or other file format). In germline DeepVariant we learn to predict 0, 1, and 2 which mean hom-ref, het, and hom-alt, respectively, which is encoded in postprocess_variants. You'll need your own equivalent to this code. Hope this helps!. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383764665
https://github.com/google/deepvariant/issues/67#issuecomment-383764665:1238,Usability,simpl,simple,1238,"nt please send them our way. There are a few separate issues here; let me try to unpack them. -- There are some engineering constraints in DeepVariant that assume we train only 3 classes. One example is https://github.com/google/deepvariant/search?utf8=%E2%9C%93&q=DEFAULT_NUM_CLASSES&type= but there may be others. If you want to change the number of classes you'll need to track these down and generalize them. If you do that please send us a CL and we'll make sure to incorporate the changes into the master codebase here. Once you do that it should be possible to train as with as many labels as you like. -- You'll need to write your own labeler algorithm to provide a label for each training variant. These classes are in deepvariant/labeler. There are two (a PositionalLabeler and a HaplotypeLabeler) so you can use these as templates to hook up your own labeling algorithm. At the core these just need to return an integer label (0 ... N) for each example. Once you get that going it should be simple to label your own examples. Technically the labels can be anything. For example, if you emitted the reference base value as 0 for A, 1 for T, etc I'm pretty sure the machinery can learn to call the reference base from the image with perfect accuracy. -- The machinery of training really doesn't know anything about NGS data. It just takes the examples and tries to learn to predict the label. Once you get your labels in and the machinery doesn't expect just 3 it should be possible to train anything you want. You can use standard evaluation machinery to decide if your DeepVariant training run is successfully learning something. -- You'll likely need to write your own version of postprocess_variants (or extend it in some way) to bridge from the predicted labels and the information you want to export in the VCF (or other file format). In germline DeepVariant we learn to predict 0, 1, and 2 which mean hom-ref, het, and hom-alt, respectively, which is encoded in postprocess_variants. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383764665
https://github.com/google/deepvariant/issues/67#issuecomment-383764665:1425,Usability,learn,learn,1425,"et me try to unpack them. -- There are some engineering constraints in DeepVariant that assume we train only 3 classes. One example is https://github.com/google/deepvariant/search?utf8=%E2%9C%93&q=DEFAULT_NUM_CLASSES&type= but there may be others. If you want to change the number of classes you'll need to track these down and generalize them. If you do that please send us a CL and we'll make sure to incorporate the changes into the master codebase here. Once you do that it should be possible to train as with as many labels as you like. -- You'll need to write your own labeler algorithm to provide a label for each training variant. These classes are in deepvariant/labeler. There are two (a PositionalLabeler and a HaplotypeLabeler) so you can use these as templates to hook up your own labeling algorithm. At the core these just need to return an integer label (0 ... N) for each example. Once you get that going it should be simple to label your own examples. Technically the labels can be anything. For example, if you emitted the reference base value as 0 for A, 1 for T, etc I'm pretty sure the machinery can learn to call the reference base from the image with perfect accuracy. -- The machinery of training really doesn't know anything about NGS data. It just takes the examples and tries to learn to predict the label. Once you get your labels in and the machinery doesn't expect just 3 it should be possible to train anything you want. You can use standard evaluation machinery to decide if your DeepVariant training run is successfully learning something. -- You'll likely need to write your own version of postprocess_variants (or extend it in some way) to bridge from the predicted labels and the information you want to export in the VCF (or other file format). In germline DeepVariant we learn to predict 0, 1, and 2 which mean hom-ref, het, and hom-alt, respectively, which is encoded in postprocess_variants. You'll need your own equivalent to this code. Hope this helps!. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383764665
https://github.com/google/deepvariant/issues/67#issuecomment-383764665:1610,Usability,learn,learn,1610,"et me try to unpack them. -- There are some engineering constraints in DeepVariant that assume we train only 3 classes. One example is https://github.com/google/deepvariant/search?utf8=%E2%9C%93&q=DEFAULT_NUM_CLASSES&type= but there may be others. If you want to change the number of classes you'll need to track these down and generalize them. If you do that please send us a CL and we'll make sure to incorporate the changes into the master codebase here. Once you do that it should be possible to train as with as many labels as you like. -- You'll need to write your own labeler algorithm to provide a label for each training variant. These classes are in deepvariant/labeler. There are two (a PositionalLabeler and a HaplotypeLabeler) so you can use these as templates to hook up your own labeling algorithm. At the core these just need to return an integer label (0 ... N) for each example. Once you get that going it should be simple to label your own examples. Technically the labels can be anything. For example, if you emitted the reference base value as 0 for A, 1 for T, etc I'm pretty sure the machinery can learn to call the reference base from the image with perfect accuracy. -- The machinery of training really doesn't know anything about NGS data. It just takes the examples and tries to learn to predict the label. Once you get your labels in and the machinery doesn't expect just 3 it should be possible to train anything you want. You can use standard evaluation machinery to decide if your DeepVariant training run is successfully learning something. -- You'll likely need to write your own version of postprocess_variants (or extend it in some way) to bridge from the predicted labels and the information you want to export in the VCF (or other file format). In germline DeepVariant we learn to predict 0, 1, and 2 which mean hom-ref, het, and hom-alt, respectively, which is encoded in postprocess_variants. You'll need your own equivalent to this code. Hope this helps!. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383764665
https://github.com/google/deepvariant/issues/67#issuecomment-383764665:1857,Usability,learn,learning,1857,"et me try to unpack them. -- There are some engineering constraints in DeepVariant that assume we train only 3 classes. One example is https://github.com/google/deepvariant/search?utf8=%E2%9C%93&q=DEFAULT_NUM_CLASSES&type= but there may be others. If you want to change the number of classes you'll need to track these down and generalize them. If you do that please send us a CL and we'll make sure to incorporate the changes into the master codebase here. Once you do that it should be possible to train as with as many labels as you like. -- You'll need to write your own labeler algorithm to provide a label for each training variant. These classes are in deepvariant/labeler. There are two (a PositionalLabeler and a HaplotypeLabeler) so you can use these as templates to hook up your own labeling algorithm. At the core these just need to return an integer label (0 ... N) for each example. Once you get that going it should be simple to label your own examples. Technically the labels can be anything. For example, if you emitted the reference base value as 0 for A, 1 for T, etc I'm pretty sure the machinery can learn to call the reference base from the image with perfect accuracy. -- The machinery of training really doesn't know anything about NGS data. It just takes the examples and tries to learn to predict the label. Once you get your labels in and the machinery doesn't expect just 3 it should be possible to train anything you want. You can use standard evaluation machinery to decide if your DeepVariant training run is successfully learning something. -- You'll likely need to write your own version of postprocess_variants (or extend it in some way) to bridge from the predicted labels and the information you want to export in the VCF (or other file format). In germline DeepVariant we learn to predict 0, 1, and 2 which mean hom-ref, het, and hom-alt, respectively, which is encoded in postprocess_variants. You'll need your own equivalent to this code. Hope this helps!. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383764665
https://github.com/google/deepvariant/issues/67#issuecomment-383764665:2113,Usability,learn,learn,2113,"et me try to unpack them. -- There are some engineering constraints in DeepVariant that assume we train only 3 classes. One example is https://github.com/google/deepvariant/search?utf8=%E2%9C%93&q=DEFAULT_NUM_CLASSES&type= but there may be others. If you want to change the number of classes you'll need to track these down and generalize them. If you do that please send us a CL and we'll make sure to incorporate the changes into the master codebase here. Once you do that it should be possible to train as with as many labels as you like. -- You'll need to write your own labeler algorithm to provide a label for each training variant. These classes are in deepvariant/labeler. There are two (a PositionalLabeler and a HaplotypeLabeler) so you can use these as templates to hook up your own labeling algorithm. At the core these just need to return an integer label (0 ... N) for each example. Once you get that going it should be simple to label your own examples. Technically the labels can be anything. For example, if you emitted the reference base value as 0 for A, 1 for T, etc I'm pretty sure the machinery can learn to call the reference base from the image with perfect accuracy. -- The machinery of training really doesn't know anything about NGS data. It just takes the examples and tries to learn to predict the label. Once you get your labels in and the machinery doesn't expect just 3 it should be possible to train anything you want. You can use standard evaluation machinery to decide if your DeepVariant training run is successfully learning something. -- You'll likely need to write your own version of postprocess_variants (or extend it in some way) to bridge from the predicted labels and the information you want to export in the VCF (or other file format). In germline DeepVariant we learn to predict 0, 1, and 2 which mean hom-ref, het, and hom-alt, respectively, which is encoded in postprocess_variants. You'll need your own equivalent to this code. Hope this helps!. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/67#issuecomment-383764665
https://github.com/google/deepvariant/pull/68#issuecomment-384434821:147,Deployability,release,release,147,"Thanks for the report. Unfortunately we can't currently take pull request from GitHub. ; This include is fixed in an internal version, but the 0.6 release was cut before that change. I will plan to create a 0.6.1 to include this fix.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/68#issuecomment-384434821
https://github.com/google/deepvariant/pull/68#issuecomment-385594090:48,Deployability,release,released,48,"Hi @wangtz , thanks again for reporting. ; I've released v0.6.1 which should addressed the issue you reported.; https://github.com/google/deepvariant/releases/tag/v0.6.1. Please let me know if you see other problems.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/68#issuecomment-385594090
https://github.com/google/deepvariant/pull/68#issuecomment-385594090:150,Deployability,release,releases,150,"Hi @wangtz , thanks again for reporting. ; I've released v0.6.1 which should addressed the issue you reported.; https://github.com/google/deepvariant/releases/tag/v0.6.1. Please let me know if you see other problems.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/68#issuecomment-385594090
https://github.com/google/deepvariant/issues/69#issuecomment-386202451:11,Testability,test,testdata,11,"We use the testdata from DeepVariant source (https://github.com/google/deepvariant/tree/r0.6/deepvariant/testdata) .The test_nist.b37_chr20_100kbp_at_10mb.bed is really small, so we just can set one example in that examples.tfrecord.gz. . **test_nist.b37_chr20_100kbp_at_10mb.bed file:**; > chr20	10000846	10002407; chr20	10002520	10004171; chr20	10004273	10004964; chr20	10004994	10006386; chr20	10006409	10007800; chr20	10007824	10008018; chr20	10008043	10008079; chr20	10008100	10008707; chr20	10008808	10008897; chr20	10009002	10009791; chr20	10009933	10010531. We will try to train model with our realistic WES data, and set with at least 10,000 examples. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/69#issuecomment-386202451
https://github.com/google/deepvariant/issues/69#issuecomment-386202451:105,Testability,test,testdata,105,"We use the testdata from DeepVariant source (https://github.com/google/deepvariant/tree/r0.6/deepvariant/testdata) .The test_nist.b37_chr20_100kbp_at_10mb.bed is really small, so we just can set one example in that examples.tfrecord.gz. . **test_nist.b37_chr20_100kbp_at_10mb.bed file:**; > chr20	10000846	10002407; chr20	10002520	10004171; chr20	10004273	10004964; chr20	10004994	10006386; chr20	10006409	10007800; chr20	10007824	10008018; chr20	10008043	10008079; chr20	10008100	10008707; chr20	10008808	10008897; chr20	10009002	10009791; chr20	10009933	10010531. We will try to train model with our realistic WES data, and set with at least 10,000 examples. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/69#issuecomment-386202451
https://github.com/google/deepvariant/issues/69#issuecomment-386515701:1205,Availability,echo,echo,1205," model at the same time?. **The make_examples script is:**; `BASE=""${HOME}/Documents/source""; BIN_DIR=""${BASE}/bin""; MODELS_DIR=""${BASE}/DeepVariant/deepvariant-model-wes_and_wgs/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard""; MODEL=""${MODELS_DIR}/model.ckpt""; N_SHARDS=""64""; BAM=""/sdbdata/WESdata/CL100026859_bwa/CL100026859_L02_14/PE150.2.rmdup.bam""; REF=""/home/suanfa/Documents/source/ref/hg19.fasta""; var=${BAM##*/}; var=${var%.*}; path=""/home/suanfa/Documents/shishiming/training_WES_model""; OUTPUT_DIR=""$path/output""; EXAMPLES=""${OUTPUT_DIR}/${var}.training.examples.tfrecord@${N_SHARDS}.gz""; CONFIDENT_REGIONS=""/home/suanfa/Documents/source/exome_region_bed/new_v4.bed""; TRUTH_VARIANTS=""/home/suanfa/Documents/source/NISTv3.3_baseline/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.new_ch rID.vcf.gz""; LOG_DIR=""${OUTPUT_DIR}/logs"" . echo 'Run make_examples'; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --confident_regions ${CONFIDENT_REGIONS} \; --truth_variants ${TRUTH_VARIANTS} \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`. **the part of the examples output:**; > PE150.2.rmdup.training.examples.tfrecord-00000-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00022-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00044-of-00064.gz; PE150.2.rmdup.training.examples.tfrecord-00001-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00023-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00045-of-00064.gz; PE150.2.rmdup.training.examples.tfrecord-00002-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00024-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00046-of-00064.gz; PE150.2.rmdup.training.examples.tfrecord-00003-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00025-of-00064.gz PE",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/69#issuecomment-386515701
https://github.com/google/deepvariant/issues/69#issuecomment-386515701:1197,Testability,log,logs,1197," can input one tfrecord file to the pbtxt file. How can we input all of the tfrecord file to train model at the same time?. **The make_examples script is:**; `BASE=""${HOME}/Documents/source""; BIN_DIR=""${BASE}/bin""; MODELS_DIR=""${BASE}/DeepVariant/deepvariant-model-wes_and_wgs/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard""; MODEL=""${MODELS_DIR}/model.ckpt""; N_SHARDS=""64""; BAM=""/sdbdata/WESdata/CL100026859_bwa/CL100026859_L02_14/PE150.2.rmdup.bam""; REF=""/home/suanfa/Documents/source/ref/hg19.fasta""; var=${BAM##*/}; var=${var%.*}; path=""/home/suanfa/Documents/shishiming/training_WES_model""; OUTPUT_DIR=""$path/output""; EXAMPLES=""${OUTPUT_DIR}/${var}.training.examples.tfrecord@${N_SHARDS}.gz""; CONFIDENT_REGIONS=""/home/suanfa/Documents/source/exome_region_bed/new_v4.bed""; TRUTH_VARIANTS=""/home/suanfa/Documents/source/NISTv3.3_baseline/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.new_ch rID.vcf.gz""; LOG_DIR=""${OUTPUT_DIR}/logs"" . echo 'Run make_examples'; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --confident_regions ${CONFIDENT_REGIONS} \; --truth_variants ${TRUTH_VARIANTS} \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`. **the part of the examples output:**; > PE150.2.rmdup.training.examples.tfrecord-00000-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00022-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00044-of-00064.gz; PE150.2.rmdup.training.examples.tfrecord-00001-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00023-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00045-of-00064.gz; PE150.2.rmdup.training.examples.tfrecord-00002-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00024-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00046-of-00064.gz; PE150.2.rmdup.training",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/69#issuecomment-386515701
https://github.com/google/deepvariant/issues/69#issuecomment-386515701:1304,Testability,log,log,1304," model at the same time?. **The make_examples script is:**; `BASE=""${HOME}/Documents/source""; BIN_DIR=""${BASE}/bin""; MODELS_DIR=""${BASE}/DeepVariant/deepvariant-model-wes_and_wgs/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard""; MODEL=""${MODELS_DIR}/model.ckpt""; N_SHARDS=""64""; BAM=""/sdbdata/WESdata/CL100026859_bwa/CL100026859_L02_14/PE150.2.rmdup.bam""; REF=""/home/suanfa/Documents/source/ref/hg19.fasta""; var=${BAM##*/}; var=${var%.*}; path=""/home/suanfa/Documents/shishiming/training_WES_model""; OUTPUT_DIR=""$path/output""; EXAMPLES=""${OUTPUT_DIR}/${var}.training.examples.tfrecord@${N_SHARDS}.gz""; CONFIDENT_REGIONS=""/home/suanfa/Documents/source/exome_region_bed/new_v4.bed""; TRUTH_VARIANTS=""/home/suanfa/Documents/source/NISTv3.3_baseline/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.new_ch rID.vcf.gz""; LOG_DIR=""${OUTPUT_DIR}/logs"" . echo 'Run make_examples'; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --confident_regions ${CONFIDENT_REGIONS} \; --truth_variants ${TRUTH_VARIANTS} \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`. **the part of the examples output:**; > PE150.2.rmdup.training.examples.tfrecord-00000-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00022-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00044-of-00064.gz; PE150.2.rmdup.training.examples.tfrecord-00001-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00023-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00045-of-00064.gz; PE150.2.rmdup.training.examples.tfrecord-00002-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00024-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00046-of-00064.gz; PE150.2.rmdup.training.examples.tfrecord-00003-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00025-of-00064.gz PE",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/69#issuecomment-386515701
https://github.com/google/deepvariant/issues/69#issuecomment-386515701:1579,Testability,log,log,1579,"bdata/WESdata/CL100026859_bwa/CL100026859_L02_14/PE150.2.rmdup.bam""; REF=""/home/suanfa/Documents/source/ref/hg19.fasta""; var=${BAM##*/}; var=${var%.*}; path=""/home/suanfa/Documents/shishiming/training_WES_model""; OUTPUT_DIR=""$path/output""; EXAMPLES=""${OUTPUT_DIR}/${var}.training.examples.tfrecord@${N_SHARDS}.gz""; CONFIDENT_REGIONS=""/home/suanfa/Documents/source/exome_region_bed/new_v4.bed""; TRUTH_VARIANTS=""/home/suanfa/Documents/source/NISTv3.3_baseline/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.new_ch rID.vcf.gz""; LOG_DIR=""${OUTPUT_DIR}/logs"" . echo 'Run make_examples'; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --confident_regions ${CONFIDENT_REGIONS} \; --truth_variants ${TRUTH_VARIANTS} \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`. **the part of the examples output:**; > PE150.2.rmdup.training.examples.tfrecord-00000-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00022-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00044-of-00064.gz; PE150.2.rmdup.training.examples.tfrecord-00001-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00023-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00045-of-00064.gz; PE150.2.rmdup.training.examples.tfrecord-00002-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00024-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00046-of-00064.gz; PE150.2.rmdup.training.examples.tfrecord-00003-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00025-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00047-of-00064.gz; PE150.2.rmdup.training.examples.tfrecord-00004-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00026-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00048-of-00064.gz; PE150.2.rmdup.training.examples.tfrecord-00005-of-00064.gz PE150",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/69#issuecomment-386515701
https://github.com/google/deepvariant/issues/70#issuecomment-386838731:73,Availability,error,error,73,"Hi Pi-Chuan,; Thanks. I set `--call_variants_cores_per_worker 8` and new error pops out. It would be great if guys can take a look of it as well.; ```; make-examp--root--180505-205721-02: FAILURE; [u'Error in job make-examp--root--180505-205721-02 - code 5: 9: Failed to localize files: failed to make local directory for /mnt/datadisk/input/gs/xxxxx/reference/Homo_sapiens_assembly19.fasta: mkdir /mnt/datadisk/input/gs/xxxxx: no space left on device']; [05/05/2018 21:27:38 ERROR gcp_deepvariant_runner.py] Job failed with error [[u'Error in job make-examp--root--180505-205721-02 - code 5: 9: Failed to localize files: failed to make local directory for /mnt/datadisk/input/gs/xxxxx/reference/Homo_sapiens_assembly19.fasta: mkdir /mnt/datadisk/input/gs/xxxxx: no space left on device']]. Job args: ['--project', 'isb-cgc-06-0004', '--logging', 'gs://xxxxx/wliang_deepvariant/ooooo.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'make_examples', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.6.0', '--input', 'INPUT_BAM=gs://yyyyy.bam', 'INPUT_BAI=gs://yyyyy.bam.bai', 'INPUT_REF=gs://xxxxx/reference/Homo_sapiens_assembly19.fasta', 'INPUT_REF_FAI=gs://xxxxx/reference/Homo_sapiens_assembly19.fasta.fai', '--output-recursive', 'EXAMPLES=gs://xxxxx/wliang_deepvariant/ooooo.stage/examples/0', '--min-cores', '8', '--min-ram', '30', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=0', '--env', 'SHARD_END_INDEX=511', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | parallel --halt 2 \\\n ./make_examples \\\n --mode calling \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz \\\n --reads ""${INPUT_BAM}"" \\\n --ref ""${INPUT_REF}"" \\\n --task {} \\\n \n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-386838731
https://github.com/google/deepvariant/issues/70#issuecomment-386838731:188,Availability,FAILURE,FAILURE,188,"Hi Pi-Chuan,; Thanks. I set `--call_variants_cores_per_worker 8` and new error pops out. It would be great if guys can take a look of it as well.; ```; make-examp--root--180505-205721-02: FAILURE; [u'Error in job make-examp--root--180505-205721-02 - code 5: 9: Failed to localize files: failed to make local directory for /mnt/datadisk/input/gs/xxxxx/reference/Homo_sapiens_assembly19.fasta: mkdir /mnt/datadisk/input/gs/xxxxx: no space left on device']; [05/05/2018 21:27:38 ERROR gcp_deepvariant_runner.py] Job failed with error [[u'Error in job make-examp--root--180505-205721-02 - code 5: 9: Failed to localize files: failed to make local directory for /mnt/datadisk/input/gs/xxxxx/reference/Homo_sapiens_assembly19.fasta: mkdir /mnt/datadisk/input/gs/xxxxx: no space left on device']]. Job args: ['--project', 'isb-cgc-06-0004', '--logging', 'gs://xxxxx/wliang_deepvariant/ooooo.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'make_examples', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.6.0', '--input', 'INPUT_BAM=gs://yyyyy.bam', 'INPUT_BAI=gs://yyyyy.bam.bai', 'INPUT_REF=gs://xxxxx/reference/Homo_sapiens_assembly19.fasta', 'INPUT_REF_FAI=gs://xxxxx/reference/Homo_sapiens_assembly19.fasta.fai', '--output-recursive', 'EXAMPLES=gs://xxxxx/wliang_deepvariant/ooooo.stage/examples/0', '--min-cores', '8', '--min-ram', '30', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=0', '--env', 'SHARD_END_INDEX=511', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | parallel --halt 2 \\\n ./make_examples \\\n --mode calling \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz \\\n --reads ""${INPUT_BAM}"" \\\n --ref ""${INPUT_REF}"" \\\n --task {} \\\n \n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-386838731
https://github.com/google/deepvariant/issues/70#issuecomment-386838731:200,Availability,Error,Error,200,"Hi Pi-Chuan,; Thanks. I set `--call_variants_cores_per_worker 8` and new error pops out. It would be great if guys can take a look of it as well.; ```; make-examp--root--180505-205721-02: FAILURE; [u'Error in job make-examp--root--180505-205721-02 - code 5: 9: Failed to localize files: failed to make local directory for /mnt/datadisk/input/gs/xxxxx/reference/Homo_sapiens_assembly19.fasta: mkdir /mnt/datadisk/input/gs/xxxxx: no space left on device']; [05/05/2018 21:27:38 ERROR gcp_deepvariant_runner.py] Job failed with error [[u'Error in job make-examp--root--180505-205721-02 - code 5: 9: Failed to localize files: failed to make local directory for /mnt/datadisk/input/gs/xxxxx/reference/Homo_sapiens_assembly19.fasta: mkdir /mnt/datadisk/input/gs/xxxxx: no space left on device']]. Job args: ['--project', 'isb-cgc-06-0004', '--logging', 'gs://xxxxx/wliang_deepvariant/ooooo.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'make_examples', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.6.0', '--input', 'INPUT_BAM=gs://yyyyy.bam', 'INPUT_BAI=gs://yyyyy.bam.bai', 'INPUT_REF=gs://xxxxx/reference/Homo_sapiens_assembly19.fasta', 'INPUT_REF_FAI=gs://xxxxx/reference/Homo_sapiens_assembly19.fasta.fai', '--output-recursive', 'EXAMPLES=gs://xxxxx/wliang_deepvariant/ooooo.stage/examples/0', '--min-cores', '8', '--min-ram', '30', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=0', '--env', 'SHARD_END_INDEX=511', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | parallel --halt 2 \\\n ./make_examples \\\n --mode calling \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz \\\n --reads ""${INPUT_BAM}"" \\\n --ref ""${INPUT_REF}"" \\\n --task {} \\\n \n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-386838731
https://github.com/google/deepvariant/issues/70#issuecomment-386838731:476,Availability,ERROR,ERROR,476,"Hi Pi-Chuan,; Thanks. I set `--call_variants_cores_per_worker 8` and new error pops out. It would be great if guys can take a look of it as well.; ```; make-examp--root--180505-205721-02: FAILURE; [u'Error in job make-examp--root--180505-205721-02 - code 5: 9: Failed to localize files: failed to make local directory for /mnt/datadisk/input/gs/xxxxx/reference/Homo_sapiens_assembly19.fasta: mkdir /mnt/datadisk/input/gs/xxxxx: no space left on device']; [05/05/2018 21:27:38 ERROR gcp_deepvariant_runner.py] Job failed with error [[u'Error in job make-examp--root--180505-205721-02 - code 5: 9: Failed to localize files: failed to make local directory for /mnt/datadisk/input/gs/xxxxx/reference/Homo_sapiens_assembly19.fasta: mkdir /mnt/datadisk/input/gs/xxxxx: no space left on device']]. Job args: ['--project', 'isb-cgc-06-0004', '--logging', 'gs://xxxxx/wliang_deepvariant/ooooo.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'make_examples', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.6.0', '--input', 'INPUT_BAM=gs://yyyyy.bam', 'INPUT_BAI=gs://yyyyy.bam.bai', 'INPUT_REF=gs://xxxxx/reference/Homo_sapiens_assembly19.fasta', 'INPUT_REF_FAI=gs://xxxxx/reference/Homo_sapiens_assembly19.fasta.fai', '--output-recursive', 'EXAMPLES=gs://xxxxx/wliang_deepvariant/ooooo.stage/examples/0', '--min-cores', '8', '--min-ram', '30', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=0', '--env', 'SHARD_END_INDEX=511', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | parallel --halt 2 \\\n ./make_examples \\\n --mode calling \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz \\\n --reads ""${INPUT_BAM}"" \\\n --ref ""${INPUT_REF}"" \\\n --task {} \\\n \n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-386838731
https://github.com/google/deepvariant/issues/70#issuecomment-386838731:525,Availability,error,error,525,"Hi Pi-Chuan,; Thanks. I set `--call_variants_cores_per_worker 8` and new error pops out. It would be great if guys can take a look of it as well.; ```; make-examp--root--180505-205721-02: FAILURE; [u'Error in job make-examp--root--180505-205721-02 - code 5: 9: Failed to localize files: failed to make local directory for /mnt/datadisk/input/gs/xxxxx/reference/Homo_sapiens_assembly19.fasta: mkdir /mnt/datadisk/input/gs/xxxxx: no space left on device']; [05/05/2018 21:27:38 ERROR gcp_deepvariant_runner.py] Job failed with error [[u'Error in job make-examp--root--180505-205721-02 - code 5: 9: Failed to localize files: failed to make local directory for /mnt/datadisk/input/gs/xxxxx/reference/Homo_sapiens_assembly19.fasta: mkdir /mnt/datadisk/input/gs/xxxxx: no space left on device']]. Job args: ['--project', 'isb-cgc-06-0004', '--logging', 'gs://xxxxx/wliang_deepvariant/ooooo.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'make_examples', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.6.0', '--input', 'INPUT_BAM=gs://yyyyy.bam', 'INPUT_BAI=gs://yyyyy.bam.bai', 'INPUT_REF=gs://xxxxx/reference/Homo_sapiens_assembly19.fasta', 'INPUT_REF_FAI=gs://xxxxx/reference/Homo_sapiens_assembly19.fasta.fai', '--output-recursive', 'EXAMPLES=gs://xxxxx/wliang_deepvariant/ooooo.stage/examples/0', '--min-cores', '8', '--min-ram', '30', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=0', '--env', 'SHARD_END_INDEX=511', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | parallel --halt 2 \\\n ./make_examples \\\n --mode calling \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz \\\n --reads ""${INPUT_BAM}"" \\\n --ref ""${INPUT_REF}"" \\\n --task {} \\\n \n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-386838731
https://github.com/google/deepvariant/issues/70#issuecomment-386838731:535,Availability,Error,Error,535,"Hi Pi-Chuan,; Thanks. I set `--call_variants_cores_per_worker 8` and new error pops out. It would be great if guys can take a look of it as well.; ```; make-examp--root--180505-205721-02: FAILURE; [u'Error in job make-examp--root--180505-205721-02 - code 5: 9: Failed to localize files: failed to make local directory for /mnt/datadisk/input/gs/xxxxx/reference/Homo_sapiens_assembly19.fasta: mkdir /mnt/datadisk/input/gs/xxxxx: no space left on device']; [05/05/2018 21:27:38 ERROR gcp_deepvariant_runner.py] Job failed with error [[u'Error in job make-examp--root--180505-205721-02 - code 5: 9: Failed to localize files: failed to make local directory for /mnt/datadisk/input/gs/xxxxx/reference/Homo_sapiens_assembly19.fasta: mkdir /mnt/datadisk/input/gs/xxxxx: no space left on device']]. Job args: ['--project', 'isb-cgc-06-0004', '--logging', 'gs://xxxxx/wliang_deepvariant/ooooo.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'make_examples', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.6.0', '--input', 'INPUT_BAM=gs://yyyyy.bam', 'INPUT_BAI=gs://yyyyy.bam.bai', 'INPUT_REF=gs://xxxxx/reference/Homo_sapiens_assembly19.fasta', 'INPUT_REF_FAI=gs://xxxxx/reference/Homo_sapiens_assembly19.fasta.fai', '--output-recursive', 'EXAMPLES=gs://xxxxx/wliang_deepvariant/ooooo.stage/examples/0', '--min-cores', '8', '--min-ram', '30', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=0', '--env', 'SHARD_END_INDEX=511', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | parallel --halt 2 \\\n ./make_examples \\\n --mode calling \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz \\\n --reads ""${INPUT_BAM}"" \\\n --ref ""${INPUT_REF}"" \\\n --task {} \\\n \n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-386838731
https://github.com/google/deepvariant/issues/70#issuecomment-386838731:2275,Availability,error,error,2275,"t--180505-205721-02 - code 5: 9: Failed to localize files: failed to make local directory for /mnt/datadisk/input/gs/xxxxx/reference/Homo_sapiens_assembly19.fasta: mkdir /mnt/datadisk/input/gs/xxxxx: no space left on device']]. Job args: ['--project', 'isb-cgc-06-0004', '--logging', 'gs://xxxxx/wliang_deepvariant/ooooo.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'make_examples', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.6.0', '--input', 'INPUT_BAM=gs://yyyyy.bam', 'INPUT_BAI=gs://yyyyy.bam.bai', 'INPUT_REF=gs://xxxxx/reference/Homo_sapiens_assembly19.fasta', 'INPUT_REF_FAI=gs://xxxxx/reference/Homo_sapiens_assembly19.fasta.fai', '--output-recursive', 'EXAMPLES=gs://xxxxx/wliang_deepvariant/ooooo.stage/examples/0', '--min-cores', '8', '--min-ram', '30', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=0', '--env', 'SHARD_END_INDEX=511', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | parallel --halt 2 \\\n ./make_examples \\\n --mode calling \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz \\\n --reads ""${INPUT_BAM}"" \\\n --ref ""${INPUT_REF}"" \\\n --task {} \\\n \n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 638, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 266, in _run_make_examples; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 567, in get; raise self._value; RuntimeError: Job failed with error [[u'Error in job make-examp--root--180505-205721-02 - code 5: 9: Failed to localize files: failed to make local directory for /mnt/datadisk/input/gs/xxxxx/reference/Homo_sapiens_assembly19.fasta: mkdir /mnt/datadisk/input/gs/xxxxxx: no space left on device']].; (exit status 1); ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-386838731
https://github.com/google/deepvariant/issues/70#issuecomment-386838731:2285,Availability,Error,Error,2285,"t--180505-205721-02 - code 5: 9: Failed to localize files: failed to make local directory for /mnt/datadisk/input/gs/xxxxx/reference/Homo_sapiens_assembly19.fasta: mkdir /mnt/datadisk/input/gs/xxxxx: no space left on device']]. Job args: ['--project', 'isb-cgc-06-0004', '--logging', 'gs://xxxxx/wliang_deepvariant/ooooo.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'make_examples', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.6.0', '--input', 'INPUT_BAM=gs://yyyyy.bam', 'INPUT_BAI=gs://yyyyy.bam.bai', 'INPUT_REF=gs://xxxxx/reference/Homo_sapiens_assembly19.fasta', 'INPUT_REF_FAI=gs://xxxxx/reference/Homo_sapiens_assembly19.fasta.fai', '--output-recursive', 'EXAMPLES=gs://xxxxx/wliang_deepvariant/ooooo.stage/examples/0', '--min-cores', '8', '--min-ram', '30', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=0', '--env', 'SHARD_END_INDEX=511', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | parallel --halt 2 \\\n ./make_examples \\\n --mode calling \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz \\\n --reads ""${INPUT_BAM}"" \\\n --ref ""${INPUT_REF}"" \\\n --task {} \\\n \n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 638, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 266, in _run_make_examples; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 567, in get; raise self._value; RuntimeError: Job failed with error [[u'Error in job make-examp--root--180505-205721-02 - code 5: 9: Failed to localize files: failed to make local directory for /mnt/datadisk/input/gs/xxxxx/reference/Homo_sapiens_assembly19.fasta: mkdir /mnt/datadisk/input/gs/xxxxxx: no space left on device']].; (exit status 1); ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-386838731
https://github.com/google/deepvariant/issues/70#issuecomment-386838731:837,Testability,log,logging,837,"Hi Pi-Chuan,; Thanks. I set `--call_variants_cores_per_worker 8` and new error pops out. It would be great if guys can take a look of it as well.; ```; make-examp--root--180505-205721-02: FAILURE; [u'Error in job make-examp--root--180505-205721-02 - code 5: 9: Failed to localize files: failed to make local directory for /mnt/datadisk/input/gs/xxxxx/reference/Homo_sapiens_assembly19.fasta: mkdir /mnt/datadisk/input/gs/xxxxx: no space left on device']; [05/05/2018 21:27:38 ERROR gcp_deepvariant_runner.py] Job failed with error [[u'Error in job make-examp--root--180505-205721-02 - code 5: 9: Failed to localize files: failed to make local directory for /mnt/datadisk/input/gs/xxxxx/reference/Homo_sapiens_assembly19.fasta: mkdir /mnt/datadisk/input/gs/xxxxx: no space left on device']]. Job args: ['--project', 'isb-cgc-06-0004', '--logging', 'gs://xxxxx/wliang_deepvariant/ooooo.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'make_examples', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.6.0', '--input', 'INPUT_BAM=gs://yyyyy.bam', 'INPUT_BAI=gs://yyyyy.bam.bai', 'INPUT_REF=gs://xxxxx/reference/Homo_sapiens_assembly19.fasta', 'INPUT_REF_FAI=gs://xxxxx/reference/Homo_sapiens_assembly19.fasta.fai', '--output-recursive', 'EXAMPLES=gs://xxxxx/wliang_deepvariant/ooooo.stage/examples/0', '--min-cores', '8', '--min-ram', '30', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=0', '--env', 'SHARD_END_INDEX=511', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | parallel --halt 2 \\\n ./make_examples \\\n --mode calling \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz \\\n --reads ""${INPUT_BAM}"" \\\n --ref ""${INPUT_REF}"" \\\n --task {} \\\n \n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-386838731
https://github.com/google/deepvariant/issues/70#issuecomment-386838731:890,Testability,log,logs,890,"Hi Pi-Chuan,; Thanks. I set `--call_variants_cores_per_worker 8` and new error pops out. It would be great if guys can take a look of it as well.; ```; make-examp--root--180505-205721-02: FAILURE; [u'Error in job make-examp--root--180505-205721-02 - code 5: 9: Failed to localize files: failed to make local directory for /mnt/datadisk/input/gs/xxxxx/reference/Homo_sapiens_assembly19.fasta: mkdir /mnt/datadisk/input/gs/xxxxx: no space left on device']; [05/05/2018 21:27:38 ERROR gcp_deepvariant_runner.py] Job failed with error [[u'Error in job make-examp--root--180505-205721-02 - code 5: 9: Failed to localize files: failed to make local directory for /mnt/datadisk/input/gs/xxxxx/reference/Homo_sapiens_assembly19.fasta: mkdir /mnt/datadisk/input/gs/xxxxx: no space left on device']]. Job args: ['--project', 'isb-cgc-06-0004', '--logging', 'gs://xxxxx/wliang_deepvariant/ooooo.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'make_examples', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.6.0', '--input', 'INPUT_BAM=gs://yyyyy.bam', 'INPUT_BAI=gs://yyyyy.bam.bai', 'INPUT_REF=gs://xxxxx/reference/Homo_sapiens_assembly19.fasta', 'INPUT_REF_FAI=gs://xxxxx/reference/Homo_sapiens_assembly19.fasta.fai', '--output-recursive', 'EXAMPLES=gs://xxxxx/wliang_deepvariant/ooooo.stage/examples/0', '--min-cores', '8', '--min-ram', '30', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=0', '--env', 'SHARD_END_INDEX=511', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | parallel --halt 2 \\\n ./make_examples \\\n --mode calling \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz \\\n --reads ""${INPUT_BAM}"" \\\n --ref ""${INPUT_REF}"" \\\n --task {} \\\n \n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-386838731
https://github.com/google/deepvariant/issues/70#issuecomment-387174391:243,Deployability,pipeline,pipeline,243,"Hi,; for your question in: https://github.com/google/deepvariant/issues/70#issuecomment-386838731; you'll need to set `--make_examples_disk_per_worker_gb` to a large enough value based on the size of their BAMs. We set it to 200gb for the WGS pipeline (it's set to 50gb by default; for quickstart). You'll need to change that if you have a really large BAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387174391
https://github.com/google/deepvariant/issues/70#issuecomment-387177566:30,Deployability,release,release,30,The issue is fixed with 0.6.1 release of docker images. Please update image version to. `IMAGE_VERSION=0.6.1`. See https://cloud.google.com/genomics/deepvariant for more info.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387177566
https://github.com/google/deepvariant/issues/70#issuecomment-387177566:63,Deployability,update,update,63,The issue is fixed with 0.6.1 release of docker images. Please update image version to. `IMAGE_VERSION=0.6.1`. See https://cloud.google.com/genomics/deepvariant for more info.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387177566
https://github.com/google/deepvariant/issues/70#issuecomment-387564029:102,Availability,error,error,102,"Hi Pi-Chuan, ; I used 0.6.1 docker images and also set `--call_variants_cores_per_worker 8`, but same error pops out. ```; call-varia--root--180508-211940-52: FAILURE; [u""Error in job call-varia--root--180508-211940-52 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]; [05/08/2018 21:19:51 ERROR gcp_deepvariant_runner.py] Job failed with error [[u""Error in job call-varia--root--180508-211940-52 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]]. Job args: ['--project', 'isb-cgc-06-0004', '--logging', 'gs://WWWWW/wliang_deepvariant/XXXXX.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'call_variants', '--image', 'gcr.io/deepvariant-docker/deepvariant_gpu:0.6.1', '--input-recursive', 'EXAMPLES=gs://WWWWW/wliang_deepvariant/XXXXX.stage/examples/15', 'MODEL=gs://deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard', '--output-recursive', 'CALLED_VARIANTS=gs://WWWWW/wliang_deepvariant/XXXXX.stage/called_variants', '--min-cores', '8', '--min-ram', '60', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=480', '--env', 'SHARD_END_INDEX=511', '--env', 'CONCURRENT_JOBS=1', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq -f ""%05g"" ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | \\\nparallel --jobs ""${CONCURRENT_JOBS}"" --halt 2 \\\n./call_variants \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --checkpoint ""${MODEL}""/model.ckpt\n', '--accelerator-type', 'nvidia-te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387564029
https://github.com/google/deepvariant/issues/70#issuecomment-387564029:159,Availability,FAILURE,FAILURE,159,"Hi Pi-Chuan, ; I used 0.6.1 docker images and also set `--call_variants_cores_per_worker 8`, but same error pops out. ```; call-varia--root--180508-211940-52: FAILURE; [u""Error in job call-varia--root--180508-211940-52 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]; [05/08/2018 21:19:51 ERROR gcp_deepvariant_runner.py] Job failed with error [[u""Error in job call-varia--root--180508-211940-52 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]]. Job args: ['--project', 'isb-cgc-06-0004', '--logging', 'gs://WWWWW/wliang_deepvariant/XXXXX.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'call_variants', '--image', 'gcr.io/deepvariant-docker/deepvariant_gpu:0.6.1', '--input-recursive', 'EXAMPLES=gs://WWWWW/wliang_deepvariant/XXXXX.stage/examples/15', 'MODEL=gs://deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard', '--output-recursive', 'CALLED_VARIANTS=gs://WWWWW/wliang_deepvariant/XXXXX.stage/called_variants', '--min-cores', '8', '--min-ram', '60', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=480', '--env', 'SHARD_END_INDEX=511', '--env', 'CONCURRENT_JOBS=1', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq -f ""%05g"" ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | \\\nparallel --jobs ""${CONCURRENT_JOBS}"" --halt 2 \\\n./call_variants \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --checkpoint ""${MODEL}""/model.ckpt\n', '--accelerator-type', 'nvidia-te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387564029
https://github.com/google/deepvariant/issues/70#issuecomment-387564029:171,Availability,Error,Error,171,"Hi Pi-Chuan, ; I used 0.6.1 docker images and also set `--call_variants_cores_per_worker 8`, but same error pops out. ```; call-varia--root--180508-211940-52: FAILURE; [u""Error in job call-varia--root--180508-211940-52 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]; [05/08/2018 21:19:51 ERROR gcp_deepvariant_runner.py] Job failed with error [[u""Error in job call-varia--root--180508-211940-52 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]]. Job args: ['--project', 'isb-cgc-06-0004', '--logging', 'gs://WWWWW/wliang_deepvariant/XXXXX.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'call_variants', '--image', 'gcr.io/deepvariant-docker/deepvariant_gpu:0.6.1', '--input-recursive', 'EXAMPLES=gs://WWWWW/wliang_deepvariant/XXXXX.stage/examples/15', 'MODEL=gs://deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard', '--output-recursive', 'CALLED_VARIANTS=gs://WWWWW/wliang_deepvariant/XXXXX.stage/called_variants', '--min-cores', '8', '--min-ram', '60', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=480', '--env', 'SHARD_END_INDEX=511', '--env', 'CONCURRENT_JOBS=1', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq -f ""%05g"" ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | \\\nparallel --jobs ""${CONCURRENT_JOBS}"" --halt 2 \\\n./call_variants \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --checkpoint ""${MODEL}""/model.ckpt\n', '--accelerator-type', 'nvidia-te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387564029
https://github.com/google/deepvariant/issues/70#issuecomment-387564029:267,Availability,Error,Error,267,"Hi Pi-Chuan, ; I used 0.6.1 docker images and also set `--call_variants_cores_per_worker 8`, but same error pops out. ```; call-varia--root--180508-211940-52: FAILURE; [u""Error in job call-varia--root--180508-211940-52 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]; [05/08/2018 21:19:51 ERROR gcp_deepvariant_runner.py] Job failed with error [[u""Error in job call-varia--root--180508-211940-52 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]]. Job args: ['--project', 'isb-cgc-06-0004', '--logging', 'gs://WWWWW/wliang_deepvariant/XXXXX.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'call_variants', '--image', 'gcr.io/deepvariant-docker/deepvariant_gpu:0.6.1', '--input-recursive', 'EXAMPLES=gs://WWWWW/wliang_deepvariant/XXXXX.stage/examples/15', 'MODEL=gs://deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard', '--output-recursive', 'CALLED_VARIANTS=gs://WWWWW/wliang_deepvariant/XXXXX.stage/called_variants', '--min-cores', '8', '--min-ram', '60', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=480', '--env', 'SHARD_END_INDEX=511', '--env', 'CONCURRENT_JOBS=1', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq -f ""%05g"" ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | \\\nparallel --jobs ""${CONCURRENT_JOBS}"" --halt 2 \\\n./call_variants \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --checkpoint ""${MODEL}""/model.ckpt\n', '--accelerator-type', 'nvidia-te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387564029
https://github.com/google/deepvariant/issues/70#issuecomment-387564029:465,Availability,ERROR,ERROR,465,"Hi Pi-Chuan, ; I used 0.6.1 docker images and also set `--call_variants_cores_per_worker 8`, but same error pops out. ```; call-varia--root--180508-211940-52: FAILURE; [u""Error in job call-varia--root--180508-211940-52 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]; [05/08/2018 21:19:51 ERROR gcp_deepvariant_runner.py] Job failed with error [[u""Error in job call-varia--root--180508-211940-52 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]]. Job args: ['--project', 'isb-cgc-06-0004', '--logging', 'gs://WWWWW/wliang_deepvariant/XXXXX.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'call_variants', '--image', 'gcr.io/deepvariant-docker/deepvariant_gpu:0.6.1', '--input-recursive', 'EXAMPLES=gs://WWWWW/wliang_deepvariant/XXXXX.stage/examples/15', 'MODEL=gs://deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard', '--output-recursive', 'CALLED_VARIANTS=gs://WWWWW/wliang_deepvariant/XXXXX.stage/called_variants', '--min-cores', '8', '--min-ram', '60', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=480', '--env', 'SHARD_END_INDEX=511', '--env', 'CONCURRENT_JOBS=1', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq -f ""%05g"" ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | \\\nparallel --jobs ""${CONCURRENT_JOBS}"" --halt 2 \\\n./call_variants \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --checkpoint ""${MODEL}""/model.ckpt\n', '--accelerator-type', 'nvidia-te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387564029
https://github.com/google/deepvariant/issues/70#issuecomment-387564029:514,Availability,error,error,514,"Hi Pi-Chuan, ; I used 0.6.1 docker images and also set `--call_variants_cores_per_worker 8`, but same error pops out. ```; call-varia--root--180508-211940-52: FAILURE; [u""Error in job call-varia--root--180508-211940-52 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]; [05/08/2018 21:19:51 ERROR gcp_deepvariant_runner.py] Job failed with error [[u""Error in job call-varia--root--180508-211940-52 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]]. Job args: ['--project', 'isb-cgc-06-0004', '--logging', 'gs://WWWWW/wliang_deepvariant/XXXXX.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'call_variants', '--image', 'gcr.io/deepvariant-docker/deepvariant_gpu:0.6.1', '--input-recursive', 'EXAMPLES=gs://WWWWW/wliang_deepvariant/XXXXX.stage/examples/15', 'MODEL=gs://deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard', '--output-recursive', 'CALLED_VARIANTS=gs://WWWWW/wliang_deepvariant/XXXXX.stage/called_variants', '--min-cores', '8', '--min-ram', '60', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=480', '--env', 'SHARD_END_INDEX=511', '--env', 'CONCURRENT_JOBS=1', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq -f ""%05g"" ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | \\\nparallel --jobs ""${CONCURRENT_JOBS}"" --halt 2 \\\n./call_variants \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --checkpoint ""${MODEL}""/model.ckpt\n', '--accelerator-type', 'nvidia-te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387564029
https://github.com/google/deepvariant/issues/70#issuecomment-387564029:524,Availability,Error,Error,524,"Hi Pi-Chuan, ; I used 0.6.1 docker images and also set `--call_variants_cores_per_worker 8`, but same error pops out. ```; call-varia--root--180508-211940-52: FAILURE; [u""Error in job call-varia--root--180508-211940-52 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]; [05/08/2018 21:19:51 ERROR gcp_deepvariant_runner.py] Job failed with error [[u""Error in job call-varia--root--180508-211940-52 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]]. Job args: ['--project', 'isb-cgc-06-0004', '--logging', 'gs://WWWWW/wliang_deepvariant/XXXXX.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'call_variants', '--image', 'gcr.io/deepvariant-docker/deepvariant_gpu:0.6.1', '--input-recursive', 'EXAMPLES=gs://WWWWW/wliang_deepvariant/XXXXX.stage/examples/15', 'MODEL=gs://deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard', '--output-recursive', 'CALLED_VARIANTS=gs://WWWWW/wliang_deepvariant/XXXXX.stage/called_variants', '--min-cores', '8', '--min-ram', '60', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=480', '--env', 'SHARD_END_INDEX=511', '--env', 'CONCURRENT_JOBS=1', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq -f ""%05g"" ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | \\\nparallel --jobs ""${CONCURRENT_JOBS}"" --halt 2 \\\n./call_variants \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --checkpoint ""${MODEL}""/model.ckpt\n', '--accelerator-type', 'nvidia-te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387564029
https://github.com/google/deepvariant/issues/70#issuecomment-387564029:620,Availability,Error,Error,620,"Hi Pi-Chuan, ; I used 0.6.1 docker images and also set `--call_variants_cores_per_worker 8`, but same error pops out. ```; call-varia--root--180508-211940-52: FAILURE; [u""Error in job call-varia--root--180508-211940-52 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]; [05/08/2018 21:19:51 ERROR gcp_deepvariant_runner.py] Job failed with error [[u""Error in job call-varia--root--180508-211940-52 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]]. Job args: ['--project', 'isb-cgc-06-0004', '--logging', 'gs://WWWWW/wliang_deepvariant/XXXXX.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'call_variants', '--image', 'gcr.io/deepvariant-docker/deepvariant_gpu:0.6.1', '--input-recursive', 'EXAMPLES=gs://WWWWW/wliang_deepvariant/XXXXX.stage/examples/15', 'MODEL=gs://deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard', '--output-recursive', 'CALLED_VARIANTS=gs://WWWWW/wliang_deepvariant/XXXXX.stage/called_variants', '--min-cores', '8', '--min-ram', '60', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=480', '--env', 'SHARD_END_INDEX=511', '--env', 'CONCURRENT_JOBS=1', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq -f ""%05g"" ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | \\\nparallel --jobs ""${CONCURRENT_JOBS}"" --halt 2 \\\n./call_variants \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --checkpoint ""${MODEL}""/model.ckpt\n', '--accelerator-type', 'nvidia-te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387564029
https://github.com/google/deepvariant/issues/70#issuecomment-387564029:1932,Availability,checkpoint,checkpoint,1932,"WWW/wliang_deepvariant/XXXXX.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'call_variants', '--image', 'gcr.io/deepvariant-docker/deepvariant_gpu:0.6.1', '--input-recursive', 'EXAMPLES=gs://WWWWW/wliang_deepvariant/XXXXX.stage/examples/15', 'MODEL=gs://deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard', '--output-recursive', 'CALLED_VARIANTS=gs://WWWWW/wliang_deepvariant/XXXXX.stage/called_variants', '--min-cores', '8', '--min-ram', '60', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=480', '--env', 'SHARD_END_INDEX=511', '--env', 'CONCURRENT_JOBS=1', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq -f ""%05g"" ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | \\\nparallel --jobs ""${CONCURRENT_JOBS}"" --halt 2 \\\n./call_variants \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --checkpoint ""${MODEL}""/model.ckpt\n', '--accelerator-type', 'nvidia-tesla-k80', '--accelerator-count', '1']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 642, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 322, in _run_call_variants; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 567, in get; raise self._value; RuntimeError: Job failed with error [[u""Error in job call-varia--root--180508-211910-49 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]].; (exit status 1); ```. Would you mind taking a look of this? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387564029
https://github.com/google/deepvariant/issues/70#issuecomment-387564029:2510,Availability,error,error,2510,"WWW/wliang_deepvariant/XXXXX.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'call_variants', '--image', 'gcr.io/deepvariant-docker/deepvariant_gpu:0.6.1', '--input-recursive', 'EXAMPLES=gs://WWWWW/wliang_deepvariant/XXXXX.stage/examples/15', 'MODEL=gs://deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard', '--output-recursive', 'CALLED_VARIANTS=gs://WWWWW/wliang_deepvariant/XXXXX.stage/called_variants', '--min-cores', '8', '--min-ram', '60', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=480', '--env', 'SHARD_END_INDEX=511', '--env', 'CONCURRENT_JOBS=1', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq -f ""%05g"" ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | \\\nparallel --jobs ""${CONCURRENT_JOBS}"" --halt 2 \\\n./call_variants \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --checkpoint ""${MODEL}""/model.ckpt\n', '--accelerator-type', 'nvidia-tesla-k80', '--accelerator-count', '1']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 642, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 322, in _run_call_variants; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 567, in get; raise self._value; RuntimeError: Job failed with error [[u""Error in job call-varia--root--180508-211910-49 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]].; (exit status 1); ```. Would you mind taking a look of this? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387564029
https://github.com/google/deepvariant/issues/70#issuecomment-387564029:2520,Availability,Error,Error,2520,"WWW/wliang_deepvariant/XXXXX.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'call_variants', '--image', 'gcr.io/deepvariant-docker/deepvariant_gpu:0.6.1', '--input-recursive', 'EXAMPLES=gs://WWWWW/wliang_deepvariant/XXXXX.stage/examples/15', 'MODEL=gs://deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard', '--output-recursive', 'CALLED_VARIANTS=gs://WWWWW/wliang_deepvariant/XXXXX.stage/called_variants', '--min-cores', '8', '--min-ram', '60', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=480', '--env', 'SHARD_END_INDEX=511', '--env', 'CONCURRENT_JOBS=1', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq -f ""%05g"" ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | \\\nparallel --jobs ""${CONCURRENT_JOBS}"" --halt 2 \\\n./call_variants \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --checkpoint ""${MODEL}""/model.ckpt\n', '--accelerator-type', 'nvidia-tesla-k80', '--accelerator-count', '1']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 642, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 322, in _run_call_variants; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 567, in get; raise self._value; RuntimeError: Job failed with error [[u""Error in job call-varia--root--180508-211910-49 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]].; (exit status 1); ```. Would you mind taking a look of this? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387564029
https://github.com/google/deepvariant/issues/70#issuecomment-387564029:2616,Availability,Error,Error,2616,"WWW/wliang_deepvariant/XXXXX.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'call_variants', '--image', 'gcr.io/deepvariant-docker/deepvariant_gpu:0.6.1', '--input-recursive', 'EXAMPLES=gs://WWWWW/wliang_deepvariant/XXXXX.stage/examples/15', 'MODEL=gs://deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard', '--output-recursive', 'CALLED_VARIANTS=gs://WWWWW/wliang_deepvariant/XXXXX.stage/called_variants', '--min-cores', '8', '--min-ram', '60', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=480', '--env', 'SHARD_END_INDEX=511', '--env', 'CONCURRENT_JOBS=1', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq -f ""%05g"" ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | \\\nparallel --jobs ""${CONCURRENT_JOBS}"" --halt 2 \\\n./call_variants \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --checkpoint ""${MODEL}""/model.ckpt\n', '--accelerator-type', 'nvidia-tesla-k80', '--accelerator-count', '1']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 642, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 322, in _run_call_variants; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 567, in get; raise self._value; RuntimeError: Job failed with error [[u""Error in job call-varia--root--180508-211910-49 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]].; (exit status 1); ```. Would you mind taking a look of this? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387564029
https://github.com/google/deepvariant/issues/70#issuecomment-387564029:844,Testability,log,logging,844,"Hi Pi-Chuan, ; I used 0.6.1 docker images and also set `--call_variants_cores_per_worker 8`, but same error pops out. ```; call-varia--root--180508-211940-52: FAILURE; [u""Error in job call-varia--root--180508-211940-52 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]; [05/08/2018 21:19:51 ERROR gcp_deepvariant_runner.py] Job failed with error [[u""Error in job call-varia--root--180508-211940-52 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]]. Job args: ['--project', 'isb-cgc-06-0004', '--logging', 'gs://WWWWW/wliang_deepvariant/XXXXX.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'call_variants', '--image', 'gcr.io/deepvariant-docker/deepvariant_gpu:0.6.1', '--input-recursive', 'EXAMPLES=gs://WWWWW/wliang_deepvariant/XXXXX.stage/examples/15', 'MODEL=gs://deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard', '--output-recursive', 'CALLED_VARIANTS=gs://WWWWW/wliang_deepvariant/XXXXX.stage/called_variants', '--min-cores', '8', '--min-ram', '60', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=480', '--env', 'SHARD_END_INDEX=511', '--env', 'CONCURRENT_JOBS=1', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq -f ""%05g"" ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | \\\nparallel --jobs ""${CONCURRENT_JOBS}"" --halt 2 \\\n./call_variants \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --checkpoint ""${MODEL}""/model.ckpt\n', '--accelerator-type', 'nvidia-te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387564029
https://github.com/google/deepvariant/issues/70#issuecomment-387564029:897,Testability,log,logs,897,"Hi Pi-Chuan, ; I used 0.6.1 docker images and also set `--call_variants_cores_per_worker 8`, but same error pops out. ```; call-varia--root--180508-211940-52: FAILURE; [u""Error in job call-varia--root--180508-211940-52 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]; [05/08/2018 21:19:51 ERROR gcp_deepvariant_runner.py] Job failed with error [[u""Error in job call-varia--root--180508-211940-52 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]]. Job args: ['--project', 'isb-cgc-06-0004', '--logging', 'gs://WWWWW/wliang_deepvariant/XXXXX.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'call_variants', '--image', 'gcr.io/deepvariant-docker/deepvariant_gpu:0.6.1', '--input-recursive', 'EXAMPLES=gs://WWWWW/wliang_deepvariant/XXXXX.stage/examples/15', 'MODEL=gs://deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard', '--output-recursive', 'CALLED_VARIANTS=gs://WWWWW/wliang_deepvariant/XXXXX.stage/called_variants', '--min-cores', '8', '--min-ram', '60', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=480', '--env', 'SHARD_END_INDEX=511', '--env', 'CONCURRENT_JOBS=1', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq -f ""%05g"" ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | \\\nparallel --jobs ""${CONCURRENT_JOBS}"" --halt 2 \\\n./call_variants \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --checkpoint ""${MODEL}""/model.ckpt\n', '--accelerator-type', 'nvidia-te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387564029
https://github.com/google/deepvariant/issues/70#issuecomment-387572014:45,Modifiability,config,config,45,"Please share the full runner log, as well as config (YAML) file used.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387572014
https://github.com/google/deepvariant/issues/70#issuecomment-387572014:29,Testability,log,log,29,"Please share the full runner log, as well as config (YAML) file used.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387572014
https://github.com/google/deepvariant/issues/70#issuecomment-387605005:30,Testability,log,log,30,The following are full runner log. ; [ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log](https://github.com/google/deepvariant/files/1986206/ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log); [ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log](https://github.com/google/deepvariant/files/1986207/ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log); [ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log](https://github.com/google/deepvariant/files/1986208/ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log). Please find the yaml file and the runner bash script here: https://github.com/ding-lab/RegulatoryGermline/tree/master/deepvariant. Thanks.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387605005
https://github.com/google/deepvariant/issues/70#issuecomment-387605005:102,Testability,log,log,102,The following are full runner log. ; [ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log](https://github.com/google/deepvariant/files/1986206/ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log); [ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log](https://github.com/google/deepvariant/files/1986207/ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log); [ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log](https://github.com/google/deepvariant/files/1986208/ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log). Please find the yaml file and the runner bash script here: https://github.com/ding-lab/RegulatoryGermline/tree/master/deepvariant. Thanks.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387605005
https://github.com/google/deepvariant/issues/70#issuecomment-387605005:223,Testability,log,log,223,The following are full runner log. ; [ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log](https://github.com/google/deepvariant/files/1986206/ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log); [ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log](https://github.com/google/deepvariant/files/1986207/ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log); [ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log](https://github.com/google/deepvariant/files/1986208/ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log). Please find the yaml file and the runner bash script here: https://github.com/ding-lab/RegulatoryGermline/tree/master/deepvariant. Thanks.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387605005
https://github.com/google/deepvariant/issues/70#issuecomment-387605005:294,Testability,log,log,294,The following are full runner log. ; [ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log](https://github.com/google/deepvariant/files/1986206/ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log); [ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log](https://github.com/google/deepvariant/files/1986207/ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log); [ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log](https://github.com/google/deepvariant/files/1986208/ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log). Please find the yaml file and the runner bash script here: https://github.com/ding-lab/RegulatoryGermline/tree/master/deepvariant. Thanks.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387605005
https://github.com/google/deepvariant/issues/70#issuecomment-387605005:415,Testability,log,log,415,The following are full runner log. ; [ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log](https://github.com/google/deepvariant/files/1986206/ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log); [ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log](https://github.com/google/deepvariant/files/1986207/ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log); [ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log](https://github.com/google/deepvariant/files/1986208/ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log). Please find the yaml file and the runner bash script here: https://github.com/ding-lab/RegulatoryGermline/tree/master/deepvariant. Thanks.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387605005
https://github.com/google/deepvariant/issues/70#issuecomment-387605005:479,Testability,log,log,479,The following are full runner log. ; [ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log](https://github.com/google/deepvariant/files/1986206/ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log); [ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log](https://github.com/google/deepvariant/files/1986207/ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log); [ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log](https://github.com/google/deepvariant/files/1986208/ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log). Please find the yaml file and the runner bash script here: https://github.com/ding-lab/RegulatoryGermline/tree/master/deepvariant. Thanks.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387605005
https://github.com/google/deepvariant/issues/70#issuecomment-387605005:593,Testability,log,log,593,The following are full runner log. ; [ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log](https://github.com/google/deepvariant/files/1986206/ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log); [ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log](https://github.com/google/deepvariant/files/1986207/ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log); [ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log](https://github.com/google/deepvariant/files/1986208/ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log). Please find the yaml file and the runner bash script here: https://github.com/ding-lab/RegulatoryGermline/tree/master/deepvariant. Thanks.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-387605005
https://github.com/google/deepvariant/issues/70#issuecomment-388183666:95,Availability,avail,available,95,"Hi Nima, ; I set the `--call_variants_ram_per_worker_gb 30`, and it seems like resource is not available. ; ```; call-varia--root--180510-193828-03: SUCCESS; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 642, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 322, in _run_call_variants; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 567, in get; raise self._value; RuntimeError: Job failed with error [[u""Error in job call-varia--root--180510-193818-02 - code 2: failed to insert instance: googleapi: Error 404: The resource 'projects/isb-cgc-06-0004/zones/us-west1-a/acceleratorTypes/nvidia-tesla-k80' was not found, notFound""]].; (exit status 1); ```. Here are the full runner log. You can find out the yam file in the same repo.; [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log](https://github.com/google/deepvariant/files/1993190/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log); [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log](https://github.com/google/deepvariant/files/1993191/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log); [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log](https://github.com/google/deepvariant/files/1993192/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-388183666
https://github.com/google/deepvariant/issues/70#issuecomment-388183666:628,Availability,error,error,628,"Hi Nima, ; I set the `--call_variants_ram_per_worker_gb 30`, and it seems like resource is not available. ; ```; call-varia--root--180510-193828-03: SUCCESS; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 642, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 322, in _run_call_variants; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 567, in get; raise self._value; RuntimeError: Job failed with error [[u""Error in job call-varia--root--180510-193818-02 - code 2: failed to insert instance: googleapi: Error 404: The resource 'projects/isb-cgc-06-0004/zones/us-west1-a/acceleratorTypes/nvidia-tesla-k80' was not found, notFound""]].; (exit status 1); ```. Here are the full runner log. You can find out the yam file in the same repo.; [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log](https://github.com/google/deepvariant/files/1993190/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log); [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log](https://github.com/google/deepvariant/files/1993191/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log); [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log](https://github.com/google/deepvariant/files/1993192/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-388183666
https://github.com/google/deepvariant/issues/70#issuecomment-388183666:638,Availability,Error,Error,638,"Hi Nima, ; I set the `--call_variants_ram_per_worker_gb 30`, and it seems like resource is not available. ; ```; call-varia--root--180510-193828-03: SUCCESS; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 642, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 322, in _run_call_variants; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 567, in get; raise self._value; RuntimeError: Job failed with error [[u""Error in job call-varia--root--180510-193818-02 - code 2: failed to insert instance: googleapi: Error 404: The resource 'projects/isb-cgc-06-0004/zones/us-west1-a/acceleratorTypes/nvidia-tesla-k80' was not found, notFound""]].; (exit status 1); ```. Here are the full runner log. You can find out the yam file in the same repo.; [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log](https://github.com/google/deepvariant/files/1993190/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log); [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log](https://github.com/google/deepvariant/files/1993191/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log); [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log](https://github.com/google/deepvariant/files/1993192/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-388183666
https://github.com/google/deepvariant/issues/70#issuecomment-388183666:734,Availability,Error,Error,734,"Hi Nima, ; I set the `--call_variants_ram_per_worker_gb 30`, and it seems like resource is not available. ; ```; call-varia--root--180510-193828-03: SUCCESS; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 642, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 322, in _run_call_variants; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 567, in get; raise self._value; RuntimeError: Job failed with error [[u""Error in job call-varia--root--180510-193818-02 - code 2: failed to insert instance: googleapi: Error 404: The resource 'projects/isb-cgc-06-0004/zones/us-west1-a/acceleratorTypes/nvidia-tesla-k80' was not found, notFound""]].; (exit status 1); ```. Here are the full runner log. You can find out the yam file in the same repo.; [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log](https://github.com/google/deepvariant/files/1993190/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log); [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log](https://github.com/google/deepvariant/files/1993191/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log); [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log](https://github.com/google/deepvariant/files/1993192/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-388183666
https://github.com/google/deepvariant/issues/70#issuecomment-388183666:912,Testability,log,log,912,"Hi Nima, ; I set the `--call_variants_ram_per_worker_gb 30`, and it seems like resource is not available. ; ```; call-varia--root--180510-193828-03: SUCCESS; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 642, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 322, in _run_call_variants; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 567, in get; raise self._value; RuntimeError: Job failed with error [[u""Error in job call-varia--root--180510-193818-02 - code 2: failed to insert instance: googleapi: Error 404: The resource 'projects/isb-cgc-06-0004/zones/us-west1-a/acceleratorTypes/nvidia-tesla-k80' was not found, notFound""]].; (exit status 1); ```. Here are the full runner log. You can find out the yam file in the same repo.; [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log](https://github.com/google/deepvariant/files/1993190/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log); [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log](https://github.com/google/deepvariant/files/1993191/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log); [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log](https://github.com/google/deepvariant/files/1993192/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-388183666
https://github.com/google/deepvariant/issues/70#issuecomment-388183666:1031,Testability,log,log,1031,"Hi Nima, ; I set the `--call_variants_ram_per_worker_gb 30`, and it seems like resource is not available. ; ```; call-varia--root--180510-193828-03: SUCCESS; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 642, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 322, in _run_call_variants; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 567, in get; raise self._value; RuntimeError: Job failed with error [[u""Error in job call-varia--root--180510-193818-02 - code 2: failed to insert instance: googleapi: Error 404: The resource 'projects/isb-cgc-06-0004/zones/us-west1-a/acceleratorTypes/nvidia-tesla-k80' was not found, notFound""]].; (exit status 1); ```. Here are the full runner log. You can find out the yam file in the same repo.; [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log](https://github.com/google/deepvariant/files/1993190/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log); [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log](https://github.com/google/deepvariant/files/1993191/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log); [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log](https://github.com/google/deepvariant/files/1993192/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-388183666
https://github.com/google/deepvariant/issues/70#issuecomment-388183666:1152,Testability,log,log,1152,"Hi Nima, ; I set the `--call_variants_ram_per_worker_gb 30`, and it seems like resource is not available. ; ```; call-varia--root--180510-193828-03: SUCCESS; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 642, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 322, in _run_call_variants; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 567, in get; raise self._value; RuntimeError: Job failed with error [[u""Error in job call-varia--root--180510-193818-02 - code 2: failed to insert instance: googleapi: Error 404: The resource 'projects/isb-cgc-06-0004/zones/us-west1-a/acceleratorTypes/nvidia-tesla-k80' was not found, notFound""]].; (exit status 1); ```. Here are the full runner log. You can find out the yam file in the same repo.; [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log](https://github.com/google/deepvariant/files/1993190/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log); [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log](https://github.com/google/deepvariant/files/1993191/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log); [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log](https://github.com/google/deepvariant/files/1993192/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-388183666
https://github.com/google/deepvariant/issues/70#issuecomment-388183666:1223,Testability,log,log,1223,"Hi Nima, ; I set the `--call_variants_ram_per_worker_gb 30`, and it seems like resource is not available. ; ```; call-varia--root--180510-193828-03: SUCCESS; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 642, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 322, in _run_call_variants; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 567, in get; raise self._value; RuntimeError: Job failed with error [[u""Error in job call-varia--root--180510-193818-02 - code 2: failed to insert instance: googleapi: Error 404: The resource 'projects/isb-cgc-06-0004/zones/us-west1-a/acceleratorTypes/nvidia-tesla-k80' was not found, notFound""]].; (exit status 1); ```. Here are the full runner log. You can find out the yam file in the same repo.; [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log](https://github.com/google/deepvariant/files/1993190/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log); [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log](https://github.com/google/deepvariant/files/1993191/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log); [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log](https://github.com/google/deepvariant/files/1993192/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-388183666
https://github.com/google/deepvariant/issues/70#issuecomment-388183666:1344,Testability,log,log,1344,"Hi Nima, ; I set the `--call_variants_ram_per_worker_gb 30`, and it seems like resource is not available. ; ```; call-varia--root--180510-193828-03: SUCCESS; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 642, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 322, in _run_call_variants; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 567, in get; raise self._value; RuntimeError: Job failed with error [[u""Error in job call-varia--root--180510-193818-02 - code 2: failed to insert instance: googleapi: Error 404: The resource 'projects/isb-cgc-06-0004/zones/us-west1-a/acceleratorTypes/nvidia-tesla-k80' was not found, notFound""]].; (exit status 1); ```. Here are the full runner log. You can find out the yam file in the same repo.; [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log](https://github.com/google/deepvariant/files/1993190/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log); [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log](https://github.com/google/deepvariant/files/1993191/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log); [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log](https://github.com/google/deepvariant/files/1993192/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-388183666
https://github.com/google/deepvariant/issues/70#issuecomment-388183666:1408,Testability,log,log,1408,"Hi Nima, ; I set the `--call_variants_ram_per_worker_gb 30`, and it seems like resource is not available. ; ```; call-varia--root--180510-193828-03: SUCCESS; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 642, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 322, in _run_call_variants; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 567, in get; raise self._value; RuntimeError: Job failed with error [[u""Error in job call-varia--root--180510-193818-02 - code 2: failed to insert instance: googleapi: Error 404: The resource 'projects/isb-cgc-06-0004/zones/us-west1-a/acceleratorTypes/nvidia-tesla-k80' was not found, notFound""]].; (exit status 1); ```. Here are the full runner log. You can find out the yam file in the same repo.; [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log](https://github.com/google/deepvariant/files/1993190/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log); [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log](https://github.com/google/deepvariant/files/1993191/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log); [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log](https://github.com/google/deepvariant/files/1993192/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-388183666
https://github.com/google/deepvariant/issues/70#issuecomment-388183666:1522,Testability,log,log,1522,"Hi Nima, ; I set the `--call_variants_ram_per_worker_gb 30`, and it seems like resource is not available. ; ```; call-varia--root--180510-193828-03: SUCCESS; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 655, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 642, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 322, in _run_call_variants; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 567, in get; raise self._value; RuntimeError: Job failed with error [[u""Error in job call-varia--root--180510-193818-02 - code 2: failed to insert instance: googleapi: Error 404: The resource 'projects/isb-cgc-06-0004/zones/us-west1-a/acceleratorTypes/nvidia-tesla-k80' was not found, notFound""]].; (exit status 1); ```. Here are the full runner log. You can find out the yam file in the same repo.; [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log](https://github.com/google/deepvariant/files/1993190/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log); [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log](https://github.com/google/deepvariant/files/1993191/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log); [ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log](https://github.com/google/deepvariant/files/1993192/ELj2mta0LBjZy6CkirjPx8ABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/70#issuecomment-388183666
https://github.com/google/deepvariant/issues/71#issuecomment-387583838:124,Availability,error,error,124,I suspect there's a problem with your BED file. Is there a bad interval in that BED file? I wonder if there's an off-by-one error somewhere in the BED reader (I really hope not).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/71#issuecomment-387583838
https://github.com/google/deepvariant/issues/71#issuecomment-387627131:44,Usability,simpl,simply,44,Thank you - I'm attaching my BED file which simply defines the entire length of the chromosome (I am working on a bacterium):; NC_000962.3 0	4411531; Is this an issue with a non-human genome?. (Saved with a .txt so that I could upload.); [confidence.bed.txt](https://github.com/google/deepvariant/files/1986525/confidence.bed.txt),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/71#issuecomment-387627131
https://github.com/google/deepvariant/issues/71#issuecomment-387816424:155,Usability,clear,clear,155,This seems like a genuine bug. Is there any way you can share the genome reference and reads along with a command line that reproduces the issue? It's not clear to me what the actual issue is and that would help a lot debugging the actual problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/71#issuecomment-387816424
https://github.com/google/deepvariant/issues/71#issuecomment-387870861:270,Testability,test,test,270,"Thank you for looking into this. I'm running DeepVariant on a docker image. Below is my command line. My reference genome, reads, and truth VCF are 300Mb, so I will send via email if that is okay.; ```; ref=H37Rv.fa; BAM=GCA_000193185.1_1_1.bowtie2.rmdup.bam; TRUTH_VCF=test.vcf.gz; base=${BAM%.rmdup.bam}. /opt/deepvariant/bin/make_examples --mode training --ref ${ref} --reads ${BAM} --examples ${base}.tfrecord --truth_variants ${TRUTH_VCF} --confident_regions confidence.bed . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/71#issuecomment-387870861
https://github.com/google/deepvariant/issues/71#issuecomment-387907040:36,Availability,error,error,36,"Just confirming I can reproduce the error myself. I think I know what the problem is. In the meantime you can use the previous labeling algorithm as a workaround. Just add ' --labeler_algorithm positional_labeler' to your command line, which is working for me just fine right now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/71#issuecomment-387907040
https://github.com/google/deepvariant/issues/71#issuecomment-387952593:16,Testability,test,test,16,Thank you! I'll test that out.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/71#issuecomment-387952593
https://github.com/google/deepvariant/issues/71#issuecomment-388507714:123,Deployability,release,release,123,The fix for the issue with the haplotype_labeler is ready in the internal codebase and will appear in the next DeepVariant release. Thanks for working through these issues with us.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/71#issuecomment-388507714
https://github.com/google/deepvariant/issues/72#issuecomment-412946034:213,Availability,error,error,213,"Further to this question, I am trying to run 2 chromosome for the whole genome case study. . The user guide said that we can do ```--regions "" chr20 chr21"" ``` to run both chromosome, however, I get the following error when I run the make sample with ``` --regions ""chr20 chr21"" ```. I have also tried ```--regions ""20 21""```, but none of these works. . ```; E0814 18:04:05.051175 140174423590656 errors.py:64] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_D92FlN/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; parallel: This job failed:; python /HD_disk/real-case-study-chr/input/bin/make_examples.zip --mode calling --ref /HD_disk/real-case-study-chr/input/data/hs37d5.fa.gz --reads /HD_disk/real-case-study-chr/input/data/HG002_NIST_150bp_50x.bam --examples /HD_disk/real-case-study-chr/output_1_2_3/HG002.examples.tfrecord@8.gz --regions 20 21 --gvcf /HD_disk/real-case-study-chr/output_1_2_3/HG002.gvcf.tfrecord@8.gz --task 2; ```. Would you please suggest how I should specify the region to get multiple chromosome running at the same time. For single chromosome, ``` --regions ""20"" ``` works fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/72#issuecomment-412946034
https://github.com/google/deepvariant/issues/72#issuecomment-412946034:397,Availability,error,errors,397,"Further to this question, I am trying to run 2 chromosome for the whole genome case study. . The user guide said that we can do ```--regions "" chr20 chr21"" ``` to run both chromosome, however, I get the following error when I run the make sample with ``` --regions ""chr20 chr21"" ```. I have also tried ```--regions ""20 21""```, but none of these works. . ```; E0814 18:04:05.051175 140174423590656 errors.py:64] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_D92FlN/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; parallel: This job failed:; python /HD_disk/real-case-study-chr/input/bin/make_examples.zip --mode calling --ref /HD_disk/real-case-study-chr/input/data/hs37d5.fa.gz --reads /HD_disk/real-case-study-chr/input/data/HG002_NIST_150bp_50x.bam --examples /HD_disk/real-case-study-chr/output_1_2_3/HG002.examples.tfrecord@8.gz --regions 20 21 --gvcf /HD_disk/real-case-study-chr/output_1_2_3/HG002.gvcf.tfrecord@8.gz --task 2; ```. Would you please suggest how I should specify the region to get multiple chromosome running at the same time. For single chromosome, ``` --regions ""20"" ``` works fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/72#issuecomment-412946034
https://github.com/google/deepvariant/issues/72#issuecomment-412946034:432,Availability,failure,failure,432,"Further to this question, I am trying to run 2 chromosome for the whole genome case study. . The user guide said that we can do ```--regions "" chr20 chr21"" ``` to run both chromosome, however, I get the following error when I run the make sample with ``` --regions ""chr20 chr21"" ```. I have also tried ```--regions ""20 21""```, but none of these works. . ```; E0814 18:04:05.051175 140174423590656 errors.py:64] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_D92FlN/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; parallel: This job failed:; python /HD_disk/real-case-study-chr/input/bin/make_examples.zip --mode calling --ref /HD_disk/real-case-study-chr/input/data/hs37d5.fa.gz --reads /HD_disk/real-case-study-chr/input/data/HG002_NIST_150bp_50x.bam --examples /HD_disk/real-case-study-chr/output_1_2_3/HG002.examples.tfrecord@8.gz --regions 20 21 --gvcf /HD_disk/real-case-study-chr/output_1_2_3/HG002.gvcf.tfrecord@8.gz --task 2; ```. Would you please suggest how I should specify the region to get multiple chromosome running at the same time. For single chromosome, ``` --regions ""20"" ``` works fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/72#issuecomment-412946034
https://github.com/google/deepvariant/issues/72#issuecomment-412946034:102,Usability,guid,guide,102,"Further to this question, I am trying to run 2 chromosome for the whole genome case study. . The user guide said that we can do ```--regions "" chr20 chr21"" ``` to run both chromosome, however, I get the following error when I run the make sample with ``` --regions ""chr20 chr21"" ```. I have also tried ```--regions ""20 21""```, but none of these works. . ```; E0814 18:04:05.051175 140174423590656 errors.py:64] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_D92FlN/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; parallel: This job failed:; python /HD_disk/real-case-study-chr/input/bin/make_examples.zip --mode calling --ref /HD_disk/real-case-study-chr/input/data/hs37d5.fa.gz --reads /HD_disk/real-case-study-chr/input/data/HG002_NIST_150bp_50x.bam --examples /HD_disk/real-case-study-chr/output_1_2_3/HG002.examples.tfrecord@8.gz --regions 20 21 --gvcf /HD_disk/real-case-study-chr/output_1_2_3/HG002.gvcf.tfrecord@8.gz --task 2; ```. Would you please suggest how I should specify the region to get multiple chromosome running at the same time. For single chromosome, ``` --regions ""20"" ``` works fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/72#issuecomment-412946034
https://github.com/google/deepvariant/issues/72#issuecomment-412948943:166,Availability,error,error,166,"I just verified that --regions ""chr20:10,000,000-10,001,000 chr20:10,002,000-10,003,000"" in fact processes two regions. So make_examples is working as expected. Your error looks like it is caused by missing the quotes around the regions argument. So make_examples interprets:. `... --regions 20 21 --gvcf /HD_disk/real-case...`. as . `... --regions 20` <= keyword args for make_examples; `21 --gvcf /HD_disk/real-case...` <= positional arguments to make_examples. hence the error you are seeing. Just doing:. `... --regions ""20 21"" --gvcf /HD_disk/real-case...`. should fix it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/72#issuecomment-412948943
https://github.com/google/deepvariant/issues/72#issuecomment-412948943:474,Availability,error,error,474,"I just verified that --regions ""chr20:10,000,000-10,001,000 chr20:10,002,000-10,003,000"" in fact processes two regions. So make_examples is working as expected. Your error looks like it is caused by missing the quotes around the regions argument. So make_examples interprets:. `... --regions 20 21 --gvcf /HD_disk/real-case...`. as . `... --regions 20` <= keyword args for make_examples; `21 --gvcf /HD_disk/real-case...` <= positional arguments to make_examples. hence the error you are seeing. Just doing:. `... --regions ""20 21"" --gvcf /HD_disk/real-case...`. should fix it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/72#issuecomment-412948943
https://github.com/google/deepvariant/issues/72#issuecomment-413179094:471,Testability,log,log,471,"Thank you for the reply. I have included the "" "" in the code however it appears missing at runtime.It worked when I replace that with ``` --regions ' ""20 21"" ' ```. However, a further question is that when should we include the chr in the region. Apparently ``` --regions ' ""chr20:10,000,000-10,001,000 chr20:10,002,000-10,003,000"" ' ``` does not work but ``` --regions ' ""20:10,000,000-10,001,000 20:10,002,000-10,003,000"" ' ``` is fine. . One more problem is about the log file of the make sample. . It appears to show variant candidate in each region and finally display the total number of variant, i.e.; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 7 candidates in 20:10000000-10000999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ``` . However, for region 20:10,002,000-10,003,000, I get; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 9 candidates in 20:10002000-10002999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ``` ; which the final candidate variant is less than the candidate in the region. When I run with region 20:10,000,000-10,001,000 20:10,002,000-10,003,000, ; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 9 candidates in 20:10002000-10002999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ```; ; which is the same as just running region 20:10,002,000-10,003,000 only. But when I check the call variant log file, it indicated that 16 samples are run, which is correct (7 for first region and 9 for second region). . As I am using ```parallel```, my guess is the log file and final variant number shown in make sample log file only show the total number in 1 partition instead of all. Is that the case?. I am not sure if I misunderstand the log file, or the make sample log does not really reflect the total number of ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/72#issuecomment-413179094
https://github.com/google/deepvariant/issues/72#issuecomment-413179094:1587,Testability,log,log,1587," that with ``` --regions ' ""20 21"" ' ```. However, a further question is that when should we include the chr in the region. Apparently ``` --regions ' ""chr20:10,000,000-10,001,000 chr20:10,002,000-10,003,000"" ' ``` does not work but ``` --regions ' ""20:10,000,000-10,001,000 20:10,002,000-10,003,000"" ' ``` is fine. . One more problem is about the log file of the make sample. . It appears to show variant candidate in each region and finally display the total number of variant, i.e.; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 7 candidates in 20:10000000-10000999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ``` . However, for region 20:10,002,000-10,003,000, I get; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 9 candidates in 20:10002000-10002999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ``` ; which the final candidate variant is less than the candidate in the region. When I run with region 20:10,000,000-10,001,000 20:10,002,000-10,003,000, ; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 9 candidates in 20:10002000-10002999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ```; ; which is the same as just running region 20:10,002,000-10,003,000 only. But when I check the call variant log file, it indicated that 16 samples are run, which is correct (7 for first region and 9 for second region). . As I am using ```parallel```, my guess is the log file and final variant number shown in make sample log file only show the total number in 1 partition instead of all. Is that the case?. I am not sure if I misunderstand the log file, or the make sample log does not really reflect the total number of variant. Should I only refer to sample number shown in Call Variant log file to get the total number of candidate variant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/72#issuecomment-413179094
https://github.com/google/deepvariant/issues/72#issuecomment-413179094:1746,Testability,log,log,1746," that with ``` --regions ' ""20 21"" ' ```. However, a further question is that when should we include the chr in the region. Apparently ``` --regions ' ""chr20:10,000,000-10,001,000 chr20:10,002,000-10,003,000"" ' ``` does not work but ``` --regions ' ""20:10,000,000-10,001,000 20:10,002,000-10,003,000"" ' ``` is fine. . One more problem is about the log file of the make sample. . It appears to show variant candidate in each region and finally display the total number of variant, i.e.; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 7 candidates in 20:10000000-10000999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ``` . However, for region 20:10,002,000-10,003,000, I get; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 9 candidates in 20:10002000-10002999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ``` ; which the final candidate variant is less than the candidate in the region. When I run with region 20:10,000,000-10,001,000 20:10,002,000-10,003,000, ; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 9 candidates in 20:10002000-10002999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ```; ; which is the same as just running region 20:10,002,000-10,003,000 only. But when I check the call variant log file, it indicated that 16 samples are run, which is correct (7 for first region and 9 for second region). . As I am using ```parallel```, my guess is the log file and final variant number shown in make sample log file only show the total number in 1 partition instead of all. Is that the case?. I am not sure if I misunderstand the log file, or the make sample log does not really reflect the total number of variant. Should I only refer to sample number shown in Call Variant log file to get the total number of candidate variant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/72#issuecomment-413179094
https://github.com/google/deepvariant/issues/72#issuecomment-413179094:1801,Testability,log,log,1801," that with ``` --regions ' ""20 21"" ' ```. However, a further question is that when should we include the chr in the region. Apparently ``` --regions ' ""chr20:10,000,000-10,001,000 chr20:10,002,000-10,003,000"" ' ``` does not work but ``` --regions ' ""20:10,000,000-10,001,000 20:10,002,000-10,003,000"" ' ``` is fine. . One more problem is about the log file of the make sample. . It appears to show variant candidate in each region and finally display the total number of variant, i.e.; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 7 candidates in 20:10000000-10000999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ``` . However, for region 20:10,002,000-10,003,000, I get; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 9 candidates in 20:10002000-10002999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ``` ; which the final candidate variant is less than the candidate in the region. When I run with region 20:10,000,000-10,001,000 20:10,002,000-10,003,000, ; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 9 candidates in 20:10002000-10002999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ```; ; which is the same as just running region 20:10,002,000-10,003,000 only. But when I check the call variant log file, it indicated that 16 samples are run, which is correct (7 for first region and 9 for second region). . As I am using ```parallel```, my guess is the log file and final variant number shown in make sample log file only show the total number in 1 partition instead of all. Is that the case?. I am not sure if I misunderstand the log file, or the make sample log does not really reflect the total number of variant. Should I only refer to sample number shown in Call Variant log file to get the total number of candidate variant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/72#issuecomment-413179094
https://github.com/google/deepvariant/issues/72#issuecomment-413179094:1924,Testability,log,log,1924," that with ``` --regions ' ""20 21"" ' ```. However, a further question is that when should we include the chr in the region. Apparently ``` --regions ' ""chr20:10,000,000-10,001,000 chr20:10,002,000-10,003,000"" ' ``` does not work but ``` --regions ' ""20:10,000,000-10,001,000 20:10,002,000-10,003,000"" ' ``` is fine. . One more problem is about the log file of the make sample. . It appears to show variant candidate in each region and finally display the total number of variant, i.e.; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 7 candidates in 20:10000000-10000999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ``` . However, for region 20:10,002,000-10,003,000, I get; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 9 candidates in 20:10002000-10002999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ``` ; which the final candidate variant is less than the candidate in the region. When I run with region 20:10,000,000-10,001,000 20:10,002,000-10,003,000, ; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 9 candidates in 20:10002000-10002999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ```; ; which is the same as just running region 20:10,002,000-10,003,000 only. But when I check the call variant log file, it indicated that 16 samples are run, which is correct (7 for first region and 9 for second region). . As I am using ```parallel```, my guess is the log file and final variant number shown in make sample log file only show the total number in 1 partition instead of all. Is that the case?. I am not sure if I misunderstand the log file, or the make sample log does not really reflect the total number of variant. Should I only refer to sample number shown in Call Variant log file to get the total number of candidate variant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/72#issuecomment-413179094
https://github.com/google/deepvariant/issues/72#issuecomment-413179094:1953,Testability,log,log,1953," that with ``` --regions ' ""20 21"" ' ```. However, a further question is that when should we include the chr in the region. Apparently ``` --regions ' ""chr20:10,000,000-10,001,000 chr20:10,002,000-10,003,000"" ' ``` does not work but ``` --regions ' ""20:10,000,000-10,001,000 20:10,002,000-10,003,000"" ' ``` is fine. . One more problem is about the log file of the make sample. . It appears to show variant candidate in each region and finally display the total number of variant, i.e.; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 7 candidates in 20:10000000-10000999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ``` . However, for region 20:10,002,000-10,003,000, I get; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 9 candidates in 20:10002000-10002999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ``` ; which the final candidate variant is less than the candidate in the region. When I run with region 20:10,000,000-10,001,000 20:10,002,000-10,003,000, ; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 9 candidates in 20:10002000-10002999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ```; ; which is the same as just running region 20:10,002,000-10,003,000 only. But when I check the call variant log file, it indicated that 16 samples are run, which is correct (7 for first region and 9 for second region). . As I am using ```parallel```, my guess is the log file and final variant number shown in make sample log file only show the total number in 1 partition instead of all. Is that the case?. I am not sure if I misunderstand the log file, or the make sample log does not really reflect the total number of variant. Should I only refer to sample number shown in Call Variant log file to get the total number of candidate variant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/72#issuecomment-413179094
https://github.com/google/deepvariant/issues/72#issuecomment-413179094:2069,Testability,log,log,2069," that with ``` --regions ' ""20 21"" ' ```. However, a further question is that when should we include the chr in the region. Apparently ``` --regions ' ""chr20:10,000,000-10,001,000 chr20:10,002,000-10,003,000"" ' ``` does not work but ``` --regions ' ""20:10,000,000-10,001,000 20:10,002,000-10,003,000"" ' ``` is fine. . One more problem is about the log file of the make sample. . It appears to show variant candidate in each region and finally display the total number of variant, i.e.; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 7 candidates in 20:10000000-10000999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ``` . However, for region 20:10,002,000-10,003,000, I get; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 9 candidates in 20:10002000-10002999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ``` ; which the final candidate variant is less than the candidate in the region. When I run with region 20:10,000,000-10,001,000 20:10,002,000-10,003,000, ; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 9 candidates in 20:10002000-10002999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ```; ; which is the same as just running region 20:10,002,000-10,003,000 only. But when I check the call variant log file, it indicated that 16 samples are run, which is correct (7 for first region and 9 for second region). . As I am using ```parallel```, my guess is the log file and final variant number shown in make sample log file only show the total number in 1 partition instead of all. Is that the case?. I am not sure if I misunderstand the log file, or the make sample log does not really reflect the total number of variant. Should I only refer to sample number shown in Call Variant log file to get the total number of candidate variant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/72#issuecomment-413179094
https://github.com/google/deepvariant/issues/72#issuecomment-413278699:34,Integrability,depend,depends,34,"The exact name for the chromosome depends on your reference file. If the reference FASTA names its chromosomes chr1, chr2 you need to use ""chr"" in the prefix. If the reference FASTA calls them 1, 2, etc you need to use that name.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/72#issuecomment-413278699
https://github.com/google/deepvariant/issues/72#issuecomment-436236198:82,Availability,error,error,82,"Even if I only set the mode(which should fail by asking for other flags) the same error appears. Using 0.7.0: . Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_TAWAeF/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '--mode', 'calling']"".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/72#issuecomment-436236198
https://github.com/google/deepvariant/issues/72#issuecomment-436236198:133,Availability,failure,failure,133,"Even if I only set the mode(which should fail by asking for other flags) the same error appears. Using 0.7.0: . Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_TAWAeF/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '--mode', 'calling']"".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/72#issuecomment-436236198
https://github.com/google/deepvariant/issues/73#issuecomment-389394212:657,Availability,error,error,657,"Hi,; as you can see in the message:; ```; I0514 22:42:53.306706 140678655026944 make_examples.py:946] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; ```; This shows that in the input files you gave to make_examples use the convention *without* the prefix ""chr"". Therefore, in this case you should use `--regions ""20""` in your argument. It could be a bit confusing, but both conventions exist. For example, depending on which version of the reference you're using they can also be different. Would it help if we add a bit more error message? ; Maybe in addition to ""Could not parse"", we can say something more verbose like:; `Could not parse chr20 : make sure if the regions you specify are in the contigs of your BAM and FASTA file. A common error is to use ""chr"" prefix on files that don't have it, or vice versa.`; Would that have helped?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/73#issuecomment-389394212
https://github.com/google/deepvariant/issues/73#issuecomment-389394212:873,Availability,error,error,873,"Hi,; as you can see in the message:; ```; I0514 22:42:53.306706 140678655026944 make_examples.py:946] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; ```; This shows that in the input files you gave to make_examples use the convention *without* the prefix ""chr"". Therefore, in this case you should use `--regions ""20""` in your argument. It could be a bit confusing, but both conventions exist. For example, depending on which version of the reference you're using they can also be different. Would it help if we add a bit more error message? ; Maybe in addition to ""Could not parse"", we can say something more verbose like:; `Could not parse chr20 : make sure if the regions you specify are in the contigs of your BAM and FASTA file. A common error is to use ""chr"" prefix on files that don't have it, or vice versa.`; Would that have helped?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/73#issuecomment-389394212
https://github.com/google/deepvariant/issues/73#issuecomment-389394212:27,Integrability,message,message,27,"Hi,; as you can see in the message:; ```; I0514 22:42:53.306706 140678655026944 make_examples.py:946] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; ```; This shows that in the input files you gave to make_examples use the convention *without* the prefix ""chr"". Therefore, in this case you should use `--regions ""20""` in your argument. It could be a bit confusing, but both conventions exist. For example, depending on which version of the reference you're using they can also be different. Would it help if we add a bit more error message? ; Maybe in addition to ""Could not parse"", we can say something more verbose like:; `Could not parse chr20 : make sure if the regions you specify are in the contigs of your BAM and FASTA file. A common error is to use ""chr"" prefix on files that don't have it, or vice versa.`; Would that have helped?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/73#issuecomment-389394212
https://github.com/google/deepvariant/issues/73#issuecomment-389394212:537,Integrability,depend,depending,537,"Hi,; as you can see in the message:; ```; I0514 22:42:53.306706 140678655026944 make_examples.py:946] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; ```; This shows that in the input files you gave to make_examples use the convention *without* the prefix ""chr"". Therefore, in this case you should use `--regions ""20""` in your argument. It could be a bit confusing, but both conventions exist. For example, depending on which version of the reference you're using they can also be different. Would it help if we add a bit more error message? ; Maybe in addition to ""Could not parse"", we can say something more verbose like:; `Could not parse chr20 : make sure if the regions you specify are in the contigs of your BAM and FASTA file. A common error is to use ""chr"" prefix on files that don't have it, or vice versa.`; Would that have helped?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/73#issuecomment-389394212
https://github.com/google/deepvariant/issues/73#issuecomment-389394212:663,Integrability,message,message,663,"Hi,; as you can see in the message:; ```; I0514 22:42:53.306706 140678655026944 make_examples.py:946] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; ```; This shows that in the input files you gave to make_examples use the convention *without* the prefix ""chr"". Therefore, in this case you should use `--regions ""20""` in your argument. It could be a bit confusing, but both conventions exist. For example, depending on which version of the reference you're using they can also be different. Would it help if we add a bit more error message? ; Maybe in addition to ""Could not parse"", we can say something more verbose like:; `Could not parse chr20 : make sure if the regions you specify are in the contigs of your BAM and FASTA file. A common error is to use ""chr"" prefix on files that don't have it, or vice versa.`; Would that have helped?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/73#issuecomment-389394212
https://github.com/google/deepvariant/issues/73#issuecomment-389700500:62,Deployability,release,releases,62,We've added a more verbose message. It will come out in later releases of Nucleus and DeepVariant. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/73#issuecomment-389700500
https://github.com/google/deepvariant/issues/73#issuecomment-389700500:27,Integrability,message,message,27,We've added a more verbose message. It will come out in later releases of Nucleus and DeepVariant. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/73#issuecomment-389700500
https://github.com/google/deepvariant/issues/74#issuecomment-391092118:806,Deployability,pipeline,pipelines,806,"Hi, thanks for reporting this issue. If you used a GCE instance, can you share the command you used to start a cloud machine? . And, if you look at your `""${LOG_DIR}/call_variants.log""`, you should be able to see lines like these:; ```; I0405 16:03:16.308625 140490269800192 call_variants.py:353] Processed 4680192 examples in 146256 batches [0.67 sec per 100]; I0405 16:03:16.524780 140490269800192 call_variants.py:353] Processed 4680224 examples in 146257 batches [0.67 sec per 100]; ```. Can you tell me what your ""sec per 100"" is? This will also confirm your speed for call_variants. I'm guessing it's much slower than 0.67 sec per 100. You should also watch your systems resources -- is there enough RAM, etc. And, I would also suggest that you check out the [cost- and speed-optimized, Docker-based pipelines](https://cloud.google.com/genomics/deepvariant) created for Google Cloud Platform. Case studies are created so that the users can understand the key components of DeepVariant, but if you want to look for production-grade performance, you should consider the Cloud pipelines instead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/74#issuecomment-391092118
https://github.com/google/deepvariant/issues/74#issuecomment-391092118:1080,Deployability,pipeline,pipelines,1080,"Hi, thanks for reporting this issue. If you used a GCE instance, can you share the command you used to start a cloud machine? . And, if you look at your `""${LOG_DIR}/call_variants.log""`, you should be able to see lines like these:; ```; I0405 16:03:16.308625 140490269800192 call_variants.py:353] Processed 4680192 examples in 146256 batches [0.67 sec per 100]; I0405 16:03:16.524780 140490269800192 call_variants.py:353] Processed 4680224 examples in 146257 batches [0.67 sec per 100]; ```. Can you tell me what your ""sec per 100"" is? This will also confirm your speed for call_variants. I'm guessing it's much slower than 0.67 sec per 100. You should also watch your systems resources -- is there enough RAM, etc. And, I would also suggest that you check out the [cost- and speed-optimized, Docker-based pipelines](https://cloud.google.com/genomics/deepvariant) created for Google Cloud Platform. Case studies are created so that the users can understand the key components of DeepVariant, but if you want to look for production-grade performance, you should consider the Cloud pipelines instead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/74#issuecomment-391092118
https://github.com/google/deepvariant/issues/74#issuecomment-391092118:782,Performance,optimiz,optimized,782,"Hi, thanks for reporting this issue. If you used a GCE instance, can you share the command you used to start a cloud machine? . And, if you look at your `""${LOG_DIR}/call_variants.log""`, you should be able to see lines like these:; ```; I0405 16:03:16.308625 140490269800192 call_variants.py:353] Processed 4680192 examples in 146256 batches [0.67 sec per 100]; I0405 16:03:16.524780 140490269800192 call_variants.py:353] Processed 4680224 examples in 146257 batches [0.67 sec per 100]; ```. Can you tell me what your ""sec per 100"" is? This will also confirm your speed for call_variants. I'm guessing it's much slower than 0.67 sec per 100. You should also watch your systems resources -- is there enough RAM, etc. And, I would also suggest that you check out the [cost- and speed-optimized, Docker-based pipelines](https://cloud.google.com/genomics/deepvariant) created for Google Cloud Platform. Case studies are created so that the users can understand the key components of DeepVariant, but if you want to look for production-grade performance, you should consider the Cloud pipelines instead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/74#issuecomment-391092118
https://github.com/google/deepvariant/issues/74#issuecomment-391092118:1037,Performance,perform,performance,1037,"Hi, thanks for reporting this issue. If you used a GCE instance, can you share the command you used to start a cloud machine? . And, if you look at your `""${LOG_DIR}/call_variants.log""`, you should be able to see lines like these:; ```; I0405 16:03:16.308625 140490269800192 call_variants.py:353] Processed 4680192 examples in 146256 batches [0.67 sec per 100]; I0405 16:03:16.524780 140490269800192 call_variants.py:353] Processed 4680224 examples in 146257 batches [0.67 sec per 100]; ```. Can you tell me what your ""sec per 100"" is? This will also confirm your speed for call_variants. I'm guessing it's much slower than 0.67 sec per 100. You should also watch your systems resources -- is there enough RAM, etc. And, I would also suggest that you check out the [cost- and speed-optimized, Docker-based pipelines](https://cloud.google.com/genomics/deepvariant) created for Google Cloud Platform. Case studies are created so that the users can understand the key components of DeepVariant, but if you want to look for production-grade performance, you should consider the Cloud pipelines instead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/74#issuecomment-391092118
https://github.com/google/deepvariant/issues/74#issuecomment-391092118:180,Testability,log,log,180,"Hi, thanks for reporting this issue. If you used a GCE instance, can you share the command you used to start a cloud machine? . And, if you look at your `""${LOG_DIR}/call_variants.log""`, you should be able to see lines like these:; ```; I0405 16:03:16.308625 140490269800192 call_variants.py:353] Processed 4680192 examples in 146256 batches [0.67 sec per 100]; I0405 16:03:16.524780 140490269800192 call_variants.py:353] Processed 4680224 examples in 146257 batches [0.67 sec per 100]; ```. Can you tell me what your ""sec per 100"" is? This will also confirm your speed for call_variants. I'm guessing it's much slower than 0.67 sec per 100. You should also watch your systems resources -- is there enough RAM, etc. And, I would also suggest that you check out the [cost- and speed-optimized, Docker-based pipelines](https://cloud.google.com/genomics/deepvariant) created for Google Cloud Platform. Case studies are created so that the users can understand the key components of DeepVariant, but if you want to look for production-grade performance, you should consider the Cloud pipelines instead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/74#issuecomment-391092118
https://github.com/google/deepvariant/issues/74#issuecomment-391321427:636,Deployability,configurat,configuration,636,"Thank you for the reply. I am running it on a linux server instead of using GCE. . The ""sec per 100"" is ""2.68 sec per 100"". My machine has 16Gb RAM and DeepVariant added 1-2 Gb RAM usage and total RAM usage peaks at 5Gb with 8 cores fully utilized. The data file, make_examples and call_variants result are all written to local storage instead of network storage. . I was trying to get a timing profile and check if the program runs as expected. One thing I am curious is that the time taken to run make_examples matches the report time closely, however, the call_variants is much slower. I wonder if there is any problem in setting or configuration as the reported time of call_variants is much faster.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/74#issuecomment-391321427
https://github.com/google/deepvariant/issues/74#issuecomment-391321427:636,Modifiability,config,configuration,636,"Thank you for the reply. I am running it on a linux server instead of using GCE. . The ""sec per 100"" is ""2.68 sec per 100"". My machine has 16Gb RAM and DeepVariant added 1-2 Gb RAM usage and total RAM usage peaks at 5Gb with 8 cores fully utilized. The data file, make_examples and call_variants result are all written to local storage instead of network storage. . I was trying to get a timing profile and check if the program runs as expected. One thing I am curious is that the time taken to run make_examples matches the report time closely, however, the call_variants is much slower. I wonder if there is any problem in setting or configuration as the reported time of call_variants is much faster.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/74#issuecomment-391321427
https://github.com/google/deepvariant/issues/74#issuecomment-391790774:1504,Availability,down,downloaded,1504,"Hi Bowen, just an FYI that I'm looking into this a bit. I'm going to try running call_variants on a 8 core machine on GCE to see how the timing looks. Can you send us the details of the CPU you are trying to run on? For example:. $ head -n 26 /proc/cpuinfo; processor	: 0; vendor_id	: GenuineIntel; cpu family	: 6; model		: 63; model name	: Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz; stepping	: 2; microcode	: 0x3c; cpu MHz		: 1200.024; cache size	: 30720 KB; physical id	: 0; siblings	: 24; core id		: 0; cpu cores	: 12; apicid		: 0; initial apicid	: 0; fpu		: yes; fpu_exception	: yes; cpuid level	: 15; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc ibpb ibrs stibp dtherm ida arat pln pts; bugs		: cpu_meltdown spectre_v1 spectre_v2; bogomips	: 5187.99; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. Also, just to confirm - you are using DV 0.6.1 with our gcp optimized TF wheel (this is downloaded by run-prereqs.sh)?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/74#issuecomment-391790774
https://github.com/google/deepvariant/issues/74#issuecomment-391790774:887,Energy Efficiency,monitor,monitor,887,"Hi Bowen, just an FYI that I'm looking into this a bit. I'm going to try running call_variants on a 8 core machine on GCE to see how the timing looks. Can you send us the details of the CPU you are trying to run on? For example:. $ head -n 26 /proc/cpuinfo; processor	: 0; vendor_id	: GenuineIntel; cpu family	: 6; model		: 63; model name	: Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz; stepping	: 2; microcode	: 0x3c; cpu MHz		: 1200.024; cache size	: 30720 KB; physical id	: 0; siblings	: 24; core id		: 0; cpu cores	: 12; apicid		: 0; initial apicid	: 0; fpu		: yes; fpu_exception	: yes; cpuid level	: 15; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc ibpb ibrs stibp dtherm ida arat pln pts; bugs		: cpu_meltdown spectre_v1 spectre_v2; bogomips	: 5187.99; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. Also, just to confirm - you are using DV 0.6.1 with our gcp optimized TF wheel (this is downloaded by run-prereqs.sh)?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/74#issuecomment-391790774
https://github.com/google/deepvariant/issues/74#issuecomment-391790774:1397,Energy Efficiency,power,power,1397,"Hi Bowen, just an FYI that I'm looking into this a bit. I'm going to try running call_variants on a 8 core machine on GCE to see how the timing looks. Can you send us the details of the CPU you are trying to run on? For example:. $ head -n 26 /proc/cpuinfo; processor	: 0; vendor_id	: GenuineIntel; cpu family	: 6; model		: 63; model name	: Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz; stepping	: 2; microcode	: 0x3c; cpu MHz		: 1200.024; cache size	: 30720 KB; physical id	: 0; siblings	: 24; core id		: 0; cpu cores	: 12; apicid		: 0; initial apicid	: 0; fpu		: yes; fpu_exception	: yes; cpuid level	: 15; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc ibpb ibrs stibp dtherm ida arat pln pts; bugs		: cpu_meltdown spectre_v1 spectre_v2; bogomips	: 5187.99; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. Also, just to confirm - you are using DV 0.6.1 with our gcp optimized TF wheel (this is downloaded by run-prereqs.sh)?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/74#issuecomment-391790774
https://github.com/google/deepvariant/issues/74#issuecomment-391790774:437,Performance,cache,cache,437,"Hi Bowen, just an FYI that I'm looking into this a bit. I'm going to try running call_variants on a 8 core machine on GCE to see how the timing looks. Can you send us the details of the CPU you are trying to run on? For example:. $ head -n 26 /proc/cpuinfo; processor	: 0; vendor_id	: GenuineIntel; cpu family	: 6; model		: 63; model name	: Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz; stepping	: 2; microcode	: 0x3c; cpu MHz		: 1200.024; cache size	: 30720 KB; physical id	: 0; siblings	: 24; core id		: 0; cpu cores	: 12; apicid		: 0; initial apicid	: 0; fpu		: yes; fpu_exception	: yes; cpuid level	: 15; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc ibpb ibrs stibp dtherm ida arat pln pts; bugs		: cpu_meltdown spectre_v1 spectre_v2; bogomips	: 5187.99; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. Also, just to confirm - you are using DV 0.6.1 with our gcp optimized TF wheel (this is downloaded by run-prereqs.sh)?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/74#issuecomment-391790774
https://github.com/google/deepvariant/issues/74#issuecomment-391790774:1476,Performance,optimiz,optimized,1476,"Hi Bowen, just an FYI that I'm looking into this a bit. I'm going to try running call_variants on a 8 core machine on GCE to see how the timing looks. Can you send us the details of the CPU you are trying to run on? For example:. $ head -n 26 /proc/cpuinfo; processor	: 0; vendor_id	: GenuineIntel; cpu family	: 6; model		: 63; model name	: Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz; stepping	: 2; microcode	: 0x3c; cpu MHz		: 1200.024; cache size	: 30720 KB; physical id	: 0; siblings	: 24; core id		: 0; cpu cores	: 12; apicid		: 0; initial apicid	: 0; fpu		: yes; fpu_exception	: yes; cpuid level	: 15; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc ibpb ibrs stibp dtherm ida arat pln pts; bugs		: cpu_meltdown spectre_v1 spectre_v2; bogomips	: 5187.99; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. Also, just to confirm - you are using DV 0.6.1 with our gcp optimized TF wheel (this is downloaded by run-prereqs.sh)?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/74#issuecomment-391790774
https://github.com/google/deepvariant/issues/74#issuecomment-391801518:2057,Deployability,install,install,2057,"endor_id : GenuineIntel; cpu family : 6; model : 94; model name : Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz; stepping : 3; microcode : 0xc2; cpu MHz : 1013.093; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 22; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb invpcid_single intel_pt rsb_ctxsw spec_ctrl retpoline kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx rdseed adx smap clflushopt xsaveopt xsavec xgetbv1 dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp; bugs : cpu_meltdown spectre_v1 spectre_v2; bogomips : 6816.62; clflush size : 64; cache_alignment : 64; address sizes : 39 bits physical, 48 bits virtual; power management:. I have used the preliminaries set in the exome case study, namely. ```; BASE=""/HD_disk/exome-case-study""; BUCKET=""gs://deepvariant""; BIN_VERSION=""0.6.1""; MODEL_VERSION=""0.6.0""; MODEL_CL=""191676894"". # Note that we don't specify the CL number for the binary, only the bin version.; BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}+cl-*""; MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${MODEL_VERSION}/DeepVariant-inception_v3-${MODEL_VERSION}+cl-${MODEL_CL}.data-wes_standard""; DATA_BUCKET=""${BUCKET}/exome-case-study-testdata""; ```. I have also run the run_prereq.sh, however, I am not entirely sure if I have the gcp optimized TF wheel. Can you show me which one is the gcp optimized TF wheel and how can I install that if I have not?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/74#issuecomment-391801518
https://github.com/google/deepvariant/issues/74#issuecomment-391801518:689,Energy Efficiency,monitor,monitor,689,"Thank you for the reply. . Following are the cpuinfo of my machine. processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 94; model name : Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz; stepping : 3; microcode : 0xc2; cpu MHz : 1013.093; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 22; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb invpcid_single intel_pt rsb_ctxsw spec_ctrl retpoline kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx rdseed adx smap clflushopt xsaveopt xsavec xgetbv1 dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp; bugs : cpu_meltdown spectre_v1 spectre_v2; bogomips : 6816.62; clflush size : 64; cache_alignment : 64; address sizes : 39 bits physical, 48 bits virtual; power management:. I have used the preliminaries set in the exome case study, namely. ```; BASE=""/HD_disk/exome-case-study""; BUCKET=""gs://deepvariant""; BIN_VERSION=""0.6.1""; MODEL_VERSION=""0.6.0""; MODEL_CL=""191676894"". # Note that we don't specify the CL number for the binary, only the bin version.; BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}+cl-*""; MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${MODEL_VERSION}/DeepVariant-inception_v3-${MODEL_VERSION}+cl-${MODEL_CL}.data-wes_standard""; DATA_BUCKET=""${BUCKET}/exome-case-study-testdata""; ```. I have also run the run_prereq.sh, however, I am not entirely sure if I have the gcp optimized TF wheel. Can you show m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/74#issuecomment-391801518
https://github.com/google/deepvariant/issues/74#issuecomment-391801518:1297,Energy Efficiency,power,power,1297,"endor_id : GenuineIntel; cpu family : 6; model : 94; model name : Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz; stepping : 3; microcode : 0xc2; cpu MHz : 1013.093; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 22; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb invpcid_single intel_pt rsb_ctxsw spec_ctrl retpoline kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx rdseed adx smap clflushopt xsaveopt xsavec xgetbv1 dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp; bugs : cpu_meltdown spectre_v1 spectre_v2; bogomips : 6816.62; clflush size : 64; cache_alignment : 64; address sizes : 39 bits physical, 48 bits virtual; power management:. I have used the preliminaries set in the exome case study, namely. ```; BASE=""/HD_disk/exome-case-study""; BUCKET=""gs://deepvariant""; BIN_VERSION=""0.6.1""; MODEL_VERSION=""0.6.0""; MODEL_CL=""191676894"". # Note that we don't specify the CL number for the binary, only the bin version.; BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}+cl-*""; MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${MODEL_VERSION}/DeepVariant-inception_v3-${MODEL_VERSION}+cl-${MODEL_CL}.data-wes_standard""; DATA_BUCKET=""${BUCKET}/exome-case-study-testdata""; ```. I have also run the run_prereq.sh, however, I am not entirely sure if I have the gcp optimized TF wheel. Can you show me which one is the gcp optimized TF wheel and how can I install that if I have not?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/74#issuecomment-391801518
https://github.com/google/deepvariant/issues/74#issuecomment-391801518:243,Performance,cache,cache,243,"Thank you for the reply. . Following are the cpuinfo of my machine. processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 94; model name : Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz; stepping : 3; microcode : 0xc2; cpu MHz : 1013.093; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 22; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb invpcid_single intel_pt rsb_ctxsw spec_ctrl retpoline kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx rdseed adx smap clflushopt xsaveopt xsavec xgetbv1 dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp; bugs : cpu_meltdown spectre_v1 spectre_v2; bogomips : 6816.62; clflush size : 64; cache_alignment : 64; address sizes : 39 bits physical, 48 bits virtual; power management:. I have used the preliminaries set in the exome case study, namely. ```; BASE=""/HD_disk/exome-case-study""; BUCKET=""gs://deepvariant""; BIN_VERSION=""0.6.1""; MODEL_VERSION=""0.6.0""; MODEL_CL=""191676894"". # Note that we don't specify the CL number for the binary, only the bin version.; BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}+cl-*""; MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${MODEL_VERSION}/DeepVariant-inception_v3-${MODEL_VERSION}+cl-${MODEL_CL}.data-wes_standard""; DATA_BUCKET=""${BUCKET}/exome-case-study-testdata""; ```. I have also run the run_prereq.sh, however, I am not entirely sure if I have the gcp optimized TF wheel. Can you show m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/74#issuecomment-391801518
https://github.com/google/deepvariant/issues/74#issuecomment-391801518:1967,Performance,optimiz,optimized,1967,"endor_id : GenuineIntel; cpu family : 6; model : 94; model name : Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz; stepping : 3; microcode : 0xc2; cpu MHz : 1013.093; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 22; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb invpcid_single intel_pt rsb_ctxsw spec_ctrl retpoline kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx rdseed adx smap clflushopt xsaveopt xsavec xgetbv1 dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp; bugs : cpu_meltdown spectre_v1 spectre_v2; bogomips : 6816.62; clflush size : 64; cache_alignment : 64; address sizes : 39 bits physical, 48 bits virtual; power management:. I have used the preliminaries set in the exome case study, namely. ```; BASE=""/HD_disk/exome-case-study""; BUCKET=""gs://deepvariant""; BIN_VERSION=""0.6.1""; MODEL_VERSION=""0.6.0""; MODEL_CL=""191676894"". # Note that we don't specify the CL number for the binary, only the bin version.; BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}+cl-*""; MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${MODEL_VERSION}/DeepVariant-inception_v3-${MODEL_VERSION}+cl-${MODEL_CL}.data-wes_standard""; DATA_BUCKET=""${BUCKET}/exome-case-study-testdata""; ```. I have also run the run_prereq.sh, however, I am not entirely sure if I have the gcp optimized TF wheel. Can you show me which one is the gcp optimized TF wheel and how can I install that if I have not?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/74#issuecomment-391801518
https://github.com/google/deepvariant/issues/74#issuecomment-391801518:2024,Performance,optimiz,optimized,2024,"endor_id : GenuineIntel; cpu family : 6; model : 94; model name : Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz; stepping : 3; microcode : 0xc2; cpu MHz : 1013.093; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 22; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb invpcid_single intel_pt rsb_ctxsw spec_ctrl retpoline kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx rdseed adx smap clflushopt xsaveopt xsavec xgetbv1 dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp; bugs : cpu_meltdown spectre_v1 spectre_v2; bogomips : 6816.62; clflush size : 64; cache_alignment : 64; address sizes : 39 bits physical, 48 bits virtual; power management:. I have used the preliminaries set in the exome case study, namely. ```; BASE=""/HD_disk/exome-case-study""; BUCKET=""gs://deepvariant""; BIN_VERSION=""0.6.1""; MODEL_VERSION=""0.6.0""; MODEL_CL=""191676894"". # Note that we don't specify the CL number for the binary, only the bin version.; BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}+cl-*""; MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${MODEL_VERSION}/DeepVariant-inception_v3-${MODEL_VERSION}+cl-${MODEL_CL}.data-wes_standard""; DATA_BUCKET=""${BUCKET}/exome-case-study-testdata""; ```. I have also run the run_prereq.sh, however, I am not entirely sure if I have the gcp optimized TF wheel. Can you show me which one is the gcp optimized TF wheel and how can I install that if I have not?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/74#issuecomment-391801518
https://github.com/google/deepvariant/issues/74#issuecomment-391801518:1866,Testability,test,testdata,1866,"endor_id : GenuineIntel; cpu family : 6; model : 94; model name : Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz; stepping : 3; microcode : 0xc2; cpu MHz : 1013.093; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 22; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb invpcid_single intel_pt rsb_ctxsw spec_ctrl retpoline kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx rdseed adx smap clflushopt xsaveopt xsavec xgetbv1 dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp; bugs : cpu_meltdown spectre_v1 spectre_v2; bogomips : 6816.62; clflush size : 64; cache_alignment : 64; address sizes : 39 bits physical, 48 bits virtual; power management:. I have used the preliminaries set in the exome case study, namely. ```; BASE=""/HD_disk/exome-case-study""; BUCKET=""gs://deepvariant""; BIN_VERSION=""0.6.1""; MODEL_VERSION=""0.6.0""; MODEL_CL=""191676894"". # Note that we don't specify the CL number for the binary, only the bin version.; BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}+cl-*""; MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${MODEL_VERSION}/DeepVariant-inception_v3-${MODEL_VERSION}+cl-${MODEL_CL}.data-wes_standard""; DATA_BUCKET=""${BUCKET}/exome-case-study-testdata""; ```. I have also run the run_prereq.sh, however, I am not entirely sure if I have the gcp optimized TF wheel. Can you show me which one is the gcp optimized TF wheel and how can I install that if I have not?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/74#issuecomment-391801518
https://github.com/google/deepvariant/issues/74#issuecomment-391839227:726,Energy Efficiency,efficient,efficiently,726,"Hi @BowenKwan . I dug into this a bit and I think it is working as expected. Here are some take aways:. There are 57238 total examples in the exome. * On a batch_size 32 on 64 cores runs this runs at 0.67 sec per 100 [from pichuan] with total runtime 6m 21s. Expected runtime is 57238 / 100 * 0.67 seconds = 6m so this is all matching. * With a batch_size 32 on a GCE 8 cores instance DV 0.6 runs at 2.53 sec per 100, so expected runtime is 57238 / 100 * 2.53 seconds = 24 min. * The expected run rate (assuming perfect scaling from 64 => 8) is 0.67 * 8 = 5.36 sec per 100. This is 2x larger than the observed rate [in the positive direction] because 64 cores isn't as well utilized as 8 cores. So we are in fact running more efficiently on the 8 cores, again as expected. * I ran the 8 core call_variants end2end on the GCE instance and in fact it takes only 24 m. * You are reporting a run rate of 2.68 sec per 100 in one comment, which is very close to my observed GCE rate. * My guess: did you leave out the --regions ${CAPTURE_BED} argument? This changes how many examples are generated and could lead to a 8 hr runtime of call_variants even at 2.53 secs per 100. Leaving out that flag will result in candidates being generated genome-wide, so you'll have many more candidates and a concomitantly longer runtime.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/74#issuecomment-391839227
https://github.com/google/deepvariant/issues/75#issuecomment-403638598:15,Testability,log,log,15,"Hi,; from your log, it seems like this call failed `read.aligned_quality[read_pos]`, which might indicate that read doesn't have a quality score?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/75#issuecomment-403638598
https://github.com/google/deepvariant/issues/77#issuecomment-394751639:21,Testability,log,log,21,"Hi,; looking at your log closely, specifically this:; ```; File ""/tmp/Bazel.runfiles_1j54j0yh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 549, in build_calling_regions; regions = ranges.RangeSet.from_contigs(contigs); ```. and ; ```; ValueError: IntervalTree: Null Interval objects not allowed in IntervalTree: Interval(0, 0); ```. It seems like you might have contigs in your reference files that are empty. Is that expected?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/77#issuecomment-394751639
https://github.com/google/deepvariant/issues/77#issuecomment-403634683:14,Deployability,update,update,14,"If there's no update on this, I'm going to close this for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/77#issuecomment-403634683
https://github.com/google/deepvariant/issues/78#issuecomment-398508100:133,Availability,error,error,133,"Hi. What OS are you running on? I actually don't see this crash on the Ubuntu 16 setup.; If you can let me know how to reproduce the error, we'll make sure to fix it and will come out with the next release. Thanks for reporting!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/78#issuecomment-398508100
https://github.com/google/deepvariant/issues/78#issuecomment-398508100:198,Deployability,release,release,198,"Hi. What OS are you running on? I actually don't see this crash on the Ubuntu 16 setup.; If you can let me know how to reproduce the error, we'll make sure to fix it and will come out with the next release. Thanks for reporting!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/78#issuecomment-398508100
https://github.com/google/deepvariant/issues/78#issuecomment-398511601:200,Testability,test,testing,200,"Hi. This is actually happening in the Docker image provided with DeepVariant. (It's been converted to a Singularity image, but it's difficult to imagine that affecting this behavior.). Doing a little testing, the ""%m"" seems to crash Python 2 but not Python 3. Perhaps that's somehow the difference? And this is (I think) coming via TensorFlow, which might also matter. ```; Python 2.7.10 (default, Jul 14 2015, 19:46:27); [GCC 4.8.2] on linux; 'asdf %0.001mean' % (3,); Traceback (most recent call last):; File ""python"", line 1, in <module>; ValueError: unsupported format character 'm' (0x6d) at index 11; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/78#issuecomment-398511601
https://github.com/google/deepvariant/issues/78#issuecomment-398532779:84,Integrability,message,message,84,"Seems like it should work. This is indeed an annoying corner case, but the `--help` message is pretty important. :-)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/78#issuecomment-398532779
https://github.com/google/deepvariant/issues/78#issuecomment-398534109:120,Deployability,release,release,120,"Yes, I agree that --help is important! Thanks for reporting. This is now fixed internally and will come out in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/78#issuecomment-398534109
https://github.com/google/deepvariant/issues/79#issuecomment-398526499:17,Deployability,release,released,17,"No. The model we released is diploid. . Long answer: in terms of the implementation, many of the utility functions try to take into account different ploidy. But in the actual case of DeepVariant we haven't done anything other than the diploid scenario. so if you're looking for an already pre-packaged caller, the answer is no. But if you're looking for Python functions that might help with your development, please feel free to look into the code in Nucleus and deepvariant, and let us know if you have any questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/79#issuecomment-398526499
https://github.com/google/deepvariant/issues/80#issuecomment-401673684:270,Availability,down,download,270,"Thanks for your interest in trying out training!. Currently, DeepVariant is trained using a combination of public and non-public data, as described in the release notes for each version. For public data sources, like Genome in a Bottle or BaseSpace, we encourage you to download the data directly from the original source. . I'm currently working on a list of the public data sources that we're using. We'll plan to release that information in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/80#issuecomment-401673684
https://github.com/google/deepvariant/issues/80#issuecomment-401673684:155,Deployability,release,release,155,"Thanks for your interest in trying out training!. Currently, DeepVariant is trained using a combination of public and non-public data, as described in the release notes for each version. For public data sources, like Genome in a Bottle or BaseSpace, we encourage you to download the data directly from the original source. . I'm currently working on a list of the public data sources that we're using. We'll plan to release that information in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/80#issuecomment-401673684
https://github.com/google/deepvariant/issues/80#issuecomment-401673684:416,Deployability,release,release,416,"Thanks for your interest in trying out training!. Currently, DeepVariant is trained using a combination of public and non-public data, as described in the release notes for each version. For public data sources, like Genome in a Bottle or BaseSpace, we encourage you to download the data directly from the original source. . I'm currently working on a list of the public data sources that we're using. We'll plan to release that information in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/80#issuecomment-401673684
https://github.com/google/deepvariant/issues/80#issuecomment-401673684:453,Deployability,release,release,453,"Thanks for your interest in trying out training!. Currently, DeepVariant is trained using a combination of public and non-public data, as described in the release notes for each version. For public data sources, like Genome in a Bottle or BaseSpace, we encourage you to download the data directly from the original source. . I'm currently working on a list of the public data sources that we're using. We'll plan to release that information in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/80#issuecomment-401673684
https://github.com/google/deepvariant/issues/80#issuecomment-415562625:19,Deployability,release,release,19,"Hi,; In the latest release we added this page:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md; As well as a training tutorial:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md. If you decide to train a model, we would love to hear your feedback as detailed as you are willing to provide us. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/80#issuecomment-415562625
https://github.com/google/deepvariant/issues/80#issuecomment-415562625:324,Usability,feedback,feedback,324,"Hi,; In the latest release we added this page:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md; As well as a training tutorial:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md. If you decide to train a model, we would love to hear your feedback as detailed as you are willing to provide us. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/80#issuecomment-415562625
https://github.com/google/deepvariant/issues/81#issuecomment-409913088:158,Availability,error,error,158,Please provide more information. What image did you use? Did you use the runner image or deepvarinat image directly? What command did you use? Did you get an error? Any log to share?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/81#issuecomment-409913088
https://github.com/google/deepvariant/issues/81#issuecomment-409913088:169,Testability,log,log,169,Please provide more information. What image did you use? Did you use the runner image or deepvarinat image directly? What command did you use? Did you get an error? Any log to share?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/81#issuecomment-409913088
https://github.com/google/deepvariant/issues/81#issuecomment-415637165:546,Availability,checkpoint,checkpoint,546,"Hi @JoelDaon , were you able to run this?; What I found recently is that I actually needed to install `nvidia-docker` in addition to GPU driver.; I documented it for myself here:; https://gist.github.com/pichuan/6465d5f7ab56dd15a8f0d5f4d2763724. Once you have `nvidia-docker`, you'll run something like:. ```; ( time sudo nvidia-docker run \; -v /home/${USER}:/home/${USER} \; gcr.io/deepvariant-docker/deepvariant_gpu:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${EXAMPLES}"" \; --checkpoint ""${MODEL}""; ) >""${LOG_DIR}/call_variants.log"" 2>&1; ```. I'd love to hear whether you're able to get it work or not. Thank you!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/81#issuecomment-415637165
https://github.com/google/deepvariant/issues/81#issuecomment-415637165:94,Deployability,install,install,94,"Hi @JoelDaon , were you able to run this?; What I found recently is that I actually needed to install `nvidia-docker` in addition to GPU driver.; I documented it for myself here:; https://gist.github.com/pichuan/6465d5f7ab56dd15a8f0d5f4d2763724. Once you have `nvidia-docker`, you'll run something like:. ```; ( time sudo nvidia-docker run \; -v /home/${USER}:/home/${USER} \; gcr.io/deepvariant-docker/deepvariant_gpu:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${EXAMPLES}"" \; --checkpoint ""${MODEL}""; ) >""${LOG_DIR}/call_variants.log"" 2>&1; ```. I'd love to hear whether you're able to get it work or not. Thank you!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/81#issuecomment-415637165
https://github.com/google/deepvariant/issues/81#issuecomment-415637165:598,Testability,log,log,598,"Hi @JoelDaon , were you able to run this?; What I found recently is that I actually needed to install `nvidia-docker` in addition to GPU driver.; I documented it for myself here:; https://gist.github.com/pichuan/6465d5f7ab56dd15a8f0d5f4d2763724. Once you have `nvidia-docker`, you'll run something like:. ```; ( time sudo nvidia-docker run \; -v /home/${USER}:/home/${USER} \; gcr.io/deepvariant-docker/deepvariant_gpu:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${EXAMPLES}"" \; --checkpoint ""${MODEL}""; ) >""${LOG_DIR}/call_variants.log"" 2>&1; ```. I'd love to hear whether you're able to get it work or not. Thank you!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/81#issuecomment-415637165
https://github.com/google/deepvariant/issues/83#issuecomment-403616978:312,Usability,simpl,simple,312,"Hi,; I understand that it's a common use case to be compatible with GATK. We'll consider potentially adding a flag for that conversion. But since we're following the spec (using the VCF v4.3 spec: page 25 of this doc https://samtools.github.io/hts-specs/VCFv4.3.pdf), this won't be of high priority. For now the simple substitution you're doing is correct. I filed a bug internally to track. I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/83#issuecomment-403616978
https://github.com/google/deepvariant/issues/83#issuecomment-553660314:31,Deployability,update,update,31,"Hi @andremrsantos. I wanted to update this issue with recent developments for cohort merging. With the DeepVariant v0.9 release, we recommend Best practices for multi-sample variant calling with DeepVariant. We encourage you or other users interested in multi-sample calling to follow these recommendations to combine multiple DeepVariant gVCFs. In our benchmarks, we found this merging approach more accurate than merging with GATK GenotypeGVCFs (and we found merging DeepVariant gVCFs using GATK GenotypeGVCFs substantially less accurate than the single-sample DeepVariant VCFs themselves). Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/83#issuecomment-553660314
https://github.com/google/deepvariant/issues/83#issuecomment-553660314:120,Deployability,release,release,120,"Hi @andremrsantos. I wanted to update this issue with recent developments for cohort merging. With the DeepVariant v0.9 release, we recommend Best practices for multi-sample variant calling with DeepVariant. We encourage you or other users interested in multi-sample calling to follow these recommendations to combine multiple DeepVariant gVCFs. In our benchmarks, we found this merging approach more accurate than merging with GATK GenotypeGVCFs (and we found merging DeepVariant gVCFs using GATK GenotypeGVCFs substantially less accurate than the single-sample DeepVariant VCFs themselves). Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/83#issuecomment-553660314
https://github.com/google/deepvariant/issues/83#issuecomment-553660314:353,Testability,benchmark,benchmarks,353,"Hi @andremrsantos. I wanted to update this issue with recent developments for cohort merging. With the DeepVariant v0.9 release, we recommend Best practices for multi-sample variant calling with DeepVariant. We encourage you or other users interested in multi-sample calling to follow these recommendations to combine multiple DeepVariant gVCFs. In our benchmarks, we found this merging approach more accurate than merging with GATK GenotypeGVCFs (and we found merging DeepVariant gVCFs using GATK GenotypeGVCFs substantially less accurate than the single-sample DeepVariant VCFs themselves). Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/83#issuecomment-553660314
https://github.com/google/deepvariant/issues/86#issuecomment-411048876:264,Deployability,pipeline,pipeline,264,"Hi Shruti,. I notice in your .yaml that both the --outfile and --gvcf_outfile flags are set to ""${OUTPUT_BUCKET}""/""${OUTPUT_FILE_NAME}"" . These flags should have different names set, as they are two distinct output files. Please reopen this issue if rerunning the pipeline with the distinct names does not resolve your problem. regards,; Cory",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/86#issuecomment-411048876
https://github.com/google/deepvariant/issues/86#issuecomment-411146581:49,Deployability,update,update,49,"Thanks Cory. I will fix that and re-run. I shall update you if that resolves the issue. Thanks,; Shruti",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/86#issuecomment-411146581
https://github.com/google/deepvariant/issues/86#issuecomment-414199549:64,Availability,error,error,64,"Hi Cory,. After fixing the issue, the run completed without any error. ; My yaml file has arguments for both vcf and gvcf but different values.; --outfile ""${OUTPUT_BUCKET}""/""${OUTPUT_FILE_NAME}"" \; --gvcf_outfile ""${OUTPUT_BUCKET}""/""${OUTPUT_GVCF_FILE_NAME}"" \. However, the vcf generated by deepvariant seems to be small in size (495M), given that this is WGS. The vcf file that I received for this case from a clinical bioinformatics core that runs GATK3 is (2.7GB). I am wondering if I am missing something in my script for running deepvariant.; I am observing this trend on couple of other WGS cases for which I ran deepvariant. I have attached my bash and yaml file for your reference. . Will appreciate any suggestions/pointers from you. Best,; Shruti",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/86#issuecomment-414199549
https://github.com/google/deepvariant/issues/86#issuecomment-414351015:59,Availability,error,error,59,"Hi Shruti,. Glad to hear that the renaming fixed the prior error. I don't see the bash or yaml files attached, but I'm curious how many variants are present in the VCF output file and how that compares to the GATK3 output (and verifying that you're running on human samples). File sizes can vary due to additional annotations added or compression (and compression level) applied to the result. On a Unix system you can find the number of lines in the output VCF file using. wc -l ""${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME}"". or, if it's gzip compressed,. zcat ""${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME}"" | wc -l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/86#issuecomment-414351015
https://github.com/google/deepvariant/issues/87#issuecomment-413716778:567,Deployability,release,release,567,"Hi Paul,; there is actually some explanation in the file (below the Copyright lines):; https://github.com/google/deepvariant/blob/r0.7/cloudbuild.yaml#L28-L34. We use these yaml files to build and push docker images. You probably could re-use them to build and push DeepVariant docker images to projects where you have write access to. We didn't document these more, because we didn't think it's a very common use case.; If it's confusing to have these files on the top level, I can check with my colleagues and see if we can move them to a sub-directory in the next release. Would that help?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-413716778
https://github.com/google/deepvariant/issues/87#issuecomment-413716778:325,Security,access,access,325,"Hi Paul,; there is actually some explanation in the file (below the Copyright lines):; https://github.com/google/deepvariant/blob/r0.7/cloudbuild.yaml#L28-L34. We use these yaml files to build and push docker images. You probably could re-use them to build and push DeepVariant docker images to projects where you have write access to. We didn't document these more, because we didn't think it's a very common use case.; If it's confusing to have these files on the top level, I can check with my colleagues and see if we can move them to a sub-directory in the next release. Would that help?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-413716778
https://github.com/google/deepvariant/issues/87#issuecomment-413745335:1739,Availability,avail,available,1739,"directories in the tree flowing naturally from it. A minimal number of obvious scripts (with associated documentation) should naturally transition them from source code, through the build phase, and as quickly to the analysis process. Basically less is more, and you do not want to add more to the main directory as you begin to approach version 1.0. So ideally what would be nice is to have one build script - let's call it `build` - that reads a `config/build` directory - and does the following:. 1. Analyzes hidden build-generated files under `.build/...`, and detects if the `prereq` and `build-and-test` phases have been successfully processed. Otherwise, it would report back to the user the build status, and query the user with a recommendation on the next required step to run. For example:. ```; $ build status. Checking DeepVariant build prerequisites... OK; Checking DeepVariant test environment compatibility [GPU]... OK; Checking DeepVariant build binaries... MISSING. Proceed with building binaries [Y/n]?. ```. 2. Once the builds have been successfully completed, then it would tell the user the available options to run (i.e. `make_examples`, `call_variants`, `postprocess_variants`, `pkrusche/hap.py`, etc.) as well as any analysis that has been completed if a `results` or `analysis-results` folder is present. 3. For Cloud Build, that would also be part of the `build` script, when they provide the `help` argument:. ```; $ ./build help; ...; cloudbuild Builds Docker images of DeepVariant, and ; pushes them on the Google Container Registry (gcr.io) ; ... $ ./build cloudbuild help; CPU Builds a DeepVariant Docker image for CPU usage.; GPU Builds a DeepVariant Docker image for GPU usage.; Runner Builds a DeepVariant Docker image for large-scale analysis run; using the Genomics Pipelines API. $; ```. Even `Runner` is a bit too general, so maybe calling it `LargeScaleAnalysis`, or something which should be instantly recognizable as to its intended use. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-413745335
https://github.com/google/deepvariant/issues/87#issuecomment-413745335:2429,Deployability,Pipeline,Pipelines,2429,"directories in the tree flowing naturally from it. A minimal number of obvious scripts (with associated documentation) should naturally transition them from source code, through the build phase, and as quickly to the analysis process. Basically less is more, and you do not want to add more to the main directory as you begin to approach version 1.0. So ideally what would be nice is to have one build script - let's call it `build` - that reads a `config/build` directory - and does the following:. 1. Analyzes hidden build-generated files under `.build/...`, and detects if the `prereq` and `build-and-test` phases have been successfully processed. Otherwise, it would report back to the user the build status, and query the user with a recommendation on the next required step to run. For example:. ```; $ build status. Checking DeepVariant build prerequisites... OK; Checking DeepVariant test environment compatibility [GPU]... OK; Checking DeepVariant build binaries... MISSING. Proceed with building binaries [Y/n]?. ```. 2. Once the builds have been successfully completed, then it would tell the user the available options to run (i.e. `make_examples`, `call_variants`, `postprocess_variants`, `pkrusche/hap.py`, etc.) as well as any analysis that has been completed if a `results` or `analysis-results` folder is present. 3. For Cloud Build, that would also be part of the `build` script, when they provide the `help` argument:. ```; $ ./build help; ...; cloudbuild Builds Docker images of DeepVariant, and ; pushes them on the Google Container Registry (gcr.io) ; ... $ ./build cloudbuild help; CPU Builds a DeepVariant Docker image for CPU usage.; GPU Builds a DeepVariant Docker image for GPU usage.; Runner Builds a DeepVariant Docker image for large-scale analysis run; using the Genomics Pipelines API. $; ```. Even `Runner` is a bit too general, so maybe calling it `LargeScaleAnalysis`, or something which should be instantly recognizable as to its intended use. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-413745335
https://github.com/google/deepvariant/issues/87#issuecomment-413745335:1075,Modifiability,config,config,1075,"d, and has a very practical use-case for many users that would like to automate their analysis. I'm sort of hinting at something else. So just imagine you are pitching this to some Bioinformatician that has been doing their analysis on an internal cluster for years. S/he has heard from others of the benefits of DeepVariant through Google Cloud resources, and now has a little bit of free time to try it out to convince their boss to use it. The idea is that the root directory should be as simple as possible, so users don't get overwhelmed or confused - with all the other directories in the tree flowing naturally from it. A minimal number of obvious scripts (with associated documentation) should naturally transition them from source code, through the build phase, and as quickly to the analysis process. Basically less is more, and you do not want to add more to the main directory as you begin to approach version 1.0. So ideally what would be nice is to have one build script - let's call it `build` - that reads a `config/build` directory - and does the following:. 1. Analyzes hidden build-generated files under `.build/...`, and detects if the `prereq` and `build-and-test` phases have been successfully processed. Otherwise, it would report back to the user the build status, and query the user with a recommendation on the next required step to run. For example:. ```; $ build status. Checking DeepVariant build prerequisites... OK; Checking DeepVariant test environment compatibility [GPU]... OK; Checking DeepVariant build binaries... MISSING. Proceed with building binaries [Y/n]?. ```. 2. Once the builds have been successfully completed, then it would tell the user the available options to run (i.e. `make_examples`, `call_variants`, `postprocess_variants`, `pkrusche/hap.py`, etc.) as well as any analysis that has been completed if a `results` or `analysis-results` folder is present. 3. For Cloud Build, that would also be part of the `build` script, when they provide the `help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-413745335
https://github.com/google/deepvariant/issues/87#issuecomment-413745335:1191,Safety,detect,detects,1191," Bioinformatician that has been doing their analysis on an internal cluster for years. S/he has heard from others of the benefits of DeepVariant through Google Cloud resources, and now has a little bit of free time to try it out to convince their boss to use it. The idea is that the root directory should be as simple as possible, so users don't get overwhelmed or confused - with all the other directories in the tree flowing naturally from it. A minimal number of obvious scripts (with associated documentation) should naturally transition them from source code, through the build phase, and as quickly to the analysis process. Basically less is more, and you do not want to add more to the main directory as you begin to approach version 1.0. So ideally what would be nice is to have one build script - let's call it `build` - that reads a `config/build` directory - and does the following:. 1. Analyzes hidden build-generated files under `.build/...`, and detects if the `prereq` and `build-and-test` phases have been successfully processed. Otherwise, it would report back to the user the build status, and query the user with a recommendation on the next required step to run. For example:. ```; $ build status. Checking DeepVariant build prerequisites... OK; Checking DeepVariant test environment compatibility [GPU]... OK; Checking DeepVariant build binaries... MISSING. Proceed with building binaries [Y/n]?. ```. 2. Once the builds have been successfully completed, then it would tell the user the available options to run (i.e. `make_examples`, `call_variants`, `postprocess_variants`, `pkrusche/hap.py`, etc.) as well as any analysis that has been completed if a `results` or `analysis-results` folder is present. 3. For Cloud Build, that would also be part of the `build` script, when they provide the `help` argument:. ```; $ ./build help; ...; cloudbuild Builds Docker images of DeepVariant, and ; pushes them on the Google Container Registry (gcr.io) ; ... $ ./build cloudbuild help;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-413745335
https://github.com/google/deepvariant/issues/87#issuecomment-413745335:1230,Testability,test,test,1230," Bioinformatician that has been doing their analysis on an internal cluster for years. S/he has heard from others of the benefits of DeepVariant through Google Cloud resources, and now has a little bit of free time to try it out to convince their boss to use it. The idea is that the root directory should be as simple as possible, so users don't get overwhelmed or confused - with all the other directories in the tree flowing naturally from it. A minimal number of obvious scripts (with associated documentation) should naturally transition them from source code, through the build phase, and as quickly to the analysis process. Basically less is more, and you do not want to add more to the main directory as you begin to approach version 1.0. So ideally what would be nice is to have one build script - let's call it `build` - that reads a `config/build` directory - and does the following:. 1. Analyzes hidden build-generated files under `.build/...`, and detects if the `prereq` and `build-and-test` phases have been successfully processed. Otherwise, it would report back to the user the build status, and query the user with a recommendation on the next required step to run. For example:. ```; $ build status. Checking DeepVariant build prerequisites... OK; Checking DeepVariant test environment compatibility [GPU]... OK; Checking DeepVariant build binaries... MISSING. Proceed with building binaries [Y/n]?. ```. 2. Once the builds have been successfully completed, then it would tell the user the available options to run (i.e. `make_examples`, `call_variants`, `postprocess_variants`, `pkrusche/hap.py`, etc.) as well as any analysis that has been completed if a `results` or `analysis-results` folder is present. 3. For Cloud Build, that would also be part of the `build` script, when they provide the `help` argument:. ```; $ ./build help; ...; cloudbuild Builds Docker images of DeepVariant, and ; pushes them on the Google Container Registry (gcr.io) ; ... $ ./build cloudbuild help;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-413745335
https://github.com/google/deepvariant/issues/87#issuecomment-413745335:1518,Testability,test,test,1518,"ctory should be as simple as possible, so users don't get overwhelmed or confused - with all the other directories in the tree flowing naturally from it. A minimal number of obvious scripts (with associated documentation) should naturally transition them from source code, through the build phase, and as quickly to the analysis process. Basically less is more, and you do not want to add more to the main directory as you begin to approach version 1.0. So ideally what would be nice is to have one build script - let's call it `build` - that reads a `config/build` directory - and does the following:. 1. Analyzes hidden build-generated files under `.build/...`, and detects if the `prereq` and `build-and-test` phases have been successfully processed. Otherwise, it would report back to the user the build status, and query the user with a recommendation on the next required step to run. For example:. ```; $ build status. Checking DeepVariant build prerequisites... OK; Checking DeepVariant test environment compatibility [GPU]... OK; Checking DeepVariant build binaries... MISSING. Proceed with building binaries [Y/n]?. ```. 2. Once the builds have been successfully completed, then it would tell the user the available options to run (i.e. `make_examples`, `call_variants`, `postprocess_variants`, `pkrusche/hap.py`, etc.) as well as any analysis that has been completed if a `results` or `analysis-results` folder is present. 3. For Cloud Build, that would also be part of the `build` script, when they provide the `help` argument:. ```; $ ./build help; ...; cloudbuild Builds Docker images of DeepVariant, and ; pushes them on the Google Container Registry (gcr.io) ; ... $ ./build cloudbuild help; CPU Builds a DeepVariant Docker image for CPU usage.; GPU Builds a DeepVariant Docker image for GPU usage.; Runner Builds a DeepVariant Docker image for large-scale analysis run; using the Genomics Pipelines API. $; ```. Even `Runner` is a bit too general, so maybe calling it `LargeScaleAnaly",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-413745335
https://github.com/google/deepvariant/issues/87#issuecomment-413745335:542,Usability,simpl,simple,542,"Hi Pichuan,. I understand the nature of Cloud Build, and has a very practical use-case for many users that would like to automate their analysis. I'm sort of hinting at something else. So just imagine you are pitching this to some Bioinformatician that has been doing their analysis on an internal cluster for years. S/he has heard from others of the benefits of DeepVariant through Google Cloud resources, and now has a little bit of free time to try it out to convince their boss to use it. The idea is that the root directory should be as simple as possible, so users don't get overwhelmed or confused - with all the other directories in the tree flowing naturally from it. A minimal number of obvious scripts (with associated documentation) should naturally transition them from source code, through the build phase, and as quickly to the analysis process. Basically less is more, and you do not want to add more to the main directory as you begin to approach version 1.0. So ideally what would be nice is to have one build script - let's call it `build` - that reads a `config/build` directory - and does the following:. 1. Analyzes hidden build-generated files under `.build/...`, and detects if the `prereq` and `build-and-test` phases have been successfully processed. Otherwise, it would report back to the user the build status, and query the user with a recommendation on the next required step to run. For example:. ```; $ build status. Checking DeepVariant build prerequisites... OK; Checking DeepVariant test environment compatibility [GPU]... OK; Checking DeepVariant build binaries... MISSING. Proceed with building binaries [Y/n]?. ```. 2. Once the builds have been successfully completed, then it would tell the user the available options to run (i.e. `make_examples`, `call_variants`, `postprocess_variants`, `pkrusche/hap.py`, etc.) as well as any analysis that has been completed if a `results` or `analysis-results` folder is present. 3. For Cloud Build, that would also be part ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-413745335
https://github.com/google/deepvariant/issues/87#issuecomment-414456827:168,Modifiability,config,config,168,"For the use case you mentioned, one should just use published docker images (see images pointed in https://cloud.google.com/genomics/docs/tutorials/deepvariant). These config files are meant to be used by cloud build. See ""Build using a build config file"" in https://cloud.google.com/cloud-build/docs/quickstart-docker for how they are used. The tool you suggested will be useful once DeepVariant accepts external contributions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-414456827
https://github.com/google/deepvariant/issues/87#issuecomment-414456827:243,Modifiability,config,config,243,"For the use case you mentioned, one should just use published docker images (see images pointed in https://cloud.google.com/genomics/docs/tutorials/deepvariant). These config files are meant to be used by cloud build. See ""Build using a build config file"" in https://cloud.google.com/cloud-build/docs/quickstart-docker for how they are used. The tool you suggested will be useful once DeepVariant accepts external contributions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-414456827
https://github.com/google/deepvariant/issues/87#issuecomment-414549806:67,Deployability,update,updated,67,"Hi Nima,. That is a good point, and glad to see the tutorials page updated today. Though it would have been nice to have it highlight the `--use_tpu` flag option and `0.7.0` version, rather than the `0.6.x`. The tags in the docker images for the runner are still as a release candidate `0.7.0rc1`, with `0.6.1` as the `latest`. In any case, you guys are doing great work, and there is quite a lot of expertise on this side of the world as well to expand on DeepVariant's capabilities, and foster an even greater community. You might want to update the [contributing document](https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md), since it mentions 1H 2018 for external contributions and CI testing on GitHub - and it's almost September. Version 0.7 seemed to have just dropped out of the sky without any formal review process, and we could have helped you iron out some of the kinks (i.e. such as minor spelling on the [TPU case study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each) [s/Optoinal/Optional]). Your [Docker documentation](https://github.com/google/deepvariant/blob/master/docs/deepvariant-docker.md#run-the-pipeline) still lists your image to use as `0.4.1`, which has an older channel representation of the input data - though using `latest` is a comment that might make a new user not realize the amount of difference between the versions. Some things seemed a bit rushed, and doing it together would have provided more eyes with expanded tutorials for attracting new users to lower any barriers to entry for new users - who can be quite picky when trying out new software. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-414549806
https://github.com/google/deepvariant/issues/87#issuecomment-414549806:268,Deployability,release,release,268,"Hi Nima,. That is a good point, and glad to see the tutorials page updated today. Though it would have been nice to have it highlight the `--use_tpu` flag option and `0.7.0` version, rather than the `0.6.x`. The tags in the docker images for the runner are still as a release candidate `0.7.0rc1`, with `0.6.1` as the `latest`. In any case, you guys are doing great work, and there is quite a lot of expertise on this side of the world as well to expand on DeepVariant's capabilities, and foster an even greater community. You might want to update the [contributing document](https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md), since it mentions 1H 2018 for external contributions and CI testing on GitHub - and it's almost September. Version 0.7 seemed to have just dropped out of the sky without any formal review process, and we could have helped you iron out some of the kinks (i.e. such as minor spelling on the [TPU case study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each) [s/Optoinal/Optional]). Your [Docker documentation](https://github.com/google/deepvariant/blob/master/docs/deepvariant-docker.md#run-the-pipeline) still lists your image to use as `0.4.1`, which has an older channel representation of the input data - though using `latest` is a comment that might make a new user not realize the amount of difference between the versions. Some things seemed a bit rushed, and doing it together would have provided more eyes with expanded tutorials for attracting new users to lower any barriers to entry for new users - who can be quite picky when trying out new software. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-414549806
https://github.com/google/deepvariant/issues/87#issuecomment-414549806:541,Deployability,update,update,541,"Hi Nima,. That is a good point, and glad to see the tutorials page updated today. Though it would have been nice to have it highlight the `--use_tpu` flag option and `0.7.0` version, rather than the `0.6.x`. The tags in the docker images for the runner are still as a release candidate `0.7.0rc1`, with `0.6.1` as the `latest`. In any case, you guys are doing great work, and there is quite a lot of expertise on this side of the world as well to expand on DeepVariant's capabilities, and foster an even greater community. You might want to update the [contributing document](https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md), since it mentions 1H 2018 for external contributions and CI testing on GitHub - and it's almost September. Version 0.7 seemed to have just dropped out of the sky without any formal review process, and we could have helped you iron out some of the kinks (i.e. such as minor spelling on the [TPU case study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each) [s/Optoinal/Optional]). Your [Docker documentation](https://github.com/google/deepvariant/blob/master/docs/deepvariant-docker.md#run-the-pipeline) still lists your image to use as `0.4.1`, which has an older channel representation of the input data - though using `latest` is a comment that might make a new user not realize the amount of difference between the versions. Some things seemed a bit rushed, and doing it together would have provided more eyes with expanded tutorials for attracting new users to lower any barriers to entry for new users - who can be quite picky when trying out new software. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-414549806
https://github.com/google/deepvariant/issues/87#issuecomment-414549806:1091,Deployability,configurat,configuration-file-for-each,1091,"Hi Nima,. That is a good point, and glad to see the tutorials page updated today. Though it would have been nice to have it highlight the `--use_tpu` flag option and `0.7.0` version, rather than the `0.6.x`. The tags in the docker images for the runner are still as a release candidate `0.7.0rc1`, with `0.6.1` as the `latest`. In any case, you guys are doing great work, and there is quite a lot of expertise on this side of the world as well to expand on DeepVariant's capabilities, and foster an even greater community. You might want to update the [contributing document](https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md), since it mentions 1H 2018 for external contributions and CI testing on GitHub - and it's almost September. Version 0.7 seemed to have just dropped out of the sky without any formal review process, and we could have helped you iron out some of the kinks (i.e. such as minor spelling on the [TPU case study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each) [s/Optoinal/Optional]). Your [Docker documentation](https://github.com/google/deepvariant/blob/master/docs/deepvariant-docker.md#run-the-pipeline) still lists your image to use as `0.4.1`, which has an older channel representation of the input data - though using `latest` is a comment that might make a new user not realize the amount of difference between the versions. Some things seemed a bit rushed, and doing it together would have provided more eyes with expanded tutorials for attracting new users to lower any barriers to entry for new users - who can be quite picky when trying out new software. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-414549806
https://github.com/google/deepvariant/issues/87#issuecomment-414549806:1257,Deployability,pipeline,pipeline,1257,"Hi Nima,. That is a good point, and glad to see the tutorials page updated today. Though it would have been nice to have it highlight the `--use_tpu` flag option and `0.7.0` version, rather than the `0.6.x`. The tags in the docker images for the runner are still as a release candidate `0.7.0rc1`, with `0.6.1` as the `latest`. In any case, you guys are doing great work, and there is quite a lot of expertise on this side of the world as well to expand on DeepVariant's capabilities, and foster an even greater community. You might want to update the [contributing document](https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md), since it mentions 1H 2018 for external contributions and CI testing on GitHub - and it's almost September. Version 0.7 seemed to have just dropped out of the sky without any formal review process, and we could have helped you iron out some of the kinks (i.e. such as minor spelling on the [TPU case study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each) [s/Optoinal/Optional]). Your [Docker documentation](https://github.com/google/deepvariant/blob/master/docs/deepvariant-docker.md#run-the-pipeline) still lists your image to use as `0.4.1`, which has an older channel representation of the input data - though using `latest` is a comment that might make a new user not realize the amount of difference between the versions. Some things seemed a bit rushed, and doing it together would have provided more eyes with expanded tutorials for attracting new users to lower any barriers to entry for new users - who can be quite picky when trying out new software. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-414549806
https://github.com/google/deepvariant/issues/87#issuecomment-414549806:1091,Modifiability,config,configuration-file-for-each,1091,"Hi Nima,. That is a good point, and glad to see the tutorials page updated today. Though it would have been nice to have it highlight the `--use_tpu` flag option and `0.7.0` version, rather than the `0.6.x`. The tags in the docker images for the runner are still as a release candidate `0.7.0rc1`, with `0.6.1` as the `latest`. In any case, you guys are doing great work, and there is quite a lot of expertise on this side of the world as well to expand on DeepVariant's capabilities, and foster an even greater community. You might want to update the [contributing document](https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md), since it mentions 1H 2018 for external contributions and CI testing on GitHub - and it's almost September. Version 0.7 seemed to have just dropped out of the sky without any formal review process, and we could have helped you iron out some of the kinks (i.e. such as minor spelling on the [TPU case study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each) [s/Optoinal/Optional]). Your [Docker documentation](https://github.com/google/deepvariant/blob/master/docs/deepvariant-docker.md#run-the-pipeline) still lists your image to use as `0.4.1`, which has an older channel representation of the input data - though using `latest` is a comment that might make a new user not realize the amount of difference between the versions. Some things seemed a bit rushed, and doing it together would have provided more eyes with expanded tutorials for attracting new users to lower any barriers to entry for new users - who can be quite picky when trying out new software. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-414549806
https://github.com/google/deepvariant/issues/87#issuecomment-414549806:704,Testability,test,testing,704,"Hi Nima,. That is a good point, and glad to see the tutorials page updated today. Though it would have been nice to have it highlight the `--use_tpu` flag option and `0.7.0` version, rather than the `0.6.x`. The tags in the docker images for the runner are still as a release candidate `0.7.0rc1`, with `0.6.1` as the `latest`. In any case, you guys are doing great work, and there is quite a lot of expertise on this side of the world as well to expand on DeepVariant's capabilities, and foster an even greater community. You might want to update the [contributing document](https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md), since it mentions 1H 2018 for external contributions and CI testing on GitHub - and it's almost September. Version 0.7 seemed to have just dropped out of the sky without any formal review process, and we could have helped you iron out some of the kinks (i.e. such as minor spelling on the [TPU case study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each) [s/Optoinal/Optional]). Your [Docker documentation](https://github.com/google/deepvariant/blob/master/docs/deepvariant-docker.md#run-the-pipeline) still lists your image to use as `0.4.1`, which has an older channel representation of the input data - though using `latest` is a comment that might make a new user not realize the amount of difference between the versions. Some things seemed a bit rushed, and doing it together would have provided more eyes with expanded tutorials for attracting new users to lower any barriers to entry for new users - who can be quite picky when trying out new software. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-414549806
https://github.com/google/deepvariant/issues/87#issuecomment-415643215:98,Deployability,update,updated,98,"Hi @pgrosu , thanks for your feedback!; Thanks to @nmousavi 's work, the Cloud runner page is now updated:; https://cloud.google.com/genomics/docs/tutorials/deepvariant with 0.7.0, and 0.7.0 deepvariant_runner image is now tagged as latest. In terms of our GitHub page --; I fixed a few small thing such as the typo (and ran spell checking and fixed a few more!) Also fixed the `0.4.1` issue. I'll also address the contributing document at some point soon. These changes are right now still just in our internal codebase. I'll get it out when I have a chance to push out some documentation fixes. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-415643215
https://github.com/google/deepvariant/issues/87#issuecomment-415643215:29,Usability,feedback,feedback,29,"Hi @pgrosu , thanks for your feedback!; Thanks to @nmousavi 's work, the Cloud runner page is now updated:; https://cloud.google.com/genomics/docs/tutorials/deepvariant with 0.7.0, and 0.7.0 deepvariant_runner image is now tagged as latest. In terms of our GitHub page --; I fixed a few small thing such as the typo (and ran spell checking and fixed a few more!) Also fixed the `0.4.1` issue. I'll also address the contributing document at some point soon. These changes are right now still just in our internal codebase. I'll get it out when I have a chance to push out some documentation fixes. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-415643215
https://github.com/google/deepvariant/issues/87#issuecomment-416830583:364,Usability,clear,clear,364,"Hi @pgrosu , I filed an internal issue to track your suggestions in https://github.com/google/deepvariant/issues/87#issuecomment-413745335; I'll need to digest it a bit more, and probably talk to to a few users in more detail to design this experience better. On a high level, I think I'll need to at least think about overhauling the GitHub page to make it super clear how to run - even just with our docker image, it's apparently still not obvious for users to find (because it's hidden in the docs). And, the suggestion of making things as simple as possible is great one - both in terms of the directory structure, and also the output that our programs output. I think I have stared DeepVariant for too long that I'm losing empathy for people who look at it for the first time. I'll close this issue on GitHub, but will plan to think about this in more detail and hope to get back with at least some more plan later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-416830583
https://github.com/google/deepvariant/issues/87#issuecomment-416830583:543,Usability,simpl,simple,543,"Hi @pgrosu , I filed an internal issue to track your suggestions in https://github.com/google/deepvariant/issues/87#issuecomment-413745335; I'll need to digest it a bit more, and probably talk to to a few users in more detail to design this experience better. On a high level, I think I'll need to at least think about overhauling the GitHub page to make it super clear how to run - even just with our docker image, it's apparently still not obvious for users to find (because it's hidden in the docs). And, the suggestion of making things as simple as possible is great one - both in terms of the directory structure, and also the output that our programs output. I think I have stared DeepVariant for too long that I'm losing empathy for people who look at it for the first time. I'll close this issue on GitHub, but will plan to think about this in more detail and hope to get back with at least some more plan later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/87#issuecomment-416830583
https://github.com/google/deepvariant/issues/88#issuecomment-414378920:23,Availability,error,error,23,"First of all, from the error you're seeing, I think you forgot to set up the variables in that shell. Basically, if you do `echo $LOG_DIR` in that shell, you'll find it's empty. And, instead of directly using a Google Cloud shell in the browser, you can consider ssh into your machine from a terminal, like in this section:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md#request-a-machine; You can do something like `gcloud compute ssh ""${USER}-deepvariant-casestudy"" --zone ""us-west1-b""`. Using screen should certainly work. You'll just need to paste in the variable settings in this section again, because otherwise they'll all be empty strings.; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md#preliminaries",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/88#issuecomment-414378920
https://github.com/google/deepvariant/issues/88#issuecomment-414378920:124,Availability,echo,echo,124,"First of all, from the error you're seeing, I think you forgot to set up the variables in that shell. Basically, if you do `echo $LOG_DIR` in that shell, you'll find it's empty. And, instead of directly using a Google Cloud shell in the browser, you can consider ssh into your machine from a terminal, like in this section:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md#request-a-machine; You can do something like `gcloud compute ssh ""${USER}-deepvariant-casestudy"" --zone ""us-west1-b""`. Using screen should certainly work. You'll just need to paste in the variable settings in this section again, because otherwise they'll all be empty strings.; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md#preliminaries",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/88#issuecomment-414378920
https://github.com/google/deepvariant/issues/88#issuecomment-414378920:77,Modifiability,variab,variables,77,"First of all, from the error you're seeing, I think you forgot to set up the variables in that shell. Basically, if you do `echo $LOG_DIR` in that shell, you'll find it's empty. And, instead of directly using a Google Cloud shell in the browser, you can consider ssh into your machine from a terminal, like in this section:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md#request-a-machine; You can do something like `gcloud compute ssh ""${USER}-deepvariant-casestudy"" --zone ""us-west1-b""`. Using screen should certainly work. You'll just need to paste in the variable settings in this section again, because otherwise they'll all be empty strings.; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md#preliminaries",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/88#issuecomment-414378920
https://github.com/google/deepvariant/issues/88#issuecomment-414378920:592,Modifiability,variab,variable,592,"First of all, from the error you're seeing, I think you forgot to set up the variables in that shell. Basically, if you do `echo $LOG_DIR` in that shell, you'll find it's empty. And, instead of directly using a Google Cloud shell in the browser, you can consider ssh into your machine from a terminal, like in this section:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md#request-a-machine; You can do something like `gcloud compute ssh ""${USER}-deepvariant-casestudy"" --zone ""us-west1-b""`. Using screen should certainly work. You'll just need to paste in the variable settings in this section again, because otherwise they'll all be empty strings.; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md#preliminaries",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/88#issuecomment-414378920
https://github.com/google/deepvariant/issues/89#issuecomment-415797913:62,Deployability,install,installed,62,"From the log, it seems like the issue is that `bazel` was not installed.; After you run `build-prereq.sh`, can you try typing in `bazel` as a command and see if it exits?; And, if install bazel failed for you, can you paste the part of log of how the installation failed for you?. By the way, is there a reason why you want to build your own binaries? In the latest instructions, we moved to using docker directly. I'm curious whether that worked for you or not. Would love to know if there's a reason to prefer building your own binaries.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415797913
https://github.com/google/deepvariant/issues/89#issuecomment-415797913:180,Deployability,install,install,180,"From the log, it seems like the issue is that `bazel` was not installed.; After you run `build-prereq.sh`, can you try typing in `bazel` as a command and see if it exits?; And, if install bazel failed for you, can you paste the part of log of how the installation failed for you?. By the way, is there a reason why you want to build your own binaries? In the latest instructions, we moved to using docker directly. I'm curious whether that worked for you or not. Would love to know if there's a reason to prefer building your own binaries.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415797913
https://github.com/google/deepvariant/issues/89#issuecomment-415797913:251,Deployability,install,installation,251,"From the log, it seems like the issue is that `bazel` was not installed.; After you run `build-prereq.sh`, can you try typing in `bazel` as a command and see if it exits?; And, if install bazel failed for you, can you paste the part of log of how the installation failed for you?. By the way, is there a reason why you want to build your own binaries? In the latest instructions, we moved to using docker directly. I'm curious whether that worked for you or not. Would love to know if there's a reason to prefer building your own binaries.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415797913
https://github.com/google/deepvariant/issues/89#issuecomment-415797913:9,Testability,log,log,9,"From the log, it seems like the issue is that `bazel` was not installed.; After you run `build-prereq.sh`, can you try typing in `bazel` as a command and see if it exits?; And, if install bazel failed for you, can you paste the part of log of how the installation failed for you?. By the way, is there a reason why you want to build your own binaries? In the latest instructions, we moved to using docker directly. I'm curious whether that worked for you or not. Would love to know if there's a reason to prefer building your own binaries.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415797913
https://github.com/google/deepvariant/issues/89#issuecomment-415797913:236,Testability,log,log,236,"From the log, it seems like the issue is that `bazel` was not installed.; After you run `build-prereq.sh`, can you try typing in `bazel` as a command and see if it exits?; And, if install bazel failed for you, can you paste the part of log of how the installation failed for you?. By the way, is there a reason why you want to build your own binaries? In the latest instructions, we moved to using docker directly. I'm curious whether that worked for you or not. Would love to know if there's a reason to prefer building your own binaries.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415797913
https://github.com/google/deepvariant/issues/89#issuecomment-415936477:101,Availability,error,error,101,"@pichuan thank you for your prompt response, i ran bazel as you instructed and and got the following error ; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ bazel; Unexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'. And i tried the installation and got this:. solokopi@solokopi-All-Series:~$ sudo apt-get install bazel; Reading package lists... Done; Building dependency tree ; Reading state information... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; E: Unable to locate package bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415936477
https://github.com/google/deepvariant/issues/89#issuecomment-415936477:184,Availability,error,error,184,"@pichuan thank you for your prompt response, i ran bazel as you instructed and and got the following error ; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ bazel; Unexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'. And i tried the installation and got this:. solokopi@solokopi-All-Series:~$ sudo apt-get install bazel; Reading package lists... Done; Building dependency tree ; Reading state information... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; E: Unable to locate package bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415936477
https://github.com/google/deepvariant/issues/89#issuecomment-415936477:300,Deployability,install,installation,300,"@pichuan thank you for your prompt response, i ran bazel as you instructed and and got the following error ; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ bazel; Unexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'. And i tried the installation and got this:. solokopi@solokopi-All-Series:~$ sudo apt-get install bazel; Reading package lists... Done; Building dependency tree ; Reading state information... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; E: Unable to locate package bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415936477
https://github.com/google/deepvariant/issues/89#issuecomment-415936477:373,Deployability,install,install,373,"@pichuan thank you for your prompt response, i ran bazel as you instructed and and got the following error ; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ bazel; Unexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'. And i tried the installation and got this:. solokopi@solokopi-All-Series:~$ sudo apt-get install bazel; Reading package lists... Done; Building dependency tree ; Reading state information... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; E: Unable to locate package bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415936477
https://github.com/google/deepvariant/issues/89#issuecomment-415936477:428,Integrability,depend,dependency,428,"@pichuan thank you for your prompt response, i ran bazel as you instructed and and got the following error ; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ bazel; Unexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'. And i tried the installation and got this:. solokopi@solokopi-All-Series:~$ sudo apt-get install bazel; Reading package lists... Done; Building dependency tree ; Reading state information... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; E: Unable to locate package bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415936477
https://github.com/google/deepvariant/issues/89#issuecomment-415936563:101,Availability,error,error,101,"@pichuan thank you for your prompt response, i ran bazel as you instructed and and got the following error solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ bazelUnexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; And i tried the installation and got this:; solokopi@solokopi-All-Series:~$ sudo apt-get install bazelReading package lists... DoneBuilding dependency tree       Reading state information... DoneN: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extensionE: Unable to locate package bazel. On Friday, August 24, 2018, 8:40:23 AM PDT, Pi-Chuan Chang <notifications@github.com> wrote: ; ; ; From the log, it seems like the issue is that bazel was not installed.; After you run build-prereq.sh, can you try typing in bazel as a command and see if it exits?; And, if install bazel failed for you, can you paste the part of log of how the installation failed for you?. By the way, is there a reason why you want to build your own binaries? In the latest instructions, we moved to using docker directly. I'm curious whether that worked for you or not. Would love to know if there's a reason to prefer building your own binaries. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415936563
https://github.com/google/deepvariant/issues/89#issuecomment-415936563:180,Availability,error,error,180,"@pichuan thank you for your prompt response, i ran bazel as you instructed and and got the following error solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ bazelUnexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; And i tried the installation and got this:; solokopi@solokopi-All-Series:~$ sudo apt-get install bazelReading package lists... DoneBuilding dependency tree       Reading state information... DoneN: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extensionE: Unable to locate package bazel. On Friday, August 24, 2018, 8:40:23 AM PDT, Pi-Chuan Chang <notifications@github.com> wrote: ; ; ; From the log, it seems like the issue is that bazel was not installed.; After you run build-prereq.sh, can you try typing in bazel as a command and see if it exits?; And, if install bazel failed for you, can you paste the part of log of how the installation failed for you?. By the way, is there a reason why you want to build your own binaries? In the latest instructions, we moved to using docker directly. I'm curious whether that worked for you or not. Would love to know if there's a reason to prefer building your own binaries. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415936563
https://github.com/google/deepvariant/issues/89#issuecomment-415936563:296,Deployability,install,installation,296,"@pichuan thank you for your prompt response, i ran bazel as you instructed and and got the following error solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ bazelUnexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; And i tried the installation and got this:; solokopi@solokopi-All-Series:~$ sudo apt-get install bazelReading package lists... DoneBuilding dependency tree       Reading state information... DoneN: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extensionE: Unable to locate package bazel. On Friday, August 24, 2018, 8:40:23 AM PDT, Pi-Chuan Chang <notifications@github.com> wrote: ; ; ; From the log, it seems like the issue is that bazel was not installed.; After you run build-prereq.sh, can you try typing in bazel as a command and see if it exits?; And, if install bazel failed for you, can you paste the part of log of how the installation failed for you?. By the way, is there a reason why you want to build your own binaries? In the latest instructions, we moved to using docker directly. I'm curious whether that worked for you or not. Would love to know if there's a reason to prefer building your own binaries. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415936563
https://github.com/google/deepvariant/issues/89#issuecomment-415936563:369,Deployability,install,install,369,"@pichuan thank you for your prompt response, i ran bazel as you instructed and and got the following error solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ bazelUnexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; And i tried the installation and got this:; solokopi@solokopi-All-Series:~$ sudo apt-get install bazelReading package lists... DoneBuilding dependency tree       Reading state information... DoneN: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extensionE: Unable to locate package bazel. On Friday, August 24, 2018, 8:40:23 AM PDT, Pi-Chuan Chang <notifications@github.com> wrote: ; ; ; From the log, it seems like the issue is that bazel was not installed.; After you run build-prereq.sh, can you try typing in bazel as a command and see if it exits?; And, if install bazel failed for you, can you paste the part of log of how the installation failed for you?. By the way, is there a reason why you want to build your own binaries? In the latest instructions, we moved to using docker directly. I'm curious whether that worked for you or not. Would love to know if there's a reason to prefer building your own binaries. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415936563
https://github.com/google/deepvariant/issues/89#issuecomment-415936563:788,Deployability,install,installed,788,"@pichuan thank you for your prompt response, i ran bazel as you instructed and and got the following error solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ bazelUnexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; And i tried the installation and got this:; solokopi@solokopi-All-Series:~$ sudo apt-get install bazelReading package lists... DoneBuilding dependency tree       Reading state information... DoneN: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extensionE: Unable to locate package bazel. On Friday, August 24, 2018, 8:40:23 AM PDT, Pi-Chuan Chang <notifications@github.com> wrote: ; ; ; From the log, it seems like the issue is that bazel was not installed.; After you run build-prereq.sh, can you try typing in bazel as a command and see if it exits?; And, if install bazel failed for you, can you paste the part of log of how the installation failed for you?. By the way, is there a reason why you want to build your own binaries? In the latest instructions, we moved to using docker directly. I'm curious whether that worked for you or not. Would love to know if there's a reason to prefer building your own binaries. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415936563
https://github.com/google/deepvariant/issues/89#issuecomment-415936563:902,Deployability,install,install,902,"@pichuan thank you for your prompt response, i ran bazel as you instructed and and got the following error solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ bazelUnexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; And i tried the installation and got this:; solokopi@solokopi-All-Series:~$ sudo apt-get install bazelReading package lists... DoneBuilding dependency tree       Reading state information... DoneN: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extensionE: Unable to locate package bazel. On Friday, August 24, 2018, 8:40:23 AM PDT, Pi-Chuan Chang <notifications@github.com> wrote: ; ; ; From the log, it seems like the issue is that bazel was not installed.; After you run build-prereq.sh, can you try typing in bazel as a command and see if it exits?; And, if install bazel failed for you, can you paste the part of log of how the installation failed for you?. By the way, is there a reason why you want to build your own binaries? In the latest instructions, we moved to using docker directly. I'm curious whether that worked for you or not. Would love to know if there's a reason to prefer building your own binaries. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415936563
https://github.com/google/deepvariant/issues/89#issuecomment-415936563:973,Deployability,install,installation,973,"@pichuan thank you for your prompt response, i ran bazel as you instructed and and got the following error solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ bazelUnexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; And i tried the installation and got this:; solokopi@solokopi-All-Series:~$ sudo apt-get install bazelReading package lists... DoneBuilding dependency tree       Reading state information... DoneN: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extensionE: Unable to locate package bazel. On Friday, August 24, 2018, 8:40:23 AM PDT, Pi-Chuan Chang <notifications@github.com> wrote: ; ; ; From the log, it seems like the issue is that bazel was not installed.; After you run build-prereq.sh, can you try typing in bazel as a command and see if it exits?; And, if install bazel failed for you, can you paste the part of log of how the installation failed for you?. By the way, is there a reason why you want to build your own binaries? In the latest instructions, we moved to using docker directly. I'm curious whether that worked for you or not. Would love to know if there's a reason to prefer building your own binaries. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415936563
https://github.com/google/deepvariant/issues/89#issuecomment-415936563:420,Integrability,depend,dependency,420,"@pichuan thank you for your prompt response, i ran bazel as you instructed and and got the following error solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ bazelUnexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; And i tried the installation and got this:; solokopi@solokopi-All-Series:~$ sudo apt-get install bazelReading package lists... DoneBuilding dependency tree       Reading state information... DoneN: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extensionE: Unable to locate package bazel. On Friday, August 24, 2018, 8:40:23 AM PDT, Pi-Chuan Chang <notifications@github.com> wrote: ; ; ; From the log, it seems like the issue is that bazel was not installed.; After you run build-prereq.sh, can you try typing in bazel as a command and see if it exits?; And, if install bazel failed for you, can you paste the part of log of how the installation failed for you?. By the way, is there a reason why you want to build your own binaries? In the latest instructions, we moved to using docker directly. I'm curious whether that worked for you or not. Would love to know if there's a reason to prefer building your own binaries. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415936563
https://github.com/google/deepvariant/issues/89#issuecomment-415936563:737,Testability,log,log,737,"@pichuan thank you for your prompt response, i ran bazel as you instructed and and got the following error solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ bazelUnexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; And i tried the installation and got this:; solokopi@solokopi-All-Series:~$ sudo apt-get install bazelReading package lists... DoneBuilding dependency tree       Reading state information... DoneN: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extensionE: Unable to locate package bazel. On Friday, August 24, 2018, 8:40:23 AM PDT, Pi-Chuan Chang <notifications@github.com> wrote: ; ; ; From the log, it seems like the issue is that bazel was not installed.; After you run build-prereq.sh, can you try typing in bazel as a command and see if it exits?; And, if install bazel failed for you, can you paste the part of log of how the installation failed for you?. By the way, is there a reason why you want to build your own binaries? In the latest instructions, we moved to using docker directly. I'm curious whether that worked for you or not. Would love to know if there's a reason to prefer building your own binaries. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415936563
https://github.com/google/deepvariant/issues/89#issuecomment-415936563:958,Testability,log,log,958,"@pichuan thank you for your prompt response, i ran bazel as you instructed and and got the following error solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ bazelUnexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; And i tried the installation and got this:; solokopi@solokopi-All-Series:~$ sudo apt-get install bazelReading package lists... DoneBuilding dependency tree       Reading state information... DoneN: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extensionE: Unable to locate package bazel. On Friday, August 24, 2018, 8:40:23 AM PDT, Pi-Chuan Chang <notifications@github.com> wrote: ; ; ; From the log, it seems like the issue is that bazel was not installed.; After you run build-prereq.sh, can you try typing in bazel as a command and see if it exits?; And, if install bazel failed for you, can you paste the part of log of how the installation failed for you?. By the way, is there a reason why you want to build your own binaries? In the latest instructions, we moved to using docker directly. I'm curious whether that worked for you or not. Would love to know if there's a reason to prefer building your own binaries. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415936563
https://github.com/google/deepvariant/issues/89#issuecomment-415937760:50,Deployability,install,installed,50,"Hi,; if you run `build-prereq.sh`, it should have installed it for you. This section is relevant:; https://github.com/google/deepvariant/blob/r0.7/build-prereq.sh#L88. But if that somehow didn't work for you, you can directly use the instructions on bazel's page:; https://docs.bazel.build/versions/master/install-ubuntu.html#install-with-installer-ubuntu. I would still recommend using our docker image instead of building your own binaries!! If you do that, you don't even need to install bazel!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415937760
https://github.com/google/deepvariant/issues/89#issuecomment-415937760:306,Deployability,install,install-ubuntu,306,"Hi,; if you run `build-prereq.sh`, it should have installed it for you. This section is relevant:; https://github.com/google/deepvariant/blob/r0.7/build-prereq.sh#L88. But if that somehow didn't work for you, you can directly use the instructions on bazel's page:; https://docs.bazel.build/versions/master/install-ubuntu.html#install-with-installer-ubuntu. I would still recommend using our docker image instead of building your own binaries!! If you do that, you don't even need to install bazel!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415937760
https://github.com/google/deepvariant/issues/89#issuecomment-415937760:326,Deployability,install,install-with-installer-ubuntu,326,"Hi,; if you run `build-prereq.sh`, it should have installed it for you. This section is relevant:; https://github.com/google/deepvariant/blob/r0.7/build-prereq.sh#L88. But if that somehow didn't work for you, you can directly use the instructions on bazel's page:; https://docs.bazel.build/versions/master/install-ubuntu.html#install-with-installer-ubuntu. I would still recommend using our docker image instead of building your own binaries!! If you do that, you don't even need to install bazel!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415937760
https://github.com/google/deepvariant/issues/89#issuecomment-415937760:483,Deployability,install,install,483,"Hi,; if you run `build-prereq.sh`, it should have installed it for you. This section is relevant:; https://github.com/google/deepvariant/blob/r0.7/build-prereq.sh#L88. But if that somehow didn't work for you, you can directly use the instructions on bazel's page:; https://docs.bazel.build/versions/master/install-ubuntu.html#install-with-installer-ubuntu. I would still recommend using our docker image instead of building your own binaries!! If you do that, you don't even need to install bazel!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415937760
https://github.com/google/deepvariant/issues/89#issuecomment-415959978:140,Deployability,install,installation,140,"so, had been finding my ways around it but still. please can you give me more information on how to use the docker image, because the bazel installation is not just working out for me. thank you i so much appreciate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415959978
https://github.com/google/deepvariant/issues/89#issuecomment-415984031:16,Deployability,update,updated,16,"Hi, we recently updated the quick start and case studies to use docker.; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md. It seems like I should rearrange and simplify our documentation to make it more clear.; Can you tell me where is the place you first read? Is it the main github page, or did you clone the codebase and directly start from there?; I will try to make some improvement next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415984031
https://github.com/google/deepvariant/issues/89#issuecomment-415984031:271,Usability,simpl,simplify,271,"Hi, we recently updated the quick start and case studies to use docker.; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md. It seems like I should rearrange and simplify our documentation to make it more clear.; Can you tell me where is the place you first read? Is it the main github page, or did you clone the codebase and directly start from there?; I will try to make some improvement next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415984031
https://github.com/google/deepvariant/issues/89#issuecomment-415984031:314,Usability,clear,clear,314,"Hi, we recently updated the quick start and case studies to use docker.; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md. It seems like I should rearrange and simplify our documentation to make it more clear.; Can you tell me where is the place you first read? Is it the main github page, or did you clone the codebase and directly start from there?; I will try to make some improvement next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-415984031
https://github.com/google/deepvariant/issues/89#issuecomment-416270251:290,Deployability,install,install,290,"@pichuan thank you so much for your help on this, running it via docker is easier but i experienced some challenges pulling the deepvariant container from the docker hub, which i presume is a proxy issue, i think i need a vpn because i am in china and google domains are blocked. trying to install a vpn on my ubuntu.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416270251
https://github.com/google/deepvariant/issues/89#issuecomment-416272658:185,Availability,down,download,185,"@solokopi since you're already on Ubuntu 16, you can also try using the binaries that we built.; You can just get it from this zip file:; https://github.com/google/deepvariant/releases/download/v0.7.0/deepvariant.zip; which has the binaries and the model files. You'll need to run `run-prereq.sh` first to set up your machine. But that will not require installing bazel.; Please let me know if that works for you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416272658
https://github.com/google/deepvariant/issues/89#issuecomment-416272658:176,Deployability,release,releases,176,"@solokopi since you're already on Ubuntu 16, you can also try using the binaries that we built.; You can just get it from this zip file:; https://github.com/google/deepvariant/releases/download/v0.7.0/deepvariant.zip; which has the binaries and the model files. You'll need to run `run-prereq.sh` first to set up your machine. But that will not require installing bazel.; Please let me know if that works for you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416272658
https://github.com/google/deepvariant/issues/89#issuecomment-416272658:353,Deployability,install,installing,353,"@solokopi since you're already on Ubuntu 16, you can also try using the binaries that we built.; You can just get it from this zip file:; https://github.com/google/deepvariant/releases/download/v0.7.0/deepvariant.zip; which has the binaries and the model files. You'll need to run `run-prereq.sh` first to set up your machine. But that will not require installing bazel.; Please let me know if that works for you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416272658
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:3237,Availability,down,download,3237,"i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://storage.googleapis.com/bazel-apt/dists/stable/InRelease Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:6811,Availability,down,download,6811,"t.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4012:1::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:485,Deployability,Release,Release,485,"@pichuan hi, i got the zip file and ra run-prereq.sh and this was the output:. solokopi@solokopi-All-Series:~/Desktop/DeepVariant-0.7.0+cl-208818123$ sudo bash run-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018年 08月 28日 星期二 10:31:05 CST] Stage 'Misc setup' starting; Ign:1 http://dl.google.com/linux/chrome/deb stable InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial InRelease ; Hit:3 http://dl.google.com/linux/chrome/deb stable Release ; Hit:4 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:6 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Hit:8 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Err:9 http://storage.googleapis.com/bazel-apt stable InRelease ; Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; Err:10 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:541,Deployability,update,updates,541,"@pichuan hi, i got the zip file and ra run-prereq.sh and this was the output:. solokopi@solokopi-All-Series:~/Desktop/DeepVariant-0.7.0+cl-208818123$ sudo bash run-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018年 08月 28日 星期二 10:31:05 CST] Stage 'Misc setup' starting; Ign:1 http://dl.google.com/linux/chrome/deb stable InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial InRelease ; Hit:3 http://dl.google.com/linux/chrome/deb stable Release ; Hit:4 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:6 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Hit:8 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Err:9 http://storage.googleapis.com/bazel-apt stable InRelease ; Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; Err:10 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:4768,Deployability,install,installed,4768,"oud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/goo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:4987,Deployability,upgrade,upgraded,4987,".list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Tra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:5005,Deployability,install,installed,5005,".list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Tra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:5038,Deployability,upgrade,upgraded,5038,".list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Tra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:5221,Deployability,Update,Update,5221,"omponents-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple ti",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:8201,Deployability,Install,Install,8201,"ist.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Buildi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:8662,Deployability,install,installed,8662,"ltiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:8881,Deployability,upgrade,upgraded,8881,":1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-da",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:8899,Deployability,install,installed,8899,":1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-da",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:8932,Deployability,upgrade,upgraded,8932,":1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-da",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:9115,Deployability,Install,Install,9115,"tc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018年 08月 28日 星期二 10:55:12 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:9479,Deployability,install,installed,9479,"n (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018年 08月 28日 星期二 10:55:12 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontain",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:9698,Deployability,upgrade,upgraded,9698,"4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018年 08月 28日 星期二 10:55:12 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontainers in /usr/local/lib/python2.7/dist-packages (from intervaltree) (1.5.3); Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0); Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:9716,Deployability,install,installed,9716,"4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018年 08月 28日 星期二 10:55:12 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontainers in /usr/local/lib/python2.7/dist-packages (from intervaltree) (1.5.3); Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0); Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:9749,Deployability,upgrade,upgraded,9749,"4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018年 08月 28日 星期二 10:55:12 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontainers in /usr/local/lib/python2.7/dist-packages (from intervaltree) (1.5.3); Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0); Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:10018,Deployability,Install,Install,10018,"d filename extension; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018年 08月 28日 星期二 10:55:12 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontainers in /usr/local/lib/python2.7/dist-packages (from intervaltree) (1.5.3); Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0); Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (4.2.0); Requirement already satisfied: funcsigs>=1; python_version < ""3.3"" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.0.2); Requirement already satisfied: six>=1.9 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.11.0); Requirement already satisfied: num",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:13800,Deployability,upgrade,upgrade,13800,"ages (1.11.0); Requirement already satisfied: sklearn in /usr/local/lib/python2.7/dist-packages (0.0); Requirement already satisfied: scikit-learn in /usr/local/lib/python2.7/dist-packages (from sklearn) (0.19.2); Requirement already satisfied: pandas in /usr/local/lib/python2.7/dist-packages (0.23.4); Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (1.14.0); Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (2.7.3); Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas) (2018.5); Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:13955,Deployability,upgrade,upgrade,13955,"cal/lib/python2.7/dist-packages (from sklearn) (0.19.2); Requirement already satisfied: pandas in /usr/local/lib/python2.7/dist-packages (0.23.4); Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (1.14.0); Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (2.7.3); Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas) (2018.5); Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/loc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:14112,Deployability,upgrade,upgrade,14112," already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (1.14.0); Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (2.7.3); Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas) (2018.5); Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:14272,Deployability,upgrade,upgrade,14272," /usr/local/lib/python2.7/dist-packages (from pandas) (2.7.3); Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas) (2018.5); Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 28日 星期二 10:55:17 CST]",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:14422,Deployability,upgrade,upgrade,14422,"om pandas) (2018.5); Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 28日 星期二 10:55:17 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:14573,Deployability,upgrade,upgrade,14573,"0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 28日 星期二 10:55:17 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:14736,Deployability,upgrade,upgrade,14736,"b/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 28日 星期二 10:55:17 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:02:09 --:--:-- 0curl: (7) Fa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:14906,Deployability,upgrade,upgrade,14906,"g upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 28日 星期二 10:55:17 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:02:09 --:--:-- 0curl: (7) Failed to connect to storage.googleapis.com port 443: Connection timed out",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:15080,Deployability,upgrade,upgrade,15080,"g upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 28日 星期二 10:55:17 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:02:09 --:--:-- 0curl: (7) Failed to connect to storage.googleapis.com port 443: Connection timed out",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:15269,Deployability,Install,Install,15269,"g upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 28日 星期二 10:55:17 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:02:09 --:--:-- 0curl: (7) Failed to connect to storage.googleapis.com port 443: Connection timed out",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:15344,Deployability,install,installed,15344,"g upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 28日 星期二 10:55:17 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:02:09 --:--:-- 0curl: (7) Failed to connect to storage.googleapis.com port 443: Connection timed out",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:15389,Deployability,install,installed,15389,"g upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 28日 星期二 10:55:17 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:02:09 --:--:-- 0curl: (7) Failed to connect to storage.googleapis.com port 443: Connection timed out",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:15438,Deployability,install,installed,15438,"g upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 28日 星期二 10:55:17 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:02:09 --:--:-- 0curl: (7) Failed to connect to storage.googleapis.com port 443: Connection timed out",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:15487,Deployability,install,installed,15487,"g upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 28日 星期二 10:55:17 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:02:09 --:--:-- 0curl: (7) Failed to connect to storage.googleapis.com port 443: Connection timed out",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:15499,Deployability,Install,Installing,15499,"g upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 28日 星期二 10:55:17 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:02:09 --:--:-- 0curl: (7) Failed to connect to storage.googleapis.com port 443: Connection timed out",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:4616,Integrability,depend,dependency,4616,"t.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Package",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:8281,Integrability,depend,dependency,8281,"ltiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:9206,Integrability,depend,dependency,9206," packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018年 08月 28日 星期二 10:55:12 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:223,Modifiability,config,config,223,"@pichuan hi, i got the zip file and ra run-prereq.sh and this was the output:. solokopi@solokopi-All-Series:~/Desktop/DeepVariant-0.7.0+cl-208818123$ sudo bash run-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018年 08月 28日 星期二 10:31:05 CST] Stage 'Misc setup' starting; Ign:1 http://dl.google.com/linux/chrome/deb stable InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial InRelease ; Hit:3 http://dl.google.com/linux/chrome/deb stable Release ; Hit:4 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:6 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Hit:8 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Err:9 http://storage.googleapis.com/bazel-apt stable InRelease ; Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; Err:10 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:1484,Modifiability,config,configured,1484,m/linux/chrome/deb stable Release ; Hit:4 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:6 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Hit:8 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Err:9 http://storage.googleapis.com/bazel-apt stable InRelease ; Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; Err:10 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-clo,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:1664,Modifiability,config,configured,1664,.aliyun.com/ubuntu xenial-security InRelease ; Hit:8 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Err:9 http://storage.googleapis.com/bazel-apt stable InRelease ; Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; Err:10 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:1843,Modifiability,config,configured,1843,ease ; Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; Err:10 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://storage.googleapis.com/bazel-apt/dists/stable/InRelease Cannot initiate the connection to storage.goo,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:2029,Modifiability,config,configured,2029,kages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://storage.googleapis.com/bazel-apt/dists/stable/InRelease Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:2212,Modifiability,config,configured,2212,e) [IP: 2404:6800:4008:803::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://storage.googleapis.com/bazel-apt/dists/stable/InRelease Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80],MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:2396,Modifiability,config,configured,2396,"lename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://storage.googleapis.com/bazel-apt/dists/stable/InRelease Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:2581,Modifiability,config,configured,2581,"d-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://storage.googleapis.com/bazel-apt/dists/stable/InRelease Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:3349,Modifiability,config,configured,3349,"Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://storage.googleapis.com/bazel-apt/dists/stable/InRelease Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:3529,Modifiability,config,configured,3529,"2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://storage.googleapis.com/bazel-apt/dists/stable/InRelease Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:3708,Modifiability,config,configured,3708,".list:2; W: Failed to fetch http://storage.googleapis.com/bazel-apt/dists/stable/InRelease Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:3894,Modifiability,config,configured,3894,"ect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-imag",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:4077,Modifiability,config,configured,4077,"on to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'goo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:4261,Modifiability,config,configured,4261,"es used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/bin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:4446,Modifiability,config,configured,4446,"d-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:5303,Modifiability,config,configured,5303,"omponents-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple ti",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:5483,Modifiability,config,configured,5483,"tc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:5662,Modifiability,config,configured,5662,"ion... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to pack",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:5848,Modifiability,config,configured,5848,"36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4012:1::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::200e 80]; W: Some index files failed to download. They have been ign",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:6031,Modifiability,config,configured,6031,"move and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4012:1::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.lis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:6215,Modifiability,config,configured,6215,"e list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4012:1::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:6400,Modifiability,config,configured,6400,"d-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4012:1::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Targ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:6923,Modifiability,config,configured,6923,"ist:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4012:1::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:7103,Modifiability,config,configured,7103,"sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4012:1::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:7282,Modifiability,config,configured,7282,"loud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4012:1::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install development packages' starting; Reading package lists... Done; Build",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:7468,Modifiability,config,configured,7468,"-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4012:1::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is alr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:7651,Modifiability,config,configured,7651,"ection to packages.cloud.google.com:80 (2404:6800:4012:1::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages w",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:7835,Modifiability,config,configured,7835,es used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt auto,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:8020,Modifiability,config,configured,8020,"d-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an inv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:8338,Modifiability,config,config,8338,"list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest versi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:218,Performance,Load,Load,218,"@pichuan hi, i got the zip file and ra run-prereq.sh and this was the output:. solokopi@solokopi-All-Series:~/Desktop/DeepVariant-0.7.0+cl-208818123$ sudo bash run-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018年 08月 28日 星期二 10:31:05 CST] Stage 'Misc setup' starting; Ign:1 http://dl.google.com/linux/chrome/deb stable InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial InRelease ; Hit:3 http://dl.google.com/linux/chrome/deb stable Release ; Hit:4 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:6 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Hit:8 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Err:9 http://storage.googleapis.com/bazel-apt stable InRelease ; Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; Err:10 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:14745,Performance,cache,cachetools,14745,"b/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 28日 星期二 10:55:17 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:02:09 --:--:-- 0curl: (7) Fa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:15532,Performance,optimiz,optimized,15532,"g upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 28日 星期二 10:55:17 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:02:09 --:--:-- 0curl: (7) Failed to connect to storage.googleapis.com port 443: Connection timed out",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:182,Security,password,password,182,"@pichuan hi, i got the zip file and ra run-prereq.sh and this was the output:. solokopi@solokopi-All-Series:~/Desktop/DeepVariant-0.7.0+cl-208818123$ sudo bash run-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018年 08月 28日 星期二 10:31:05 CST] Stage 'Misc setup' starting; Ign:1 http://dl.google.com/linux/chrome/deb stable InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial InRelease ; Hit:3 http://dl.google.com/linux/chrome/deb stable Release ; Hit:4 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:6 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Hit:8 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Err:9 http://storage.googleapis.com/bazel-apt stable InRelease ; Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; Err:10 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:684,Security,secur,security,684,"@pichuan hi, i got the zip file and ra run-prereq.sh and this was the output:. solokopi@solokopi-All-Series:~/Desktop/DeepVariant-0.7.0+cl-208818123$ sudo bash run-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018年 08月 28日 星期二 10:31:05 CST] Stage 'Misc setup' starting; Ign:1 http://dl.google.com/linux/chrome/deb stable InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial InRelease ; Hit:3 http://dl.google.com/linux/chrome/deb stable Release ; Hit:4 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:6 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Hit:8 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Err:9 http://storage.googleapis.com/bazel-apt stable InRelease ; Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; Err:10 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:10587,Testability,mock,mock,10587,"-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018年 08月 28日 星期二 10:55:12 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontainers in /usr/local/lib/python2.7/dist-packages (from intervaltree) (1.5.3); Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0); Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (4.2.0); Requirement already satisfied: funcsigs>=1; python_version < ""3.3"" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.0.2); Requirement already satisfied: six>=1.9 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.11.0); Requirement already satisfied: numpy==1.14 in /usr/local/lib/python2.7/dist-packages (1.14.0); Requirement already satisfied: requests>=2.18 in /usr/local/lib/python2.7/dist-packages (2.19.1); Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (2018.8.13); Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (3.0.4); Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:10739,Testability,mock,mock,10739,"remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018年 08月 28日 星期二 10:55:12 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontainers in /usr/local/lib/python2.7/dist-packages (from intervaltree) (1.5.3); Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0); Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (4.2.0); Requirement already satisfied: funcsigs>=1; python_version < ""3.3"" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.0.2); Requirement already satisfied: six>=1.9 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.11.0); Requirement already satisfied: numpy==1.14 in /usr/local/lib/python2.7/dist-packages (1.14.0); Requirement already satisfied: requests>=2.18 in /usr/local/lib/python2.7/dist-packages (2.19.1); Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (2018.8.13); Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (3.0.4); Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (1.23); Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (2.7); Requirement already satisfied: scip",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:10876,Testability,mock,mock,10876," extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018年 08月 28日 星期二 10:55:12 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontainers in /usr/local/lib/python2.7/dist-packages (from intervaltree) (1.5.3); Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0); Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (4.2.0); Requirement already satisfied: funcsigs>=1; python_version < ""3.3"" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.0.2); Requirement already satisfied: six>=1.9 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.11.0); Requirement already satisfied: numpy==1.14 in /usr/local/lib/python2.7/dist-packages (1.14.0); Requirement already satisfied: requests>=2.18 in /usr/local/lib/python2.7/dist-packages (2.19.1); Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (2018.8.13); Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (3.0.4); Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (1.23); Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (2.7); Requirement already satisfied: scipy==1.0 in /usr/local/lib/python2.7/dist-packages (1.0.0); Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python2.7/dist-pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:10986,Testability,mock,mock,10986,"2018年 08月 28日 星期二 10:55:12 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontainers in /usr/local/lib/python2.7/dist-packages (from intervaltree) (1.5.3); Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0); Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (4.2.0); Requirement already satisfied: funcsigs>=1; python_version < ""3.3"" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.0.2); Requirement already satisfied: six>=1.9 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.11.0); Requirement already satisfied: numpy==1.14 in /usr/local/lib/python2.7/dist-packages (1.14.0); Requirement already satisfied: requests>=2.18 in /usr/local/lib/python2.7/dist-packages (2.19.1); Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (2018.8.13); Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (3.0.4); Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (1.23); Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (2.7); Requirement already satisfied: scipy==1.0 in /usr/local/lib/python2.7/dist-packages (1.0.0); Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python2.7/dist-packages (from scipy==1.0) (1.14.0); Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416438760:12932,Usability,learn,learn,12932,"isfied: oauth2client>=4.0.0 in /usr/local/lib/python2.7/dist-packages (4.1.2); Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (0.4.4); Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (0.2.2); Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (1.11.0); Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (0.11.3); Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (3.4.2); Requirement already satisfied: crcmod>=1.7 in /usr/local/lib/python2.7/dist-packages (1.7); Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (1.11.0); Requirement already satisfied: sklearn in /usr/local/lib/python2.7/dist-packages (0.0); Requirement already satisfied: scikit-learn in /usr/local/lib/python2.7/dist-packages (from sklearn) (0.19.2); Requirement already satisfied: pandas in /usr/local/lib/python2.7/dist-packages (0.23.4); Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (1.14.0); Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (2.7.3); Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas) (2018.5); Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760
https://github.com/google/deepvariant/issues/89#issuecomment-416439549:118,Performance,optimiz,optimized,118,"It seems like not having connection to ""storage.googleapis.com"" is the issue. And it seems like it's trying to get an optimized tensorflow wheel.; What if you try with `DV_USE_GCP_OPTIMIZED_TF_WHL=0` ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416439549
https://github.com/google/deepvariant/issues/89#issuecomment-416480421:30,Availability,error,errors,30,i still get the same previous errors.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416480421
https://github.com/google/deepvariant/issues/89#issuecomment-416721742:75,Security,access,access,75,"@solokopi What country is the `solokopi-All-Series` host are you trying to access the `storage.googleapis.com` domain from? Do you have root access on that machine? If you cannot access the domain, try to get some VPN access on your machine that would allow you access to the `storage.googleapis.com` domain.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416721742
https://github.com/google/deepvariant/issues/89#issuecomment-416721742:141,Security,access,access,141,"@solokopi What country is the `solokopi-All-Series` host are you trying to access the `storage.googleapis.com` domain from? Do you have root access on that machine? If you cannot access the domain, try to get some VPN access on your machine that would allow you access to the `storage.googleapis.com` domain.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416721742
https://github.com/google/deepvariant/issues/89#issuecomment-416721742:179,Security,access,access,179,"@solokopi What country is the `solokopi-All-Series` host are you trying to access the `storage.googleapis.com` domain from? Do you have root access on that machine? If you cannot access the domain, try to get some VPN access on your machine that would allow you access to the `storage.googleapis.com` domain.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416721742
https://github.com/google/deepvariant/issues/89#issuecomment-416721742:218,Security,access,access,218,"@solokopi What country is the `solokopi-All-Series` host are you trying to access the `storage.googleapis.com` domain from? Do you have root access on that machine? If you cannot access the domain, try to get some VPN access on your machine that would allow you access to the `storage.googleapis.com` domain.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416721742
https://github.com/google/deepvariant/issues/89#issuecomment-416721742:262,Security,access,access,262,"@solokopi What country is the `solokopi-All-Series` host are you trying to access the `storage.googleapis.com` domain from? Do you have root access on that machine? If you cannot access the domain, try to get some VPN access on your machine that would allow you access to the `storage.googleapis.com` domain.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-416721742
https://github.com/google/deepvariant/issues/89#issuecomment-417210803:95,Availability,Error,Error,95,thank you @pgrosu please could it had been same vpn problem with this docker pulling command?. Error response from daemon: Get https://gcr.io/v1/_ping: dial tcp 64.233.187.82:443: i/o timeout,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-417210803
https://github.com/google/deepvariant/issues/89#issuecomment-417210803:184,Safety,timeout,timeout,184,thank you @pgrosu please could it had been same vpn problem with this docker pulling command?. Error response from daemon: Get https://gcr.io/v1/_ping: dial tcp 64.233.187.82:443: i/o timeout,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/89#issuecomment-417210803
https://github.com/google/deepvariant/issues/90#issuecomment-416729239:87,Testability,log,logical,87,"@A-Tsai Try using `taskset` to assign which cores your process should go to, where the logical cores as enabled by hyperthreading keep an architectural state of your running process. Here's a link to the `taskset` manpage:. https://linux.die.net/man/1/taskset",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-416729239
https://github.com/google/deepvariant/issues/90#issuecomment-416812290:400,Availability,mask,mask,400,"@A-Tsai Of course you can especially with `pipe()`, which can take a script/command as input. At the least, you have the following two options to play with:. 1. If you use pipe to launch a script, then you can launch the application through `taskset` to limit the number of cores at launch-time. Here's the options for launching with an example: . _*Options to launch a program*_: `taskset [options] mask command [argument...]`. _*Example to use only 2 specific cores*_: `taskset -c 0,2 python ~/loop.py`. The `pipe(...)` command [as defined in the RDD base-class](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD) takes an external command, which in this case would be `taskset -c core-list your-program`:. ![rdd-pipe](https://user-images.githubusercontent.com/6555937/44763439-4c73f500-ab19-11e8-99d7-99adac28c913.png). 2. In Spark, you can also limit the number of cores per task in Spark through the `spark.task.cpus` setting. You probably want to set `spark.cores.max`, and not change the `spark.executor.cores` and `spark.driver.cores`. The Spark config page explains everything in more detail here: . https://spark.apache.org/docs/latest/configuration.html. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-416812290
https://github.com/google/deepvariant/issues/90#issuecomment-416812290:1179,Deployability,configurat,configuration,1179,"@A-Tsai Of course you can especially with `pipe()`, which can take a script/command as input. At the least, you have the following two options to play with:. 1. If you use pipe to launch a script, then you can launch the application through `taskset` to limit the number of cores at launch-time. Here's the options for launching with an example: . _*Options to launch a program*_: `taskset [options] mask command [argument...]`. _*Example to use only 2 specific cores*_: `taskset -c 0,2 python ~/loop.py`. The `pipe(...)` command [as defined in the RDD base-class](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD) takes an external command, which in this case would be `taskset -c core-list your-program`:. ![rdd-pipe](https://user-images.githubusercontent.com/6555937/44763439-4c73f500-ab19-11e8-99d7-99adac28c913.png). 2. In Spark, you can also limit the number of cores per task in Spark through the `spark.task.cpus` setting. You probably want to set `spark.cores.max`, and not change the `spark.executor.cores` and `spark.driver.cores`. The Spark config page explains everything in more detail here: . https://spark.apache.org/docs/latest/configuration.html. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-416812290
https://github.com/google/deepvariant/issues/90#issuecomment-416812290:1087,Modifiability,config,config,1087,"@A-Tsai Of course you can especially with `pipe()`, which can take a script/command as input. At the least, you have the following two options to play with:. 1. If you use pipe to launch a script, then you can launch the application through `taskset` to limit the number of cores at launch-time. Here's the options for launching with an example: . _*Options to launch a program*_: `taskset [options] mask command [argument...]`. _*Example to use only 2 specific cores*_: `taskset -c 0,2 python ~/loop.py`. The `pipe(...)` command [as defined in the RDD base-class](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD) takes an external command, which in this case would be `taskset -c core-list your-program`:. ![rdd-pipe](https://user-images.githubusercontent.com/6555937/44763439-4c73f500-ab19-11e8-99d7-99adac28c913.png). 2. In Spark, you can also limit the number of cores per task in Spark through the `spark.task.cpus` setting. You probably want to set `spark.cores.max`, and not change the `spark.executor.cores` and `spark.driver.cores`. The Spark config page explains everything in more detail here: . https://spark.apache.org/docs/latest/configuration.html. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-416812290
https://github.com/google/deepvariant/issues/90#issuecomment-416812290:1179,Modifiability,config,configuration,1179,"@A-Tsai Of course you can especially with `pipe()`, which can take a script/command as input. At the least, you have the following two options to play with:. 1. If you use pipe to launch a script, then you can launch the application through `taskset` to limit the number of cores at launch-time. Here's the options for launching with an example: . _*Options to launch a program*_: `taskset [options] mask command [argument...]`. _*Example to use only 2 specific cores*_: `taskset -c 0,2 python ~/loop.py`. The `pipe(...)` command [as defined in the RDD base-class](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD) takes an external command, which in this case would be `taskset -c core-list your-program`:. ![rdd-pipe](https://user-images.githubusercontent.com/6555937/44763439-4c73f500-ab19-11e8-99d7-99adac28c913.png). 2. In Spark, you can also limit the number of cores per task in Spark through the `spark.task.cpus` setting. You probably want to set `spark.cores.max`, and not change the `spark.executor.cores` and `spark.driver.cores`. The Spark config page explains everything in more detail here: . https://spark.apache.org/docs/latest/configuration.html. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-416812290
https://github.com/google/deepvariant/issues/90#issuecomment-416891480:1025,Availability,avail,available,1025,"@pgrosu Thank you for the prompt response. Correct me if I'm wrong. ; taskset need to specify the specific cpu cores the process wants to occupy. Since my Spark cluster is multi-tenant, some processes may be running in the cluster and occupy some CPU cores. In addition, the dynamic resource allocation is enabled in my Spark cluster, so I can't assume all of my tasks can be equally assigned to each computing node. If my data are stored in 200 partitions, it mean that my program will launch 200 tasks by using pipe() to call taskset. I can't make sure which partition will be assigned to which computing node. Round-Robin assignment is a way, but it's violated the policy of the resource management (like Spark standalone or YARN). For example, I have 8 computing node with 4 cores per each. My Spark process might allocate 24 cores. It might be 8 computing nodes with 3 cores per each or 6 computing nodes with 4 cores per each. Furthermore, there is no information to let me know which task is done or which cpu core is available in my Spark program. the resource allocation might be unbalance and performance might be impacted severely, especially when several iterations of task assignment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-416891480
https://github.com/google/deepvariant/issues/90#issuecomment-416891480:818,Energy Efficiency,allocate,allocate,818,"@pgrosu Thank you for the prompt response. Correct me if I'm wrong. ; taskset need to specify the specific cpu cores the process wants to occupy. Since my Spark cluster is multi-tenant, some processes may be running in the cluster and occupy some CPU cores. In addition, the dynamic resource allocation is enabled in my Spark cluster, so I can't assume all of my tasks can be equally assigned to each computing node. If my data are stored in 200 partitions, it mean that my program will launch 200 tasks by using pipe() to call taskset. I can't make sure which partition will be assigned to which computing node. Round-Robin assignment is a way, but it's violated the policy of the resource management (like Spark standalone or YARN). For example, I have 8 computing node with 4 cores per each. My Spark process might allocate 24 cores. It might be 8 computing nodes with 3 cores per each or 6 computing nodes with 4 cores per each. Furthermore, there is no information to let me know which task is done or which cpu core is available in my Spark program. the resource allocation might be unbalance and performance might be impacted severely, especially when several iterations of task assignment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-416891480
https://github.com/google/deepvariant/issues/90#issuecomment-416891480:1103,Performance,perform,performance,1103,"@pgrosu Thank you for the prompt response. Correct me if I'm wrong. ; taskset need to specify the specific cpu cores the process wants to occupy. Since my Spark cluster is multi-tenant, some processes may be running in the cluster and occupy some CPU cores. In addition, the dynamic resource allocation is enabled in my Spark cluster, so I can't assume all of my tasks can be equally assigned to each computing node. If my data are stored in 200 partitions, it mean that my program will launch 200 tasks by using pipe() to call taskset. I can't make sure which partition will be assigned to which computing node. Round-Robin assignment is a way, but it's violated the policy of the resource management (like Spark standalone or YARN). For example, I have 8 computing node with 4 cores per each. My Spark process might allocate 24 cores. It might be 8 computing nodes with 3 cores per each or 6 computing nodes with 4 cores per each. Furthermore, there is no information to let me know which task is done or which cpu core is available in my Spark program. the resource allocation might be unbalance and performance might be impacted severely, especially when several iterations of task assignment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-416891480
https://github.com/google/deepvariant/issues/90#issuecomment-417185546:672,Availability,down,down,672,"@A-Tsai So now you're getting into determining your lineage graph of computations, which requires a lot of thought on how you set up your data and computational steps. Wide dependencies (multiple child partitions from the parent RDD) should be kept on the same node, while narrow dependencies (one child partition per parent RDD) can go over the network. Spark does hash partitioning to shuffle, and tries by the modulo of the number of partitions that it is somewhat uniformly distributed. Basically you should model your computation tree in order to best organize the data for good distribution and maximal local computation, otherwise you flood you network which slows down your completion-time. If you have 8 compute node with 4 cores each, then the node number will not change. The key to think about is the cores, but you have to determine to resource-allocate properly. The following can be helpful:. http://site.clairvoyantsoft.com/understanding-resource-allocation-configurations-spark-application/. Regarding task information, you get the duration here:. https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.TaskInfo@finishTime:Long. You can control the policy with the application as noted here:. https://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application. You can control the assignment by creating pools with a `minShare` of cpu cores guaranteed for that pool. If you have one pool, then that can control it all - especially in FIFO mode - but that would not be efficient use of a Spark cluster. Spark has many moving parts, including how you structure your data and how to chose to process it (batch vs. stream). To properly perform this you have to analyze carefully the design of your computational DAG, and how your work should be processed given the resources. To me 200 partitions does not quite feel right, but if you think that is optimal for your design that's fine. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-417185546
https://github.com/google/deepvariant/issues/90#issuecomment-417185546:974,Deployability,configurat,configurations-spark-application,974,"@A-Tsai So now you're getting into determining your lineage graph of computations, which requires a lot of thought on how you set up your data and computational steps. Wide dependencies (multiple child partitions from the parent RDD) should be kept on the same node, while narrow dependencies (one child partition per parent RDD) can go over the network. Spark does hash partitioning to shuffle, and tries by the modulo of the number of partitions that it is somewhat uniformly distributed. Basically you should model your computation tree in order to best organize the data for good distribution and maximal local computation, otherwise you flood you network which slows down your completion-time. If you have 8 compute node with 4 cores each, then the node number will not change. The key to think about is the cores, but you have to determine to resource-allocate properly. The following can be helpful:. http://site.clairvoyantsoft.com/understanding-resource-allocation-configurations-spark-application/. Regarding task information, you get the duration here:. https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.TaskInfo@finishTime:Long. You can control the policy with the application as noted here:. https://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application. You can control the assignment by creating pools with a `minShare` of cpu cores guaranteed for that pool. If you have one pool, then that can control it all - especially in FIFO mode - but that would not be efficient use of a Spark cluster. Spark has many moving parts, including how you structure your data and how to chose to process it (batch vs. stream). To properly perform this you have to analyze carefully the design of your computational DAG, and how your work should be processed given the resources. To me 200 partitions does not quite feel right, but if you think that is optimal for your design that's fine. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-417185546
https://github.com/google/deepvariant/issues/90#issuecomment-417185546:858,Energy Efficiency,allocate,allocate,858,"@A-Tsai So now you're getting into determining your lineage graph of computations, which requires a lot of thought on how you set up your data and computational steps. Wide dependencies (multiple child partitions from the parent RDD) should be kept on the same node, while narrow dependencies (one child partition per parent RDD) can go over the network. Spark does hash partitioning to shuffle, and tries by the modulo of the number of partitions that it is somewhat uniformly distributed. Basically you should model your computation tree in order to best organize the data for good distribution and maximal local computation, otherwise you flood you network which slows down your completion-time. If you have 8 compute node with 4 cores each, then the node number will not change. The key to think about is the cores, but you have to determine to resource-allocate properly. The following can be helpful:. http://site.clairvoyantsoft.com/understanding-resource-allocation-configurations-spark-application/. Regarding task information, you get the duration here:. https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.TaskInfo@finishTime:Long. You can control the policy with the application as noted here:. https://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application. You can control the assignment by creating pools with a `minShare` of cpu cores guaranteed for that pool. If you have one pool, then that can control it all - especially in FIFO mode - but that would not be efficient use of a Spark cluster. Spark has many moving parts, including how you structure your data and how to chose to process it (batch vs. stream). To properly perform this you have to analyze carefully the design of your computational DAG, and how your work should be processed given the resources. To me 200 partitions does not quite feel right, but if you think that is optimal for your design that's fine. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-417185546
https://github.com/google/deepvariant/issues/90#issuecomment-417185546:1140,Energy Efficiency,schedul,scheduler,1140,"@A-Tsai So now you're getting into determining your lineage graph of computations, which requires a lot of thought on how you set up your data and computational steps. Wide dependencies (multiple child partitions from the parent RDD) should be kept on the same node, while narrow dependencies (one child partition per parent RDD) can go over the network. Spark does hash partitioning to shuffle, and tries by the modulo of the number of partitions that it is somewhat uniformly distributed. Basically you should model your computation tree in order to best organize the data for good distribution and maximal local computation, otherwise you flood you network which slows down your completion-time. If you have 8 compute node with 4 cores each, then the node number will not change. The key to think about is the cores, but you have to determine to resource-allocate properly. The following can be helpful:. http://site.clairvoyantsoft.com/understanding-resource-allocation-configurations-spark-application/. Regarding task information, you get the duration here:. https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.TaskInfo@finishTime:Long. You can control the policy with the application as noted here:. https://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application. You can control the assignment by creating pools with a `minShare` of cpu cores guaranteed for that pool. If you have one pool, then that can control it all - especially in FIFO mode - but that would not be efficient use of a Spark cluster. Spark has many moving parts, including how you structure your data and how to chose to process it (batch vs. stream). To properly perform this you have to analyze carefully the design of your computational DAG, and how your work should be processed given the resources. To me 200 partitions does not quite feel right, but if you think that is optimal for your design that's fine. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-417185546
https://github.com/google/deepvariant/issues/90#issuecomment-417185546:1281,Energy Efficiency,schedul,scheduling,1281,"@A-Tsai So now you're getting into determining your lineage graph of computations, which requires a lot of thought on how you set up your data and computational steps. Wide dependencies (multiple child partitions from the parent RDD) should be kept on the same node, while narrow dependencies (one child partition per parent RDD) can go over the network. Spark does hash partitioning to shuffle, and tries by the modulo of the number of partitions that it is somewhat uniformly distributed. Basically you should model your computation tree in order to best organize the data for good distribution and maximal local computation, otherwise you flood you network which slows down your completion-time. If you have 8 compute node with 4 cores each, then the node number will not change. The key to think about is the cores, but you have to determine to resource-allocate properly. The following can be helpful:. http://site.clairvoyantsoft.com/understanding-resource-allocation-configurations-spark-application/. Regarding task information, you get the duration here:. https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.TaskInfo@finishTime:Long. You can control the policy with the application as noted here:. https://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application. You can control the assignment by creating pools with a `minShare` of cpu cores guaranteed for that pool. If you have one pool, then that can control it all - especially in FIFO mode - but that would not be efficient use of a Spark cluster. Spark has many moving parts, including how you structure your data and how to chose to process it (batch vs. stream). To properly perform this you have to analyze carefully the design of your computational DAG, and how your work should be processed given the resources. To me 200 partitions does not quite feel right, but if you think that is optimal for your design that's fine. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-417185546
https://github.com/google/deepvariant/issues/90#issuecomment-417185546:1297,Energy Efficiency,schedul,scheduling-within-an-application,1297,"@A-Tsai So now you're getting into determining your lineage graph of computations, which requires a lot of thought on how you set up your data and computational steps. Wide dependencies (multiple child partitions from the parent RDD) should be kept on the same node, while narrow dependencies (one child partition per parent RDD) can go over the network. Spark does hash partitioning to shuffle, and tries by the modulo of the number of partitions that it is somewhat uniformly distributed. Basically you should model your computation tree in order to best organize the data for good distribution and maximal local computation, otherwise you flood you network which slows down your completion-time. If you have 8 compute node with 4 cores each, then the node number will not change. The key to think about is the cores, but you have to determine to resource-allocate properly. The following can be helpful:. http://site.clairvoyantsoft.com/understanding-resource-allocation-configurations-spark-application/. Regarding task information, you get the duration here:. https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.TaskInfo@finishTime:Long. You can control the policy with the application as noted here:. https://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application. You can control the assignment by creating pools with a `minShare` of cpu cores guaranteed for that pool. If you have one pool, then that can control it all - especially in FIFO mode - but that would not be efficient use of a Spark cluster. Spark has many moving parts, including how you structure your data and how to chose to process it (batch vs. stream). To properly perform this you have to analyze carefully the design of your computational DAG, and how your work should be processed given the resources. To me 200 partitions does not quite feel right, but if you think that is optimal for your design that's fine. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-417185546
https://github.com/google/deepvariant/issues/90#issuecomment-417185546:1538,Energy Efficiency,efficient,efficient,1538,"@A-Tsai So now you're getting into determining your lineage graph of computations, which requires a lot of thought on how you set up your data and computational steps. Wide dependencies (multiple child partitions from the parent RDD) should be kept on the same node, while narrow dependencies (one child partition per parent RDD) can go over the network. Spark does hash partitioning to shuffle, and tries by the modulo of the number of partitions that it is somewhat uniformly distributed. Basically you should model your computation tree in order to best organize the data for good distribution and maximal local computation, otherwise you flood you network which slows down your completion-time. If you have 8 compute node with 4 cores each, then the node number will not change. The key to think about is the cores, but you have to determine to resource-allocate properly. The following can be helpful:. http://site.clairvoyantsoft.com/understanding-resource-allocation-configurations-spark-application/. Regarding task information, you get the duration here:. https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.TaskInfo@finishTime:Long. You can control the policy with the application as noted here:. https://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application. You can control the assignment by creating pools with a `minShare` of cpu cores guaranteed for that pool. If you have one pool, then that can control it all - especially in FIFO mode - but that would not be efficient use of a Spark cluster. Spark has many moving parts, including how you structure your data and how to chose to process it (batch vs. stream). To properly perform this you have to analyze carefully the design of your computational DAG, and how your work should be processed given the resources. To me 200 partitions does not quite feel right, but if you think that is optimal for your design that's fine. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-417185546
https://github.com/google/deepvariant/issues/90#issuecomment-417185546:173,Integrability,depend,dependencies,173,"@A-Tsai So now you're getting into determining your lineage graph of computations, which requires a lot of thought on how you set up your data and computational steps. Wide dependencies (multiple child partitions from the parent RDD) should be kept on the same node, while narrow dependencies (one child partition per parent RDD) can go over the network. Spark does hash partitioning to shuffle, and tries by the modulo of the number of partitions that it is somewhat uniformly distributed. Basically you should model your computation tree in order to best organize the data for good distribution and maximal local computation, otherwise you flood you network which slows down your completion-time. If you have 8 compute node with 4 cores each, then the node number will not change. The key to think about is the cores, but you have to determine to resource-allocate properly. The following can be helpful:. http://site.clairvoyantsoft.com/understanding-resource-allocation-configurations-spark-application/. Regarding task information, you get the duration here:. https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.TaskInfo@finishTime:Long. You can control the policy with the application as noted here:. https://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application. You can control the assignment by creating pools with a `minShare` of cpu cores guaranteed for that pool. If you have one pool, then that can control it all - especially in FIFO mode - but that would not be efficient use of a Spark cluster. Spark has many moving parts, including how you structure your data and how to chose to process it (batch vs. stream). To properly perform this you have to analyze carefully the design of your computational DAG, and how your work should be processed given the resources. To me 200 partitions does not quite feel right, but if you think that is optimal for your design that's fine. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-417185546
https://github.com/google/deepvariant/issues/90#issuecomment-417185546:280,Integrability,depend,dependencies,280,"@A-Tsai So now you're getting into determining your lineage graph of computations, which requires a lot of thought on how you set up your data and computational steps. Wide dependencies (multiple child partitions from the parent RDD) should be kept on the same node, while narrow dependencies (one child partition per parent RDD) can go over the network. Spark does hash partitioning to shuffle, and tries by the modulo of the number of partitions that it is somewhat uniformly distributed. Basically you should model your computation tree in order to best organize the data for good distribution and maximal local computation, otherwise you flood you network which slows down your completion-time. If you have 8 compute node with 4 cores each, then the node number will not change. The key to think about is the cores, but you have to determine to resource-allocate properly. The following can be helpful:. http://site.clairvoyantsoft.com/understanding-resource-allocation-configurations-spark-application/. Regarding task information, you get the duration here:. https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.TaskInfo@finishTime:Long. You can control the policy with the application as noted here:. https://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application. You can control the assignment by creating pools with a `minShare` of cpu cores guaranteed for that pool. If you have one pool, then that can control it all - especially in FIFO mode - but that would not be efficient use of a Spark cluster. Spark has many moving parts, including how you structure your data and how to chose to process it (batch vs. stream). To properly perform this you have to analyze carefully the design of your computational DAG, and how your work should be processed given the resources. To me 200 partitions does not quite feel right, but if you think that is optimal for your design that's fine. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-417185546
https://github.com/google/deepvariant/issues/90#issuecomment-417185546:974,Modifiability,config,configurations-spark-application,974,"@A-Tsai So now you're getting into determining your lineage graph of computations, which requires a lot of thought on how you set up your data and computational steps. Wide dependencies (multiple child partitions from the parent RDD) should be kept on the same node, while narrow dependencies (one child partition per parent RDD) can go over the network. Spark does hash partitioning to shuffle, and tries by the modulo of the number of partitions that it is somewhat uniformly distributed. Basically you should model your computation tree in order to best organize the data for good distribution and maximal local computation, otherwise you flood you network which slows down your completion-time. If you have 8 compute node with 4 cores each, then the node number will not change. The key to think about is the cores, but you have to determine to resource-allocate properly. The following can be helpful:. http://site.clairvoyantsoft.com/understanding-resource-allocation-configurations-spark-application/. Regarding task information, you get the duration here:. https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.TaskInfo@finishTime:Long. You can control the policy with the application as noted here:. https://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application. You can control the assignment by creating pools with a `minShare` of cpu cores guaranteed for that pool. If you have one pool, then that can control it all - especially in FIFO mode - but that would not be efficient use of a Spark cluster. Spark has many moving parts, including how you structure your data and how to chose to process it (batch vs. stream). To properly perform this you have to analyze carefully the design of your computational DAG, and how your work should be processed given the resources. To me 200 partitions does not quite feel right, but if you think that is optimal for your design that's fine. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-417185546
https://github.com/google/deepvariant/issues/90#issuecomment-417185546:1702,Performance,perform,perform,1702,"@A-Tsai So now you're getting into determining your lineage graph of computations, which requires a lot of thought on how you set up your data and computational steps. Wide dependencies (multiple child partitions from the parent RDD) should be kept on the same node, while narrow dependencies (one child partition per parent RDD) can go over the network. Spark does hash partitioning to shuffle, and tries by the modulo of the number of partitions that it is somewhat uniformly distributed. Basically you should model your computation tree in order to best organize the data for good distribution and maximal local computation, otherwise you flood you network which slows down your completion-time. If you have 8 compute node with 4 cores each, then the node number will not change. The key to think about is the cores, but you have to determine to resource-allocate properly. The following can be helpful:. http://site.clairvoyantsoft.com/understanding-resource-allocation-configurations-spark-application/. Regarding task information, you get the duration here:. https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.TaskInfo@finishTime:Long. You can control the policy with the application as noted here:. https://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application. You can control the assignment by creating pools with a `minShare` of cpu cores guaranteed for that pool. If you have one pool, then that can control it all - especially in FIFO mode - but that would not be efficient use of a Spark cluster. Spark has many moving parts, including how you structure your data and how to chose to process it (batch vs. stream). To properly perform this you have to analyze carefully the design of your computational DAG, and how your work should be processed given the resources. To me 200 partitions does not quite feel right, but if you think that is optimal for your design that's fine. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-417185546
https://github.com/google/deepvariant/issues/90#issuecomment-417185546:366,Security,hash,hash,366,"@A-Tsai So now you're getting into determining your lineage graph of computations, which requires a lot of thought on how you set up your data and computational steps. Wide dependencies (multiple child partitions from the parent RDD) should be kept on the same node, while narrow dependencies (one child partition per parent RDD) can go over the network. Spark does hash partitioning to shuffle, and tries by the modulo of the number of partitions that it is somewhat uniformly distributed. Basically you should model your computation tree in order to best organize the data for good distribution and maximal local computation, otherwise you flood you network which slows down your completion-time. If you have 8 compute node with 4 cores each, then the node number will not change. The key to think about is the cores, but you have to determine to resource-allocate properly. The following can be helpful:. http://site.clairvoyantsoft.com/understanding-resource-allocation-configurations-spark-application/. Regarding task information, you get the duration here:. https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.TaskInfo@finishTime:Long. You can control the policy with the application as noted here:. https://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application. You can control the assignment by creating pools with a `minShare` of cpu cores guaranteed for that pool. If you have one pool, then that can control it all - especially in FIFO mode - but that would not be efficient use of a Spark cluster. Spark has many moving parts, including how you structure your data and how to chose to process it (batch vs. stream). To properly perform this you have to analyze carefully the design of your computational DAG, and how your work should be processed given the resources. To me 200 partitions does not quite feel right, but if you think that is optimal for your design that's fine. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-417185546
https://github.com/google/deepvariant/issues/90#issuecomment-417995073:149,Availability,avail,available,149,"@pgrosu Thank you for the information. Your suggestion might be a way to port DeepVariant on Spark, but it's not fit on dynamic allocation since the available resource is dynamic changed. ; I did port several popular bioinformatics tools (e.g. BWA, GATK, DELLY2, Samtools, ...) on Spark and they run well. I think the fundamental problem is that the resource configuration (intra_op_parallelism_threads=1, inter_op_parallelism_threads=1) of TensorFlow doesn't work. If there is no way to limit CPU resource on DeepVariant, should I submit this issue to TensorFlow GitHub?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-417995073
https://github.com/google/deepvariant/issues/90#issuecomment-417995073:359,Deployability,configurat,configuration,359,"@pgrosu Thank you for the information. Your suggestion might be a way to port DeepVariant on Spark, but it's not fit on dynamic allocation since the available resource is dynamic changed. ; I did port several popular bioinformatics tools (e.g. BWA, GATK, DELLY2, Samtools, ...) on Spark and they run well. I think the fundamental problem is that the resource configuration (intra_op_parallelism_threads=1, inter_op_parallelism_threads=1) of TensorFlow doesn't work. If there is no way to limit CPU resource on DeepVariant, should I submit this issue to TensorFlow GitHub?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-417995073
https://github.com/google/deepvariant/issues/90#issuecomment-417995073:359,Modifiability,config,configuration,359,"@pgrosu Thank you for the information. Your suggestion might be a way to port DeepVariant on Spark, but it's not fit on dynamic allocation since the available resource is dynamic changed. ; I did port several popular bioinformatics tools (e.g. BWA, GATK, DELLY2, Samtools, ...) on Spark and they run well. I think the fundamental problem is that the resource configuration (intra_op_parallelism_threads=1, inter_op_parallelism_threads=1) of TensorFlow doesn't work. If there is no way to limit CPU resource on DeepVariant, should I submit this issue to TensorFlow GitHub?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-417995073
https://github.com/google/deepvariant/issues/90#issuecomment-418211657:42,Usability,feedback,feedback,42,@pichuan and @pgrosu : Thank you for your feedback. Will keep you posted on the issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/90#issuecomment-418211657
https://github.com/google/deepvariant/issues/91#issuecomment-418172888:727,Testability,log,log,727,"Hi,; As you can see from our training case study, for larger datasets, we shuffled it on a distributed runner (dataflow). I think Beam also supports other things like Spark, but I have not tried it myself.; This shuffling code achieves two things: 1. Shuffle the examples, 2. Count the number of examples. Shuffling is important for training a model. Internally our tensorflow code does some extra shuffling in each batch, but that might not be enough on a global scale. I think a global shuffling is a better practice from a machine learning perspective. But empirically it might also be ok to not shuffle globally.; The other thing that you'll need is the total number of training examples, which I think you can find in the log when you made the examples (but you'll need to sum them up across the shards). The number of examples is used in the calculation of learning rate. You can certainly try without shuffling, but it's not the best practice I'd recommend.; Let me know if it works for you. If you find a good way to shuffle on Spark or other distributed system that you'd like to share, that will be great! Please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/91#issuecomment-418172888
https://github.com/google/deepvariant/issues/91#issuecomment-418172888:534,Usability,learn,learning,534,"Hi,; As you can see from our training case study, for larger datasets, we shuffled it on a distributed runner (dataflow). I think Beam also supports other things like Spark, but I have not tried it myself.; This shuffling code achieves two things: 1. Shuffle the examples, 2. Count the number of examples. Shuffling is important for training a model. Internally our tensorflow code does some extra shuffling in each batch, but that might not be enough on a global scale. I think a global shuffling is a better practice from a machine learning perspective. But empirically it might also be ok to not shuffle globally.; The other thing that you'll need is the total number of training examples, which I think you can find in the log when you made the examples (but you'll need to sum them up across the shards). The number of examples is used in the calculation of learning rate. You can certainly try without shuffling, but it's not the best practice I'd recommend.; Let me know if it works for you. If you find a good way to shuffle on Spark or other distributed system that you'd like to share, that will be great! Please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/91#issuecomment-418172888
https://github.com/google/deepvariant/issues/91#issuecomment-418172888:863,Usability,learn,learning,863,"Hi,; As you can see from our training case study, for larger datasets, we shuffled it on a distributed runner (dataflow). I think Beam also supports other things like Spark, but I have not tried it myself.; This shuffling code achieves two things: 1. Shuffle the examples, 2. Count the number of examples. Shuffling is important for training a model. Internally our tensorflow code does some extra shuffling in each batch, but that might not be enough on a global scale. I think a global shuffling is a better practice from a machine learning perspective. But empirically it might also be ok to not shuffle globally.; The other thing that you'll need is the total number of training examples, which I think you can find in the log when you made the examples (but you'll need to sum them up across the shards). The number of examples is used in the calculation of learning rate. You can certainly try without shuffling, but it's not the best practice I'd recommend.; Let me know if it works for you. If you find a good way to shuffle on Spark or other distributed system that you'd like to share, that will be great! Please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/91#issuecomment-418172888
https://github.com/google/deepvariant/issues/91#issuecomment-423019414:402,Performance,perform,performance,402,"Hi, pichuan.; Thanks for your help. I just change the name of tfrecord file to shuffle the training dataset. Is it OK to use this way to shuffle the training dataset? This way is introduced from ""Improve DeepVariant for ; BGISEQ germline variant calling"" file. The accuracy of trained model have raised compared with trained model without shuffling. But I'm not sure whether it is the highest accuracy performance. I don't know how to shuffle on Spark. Maybe I would try to shuffle on Spark in future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/91#issuecomment-423019414
https://github.com/google/deepvariant/issues/91#issuecomment-423040145:131,Usability,guid,guide,131,@ssm0808 FYI on how to shuffle on Spark: http://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/rdd-programming-guide.html#shuffle-operations,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/91#issuecomment-423040145
https://github.com/google/deepvariant/issues/91#issuecomment-1019990521:263,Availability,Down,Downside,263,"Just for the record, I wrote a script which shuffles the records locally using as little memory as possible: [TFrecordShuffler](https://github.com/GuillaumeHolley/TFrecordShuffler). It uses about as much RAM as the total size of the input (record) files on disk. Downside is obviously the time it takes which is much longer than with a distributed google cloud or spark system I imagine. As an example, shuffling ~30 million records totaling 125 GB of files took 46h (wall-clock and CPU) and 150 GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/91#issuecomment-1019990521
https://github.com/google/deepvariant/issues/92#issuecomment-418171330:444,Availability,robust,robust,444,"(1) Have you considered using docker instead of the prebuilt binaries? In this version, we updated the quick start and case studies to use docker. I personally find that much more convenient. ; (2) For bazel, can you try the official instructions: https://docs.bazel.build/versions/master/install-ubuntu.html; Last time I tried our script on a Ubuntu 16 machine on GCP, it worked. But maybe I should try one not on GCP.; If you find out a more robust installation for bazel, please share and I can update our instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/92#issuecomment-418171330
https://github.com/google/deepvariant/issues/92#issuecomment-418171330:91,Deployability,update,updated,91,"(1) Have you considered using docker instead of the prebuilt binaries? In this version, we updated the quick start and case studies to use docker. I personally find that much more convenient. ; (2) For bazel, can you try the official instructions: https://docs.bazel.build/versions/master/install-ubuntu.html; Last time I tried our script on a Ubuntu 16 machine on GCP, it worked. But maybe I should try one not on GCP.; If you find out a more robust installation for bazel, please share and I can update our instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/92#issuecomment-418171330
https://github.com/google/deepvariant/issues/92#issuecomment-418171330:289,Deployability,install,install-ubuntu,289,"(1) Have you considered using docker instead of the prebuilt binaries? In this version, we updated the quick start and case studies to use docker. I personally find that much more convenient. ; (2) For bazel, can you try the official instructions: https://docs.bazel.build/versions/master/install-ubuntu.html; Last time I tried our script on a Ubuntu 16 machine on GCP, it worked. But maybe I should try one not on GCP.; If you find out a more robust installation for bazel, please share and I can update our instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/92#issuecomment-418171330
https://github.com/google/deepvariant/issues/92#issuecomment-418171330:451,Deployability,install,installation,451,"(1) Have you considered using docker instead of the prebuilt binaries? In this version, we updated the quick start and case studies to use docker. I personally find that much more convenient. ; (2) For bazel, can you try the official instructions: https://docs.bazel.build/versions/master/install-ubuntu.html; Last time I tried our script on a Ubuntu 16 machine on GCP, it worked. But maybe I should try one not on GCP.; If you find out a more robust installation for bazel, please share and I can update our instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/92#issuecomment-418171330
https://github.com/google/deepvariant/issues/92#issuecomment-418171330:498,Deployability,update,update,498,"(1) Have you considered using docker instead of the prebuilt binaries? In this version, we updated the quick start and case studies to use docker. I personally find that much more convenient. ; (2) For bazel, can you try the official instructions: https://docs.bazel.build/versions/master/install-ubuntu.html; Last time I tried our script on a Ubuntu 16 machine on GCP, it worked. But maybe I should try one not on GCP.; If you find out a more robust installation for bazel, please share and I can update our instructions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/92#issuecomment-418171330
https://github.com/google/deepvariant/issues/93#issuecomment-421252090:139,Deployability,update,update,139,"Hi @ramcn,. Just go into the WORKSPACE file at the following location:. https://github.com/google/deepvariant/blob/r0.7/WORKSPACE#L87. And update your path to the full path of `opt` - (below is an example):. ```; new_local_repository(; name = ""clif"",; build_file = ""third_party/clif.BUILD"",; path = ""/home/ramcn/opt"",; ); ```. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/93#issuecomment-421252090
https://github.com/google/deepvariant/issues/94#issuecomment-422241830:19,Deployability,update,update,19,"Hi Ram,. Could you update the script on the bazel lines with `--verbose_failures`, rerun and show us the log. I'm assuming you have plenty of memory and CPU resources, which means that either you are not on Ubuntu or have an older GCC. What's your Linux distribution and GCC version?. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-422241830
https://github.com/google/deepvariant/issues/94#issuecomment-422241830:105,Testability,log,log,105,"Hi Ram,. Could you update the script on the bazel lines with `--verbose_failures`, rerun and show us the log. I'm assuming you have plenty of memory and CPU resources, which means that either you are not on Ubuntu or have an older GCC. What's your Linux distribution and GCC version?. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-422241830
https://github.com/google/deepvariant/issues/94#issuecomment-422487671:273,Deployability,install,installed,273,"[log1.txt](https://github.com/google/deepvariant/files/2393958/log1.txt); Hi Paul,. Attached is the log with --verbose_failures. Yes I am using a node on our HPC cluster. Some more details about my environment:; 1. gcc version 4.8.5; 2. centos 7; 3. protobuf version 3.5.1 installed as module with PATH and LD_LIB_PATH set accordingly.; 4. clif installed under HOME directory and WORKSPACE updated accordingly. Thank you; Ram",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-422487671
https://github.com/google/deepvariant/issues/94#issuecomment-422487671:345,Deployability,install,installed,345,"[log1.txt](https://github.com/google/deepvariant/files/2393958/log1.txt); Hi Paul,. Attached is the log with --verbose_failures. Yes I am using a node on our HPC cluster. Some more details about my environment:; 1. gcc version 4.8.5; 2. centos 7; 3. protobuf version 3.5.1 installed as module with PATH and LD_LIB_PATH set accordingly.; 4. clif installed under HOME directory and WORKSPACE updated accordingly. Thank you; Ram",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-422487671
https://github.com/google/deepvariant/issues/94#issuecomment-422487671:390,Deployability,update,updated,390,"[log1.txt](https://github.com/google/deepvariant/files/2393958/log1.txt); Hi Paul,. Attached is the log with --verbose_failures. Yes I am using a node on our HPC cluster. Some more details about my environment:; 1. gcc version 4.8.5; 2. centos 7; 3. protobuf version 3.5.1 installed as module with PATH and LD_LIB_PATH set accordingly.; 4. clif installed under HOME directory and WORKSPACE updated accordingly. Thank you; Ram",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-422487671
https://github.com/google/deepvariant/issues/94#issuecomment-422487671:100,Testability,log,log,100,"[log1.txt](https://github.com/google/deepvariant/files/2393958/log1.txt); Hi Paul,. Attached is the log with --verbose_failures. Yes I am using a node on our HPC cluster. Some more details about my environment:; 1. gcc version 4.8.5; 2. centos 7; 3. protobuf version 3.5.1 installed as module with PATH and LD_LIB_PATH set accordingly.; 4. clif installed under HOME directory and WORKSPACE updated accordingly. Thank you; Ram",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-422487671
https://github.com/google/deepvariant/issues/94#issuecomment-422619015:292,Integrability,protocol,protocolbuffers,292,"Hi Ram,. I see what's happening. You have protobuf 3.5.1 in your include paths, but this is trying to compile the protobuf 3.6 version. You will notice in this line:. ```; new (initial_block_) Block(options_.initial_block_size, NULL);; ```. Which is only part of `3.6.x`:. https://github.com/protocolbuffers/protobuf/blob/3.6.x/src/google/protobuf/arena.cc#L77. Basically a bunch of conflicts between declarations and use. So to simplify things, could you remove your protobuf 3.5.1 from your paths. If you are on a university cluster, it's usually something like `module unload MODULE_NAME`. After that try rerunning it again. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-422619015
https://github.com/google/deepvariant/issues/94#issuecomment-422619015:429,Usability,simpl,simplify,429,"Hi Ram,. I see what's happening. You have protobuf 3.5.1 in your include paths, but this is trying to compile the protobuf 3.6 version. You will notice in this line:. ```; new (initial_block_) Block(options_.initial_block_size, NULL);; ```. Which is only part of `3.6.x`:. https://github.com/protocolbuffers/protobuf/blob/3.6.x/src/google/protobuf/arena.cc#L77. Basically a bunch of conflicts between declarations and use. So to simplify things, could you remove your protobuf 3.5.1 from your paths. If you are on a university cluster, it's usually something like `module unload MODULE_NAME`. After that try rerunning it again. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-422619015
https://github.com/google/deepvariant/issues/94#issuecomment-422863842:167,Availability,error,error,167,"Hi Paul,. I simplified my setup a bit and have removed protobuf in my PATHs. I was able to complete the build, but most of the tests seem to be failing with the below error. TypeError: __new__() got an unexpected keyword argument 'serialized_options'. Attached is the log with verbose failures. Please let me know if you have any clue.; [log2.txt](https://github.com/google/deepvariant/files/2397845/log2.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-422863842
https://github.com/google/deepvariant/issues/94#issuecomment-422863842:285,Availability,failure,failures,285,"Hi Paul,. I simplified my setup a bit and have removed protobuf in my PATHs. I was able to complete the build, but most of the tests seem to be failing with the below error. TypeError: __new__() got an unexpected keyword argument 'serialized_options'. Attached is the log with verbose failures. Please let me know if you have any clue.; [log2.txt](https://github.com/google/deepvariant/files/2397845/log2.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-422863842
https://github.com/google/deepvariant/issues/94#issuecomment-422863842:127,Testability,test,tests,127,"Hi Paul,. I simplified my setup a bit and have removed protobuf in my PATHs. I was able to complete the build, but most of the tests seem to be failing with the below error. TypeError: __new__() got an unexpected keyword argument 'serialized_options'. Attached is the log with verbose failures. Please let me know if you have any clue.; [log2.txt](https://github.com/google/deepvariant/files/2397845/log2.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-422863842
https://github.com/google/deepvariant/issues/94#issuecomment-422863842:268,Testability,log,log,268,"Hi Paul,. I simplified my setup a bit and have removed protobuf in my PATHs. I was able to complete the build, but most of the tests seem to be failing with the below error. TypeError: __new__() got an unexpected keyword argument 'serialized_options'. Attached is the log with verbose failures. Please let me know if you have any clue.; [log2.txt](https://github.com/google/deepvariant/files/2397845/log2.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-422863842
https://github.com/google/deepvariant/issues/94#issuecomment-422863842:12,Usability,simpl,simplified,12,"Hi Paul,. I simplified my setup a bit and have removed protobuf in my PATHs. I was able to complete the build, but most of the tests seem to be failing with the below error. TypeError: __new__() got an unexpected keyword argument 'serialized_options'. Attached is the log with verbose failures. Please let me know if you have any clue.; [log2.txt](https://github.com/google/deepvariant/files/2397845/log2.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-422863842
https://github.com/google/deepvariant/issues/94#issuecomment-423037408:44,Availability,error,errors,44,"Hi Ram,. You really do not want to have any errors show up in any of your tests, especially when performing variant calling. It's an easily fixable issue where the Google folks would just need to update their TensorFlow Python package distributed with DeepVariant, which was probably compiled with an earlier version of protobuf, while currently the Tensorflow requires `3.6` of protobuf and the Protobuf version being compiled with DeepVariant is `3.6`. Looking at your log file, you are using the CPU version. The current version on PyPI of TensorFlow is 1.10.1, and DeepVariant uses version 1.9. Here's the process by which I went about to determine what is happening:. 1. If you download both `whl` files of Tensorflow like this:. ```; wget https://storage.googleapis.com/deepvariant/packages/tensorflow/tensorflow-1.9.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. wget https://files.pythonhosted.org/packages/1a/c4/8cb95df0bf06089014259b25997c3921a87aa08e2cd981417d91ca92f7e9/tensorflow-1.10.1-cp27-cp27mu-manylinux1_x86_64.whl; ```. 2. Next rename each of the `whl` files to `zip`, and then uncompress them in separate directories. If you then look at one of the version `1.10.1` files that has the `serialized_options` such as `purelib/tensorflow/core/framework/resource_handle_pb2.py`, you will see the following difference between the versions, where the `serialized_options` keyword is present in the `1.10.1` version, but not in the `1.9.0` version:. #### _*For version 1.9*_. ```Python; DESCRIPTOR = _descriptor.FileDescriptor(; name='tensorflow/core/framework/resource_handle.proto',; package='tensorflow',; syntax='proto3',; serialized_pb=_b('\n/tensorflow/core/framework/resource_handle.proto\x12\ntensorflow\""r\n\x13ResourceHandleProto\x12\x0e\n\x06\x64\x65vice\x18\x01 \x01(\t\x12\x11\n\tcontainer\x18\x02 \x01(\t\x12\x0c\n\x04name\x18\x03 \x01(\t\x12\x11\n\thash_code\x18\x04 \x01(\x04\x12\x17\n\x0fmaybe_type_name\x18\x05 \x01(\tBn\n\x18org.tensorflow.frameworkB\x0eResourceHandleP\",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-423037408
https://github.com/google/deepvariant/issues/94#issuecomment-423037408:683,Availability,down,download,683,"Hi Ram,. You really do not want to have any errors show up in any of your tests, especially when performing variant calling. It's an easily fixable issue where the Google folks would just need to update their TensorFlow Python package distributed with DeepVariant, which was probably compiled with an earlier version of protobuf, while currently the Tensorflow requires `3.6` of protobuf and the Protobuf version being compiled with DeepVariant is `3.6`. Looking at your log file, you are using the CPU version. The current version on PyPI of TensorFlow is 1.10.1, and DeepVariant uses version 1.9. Here's the process by which I went about to determine what is happening:. 1. If you download both `whl` files of Tensorflow like this:. ```; wget https://storage.googleapis.com/deepvariant/packages/tensorflow/tensorflow-1.9.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. wget https://files.pythonhosted.org/packages/1a/c4/8cb95df0bf06089014259b25997c3921a87aa08e2cd981417d91ca92f7e9/tensorflow-1.10.1-cp27-cp27mu-manylinux1_x86_64.whl; ```. 2. Next rename each of the `whl` files to `zip`, and then uncompress them in separate directories. If you then look at one of the version `1.10.1` files that has the `serialized_options` such as `purelib/tensorflow/core/framework/resource_handle_pb2.py`, you will see the following difference between the versions, where the `serialized_options` keyword is present in the `1.10.1` version, but not in the `1.9.0` version:. #### _*For version 1.9*_. ```Python; DESCRIPTOR = _descriptor.FileDescriptor(; name='tensorflow/core/framework/resource_handle.proto',; package='tensorflow',; syntax='proto3',; serialized_pb=_b('\n/tensorflow/core/framework/resource_handle.proto\x12\ntensorflow\""r\n\x13ResourceHandleProto\x12\x0e\n\x06\x64\x65vice\x18\x01 \x01(\t\x12\x11\n\tcontainer\x18\x02 \x01(\t\x12\x0c\n\x04name\x18\x03 \x01(\t\x12\x11\n\thash_code\x18\x04 \x01(\x04\x12\x17\n\x0fmaybe_type_name\x18\x05 \x01(\tBn\n\x18org.tensorflow.frameworkB\x0eResourceHandleP\",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-423037408
https://github.com/google/deepvariant/issues/94#issuecomment-423037408:196,Deployability,update,update,196,"Hi Ram,. You really do not want to have any errors show up in any of your tests, especially when performing variant calling. It's an easily fixable issue where the Google folks would just need to update their TensorFlow Python package distributed with DeepVariant, which was probably compiled with an earlier version of protobuf, while currently the Tensorflow requires `3.6` of protobuf and the Protobuf version being compiled with DeepVariant is `3.6`. Looking at your log file, you are using the CPU version. The current version on PyPI of TensorFlow is 1.10.1, and DeepVariant uses version 1.9. Here's the process by which I went about to determine what is happening:. 1. If you download both `whl` files of Tensorflow like this:. ```; wget https://storage.googleapis.com/deepvariant/packages/tensorflow/tensorflow-1.9.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. wget https://files.pythonhosted.org/packages/1a/c4/8cb95df0bf06089014259b25997c3921a87aa08e2cd981417d91ca92f7e9/tensorflow-1.10.1-cp27-cp27mu-manylinux1_x86_64.whl; ```. 2. Next rename each of the `whl` files to `zip`, and then uncompress them in separate directories. If you then look at one of the version `1.10.1` files that has the `serialized_options` such as `purelib/tensorflow/core/framework/resource_handle_pb2.py`, you will see the following difference between the versions, where the `serialized_options` keyword is present in the `1.10.1` version, but not in the `1.9.0` version:. #### _*For version 1.9*_. ```Python; DESCRIPTOR = _descriptor.FileDescriptor(; name='tensorflow/core/framework/resource_handle.proto',; package='tensorflow',; syntax='proto3',; serialized_pb=_b('\n/tensorflow/core/framework/resource_handle.proto\x12\ntensorflow\""r\n\x13ResourceHandleProto\x12\x0e\n\x06\x64\x65vice\x18\x01 \x01(\t\x12\x11\n\tcontainer\x18\x02 \x01(\t\x12\x0c\n\x04name\x18\x03 \x01(\t\x12\x11\n\thash_code\x18\x04 \x01(\x04\x12\x17\n\x0fmaybe_type_name\x18\x05 \x01(\tBn\n\x18org.tensorflow.frameworkB\x0eResourceHandleP\",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-423037408
https://github.com/google/deepvariant/issues/94#issuecomment-423037408:3022,Integrability,protocol,protocolbuffers,3022,"\x18\x05 \x01(\tBn\n\x18org.tensorflow.frameworkB\x0eResourceHandleP\x01Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\xf8\x01\x01\x62\x06proto3'); ). ```. #### _*For version 1.10.1*_. ```Python; DESCRIPTOR = _descriptor.FileDescriptor(; name='tensorflow/core/framework/resource_handle.proto',; package='tensorflow',; syntax='proto3',; serialized_options=_b('\n\030org.tensorflow.frameworkB\016ResourceHandleP\001Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\370\001\001'),; serialized_pb=_b('\n/tensorflow/core/framework/resource_handle.proto\x12\ntensorflow\""r\n\x13ResourceHandleProto\x12\x0e\n\x06\x64\x65vice\x18\x01 \x01(\t\x12\x11\n\tcontainer\x18\x02 \x01(\t\x12\x0c\n\x04name\x18\x03 \x01(\t\x12\x11\n\thash_code\x18\x04 \x01(\x04\x12\x17\n\x0fmaybe_type_name\x18\x05 \x01(\tBn\n\x18org.tensorflow.frameworkB\x0eResourceHandleP\x01Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\xf8\x01\x01\x62\x06proto3'); ); ```. ProtoBuf 3.6 passes the `serialized_options` argument in the call to `__new__` as shown here:. https://github.com/protocolbuffers/protobuf/blob/3.6.x/python/google/protobuf/descriptor.py#L283. ```Python; def __new__(cls, name, full_name, filename, containing_type, fields,; nested_types, enum_types, extensions, options=None,; serialized_options=None,; is_extendable=True, extension_ranges=None, oneofs=None,; file=None, serialized_start=None, serialized_end=None, # pylint: disable=redefined-builtin; syntax=None):; _message.Message._CheckCalledFromGeneratedFile(); return _message.default_pool.FindMessageTypeByName(full_name); ```. 3. If you look at the `METADATA` file of each TensorFlow package to see the protobuf version requirement, it will look like this:. ```; paul:~/tensorflow$ cat unzip-1.9/tensorflow-1.9.0.dist-info/METADATA | grep protobuf; Requires-Dist: protobuf (>=3.4.0). paul:~/tensorflow$ cat unzip-1.10/tensorflow-1.10.1.dist-info/METADATA | grep protobuf; Requires-Dist: protobuf (>=3.6.0); ```. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-423037408
https://github.com/google/deepvariant/issues/94#issuecomment-423037408:3434,Integrability,Message,Message,3434,"\x18\x05 \x01(\tBn\n\x18org.tensorflow.frameworkB\x0eResourceHandleP\x01Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\xf8\x01\x01\x62\x06proto3'); ). ```. #### _*For version 1.10.1*_. ```Python; DESCRIPTOR = _descriptor.FileDescriptor(; name='tensorflow/core/framework/resource_handle.proto',; package='tensorflow',; syntax='proto3',; serialized_options=_b('\n\030org.tensorflow.frameworkB\016ResourceHandleP\001Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\370\001\001'),; serialized_pb=_b('\n/tensorflow/core/framework/resource_handle.proto\x12\ntensorflow\""r\n\x13ResourceHandleProto\x12\x0e\n\x06\x64\x65vice\x18\x01 \x01(\t\x12\x11\n\tcontainer\x18\x02 \x01(\t\x12\x0c\n\x04name\x18\x03 \x01(\t\x12\x11\n\thash_code\x18\x04 \x01(\x04\x12\x17\n\x0fmaybe_type_name\x18\x05 \x01(\tBn\n\x18org.tensorflow.frameworkB\x0eResourceHandleP\x01Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\xf8\x01\x01\x62\x06proto3'); ); ```. ProtoBuf 3.6 passes the `serialized_options` argument in the call to `__new__` as shown here:. https://github.com/protocolbuffers/protobuf/blob/3.6.x/python/google/protobuf/descriptor.py#L283. ```Python; def __new__(cls, name, full_name, filename, containing_type, fields,; nested_types, enum_types, extensions, options=None,; serialized_options=None,; is_extendable=True, extension_ranges=None, oneofs=None,; file=None, serialized_start=None, serialized_end=None, # pylint: disable=redefined-builtin; syntax=None):; _message.Message._CheckCalledFromGeneratedFile(); return _message.default_pool.FindMessageTypeByName(full_name); ```. 3. If you look at the `METADATA` file of each TensorFlow package to see the protobuf version requirement, it will look like this:. ```; paul:~/tensorflow$ cat unzip-1.9/tensorflow-1.9.0.dist-info/METADATA | grep protobuf; Requires-Dist: protobuf (>=3.4.0). paul:~/tensorflow$ cat unzip-1.10/tensorflow-1.10.1.dist-info/METADATA | grep protobuf; Requires-Dist: protobuf (>=3.6.0); ```. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-423037408
https://github.com/google/deepvariant/issues/94#issuecomment-423037408:97,Performance,perform,performing,97,"Hi Ram,. You really do not want to have any errors show up in any of your tests, especially when performing variant calling. It's an easily fixable issue where the Google folks would just need to update their TensorFlow Python package distributed with DeepVariant, which was probably compiled with an earlier version of protobuf, while currently the Tensorflow requires `3.6` of protobuf and the Protobuf version being compiled with DeepVariant is `3.6`. Looking at your log file, you are using the CPU version. The current version on PyPI of TensorFlow is 1.10.1, and DeepVariant uses version 1.9. Here's the process by which I went about to determine what is happening:. 1. If you download both `whl` files of Tensorflow like this:. ```; wget https://storage.googleapis.com/deepvariant/packages/tensorflow/tensorflow-1.9.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. wget https://files.pythonhosted.org/packages/1a/c4/8cb95df0bf06089014259b25997c3921a87aa08e2cd981417d91ca92f7e9/tensorflow-1.10.1-cp27-cp27mu-manylinux1_x86_64.whl; ```. 2. Next rename each of the `whl` files to `zip`, and then uncompress them in separate directories. If you then look at one of the version `1.10.1` files that has the `serialized_options` such as `purelib/tensorflow/core/framework/resource_handle_pb2.py`, you will see the following difference between the versions, where the `serialized_options` keyword is present in the `1.10.1` version, but not in the `1.9.0` version:. #### _*For version 1.9*_. ```Python; DESCRIPTOR = _descriptor.FileDescriptor(; name='tensorflow/core/framework/resource_handle.proto',; package='tensorflow',; syntax='proto3',; serialized_pb=_b('\n/tensorflow/core/framework/resource_handle.proto\x12\ntensorflow\""r\n\x13ResourceHandleProto\x12\x0e\n\x06\x64\x65vice\x18\x01 \x01(\t\x12\x11\n\tcontainer\x18\x02 \x01(\t\x12\x0c\n\x04name\x18\x03 \x01(\t\x12\x11\n\thash_code\x18\x04 \x01(\x04\x12\x17\n\x0fmaybe_type_name\x18\x05 \x01(\tBn\n\x18org.tensorflow.frameworkB\x0eResourceHandleP\",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-423037408
https://github.com/google/deepvariant/issues/94#issuecomment-423037408:74,Testability,test,tests,74,"Hi Ram,. You really do not want to have any errors show up in any of your tests, especially when performing variant calling. It's an easily fixable issue where the Google folks would just need to update their TensorFlow Python package distributed with DeepVariant, which was probably compiled with an earlier version of protobuf, while currently the Tensorflow requires `3.6` of protobuf and the Protobuf version being compiled with DeepVariant is `3.6`. Looking at your log file, you are using the CPU version. The current version on PyPI of TensorFlow is 1.10.1, and DeepVariant uses version 1.9. Here's the process by which I went about to determine what is happening:. 1. If you download both `whl` files of Tensorflow like this:. ```; wget https://storage.googleapis.com/deepvariant/packages/tensorflow/tensorflow-1.9.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. wget https://files.pythonhosted.org/packages/1a/c4/8cb95df0bf06089014259b25997c3921a87aa08e2cd981417d91ca92f7e9/tensorflow-1.10.1-cp27-cp27mu-manylinux1_x86_64.whl; ```. 2. Next rename each of the `whl` files to `zip`, and then uncompress them in separate directories. If you then look at one of the version `1.10.1` files that has the `serialized_options` such as `purelib/tensorflow/core/framework/resource_handle_pb2.py`, you will see the following difference between the versions, where the `serialized_options` keyword is present in the `1.10.1` version, but not in the `1.9.0` version:. #### _*For version 1.9*_. ```Python; DESCRIPTOR = _descriptor.FileDescriptor(; name='tensorflow/core/framework/resource_handle.proto',; package='tensorflow',; syntax='proto3',; serialized_pb=_b('\n/tensorflow/core/framework/resource_handle.proto\x12\ntensorflow\""r\n\x13ResourceHandleProto\x12\x0e\n\x06\x64\x65vice\x18\x01 \x01(\t\x12\x11\n\tcontainer\x18\x02 \x01(\t\x12\x0c\n\x04name\x18\x03 \x01(\t\x12\x11\n\thash_code\x18\x04 \x01(\x04\x12\x17\n\x0fmaybe_type_name\x18\x05 \x01(\tBn\n\x18org.tensorflow.frameworkB\x0eResourceHandleP\",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-423037408
https://github.com/google/deepvariant/issues/94#issuecomment-423037408:471,Testability,log,log,471,"Hi Ram,. You really do not want to have any errors show up in any of your tests, especially when performing variant calling. It's an easily fixable issue where the Google folks would just need to update their TensorFlow Python package distributed with DeepVariant, which was probably compiled with an earlier version of protobuf, while currently the Tensorflow requires `3.6` of protobuf and the Protobuf version being compiled with DeepVariant is `3.6`. Looking at your log file, you are using the CPU version. The current version on PyPI of TensorFlow is 1.10.1, and DeepVariant uses version 1.9. Here's the process by which I went about to determine what is happening:. 1. If you download both `whl` files of Tensorflow like this:. ```; wget https://storage.googleapis.com/deepvariant/packages/tensorflow/tensorflow-1.9.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. wget https://files.pythonhosted.org/packages/1a/c4/8cb95df0bf06089014259b25997c3921a87aa08e2cd981417d91ca92f7e9/tensorflow-1.10.1-cp27-cp27mu-manylinux1_x86_64.whl; ```. 2. Next rename each of the `whl` files to `zip`, and then uncompress them in separate directories. If you then look at one of the version `1.10.1` files that has the `serialized_options` such as `purelib/tensorflow/core/framework/resource_handle_pb2.py`, you will see the following difference between the versions, where the `serialized_options` keyword is present in the `1.10.1` version, but not in the `1.9.0` version:. #### _*For version 1.9*_. ```Python; DESCRIPTOR = _descriptor.FileDescriptor(; name='tensorflow/core/framework/resource_handle.proto',; package='tensorflow',; syntax='proto3',; serialized_pb=_b('\n/tensorflow/core/framework/resource_handle.proto\x12\ntensorflow\""r\n\x13ResourceHandleProto\x12\x0e\n\x06\x64\x65vice\x18\x01 \x01(\t\x12\x11\n\tcontainer\x18\x02 \x01(\t\x12\x0c\n\x04name\x18\x03 \x01(\t\x12\x11\n\thash_code\x18\x04 \x01(\x04\x12\x17\n\x0fmaybe_type_name\x18\x05 \x01(\tBn\n\x18org.tensorflow.frameworkB\x0eResourceHandleP\",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-423037408
https://github.com/google/deepvariant/issues/94#issuecomment-423268455:219,Deployability,install,installs,219,"Thanks paul for the detailed analysis and explanation of how you went about it. To confirm this analysis I tried out by changing TF_WHL_VERSION and other related symbols to 1.10.1 in settings.sh so that build-prereq.sh installs the matching tensorflow. Now, the build and test passes and I am able to run variant calling. I presume this is a good workaround to address this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-423268455
https://github.com/google/deepvariant/issues/94#issuecomment-423268455:272,Testability,test,test,272,"Thanks paul for the detailed analysis and explanation of how you went about it. To confirm this analysis I tried out by changing TF_WHL_VERSION and other related symbols to 1.10.1 in settings.sh so that build-prereq.sh installs the matching tensorflow. Now, the build and test passes and I am able to run variant calling. I presume this is a good workaround to address this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-423268455
https://github.com/google/deepvariant/issues/94#issuecomment-423269790:400,Performance,optimiz,optimized,400,"@ramcn I'm delighted you've gotten your build sorted out.; @ramcn If you are building DeepVariant from scratch, and in particular TF's wheels directly, I'd recommend looking into the exact version of TF you want to use with DeepVariant. In particular, if you are intending to run on CPUs, we've found that the MKL extensions to TF make call_variants 3-4x faster. It may be worth building yourself an optimized TF wheel for DeepVariant to maximize performance. @pgrosu Thank you for all of your insights into these issues Paul. It is much appreciate by myself and the rest of the team here at Google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-423269790
https://github.com/google/deepvariant/issues/94#issuecomment-423269790:447,Performance,perform,performance,447,"@ramcn I'm delighted you've gotten your build sorted out.; @ramcn If you are building DeepVariant from scratch, and in particular TF's wheels directly, I'd recommend looking into the exact version of TF you want to use with DeepVariant. In particular, if you are intending to run on CPUs, we've found that the MKL extensions to TF make call_variants 3-4x faster. It may be worth building yourself an optimized TF wheel for DeepVariant to maximize performance. @pgrosu Thank you for all of your insights into these issues Paul. It is much appreciate by myself and the rest of the team here at Google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-423269790
https://github.com/google/deepvariant/issues/94#issuecomment-423408772:63,Testability,log,logs,63,"That's great news Ram, and glad to hear it all worked out. The logs were basically guiding me through rules of implication, and it looked like we were getting close. Let us know if you run into any other issues, as we would be happy to help you out. @depristo Thank you Mark for the nice compliments, that means quite a lot. I always enjoy helping out folks and the team. I find exploring complex and challenging problems interesting and fun.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-423408772
https://github.com/google/deepvariant/issues/94#issuecomment-423408772:83,Usability,guid,guiding,83,"That's great news Ram, and glad to hear it all worked out. The logs were basically guiding me through rules of implication, and it looked like we were getting close. Let us know if you run into any other issues, as we would be happy to help you out. @depristo Thank you Mark for the nice compliments, that means quite a lot. I always enjoy helping out folks and the team. I find exploring complex and challenging problems interesting and fun.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/94#issuecomment-423408772
https://github.com/google/deepvariant/issues/95#issuecomment-423039615:36,Deployability,install,installed,36,"Hi Simon,. You'll need the bz2 libs installed. Either yum bzip2-libs and/or libbz2, as they are required. @ramcn Which do you have installed on your system?. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/95#issuecomment-423039615
https://github.com/google/deepvariant/issues/95#issuecomment-423039615:131,Deployability,install,installed,131,"Hi Simon,. You'll need the bz2 libs installed. Either yum bzip2-libs and/or libbz2, as they are required. @ramcn Which do you have installed on your system?. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/95#issuecomment-423039615
https://github.com/google/deepvariant/issues/95#issuecomment-470241986:227,Deployability,install,install,227,"Closing this issue as there has been no activity for a while. For anyone referencing this discussion, [here is a link](https://github.com/google/deepvariant/issues/137#issuecomment-452921108) to detailed instructions on how to install DeepVariant for CentOS7.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/95#issuecomment-470241986
https://github.com/google/deepvariant/issues/96#issuecomment-423515203:162,Deployability,pipeline,pipeline,162,"Hi Shruti,. The issue is that computations have locality, and would prefer to stay within the same data center. In your 0.6.1 scripts you specified zones for you pipeline runs (`--zones us-west1-b`) and for your gcp_deepvariant_runner (`--zones 'us-*'`), and not a region. In your 0.7.0 script you included both a zone `--zones us-west1-*` for your gcp_deepvariant_runner, and a region in your pipelines run `--regions us-west1`. Try to stick to one or the other, where a zone is more precise and a region contains a collection of zones. For more information here a couple of useful links:. https://cloud.google.com/compute/docs/regions-zones/. https://cloud.google.com/compute/docs/regions-zones/global-regional-zonal-resources. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/96#issuecomment-423515203
https://github.com/google/deepvariant/issues/96#issuecomment-423515203:394,Deployability,pipeline,pipelines,394,"Hi Shruti,. The issue is that computations have locality, and would prefer to stay within the same data center. In your 0.6.1 scripts you specified zones for you pipeline runs (`--zones us-west1-b`) and for your gcp_deepvariant_runner (`--zones 'us-*'`), and not a region. In your 0.7.0 script you included both a zone `--zones us-west1-*` for your gcp_deepvariant_runner, and a region in your pipelines run `--regions us-west1`. Try to stick to one or the other, where a zone is more precise and a region contains a collection of zones. For more information here a couple of useful links:. https://cloud.google.com/compute/docs/regions-zones/. https://cloud.google.com/compute/docs/regions-zones/global-regional-zonal-resources. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/96#issuecomment-423515203
https://github.com/google/deepvariant/issues/96#issuecomment-423567456:28,Modifiability,config,config,28,"Check the result of `gcloud config list`. I suspect you have region set to some default there. If so, you should unset it by `gcloud config set compute/region """"`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/96#issuecomment-423567456
https://github.com/google/deepvariant/issues/96#issuecomment-423567456:133,Modifiability,config,config,133,"Check the result of `gcloud config list`. I suspect you have region set to some default there. If so, you should unset it by `gcloud config set compute/region """"`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/96#issuecomment-423567456
https://github.com/google/deepvariant/issues/96#issuecomment-427101350:79,Modifiability,config,config,79,"Sorry @pichuan, I forgot to follow up. @nmousavi 's suggestion to unset gcloud config set compute/region """" helped.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/96#issuecomment-427101350
https://github.com/google/deepvariant/pull/97#issuecomment-424431766:34,Deployability,update,updated,34,Thanks for the pull request. I've updated the code internally and it'll go out with the next release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/97#issuecomment-424431766
https://github.com/google/deepvariant/pull/97#issuecomment-424431766:93,Deployability,release,release,93,Thanks for the pull request. I've updated the code internally and it'll go out with the next release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/97#issuecomment-424431766
https://github.com/google/deepvariant/issues/98#issuecomment-424847921:16,Deployability,install,installation,16,"I wonder if the installation for bazel is different on Ubuntu 18.04.; Can you try installing bazel separately, see https://docs.bazel.build/versions/master/install-ubuntu.html and see if you can install bazel?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/98#issuecomment-424847921
https://github.com/google/deepvariant/issues/98#issuecomment-424847921:82,Deployability,install,installing,82,"I wonder if the installation for bazel is different on Ubuntu 18.04.; Can you try installing bazel separately, see https://docs.bazel.build/versions/master/install-ubuntu.html and see if you can install bazel?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/98#issuecomment-424847921
https://github.com/google/deepvariant/issues/98#issuecomment-424847921:156,Deployability,install,install-ubuntu,156,"I wonder if the installation for bazel is different on Ubuntu 18.04.; Can you try installing bazel separately, see https://docs.bazel.build/versions/master/install-ubuntu.html and see if you can install bazel?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/98#issuecomment-424847921
https://github.com/google/deepvariant/issues/98#issuecomment-424847921:195,Deployability,install,install,195,"I wonder if the installation for bazel is different on Ubuntu 18.04.; Can you try installing bazel separately, see https://docs.bazel.build/versions/master/install-ubuntu.html and see if you can install bazel?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/98#issuecomment-424847921
https://github.com/google/deepvariant/issues/98#issuecomment-424863291:126,Availability,error,error,126,"Hello @pichuan , thanks for the swift reply. I installed bazel manually, the installations is working, however I get the same error from ./build-prereq.sh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/98#issuecomment-424863291
https://github.com/google/deepvariant/issues/98#issuecomment-424863291:47,Deployability,install,installed,47,"Hello @pichuan , thanks for the swift reply. I installed bazel manually, the installations is working, however I get the same error from ./build-prereq.sh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/98#issuecomment-424863291
https://github.com/google/deepvariant/issues/98#issuecomment-424863291:77,Deployability,install,installations,77,"Hello @pichuan , thanks for the swift reply. I installed bazel manually, the installations is working, however I get the same error from ./build-prereq.sh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/98#issuecomment-424863291
https://github.com/google/deepvariant/issues/98#issuecomment-424863854:56,Testability,log,log,56,"@vinisalazar Can you create a gist or attach a complete log of what you see when you run `./build-prereq.sh`? Also if you type `which bazel` what does show up? It might not be in your path. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/98#issuecomment-424863854
https://github.com/google/deepvariant/issues/98#issuecomment-424864841:89,Deployability,install,installed,89,"Hello, @pgrosu , thanks for the support. `which bazel` returns `/usr/local/bin/bazel` (I installed with sudo). As for the gist, [here it is.](https://gist.github.com/vinisalazar/c68d290a68f12677211c1398ba3c6dcc)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/98#issuecomment-424864841
https://github.com/google/deepvariant/issues/98#issuecomment-424941786:84,Deployability,install,install,84,"Hi @vinisalazar,. Out of curiosity, what happens if you type the following:. `conda install -c bioconda deepvariant`. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/98#issuecomment-424941786
https://github.com/google/deepvariant/issues/98#issuecomment-425108869:137,Deployability,install,install,137,"Hi @pgrosu ,. Thanks, that solved it. I had no idea there was a conda build for this package, otherwise it'd be the first way I'd try to install it. However, I'd recommend looking into these building problems. Thank you very much, ; V",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/98#issuecomment-425108869
https://github.com/google/deepvariant/issues/98#issuecomment-425162985:198,Deployability,update,update,198,"Hi @vinisalazar , note that the current bioconda version seems to be 0.6.1. It's also not directly maintained by our team, but it's contribution from @chapmanb. I'll check with him to see if we can update it. And yes, we'll look into the building issues. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/98#issuecomment-425162985
https://github.com/google/deepvariant/issues/98#issuecomment-425266931:274,Deployability,update,update,274,"Hi @vinisalazar , I just tried to build DeepVariant on a Ubuntu 18.04 machine. I did encounter some issue, but it didn't seem to be the same as the one you have. I suspect when you run `build-prereq.sh`, it actually failed already at this line:. ```; sudo -H apt-get -qq -y update; ```. Can you let me know if that works for you?; Maybe you can run it with out the `-qq` so it will actually report why it fails. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/98#issuecomment-425266931
https://github.com/google/deepvariant/issues/98#issuecomment-425286485:110,Deployability,update,update,110,"Hello @pichuan, thank you, I will try this on Monday when I get back to my work machine and come back with an update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/98#issuecomment-425286485
https://github.com/google/deepvariant/issues/98#issuecomment-425452266:46,Availability,ping,ping,46,"Vini, Paul and Pi-Chuan;; Thanks much for the ping about updating the conda recipe. This is now synced to the latest release (0.7.0) so you can use that as a backup option in addition to your work on getting it built on 18.04. Please let me know if you run into any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/98#issuecomment-425452266
https://github.com/google/deepvariant/issues/98#issuecomment-425452266:117,Deployability,release,release,117,"Vini, Paul and Pi-Chuan;; Thanks much for the ping about updating the conda recipe. This is now synced to the latest release (0.7.0) so you can use that as a backup option in addition to your work on getting it built on 18.04. Please let me know if you run into any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/98#issuecomment-425452266
https://github.com/google/deepvariant/issues/98#issuecomment-433210620:23,Deployability,update,update,23,"Hi,; I want to give an update on this - ; in the next minor release of DeepVariant, we'll plan to update the building script to allow you build on Ubuntu 18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/98#issuecomment-433210620
https://github.com/google/deepvariant/issues/98#issuecomment-433210620:60,Deployability,release,release,60,"Hi,; I want to give an update on this - ; in the next minor release of DeepVariant, we'll plan to update the building script to allow you build on Ubuntu 18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/98#issuecomment-433210620
https://github.com/google/deepvariant/issues/98#issuecomment-433210620:98,Deployability,update,update,98,"Hi,; I want to give an update on this - ; in the next minor release of DeepVariant, we'll plan to update the building script to allow you build on Ubuntu 18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/98#issuecomment-433210620
https://github.com/google/deepvariant/issues/99#issuecomment-426810731:10,Availability,error,error,10,"Hi,; this error corresponding to this line in the Nucleus codebase:; https://github.com/google/nucleus/blob/master/nucleus/io/sam_reader.cc#L409. which I think is indicating your BAM file actually isn't quite we're expecting. One qustion for you:; Is there anything after `fragment_name`? If so, it's useful to find out the read in your BAM file so we can understand it better. ; samtools view YOUR_BAM | grep FRAGMENT_NAME; might give you more information about the read that our SamReader is complaining. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-426810731
https://github.com/google/deepvariant/issues/99#issuecomment-426810736:134,Modifiability,variab,variable,134,"Hi Zhuyi,. DeepVariant is complaining here because your BAM has a record that says it has a mapped mate but mtid, which is the htslib variable holding the offset into the chromosome array, isn't set so its actual mapped location isn't present. This would normally indicate that's something corrupted with your BAM. Where did you get? How was it aligned? I'd recommend running ValidateSamFile to see if it complains. It could be we've being overly strict in Nucleus for parsing our BAMs. It'd be great if you can determine if your BAM is considered valid or not, and let us know if we need to be more permissive in our parsing or if the BAM needs to be fixed up. All the best,. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-426810736
https://github.com/google/deepvariant/issues/99#issuecomment-426810736:376,Security,Validat,ValidateSamFile,376,"Hi Zhuyi,. DeepVariant is complaining here because your BAM has a record that says it has a mapped mate but mtid, which is the htslib variable holding the offset into the chromosome array, isn't set so its actual mapped location isn't present. This would normally indicate that's something corrupted with your BAM. Where did you get? How was it aligned? I'd recommend running ValidateSamFile to see if it complains. It could be we've being overly strict in Nucleus for parsing our BAMs. It'd be great if you can determine if your BAM is considered valid or not, and let us know if we need to be more permissive in our parsing or if the BAM needs to be fixed up. All the best,. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-426810736
https://github.com/google/deepvariant/issues/99#issuecomment-426813624:68,Availability,down,downloaded,68,"It is the bam file of a TCGA sample. I didn't align it myself, it's downloaded directly from a TCGA host. I will check and see it's validity, thank you for the information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-426813624
https://github.com/google/deepvariant/issues/99#issuecomment-426828245:472,Availability,error,error,472,"I identified the read pairs that are causing the problem:. ```; [read_id]/2 145 chr1 10535 59 50M * 0 0 [read_seq] CC?DD@DDDDDEEEEEFFFFFFHHHHJIJJJJJIIJJGHHHHFFFFFCCC ...; [read_id]/1 81 chr10 38696640 69 29M1062N21M * 0 0 [read_seq] JJJJJJJJJJJJJJJJJJJJIJIFIGIJJIJJIGHGJHHFHHFFFFFCCB ...; ```; These are the primary alignments, they also has a few non-primary alignments. Hmm... the pair of reads are aligned to two chromosomes, is that why `make_examples` is raising the error? Could those reads just be ignored? I tried `make_examples` with one other bam, and see the same error. Also, wondering how `make_examples` deals with non-primary alignments?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-426828245
https://github.com/google/deepvariant/issues/99#issuecomment-426828245:575,Availability,error,error,575,"I identified the read pairs that are causing the problem:. ```; [read_id]/2 145 chr1 10535 59 50M * 0 0 [read_seq] CC?DD@DDDDDEEEEEFFFFFFHHHHJIJJJJJIIJJGHHHHFFFFFCCC ...; [read_id]/1 81 chr10 38696640 69 29M1062N21M * 0 0 [read_seq] JJJJJJJJJJJJJJJJJJJJIJIFIGIJJIJJIGHGJHHFHHFFFFFCCB ...; ```; These are the primary alignments, they also has a few non-primary alignments. Hmm... the pair of reads are aligned to two chromosomes, is that why `make_examples` is raising the error? Could those reads just be ignored? I tried `make_examples` with one other bam, and see the same error. Also, wondering how `make_examples` deals with non-primary alignments?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-426828245
https://github.com/google/deepvariant/issues/99#issuecomment-427117714:117,Availability,ping,ping,117,"Hi @zyxue , we'll look into this a bit more. Might be something we can improve on the Nucleus codebase. Feel free to ping again if we don't give another updates in a few days.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-427117714
https://github.com/google/deepvariant/issues/99#issuecomment-427117714:153,Deployability,update,updates,153,"Hi @zyxue , we'll look into this a bit more. Might be something we can improve on the Nucleus codebase. Feel free to ping again if we don't give another updates in a few days.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-427117714
https://github.com/google/deepvariant/issues/99#issuecomment-427460549:4,Deployability,update,update,4,"Any update on this, please? Is there a quick way that I could do to work around this, maybe even temporarily? By the way, I am using the docker image of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-427460549
https://github.com/google/deepvariant/issues/99#issuecomment-427462549:62,Availability,avail,available,62,"@zyxue I believe a proper bugfix is going in soon and will be available when we release an updated version of 0.7. Since the ETA for that isn't *right now* you can fix this in a local build yourself by:. changing:; https://github.com/google/deepvariant/blob/3c43de4541c45673e30d14daef742fca68fdf69b/third_party/nucleus/io/sam_reader.cc#L448. so that you have:. ```; if (c->mtid < -1); return tf::errors::DataLoss(; ""Expected mtid >= 0 as mate is supposedly mapped: "",; read_message->ShortDebugString());; else if (c->mtid == -1) {; mate_position->set_reference_name(""*"");; } else {; mate_position->set_reference_name(h->target_name[c->mtid]);; }; mate_position->set_position(c->mpos);; mate_position->set_reverse_strand(bam_is_mrev(b));; ```. which I believe should get your running again. You'll need to build DV from sources though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-427462549
https://github.com/google/deepvariant/issues/99#issuecomment-427462549:396,Availability,error,errors,396,"@zyxue I believe a proper bugfix is going in soon and will be available when we release an updated version of 0.7. Since the ETA for that isn't *right now* you can fix this in a local build yourself by:. changing:; https://github.com/google/deepvariant/blob/3c43de4541c45673e30d14daef742fca68fdf69b/third_party/nucleus/io/sam_reader.cc#L448. so that you have:. ```; if (c->mtid < -1); return tf::errors::DataLoss(; ""Expected mtid >= 0 as mate is supposedly mapped: "",; read_message->ShortDebugString());; else if (c->mtid == -1) {; mate_position->set_reference_name(""*"");; } else {; mate_position->set_reference_name(h->target_name[c->mtid]);; }; mate_position->set_position(c->mpos);; mate_position->set_reverse_strand(bam_is_mrev(b));; ```. which I believe should get your running again. You'll need to build DV from sources though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-427462549
https://github.com/google/deepvariant/issues/99#issuecomment-427462549:80,Deployability,release,release,80,"@zyxue I believe a proper bugfix is going in soon and will be available when we release an updated version of 0.7. Since the ETA for that isn't *right now* you can fix this in a local build yourself by:. changing:; https://github.com/google/deepvariant/blob/3c43de4541c45673e30d14daef742fca68fdf69b/third_party/nucleus/io/sam_reader.cc#L448. so that you have:. ```; if (c->mtid < -1); return tf::errors::DataLoss(; ""Expected mtid >= 0 as mate is supposedly mapped: "",; read_message->ShortDebugString());; else if (c->mtid == -1) {; mate_position->set_reference_name(""*"");; } else {; mate_position->set_reference_name(h->target_name[c->mtid]);; }; mate_position->set_position(c->mpos);; mate_position->set_reverse_strand(bam_is_mrev(b));; ```. which I believe should get your running again. You'll need to build DV from sources though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-427462549
https://github.com/google/deepvariant/issues/99#issuecomment-427462549:91,Deployability,update,updated,91,"@zyxue I believe a proper bugfix is going in soon and will be available when we release an updated version of 0.7. Since the ETA for that isn't *right now* you can fix this in a local build yourself by:. changing:; https://github.com/google/deepvariant/blob/3c43de4541c45673e30d14daef742fca68fdf69b/third_party/nucleus/io/sam_reader.cc#L448. so that you have:. ```; if (c->mtid < -1); return tf::errors::DataLoss(; ""Expected mtid >= 0 as mate is supposedly mapped: "",; read_message->ShortDebugString());; else if (c->mtid == -1) {; mate_position->set_reference_name(""*"");; } else {; mate_position->set_reference_name(h->target_name[c->mtid]);; }; mate_position->set_position(c->mpos);; mate_position->set_reverse_strand(bam_is_mrev(b));; ```. which I believe should get your running again. You'll need to build DV from sources though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-427462549
https://github.com/google/deepvariant/issues/99#issuecomment-428356537:648,Availability,error,errors,648,"I've made the suggested changes at https://github.com/zyxue/deepvariant/commit/cc09d6da17dcb700144cb3cd8dd2b7ece2c3ad86. ```diff; diff --git a/third_party/nucleus/io/sam_reader.cc b/third_party/nucleus/io/sam_reader.cc; index d8c7cae..1b020be 100644; --- a/third_party/nucleus/io/sam_reader.cc; +++ b/third_party/nucleus/io/sam_reader.cc; @@ -445,11 +445,15 @@ tf::Status ConvertToPb(const bam_hdr_t* h, const bam1_t* b,; // Set the mates map position if the mate is not unmapped.; if (paired && !(c->flag & BAM_FMUNMAP)) {; Position* mate_position = read_message->mutable_next_mate_position();; - if (c->mtid < 0); + if (c->mtid < -1); return tf::errors::DataLoss(; ""Expected mtid >= 0 as mate is supposedly mapped: "",; read_message->ShortDebugString());; - mate_position->set_reference_name(h->target_name[c->mtid]);; + else if (c->mtid == -1) {; + mate_position->set_reference_name(""*"");; + } else {; + mate_position->set_reference_name(h->target_name[c->mtid]);; + }; mate_position->set_position(c->mpos);; mate_position->set_reverse_strand(bam_is_mrev(b));; }; ```. What command do you use to build the docker image, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428356537
https://github.com/google/deepvariant/issues/99#issuecomment-428366972:287,Modifiability,config,config,287,"Use could build docker image with this command. It needs to be run from 'deepvariant' directory.; I used arbitrary values for PROJECT_ID and VERSION_NUMBER, you may replace them. . PROJECT_ID=my-deepvariant-docker; VERSION_NUMBER=0.7.2 ; gcloud builds submit --project ""${PROJECT_ID}"" --config cloudbuild.yaml --substitutions TAG_NAME=""${VERSION_NUMBER}"" --timeout 2h .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428366972
https://github.com/google/deepvariant/issues/99#issuecomment-428366972:357,Safety,timeout,timeout,357,"Use could build docker image with this command. It needs to be run from 'deepvariant' directory.; I used arbitrary values for PROJECT_ID and VERSION_NUMBER, you may replace them. . PROJECT_ID=my-deepvariant-docker; VERSION_NUMBER=0.7.2 ; gcloud builds submit --project ""${PROJECT_ID}"" --config cloudbuild.yaml --substitutions TAG_NAME=""${VERSION_NUMBER}"" --timeout 2h .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428366972
https://github.com/google/deepvariant/issues/99#issuecomment-428369416:111,Security,access,access,111,"Thanks for your command @akolesnikov, I've made it running. Also, I wonder how to build it locally without GCP access, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428369416
https://github.com/google/deepvariant/issues/99#issuecomment-428375827:189,Deployability,install,install,189,"@zyxue Via `cloud-build-local`, below is a link to more information about it: . https://cloud.google.com/cloud-build/docs/build-debug-locally. You will need to run the following command to install it:. `gcloud components install cloud-build-local`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428375827
https://github.com/google/deepvariant/issues/99#issuecomment-428375827:221,Deployability,install,install,221,"@zyxue Via `cloud-build-local`, below is a link to more information about it: . https://cloud.google.com/cloud-build/docs/build-debug-locally. You will need to run the following command to install it:. `gcloud components install cloud-build-local`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428375827
https://github.com/google/deepvariant/issues/99#issuecomment-428382405:31,Availability,ping,ping,31,"@zyxue Fantastic! Feel free to ping us if you run into any other issues :). Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428382405
https://github.com/google/deepvariant/issues/99#issuecomment-428623317:150,Energy Efficiency,Monitor,Monitoring,150,"Thanks, @pichuan. Any suggestion about the long hangs, please? My BAM size is 8.5G. I confirm the machine has been running overnight based on the GCE Monitoring tab.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428623317
https://github.com/google/deepvariant/issues/99#issuecomment-428637497:156,Deployability,configurat,configuration,156,"Hi @pichuan, are you sure that doc uses GPU?. >For this case study, we used a 64-core non-preemptible instance with 128GiB and no GPU. and I didn't see GPU configuration in the link to the command you posted?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428637497
https://github.com/google/deepvariant/issues/99#issuecomment-428637497:156,Modifiability,config,configuration,156,"Hi @pichuan, are you sure that doc uses GPU?. >For this case study, we used a 64-core non-preemptible instance with 128GiB and no GPU. and I didn't see GPU configuration in the link to the command you posted?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428637497
https://github.com/google/deepvariant/issues/99#issuecomment-428646375:143,Deployability,update,updates,143,"Hi @zyxue ; since your questions seem to be no longer relevant to the original question, I'll close this issue. Internally we've made the code updates to address the original issue, and will come out in a next release. It also seems like the local changes that @depristo suggested works for you now. Regarding your latest questions:; (1) There are areas in the genome that do take longer to run. This is usually areas where more reads piled up. We've made a lot of speed optimization over time, but it's still certainly true that will be regions that seem to run for much longer.; (2) Case study says ""no GPU"" in the sentence you quoted. Currently the case studies (WGS and WES) were both CPU only.; (3) For how to run with GPU, see this previous issue: https://github.com/google/deepvariant/issues/81",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428646375
https://github.com/google/deepvariant/issues/99#issuecomment-428646375:210,Deployability,release,release,210,"Hi @zyxue ; since your questions seem to be no longer relevant to the original question, I'll close this issue. Internally we've made the code updates to address the original issue, and will come out in a next release. It also seems like the local changes that @depristo suggested works for you now. Regarding your latest questions:; (1) There are areas in the genome that do take longer to run. This is usually areas where more reads piled up. We've made a lot of speed optimization over time, but it's still certainly true that will be regions that seem to run for much longer.; (2) Case study says ""no GPU"" in the sentence you quoted. Currently the case studies (WGS and WES) were both CPU only.; (3) For how to run with GPU, see this previous issue: https://github.com/google/deepvariant/issues/81",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428646375
https://github.com/google/deepvariant/issues/99#issuecomment-428646375:471,Performance,optimiz,optimization,471,"Hi @zyxue ; since your questions seem to be no longer relevant to the original question, I'll close this issue. Internally we've made the code updates to address the original issue, and will come out in a next release. It also seems like the local changes that @depristo suggested works for you now. Regarding your latest questions:; (1) There are areas in the genome that do take longer to run. This is usually areas where more reads piled up. We've made a lot of speed optimization over time, but it's still certainly true that will be regions that seem to run for much longer.; (2) Case study says ""no GPU"" in the sentence you quoted. Currently the case studies (WGS and WES) were both CPU only.; (3) For how to run with GPU, see this previous issue: https://github.com/google/deepvariant/issues/81",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428646375
https://github.com/google/deepvariant/issues/99#issuecomment-428649613:341,Energy Efficiency,power,power,341,"OK, thanks, you could close the ticket now. I meant to ask whether `make_examples` could be parallelized with or without GPU. Your previous link and #81 only show GPU use `call_variants`. From my understanding call_variant loads the Inception model and do NN forward computation to do prediction, so it makes sense it leverages the parallel power from GPU. But make_examples just convert BAM into images. Also, hanging for > 4hr doesn't seem to be caused only by deep pileup. I am currently rerunning make_examples, and will report in a new issue if it hangs again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428649613
https://github.com/google/deepvariant/issues/99#issuecomment-428649613:223,Performance,load,loads,223,"OK, thanks, you could close the ticket now. I meant to ask whether `make_examples` could be parallelized with or without GPU. Your previous link and #81 only show GPU use `call_variants`. From my understanding call_variant loads the Inception model and do NN forward computation to do prediction, so it makes sense it leverages the parallel power from GPU. But make_examples just convert BAM into images. Also, hanging for > 4hr doesn't seem to be caused only by deep pileup. I am currently rerunning make_examples, and will report in a new issue if it hangs again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428649613
https://github.com/google/deepvariant/issues/99#issuecomment-428649613:285,Safety,predict,prediction,285,"OK, thanks, you could close the ticket now. I meant to ask whether `make_examples` could be parallelized with or without GPU. Your previous link and #81 only show GPU use `call_variants`. From my understanding call_variant loads the Inception model and do NN forward computation to do prediction, so it makes sense it leverages the parallel power from GPU. But make_examples just convert BAM into images. Also, hanging for > 4hr doesn't seem to be caused only by deep pileup. I am currently rerunning make_examples, and will report in a new issue if it hangs again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428649613
https://github.com/google/deepvariant/issues/99#issuecomment-428662299:133,Availability,down,down,133,One possibility make_examples could take long time is if reads are too long. If your reads are greater than 250 there will be a slow down.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428662299
https://github.com/google/deepvariant/issues/99#issuecomment-428677704:72,Energy Efficiency,adapt,adapting,72,"Ahh, I see. Thank you @pichuan, now it makes total sense! I will rerun, adapting the command from https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-exome-case-study.md#run-make_examples",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428677704
https://github.com/google/deepvariant/issues/99#issuecomment-428677704:72,Modifiability,adapt,adapting,72,"Ahh, I see. Thank you @pichuan, now it makes total sense! I will rerun, adapting the command from https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-exome-case-study.md#run-make_examples",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428677704
https://github.com/google/deepvariant/issues/99#issuecomment-428686329:902,Availability,error,errors,902,"@zyxue I did an analysis a while ago, that posted at the following link:. [Generalized performance analysis between the versions](https://github.com/google/deepvariant/issues/50). The thing that dominated the processing at that time was the aligner, and would require more surgery to determine possibilities for optimization even though it got recently updated with the [FastPassAligner](https://github.com/google/deepvariant/blob/r0.7/deepvariant/realigner/fast_pass_aligner.cc) - which would require re-profiling. Basically a bunch of profiling tools would need to be built for you to then run, in order to determine for your scenario what the best optimization path would be. You probably noticed the following lines in [make_examples.py](https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1150-L1153) regarding the core restriction:. ```Python; if options.n_cores != 1:; errors.log_and_raise(; 'Currently only supports n_cores == 1 but got {}.'.format(; options.n_cores), errors.CommandLineError); ```. Though there are possibilities around that like @pichuan mentioned. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428686329
https://github.com/google/deepvariant/issues/99#issuecomment-428686329:1003,Availability,error,errors,1003,"@zyxue I did an analysis a while ago, that posted at the following link:. [Generalized performance analysis between the versions](https://github.com/google/deepvariant/issues/50). The thing that dominated the processing at that time was the aligner, and would require more surgery to determine possibilities for optimization even though it got recently updated with the [FastPassAligner](https://github.com/google/deepvariant/blob/r0.7/deepvariant/realigner/fast_pass_aligner.cc) - which would require re-profiling. Basically a bunch of profiling tools would need to be built for you to then run, in order to determine for your scenario what the best optimization path would be. You probably noticed the following lines in [make_examples.py](https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1150-L1153) regarding the core restriction:. ```Python; if options.n_cores != 1:; errors.log_and_raise(; 'Currently only supports n_cores == 1 but got {}.'.format(; options.n_cores), errors.CommandLineError); ```. Though there are possibilities around that like @pichuan mentioned. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428686329
https://github.com/google/deepvariant/issues/99#issuecomment-428686329:353,Deployability,update,updated,353,"@zyxue I did an analysis a while ago, that posted at the following link:. [Generalized performance analysis between the versions](https://github.com/google/deepvariant/issues/50). The thing that dominated the processing at that time was the aligner, and would require more surgery to determine possibilities for optimization even though it got recently updated with the [FastPassAligner](https://github.com/google/deepvariant/blob/r0.7/deepvariant/realigner/fast_pass_aligner.cc) - which would require re-profiling. Basically a bunch of profiling tools would need to be built for you to then run, in order to determine for your scenario what the best optimization path would be. You probably noticed the following lines in [make_examples.py](https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1150-L1153) regarding the core restriction:. ```Python; if options.n_cores != 1:; errors.log_and_raise(; 'Currently only supports n_cores == 1 but got {}.'.format(; options.n_cores), errors.CommandLineError); ```. Though there are possibilities around that like @pichuan mentioned. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428686329
https://github.com/google/deepvariant/issues/99#issuecomment-428686329:87,Performance,perform,performance,87,"@zyxue I did an analysis a while ago, that posted at the following link:. [Generalized performance analysis between the versions](https://github.com/google/deepvariant/issues/50). The thing that dominated the processing at that time was the aligner, and would require more surgery to determine possibilities for optimization even though it got recently updated with the [FastPassAligner](https://github.com/google/deepvariant/blob/r0.7/deepvariant/realigner/fast_pass_aligner.cc) - which would require re-profiling. Basically a bunch of profiling tools would need to be built for you to then run, in order to determine for your scenario what the best optimization path would be. You probably noticed the following lines in [make_examples.py](https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1150-L1153) regarding the core restriction:. ```Python; if options.n_cores != 1:; errors.log_and_raise(; 'Currently only supports n_cores == 1 but got {}.'.format(; options.n_cores), errors.CommandLineError); ```. Though there are possibilities around that like @pichuan mentioned. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428686329
https://github.com/google/deepvariant/issues/99#issuecomment-428686329:312,Performance,optimiz,optimization,312,"@zyxue I did an analysis a while ago, that posted at the following link:. [Generalized performance analysis between the versions](https://github.com/google/deepvariant/issues/50). The thing that dominated the processing at that time was the aligner, and would require more surgery to determine possibilities for optimization even though it got recently updated with the [FastPassAligner](https://github.com/google/deepvariant/blob/r0.7/deepvariant/realigner/fast_pass_aligner.cc) - which would require re-profiling. Basically a bunch of profiling tools would need to be built for you to then run, in order to determine for your scenario what the best optimization path would be. You probably noticed the following lines in [make_examples.py](https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1150-L1153) regarding the core restriction:. ```Python; if options.n_cores != 1:; errors.log_and_raise(; 'Currently only supports n_cores == 1 but got {}.'.format(; options.n_cores), errors.CommandLineError); ```. Though there are possibilities around that like @pichuan mentioned. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428686329
https://github.com/google/deepvariant/issues/99#issuecomment-428686329:651,Performance,optimiz,optimization,651,"@zyxue I did an analysis a while ago, that posted at the following link:. [Generalized performance analysis between the versions](https://github.com/google/deepvariant/issues/50). The thing that dominated the processing at that time was the aligner, and would require more surgery to determine possibilities for optimization even though it got recently updated with the [FastPassAligner](https://github.com/google/deepvariant/blob/r0.7/deepvariant/realigner/fast_pass_aligner.cc) - which would require re-profiling. Basically a bunch of profiling tools would need to be built for you to then run, in order to determine for your scenario what the best optimization path would be. You probably noticed the following lines in [make_examples.py](https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1150-L1153) regarding the core restriction:. ```Python; if options.n_cores != 1:; errors.log_and_raise(; 'Currently only supports n_cores == 1 but got {}.'.format(; options.n_cores), errors.CommandLineError); ```. Though there are possibilities around that like @pichuan mentioned. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428686329
https://github.com/google/deepvariant/issues/99#issuecomment-428692945:472,Testability,log,log,472,"I have a couple of questions about https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-exome-case-study.md#run-make_examples,. 1. how is `examples.tfrecord@${N_SHARDS}.gz` evaluated to become `examples.tfrecord-00006-of-00008.gz`; 1. what does `--task {}` mean?. Could you please explain briefly how this command parallelize the processing? N_SHARDS docker containers have been started, and it seems each 1kbp region is sent to a different processor based on log, but I don't quite get it how each container knows which 1kbp region to parse.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/99#issuecomment-428692945
https://github.com/google/deepvariant/issues/101#issuecomment-430074459:15,Deployability,install,install,15,"Is the one you install on conda this one?; https://bioconda.github.io/recipes/deepvariant/README.html; If it's that, I believe it's recently updated to 0.7. ; This package actually isn't manage by the team at Google. But the owner ( @chapmanb ) has updated to the latest version (0.7). A quick search of `dv_make_examples.py` seems to be pointed to this file; https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/dv_make_examples.py; which it does call python make_examples.zip.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/101#issuecomment-430074459
https://github.com/google/deepvariant/issues/101#issuecomment-430074459:141,Deployability,update,updated,141,"Is the one you install on conda this one?; https://bioconda.github.io/recipes/deepvariant/README.html; If it's that, I believe it's recently updated to 0.7. ; This package actually isn't manage by the team at Google. But the owner ( @chapmanb ) has updated to the latest version (0.7). A quick search of `dv_make_examples.py` seems to be pointed to this file; https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/dv_make_examples.py; which it does call python make_examples.zip.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/101#issuecomment-430074459
https://github.com/google/deepvariant/issues/101#issuecomment-430074459:249,Deployability,update,updated,249,"Is the one you install on conda this one?; https://bioconda.github.io/recipes/deepvariant/README.html; If it's that, I believe it's recently updated to 0.7. ; This package actually isn't manage by the team at Google. But the owner ( @chapmanb ) has updated to the latest version (0.7). A quick search of `dv_make_examples.py` seems to be pointed to this file; https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/dv_make_examples.py; which it does call python make_examples.zip.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/101#issuecomment-430074459
https://github.com/google/deepvariant/issues/101#issuecomment-430171385:287,Deployability,install,installs,287,"Phil and Pi-Chuan;; That's exactly right, `dv_make_examples.py` is a simple wrapper for `make_examples.zip` to make it directly callable and is part of the conda packaging process, not a default script from the Google team. If it's not useful, feel free to call directly at the standard installs, it just tries to avoid needing to know the full install path and other details about the parallelization so the command line syntax is simplified. For your example you'd use it like:; ```; dv_make_examples.py; --ref chr20.fa.gz \; --reads test.bam \; --examples shardedExamples/examples.tfrecord@2.gz \; --regions regions.bed \; --sample test \; --logdir location/to/place/logfiles; --cores 1; ```; Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/101#issuecomment-430171385
https://github.com/google/deepvariant/issues/101#issuecomment-430171385:345,Deployability,install,install,345,"Phil and Pi-Chuan;; That's exactly right, `dv_make_examples.py` is a simple wrapper for `make_examples.zip` to make it directly callable and is part of the conda packaging process, not a default script from the Google team. If it's not useful, feel free to call directly at the standard installs, it just tries to avoid needing to know the full install path and other details about the parallelization so the command line syntax is simplified. For your example you'd use it like:; ```; dv_make_examples.py; --ref chr20.fa.gz \; --reads test.bam \; --examples shardedExamples/examples.tfrecord@2.gz \; --regions regions.bed \; --sample test \; --logdir location/to/place/logfiles; --cores 1; ```; Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/101#issuecomment-430171385
https://github.com/google/deepvariant/issues/101#issuecomment-430171385:76,Integrability,wrap,wrapper,76,"Phil and Pi-Chuan;; That's exactly right, `dv_make_examples.py` is a simple wrapper for `make_examples.zip` to make it directly callable and is part of the conda packaging process, not a default script from the Google team. If it's not useful, feel free to call directly at the standard installs, it just tries to avoid needing to know the full install path and other details about the parallelization so the command line syntax is simplified. For your example you'd use it like:; ```; dv_make_examples.py; --ref chr20.fa.gz \; --reads test.bam \; --examples shardedExamples/examples.tfrecord@2.gz \; --regions regions.bed \; --sample test \; --logdir location/to/place/logfiles; --cores 1; ```; Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/101#issuecomment-430171385
https://github.com/google/deepvariant/issues/101#issuecomment-430171385:314,Safety,avoid,avoid,314,"Phil and Pi-Chuan;; That's exactly right, `dv_make_examples.py` is a simple wrapper for `make_examples.zip` to make it directly callable and is part of the conda packaging process, not a default script from the Google team. If it's not useful, feel free to call directly at the standard installs, it just tries to avoid needing to know the full install path and other details about the parallelization so the command line syntax is simplified. For your example you'd use it like:; ```; dv_make_examples.py; --ref chr20.fa.gz \; --reads test.bam \; --examples shardedExamples/examples.tfrecord@2.gz \; --regions regions.bed \; --sample test \; --logdir location/to/place/logfiles; --cores 1; ```; Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/101#issuecomment-430171385
https://github.com/google/deepvariant/issues/101#issuecomment-430171385:536,Testability,test,test,536,"Phil and Pi-Chuan;; That's exactly right, `dv_make_examples.py` is a simple wrapper for `make_examples.zip` to make it directly callable and is part of the conda packaging process, not a default script from the Google team. If it's not useful, feel free to call directly at the standard installs, it just tries to avoid needing to know the full install path and other details about the parallelization so the command line syntax is simplified. For your example you'd use it like:; ```; dv_make_examples.py; --ref chr20.fa.gz \; --reads test.bam \; --examples shardedExamples/examples.tfrecord@2.gz \; --regions regions.bed \; --sample test \; --logdir location/to/place/logfiles; --cores 1; ```; Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/101#issuecomment-430171385
https://github.com/google/deepvariant/issues/101#issuecomment-430171385:635,Testability,test,test,635,"Phil and Pi-Chuan;; That's exactly right, `dv_make_examples.py` is a simple wrapper for `make_examples.zip` to make it directly callable and is part of the conda packaging process, not a default script from the Google team. If it's not useful, feel free to call directly at the standard installs, it just tries to avoid needing to know the full install path and other details about the parallelization so the command line syntax is simplified. For your example you'd use it like:; ```; dv_make_examples.py; --ref chr20.fa.gz \; --reads test.bam \; --examples shardedExamples/examples.tfrecord@2.gz \; --regions regions.bed \; --sample test \; --logdir location/to/place/logfiles; --cores 1; ```; Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/101#issuecomment-430171385
https://github.com/google/deepvariant/issues/101#issuecomment-430171385:645,Testability,log,logdir,645,"Phil and Pi-Chuan;; That's exactly right, `dv_make_examples.py` is a simple wrapper for `make_examples.zip` to make it directly callable and is part of the conda packaging process, not a default script from the Google team. If it's not useful, feel free to call directly at the standard installs, it just tries to avoid needing to know the full install path and other details about the parallelization so the command line syntax is simplified. For your example you'd use it like:; ```; dv_make_examples.py; --ref chr20.fa.gz \; --reads test.bam \; --examples shardedExamples/examples.tfrecord@2.gz \; --regions regions.bed \; --sample test \; --logdir location/to/place/logfiles; --cores 1; ```; Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/101#issuecomment-430171385
https://github.com/google/deepvariant/issues/101#issuecomment-430171385:670,Testability,log,logfiles,670,"Phil and Pi-Chuan;; That's exactly right, `dv_make_examples.py` is a simple wrapper for `make_examples.zip` to make it directly callable and is part of the conda packaging process, not a default script from the Google team. If it's not useful, feel free to call directly at the standard installs, it just tries to avoid needing to know the full install path and other details about the parallelization so the command line syntax is simplified. For your example you'd use it like:; ```; dv_make_examples.py; --ref chr20.fa.gz \; --reads test.bam \; --examples shardedExamples/examples.tfrecord@2.gz \; --regions regions.bed \; --sample test \; --logdir location/to/place/logfiles; --cores 1; ```; Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/101#issuecomment-430171385
https://github.com/google/deepvariant/issues/101#issuecomment-430171385:69,Usability,simpl,simple,69,"Phil and Pi-Chuan;; That's exactly right, `dv_make_examples.py` is a simple wrapper for `make_examples.zip` to make it directly callable and is part of the conda packaging process, not a default script from the Google team. If it's not useful, feel free to call directly at the standard installs, it just tries to avoid needing to know the full install path and other details about the parallelization so the command line syntax is simplified. For your example you'd use it like:; ```; dv_make_examples.py; --ref chr20.fa.gz \; --reads test.bam \; --examples shardedExamples/examples.tfrecord@2.gz \; --regions regions.bed \; --sample test \; --logdir location/to/place/logfiles; --cores 1; ```; Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/101#issuecomment-430171385
https://github.com/google/deepvariant/issues/101#issuecomment-430171385:432,Usability,simpl,simplified,432,"Phil and Pi-Chuan;; That's exactly right, `dv_make_examples.py` is a simple wrapper for `make_examples.zip` to make it directly callable and is part of the conda packaging process, not a default script from the Google team. If it's not useful, feel free to call directly at the standard installs, it just tries to avoid needing to know the full install path and other details about the parallelization so the command line syntax is simplified. For your example you'd use it like:; ```; dv_make_examples.py; --ref chr20.fa.gz \; --reads test.bam \; --examples shardedExamples/examples.tfrecord@2.gz \; --regions regions.bed \; --sample test \; --logdir location/to/place/logfiles; --cores 1; ```; Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/101#issuecomment-430171385
https://github.com/google/deepvariant/issues/102#issuecomment-429139266:692,Testability,stub,stubs,692,"@zyxue Two small questions:. 1) Do you have a NVidia GPU on the machine you are running DeepVariant on?. 2) If you type `find / -name ""*libcublas*"" -print 2>/dev/null` on the command prompt, do you see something similar to this:. ```; $ find / -name ""*libcublas*"" -print 2>/dev/null; /opt/apps/cuda75/sdk/7.5.18/doc/man/man7/libcublas.7; /opt/apps/cuda75/sdk/7.5.18/doc/man/man7/libcublas.so.7; /opt/apps/cuda75/sdk/7.5.18/lib64/libcublas_static.a; /opt/apps/cuda75/sdk/7.5.18/lib64/libcublas.so; /opt/apps/cuda75/sdk/7.5.18/lib64/libcublas_device.a; /opt/apps/cuda75/sdk/7.5.18/lib64/libcublas.so.7.5; /opt/apps/cuda75/sdk/7.5.18/lib64/libcublas.so.7.5.18; /opt/apps/cuda75/sdk/7.5.18/lib64/stubs/libcublas.so; $; ```. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/102#issuecomment-429139266
https://github.com/google/deepvariant/issues/102#issuecomment-429143561:149,Availability,error,error,149,"Yes, I have a GPU. I've made it work with. ```; gcr.io/deepvariant-docker/deepvariant_gpu:0.7.0; ```. But it pops out the `libcublas.so.9.0` related error with my own built (following instruction on https://github.com/google/deepvariant/issues/99#issuecomment-428366972). ```; gcr.io/my_project/deepvariant_gpu:0.7.2; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/102#issuecomment-429143561
https://github.com/google/deepvariant/issues/102#issuecomment-429154858:292,Availability,down,download-archive,292,"Right, but which version of the Cuda Toolkit do you have? You can easily install the 9.0 from the link below, but I would do it as a local (non-sudo) user so you sandbox the changes to your environment in order to easily remove them later, if necessary:. https://developer.nvidia.com/cuda-90-download-archive. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/102#issuecomment-429154858
https://github.com/google/deepvariant/issues/102#issuecomment-429154858:73,Deployability,install,install,73,"Right, but which version of the Cuda Toolkit do you have? You can easily install the 9.0 from the link below, but I would do it as a local (non-sudo) user so you sandbox the changes to your environment in order to easily remove them later, if necessary:. https://developer.nvidia.com/cuda-90-download-archive. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/102#issuecomment-429154858
https://github.com/google/deepvariant/issues/102#issuecomment-429154858:162,Modifiability,sandbox,sandbox,162,"Right, but which version of the Cuda Toolkit do you have? You can easily install the 9.0 from the link below, but I would do it as a local (non-sudo) user so you sandbox the changes to your environment in order to easily remove them later, if necessary:. https://developer.nvidia.com/cuda-90-download-archive. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/102#issuecomment-429154858
https://github.com/google/deepvariant/issues/102#issuecomment-429154858:162,Testability,sandbox,sandbox,162,"Right, but which version of the Cuda Toolkit do you have? You can easily install the 9.0 from the link below, but I would do it as a local (non-sudo) user so you sandbox the changes to your environment in order to easily remove them later, if necessary:. https://developer.nvidia.com/cuda-90-download-archive. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/102#issuecomment-429154858
https://github.com/google/deepvariant/issues/102#issuecomment-429155535:2,Deployability,install,installed,2,"I installed CUDA-9.0 based on instruction here, https://cloud.google.com/compute/docs/gpus/add-gpus#install-driver-script. and nvidia-docker based on instruction here, https://github.com/NVIDIA/nvidia-docker#ubuntu-140416041804-debian-jessiestretch. I am on a Ubuntu-16.04 GCE box. Sorry, there was a typo in my previous comment, corrected it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/102#issuecomment-429155535
https://github.com/google/deepvariant/issues/102#issuecomment-429155535:100,Deployability,install,install-driver-script,100,"I installed CUDA-9.0 based on instruction here, https://cloud.google.com/compute/docs/gpus/add-gpus#install-driver-script. and nvidia-docker based on instruction here, https://github.com/NVIDIA/nvidia-docker#ubuntu-140416041804-debian-jessiestretch. I am on a Ubuntu-16.04 GCE box. Sorry, there was a typo in my previous comment, corrected it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/102#issuecomment-429155535
https://github.com/google/deepvariant/issues/102#issuecomment-429170838:51,Availability,echo,echo,51,"Could you please run the following commands:. ```; echo $LD_LIBRARY_PATH; echo $PATH; ```. And also paste the path of where `libcublas.so.9.0` actually resides. Then we can patch the paths appropriately. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/102#issuecomment-429170838
https://github.com/google/deepvariant/issues/102#issuecomment-429170838:74,Availability,echo,echo,74,"Could you please run the following commands:. ```; echo $LD_LIBRARY_PATH; echo $PATH; ```. And also paste the path of where `libcublas.so.9.0` actually resides. Then we can patch the paths appropriately. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/102#issuecomment-429170838
https://github.com/google/deepvariant/issues/102#issuecomment-429170838:173,Deployability,patch,patch,173,"Could you please run the following commands:. ```; echo $LD_LIBRARY_PATH; echo $PATH; ```. And also paste the path of where `libcublas.so.9.0` actually resides. Then we can patch the paths appropriately. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/102#issuecomment-429170838
https://github.com/google/deepvariant/issues/102#issuecomment-436138829:65,Deployability,update,updates,65,Closing because of inactivity. Feel free to open again with more updates.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/102#issuecomment-436138829
https://github.com/google/deepvariant/issues/103#issuecomment-429462894:619,Performance,load,loader,619,"The start seems to be quite slow, after about 27s it exits, here is the output; ```; root@52e2e7bb0093:/opt/deepvariant/bin/unzip-post/runfiles/com_google_deepvariant/deepvariant# time python -mtrace --trackcalls postprocess_variants.py --ref /home/zxue/deepvariant_exp/tcga-data/hg19.fa --infile /home/zxue/deepvariant_exp/output/cvo.tfrecord.gz ; --outfile /home/zxue/deepvariant_exp/output/output.vcf.gz --nonvariant_site_tfrecord_path /home/zxue/deepvariant_exp/output/gvcf.tfrecord@8.gz ; Traceback (most recent call last):; File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main; ""__main__"", fname, loader, pkg_name); File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code; exec code in run_globals; File ""/usr/lib/python2.7/trace.py"", line 819, in <module>; main(); File ""/usr/lib/python2.7/trace.py"", line 807, in main; t.runctx(code, globs, globs); File ""/usr/lib/python2.7/trace.py"", line 513, in runctx; exec cmd in globals, locals; File ""postprocess_variants.py"", line 46, in <module>; from third_party.nucleus.io import fasta; ImportError: No module named third_party.nucleus.io. real 0m27.273s; user 0m27.133s; sys 0m0.904s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/103#issuecomment-429462894
https://github.com/google/deepvariant/issues/103#issuecomment-436139125:92,Deployability,update,update,92,"@zyxue curious whether you're able to resolve this?; I'm going to close this since the last update was a while ago. If you have more information, feel free to update and re-open.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/103#issuecomment-436139125
https://github.com/google/deepvariant/issues/103#issuecomment-436139125:159,Deployability,update,update,159,"@zyxue curious whether you're able to resolve this?; I'm going to close this since the last update was a while ago. If you have more information, feel free to update and re-open.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/103#issuecomment-436139125
https://github.com/google/deepvariant/issues/104#issuecomment-429383473:349,Availability,error,error,349,"@AlfWa Have you tried it without running it being in interactive mode, basically without the `-it` flag? In interactive mode you usually need to do more work to get it to work. Try it first without that flag, but just curious why are you working with it in interactive mode as it should work outside of that mode? Are you getting the same attribute error that way too? . Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-429383473
https://github.com/google/deepvariant/issues/104#issuecomment-429700976:74,Availability,Error,Error,74,"met same issue when we were trying to run deep variant on GCP Centos7 vm. Error was raised by the function call of psutil.cpu_freq(); https://github.com/google/deepvariant/blob/r0.7/deepvariant/resources.py#L158 . psutil.cpu_freq function is only defined under the condition ; os.path.exists(""/sys/devices/system/cpu/cpufreq"") or os.path.exists(""/sys/devices/system/cpu/cpu0/cpufreq""); https://github.com/giampaolo/psutil/blob/release-5.4.2/psutil/_pslinux.py#L656-L657. So the issue is that the required paths do not exist in the CentOS 7 release. existence check of the paths before calling psutil.cpu_freq() should solve the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-429700976
https://github.com/google/deepvariant/issues/104#issuecomment-429700976:427,Deployability,release,release-,427,"met same issue when we were trying to run deep variant on GCP Centos7 vm. Error was raised by the function call of psutil.cpu_freq(); https://github.com/google/deepvariant/blob/r0.7/deepvariant/resources.py#L158 . psutil.cpu_freq function is only defined under the condition ; os.path.exists(""/sys/devices/system/cpu/cpufreq"") or os.path.exists(""/sys/devices/system/cpu/cpu0/cpufreq""); https://github.com/giampaolo/psutil/blob/release-5.4.2/psutil/_pslinux.py#L656-L657. So the issue is that the required paths do not exist in the CentOS 7 release. existence check of the paths before calling psutil.cpu_freq() should solve the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-429700976
https://github.com/google/deepvariant/issues/104#issuecomment-429700976:540,Deployability,release,release,540,"met same issue when we were trying to run deep variant on GCP Centos7 vm. Error was raised by the function call of psutil.cpu_freq(); https://github.com/google/deepvariant/blob/r0.7/deepvariant/resources.py#L158 . psutil.cpu_freq function is only defined under the condition ; os.path.exists(""/sys/devices/system/cpu/cpufreq"") or os.path.exists(""/sys/devices/system/cpu/cpu0/cpufreq""); https://github.com/giampaolo/psutil/blob/release-5.4.2/psutil/_pslinux.py#L656-L657. So the issue is that the required paths do not exist in the CentOS 7 release. existence check of the paths before calling psutil.cpu_freq() should solve the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-429700976
https://github.com/google/deepvariant/issues/104#issuecomment-429714006:680,Security,access,access,680,"Unbelievable! Thanks for the pointing this out @obigbando! . @AlfWa you probably would need to set `intel_pstate=disable` at boot time as a kernel argument, and then `cpupower` - or if really necessary `modprobe` - according to what is listed in the directory using this command:. ```; ls /lib/modules/`uname -r`/kernel/drivers/cpufreq/; ```. You'll probably see something like the following, or close to it which might include `acpi-cpufreq`:. ```; $ ls /lib/modules/`uname -r`/kernel/drivers/cpufreq/; cpufreq_conservative.ko cpufreq_powersave.ko freq_table.ko; cpufreq_ondemand.ko cpufreq_stats.ko; $; ```; Try reading the following version 7 documentation carefully:. https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/power_management_guide/cpufreq_governors. I included also the version 6 with `modprobe` just in case:. https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/power_management_guide/cpufreq_setup. Let us know if you run into any issues. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-429714006
https://github.com/google/deepvariant/issues/104#issuecomment-429714006:861,Security,access,access,861,"Unbelievable! Thanks for the pointing this out @obigbando! . @AlfWa you probably would need to set `intel_pstate=disable` at boot time as a kernel argument, and then `cpupower` - or if really necessary `modprobe` - according to what is listed in the directory using this command:. ```; ls /lib/modules/`uname -r`/kernel/drivers/cpufreq/; ```. You'll probably see something like the following, or close to it which might include `acpi-cpufreq`:. ```; $ ls /lib/modules/`uname -r`/kernel/drivers/cpufreq/; cpufreq_conservative.ko cpufreq_powersave.ko freq_table.ko; cpufreq_ondemand.ko cpufreq_stats.ko; $; ```; Try reading the following version 7 documentation carefully:. https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/power_management_guide/cpufreq_governors. I included also the version 6 with `modprobe` just in case:. https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/power_management_guide/cpufreq_setup. Let us know if you run into any issues. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-429714006
https://github.com/google/deepvariant/issues/104#issuecomment-430404177:120,Integrability,wrap,wrap,120,We're seeing this same problem on centos7 on new processors for which there is no working cpufreq support. Why not just wrap the call to `psutil.cpu_freq` in a `try`? I'm happy to provide a PR for that. (cc: @njcarriero @chris-sf),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-430404177
https://github.com/google/deepvariant/issues/104#issuecomment-437599222:52,Availability,error,error,52,"Hi,; Is there a fix for this? I am getting the same error: AttributeError: 'module' object has no attribute 'cpu_freq'; This error persists even when I include intel_pstate=disable .; Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-437599222
https://github.com/google/deepvariant/issues/104#issuecomment-437599222:125,Availability,error,error,125,"Hi,; Is there a fix for this? I am getting the same error: AttributeError: 'module' object has no attribute 'cpu_freq'; This error persists even when I include intel_pstate=disable .; Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-437599222
https://github.com/google/deepvariant/issues/104#issuecomment-437647446:587,Deployability,release,release,587,"Hi,; can you try the 0.7.1 image? It seems to work for me on CentOS7 now. I was able to run the Exome case study with the following steps:. I used a GCE instance for testing:. ```; gcloud beta compute instances create ""${USER}-centos-test"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image ""centos-7-v20181011"" \; --image-project centos-cloud \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --boot-disk-type ""pd-ssd"" \; --zone ""us-west1-b""; ```. I sshed into the machine, and checked the OS version:. ```; [pichuan@pichuan-centos-test ~]$ cat /etc/redhat-release; CentOS Linux release 7.5.1804 (Core) ; ```. Then:. ```; [pichuan@pichuan-centos-test ~]$ sudo yum install -y docker; [pichuan@pichuan-centos-test ~]$ sudo service docker start; Redirecting to /bin/systemctl start docker.service; [pichuan@pichuan-centos-test ~]$ sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.1; ```. Then, I use a modified script to run WES case study:. ```; root@5189b7fba95b:/# curl https://gist.githubusercontent.com/pichuan/7916a5574c47fd7a2422bad4e5f73860/raw/44fb4377c9ad8b54e195fdb31489d8e9fbc459e2/inside_docker.sh | bash; ```. You can see:; https://gist.githubusercontent.com/pichuan/7916a5574c47fd7a2422bad4e5f73860/raw/44fb4377c9ad8b54e195fdb31489d8e9fbc459e2/inside_docker.sh; for the script that I ran inside the docker. If the 0.7.1 version still doesn't work, feel free to reopen. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-437647446
https://github.com/google/deepvariant/issues/104#issuecomment-437647446:609,Deployability,release,release,609,"Hi,; can you try the 0.7.1 image? It seems to work for me on CentOS7 now. I was able to run the Exome case study with the following steps:. I used a GCE instance for testing:. ```; gcloud beta compute instances create ""${USER}-centos-test"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image ""centos-7-v20181011"" \; --image-project centos-cloud \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --boot-disk-type ""pd-ssd"" \; --zone ""us-west1-b""; ```. I sshed into the machine, and checked the OS version:. ```; [pichuan@pichuan-centos-test ~]$ cat /etc/redhat-release; CentOS Linux release 7.5.1804 (Core) ; ```. Then:. ```; [pichuan@pichuan-centos-test ~]$ sudo yum install -y docker; [pichuan@pichuan-centos-test ~]$ sudo service docker start; Redirecting to /bin/systemctl start docker.service; [pichuan@pichuan-centos-test ~]$ sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.1; ```. Then, I use a modified script to run WES case study:. ```; root@5189b7fba95b:/# curl https://gist.githubusercontent.com/pichuan/7916a5574c47fd7a2422bad4e5f73860/raw/44fb4377c9ad8b54e195fdb31489d8e9fbc459e2/inside_docker.sh | bash; ```. You can see:; https://gist.githubusercontent.com/pichuan/7916a5574c47fd7a2422bad4e5f73860/raw/44fb4377c9ad8b54e195fdb31489d8e9fbc459e2/inside_docker.sh; for the script that I ran inside the docker. If the 0.7.1 version still doesn't work, feel free to reopen. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-437647446
https://github.com/google/deepvariant/issues/104#issuecomment-437647446:694,Deployability,install,install,694,"Hi,; can you try the 0.7.1 image? It seems to work for me on CentOS7 now. I was able to run the Exome case study with the following steps:. I used a GCE instance for testing:. ```; gcloud beta compute instances create ""${USER}-centos-test"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image ""centos-7-v20181011"" \; --image-project centos-cloud \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --boot-disk-type ""pd-ssd"" \; --zone ""us-west1-b""; ```. I sshed into the machine, and checked the OS version:. ```; [pichuan@pichuan-centos-test ~]$ cat /etc/redhat-release; CentOS Linux release 7.5.1804 (Core) ; ```. Then:. ```; [pichuan@pichuan-centos-test ~]$ sudo yum install -y docker; [pichuan@pichuan-centos-test ~]$ sudo service docker start; Redirecting to /bin/systemctl start docker.service; [pichuan@pichuan-centos-test ~]$ sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.1; ```. Then, I use a modified script to run WES case study:. ```; root@5189b7fba95b:/# curl https://gist.githubusercontent.com/pichuan/7916a5574c47fd7a2422bad4e5f73860/raw/44fb4377c9ad8b54e195fdb31489d8e9fbc459e2/inside_docker.sh | bash; ```. You can see:; https://gist.githubusercontent.com/pichuan/7916a5574c47fd7a2422bad4e5f73860/raw/44fb4377c9ad8b54e195fdb31489d8e9fbc459e2/inside_docker.sh; for the script that I ran inside the docker. If the 0.7.1 version still doesn't work, feel free to reopen. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-437647446
https://github.com/google/deepvariant/issues/104#issuecomment-437647446:166,Testability,test,testing,166,"Hi,; can you try the 0.7.1 image? It seems to work for me on CentOS7 now. I was able to run the Exome case study with the following steps:. I used a GCE instance for testing:. ```; gcloud beta compute instances create ""${USER}-centos-test"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image ""centos-7-v20181011"" \; --image-project centos-cloud \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --boot-disk-type ""pd-ssd"" \; --zone ""us-west1-b""; ```. I sshed into the machine, and checked the OS version:. ```; [pichuan@pichuan-centos-test ~]$ cat /etc/redhat-release; CentOS Linux release 7.5.1804 (Core) ; ```. Then:. ```; [pichuan@pichuan-centos-test ~]$ sudo yum install -y docker; [pichuan@pichuan-centos-test ~]$ sudo service docker start; Redirecting to /bin/systemctl start docker.service; [pichuan@pichuan-centos-test ~]$ sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.1; ```. Then, I use a modified script to run WES case study:. ```; root@5189b7fba95b:/# curl https://gist.githubusercontent.com/pichuan/7916a5574c47fd7a2422bad4e5f73860/raw/44fb4377c9ad8b54e195fdb31489d8e9fbc459e2/inside_docker.sh | bash; ```. You can see:; https://gist.githubusercontent.com/pichuan/7916a5574c47fd7a2422bad4e5f73860/raw/44fb4377c9ad8b54e195fdb31489d8e9fbc459e2/inside_docker.sh; for the script that I ran inside the docker. If the 0.7.1 version still doesn't work, feel free to reopen. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-437647446
https://github.com/google/deepvariant/issues/104#issuecomment-437647446:234,Testability,test,test,234,"Hi,; can you try the 0.7.1 image? It seems to work for me on CentOS7 now. I was able to run the Exome case study with the following steps:. I used a GCE instance for testing:. ```; gcloud beta compute instances create ""${USER}-centos-test"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image ""centos-7-v20181011"" \; --image-project centos-cloud \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --boot-disk-type ""pd-ssd"" \; --zone ""us-west1-b""; ```. I sshed into the machine, and checked the OS version:. ```; [pichuan@pichuan-centos-test ~]$ cat /etc/redhat-release; CentOS Linux release 7.5.1804 (Core) ; ```. Then:. ```; [pichuan@pichuan-centos-test ~]$ sudo yum install -y docker; [pichuan@pichuan-centos-test ~]$ sudo service docker start; Redirecting to /bin/systemctl start docker.service; [pichuan@pichuan-centos-test ~]$ sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.1; ```. Then, I use a modified script to run WES case study:. ```; root@5189b7fba95b:/# curl https://gist.githubusercontent.com/pichuan/7916a5574c47fd7a2422bad4e5f73860/raw/44fb4377c9ad8b54e195fdb31489d8e9fbc459e2/inside_docker.sh | bash; ```. You can see:; https://gist.githubusercontent.com/pichuan/7916a5574c47fd7a2422bad4e5f73860/raw/44fb4377c9ad8b54e195fdb31489d8e9fbc459e2/inside_docker.sh; for the script that I ran inside the docker. If the 0.7.1 version still doesn't work, feel free to reopen. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-437647446
https://github.com/google/deepvariant/issues/104#issuecomment-437647446:562,Testability,test,test,562,"Hi,; can you try the 0.7.1 image? It seems to work for me on CentOS7 now. I was able to run the Exome case study with the following steps:. I used a GCE instance for testing:. ```; gcloud beta compute instances create ""${USER}-centos-test"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image ""centos-7-v20181011"" \; --image-project centos-cloud \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --boot-disk-type ""pd-ssd"" \; --zone ""us-west1-b""; ```. I sshed into the machine, and checked the OS version:. ```; [pichuan@pichuan-centos-test ~]$ cat /etc/redhat-release; CentOS Linux release 7.5.1804 (Core) ; ```. Then:. ```; [pichuan@pichuan-centos-test ~]$ sudo yum install -y docker; [pichuan@pichuan-centos-test ~]$ sudo service docker start; Redirecting to /bin/systemctl start docker.service; [pichuan@pichuan-centos-test ~]$ sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.1; ```. Then, I use a modified script to run WES case study:. ```; root@5189b7fba95b:/# curl https://gist.githubusercontent.com/pichuan/7916a5574c47fd7a2422bad4e5f73860/raw/44fb4377c9ad8b54e195fdb31489d8e9fbc459e2/inside_docker.sh | bash; ```. You can see:; https://gist.githubusercontent.com/pichuan/7916a5574c47fd7a2422bad4e5f73860/raw/44fb4377c9ad8b54e195fdb31489d8e9fbc459e2/inside_docker.sh; for the script that I ran inside the docker. If the 0.7.1 version still doesn't work, feel free to reopen. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-437647446
https://github.com/google/deepvariant/issues/104#issuecomment-437647446:676,Testability,test,test,676,"Hi,; can you try the 0.7.1 image? It seems to work for me on CentOS7 now. I was able to run the Exome case study with the following steps:. I used a GCE instance for testing:. ```; gcloud beta compute instances create ""${USER}-centos-test"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image ""centos-7-v20181011"" \; --image-project centos-cloud \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --boot-disk-type ""pd-ssd"" \; --zone ""us-west1-b""; ```. I sshed into the machine, and checked the OS version:. ```; [pichuan@pichuan-centos-test ~]$ cat /etc/redhat-release; CentOS Linux release 7.5.1804 (Core) ; ```. Then:. ```; [pichuan@pichuan-centos-test ~]$ sudo yum install -y docker; [pichuan@pichuan-centos-test ~]$ sudo service docker start; Redirecting to /bin/systemctl start docker.service; [pichuan@pichuan-centos-test ~]$ sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.1; ```. Then, I use a modified script to run WES case study:. ```; root@5189b7fba95b:/# curl https://gist.githubusercontent.com/pichuan/7916a5574c47fd7a2422bad4e5f73860/raw/44fb4377c9ad8b54e195fdb31489d8e9fbc459e2/inside_docker.sh | bash; ```. You can see:; https://gist.githubusercontent.com/pichuan/7916a5574c47fd7a2422bad4e5f73860/raw/44fb4377c9ad8b54e195fdb31489d8e9fbc459e2/inside_docker.sh; for the script that I ran inside the docker. If the 0.7.1 version still doesn't work, feel free to reopen. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-437647446
https://github.com/google/deepvariant/issues/104#issuecomment-437647446:737,Testability,test,test,737,"Hi,; can you try the 0.7.1 image? It seems to work for me on CentOS7 now. I was able to run the Exome case study with the following steps:. I used a GCE instance for testing:. ```; gcloud beta compute instances create ""${USER}-centos-test"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image ""centos-7-v20181011"" \; --image-project centos-cloud \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --boot-disk-type ""pd-ssd"" \; --zone ""us-west1-b""; ```. I sshed into the machine, and checked the OS version:. ```; [pichuan@pichuan-centos-test ~]$ cat /etc/redhat-release; CentOS Linux release 7.5.1804 (Core) ; ```. Then:. ```; [pichuan@pichuan-centos-test ~]$ sudo yum install -y docker; [pichuan@pichuan-centos-test ~]$ sudo service docker start; Redirecting to /bin/systemctl start docker.service; [pichuan@pichuan-centos-test ~]$ sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.1; ```. Then, I use a modified script to run WES case study:. ```; root@5189b7fba95b:/# curl https://gist.githubusercontent.com/pichuan/7916a5574c47fd7a2422bad4e5f73860/raw/44fb4377c9ad8b54e195fdb31489d8e9fbc459e2/inside_docker.sh | bash; ```. You can see:; https://gist.githubusercontent.com/pichuan/7916a5574c47fd7a2422bad4e5f73860/raw/44fb4377c9ad8b54e195fdb31489d8e9fbc459e2/inside_docker.sh; for the script that I ran inside the docker. If the 0.7.1 version still doesn't work, feel free to reopen. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-437647446
https://github.com/google/deepvariant/issues/104#issuecomment-437647446:849,Testability,test,test,849,"Hi,; can you try the 0.7.1 image? It seems to work for me on CentOS7 now. I was able to run the Exome case study with the following steps:. I used a GCE instance for testing:. ```; gcloud beta compute instances create ""${USER}-centos-test"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image ""centos-7-v20181011"" \; --image-project centos-cloud \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --boot-disk-type ""pd-ssd"" \; --zone ""us-west1-b""; ```. I sshed into the machine, and checked the OS version:. ```; [pichuan@pichuan-centos-test ~]$ cat /etc/redhat-release; CentOS Linux release 7.5.1804 (Core) ; ```. Then:. ```; [pichuan@pichuan-centos-test ~]$ sudo yum install -y docker; [pichuan@pichuan-centos-test ~]$ sudo service docker start; Redirecting to /bin/systemctl start docker.service; [pichuan@pichuan-centos-test ~]$ sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.1; ```. Then, I use a modified script to run WES case study:. ```; root@5189b7fba95b:/# curl https://gist.githubusercontent.com/pichuan/7916a5574c47fd7a2422bad4e5f73860/raw/44fb4377c9ad8b54e195fdb31489d8e9fbc459e2/inside_docker.sh | bash; ```. You can see:; https://gist.githubusercontent.com/pichuan/7916a5574c47fd7a2422bad4e5f73860/raw/44fb4377c9ad8b54e195fdb31489d8e9fbc459e2/inside_docker.sh; for the script that I ran inside the docker. If the 0.7.1 version still doesn't work, feel free to reopen. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-437647446
https://github.com/google/deepvariant/issues/104#issuecomment-438484727:69,Availability,error,error,69,"Thank you - yes, I just tried in 0.7.1 and now am running into a new error. I have python and numpy installed and I am wondering how to make this accessible or if this is a Docker issue?; Thank you in advance,; Best,; ```; File ""/tmp/Bazel.runfiles_uDUZWS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 37, in <module>; import numpy as np; ImportError: No module named numpy; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-438484727
https://github.com/google/deepvariant/issues/104#issuecomment-438484727:100,Deployability,install,installed,100,"Thank you - yes, I just tried in 0.7.1 and now am running into a new error. I have python and numpy installed and I am wondering how to make this accessible or if this is a Docker issue?; Thank you in advance,; Best,; ```; File ""/tmp/Bazel.runfiles_uDUZWS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 37, in <module>; import numpy as np; ImportError: No module named numpy; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-438484727
https://github.com/google/deepvariant/issues/104#issuecomment-438484727:146,Security,access,accessible,146,"Thank you - yes, I just tried in 0.7.1 and now am running into a new error. I have python and numpy installed and I am wondering how to make this accessible or if this is a Docker issue?; Thank you in advance,; Best,; ```; File ""/tmp/Bazel.runfiles_uDUZWS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 37, in <module>; import numpy as np; ImportError: No module named numpy; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-438484727
https://github.com/google/deepvariant/issues/104#issuecomment-438545095:860,Availability,Down,Downloaded,860,"@ksw9 can you post the full sequence of commands? I tried inside the docker, and it seems to be fine:. ```; [pichuan@pichuan-centos-test ~]$ sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.1; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:0.7.1' locally; Trying to pull repository gcr.io/deepvariant-docker/deepvariant ... ; 0.7.1: Pulling from gcr.io/deepvariant-docker/deepvariant; 18d680d61657: Pull complete ; 0addb6fece63: Pull complete ; 78e58219b215: Pull complete ; eb6959a66df2: Pull complete ; 54de1d38bbd7: Pull complete ; d17c3563217d: Pull complete ; ba1bdbdefce9: Pull complete ; 94eba53c4ad9: Pull complete ; 413f494b0501: Pull complete ; 4d89363e7fb4: Pull complete ; e9213d1ccf36: Pull complete ; fb6121657d6b: Pull complete ; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:0.7.1; root@e2fb03e85f9e:/# python -c ""import numpy""; root@e2fb03e85f9e:/# python -c ""import numpy as np""; root@e2fb03e85f9e:/# ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-438545095
https://github.com/google/deepvariant/issues/104#issuecomment-438545095:132,Testability,test,test,132,"@ksw9 can you post the full sequence of commands? I tried inside the docker, and it seems to be fine:. ```; [pichuan@pichuan-centos-test ~]$ sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.1; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:0.7.1' locally; Trying to pull repository gcr.io/deepvariant-docker/deepvariant ... ; 0.7.1: Pulling from gcr.io/deepvariant-docker/deepvariant; 18d680d61657: Pull complete ; 0addb6fece63: Pull complete ; 78e58219b215: Pull complete ; eb6959a66df2: Pull complete ; 54de1d38bbd7: Pull complete ; d17c3563217d: Pull complete ; ba1bdbdefce9: Pull complete ; 94eba53c4ad9: Pull complete ; 413f494b0501: Pull complete ; 4d89363e7fb4: Pull complete ; e9213d1ccf36: Pull complete ; fb6121657d6b: Pull complete ; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:0.7.1; root@e2fb03e85f9e:/# python -c ""import numpy""; root@e2fb03e85f9e:/# python -c ""import numpy as np""; root@e2fb03e85f9e:/# ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-438545095
https://github.com/google/deepvariant/issues/104#issuecomment-439540680:170,Availability,error,error,170,"Hi, ; I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and ; ```; OUTPUT_DIR=quickstart-testdata/; REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta; BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant; docker run \; -v /home/${USER}:/home/${USER} \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --regions ""chr20:10,000,000-10,010,000"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-439540680
https://github.com/google/deepvariant/issues/104#issuecomment-439540680:15,Testability,test,testing,15,"Hi, ; I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and ; ```; OUTPUT_DIR=quickstart-testdata/; REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta; BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant; docker run \; -v /home/${USER}:/home/${USER} \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --regions ""chr20:10,000,000-10,010,000"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-439540680
https://github.com/google/deepvariant/issues/104#issuecomment-439540680:47,Testability,test,test,47,"Hi, ; I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and ; ```; OUTPUT_DIR=quickstart-testdata/; REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta; BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant; docker run \; -v /home/${USER}:/home/${USER} \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --regions ""chr20:10,000,000-10,010,000"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-439540680
https://github.com/google/deepvariant/issues/104#issuecomment-439540680:225,Testability,test,testdata,225,"Hi, ; I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and ; ```; OUTPUT_DIR=quickstart-testdata/; REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta; BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant; docker run \; -v /home/${USER}:/home/${USER} \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --regions ""chr20:10,000,000-10,010,000"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-439540680
https://github.com/google/deepvariant/issues/104#issuecomment-439540680:327,Testability,test,testdata,327,"Hi, ; I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and ; ```; OUTPUT_DIR=quickstart-testdata/; REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta; BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant; docker run \; -v /home/${USER}:/home/${USER} \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --regions ""chr20:10,000,000-10,010,000"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-439540680
https://github.com/google/deepvariant/issues/104#issuecomment-439540680:353,Testability,test,testdata,353,"Hi, ; I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and ; ```; OUTPUT_DIR=quickstart-testdata/; REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta; BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant; docker run \; -v /home/${USER}:/home/${USER} \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --regions ""chr20:10,000,000-10,010,000"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-439540680
https://github.com/google/deepvariant/issues/104#issuecomment-439540680:409,Testability,test,testdata,409,"Hi, ; I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and ; ```; OUTPUT_DIR=quickstart-testdata/; REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta; BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant; docker run \; -v /home/${USER}:/home/${USER} \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --regions ""chr20:10,000,000-10,010,000"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-439540680
https://github.com/google/deepvariant/issues/104#issuecomment-439540680:453,Testability,test,test,453,"Hi, ; I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and ; ```; OUTPUT_DIR=quickstart-testdata/; REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta; BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant; docker run \; -v /home/${USER}:/home/${USER} \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --regions ""chr20:10,000,000-10,010,000"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-439540680
https://github.com/google/deepvariant/issues/104#issuecomment-439540680:829,Testability,test,testdata,829,"Hi, ; I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and ; ```; OUTPUT_DIR=quickstart-testdata/; REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta; BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant; docker run \; -v /home/${USER}:/home/${USER} \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --regions ""chr20:10,000,000-10,010,000"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-439540680
https://github.com/google/deepvariant/issues/104#issuecomment-439585565:56,Deployability,update,updated,56,"@ksw9 ; Hi, the current quickstart needs a small fix (I updated it internally; should come out in the next release):; `/home/${USER}` should be `${HOME}` to be more general.; And, note that in order for docker to access your file system, you do need the `-v` path.; So you probably want something like:. ```; OUTPUT_DIR=${HOME}/quickstart-output; mkdir -p ""${OUTPUT_DIR}""; REF=${HOME}/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta; BAM=${HOME}/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```. and ; `-v ${HOME}:${HOME} `; in the docker command",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-439585565
https://github.com/google/deepvariant/issues/104#issuecomment-439585565:107,Deployability,release,release,107,"@ksw9 ; Hi, the current quickstart needs a small fix (I updated it internally; should come out in the next release):; `/home/${USER}` should be `${HOME}` to be more general.; And, note that in order for docker to access your file system, you do need the `-v` path.; So you probably want something like:. ```; OUTPUT_DIR=${HOME}/quickstart-output; mkdir -p ""${OUTPUT_DIR}""; REF=${HOME}/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta; BAM=${HOME}/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```. and ; `-v ${HOME}:${HOME} `; in the docker command",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-439585565
https://github.com/google/deepvariant/issues/104#issuecomment-439585565:213,Security,access,access,213,"@ksw9 ; Hi, the current quickstart needs a small fix (I updated it internally; should come out in the next release):; `/home/${USER}` should be `${HOME}` to be more general.; And, note that in order for docker to access your file system, you do need the `-v` path.; So you probably want something like:. ```; OUTPUT_DIR=${HOME}/quickstart-output; mkdir -p ""${OUTPUT_DIR}""; REF=${HOME}/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta; BAM=${HOME}/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```. and ; `-v ${HOME}:${HOME} `; in the docker command",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-439585565
https://github.com/google/deepvariant/issues/104#issuecomment-439585565:396,Testability,test,testdata,396,"@ksw9 ; Hi, the current quickstart needs a small fix (I updated it internally; should come out in the next release):; `/home/${USER}` should be `${HOME}` to be more general.; And, note that in order for docker to access your file system, you do need the `-v` path.; So you probably want something like:. ```; OUTPUT_DIR=${HOME}/quickstart-output; mkdir -p ""${OUTPUT_DIR}""; REF=${HOME}/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta; BAM=${HOME}/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```. and ; `-v ${HOME}:${HOME} `; in the docker command",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-439585565
https://github.com/google/deepvariant/issues/104#issuecomment-439585565:460,Testability,test,testdata,460,"@ksw9 ; Hi, the current quickstart needs a small fix (I updated it internally; should come out in the next release):; `/home/${USER}` should be `${HOME}` to be more general.; And, note that in order for docker to access your file system, you do need the `-v` path.; So you probably want something like:. ```; OUTPUT_DIR=${HOME}/quickstart-output; mkdir -p ""${OUTPUT_DIR}""; REF=${HOME}/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta; BAM=${HOME}/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```. and ; `-v ${HOME}:${HOME} `; in the docker command",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-439585565
https://github.com/google/deepvariant/issues/104#issuecomment-443661136:94,Testability,test,testdata,94,"Hi, This does not work for me. I still get ; ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-443661136
https://github.com/google/deepvariant/issues/104#issuecomment-443831937:37,Availability,error,errors,37,Do you mean you're still getting the errors after that? Can you post your commands?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104#issuecomment-443831937
https://github.com/google/deepvariant/issues/105#issuecomment-429911563:156,Testability,log,log,156,"Hi,; we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. ; If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```; I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]; I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]; I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]; ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-429911563
https://github.com/google/deepvariant/issues/105#issuecomment-429911563:227,Testability,log,logs,227,"Hi,; we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. ; If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```; I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]; I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]; I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]; ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-429911563
https://github.com/google/deepvariant/issues/105#issuecomment-429911563:250,Testability,log,log,250,"Hi,; we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. ; If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```; I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]; I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]; I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]; ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-429911563
https://github.com/google/deepvariant/issues/105#issuecomment-429911563:872,Testability,log,logs,872,"Hi,; we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. ; If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```; I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]; I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]; I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]; ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-429911563
https://github.com/google/deepvariant/issues/105#issuecomment-430634265:670,Availability,down,down,670,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430634265
https://github.com/google/deepvariant/issues/105#issuecomment-430634265:390,Energy Efficiency,Monitor,MonitoredSession,390,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430634265
https://github.com/google/deepvariant/issues/105#issuecomment-430634265:586,Energy Efficiency,monitor,monitor,586,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430634265
https://github.com/google/deepvariant/issues/105#issuecomment-430634265:361,Safety,predict,predict,361,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430634265
https://github.com/google/deepvariant/issues/105#issuecomment-430634265:549,Safety,safe,safe,549,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430634265
https://github.com/google/deepvariant/issues/105#issuecomment-430711053:137,Availability,mask,mask,137,"@mclaugsf Thanks for the update! ; 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release.; For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like?; In my run for the WGS casestudy, it converges to something like:. ```; I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]; I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]; I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]; ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430711053
https://github.com/google/deepvariant/issues/105#issuecomment-430711053:25,Deployability,update,update,25,"@mclaugsf Thanks for the update! ; 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release.; For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like?; In my run for the WGS casestudy, it converges to something like:. ```; I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]; I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]; I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]; ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430711053
https://github.com/google/deepvariant/issues/105#issuecomment-430711053:415,Deployability,release,release,415,"@mclaugsf Thanks for the update! ; 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release.; For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like?; In my run for the WGS casestudy, it converges to something like:. ```; I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]; I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]; I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]; ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430711053
https://github.com/google/deepvariant/issues/105#issuecomment-430711053:224,Testability,log,log,224,"@mclaugsf Thanks for the update! ; 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release.; For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like?; In my run for the WGS casestudy, it converges to something like:. ```; I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]; I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]; I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]; ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430711053
https://github.com/google/deepvariant/issues/105#issuecomment-430711053:468,Testability,log,log,468,"@mclaugsf Thanks for the update! ; 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release.; For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like?; In my run for the WGS casestudy, it converges to something like:. ```; I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]; I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]; I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]; ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430711053
https://github.com/google/deepvariant/issues/105#issuecomment-430711053:603,Testability,log,log,603,"@mclaugsf Thanks for the update! ; 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release.; For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like?; In my run for the WGS casestudy, it converges to something like:. ```; I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]; I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]; I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]; ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430711053
https://github.com/google/deepvariant/issues/105#issuecomment-430711053:316,Usability,clear,clear,316,"@mclaugsf Thanks for the update! ; 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release.; For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like?; In my run for the WGS casestudy, it converges to something like:. ```; I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]; I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]; I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]; ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430711053
https://github.com/google/deepvariant/issues/105#issuecomment-430714499:196,Availability,avail,available,196,"It seems like original question is resolved for now.; If there are more questions for `call_variants`, feel free to open another issue.; I think it's also possible to do with whether Intel MKL is available or not on your machine. If your speed reported in the `call_variants` step is much slower, feel free to open another issue and we can discuss there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430714499
https://github.com/google/deepvariant/issues/105#issuecomment-430723173:194,Availability,avail,available,194,"Thanks, just to followup: my `call_variants` step eventually finished after 1d2h31m. The last lines in my call_variants.log file read:. <pre>; 2018-10-17T14:30:33.396510159Z statfs 424901734400 available 4378992640 used 429280727040 total -- interval 10.0000 seconds 0 used; 2018-10-17T14:30:33.786897949Z I1017 14:30:33.786412 140161207068416 call_variants.py:359] Processed 10551585 examples in 329738 batches [0.904 sec per 100]; 2018-10-17T14:30:34.069377504Z I1017 14:30:34.068934 140161207068416 call_variants.py:359] Processed 10551617 examples in 329739 batches [0.904 sec per 100]; 2018-10-17T14:30:34.164880374Z I1017 14:30:34.164383 140161207068416 call_variants.py:359] Processed 10551649 examples in 329740 batches [0.904 sec per 100]; 2018-10-17T14:30:34.166325693Z I1017 14:30:34.166042 140161207068416 call_variants.py:361] Done evaluating variants; </pre>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430723173
https://github.com/google/deepvariant/issues/105#issuecomment-430723173:120,Testability,log,log,120,"Thanks, just to followup: my `call_variants` step eventually finished after 1d2h31m. The last lines in my call_variants.log file read:. <pre>; 2018-10-17T14:30:33.396510159Z statfs 424901734400 available 4378992640 used 429280727040 total -- interval 10.0000 seconds 0 used; 2018-10-17T14:30:33.786897949Z I1017 14:30:33.786412 140161207068416 call_variants.py:359] Processed 10551585 examples in 329738 batches [0.904 sec per 100]; 2018-10-17T14:30:34.069377504Z I1017 14:30:34.068934 140161207068416 call_variants.py:359] Processed 10551617 examples in 329739 batches [0.904 sec per 100]; 2018-10-17T14:30:34.164880374Z I1017 14:30:34.164383 140161207068416 call_variants.py:359] Processed 10551649 examples in 329740 batches [0.904 sec per 100]; 2018-10-17T14:30:34.166325693Z I1017 14:30:34.166042 140161207068416 call_variants.py:361] Done evaluating variants; </pre>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430723173
https://github.com/google/deepvariant/issues/105#issuecomment-430736386:416,Availability,avail,available,416,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430736386
https://github.com/google/deepvariant/issues/105#issuecomment-430736386:864,Energy Efficiency,efficient,efficient,864,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430736386
https://github.com/google/deepvariant/issues/105#issuecomment-430736386:114,Usability,learn,learning,114,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430736386
https://github.com/google/deepvariant/issues/105#issuecomment-430745296:66,Availability,avail,available,66,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430745296
https://github.com/google/deepvariant/issues/105#issuecomment-430745296:1001,Availability,reliab,reliably,1001,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105#issuecomment-430745296
https://github.com/google/deepvariant/issues/106#issuecomment-430047336:17,Availability,error,error,17,"Most likely this error is due to your reference and input BAM/SAM have different contig names. The error you see may happen if for example reference contigs are named ""chr1"", ""chr2"", etc but input input BAM/SAM contigs have names without ""chr"" prefix.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/106#issuecomment-430047336
https://github.com/google/deepvariant/issues/106#issuecomment-430047336:99,Availability,error,error,99,"Most likely this error is due to your reference and input BAM/SAM have different contig names. The error you see may happen if for example reference contigs are named ""chr1"", ""chr2"", etc but input input BAM/SAM contigs have names without ""chr"" prefix.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/106#issuecomment-430047336
https://github.com/google/deepvariant/issues/107#issuecomment-430072409:294,Availability,robust,robust,294,"Since DeepVariant isn't trained on this data, it will depend on how different this new datatype is from our existing training data. For the training data we used, you can see: https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md. DeepVariant is known to be robust even on certain new data types. For example, when DNAnexus published this blog post, DeepVariant wasn't trained on NovaSeq, and yet DeepVariant performed well on NovaSeq data:; https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/. However, we also know that DeepVariant can be significantly more accurate if it has a chance to see the data in its training set. (For example, see: https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html). If you'd like to give it a try and let us know, that will be great, especially if you compare it to other baselines. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/107#issuecomment-430072409
https://github.com/google/deepvariant/issues/107#issuecomment-430072409:549,Deployability,pipeline,pipelines-on-noisy-wgs-data,549,"Since DeepVariant isn't trained on this data, it will depend on how different this new datatype is from our existing training data. For the training data we used, you can see: https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md. DeepVariant is known to be robust even on certain new data types. For example, when DNAnexus published this blog post, DeepVariant wasn't trained on NovaSeq, and yet DeepVariant performed well on NovaSeq data:; https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/. However, we also know that DeepVariant can be significantly more accurate if it has a chance to see the data in its training set. (For example, see: https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html). If you'd like to give it a try and let us know, that will be great, especially if you compare it to other baselines. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/107#issuecomment-430072409
https://github.com/google/deepvariant/issues/107#issuecomment-430072409:54,Integrability,depend,depend,54,"Since DeepVariant isn't trained on this data, it will depend on how different this new datatype is from our existing training data. For the training data we used, you can see: https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md. DeepVariant is known to be robust even on certain new data types. For example, when DNAnexus published this blog post, DeepVariant wasn't trained on NovaSeq, and yet DeepVariant performed well on NovaSeq data:; https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/. However, we also know that DeepVariant can be significantly more accurate if it has a chance to see the data in its training set. (For example, see: https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html). If you'd like to give it a try and let us know, that will be great, especially if you compare it to other baselines. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/107#issuecomment-430072409
https://github.com/google/deepvariant/issues/107#issuecomment-430072409:445,Performance,perform,performed,445,"Since DeepVariant isn't trained on this data, it will depend on how different this new datatype is from our existing training data. For the training data we used, you can see: https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md. DeepVariant is known to be robust even on certain new data types. For example, when DNAnexus published this blog post, DeepVariant wasn't trained on NovaSeq, and yet DeepVariant performed well on NovaSeq data:; https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/. However, we also know that DeepVariant can be significantly more accurate if it has a chance to see the data in its training set. (For example, see: https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html). If you'd like to give it a try and let us know, that will be great, especially if you compare it to other baselines. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/107#issuecomment-430072409
https://github.com/google/deepvariant/issues/107#issuecomment-430072409:530,Performance,perform,performance-of-ngs-pipelines-on-noisy-wgs-data,530,"Since DeepVariant isn't trained on this data, it will depend on how different this new datatype is from our existing training data. For the training data we used, you can see: https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md. DeepVariant is known to be robust even on certain new data types. For example, when DNAnexus published this blog post, DeepVariant wasn't trained on NovaSeq, and yet DeepVariant performed well on NovaSeq data:; https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/. However, we also know that DeepVariant can be significantly more accurate if it has a chance to see the data in its training set. (For example, see: https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html). If you'd like to give it a try and let us know, that will be great, especially if you compare it to other baselines. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/107#issuecomment-430072409
https://github.com/google/deepvariant/issues/109#issuecomment-431125198:150,Safety,predict,prediction,150,@pichuan Can you inspect more closely the convolutions in inception v3 to trace out what is happening? How about building an ensemble model where the prediction runs through multiple learned models by either majority vote or a more fine-grained decision boundary.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109#issuecomment-431125198
https://github.com/google/deepvariant/issues/109#issuecomment-431125198:183,Usability,learn,learned,183,@pichuan Can you inspect more closely the convolutions in inception v3 to trace out what is happening? How about building an ensemble model where the prediction runs through multiple learned models by either majority vote or a more fine-grained decision boundary.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109#issuecomment-431125198
https://github.com/google/deepvariant/issues/109#issuecomment-431172105:691,Deployability,update,update,691,"@pgrosu ; Thanks for the suggestion! We've had the idea of using ensemble model for a while, but haven't really done it because it also means that the inference cost will go up linearly. In this case, we have a few ideas about understanding how confident the model is. ; It will require some effort to investigate. Once we have more understanding of the core issue, we'll try to come up with a way to improve the model. And I'm also hoping that we'll share the finding here later as well. ; This might take a while though. (More than what we'll need to answer usual github issues :)); But this is definitely a priority for us and we'll start very soon. I'll set a reminder for us to give an update later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109#issuecomment-431172105
https://github.com/google/deepvariant/issues/109#issuecomment-431180531:41,Availability,avail,available,41,"@pichuan That sounds reasonable, and I'm available if your team needs extra cycles with investigating this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109#issuecomment-431180531
https://github.com/google/deepvariant/issues/109#issuecomment-436138293:486,Availability,robust,robust,486,"@ssm0808 ; Looking at the VCF file, it actually doesn't look like there are variants nearby. So it's not related to the issue I mentioned before.; If this data is sharable, it'll certainly be helpful if you can share the data (just chrX). An IGV screenshot at that location can also be helpful.; Otherwise, I'm not sure what's a best way that we can help diagnose this issue.; Let me know if you're able to provide more information. If not, I hope that one day our future model will be robust enough for your data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109#issuecomment-436138293
https://github.com/google/deepvariant/issues/109#issuecomment-2107852838:114,Availability,error,error,114,"Hi Andrew,. Thank you for your response. I de-identified the BAM file and wanna upload it here, but I received an error that ""We don’t support that file type."" Is there a way to send it? I appreciate your time and help!. Best,; Feng",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109#issuecomment-2107852838
https://github.com/google/deepvariant/issues/109#issuecomment-2108524366:146,Security,access,access,146,I upload it to Google Drive: https://drive.google.com/file/d/1aZnuOlCpcDhudfJa28XhvUggOLHuJAyv/view?usp=sharing. Please let me know if you cannot access it. Thanks Andrew!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109#issuecomment-2108524366
https://github.com/google/deepvariant/issues/110#issuecomment-432092110:106,Deployability,release,release,106,"Hi,; I found a bug in the visualizing ipynb and fixed it internally already. It will come out in the next release (which I'm hoping will be soon). For now, please update the `channels_to_rgb` function to this:. ```; def channels_to_rgb(channels):; # Reconstruct the original channels; base = channels[0]; qual = np.minimum(channels[1], channels[2]); strand = channels[3]; alpha = np.multiply(; channels[4] / 254.0,; channels[5] / 254.0); return np.multiply(; np.stack([base, qual, strand]),; alpha).astype(np.uint8).transpose([1, 2, 0]); ```. It was actually a pretty obvious mistake. I wish I unit tested my notebook.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110#issuecomment-432092110
https://github.com/google/deepvariant/issues/110#issuecomment-432092110:163,Deployability,update,update,163,"Hi,; I found a bug in the visualizing ipynb and fixed it internally already. It will come out in the next release (which I'm hoping will be soon). For now, please update the `channels_to_rgb` function to this:. ```; def channels_to_rgb(channels):; # Reconstruct the original channels; base = channels[0]; qual = np.minimum(channels[1], channels[2]); strand = channels[3]; alpha = np.multiply(; channels[4] / 254.0,; channels[5] / 254.0); return np.multiply(; np.stack([base, qual, strand]),; alpha).astype(np.uint8).transpose([1, 2, 0]); ```. It was actually a pretty obvious mistake. I wish I unit tested my notebook.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110#issuecomment-432092110
https://github.com/google/deepvariant/issues/110#issuecomment-432092110:599,Testability,test,tested,599,"Hi,; I found a bug in the visualizing ipynb and fixed it internally already. It will come out in the next release (which I'm hoping will be soon). For now, please update the `channels_to_rgb` function to this:. ```; def channels_to_rgb(channels):; # Reconstruct the original channels; base = channels[0]; qual = np.minimum(channels[1], channels[2]); strand = channels[3]; alpha = np.multiply(; channels[4] / 254.0,; channels[5] / 254.0); return np.multiply(; np.stack([base, qual, strand]),; alpha).astype(np.uint8).transpose([1, 2, 0]); ```. It was actually a pretty obvious mistake. I wish I unit tested my notebook.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110#issuecomment-432092110
https://github.com/google/deepvariant/issues/110#issuecomment-432092602:28,Availability,error,error,28,"After taking a look at your error, it might also be a different problem. You might want to see what came out from you `get_int64_list(example, 'label')`. Is it possible that your example don't have a `'label'`? If you created your examples with `calling` mode in `make_examples`, they probably don't have a label, and might have resulted in that error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110#issuecomment-432092602
https://github.com/google/deepvariant/issues/110#issuecomment-432092602:346,Availability,error,error,346,"After taking a look at your error, it might also be a different problem. You might want to see what came out from you `get_int64_list(example, 'label')`. Is it possible that your example don't have a `'label'`? If you created your examples with `calling` mode in `make_examples`, they probably don't have a label, and might have resulted in that error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110#issuecomment-432092602
https://github.com/google/deepvariant/issues/110#issuecomment-432111752:167,Deployability,install,installed,167,"Close :) It's more indicative that `read_tfrecords` is not probably working properly, since `examples` is an empty list to iterate from. So probably an issue with the installed version of Tensorflow.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110#issuecomment-432111752
https://github.com/google/deepvariant/issues/111#issuecomment-432491512:118,Availability,error,error,118,"thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr.; I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service.; W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; Traceback (most recent call last):;   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>;     tf.app.run();   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run;     _sys.exit(main(_sys.argv[:1] + flags_passthrough));   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main;     make_examples_runner(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner;     regions = processing_regions_from_options(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options;     options.min_shared_contigs_basepairs);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"",",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111#issuecomment-432491512
https://github.com/google/deepvariant/issues/111#issuecomment-432491512:829,Performance,perform,performance-testdata,829,"thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr.; I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service.; W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; Traceback (most recent call last):;   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>;     tf.app.run();   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run;     _sys.exit(main(_sys.argv[:1] + flags_passthrough));   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main;     make_examples_runner(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner;     regions = processing_regions_from_options(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options;     options.min_shared_contigs_basepairs);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"",",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111#issuecomment-432491512
https://github.com/google/deepvariant/issues/111#issuecomment-432491512:1003,Performance,perform,performance-testdata,1003,"it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr.; I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service.; W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; Traceback (most recent call last):;   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>;     tf.app.run();   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run;     _sys.exit(main(_sys.argv[:1] + flags_passthrough));   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main;     make_examples_runner(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner;     regions = processing_regions_from_options(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options;     options.min_shared_contigs_basepairs);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 495,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111#issuecomment-432491512
https://github.com/google/deepvariant/issues/111#issuecomment-432491512:282,Safety,Timeout,Timeout,282,"thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr.; I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service.; W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; Traceback (most recent call last):;   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>;     tf.app.run();   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run;     _sys.exit(main(_sys.argv[:1] + flags_passthrough));   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main;     make_examples_runner(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner;     regions = processing_regions_from_options(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options;     options.min_shared_contigs_basepairs);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"",",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111#issuecomment-432491512
https://github.com/google/deepvariant/issues/111#issuecomment-432491512:469,Security,access,accessible,469,"thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr.; I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service.; W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; Traceback (most recent call last):;   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>;     tf.app.run();   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run;     _sys.exit(main(_sys.argv[:1] + flags_passthrough));   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main;     make_examples_runner(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner;     regions = processing_regions_from_options(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options;     options.min_shared_contigs_basepairs);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"",",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111#issuecomment-432491512
https://github.com/google/deepvariant/issues/111#issuecomment-432491512:183,Testability,Log,Logging,183,"thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr.; I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service.; W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; Traceback (most recent call last):;   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>;     tf.app.run();   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run;     _sys.exit(main(_sys.argv[:1] + flags_passthrough));   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main;     make_examples_runner(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner;     regions = processing_regions_from_options(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options;     options.min_shared_contigs_basepairs);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"",",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111#issuecomment-432491512
https://github.com/google/deepvariant/issues/111#issuecomment-432491512:841,Testability,test,testdata,841,"thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr.; I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service.; W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; Traceback (most recent call last):;   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>;     tf.app.run();   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run;     _sys.exit(main(_sys.argv[:1] + flags_passthrough));   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main;     make_examples_runner(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner;     regions = processing_regions_from_options(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options;     options.min_shared_contigs_basepairs);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"",",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111#issuecomment-432491512
https://github.com/google/deepvariant/issues/111#issuecomment-432491512:1015,Testability,test,testdata,1015,"it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr.; I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service.; W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; Traceback (most recent call last):;   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>;     tf.app.run();   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run;     _sys.exit(main(_sys.argv[:1] + flags_passthrough));   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main;     make_examples_runner(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner;     regions = processing_regions_from_options(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options;     options.min_shared_contigs_basepairs);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 495,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111#issuecomment-432491512
https://github.com/google/deepvariant/issues/112#issuecomment-433250645:358,Deployability,configurat,configuration,358,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112#issuecomment-433250645
https://github.com/google/deepvariant/issues/112#issuecomment-433250645:358,Modifiability,config,configuration,358,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112#issuecomment-433250645
https://github.com/google/deepvariant/issues/112#issuecomment-433250645:236,Safety,predict,predictability,236,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112#issuecomment-433250645
https://github.com/google/deepvariant/issues/112#issuecomment-433250645:152,Security,validat,validation,152,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112#issuecomment-433250645
https://github.com/google/deepvariant/issues/112#issuecomment-433250645:573,Security,validat,validated,573,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112#issuecomment-433250645
https://github.com/google/deepvariant/issues/112#issuecomment-433250645:163,Testability,test,tests,163,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112#issuecomment-433250645
https://github.com/google/deepvariant/issues/112#issuecomment-433250645:204,Testability,benchmark,benchmarks,204,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112#issuecomment-433250645
https://github.com/google/deepvariant/issues/112#issuecomment-433250645:483,Testability,test,tests,483,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112#issuecomment-433250645
https://github.com/google/deepvariant/issues/112#issuecomment-433598823:384,Availability,down,down,384,"This was really puzzling me, so I started with a smallest possible reproducible setting and started debugging. . Now I understand why the results are different. . This update will be about the technical explanation of why different number of shards result in different result:. This is because when we're processing reads in a region, we (by default) first do a sampling of the reads down to `max_reads_per_partition` (default=1500) reads. Here is the code snippets that does that:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L842-L844. ```; if self.options.max_reads_per_partition > 0:; reads = utils.reservoir_sample(; reads, self.options.max_reads_per_partition, self.random); ```. This was originally an optimization for speed. A while ago we noticed that some regions in the genome took really long time to create examples. And this is because some regions have a lot of reads piled up.; Because of the way that germline DeepVariant is set up, the final pileup image has only a image height of 100 anyway, so we don't really need to process all the reads. But at this stage we still need to go through the realigner step later, so we decided to sample enough reads here so that the following steps can still use more information, but we cut off at the number of `max_reads_per_partition` (default=1500) reads. The randomization here is done with `self.random`, which is initialize with the same seed at the beginning. Because the seed is the same at the beginning, we're able to generate deterministic and reproducible results if you fix the number of tasks. However, when you run make_examples with different number of tasks, that means each task has its own `self.random`. Even though each of them started with the same seed, when you have different number of tasks(shards), these RandomState ends up progressing differently. This means when you look at a same region in two different runs (for example, I was looking at `20:3208913-3209912` in the exome), the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112#issuecomment-433598823
https://github.com/google/deepvariant/issues/112#issuecomment-433598823:168,Deployability,update,update,168,"This was really puzzling me, so I started with a smallest possible reproducible setting and started debugging. . Now I understand why the results are different. . This update will be about the technical explanation of why different number of shards result in different result:. This is because when we're processing reads in a region, we (by default) first do a sampling of the reads down to `max_reads_per_partition` (default=1500) reads. Here is the code snippets that does that:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L842-L844. ```; if self.options.max_reads_per_partition > 0:; reads = utils.reservoir_sample(; reads, self.options.max_reads_per_partition, self.random); ```. This was originally an optimization for speed. A while ago we noticed that some regions in the genome took really long time to create examples. And this is because some regions have a lot of reads piled up.; Because of the way that germline DeepVariant is set up, the final pileup image has only a image height of 100 anyway, so we don't really need to process all the reads. But at this stage we still need to go through the realigner step later, so we decided to sample enough reads here so that the following steps can still use more information, but we cut off at the number of `max_reads_per_partition` (default=1500) reads. The randomization here is done with `self.random`, which is initialize with the same seed at the beginning. Because the seed is the same at the beginning, we're able to generate deterministic and reproducible results if you fix the number of tasks. However, when you run make_examples with different number of tasks, that means each task has its own `self.random`. Even though each of them started with the same seed, when you have different number of tasks(shards), these RandomState ends up progressing differently. This means when you look at a same region in two different runs (for example, I was looking at `20:3208913-3209912` in the exome), the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112#issuecomment-433598823
https://github.com/google/deepvariant/issues/112#issuecomment-433598823:742,Performance,optimiz,optimization,742,"This was really puzzling me, so I started with a smallest possible reproducible setting and started debugging. . Now I understand why the results are different. . This update will be about the technical explanation of why different number of shards result in different result:. This is because when we're processing reads in a region, we (by default) first do a sampling of the reads down to `max_reads_per_partition` (default=1500) reads. Here is the code snippets that does that:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L842-L844. ```; if self.options.max_reads_per_partition > 0:; reads = utils.reservoir_sample(; reads, self.options.max_reads_per_partition, self.random); ```. This was originally an optimization for speed. A while ago we noticed that some regions in the genome took really long time to create examples. And this is because some regions have a lot of reads piled up.; Because of the way that germline DeepVariant is set up, the final pileup image has only a image height of 100 anyway, so we don't really need to process all the reads. But at this stage we still need to go through the realigner step later, so we decided to sample enough reads here so that the following steps can still use more information, but we cut off at the number of `max_reads_per_partition` (default=1500) reads. The randomization here is done with `self.random`, which is initialize with the same seed at the beginning. Because the seed is the same at the beginning, we're able to generate deterministic and reproducible results if you fix the number of tasks. However, when you run make_examples with different number of tasks, that means each task has its own `self.random`. Even though each of them started with the same seed, when you have different number of tasks(shards), these RandomState ends up progressing differently. This means when you look at a same region in two different runs (for example, I was looking at `20:3208913-3209912` in the exome), the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112#issuecomment-433598823
https://github.com/google/deepvariant/issues/112#issuecomment-442521534:138,Deployability,release,release,138,"Hi @chris-sf ; Thanks again for reporting this issue. We made an internal change to address this issue, which should come out in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112#issuecomment-442521534
https://github.com/google/deepvariant/issues/114#issuecomment-434889612:1497,Performance,perform,performs,1497,"This is a very interesting question, and the answers to it are complex. I assume for the sake of the question that you have FASTQ data from sequencing a single bacterial colony (so this is not a metagenomics question). Let's divide the question between ""can it technically be done"" and ""will the answers be scientifically valid"". To the question ""can it technically be done"", the answer is probably yes. I am not aware that we have specifically attempted this in bacteria. But if you have a FASTA file with a reference for a species and FASTQ reads, you should be able to generate variant calls for it. To the question ""will the answers be scientifically valid"", it is important to note calling variants in bacterial genomes is an area of open research. Using DeepVariant is reasonable, but I don't think you'll able to consider the output of any method (DeepVariant or other) as certain to give you fully correct results on this problem right out of the box. You'll want to use a few methods (use Freebayes and GATK) and compare between them with metrics you can independently validate, then decide what works and doesn't for your use case. One way to do this could be that for a clonal lineage you expect variants to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin used this measure in a similar way to compare DeepVariant and other methods on inbred rice strains from the 3000 Rice Genomes Project. We would be quite interested to receive your feedback on how DeepVariant performs in this use case, as this may help us understand the value of DeepVariant and improve it for the community.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-434889612
https://github.com/google/deepvariant/issues/114#issuecomment-434889612:1078,Security,validat,validate,1078,"This is a very interesting question, and the answers to it are complex. I assume for the sake of the question that you have FASTQ data from sequencing a single bacterial colony (so this is not a metagenomics question). Let's divide the question between ""can it technically be done"" and ""will the answers be scientifically valid"". To the question ""can it technically be done"", the answer is probably yes. I am not aware that we have specifically attempted this in bacteria. But if you have a FASTA file with a reference for a species and FASTQ reads, you should be able to generate variant calls for it. To the question ""will the answers be scientifically valid"", it is important to note calling variants in bacterial genomes is an area of open research. Using DeepVariant is reasonable, but I don't think you'll able to consider the output of any method (DeepVariant or other) as certain to give you fully correct results on this problem right out of the box. You'll want to use a few methods (use Freebayes and GATK) and compare between them with metrics you can independently validate, then decide what works and doesn't for your use case. One way to do this could be that for a clonal lineage you expect variants to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin used this measure in a similar way to compare DeepVariant and other methods on inbred rice strains from the 3000 Rice Genomes Project. We would be quite interested to receive your feedback on how DeepVariant performs in this use case, as this may help us understand the value of DeepVariant and improve it for the community.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-434889612
https://github.com/google/deepvariant/issues/114#issuecomment-434889612:1469,Usability,feedback,feedback,1469,"This is a very interesting question, and the answers to it are complex. I assume for the sake of the question that you have FASTQ data from sequencing a single bacterial colony (so this is not a metagenomics question). Let's divide the question between ""can it technically be done"" and ""will the answers be scientifically valid"". To the question ""can it technically be done"", the answer is probably yes. I am not aware that we have specifically attempted this in bacteria. But if you have a FASTA file with a reference for a species and FASTQ reads, you should be able to generate variant calls for it. To the question ""will the answers be scientifically valid"", it is important to note calling variants in bacterial genomes is an area of open research. Using DeepVariant is reasonable, but I don't think you'll able to consider the output of any method (DeepVariant or other) as certain to give you fully correct results on this problem right out of the box. You'll want to use a few methods (use Freebayes and GATK) and compare between them with metrics you can independently validate, then decide what works and doesn't for your use case. One way to do this could be that for a clonal lineage you expect variants to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin used this measure in a similar way to compare DeepVariant and other methods on inbred rice strains from the 3000 Rice Genomes Project. We would be quite interested to receive your feedback on how DeepVariant performs in this use case, as this may help us understand the value of DeepVariant and improve it for the community.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-434889612
https://github.com/google/deepvariant/issues/114#issuecomment-435084063:398,Availability,error,error,398,"Hi Andrew,; Thanks for answering my questions.It is very informative.; Currently I am using the GATK pipeline to identify any novel strains for a; particular bacteria genus. So far it makes sense, meaning the known mutated; locations on the genome can be found using the pipeline. However, there are; a lot of possible false positives, which could be either potential new; SNPs, or just sequencing error. That's what prompted me to explore; DeepVarient in the first place, to minimize false positives from sequencing; error.; As I understand that GATK has a step to incorporate known variants to; correct for downstream analysis. But bacteria in general do not have those; information. I am wondering do you have any toolbox on this topic?; I'd love to perform some comparisons between different variant detection of; bacteria genomes in the near future. Will keep you posted if I find; anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>; wrote:. > This is a very interesting question, and the answers to it are complex. I; > assume for the sake of the question that you have FASTQ data from; > sequencing a single bacterial colony (so this is not a metagenomics; > question).; >; > Let's divide the question between ""can it technically be done"" and ""will; > the answers be scientifically valid""; >; > To the question ""can it technically be done"", the answer is probably yes.; > I am not aware that we have specifically attempted this in bacteria. But if; > you have a FASTA file with a reference for a species and FASTQ reads, you; > should be able to generate variant calls for it.; >; > To the question ""will the answers be scientifically valid"", it is; > important to note calling variants in bacterial genomes is an area of open; > research. Using DeepVariant is reasonable, but I don't think you'll able to; > consider the output of any method (DeepVariant or other) as certain to give; > you fully correct results on this problem right out of the bo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-435084063
https://github.com/google/deepvariant/issues/114#issuecomment-435084063:518,Availability,error,error,518,"Hi Andrew,; Thanks for answering my questions.It is very informative.; Currently I am using the GATK pipeline to identify any novel strains for a; particular bacteria genus. So far it makes sense, meaning the known mutated; locations on the genome can be found using the pipeline. However, there are; a lot of possible false positives, which could be either potential new; SNPs, or just sequencing error. That's what prompted me to explore; DeepVarient in the first place, to minimize false positives from sequencing; error.; As I understand that GATK has a step to incorporate known variants to; correct for downstream analysis. But bacteria in general do not have those; information. I am wondering do you have any toolbox on this topic?; I'd love to perform some comparisons between different variant detection of; bacteria genomes in the near future. Will keep you posted if I find; anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>; wrote:. > This is a very interesting question, and the answers to it are complex. I; > assume for the sake of the question that you have FASTQ data from; > sequencing a single bacterial colony (so this is not a metagenomics; > question).; >; > Let's divide the question between ""can it technically be done"" and ""will; > the answers be scientifically valid""; >; > To the question ""can it technically be done"", the answer is probably yes.; > I am not aware that we have specifically attempted this in bacteria. But if; > you have a FASTA file with a reference for a species and FASTQ reads, you; > should be able to generate variant calls for it.; >; > To the question ""will the answers be scientifically valid"", it is; > important to note calling variants in bacterial genomes is an area of open; > research. Using DeepVariant is reasonable, but I don't think you'll able to; > consider the output of any method (DeepVariant or other) as certain to give; > you fully correct results on this problem right out of the bo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-435084063
https://github.com/google/deepvariant/issues/114#issuecomment-435084063:609,Availability,down,downstream,609,"Hi Andrew,; Thanks for answering my questions.It is very informative.; Currently I am using the GATK pipeline to identify any novel strains for a; particular bacteria genus. So far it makes sense, meaning the known mutated; locations on the genome can be found using the pipeline. However, there are; a lot of possible false positives, which could be either potential new; SNPs, or just sequencing error. That's what prompted me to explore; DeepVarient in the first place, to minimize false positives from sequencing; error.; As I understand that GATK has a step to incorporate known variants to; correct for downstream analysis. But bacteria in general do not have those; information. I am wondering do you have any toolbox on this topic?; I'd love to perform some comparisons between different variant detection of; bacteria genomes in the near future. Will keep you posted if I find; anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>; wrote:. > This is a very interesting question, and the answers to it are complex. I; > assume for the sake of the question that you have FASTQ data from; > sequencing a single bacterial colony (so this is not a metagenomics; > question).; >; > Let's divide the question between ""can it technically be done"" and ""will; > the answers be scientifically valid""; >; > To the question ""can it technically be done"", the answer is probably yes.; > I am not aware that we have specifically attempted this in bacteria. But if; > you have a FASTA file with a reference for a species and FASTQ reads, you; > should be able to generate variant calls for it.; >; > To the question ""will the answers be scientifically valid"", it is; > important to note calling variants in bacterial genomes is an area of open; > research. Using DeepVariant is reasonable, but I don't think you'll able to; > consider the output of any method (DeepVariant or other) as certain to give; > you fully correct results on this problem right out of the bo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-435084063
https://github.com/google/deepvariant/issues/114#issuecomment-435084063:101,Deployability,pipeline,pipeline,101,"Hi Andrew,; Thanks for answering my questions.It is very informative.; Currently I am using the GATK pipeline to identify any novel strains for a; particular bacteria genus. So far it makes sense, meaning the known mutated; locations on the genome can be found using the pipeline. However, there are; a lot of possible false positives, which could be either potential new; SNPs, or just sequencing error. That's what prompted me to explore; DeepVarient in the first place, to minimize false positives from sequencing; error.; As I understand that GATK has a step to incorporate known variants to; correct for downstream analysis. But bacteria in general do not have those; information. I am wondering do you have any toolbox on this topic?; I'd love to perform some comparisons between different variant detection of; bacteria genomes in the near future. Will keep you posted if I find; anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>; wrote:. > This is a very interesting question, and the answers to it are complex. I; > assume for the sake of the question that you have FASTQ data from; > sequencing a single bacterial colony (so this is not a metagenomics; > question).; >; > Let's divide the question between ""can it technically be done"" and ""will; > the answers be scientifically valid""; >; > To the question ""can it technically be done"", the answer is probably yes.; > I am not aware that we have specifically attempted this in bacteria. But if; > you have a FASTA file with a reference for a species and FASTQ reads, you; > should be able to generate variant calls for it.; >; > To the question ""will the answers be scientifically valid"", it is; > important to note calling variants in bacterial genomes is an area of open; > research. Using DeepVariant is reasonable, but I don't think you'll able to; > consider the output of any method (DeepVariant or other) as certain to give; > you fully correct results on this problem right out of the bo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-435084063
https://github.com/google/deepvariant/issues/114#issuecomment-435084063:271,Deployability,pipeline,pipeline,271,"Hi Andrew,; Thanks for answering my questions.It is very informative.; Currently I am using the GATK pipeline to identify any novel strains for a; particular bacteria genus. So far it makes sense, meaning the known mutated; locations on the genome can be found using the pipeline. However, there are; a lot of possible false positives, which could be either potential new; SNPs, or just sequencing error. That's what prompted me to explore; DeepVarient in the first place, to minimize false positives from sequencing; error.; As I understand that GATK has a step to incorporate known variants to; correct for downstream analysis. But bacteria in general do not have those; information. I am wondering do you have any toolbox on this topic?; I'd love to perform some comparisons between different variant detection of; bacteria genomes in the near future. Will keep you posted if I find; anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>; wrote:. > This is a very interesting question, and the answers to it are complex. I; > assume for the sake of the question that you have FASTQ data from; > sequencing a single bacterial colony (so this is not a metagenomics; > question).; >; > Let's divide the question between ""can it technically be done"" and ""will; > the answers be scientifically valid""; >; > To the question ""can it technically be done"", the answer is probably yes.; > I am not aware that we have specifically attempted this in bacteria. But if; > you have a FASTA file with a reference for a species and FASTQ reads, you; > should be able to generate variant calls for it.; >; > To the question ""will the answers be scientifically valid"", it is; > important to note calling variants in bacterial genomes is an area of open; > research. Using DeepVariant is reasonable, but I don't think you'll able to; > consider the output of any method (DeepVariant or other) as certain to give; > you fully correct results on this problem right out of the bo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-435084063
https://github.com/google/deepvariant/issues/114#issuecomment-435084063:753,Performance,perform,perform,753,"Hi Andrew,; Thanks for answering my questions.It is very informative.; Currently I am using the GATK pipeline to identify any novel strains for a; particular bacteria genus. So far it makes sense, meaning the known mutated; locations on the genome can be found using the pipeline. However, there are; a lot of possible false positives, which could be either potential new; SNPs, or just sequencing error. That's what prompted me to explore; DeepVarient in the first place, to minimize false positives from sequencing; error.; As I understand that GATK has a step to incorporate known variants to; correct for downstream analysis. But bacteria in general do not have those; information. I am wondering do you have any toolbox on this topic?; I'd love to perform some comparisons between different variant detection of; bacteria genomes in the near future. Will keep you posted if I find; anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>; wrote:. > This is a very interesting question, and the answers to it are complex. I; > assume for the sake of the question that you have FASTQ data from; > sequencing a single bacterial colony (so this is not a metagenomics; > question).; >; > Let's divide the question between ""can it technically be done"" and ""will; > the answers be scientifically valid""; >; > To the question ""can it technically be done"", the answer is probably yes.; > I am not aware that we have specifically attempted this in bacteria. But if; > you have a FASTA file with a reference for a species and FASTQ reads, you; > should be able to generate variant calls for it.; >; > To the question ""will the answers be scientifically valid"", it is; > important to note calling variants in bacterial genomes is an area of open; > research. Using DeepVariant is reasonable, but I don't think you'll able to; > consider the output of any method (DeepVariant or other) as certain to give; > you fully correct results on this problem right out of the bo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-435084063
https://github.com/google/deepvariant/issues/114#issuecomment-435084063:2574,Performance,perform,performs,2574,"on, and the answers to it are complex. I; > assume for the sake of the question that you have FASTQ data from; > sequencing a single bacterial colony (so this is not a metagenomics; > question).; >; > Let's divide the question between ""can it technically be done"" and ""will; > the answers be scientifically valid""; >; > To the question ""can it technically be done"", the answer is probably yes.; > I am not aware that we have specifically attempted this in bacteria. But if; > you have a FASTA file with a reference for a species and FASTQ reads, you; > should be able to generate variant calls for it.; >; > To the question ""will the answers be scientifically valid"", it is; > important to note calling variants in bacterial genomes is an area of open; > research. Using DeepVariant is reasonable, but I don't think you'll able to; > consider the output of any method (DeepVariant or other) as certain to give; > you fully correct results on this problem right out of the box. You'll want; > to use a few methods (use Freebayes and GATK) and compare between them with; > metrics you can independently validate, then decide what works and doesn't; > for your use case.; >; > One way to do this could be that for a clonal lineage you expect variants; > to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin; > used this measure in a similar way to compare DeepVariant and other methods; > on inbred rice strains from the 3000 Rice Genomes Project.; >; > We would be quite interested to receive your feedback on how DeepVariant; > performs in this use case, as this may help us understand the value of; > DeepVariant and improve it for the community.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/114#issuecomment-434889612>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AHCP08xLJBFnbpQzdBn9MXeFKOyacKs9ks5uqjzDgaJpZM4YEtb1>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-435084063
https://github.com/google/deepvariant/issues/114#issuecomment-435084063:804,Safety,detect,detection,804,"Hi Andrew,; Thanks for answering my questions.It is very informative.; Currently I am using the GATK pipeline to identify any novel strains for a; particular bacteria genus. So far it makes sense, meaning the known mutated; locations on the genome can be found using the pipeline. However, there are; a lot of possible false positives, which could be either potential new; SNPs, or just sequencing error. That's what prompted me to explore; DeepVarient in the first place, to minimize false positives from sequencing; error.; As I understand that GATK has a step to incorporate known variants to; correct for downstream analysis. But bacteria in general do not have those; information. I am wondering do you have any toolbox on this topic?; I'd love to perform some comparisons between different variant detection of; bacteria genomes in the near future. Will keep you posted if I find; anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>; wrote:. > This is a very interesting question, and the answers to it are complex. I; > assume for the sake of the question that you have FASTQ data from; > sequencing a single bacterial colony (so this is not a metagenomics; > question).; >; > Let's divide the question between ""can it technically be done"" and ""will; > the answers be scientifically valid""; >; > To the question ""can it technically be done"", the answer is probably yes.; > I am not aware that we have specifically attempted this in bacteria. But if; > you have a FASTA file with a reference for a species and FASTQ reads, you; > should be able to generate variant calls for it.; >; > To the question ""will the answers be scientifically valid"", it is; > important to note calling variants in bacterial genomes is an area of open; > research. Using DeepVariant is reasonable, but I don't think you'll able to; > consider the output of any method (DeepVariant or other) as certain to give; > you fully correct results on this problem right out of the bo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-435084063
https://github.com/google/deepvariant/issues/114#issuecomment-435084063:2128,Security,validat,validate,2128,"on, and the answers to it are complex. I; > assume for the sake of the question that you have FASTQ data from; > sequencing a single bacterial colony (so this is not a metagenomics; > question).; >; > Let's divide the question between ""can it technically be done"" and ""will; > the answers be scientifically valid""; >; > To the question ""can it technically be done"", the answer is probably yes.; > I am not aware that we have specifically attempted this in bacteria. But if; > you have a FASTA file with a reference for a species and FASTQ reads, you; > should be able to generate variant calls for it.; >; > To the question ""will the answers be scientifically valid"", it is; > important to note calling variants in bacterial genomes is an area of open; > research. Using DeepVariant is reasonable, but I don't think you'll able to; > consider the output of any method (DeepVariant or other) as certain to give; > you fully correct results on this problem right out of the box. You'll want; > to use a few methods (use Freebayes and GATK) and compare between them with; > metrics you can independently validate, then decide what works and doesn't; > for your use case.; >; > One way to do this could be that for a clonal lineage you expect variants; > to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin; > used this measure in a similar way to compare DeepVariant and other methods; > on inbred rice strains from the 3000 Rice Genomes Project.; >; > We would be quite interested to receive your feedback on how DeepVariant; > performs in this use case, as this may help us understand the value of; > DeepVariant and improve it for the community.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/114#issuecomment-434889612>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AHCP08xLJBFnbpQzdBn9MXeFKOyacKs9ks5uqjzDgaJpZM4YEtb1>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-435084063
https://github.com/google/deepvariant/issues/114#issuecomment-435084063:2543,Usability,feedback,feedback,2543,"on, and the answers to it are complex. I; > assume for the sake of the question that you have FASTQ data from; > sequencing a single bacterial colony (so this is not a metagenomics; > question).; >; > Let's divide the question between ""can it technically be done"" and ""will; > the answers be scientifically valid""; >; > To the question ""can it technically be done"", the answer is probably yes.; > I am not aware that we have specifically attempted this in bacteria. But if; > you have a FASTA file with a reference for a species and FASTQ reads, you; > should be able to generate variant calls for it.; >; > To the question ""will the answers be scientifically valid"", it is; > important to note calling variants in bacterial genomes is an area of open; > research. Using DeepVariant is reasonable, but I don't think you'll able to; > consider the output of any method (DeepVariant or other) as certain to give; > you fully correct results on this problem right out of the box. You'll want; > to use a few methods (use Freebayes and GATK) and compare between them with; > metrics you can independently validate, then decide what works and doesn't; > for your use case.; >; > One way to do this could be that for a clonal lineage you expect variants; > to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin; > used this measure in a similar way to compare DeepVariant and other methods; > on inbred rice strains from the 3000 Rice Genomes Project.; >; > We would be quite interested to receive your feedback on how DeepVariant; > performs in this use case, as this may help us understand the value of; > DeepVariant and improve it for the community.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/114#issuecomment-434889612>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AHCP08xLJBFnbpQzdBn9MXeFKOyacKs9ks5uqjzDgaJpZM4YEtb1>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-435084063
https://github.com/google/deepvariant/issues/114#issuecomment-436379034:361,Availability,avail,available,361,"When you refer to the method GATK has to incorporate known variants for quality control, I believe that you are referring to VQSR (variant quality score recalibrator). You are correct that this requires a set of ground truth information. It seems likely to me that VQSR and similar systems would not behave correctly on bacterial data even if ground truth were available. To understand whether events are false positives or real SNPs, I think you will want to get some orthogonal measure of whether they are accurate. It probably makes sense to look at the reads data for some of these sites in a tool like IGV to get a feel for whether they look reliable or not. One nice thing about DeepVariant is that the quality scores for variants calls are generally normally distributed and predict the human variant quality confidence quite well. One approach you can take is to see what the distribution of DeepVariant quality scores is for the variants you know are real and see whether the new variants have a similar confidence distribution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-436379034
https://github.com/google/deepvariant/issues/114#issuecomment-436379034:647,Availability,reliab,reliable,647,"When you refer to the method GATK has to incorporate known variants for quality control, I believe that you are referring to VQSR (variant quality score recalibrator). You are correct that this requires a set of ground truth information. It seems likely to me that VQSR and similar systems would not behave correctly on bacterial data even if ground truth were available. To understand whether events are false positives or real SNPs, I think you will want to get some orthogonal measure of whether they are accurate. It probably makes sense to look at the reads data for some of these sites in a tool like IGV to get a feel for whether they look reliable or not. One nice thing about DeepVariant is that the quality scores for variants calls are generally normally distributed and predict the human variant quality confidence quite well. One approach you can take is to see what the distribution of DeepVariant quality scores is for the variants you know are real and see whether the new variants have a similar confidence distribution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-436379034
https://github.com/google/deepvariant/issues/114#issuecomment-436379034:782,Safety,predict,predict,782,"When you refer to the method GATK has to incorporate known variants for quality control, I believe that you are referring to VQSR (variant quality score recalibrator). You are correct that this requires a set of ground truth information. It seems likely to me that VQSR and similar systems would not behave correctly on bacterial data even if ground truth were available. To understand whether events are false positives or real SNPs, I think you will want to get some orthogonal measure of whether they are accurate. It probably makes sense to look at the reads data for some of these sites in a tool like IGV to get a feel for whether they look reliable or not. One nice thing about DeepVariant is that the quality scores for variants calls are generally normally distributed and predict the human variant quality confidence quite well. One approach you can take is to see what the distribution of DeepVariant quality scores is for the variants you know are real and see whether the new variants have a similar confidence distribution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-436379034
https://github.com/google/deepvariant/issues/114#issuecomment-437149890:124,Deployability,pipeline,pipeline,124,We are also interested in exploring the possibility of using DeepVariant to call SNPs in bacterial genomes. For our current pipeline we are using BWA to map reads to the reference and then PILON from the Broad to call variants. Specifically we are working on the Tuberculosis genome.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-437149890
https://github.com/google/deepvariant/issues/114#issuecomment-437155211:506,Deployability,pipeline,pipeline,506,"Thanks Michael, I'd love to know what conclusions about how the accuracy; and background noise detection you have on the TB genome. If it is not too; much trouble, can you keep me posted? Or you have some blog/post or paper I; can follow up on?. You guys are totally rocking it!! Very exciting study!!. On Thu, Nov 8, 2018 at 2:47 PM Michael <notifications@github.com> wrote:. > We are also interested in exploring the possibility of using DeepVariant; > to call SNPs in bacterial genomes. For our current pipeline we are using; > BWA to map reads to the reference and then PILON from the Broad to call; > variants. Specifically we are working on the Tuberculosis genome.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/114#issuecomment-437149890>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AHCP00NuLoTAyzg9zh_dKzzgjzby9ytMks5utJhlgaJpZM4YEtb1>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-437155211
https://github.com/google/deepvariant/issues/114#issuecomment-437155211:95,Safety,detect,detection,95,"Thanks Michael, I'd love to know what conclusions about how the accuracy; and background noise detection you have on the TB genome. If it is not too; much trouble, can you keep me posted? Or you have some blog/post or paper I; can follow up on?. You guys are totally rocking it!! Very exciting study!!. On Thu, Nov 8, 2018 at 2:47 PM Michael <notifications@github.com> wrote:. > We are also interested in exploring the possibility of using DeepVariant; > to call SNPs in bacterial genomes. For our current pipeline we are using; > BWA to map reads to the reference and then PILON from the Broad to call; > variants. Specifically we are working on the Tuberculosis genome.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/114#issuecomment-437149890>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AHCP00NuLoTAyzg9zh_dKzzgjzby9ytMks5utJhlgaJpZM4YEtb1>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-437155211
https://github.com/google/deepvariant/issues/114#issuecomment-437181543:214,Deployability,update,updates,214,"I'm happy to see that this forum has sparked potential research collaborations!; I'm going to close this issue for now so it's easier for me to track what issues still need our attention. Feel free to post further updates on this topic if you like. If you have more questions for the DeepVariant team, also feel free to open more issues. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114#issuecomment-437181543
https://github.com/google/deepvariant/issues/115#issuecomment-436861158:164,Usability,learn,learning,164,"Hi @melkerdawy ,; the DeepVariant codebase is currently designed to for DNA data only. The underlying tool and principle (of converting genomic data into a machine learning problem) could be generalized. But the existing tool as is isn't designed or used for RNA-seq data. In another word - it could work, but it will be open-ended research. I'd recommend you looking into how DeepVariant is done, and look into the [Nucleus](https://github.com/google/nucleus) library as well. We just recently announced a pip package for Nucleus. . Feel free to share your experimental results and discuss any issues you've encountered. We'll try our best to answer and discuss with you here. (Closing for now. Feel free to re-open)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115#issuecomment-436861158
https://github.com/google/deepvariant/issues/115#issuecomment-457703903:455,Testability,test,testing,455,"Sort of a follow-up question, on a related application: can DeepVariant take an RNA-seq bam file obtained via GATK's best practices (link [here](https://software.broadinstitute.org/gatk/documentation/article.php?id=3891)), and get a trustworthy output? . A quick check on a specific locus indicates that DeepVariant's WGS applied to RNA-seq retrieves a few more variants than GATK's ""HaplotypeCaller"". Can we trust it? Has anybody done more comprehensive testing of this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115#issuecomment-457703903
https://github.com/google/deepvariant/issues/115#issuecomment-457857605:54,Performance,perform,performance,54,"Hi @AldoCP,. We have not comprehensively investigated performance on RNA-Seq samples yet. Because some components of the RNA-Seq problem more closely resemble variant calling in DNA-Seq exomes, I recommend that you use the exome model instead of the WGS one. A proper investigation would probably involve looking at the variant calls that result from the same sample with both DNA-Seq and RNA-Seq and investigating the concordance of calls. I expect that sort of comparison is probably worth a paper. . In general, we observe that when DeepVariant is applied in other domains beyond its training data, it tends to undercall as opposed to call false positive events, so if you are getting calls in DeepVariant and not HaplotypeCaller, those are worth looking into. If you do so, we would be interested in your feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115#issuecomment-457857605
https://github.com/google/deepvariant/issues/115#issuecomment-457857605:809,Usability,feedback,feedback,809,"Hi @AldoCP,. We have not comprehensively investigated performance on RNA-Seq samples yet. Because some components of the RNA-Seq problem more closely resemble variant calling in DNA-Seq exomes, I recommend that you use the exome model instead of the WGS one. A proper investigation would probably involve looking at the variant calls that result from the same sample with both DNA-Seq and RNA-Seq and investigating the concordance of calls. I expect that sort of comparison is probably worth a paper. . In general, we observe that when DeepVariant is applied in other domains beyond its training data, it tends to undercall as opposed to call false positive events, so if you are getting calls in DeepVariant and not HaplotypeCaller, those are worth looking into. If you do so, we would be interested in your feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115#issuecomment-457857605
https://github.com/google/deepvariant/issues/115#issuecomment-462075143:219,Safety,detect,detected,219,"Thanks for the feedback. I went back to my files and just realized that my previous comment was inaccurate: the locus I analyzed on RNASeq was ""chr20:10,000,000-10,040,000""; the same exonic variant (chr20:10019093) was detected by both GATK and DeepVariant (WGS model) in my sample. As mentioned, I didn't do extensive tests at all (it was just that one locus) -- I'm happy to do further analysis if relevant,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115#issuecomment-462075143
https://github.com/google/deepvariant/issues/115#issuecomment-462075143:319,Testability,test,tests,319,"Thanks for the feedback. I went back to my files and just realized that my previous comment was inaccurate: the locus I analyzed on RNASeq was ""chr20:10,000,000-10,040,000""; the same exonic variant (chr20:10019093) was detected by both GATK and DeepVariant (WGS model) in my sample. As mentioned, I didn't do extensive tests at all (it was just that one locus) -- I'm happy to do further analysis if relevant,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115#issuecomment-462075143
https://github.com/google/deepvariant/issues/115#issuecomment-462075143:15,Usability,feedback,feedback,15,"Thanks for the feedback. I went back to my files and just realized that my previous comment was inaccurate: the locus I analyzed on RNASeq was ""chr20:10,000,000-10,040,000""; the same exonic variant (chr20:10019093) was detected by both GATK and DeepVariant (WGS model) in my sample. As mentioned, I didn't do extensive tests at all (it was just that one locus) -- I'm happy to do further analysis if relevant,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115#issuecomment-462075143
https://github.com/google/deepvariant/issues/115#issuecomment-1281192222:70,Deployability,update,update,70,"@AldoCP @melkerdawy although this was closed some time ago, I have an update related to this question. We have successfully trained and released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). . DeepVariant now has a flag (`--split_skip_reads`) that can be used in conjunction with training or inference with RNA-seq data. The flag is required to efficiently process RNA-seq data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115#issuecomment-1281192222
https://github.com/google/deepvariant/issues/115#issuecomment-1281192222:136,Deployability,release,released,136,"@AldoCP @melkerdawy although this was closed some time ago, I have an update related to this question. We have successfully trained and released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). . DeepVariant now has a flag (`--split_skip_reads`) that can be used in conjunction with training or inference with RNA-seq data. The flag is required to efficiently process RNA-seq data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115#issuecomment-1281192222
https://github.com/google/deepvariant/issues/115#issuecomment-1281192222:430,Energy Efficiency,efficient,efficiently,430,"@AldoCP @melkerdawy although this was closed some time ago, I have an update related to this question. We have successfully trained and released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). . DeepVariant now has a flag (`--split_skip_reads`) that can be used in conjunction with training or inference with RNA-seq data. The flag is required to efficiently process RNA-seq data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115#issuecomment-1281192222
https://github.com/google/deepvariant/issues/116#issuecomment-436856528:309,Availability,error,error,309,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa; samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz; GRCh38_Verily_v1.genome.fa.gz.gzi; GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-436856528
https://github.com/google/deepvariant/issues/116#issuecomment-436856528:1081,Security,access,access,1081,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa; samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz; GRCh38_Verily_v1.genome.fa.gz.gzi; GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-436856528
https://github.com/google/deepvariant/issues/116#issuecomment-436856528:224,Testability,log,log,224,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa; samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz; GRCh38_Verily_v1.genome.fa.gz.gzi; GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-436856528
https://github.com/google/deepvariant/issues/116#issuecomment-436856528:261,Testability,log,log,261,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa; samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz; GRCh38_Verily_v1.genome.fa.gz.gzi; GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-436856528
https://github.com/google/deepvariant/issues/116#issuecomment-436856528:387,Testability,log,logs,387,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa; samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz; GRCh38_Verily_v1.genome.fa.gz.gzi; GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-436856528
https://github.com/google/deepvariant/issues/116#issuecomment-436856528:443,Testability,log,log,443,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa; samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz; GRCh38_Verily_v1.genome.fa.gz.gzi; GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-436856528
https://github.com/google/deepvariant/issues/116#issuecomment-436856528:467,Testability,log,log,467,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa; samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz; GRCh38_Verily_v1.genome.fa.gz.gzi; GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-436856528
https://github.com/google/deepvariant/issues/116#issuecomment-437006790:147,Deployability,configurat,configuration,147,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:; ```; #!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=valis-194104; OUTPUT_BUCKET=gs://canis/CNR-data; STAGING_FOLDER_NAME=deep_variant_files; OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf; # Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-b \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://canis/CNR-data/CDS-canonical.bed \; --bam gs://canis/CNR-data/TLE_a_001.bam \; --bai gs://canis/CNR-data/TLE_a_001.bam.bai \; --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \; --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \; --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones us-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437006790
https://github.com/google/deepvariant/issues/116#issuecomment-437006790:1234,Deployability,pipeline,pipeline,1234,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:; ```; #!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=valis-194104; OUTPUT_BUCKET=gs://canis/CNR-data; STAGING_FOLDER_NAME=deep_variant_files; OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf; # Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-b \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://canis/CNR-data/CDS-canonical.bed \; --bam gs://canis/CNR-data/TLE_a_001.bam \; --bai gs://canis/CNR-data/TLE_a_001.bam.bai \; --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \; --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \; --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones us-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437006790
https://github.com/google/deepvariant/issues/116#issuecomment-437006790:1267,Deployability,pipeline,pipelines,1267,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:; ```; #!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=valis-194104; OUTPUT_BUCKET=gs://canis/CNR-data; STAGING_FOLDER_NAME=deep_variant_files; OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf; # Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-b \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://canis/CNR-data/CDS-canonical.bed \; --bam gs://canis/CNR-data/TLE_a_001.bam \; --bai gs://canis/CNR-data/TLE_a_001.bam.bai \; --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \; --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \; --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones us-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437006790
https://github.com/google/deepvariant/issues/116#issuecomment-437006790:147,Modifiability,config,configuration,147,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:; ```; #!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=valis-194104; OUTPUT_BUCKET=gs://canis/CNR-data; STAGING_FOLDER_NAME=deep_variant_files; OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf; # Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-b \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://canis/CNR-data/CDS-canonical.bed \; --bam gs://canis/CNR-data/TLE_a_001.bam \; --bai gs://canis/CNR-data/TLE_a_001.bam.bai \; --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \; --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \; --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones us-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437006790
https://github.com/google/deepvariant/issues/116#issuecomment-437006790:1392,Testability,log,logging,1392,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:; ```; #!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=valis-194104; OUTPUT_BUCKET=gs://canis/CNR-data; STAGING_FOLDER_NAME=deep_variant_files; OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf; # Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-b \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://canis/CNR-data/CDS-canonical.bed \; --bam gs://canis/CNR-data/TLE_a_001.bam \; --bai gs://canis/CNR-data/TLE_a_001.bam.bai \; --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \; --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \; --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones us-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437006790
https://github.com/google/deepvariant/issues/116#issuecomment-437006790:1476,Testability,log,log,1476,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:; ```; #!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=valis-194104; OUTPUT_BUCKET=gs://canis/CNR-data; STAGING_FOLDER_NAME=deep_variant_files; OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf; # Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-b \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://canis/CNR-data/CDS-canonical.bed \; --bam gs://canis/CNR-data/TLE_a_001.bam \; --bai gs://canis/CNR-data/TLE_a_001.bam.bai \; --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \; --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \; --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones us-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437006790
https://github.com/google/deepvariant/issues/116#issuecomment-437055644:32,Availability,failure,failures,32,"Ran it again, and still hitting failures:; ```; done: true; error:; code: 9; message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; metadata:; '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata; createTime: '2018-11-08T14:27:06.016940Z'; endTime: '2018-11-08T14:30:59.324697Z'; events:; - description: Worker released; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:30:59.324697Z'; - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent; cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; code: FAILED_PRECONDITION; timestamp: '2018-11-08T14:30:58.518326Z'; - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent; actionId: 2; exitStatus: 0; stderr: ''; timestamp: '2018-11-08T14:30:58.416239Z'; - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent; actionId: 2; ipAddress: ''; portMappings: {}; timestamp: '2018-11-08T14:30:55.929647Z'; - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project; valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs:/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644
https://github.com/google/deepvariant/issues/116#issuecomment-437055644:60,Availability,error,error,60,"Ran it again, and still hitting failures:; ```; done: true; error:; code: 9; message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; metadata:; '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata; createTime: '2018-11-08T14:27:06.016940Z'; endTime: '2018-11-08T14:30:59.324697Z'; events:; - description: Worker released; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:30:59.324697Z'; - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent; cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; code: FAILED_PRECONDITION; timestamp: '2018-11-08T14:30:58.518326Z'; - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent; actionId: 2; exitStatus: 0; stderr: ''; timestamp: '2018-11-08T14:30:58.416239Z'; - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent; actionId: 2; ipAddress: ''; portMappings: {}; timestamp: '2018-11-08T14:30:55.929647Z'; - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project; valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs:/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644
https://github.com/google/deepvariant/issues/116#issuecomment-437055644:3958,Availability,error,error,3958,"8_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent; actionId: 1; exitStatus: 1; stderr: |+; /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeEr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644
https://github.com/google/deepvariant/issues/116#issuecomment-437055644:5075,Availability,error,error,5075,"ON); details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent; actionId: 1; exitStatus: 1; stderr: |+; /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'; - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project; valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai; gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai; gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent; actionId: 1; ipAddress:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644
https://github.com/google/deepvariant/issues/116#issuecomment-437055644:347,Deployability,release,released,347,"Ran it again, and still hitting failures:; ```; done: true; error:; code: 9; message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; metadata:; '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata; createTime: '2018-11-08T14:27:06.016940Z'; endTime: '2018-11-08T14:30:59.324697Z'; events:; - description: Worker released; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:30:59.324697Z'; - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent; cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; code: FAILED_PRECONDITION; timestamp: '2018-11-08T14:30:58.518326Z'; - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent; actionId: 2; exitStatus: 0; stderr: ''; timestamp: '2018-11-08T14:30:58.416239Z'; - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent; actionId: 2; ipAddress: ''; portMappings: {}; timestamp: '2018-11-08T14:30:55.929647Z'; - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project; valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs:/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644
https://github.com/google/deepvariant/issues/116#issuecomment-437055644:459,Deployability,pipeline,pipelines-worker-,459,"Ran it again, and still hitting failures:; ```; done: true; error:; code: 9; message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; metadata:; '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata; createTime: '2018-11-08T14:27:06.016940Z'; endTime: '2018-11-08T14:30:59.324697Z'; events:; - description: Worker released; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:30:59.324697Z'; - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent; cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; code: FAILED_PRECONDITION; timestamp: '2018-11-08T14:30:58.518326Z'; - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent; actionId: 2; exitStatus: 0; stderr: ''; timestamp: '2018-11-08T14:30:58.416239Z'; - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent; actionId: 2; ipAddress: ''; portMappings: {}; timestamp: '2018-11-08T14:30:55.929647Z'; - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project; valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs:/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644
https://github.com/google/deepvariant/issues/116#issuecomment-437055644:4053,Deployability,pipeline,pipeline,4053,"8_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent; actionId: 1; exitStatus: 1; stderr: |+; /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeEr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644
https://github.com/google/deepvariant/issues/116#issuecomment-437055644:5170,Deployability,pipeline,pipeline,5170,"ON); details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent; actionId: 1; exitStatus: 1; stderr: |+; /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'; - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project; valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai; gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai; gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent; actionId: 1; ipAddress:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644
https://github.com/google/deepvariant/issues/116#issuecomment-437055644:7229,Deployability,pipeline,pipelines-worker-,7229,"e/cloud-sdk:alpine""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent; imageUri: google/cloud-sdk:alpine; timestamp: '2018-11-08T14:28:12.427407Z'; - description: Started pulling ""google/cloud-sdk:alpine""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent; imageUri: google/cloud-sdk:alpine; timestamp: '2018-11-08T14:28:05.897359Z'; - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent; imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0; timestamp: '2018-11-08T14:28:05.747135Z'; - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent; imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0; timestamp: '2018-11-08T14:27:34.961215Z'; - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7""; assigned in ""us-west1-b""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:27:06.604193Z'; labels: {}; pipeline:; actions:; - commands:; - -c; - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones; us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai; gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai; gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644
https://github.com/google/deepvariant/issues/116#issuecomment-437055644:7409,Deployability,pipeline,pipelines-worker-,7409,"- description: Started pulling ""google/cloud-sdk:alpine""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent; imageUri: google/cloud-sdk:alpine; timestamp: '2018-11-08T14:28:05.897359Z'; - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent; imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0; timestamp: '2018-11-08T14:28:05.747135Z'; - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent; imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0; timestamp: '2018-11-08T14:27:34.961215Z'; - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7""; assigned in ""us-west1-b""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:27:06.604193Z'; labels: {}; pipeline:; actions:; - commands:; - -c; - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones; us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai; gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai; gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse; entrypoint: bash; environment: {}; flags: []; imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0; labels: {}; mounts: []; name: ''; pidNamespace: ''; portMappings: {};",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644
https://github.com/google/deepvariant/issues/116#issuecomment-437055644:7532,Deployability,pipeline,pipeline,7532,"loud-sdk:alpine; timestamp: '2018-11-08T14:28:05.897359Z'; - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent; imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0; timestamp: '2018-11-08T14:28:05.747135Z'; - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent; imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0; timestamp: '2018-11-08T14:27:34.961215Z'; - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7""; assigned in ""us-west1-b""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:27:06.604193Z'; labels: {}; pipeline:; actions:; - commands:; - -c; - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones; us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai; gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai; gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse; entrypoint: bash; environment: {}; flags: []; imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0; labels: {}; mounts: []; name: ''; pidNamespace: ''; portMappings: {}; - commands:; - /bin/sh; - -c; - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log; entrypoint: ''; envi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644
https://github.com/google/deepvariant/issues/116#issuecomment-437055644:77,Integrability,message,message,77,"Ran it again, and still hitting failures:; ```; done: true; error:; code: 9; message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; metadata:; '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata; createTime: '2018-11-08T14:27:06.016940Z'; endTime: '2018-11-08T14:30:59.324697Z'; events:; - description: Worker released; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:30:59.324697Z'; - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent; cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; code: FAILED_PRECONDITION; timestamp: '2018-11-08T14:30:58.518326Z'; - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent; actionId: 2; exitStatus: 0; stderr: ''; timestamp: '2018-11-08T14:30:58.416239Z'; - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent; actionId: 2; ipAddress: ''; portMappings: {}; timestamp: '2018-11-08T14:30:55.929647Z'; - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project; valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs:/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644
https://github.com/google/deepvariant/issues/116#issuecomment-437055644:9252,Safety,timeout,timeout,9252,rAssignedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:27:06.604193Z'; labels: {}; pipeline:; actions:; - commands:; - -c; - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones; us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai; gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai; gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse; entrypoint: bash; environment: {}; flags: []; imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0; labels: {}; mounts: []; name: ''; pidNamespace: ''; portMappings: {}; - commands:; - /bin/sh; - -c; - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log; entrypoint: ''; environment: {}; flags:; - ALWAYS_RUN; imageUri: google/cloud-sdk:alpine; labels: {}; mounts: []; name: ''; pidNamespace: ''; portMappings: {}; environment: {}; resources:; projectId: valis-194104; regions: []; virtualMachine:; accelerators: []; bootDiskSizeGb: 10; bootImage: projects/cos-cloud/global/images/family/cos-stable; cpuPlatform: ''; disks: []; labels: {}; machineType: n1-standard-1; nvidiaDriverVersion: ''; preemptible: false; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/cloud-platform; - https://www.googleapis.com/auth/devstorage.read_write; - https://www.googleapis.com/auth/genomics; zones:; - us-west1-b; timeout: 604800s; startTime: '2018-11-08T14:27:06.604193Z'; name: projects/valis-194104/operations/12097970745380060156; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644
https://github.com/google/deepvariant/issues/116#issuecomment-437055644:946,Testability,log,logs,946,"Ran it again, and still hitting failures:; ```; done: true; error:; code: 9; message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; metadata:; '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata; createTime: '2018-11-08T14:27:06.016940Z'; endTime: '2018-11-08T14:30:59.324697Z'; events:; - description: Worker released; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:30:59.324697Z'; - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent; cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; code: FAILED_PRECONDITION; timestamp: '2018-11-08T14:30:58.518326Z'; - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent; actionId: 2; exitStatus: 0; stderr: ''; timestamp: '2018-11-08T14:30:58.416239Z'; - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent; actionId: 2; ipAddress: ''; portMappings: {}; timestamp: '2018-11-08T14:30:55.929647Z'; - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project; valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs:/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644
https://github.com/google/deepvariant/issues/116#issuecomment-437055644:1025,Testability,log,log,1025,":; ```; done: true; error:; code: 9; message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; metadata:; '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata; createTime: '2018-11-08T14:27:06.016940Z'; endTime: '2018-11-08T14:30:59.324697Z'; events:; - description: Worker released; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:30:59.324697Z'; - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent; cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; code: FAILED_PRECONDITION; timestamp: '2018-11-08T14:30:58.518326Z'; - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent; actionId: 2; exitStatus: 0; stderr: ''; timestamp: '2018-11-08T14:30:58.416239Z'; - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent; actionId: 2; ipAddress: ''; portMappings: {}; timestamp: '2018-11-08T14:30:55.929647Z'; - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project; valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai; gs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644
https://github.com/google/deepvariant/issues/116#issuecomment-437055644:1264,Testability,log,logs,1264,"016940Z'; endTime: '2018-11-08T14:30:59.324697Z'; events:; - description: Worker released; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:30:59.324697Z'; - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent; cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; code: FAILED_PRECONDITION; timestamp: '2018-11-08T14:30:58.518326Z'; - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent; actionId: 2; exitStatus: 0; stderr: ''; timestamp: '2018-11-08T14:30:58.416239Z'; - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent; actionId: 2; ipAddress: ''; portMappings: {}; timestamp: '2018-11-08T14:30:55.929647Z'; - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project; valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai; gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai; gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644
https://github.com/google/deepvariant/issues/116#issuecomment-437055644:1343,Testability,log,log,1343,"etails:; '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:30:59.324697Z'; - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent; cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; code: FAILED_PRECONDITION; timestamp: '2018-11-08T14:30:58.518326Z'; - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent; actionId: 2; exitStatus: 0; stderr: ''; timestamp: '2018-11-08T14:30:58.416239Z'; - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent; actionId: 2; ipAddress: ''; portMappings: {}; timestamp: '2018-11-08T14:30:55.929647Z'; - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project; valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai; gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai; gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.UnexpectedExitStatusE",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644
https://github.com/google/deepvariant/issues/116#issuecomment-437055644:8495,Testability,log,logs,8495,rAssignedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:27:06.604193Z'; labels: {}; pipeline:; actions:; - commands:; - -c; - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones; us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai; gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai; gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse; entrypoint: bash; environment: {}; flags: []; imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0; labels: {}; mounts: []; name: ''; pidNamespace: ''; portMappings: {}; - commands:; - /bin/sh; - -c; - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log; entrypoint: ''; environment: {}; flags:; - ALWAYS_RUN; imageUri: google/cloud-sdk:alpine; labels: {}; mounts: []; name: ''; pidNamespace: ''; portMappings: {}; environment: {}; resources:; projectId: valis-194104; regions: []; virtualMachine:; accelerators: []; bootDiskSizeGb: 10; bootImage: projects/cos-cloud/global/images/family/cos-stable; cpuPlatform: ''; disks: []; labels: {}; machineType: n1-standard-1; nvidiaDriverVersion: ''; preemptible: false; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/cloud-platform; - https://www.googleapis.com/auth/devstorage.read_write; - https://www.googleapis.com/auth/genomics; zones:; - us-west1-b; timeout: 604800s; startTime: '2018-11-08T14:27:06.604193Z'; name: projects/valis-194104/operations/12097970745380060156; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644
https://github.com/google/deepvariant/issues/116#issuecomment-437055644:8574,Testability,log,log,8574,rAssignedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:27:06.604193Z'; labels: {}; pipeline:; actions:; - commands:; - -c; - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones; us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai; gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai; gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse; entrypoint: bash; environment: {}; flags: []; imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0; labels: {}; mounts: []; name: ''; pidNamespace: ''; portMappings: {}; - commands:; - /bin/sh; - -c; - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log; entrypoint: ''; environment: {}; flags:; - ALWAYS_RUN; imageUri: google/cloud-sdk:alpine; labels: {}; mounts: []; name: ''; pidNamespace: ''; portMappings: {}; environment: {}; resources:; projectId: valis-194104; regions: []; virtualMachine:; accelerators: []; bootDiskSizeGb: 10; bootImage: projects/cos-cloud/global/images/family/cos-stable; cpuPlatform: ''; disks: []; labels: {}; machineType: n1-standard-1; nvidiaDriverVersion: ''; preemptible: false; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/cloud-platform; - https://www.googleapis.com/auth/devstorage.read_write; - https://www.googleapis.com/auth/genomics; zones:; - us-west1-b; timeout: 604800s; startTime: '2018-11-08T14:27:06.604193Z'; name: projects/valis-194104/operations/12097970745380060156; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644
https://github.com/google/deepvariant/issues/116#issuecomment-437076116:63,Availability,failure,failure,63,"As @AndrewCarroll said, we need workers log to investigate the failure. Please share workers log (you should be able to find them under `gs://canis/CNR-data/deep_variant_files/logs`).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437076116
https://github.com/google/deepvariant/issues/116#issuecomment-437076116:40,Testability,log,log,40,"As @AndrewCarroll said, we need workers log to investigate the failure. Please share workers log (you should be able to find them under `gs://canis/CNR-data/deep_variant_files/logs`).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437076116
https://github.com/google/deepvariant/issues/116#issuecomment-437076116:93,Testability,log,log,93,"As @AndrewCarroll said, we need workers log to investigate the failure. Please share workers log (you should be able to find them under `gs://canis/CNR-data/deep_variant_files/logs`).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437076116
https://github.com/google/deepvariant/issues/116#issuecomment-437076116:176,Testability,log,logs,176,"As @AndrewCarroll said, we need workers log to investigate the failure. Please share workers log (you should be able to find them under `gs://canis/CNR-data/deep_variant_files/logs`).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437076116
https://github.com/google/deepvariant/issues/116#issuecomment-437081677:66,Testability,log,logs,66,Actually the answer is right in front of you - you don't need the logs. The prize goes to the first one who sees it :),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437081677
https://github.com/google/deepvariant/issues/116#issuecomment-437180541:36,Deployability,pipeline,pipeline,36,"The Docker container for the Google pipeline backend would need to be fixed. Take a look, I can cause an exit status of 1 with `slim` and `alpine`, but I can only get a clean exit from `slim` - basically it's getting stuck with `alpine` and not exiting:. ```; paul$ sudo docker run google/cloud-sdk:slim gsutil -m cp /not/there gs://my-bucket/; CommandException: No URLs matched: /not/there; CommandException: 1 file/object could not be transferred.; paul$ sudo docker run google/cloud-sdk:alpine gsutil -m cp /not/there gs://my-bucket/; CommandException: No URLs matched: /not/there; CommandException: 1 file/object could not be transferred. ```. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437180541
https://github.com/google/deepvariant/issues/116#issuecomment-437187170:28,Testability,log,log,28,"Again, please share workers log so we can help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437187170
https://github.com/google/deepvariant/issues/116#issuecomment-437189603:205,Deployability,update,update,205,"@nmousavi I made a readonly public bucket you can access here:; http://storage.googleapis.com/public-debug. @pgrosu I am not using Docker. I am running the shell script from my local machine. Do I need to update my google-cloud-sdk version to slim? Or are you referring to the DeepVariant container image itself? If so, I am using the Cloud Genomics API -- not hosting my own image of DeepVariant... see:; https://cloud.google.com/genomics/docs/tutorials/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437189603
https://github.com/google/deepvariant/issues/116#issuecomment-437189603:50,Security,access,access,50,"@nmousavi I made a readonly public bucket you can access here:; http://storage.googleapis.com/public-debug. @pgrosu I am not using Docker. I am running the shell script from my local machine. Do I need to update my google-cloud-sdk version to slim? Or are you referring to the DeepVariant container image itself? If so, I am using the Cloud Genomics API -- not hosting my own image of DeepVariant... see:; https://cloud.google.com/genomics/docs/tutorials/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437189603
https://github.com/google/deepvariant/issues/116#issuecomment-437191865:142,Testability,log,log,142,"It has nothing to do with docker image. The problem is with input file (with err `Unrecognized SAM header type`). Pasting make_example worker log:. ```; Unrecognized SAM header type, ignoring:; I1108 21:36:46.455516 140363989006080 genomics_reader.py:213] Reading /input-gcsfused-2/CNR-data/TLE_a_001.bam with NativeSamReader; Traceback (most recent call last):; File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main; make_examples_runner(options); File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner; regions = processing_regions_from_options(options); File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options; options.min_shared_contigs_basepairs); File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs; min_coverage_fraction); File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage; ref_bp, common_bp, coverage, format_contig_matches())); ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437191865
https://github.com/google/deepvariant/issues/116#issuecomment-437207660:188,Availability,error,error,188,"@saliksyed The real issue is that your BAM file does not contain those chromosomes/contigs as based on the reference file. That would probably will fix it. Sorry for the confusion, as the error was giving me a different signal. @nmousavi Not exactly, but I understand what's the trigger, and it probably would be helpful to provide more direct causes directly in the description of the events. The real thing that's happening here is that he doesn't have the right chromosome labels in his BAM files as compared to the reference genome, which misses all of them and triggers the error on the following lines of code:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L556-L561. The `Unrecognized SAM header type, ignoring:` is just the following warning-line of code that doesn't trigger the exit status of 1:. https://github.com/google/deepvariant/blob/aba55537eec832e6cea678349422124ef50680f4/third_party/nucleus/io/sam_reader.cc#L525. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437207660
https://github.com/google/deepvariant/issues/116#issuecomment-437207660:579,Availability,error,error,579,"@saliksyed The real issue is that your BAM file does not contain those chromosomes/contigs as based on the reference file. That would probably will fix it. Sorry for the confusion, as the error was giving me a different signal. @nmousavi Not exactly, but I understand what's the trigger, and it probably would be helpful to provide more direct causes directly in the description of the events. The real thing that's happening here is that he doesn't have the right chromosome labels in his BAM files as compared to the reference genome, which misses all of them and triggers the error on the following lines of code:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L556-L561. The `Unrecognized SAM header type, ignoring:` is just the following warning-line of code that doesn't trigger the exit status of 1:. https://github.com/google/deepvariant/blob/aba55537eec832e6cea678349422124ef50680f4/third_party/nucleus/io/sam_reader.cc#L525. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437207660
https://github.com/google/deepvariant/issues/116#issuecomment-437359605:176,Deployability,pipeline,pipeline,176,Thank you also to @pgrosu for taking a look. I think the next steps for me are:; 1 -- disassemble the BAM file and re-align using BWA-MEM to the Verily Genome; 2 -- Re run the pipeline with the GRCh38 aligned file,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437359605
https://github.com/google/deepvariant/issues/116#issuecomment-437414607:21,Deployability,pipeline,pipeline,21,Why not just run the pipeline using the b37 version of the genome? The gcp runner pipeline should support any reference genome,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437414607
https://github.com/google/deepvariant/issues/116#issuecomment-437414607:82,Deployability,pipeline,pipeline,82,Why not just run the pipeline using the b37 version of the genome? The gcp runner pipeline should support any reference genome,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437414607
https://github.com/google/deepvariant/issues/116#issuecomment-437426660:103,Performance,perform,performance-testdata,103,@saliksyed the example on the Cloud runner page is actually a b37 genome:. ```; --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \; ```. See if that works for you? It should also already come with the corresponding indexed file and ready to go.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437426660
https://github.com/google/deepvariant/issues/116#issuecomment-437426660:115,Testability,test,testdata,115,@saliksyed the example on the Cloud runner page is actually a b37 genome:. ```; --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \; ```. See if that works for you? It should also already come with the corresponding indexed file and ready to go.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437426660
https://github.com/google/deepvariant/issues/116#issuecomment-437596560:320,Deployability,patch,patch,320,"I ran using the reference genome mentioned earlier, but it seems that the reference has the chromosomes named without the ""chr"" prefix, which the BAM file has for the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:; ```; 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader; I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs; 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader; Traceback (most recent call last):; File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main; make_examples_runner(options); File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner; regions = processing_regions_from_options(options); File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options; options.min_shared_contigs_basepairs); File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs; min_coverage_fraction); File ""/mnt/google/.google/tmp/Bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437596560
https://github.com/google/deepvariant/issues/116#issuecomment-437596560:3635,Performance,perform,performance-testdata,3635,"riant/make_examples.py"", line 485, in _ensure_consistent_contigs; min_coverage_fraction); File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage; ref_bp, common_bp, coverage, format_contig_matches())); ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS MISSING, ""22"" is 51304566 bp and IS MISSING, ""X"" is 155270560 bp and IS MISSING, ""Y"" is 59373566 bp and IS MISSING; parallel: This job failed:; mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed; Using mount point: /input-gcsfused-0; Opening GCS connection...; Opening bucket...; Mounting file system...; File system has been successfully mounted.; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437596560
https://github.com/google/deepvariant/issues/116#issuecomment-437596560:219,Safety,detect,detect,219,"I ran using the reference genome mentioned earlier, but it seems that the reference has the chromosomes named without the ""chr"" prefix, which the BAM file has for the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:; ```; 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader; I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs; 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader; Traceback (most recent call last):; File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main; make_examples_runner(options); File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner; regions = processing_regions_from_options(options); File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options; options.min_shared_contigs_basepairs); File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs; min_coverage_fraction); File ""/mnt/google/.google/tmp/Bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437596560
https://github.com/google/deepvariant/issues/116#issuecomment-437596560:3647,Testability,test,testdata,3647,"riant/make_examples.py"", line 485, in _ensure_consistent_contigs; min_coverage_fraction); File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage; ref_bp, common_bp, coverage, format_contig_matches())); ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS MISSING, ""22"" is 51304566 bp and IS MISSING, ""X"" is 155270560 bp and IS MISSING, ""Y"" is 59373566 bp and IS MISSING; parallel: This job failed:; mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed; Using mount point: /input-gcsfused-0; Opening GCS connection...; Opening bucket...; Mounting file system...; File system has been successfully mounted.; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437596560
https://github.com/google/deepvariant/issues/116#issuecomment-437689349:4106,Performance,perform,performance-testdata,4106,"ons_to_include, contig_dict)); File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions; return cls(ranges=from_regions(regions, contig_map=contig_map)); File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__; for i, range_ in enumerate(ranges):; File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions; for elt in reader(region):; File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser; with bed.BedReader(filename) as fin:; File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader; return NativeBedReader(input_path, **kwargs); File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__; self._reader = bed_reader.BedReader.from_file(bed_path, options); ValueError: Not found: Could not open gs://canis/CNR-data/exomes.bed; parallel: This job failed:; mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed; ```. I've added the BED file to the public bucket:; gs://public-debug/exomes.bed",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437689349
https://github.com/google/deepvariant/issues/116#issuecomment-437689349:4118,Testability,test,testdata,4118,"ons_to_include, contig_dict)); File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions; return cls(ranges=from_regions(regions, contig_map=contig_map)); File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__; for i, range_ in enumerate(ranges):; File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions; for elt in reader(region):; File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser; with bed.BedReader(filename) as fin:; File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader; return NativeBedReader(input_path, **kwargs); File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__; self._reader = bed_reader.BedReader.from_file(bed_path, options); ValueError: Not found: Could not open gs://canis/CNR-data/exomes.bed; parallel: This job failed:; mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed; ```. I've added the BED file to the public bucket:; gs://public-debug/exomes.bed",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437689349
https://github.com/google/deepvariant/issues/116#issuecomment-437712479:805,Performance,perform,performance-testdata,805,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file?; If you do:; `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:; `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look?. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437712479
https://github.com/google/deepvariant/issues/116#issuecomment-437712479:37,Security,access,access,37,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file?; If you do:; `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:; `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look?. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437712479
https://github.com/google/deepvariant/issues/116#issuecomment-437712479:231,Security,access,access,231,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file?; If you do:; `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:; `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look?. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437712479
https://github.com/google/deepvariant/issues/116#issuecomment-437712479:428,Testability,log,log,428,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file?; If you do:; `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:; `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look?. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437712479
https://github.com/google/deepvariant/issues/116#issuecomment-437712479:817,Testability,test,testdata,817,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file?; If you do:; `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:; `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look?. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437712479
https://github.com/google/deepvariant/issues/116#issuecomment-437887566:55,Security,access,access,55,@pichuan Thank you for the followup. I definitely have access to the exomes.bed file and the gsutil cat works fine. . I see what you mean re: the `/input-gcsfused-0/CNR-data/` ...it seems like the parameter did not get translated properly -- I noted in the original launch shell script the bam and bai files are also sent using the gs:// format and assume this is the proper means for the bed file,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437887566
https://github.com/google/deepvariant/issues/116#issuecomment-437974705:25,Deployability,pipeline,pipeline,25,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```; I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader; I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437974705
https://github.com/google/deepvariant/issues/116#issuecomment-437974705:134,Deployability,pipeline,pipeline,134,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```; I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader; I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437974705
https://github.com/google/deepvariant/issues/116#issuecomment-437974705:20,Testability,test,test,20,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```; I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader; I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437974705
https://github.com/google/deepvariant/issues/116#issuecomment-437974705:268,Testability,log,log,268,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```; I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader; I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437974705
https://github.com/google/deepvariant/issues/116#issuecomment-437974705:516,Testability,test,test,516,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```; I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader; I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437974705
https://github.com/google/deepvariant/issues/116#issuecomment-437974705:524,Testability,test,test,524,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```; I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader; I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-437974705
https://github.com/google/deepvariant/issues/116#issuecomment-438034821:90,Deployability,pipeline,pipeline,90,"Okay, I will try this with my exomes.bed file. I was already able to successfully run the pipeline with the following params; ```; --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \; --bam gs://canis/CNR-data/TLE_a_001.reheader.bam \; --bai gs://canis/CNR-data/TLE_a_001.reheader.bam.bai \; --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \; ```. It is odd that the bed file needs to lie in a public bucket, while the genomic data does not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-438034821
https://github.com/google/deepvariant/issues/116#issuecomment-438034821:357,Performance,perform,performance-testdata,357,"Okay, I will try this with my exomes.bed file. I was already able to successfully run the pipeline with the following params; ```; --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \; --bam gs://canis/CNR-data/TLE_a_001.reheader.bam \; --bai gs://canis/CNR-data/TLE_a_001.reheader.bam.bai \; --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \; ```. It is odd that the bed file needs to lie in a public bucket, while the genomic data does not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-438034821
https://github.com/google/deepvariant/issues/116#issuecomment-438034821:175,Testability,test,testdata,175,"Okay, I will try this with my exomes.bed file. I was already able to successfully run the pipeline with the following params; ```; --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \; --bam gs://canis/CNR-data/TLE_a_001.reheader.bam \; --bai gs://canis/CNR-data/TLE_a_001.reheader.bam.bai \; --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \; ```. It is odd that the bed file needs to lie in a public bucket, while the genomic data does not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-438034821
https://github.com/google/deepvariant/issues/116#issuecomment-438034821:369,Testability,test,testdata,369,"Okay, I will try this with my exomes.bed file. I was already able to successfully run the pipeline with the following params; ```; --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \; --bam gs://canis/CNR-data/TLE_a_001.reheader.bam \; --bai gs://canis/CNR-data/TLE_a_001.reheader.bam.bai \; --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \; ```. It is odd that the bed file needs to lie in a public bucket, while the genomic data does not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-438034821
https://github.com/google/deepvariant/issues/116#issuecomment-438063018:128,Deployability,pipeline,pipeline,128,I can confirm that placing the exomes.bed file in `gs://public-debug` results in successful reading of the BED file (though the pipeline fails at a latter step due to inconsistent contig names within the BED vs. the reference genome).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-438063018
https://github.com/google/deepvariant/issues/116#issuecomment-438403390:48,Testability,test,testings,48,"Thanks for confirmation. I have done additional testings, and the conclusion is that the underlying htslib used in DeepVariant is the culprit. I have created a issue for my self to fix it (#119). Meanwhile, if it's possible please put your BED file into a public bucket and rerun DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-438403390
https://github.com/google/deepvariant/issues/116#issuecomment-447731903:52,Deployability,release,release,52,"@saliksyed ; Hopefully this is resolved with v0.7.2 release, and with the workaround of putting the BED file in a public bucket.; If you encounter more issues, let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116#issuecomment-447731903
https://github.com/google/deepvariant/issues/117#issuecomment-437086785:91,Availability,checkpoint,checkpoint,91,"From looking at the shapes of the tensors, it seems like you might be using an older model checkpoint?; In an older release, we used to have 7 channels instead of 6. Can you confirm whether you're using the 0.7.0 model checkpoint?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/117#issuecomment-437086785
https://github.com/google/deepvariant/issues/117#issuecomment-437086785:219,Availability,checkpoint,checkpoint,219,"From looking at the shapes of the tensors, it seems like you might be using an older model checkpoint?; In an older release, we used to have 7 channels instead of 6. Can you confirm whether you're using the 0.7.0 model checkpoint?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/117#issuecomment-437086785
https://github.com/google/deepvariant/issues/117#issuecomment-437086785:116,Deployability,release,release,116,"From looking at the shapes of the tensors, it seems like you might be using an older model checkpoint?; In an older release, we used to have 7 channels instead of 6. Can you confirm whether you're using the 0.7.0 model checkpoint?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/117#issuecomment-437086785
https://github.com/google/deepvariant/issues/118#issuecomment-437114999:80,Deployability,Pipeline,Pipeline,80,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118#issuecomment-437114999
https://github.com/google/deepvariant/issues/118#issuecomment-437114999:272,Deployability,update,updated,272,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118#issuecomment-437114999
https://github.com/google/deepvariant/issues/118#issuecomment-437114999:291,Deployability,release,release,291,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118#issuecomment-437114999
https://github.com/google/deepvariant/issues/118#issuecomment-437114999:413,Testability,log,log,413,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118#issuecomment-437114999
https://github.com/google/deepvariant/issues/118#issuecomment-437114999:557,Testability,log,logs,557,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118#issuecomment-437114999
https://github.com/google/deepvariant/issues/118#issuecomment-437461639:119,Testability,log,log,119,"Thanks nmousavi. I have the yaml file only for v0.6.1 and not for v_0.7.0. However, thanks for pointing toward workers log folder. This gives a better idea in debugging. Thanks,; Shruti",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118#issuecomment-437461639
https://github.com/google/deepvariant/issues/118#issuecomment-437467869:25,Testability,log,log,25,"NP, please share workers log so we can help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118#issuecomment-437467869
https://github.com/google/deepvariant/issues/118#issuecomment-437503051:44,Availability,error,error,44,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai; 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader; I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai; 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader; I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**; Traceback (most recent call last):; File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118#issuecomment-437503051
https://github.com/google/deepvariant/issues/118#issuecomment-437503051:414,Availability,Error,Error,414,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai; 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader; I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai; 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader; I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**; Traceback (most recent call last):; File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118#issuecomment-437503051
https://github.com/google/deepvariant/issues/118#issuecomment-437503051:179,Security,access,access,179,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai; 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader; I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai; 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader; I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**; Traceback (most recent call last):; File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118#issuecomment-437503051
https://github.com/google/deepvariant/issues/118#issuecomment-437503051:362,Testability,test,testdata,362,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai; 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader; I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai; 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader; I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**; Traceback (most recent call last):; File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118#issuecomment-437503051
https://github.com/google/deepvariant/issues/118#issuecomment-437503051:519,Testability,log,logs,519,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai; 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader; I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai; 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader; I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**; Traceback (most recent call last):; File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118#issuecomment-437503051
https://github.com/google/deepvariant/issues/118#issuecomment-437503051:1723,Testability,test,test,1723,"d-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai; 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader; I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai; 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader; I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**; Traceback (most recent call last):; File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main; make_examples_runner(options); File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner; regions = processing_regions_from_options(options); File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 993, in processing_regions_from_options",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118#issuecomment-437503051
https://github.com/google/deepvariant/issues/118#issuecomment-437503051:4296,Testability,test,test,4296,"nt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions; for elt in reader(region):; File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser; with bed.BedReader(filename) as fin:; File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader; return NativeBedReader(input_path, **kwargs); File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__; self._reader = bed_reader.BedReader.from_file(bed_path, options); ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed; parallel: This job failed:; Using mount point: /input-gcsfused-48; Opening GCS connection...; Opening bucket...; Mounting file system...; File system has been successfully mounted.; mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed; gs://gbsc-gcp-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118#issuecomment-437503051
https://github.com/google/deepvariant/issues/118#issuecomment-437503051:4997,Testability,test,test,4997,"iles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser; with bed.BedReader(filename) as fin:; File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader; return NativeBedReader(input_path, **kwargs); File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__; self._reader = bed_reader.BedReader.from_file(bed_path, options); ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed; parallel: This job failed:; Using mount point: /input-gcsfused-48; Opening GCS connection...; Opening bucket...; Mounting file system...; File system has been successfully mounted.; mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed; gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test.sh.txt](https://github.com/google/deepvariant/files/2567583/deepvariant_v0.7.0_UDN668131_test.sh.txt). Thanks,; Shruti",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118#issuecomment-437503051
https://github.com/google/deepvariant/issues/118#issuecomment-437503051:5073,Testability,test,test,5073,"iles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser; with bed.BedReader(filename) as fin:; File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader; return NativeBedReader(input_path, **kwargs); File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__; self._reader = bed_reader.BedReader.from_file(bed_path, options); ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed; parallel: This job failed:; Using mount point: /input-gcsfused-48; Opening GCS connection...; Opening bucket...; Mounting file system...; File system has been successfully mounted.; mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed; gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test.sh.txt](https://github.com/google/deepvariant/files/2567583/deepvariant_v0.7.0_UDN668131_test.sh.txt). Thanks,; Shruti",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118#issuecomment-437503051
https://github.com/google/deepvariant/issues/118#issuecomment-437503051:5204,Testability,test,test,5204,"iles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser; with bed.BedReader(filename) as fin:; File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader; return NativeBedReader(input_path, **kwargs); File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__; self._reader = bed_reader.BedReader.from_file(bed_path, options); ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed; parallel: This job failed:; Using mount point: /input-gcsfused-48; Opening GCS connection...; Opening bucket...; Mounting file system...; File system has been successfully mounted.; mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed; gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test.sh.txt](https://github.com/google/deepvariant/files/2567583/deepvariant_v0.7.0_UDN668131_test.sh.txt). Thanks,; Shruti",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118#issuecomment-437503051
https://github.com/google/deepvariant/issues/118#issuecomment-437503051:5288,Testability,test,test,5288,"iles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser; with bed.BedReader(filename) as fin:; File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader; return NativeBedReader(input_path, **kwargs); File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__; self._reader = bed_reader.BedReader.from_file(bed_path, options); ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed; parallel: This job failed:; Using mount point: /input-gcsfused-48; Opening GCS connection...; Opening bucket...; Mounting file system...; File system has been successfully mounted.; mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed; gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test.sh.txt](https://github.com/google/deepvariant/files/2567583/deepvariant_v0.7.0_UDN668131_test.sh.txt). Thanks,; Shruti",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118#issuecomment-437503051
https://github.com/google/deepvariant/issues/118#issuecomment-438037122:62,Deployability,update,update,62,"We are working on this, please check related bug #116 for the update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118#issuecomment-438037122
https://github.com/google/deepvariant/issues/118#issuecomment-438773230:38,Integrability,message,message,38,Thanks Nima. Can you (or an automated message) let me know once this will be fixed.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118#issuecomment-438773230
https://github.com/google/deepvariant/issues/118#issuecomment-447731692:107,Deployability,release,release,107,"Hi @ShrutiMarwaha ; The fix for https://github.com/google/deepvariant/issues/119 is included in the v0.7.2 release.; If you still see any issues, let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118#issuecomment-447731692
https://github.com/google/deepvariant/issues/119#issuecomment-441712357:50,Deployability,release,release,50,"The fix is done, and will be included in the next release. . For the ref: Htslib is not able to read from private gcs bucket and there is a TODO for it. . https://github.com/samtools/htslib/blob/9cd85ab512a74fe42af31fc6321c70203d161ed6/hfile_gcs.c#L75",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/119#issuecomment-441712357
https://github.com/google/deepvariant/issues/119#issuecomment-447731582:33,Deployability,release,release,33,This fixed is included in v0.7.2 release:; https://github.com/google/deepvariant/releases/tag/v0.7.2,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/119#issuecomment-447731582
https://github.com/google/deepvariant/issues/119#issuecomment-447731582:81,Deployability,release,releases,81,This fixed is included in v0.7.2 release:; https://github.com/google/deepvariant/releases/tag/v0.7.2,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/119#issuecomment-447731582
https://github.com/google/deepvariant/issues/120#issuecomment-439711961:31,Testability,log,log,31,"Hi, can you provide the worker log as well?; (See this example : https://github.com/google/deepvariant/issues/118#issuecomment-437114999 )",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120#issuecomment-439711961
https://github.com/google/deepvariant/issues/120#issuecomment-439734780:138,Availability,error,error,138,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. ; It is really helpful!. The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: ; CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! ; Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120#issuecomment-439734780
https://github.com/google/deepvariant/issues/120#issuecomment-439734780:385,Deployability,pipeline,pipeline,385,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. ; It is really helpful!. The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: ; CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! ; Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120#issuecomment-439734780
https://github.com/google/deepvariant/issues/120#issuecomment-439734780:51,Testability,log,log,51,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. ; It is really helpful!. The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: ; CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! ; Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120#issuecomment-439734780
https://github.com/google/deepvariant/issues/120#issuecomment-439734780:98,Testability,log,log,98,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. ; It is really helpful!. The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: ; CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! ; Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120#issuecomment-439734780
https://github.com/google/deepvariant/issues/120#issuecomment-439734780:236,Testability,log,log,236,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. ; It is really helpful!. The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: ; CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! ; Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120#issuecomment-439734780
https://github.com/google/deepvariant/issues/123#issuecomment-441822330:580,Availability,down,down,580,"Hi @DiableJambe, . We use libssw (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library) for our local alignment. This code directly uses emmintrin.h in ssw.c so there's no easy way to remove it for the powerpc architecture. That said, there appears to be a pull request to add PPC64 support (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library/pull/35) but this has been pending for more than a year. We'd be more than happy to update libssw if you can get an implementation in there that doesn't require SSE intrinsics. Let us know if you want to go down that route.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-441822330
https://github.com/google/deepvariant/issues/123#issuecomment-441822330:457,Deployability,update,update,457,"Hi @DiableJambe, . We use libssw (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library) for our local alignment. This code directly uses emmintrin.h in ssw.c so there's no easy way to remove it for the powerpc architecture. That said, there appears to be a pull request to add PPC64 support (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library/pull/35) but this has been pending for more than a year. We'd be more than happy to update libssw if you can get an implementation in there that doesn't require SSE intrinsics. Let us know if you want to go down that route.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-441822330
https://github.com/google/deepvariant/issues/123#issuecomment-441822330:216,Energy Efficiency,power,powerpc,216,"Hi @DiableJambe, . We use libssw (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library) for our local alignment. This code directly uses emmintrin.h in ssw.c so there's no easy way to remove it for the powerpc architecture. That said, there appears to be a pull request to add PPC64 support (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library/pull/35) but this has been pending for more than a year. We'd be more than happy to update libssw if you can get an implementation in there that doesn't require SSE intrinsics. Let us know if you want to go down that route.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-441822330
https://github.com/google/deepvariant/issues/123#issuecomment-441822330:590,Integrability,rout,route,590,"Hi @DiableJambe, . We use libssw (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library) for our local alignment. This code directly uses emmintrin.h in ssw.c so there's no easy way to remove it for the powerpc architecture. That said, there appears to be a pull request to add PPC64 support (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library/pull/35) but this has been pending for more than a year. We'd be more than happy to update libssw if you can get an implementation in there that doesn't require SSE intrinsics. Let us know if you want to go down that route.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-441822330
https://github.com/google/deepvariant/issues/123#issuecomment-441838626:448,Energy Efficiency,Power,Power,448,"Hi @depristo . Thanks for the clarification and the resource!. Right now, my goal is to get the flow working, so I may need to use an Intel machine for the time being. I will need to revisit this later. I wonder whether this affects only make_examples. If so, it may be good to separate the build into two parts, one for make_examples, and one for the rest. I am not sure whether this is worth supporting from your side. If I am constrained to use Power machines for my subsequent runs, I will be able to look into this aspect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-441838626
https://github.com/google/deepvariant/issues/123#issuecomment-442966182:117,Deployability,update,updates,117,"Hi @DiableJambe ,; thanks for reporting the issue. I can keep this bug open for now in case you want to give us more updates. Just note that our team is not actively looking into this right now. (I also receive notification emails on updated GitHub issues even after they're closed. So keeping this open or closed doesn't really matter much in terms of communication)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-442966182
https://github.com/google/deepvariant/issues/123#issuecomment-442966182:234,Deployability,update,updated,234,"Hi @DiableJambe ,; thanks for reporting the issue. I can keep this bug open for now in case you want to give us more updates. Just note that our team is not actively looking into this right now. (I also receive notification emails on updated GitHub issues even after they're closed. So keeping this open or closed doesn't really matter much in terms of communication)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-442966182
https://github.com/google/deepvariant/issues/123#issuecomment-464686381:615,Availability,error,error,615,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc""; cmake 3.13.3; Protobuf 3.6.1 C++ (static build with --enable-static for bazel); bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc; Python 2 and Pip 19.0.2; Protobuf 3.6.1 C++ (uninstall static and build shared); Protobuf 3.6.1 Python (should build and install from source or CLIF will fail); TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f); CLIF; Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```; ================================================================================; (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s; (05:42:50) INFO: 1835 processes: 1835 local.; (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions; //deepvariant:allelecounter_test PASSED in 0.1s; //deepvariant:call_variants_test PASSED in 59.8s; //deepvariant:data_providers_test PASSED in 11.8s; //deepvariant:dv_vcf_constants_test PASSED in 0.5s; //deepvariant:exclude_contigs_test PASSED in 1.6s; //deepvariant:haplotypes_test PASSED in 1.7s. ▽; //deepvariant:modeling_test PASSED in 48.2s; //deepvariant:pileup_image_test PASSED in 1.8s; //deepvariant:postprocess_variants_lib_test PASSED in 0.1s; //deepvariant:postprocess_variants_test PASSED in 4.8s; //deepvariant:resources_test PASSED in 1.8s; //deepvariant:tf_utils_test PASSED in 3.8s; //deepvariant:utils_test PASSED in 0.1s; //deepvariant:variant_caller_test PASSED in 2.4s; //deepvariant:variant_calling_test PASSED in 0.1s; //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s; //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-464686381
https://github.com/google/deepvariant/issues/123#issuecomment-464686381:644,Availability,error,error,644,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc""; cmake 3.13.3; Protobuf 3.6.1 C++ (static build with --enable-static for bazel); bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc; Python 2 and Pip 19.0.2; Protobuf 3.6.1 C++ (uninstall static and build shared); Protobuf 3.6.1 Python (should build and install from source or CLIF will fail); TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f); CLIF; Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```; ================================================================================; (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s; (05:42:50) INFO: 1835 processes: 1835 local.; (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions; //deepvariant:allelecounter_test PASSED in 0.1s; //deepvariant:call_variants_test PASSED in 59.8s; //deepvariant:data_providers_test PASSED in 11.8s; //deepvariant:dv_vcf_constants_test PASSED in 0.5s; //deepvariant:exclude_contigs_test PASSED in 1.6s; //deepvariant:haplotypes_test PASSED in 1.7s. ▽; //deepvariant:modeling_test PASSED in 48.2s; //deepvariant:pileup_image_test PASSED in 1.8s; //deepvariant:postprocess_variants_lib_test PASSED in 0.1s; //deepvariant:postprocess_variants_test PASSED in 4.8s; //deepvariant:resources_test PASSED in 1.8s; //deepvariant:tf_utils_test PASSED in 3.8s; //deepvariant:utils_test PASSED in 0.1s; //deepvariant:variant_caller_test PASSED in 2.4s; //deepvariant:variant_calling_test PASSED in 0.1s; //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s; //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-464686381
https://github.com/google/deepvariant/issues/123#issuecomment-464686381:332,Deployability,Install,Install,332,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc""; cmake 3.13.3; Protobuf 3.6.1 C++ (static build with --enable-static for bazel); bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc; Python 2 and Pip 19.0.2; Protobuf 3.6.1 C++ (uninstall static and build shared); Protobuf 3.6.1 Python (should build and install from source or CLIF will fail); TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f); CLIF; Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```; ================================================================================; (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s; (05:42:50) INFO: 1835 processes: 1835 local.; (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions; //deepvariant:allelecounter_test PASSED in 0.1s; //deepvariant:call_variants_test PASSED in 59.8s; //deepvariant:data_providers_test PASSED in 11.8s; //deepvariant:dv_vcf_constants_test PASSED in 0.5s; //deepvariant:exclude_contigs_test PASSED in 1.6s; //deepvariant:haplotypes_test PASSED in 1.7s. ▽; //deepvariant:modeling_test PASSED in 48.2s; //deepvariant:pileup_image_test PASSED in 1.8s; //deepvariant:postprocess_variants_lib_test PASSED in 0.1s; //deepvariant:postprocess_variants_test PASSED in 4.8s; //deepvariant:resources_test PASSED in 1.8s; //deepvariant:tf_utils_test PASSED in 3.8s; //deepvariant:utils_test PASSED in 0.1s; //deepvariant:variant_caller_test PASSED in 2.4s; //deepvariant:variant_calling_test PASSED in 0.1s; //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s; //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-464686381
https://github.com/google/deepvariant/issues/123#issuecomment-464686381:543,Deployability,install,install,543,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc""; cmake 3.13.3; Protobuf 3.6.1 C++ (static build with --enable-static for bazel); bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc; Python 2 and Pip 19.0.2; Protobuf 3.6.1 C++ (uninstall static and build shared); Protobuf 3.6.1 Python (should build and install from source or CLIF will fail); TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f); CLIF; Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```; ================================================================================; (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s; (05:42:50) INFO: 1835 processes: 1835 local.; (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions; //deepvariant:allelecounter_test PASSED in 0.1s; //deepvariant:call_variants_test PASSED in 59.8s; //deepvariant:data_providers_test PASSED in 11.8s; //deepvariant:dv_vcf_constants_test PASSED in 0.5s; //deepvariant:exclude_contigs_test PASSED in 1.6s; //deepvariant:haplotypes_test PASSED in 1.7s. ▽; //deepvariant:modeling_test PASSED in 48.2s; //deepvariant:pileup_image_test PASSED in 1.8s; //deepvariant:postprocess_variants_lib_test PASSED in 0.1s; //deepvariant:postprocess_variants_test PASSED in 4.8s; //deepvariant:resources_test PASSED in 1.8s; //deepvariant:tf_utils_test PASSED in 3.8s; //deepvariant:utils_test PASSED in 0.1s; //deepvariant:variant_caller_test PASSED in 2.4s; //deepvariant:variant_calling_test PASSED in 0.1s; //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s; //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-464686381
https://github.com/google/deepvariant/issues/123#issuecomment-464686381:766,Deployability,install,install,766,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc""; cmake 3.13.3; Protobuf 3.6.1 C++ (static build with --enable-static for bazel); bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc; Python 2 and Pip 19.0.2; Protobuf 3.6.1 C++ (uninstall static and build shared); Protobuf 3.6.1 Python (should build and install from source or CLIF will fail); TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f); CLIF; Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```; ================================================================================; (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s; (05:42:50) INFO: 1835 processes: 1835 local.; (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions; //deepvariant:allelecounter_test PASSED in 0.1s; //deepvariant:call_variants_test PASSED in 59.8s; //deepvariant:data_providers_test PASSED in 11.8s; //deepvariant:dv_vcf_constants_test PASSED in 0.5s; //deepvariant:exclude_contigs_test PASSED in 1.6s; //deepvariant:haplotypes_test PASSED in 1.7s. ▽; //deepvariant:modeling_test PASSED in 48.2s; //deepvariant:pileup_image_test PASSED in 1.8s; //deepvariant:postprocess_variants_lib_test PASSED in 0.1s; //deepvariant:postprocess_variants_test PASSED in 4.8s; //deepvariant:resources_test PASSED in 1.8s; //deepvariant:tf_utils_test PASSED in 3.8s; //deepvariant:utils_test PASSED in 0.1s; //deepvariant:variant_caller_test PASSED in 2.4s; //deepvariant:variant_calling_test PASSED in 0.1s; //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s; //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-464686381
https://github.com/google/deepvariant/issues/123#issuecomment-464686381:3197,Performance,cache,cache,3197," //deepvariant:tf_utils_test PASSED in 3.8s; //deepvariant:utils_test PASSED in 0.1s; //deepvariant:variant_caller_test PASSED in 2.4s; //deepvariant:variant_calling_test PASSED in 0.1s; //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s; //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s; //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s; //deepvariant/labeler:labeled_examples_to_vcf_test PASSED in 1.9s; //deepvariant/labeler:positional_labeler_test PASSED in 1.8s; //deepvariant/labeler:variant_labeler_test PASSED in 1.8s; //deepvariant/python:allelecounter_wrap_test PASSED in 1.7s; //deepvariant/python:variant_calling_wrap_test PASSED in 1.8s; //deepvariant/realigner:aligner_test PASSED in 1.7s; //deepvariant/realigner:fast_pass_aligner_test PASSED in 0.1s; //deepvariant/realigner:realigner_test PASSED in 3.1s; //deepvariant/realigner:ssw_test PASSED in 0.1s; //deepvariant/realigner:window_selector_test PASSED in 1.8s; //deepvariant/realigner/allele_count_linear:generate_trained_model_test PASSED in 2.7s; //deepvariant/realigner/allele_count_linear:model_evaluation_test PASSED in 4.3s; //deepvariant/realigner/python:debruijn_graph_wrap_test PASSED in 1.8s; //deepvariant/realigner/python:ssw_misc_test PASSED in 0.3s; //deepvariant/realigner/python:ssw_wrap_test PASSED in 0.3s; //deepvariant/vendor:timer_test PASSED in 0.5s; //deepvariant/labeler:haplotype_labeler_test FAILED in 2.6s; /root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant:make_examples_test PASSED in 13.4s; Stats over 2 runs: max = 13.4s, min = 12.8s, avg = 13.1s, dev = 0.3s; //deepvariant:model_eval_test PASSED in 40.9s; Stats over 10 runs: max = 40.9s, min = 2.8s, avg = 7.3s, dev = 11.3s; //deepvariant:model_train_test PASSED in 120.0s; Stats over 10 runs: max = 120.0s, min = 3.0s, avg = 38.6s, dev = 44.3s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-464686381
https://github.com/google/deepvariant/issues/123#issuecomment-464686381:1139,Testability,test,test,1139,"n Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc""; cmake 3.13.3; Protobuf 3.6.1 C++ (static build with --enable-static for bazel); bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc; Python 2 and Pip 19.0.2; Protobuf 3.6.1 C++ (uninstall static and build shared); Protobuf 3.6.1 Python (should build and install from source or CLIF will fail); TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f); CLIF; Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```; ================================================================================; (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s; (05:42:50) INFO: 1835 processes: 1835 local.; (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions; //deepvariant:allelecounter_test PASSED in 0.1s; //deepvariant:call_variants_test PASSED in 59.8s; //deepvariant:data_providers_test PASSED in 11.8s; //deepvariant:dv_vcf_constants_test PASSED in 0.5s; //deepvariant:exclude_contigs_test PASSED in 1.6s; //deepvariant:haplotypes_test PASSED in 1.7s. ▽; //deepvariant:modeling_test PASSED in 48.2s; //deepvariant:pileup_image_test PASSED in 1.8s; //deepvariant:postprocess_variants_lib_test PASSED in 0.1s; //deepvariant:postprocess_variants_test PASSED in 4.8s; //deepvariant:resources_test PASSED in 1.8s; //deepvariant:tf_utils_test PASSED in 3.8s; //deepvariant:utils_test PASSED in 0.1s; //deepvariant:variant_caller_test PASSED in 2.4s; //deepvariant:variant_calling_test PASSED in 0.1s; //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s; //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s; //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s; //deepvariant/labeler:labeled_examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-464686381
https://github.com/google/deepvariant/issues/123#issuecomment-464686381:3304,Testability,test,testlogs,3304," //deepvariant:tf_utils_test PASSED in 3.8s; //deepvariant:utils_test PASSED in 0.1s; //deepvariant:variant_caller_test PASSED in 2.4s; //deepvariant:variant_calling_test PASSED in 0.1s; //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s; //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s; //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s; //deepvariant/labeler:labeled_examples_to_vcf_test PASSED in 1.9s; //deepvariant/labeler:positional_labeler_test PASSED in 1.8s; //deepvariant/labeler:variant_labeler_test PASSED in 1.8s; //deepvariant/python:allelecounter_wrap_test PASSED in 1.7s; //deepvariant/python:variant_calling_wrap_test PASSED in 1.8s; //deepvariant/realigner:aligner_test PASSED in 1.7s; //deepvariant/realigner:fast_pass_aligner_test PASSED in 0.1s; //deepvariant/realigner:realigner_test PASSED in 3.1s; //deepvariant/realigner:ssw_test PASSED in 0.1s; //deepvariant/realigner:window_selector_test PASSED in 1.8s; //deepvariant/realigner/allele_count_linear:generate_trained_model_test PASSED in 2.7s; //deepvariant/realigner/allele_count_linear:model_evaluation_test PASSED in 4.3s; //deepvariant/realigner/python:debruijn_graph_wrap_test PASSED in 1.8s; //deepvariant/realigner/python:ssw_misc_test PASSED in 0.3s; //deepvariant/realigner/python:ssw_wrap_test PASSED in 0.3s; //deepvariant/vendor:timer_test PASSED in 0.5s; //deepvariant/labeler:haplotype_labeler_test FAILED in 2.6s; /root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant:make_examples_test PASSED in 13.4s; Stats over 2 runs: max = 13.4s, min = 12.8s, avg = 13.1s, dev = 0.3s; //deepvariant:model_eval_test PASSED in 40.9s; Stats over 10 runs: max = 40.9s, min = 2.8s, avg = 7.3s, dev = 11.3s; //deepvariant:model_train_test PASSED in 120.0s; Stats over 10 runs: max = 120.0s, min = 3.0s, avg = 38.6s, dev = 44.3s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-464686381
https://github.com/google/deepvariant/issues/123#issuecomment-464686381:3356,Testability,test,test,3356," //deepvariant:tf_utils_test PASSED in 3.8s; //deepvariant:utils_test PASSED in 0.1s; //deepvariant:variant_caller_test PASSED in 2.4s; //deepvariant:variant_calling_test PASSED in 0.1s; //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s; //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s; //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s; //deepvariant/labeler:labeled_examples_to_vcf_test PASSED in 1.9s; //deepvariant/labeler:positional_labeler_test PASSED in 1.8s; //deepvariant/labeler:variant_labeler_test PASSED in 1.8s; //deepvariant/python:allelecounter_wrap_test PASSED in 1.7s; //deepvariant/python:variant_calling_wrap_test PASSED in 1.8s; //deepvariant/realigner:aligner_test PASSED in 1.7s; //deepvariant/realigner:fast_pass_aligner_test PASSED in 0.1s; //deepvariant/realigner:realigner_test PASSED in 3.1s; //deepvariant/realigner:ssw_test PASSED in 0.1s; //deepvariant/realigner:window_selector_test PASSED in 1.8s; //deepvariant/realigner/allele_count_linear:generate_trained_model_test PASSED in 2.7s; //deepvariant/realigner/allele_count_linear:model_evaluation_test PASSED in 4.3s; //deepvariant/realigner/python:debruijn_graph_wrap_test PASSED in 1.8s; //deepvariant/realigner/python:ssw_misc_test PASSED in 0.3s; //deepvariant/realigner/python:ssw_wrap_test PASSED in 0.3s; //deepvariant/vendor:timer_test PASSED in 0.5s; //deepvariant/labeler:haplotype_labeler_test FAILED in 2.6s; /root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant:make_examples_test PASSED in 13.4s; Stats over 2 runs: max = 13.4s, min = 12.8s, avg = 13.1s, dev = 0.3s; //deepvariant:model_eval_test PASSED in 40.9s; Stats over 10 runs: max = 40.9s, min = 2.8s, avg = 7.3s, dev = 11.3s; //deepvariant:model_train_test PASSED in 120.0s; Stats over 10 runs: max = 120.0s, min = 3.0s, avg = 38.6s, dev = 44.3s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-464686381
https://github.com/google/deepvariant/issues/123#issuecomment-464686381:3361,Testability,log,log,3361," //deepvariant:tf_utils_test PASSED in 3.8s; //deepvariant:utils_test PASSED in 0.1s; //deepvariant:variant_caller_test PASSED in 2.4s; //deepvariant:variant_calling_test PASSED in 0.1s; //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s; //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s; //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s; //deepvariant/labeler:labeled_examples_to_vcf_test PASSED in 1.9s; //deepvariant/labeler:positional_labeler_test PASSED in 1.8s; //deepvariant/labeler:variant_labeler_test PASSED in 1.8s; //deepvariant/python:allelecounter_wrap_test PASSED in 1.7s; //deepvariant/python:variant_calling_wrap_test PASSED in 1.8s; //deepvariant/realigner:aligner_test PASSED in 1.7s; //deepvariant/realigner:fast_pass_aligner_test PASSED in 0.1s; //deepvariant/realigner:realigner_test PASSED in 3.1s; //deepvariant/realigner:ssw_test PASSED in 0.1s; //deepvariant/realigner:window_selector_test PASSED in 1.8s; //deepvariant/realigner/allele_count_linear:generate_trained_model_test PASSED in 2.7s; //deepvariant/realigner/allele_count_linear:model_evaluation_test PASSED in 4.3s; //deepvariant/realigner/python:debruijn_graph_wrap_test PASSED in 1.8s; //deepvariant/realigner/python:ssw_misc_test PASSED in 0.3s; //deepvariant/realigner/python:ssw_wrap_test PASSED in 0.3s; //deepvariant/vendor:timer_test PASSED in 0.5s; //deepvariant/labeler:haplotype_labeler_test FAILED in 2.6s; /root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant:make_examples_test PASSED in 13.4s; Stats over 2 runs: max = 13.4s, min = 12.8s, avg = 13.1s, dev = 0.3s; //deepvariant:model_eval_test PASSED in 40.9s; Stats over 10 runs: max = 40.9s, min = 2.8s, avg = 7.3s, dev = 11.3s; //deepvariant:model_train_test PASSED in 120.0s; Stats over 10 runs: max = 120.0s, min = 3.0s, avg = 38.6s, dev = 44.3s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-464686381
https://github.com/google/deepvariant/issues/123#issuecomment-467621007:66,Availability,avail,available,66,"Hey @qili93 . Thanks for letting me know! Is there a docker image available for this build (DeepVariant or prereqs)? If not, I will try to do it myself. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-467621007
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:955,Availability,failure,failure,955,"Hi @DiableJambe ,. Sorry for my late response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash; # Power8 environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment; source /etc/profile; module load at11.0; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash; # download source code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$C",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:975,Availability,down,download,975,"Hi @DiableJambe ,. Sorry for my late response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash; # Power8 environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment; source /etc/profile; module load at11.0; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash; # download source code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$C",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:1044,Availability,down,download,1044," response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash; # Power8 environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment; source /etc/profile; module load at11.0; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash; # download source code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:1494,Availability,down,download,1494,"hon2 and pip environment; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash; # download source code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:1574,Availability,down,download,1574,"me/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash; # download source code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-sour",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:2319,Availability,echo,echo,2319,"//github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash; # download source code; wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip; mkdir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # veri",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:2343,Availability,echo,echo,2343,"//github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash; # download source code; wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip; mkdir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # veri",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:2786,Availability,down,download,2786,"export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash; # download source code; wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip; mkdir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvaria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:2858,Availability,down,download,2858,"10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash; # download source code; wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip; mkdir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:4009,Availability,failover,failovermethod,4009,xport HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:4885,Availability,down,download,4885,"]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:5598,Availability,echo,echo,5598,"ment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:5820,Availability,echo,echo,5820,"). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared librar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:6223,Availability,down,download,6223,"efix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # shar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:6303,Availability,down,download,6303,"; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:6879,Availability,echo,echo,6879,"3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:6903,Availability,echo,echo,6903,"3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:8424,Availability,error,error,8424," needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz; tar xzf boost_1_66_0.tar.gz; cd boost_1_66_0; ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash; # development packages; yum install python-devel python-pip -y. # dependency of numpy 1.14.6; OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6; # verify; python -c ""import numpy"". # dependecy of scipy 1.2.0; OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0; # verify; python -c ""imp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:9623,Availability,down,download,9623,"dependency of numpy 1.14.6; OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6; # verify; python -c ""import numpy"". # dependecy of scipy 1.2.0; OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0; # verify; python -c ""import scipy"". # pip package dependencies; # pip install pip six wheel mock; pip install wheel autograd h5py==2.9.0 enum34; pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code; git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12; cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc; PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \; PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \; TF_NEED_IGNITE=""0"" \; TF_ENABLE_XLA=""0"" \; TF_NEED_OPENCL_SYCL=""0"" \; TF_NEED_ROCM=""0"" \; TF_NEED_MPI=""0"" \; TF_NEED_TENSORRT=""0"" \; TF_NEED_CUDA=""1"" \; TF_CUDA_VERSION=""10.0"" \; CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \; TF_CUDNN_VERSION=""7"" \; CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \; TF_NCCL_VERSION=""2"" \; NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \; NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \; TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \; TF_CUDA_CLANG=""0"" \; GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \; CC_OPT_FLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" \; TF_SET_ANDROID_WORKSPACE=0 \; ./configure. # fix build error; vim /opt/at11.0/include/bits/floatn.h; -------------------------",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:10570,Availability,error,error,10570,"pplications==1.0.6 keras_preprocessing==1.0.5. # download source code; git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12; cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc; PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \; PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \; TF_NEED_IGNITE=""0"" \; TF_ENABLE_XLA=""0"" \; TF_NEED_OPENCL_SYCL=""0"" \; TF_NEED_ROCM=""0"" \; TF_NEED_MPI=""0"" \; TF_NEED_TENSORRT=""0"" \; TF_NEED_CUDA=""1"" \; TF_CUDA_VERSION=""10.0"" \; CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \; TF_CUDNN_VERSION=""7"" \; CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \; TF_NCCL_VERSION=""2"" \; NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \; NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \; TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \; TF_CUDA_CLANG=""0"" \; GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \; CC_OPT_FLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" \; TF_SET_ANDROID_WORKSPACE=0 \; ./configure. # fix build error; vim /opt/at11.0/include/bits/floatn.h; -------------------------------------; #include <features.h>. /* Defined to 1 if the current compiler invocation provides a; floating-point type with the IEEE 754 binary128 format, and this glibc; includes corresponding *f128 interfaces for it. */; #if defined _ARCH_PWR8 && defined __LITTLE_ENDIAN__ && (_CALL_ELF == 2) \; && defined __FLOAT128__; # define __HAVE_FLOAT128 1; #else; # define __HAVE_FLOAT128 0; #endif. /* add the following block of fix tensorflow build error */; #if CUDART_VERSION; #undef __HAVE_FLOAT128; #define __HAVE_FLOAT128 0; #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct; from the default float, double and long double types in this glibc. */; #if __HAVE_FLOAT128; -------------------------------------. # build; bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python""; bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:11087,Availability,error,error,11087,"usr/local/cuda"" \; TF_CUDNN_VERSION=""7"" \; CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \; TF_NCCL_VERSION=""2"" \; NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \; NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \; TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \; TF_CUDA_CLANG=""0"" \; GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \; CC_OPT_FLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" \; TF_SET_ANDROID_WORKSPACE=0 \; ./configure. # fix build error; vim /opt/at11.0/include/bits/floatn.h; -------------------------------------; #include <features.h>. /* Defined to 1 if the current compiler invocation provides a; floating-point type with the IEEE 754 binary128 format, and this glibc; includes corresponding *f128 interfaces for it. */; #if defined _ARCH_PWR8 && defined __LITTLE_ENDIAN__ && (_CALL_ELF == 2) \; && defined __FLOAT128__; # define __HAVE_FLOAT128 1; #else; # define __HAVE_FLOAT128 0; #endif. /* add the following block of fix tensorflow build error */; #if CUDART_VERSION; #undef __HAVE_FLOAT128; #define __HAVE_FLOAT128 0; #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct; from the default float, double and long double types in this glibc. */; #if __HAVE_FLOAT128; -------------------------------------. # build; bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python""; bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package; bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install; pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification; python -c ""import tensorflow as tf; print(tf.__version__)""; ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:12385,Availability,down,download,12385,"BIN_PATH=""$HOMEPATH/inst/bin/python""; bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package; bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install; pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification; python -c ""import tensorflow as tf; print(tf.__version__)""; ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```bash; # Prerequisites; cmake --version #3.5+; protoc --version # 3.2.0+ build from source code for both C++ and Python; pip install virtualenv; pip install pyparsing; yum install subversion; yum install ocaml; pip install 'pyparsing>=2.2.0'; pkg-config --libs python # workable. # download source code; cd $HOMEPATH; git clone https://github.com/google/clif.git; cd clif. # set environment; export INSTALL_DIR=""$HOMEPATH/inst""; export CLIFSRC_DIR=""$HOMEPATH/clif""; export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend""; export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python""; export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages""; export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif; export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip; $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees; mkdir -p $LLVM_DIR; cd $LLVM_DIR; svn co https://llvm.org/svn/llvm-project/llvm/trunk@307315 llvm; cd llvm/tools; svn co https://llvm.org/svn/llvm-project/cfe/trunk@307315 clang; ln -s -f -n $CLIFSRC_DIR/clif clif. # Builds must be done outside of the LLVM tree.; mkdir -p $BUILD_DIR; cd $BUILD_DIR; # Note ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:14671,Availability,echo,echo,14671,"M_TARGETS_TO_BUILD=PowerPC \; -DPYTHON_INCLUDE_DIR=""$HOMEPATH/inst/include/python2.7"" \; -DPYTHON_LIBRARY=""$HOMEPATH/inst/lib/libpython2.7.so"" \; -DPYTHON_EXECUTABLE=""$HOMEPATH/inst/bin/python"" \; ""$LLVM_DIR/llvm""; make -j20 clif-matcher; # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; make -j20 clif_python_utils_proto_util; make -j20 install. ## Get back to the CLIF Python directory and have pip run setup.py.; cd ""$CLIFSRC_DIR""; # Grab the python compiled .proto; cp ""$BUILD_DIR/tools/clif/protos/ast_pb2.py"" clif/protos/; # Grab CLIF generated wrapper implementation for proto_util.; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.cc"" clif/python/utils/; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.h"" clif/python/utils/; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/; # install; export C_INCLUDE_PATH=/home/qilibj/inst/include; export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include; ""$CLIF_PIP"" install .; # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif.""; python setup.py bdist_wheel; # Note: pyclif should be installed into virtualenv; ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl; pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify; python -c ""from clif.python.proto import start"". # link for deepvariant; ln -s /home/qilibj/inst/clif /usr/local/; ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash; # Checkout repository and submodules; git clone https://github.com/skvark/opencv-python.git; cd opencv-python/; # fetch the tags to your local repository; git fetch --all --tags --prune; # check out tag 3.4.5.20; git checkout tags/20; # load submoduel; git submodule update --init --recursive. # Dependency; pip install pyparsing; yum install qt-devel; # Build; python setup.py bdist_wheel. # Insatll; pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:16250,Availability,echo,echo,16250,"com/skvark/opencv-python.git; cd opencv-python/; # fetch the tags to your local repository; git fetch --all --tags --prune; # check out tag 3.4.5.20; git checkout tags/20; # load submoduel; git submodule update --init --recursive. # Dependency; pip install pyparsing; yum install qt-devel; # Build; python setup.py bdist_wheel. # Insatll; pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session; python -c ""import cv2""; ```. ## DV Prerequisite. ```bash; ####################################################################; # misc setup; ####################################################################. # development packages; yum install python2-pkgconfig zip zlib-devel unzip curl -y; # python packages; yum install python-devel python-pip python-wheel -y. ####################################################################; # python packages; ####################################################################. # python 2 required; echo ""$(python --version)""; echo ""$(pip --version)"". # Install python packages; pip install contextlib2; pip install enum34; pip install intervaltree; pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'; # pip install 'scipy==1.0' => skip as installed in TF; pip install 'oauth2client>=4.0.0'; pip install 'crcmod>=1.7'; pip install six; pip install sklearn; pip install pandas; pip install psutil; pip install --upgrade google-api-python-client. ####################################################################; # depend on opencv-python wheel - build from source; ####################################################################; pip install 'tensor2tensor>=1.9.0'. ####################################################################; # depend on - TensorFlow - 1.12 build from source; ####################################################################; pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ###########################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:16278,Availability,echo,echo,16278,"com/skvark/opencv-python.git; cd opencv-python/; # fetch the tags to your local repository; git fetch --all --tags --prune; # check out tag 3.4.5.20; git checkout tags/20; # load submoduel; git submodule update --init --recursive. # Dependency; pip install pyparsing; yum install qt-devel; # Build; python setup.py bdist_wheel. # Insatll; pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session; python -c ""import cv2""; ```. ## DV Prerequisite. ```bash; ####################################################################; # misc setup; ####################################################################. # development packages; yum install python2-pkgconfig zip zlib-devel unzip curl -y; # python packages; yum install python-devel python-pip python-wheel -y. ####################################################################; # python packages; ####################################################################. # python 2 required; echo ""$(python --version)""; echo ""$(pip --version)"". # Install python packages; pip install contextlib2; pip install enum34; pip install intervaltree; pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'; # pip install 'scipy==1.0' => skip as installed in TF; pip install 'oauth2client>=4.0.0'; pip install 'crcmod>=1.7'; pip install six; pip install sklearn; pip install pandas; pip install psutil; pip install --upgrade google-api-python-client. ####################################################################; # depend on opencv-python wheel - build from source; ####################################################################; pip install 'tensor2tensor>=1.9.0'. ####################################################################; # depend on - TensorFlow - 1.12 build from source; ####################################################################; pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ###########################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:19607,Availability,echo,echo,19607,"-char"". # for GPU enabled; # fix ""ImportError: No module named google.protobuf"" by install protobuf from source; bazel clean; bazel shutdown; bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant/... \; --action_env=LD_LIBRARY_PATH --test_env=LD_LIBRARY_PATH \; --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH \; --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH \; --action_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH --test_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH \; --action_env=BAZEL_PYTHON=$BAZEL_PYTHON --test_env=BAZEL_PYTHON=$BAZEL_PYTHON >& output.log 2>&1 &; bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant:gpu_tests --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # for CPU only; bazel test -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant/... # build binary; bazel build -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant:binaries --build_python_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; echo 'Expect a usage message:'; (bazel-bin/deepvariant/call_variants --help || : ) | grep '/call_variants.py:'. bazel build :licenses_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; ```. ## Fix DV Error. ```bash; ################################################################################; # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test; # use lscpu to show the actual CPU number; ################################################################################; python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160; python -c ""import psutil;print(p/sutil.cpu_count; ())"" #160. vim deepvariant/resources.py; --------------------------------; def _get_cpu_count():; """"""Gets the number of physical cores in this machine.; Returns:; int >= 1 if the call to get the cpu_count succeeded, or 0 if not.; """"""; # return psutil.cpu_count(logical=False) or 0 ==> comment; return 20;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:19831,Availability,Error,Error,19831,"est_env=LD_LIBRARY_PATH \; --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH \; --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH \; --action_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH --test_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH \; --action_env=BAZEL_PYTHON=$BAZEL_PYTHON --test_env=BAZEL_PYTHON=$BAZEL_PYTHON >& output.log 2>&1 &; bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant:gpu_tests --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # for CPU only; bazel test -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant/... # build binary; bazel build -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant:binaries --build_python_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; echo 'Expect a usage message:'; (bazel-bin/deepvariant/call_variants --help || : ) | grep '/call_variants.py:'. bazel build :licenses_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; ```. ## Fix DV Error. ```bash; ################################################################################; # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test; # use lscpu to show the actual CPU number; ################################################################################; python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160; python -c ""import psutil;print(p/sutil.cpu_count; ())"" #160. vim deepvariant/resources.py; --------------------------------; def _get_cpu_count():; """"""Gets the number of physical cores in this machine.; Returns:; int >= 1 if the call to get the cpu_count succeeded, or 0 if not.; """"""; # return psutil.cpu_count(logical=False) or 0 ==> comment; return 20; --------------------------------. vim deepvariant/resources_test.py; --------------------------------; def test_metrics_is_ok_when_cpu_count_returns_none(self):; # Some psutil functions, such as cpu_freq(), can return None depending on; # the environme",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:818,Deployability,install,install,818,"Hi @DiableJambe ,. Sorry for my late response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash; # Power8 environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment; source /etc/profile; module load at11.0; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash; # download source code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$C",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:850,Deployability,install,install,850,"Hi @DiableJambe ,. Sorry for my late response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash; # Power8 environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment; source /etc/profile; module load at11.0; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash; # download source code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$C",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:878,Deployability,install,install,878,"Hi @DiableJambe ,. Sorry for my late response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash; # Power8 environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment; source /etc/profile; module load at11.0; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash; # download source code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$C",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:1035,Deployability,release,releases,1035," response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash; # Power8 environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment; source /etc/profile; module load at11.0; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash; # download source code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:1180,Deployability,install,install,1180,"nt. ```bash; # Power8 environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment; source /etc/profile; module load at11.0; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash; # download source code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:1565,Deployability,release,releases,1565,"me/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash; # download source code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-sour",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:1943,Deployability,install,install,1943,"ce code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash; # download source code; wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip; mkdir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # env",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:2205,Deployability,install,install,2205,"ke -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash; # download source code; wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip; mkdir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:2219,Deployability,install,install,2219,"ke -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash; # download source code; wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip; mkdir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:2455,Deployability,install,install-compile-source,2455,"/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash; # download source code; wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip; mkdir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolcha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:2541,Deployability,install,install-compile-source,2541,"rotocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash; # download source code; wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip; mkdir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/adv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:2849,Deployability,release,releases,2849,"10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash; # download source code; wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip; mkdir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:3467,Deployability,install,installation,3467,-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash; # download source code; wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip; mkdir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbi,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:3547,Deployability,install,installation,3547,/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash; # download source code; wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip; mkdir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:3861,Deployability,configurat,configuration,3861,ir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:4165,Deployability,configurat,configuration,4165,"in:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:4187,Deployability,Install,Install,4187,"in. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:4222,Deployability,install,install,4222,"in. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:4268,Deployability,install,install,4268," ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:4312,Deployability,install,install,4312,"ATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # se",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:4355,Deployability,install,install,4355,"```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:4594,Deployability,install,install,4594,"tp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:5308,Deployability,install,install,5308,"nstall advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:5779,Deployability,install,install,5779,"). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared librar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:5789,Deployability,upgrade,upgrade,5789,"). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared librar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:5849,Deployability,install,install,5849,"). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared librar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:5921,Deployability,install,install,5921,"n.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)"";",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:5931,Deployability,upgrade,upgrade,5931,"n.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)"";",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:6294,Deployability,release,releases,6294,"; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:6491,Deployability,install,install,6491,"ome/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:6765,Deployability,install,install,6765,"in/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git Open",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:6779,Deployability,install,install,6779,"in/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git Open",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:7399,Deployability,install,install,7399,"tobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz; tar xzf boost_1_66_0.tar.gz; cd boost_1_66_0; ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:7458,Deployability,install,install,7458,"d for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz; tar xzf boost_1_66_0.tar.gz; cd boost_1_66_0; ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:7469,Deployability,install,install,7469,"d for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz; tar xzf boost_1_66_0.tar.gz; cd boost_1_66_0; ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:7522,Deployability,install,install,7522,"/usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz; tar xzf boost_1_66_0.tar.gz; cd boost_1_66_0; ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:7819,Deployability,install,install,7819,"ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz; tar xzf boost_1_66_0.tar.gz; cd boost_1_66_0; ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash; # development packages; yum install python-devel python-pip -y. # dependency of numpy 1.14.6; OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:7900,Deployability,release,release,7900,"c)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz; tar xzf boost_1_66_0.tar.gz; cd boost_1_66_0; ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash; # development packages; yum install python-devel python-pip -y. # dependency of numpy 1.14.6; OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:8083,Deployability,install,install,8083,"thub.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz; tar xzf boost_1_66_0.tar.gz; cd boost_1_66_0; ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash; # development packages; yum install python-devel python-pip -y. # dependency of numpy 1.14.6; OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6; # verify; python -c ""import numpy"". # dependecy of scipy 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:8220,Deployability,install,install,8220,"# share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz; tar xzf boost_1_66_0.tar.gz; cd boost_1_66_0; ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash; # development packages; yum install python-devel python-pip -y. # dependency of numpy 1.14.6; OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6; # verify; python -c ""import numpy"". # dependecy of scipy 1.2.0; OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-bui",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:8263,Deployability,install,install,8263,"rsion # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz; tar xzf boost_1_66_0.tar.gz; cd boost_1_66_0; ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash; # development packages; yum install python-devel python-pip -y. # dependency of numpy 1.14.6; OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6; # verify; python -c ""import numpy"". # dependecy of scipy 1.2.0; OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:8602,Deployability,install,install,8602,"n -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz; tar xzf boost_1_66_0.tar.gz; cd boost_1_66_0; ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash; # development packages; yum install python-devel python-pip -y. # dependency of numpy 1.14.6; OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6; # verify; python -c ""import numpy"". # dependecy of scipy 1.2.0; OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0; # verify; python -c ""import scipy"". # pip package dependencies; # pip install pip six wheel mock; pip install wheel autograd h5py==2.9.0 enum34; pip install keras_applications==1.0.6 keras_pr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
https://github.com/google/deepvariant/issues/123#issuecomment-469190994:8992,Deployability,install,install,8992,"rap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash; # development packages; yum install python-devel python-pip -y. # dependency of numpy 1.14.6; OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6; # verify; python -c ""import numpy"". # dependecy of scipy 1.2.0; OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0; # verify; python -c ""import scipy"". # pip package dependencies; # pip install pip six wheel mock; pip install wheel autograd h5py==2.9.0 enum34; pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code; git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12; cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc; PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \; PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \; TF_NEED_IGNITE=""0"" \; TF_ENABLE_XLA=""0"" \; TF_NEED_OPENCL_SYCL=""0"" \; TF_NEED_ROCM=""0"" \; T",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994
